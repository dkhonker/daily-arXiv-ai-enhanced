<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 127]
- [cs.AI](#cs.AI) [总数: 43]
- [stat.ML](#stat.ML) [总数: 14]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes](https://arxiv.org/abs/2506.01959)
*Irmi Schneider*

**主要类别:** cs.LG

**AI概要:** 本文研究了神经网络和其他系统中损失函数的对称性现象，发现所有临界点均具有显著的对称性，并提出新的对称性度量来揭示更多未被捕捉的对称性结构。


<details>
  <summary>更多</summary>
  
**动机:** 最近的工作探讨了神经网络中损失函数的不变性（行和列置换下的不变性）以及局部最小值的对称性现象。这项工作将探索扩展到更广泛的实值损失函数空间，以进一步理解这种现象。

**方法:** 研究定义在更广泛空间类上的实值损失函数的对称性现象，并介绍四种新情况：有限域上的射影情况、八面体图情况、完美匹配情况和粒子吸引力情况。

**结果:** 观察到的所有临界点都具有非平凡的对称性，与神经网络的情况类似。此外，引入的新对称性度量揭示了之前未被捕捉到的更多对称性结构。

**结论:** 通过引入新的对称性度量，揭示了比以前的度量更多的对称性结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ubiquitous+Symmetry+at+Critical+Points+Across+Diverse+Optimization+Landscapes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01959，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01959&send_immediately=true&force_search=false)

**原文摘要:** Symmetry plays a crucial role in understanding the properties of mathematical
structures and optimization problems. Recent work has explored this phenomenon
in the context of neural networks, where the loss function is invariant under
column and row permutations of the network weights. It has been observed that
local minima exhibit significant symmetry with respect to the network weights
(invariance to row and column permutations). And moreover no critical point was
found that lacked symmetry. We extend this line of inquiry by investigating
symmetry phenomena in real-valued loss functions defined on a broader class of
spaces. We will introduce four more cases: the projective case over a finite
field, the octahedral graph case, the perfect matching case, and the particle
attraction case. We show that as in the neural network case, all the critical
points observed have non-trivial symmetry. Finally we introduce a new measure
of symmetry in the system and show that it reveals additional symmetry
structures not captured by the previous measure.

</details>


### [2] [Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition](https://arxiv.org/abs/2506.01962)
*Xiaozhou Ye, Kevin I-Kai Wang*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于图神经网络与对抗域泛化的新型方法GNN-ADG，用于解决传感器数据中跨用户识别活动的泛化问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于传统模型在不同用户间难以泛化，主要因为行为、传感器位置和数据分布的差异，因此需要一种能够鲁棒适应不同用户的活动识别方法。

**方法:** GNN-ADG结合了图神经网络和对抗学习的优势，通过建模传感器之间的空间关系，提取三种解剖单元（互连单元、类比单元和侧向单元），并采用循环训练策略将这些信息融合到一个统一的图结构中。

**结果:** 实验表明，GNN-ADG能够动态整合空间、功能和侧向相关性，从而形成整体且与用户无关的表示，在未见过的用户上具有良好的泛化能力，且不需要目标用户的数据进行训练。

**结论:** GNN-ADG有效解决了跨用户变异性带来的挑战，为实际应用中的用户自适应活动识别提供了可行方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph-Based+Adversarial+Domain+Generalization+with+Anatomical+Correlation+Knowledge+for+Cross-User+Human+Activity+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01962，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01962&send_immediately=true&force_search=false)

**原文摘要:** Cross-user variability poses a significant challenge in sensor-based Human
Activity Recognition (HAR) systems, as traditional models struggle to
generalize across users due to differences in behavior, sensor placement, and
data distribution. To address this, we propose GNN-ADG (Graph Neural Network
with Adversarial Domain Generalization), a novel method that leverages both the
strength from both the Graph Neural Networks (GNNs) and adversarial learning to
achieve robust cross-user generalization. GNN-ADG models spatial relationships
between sensors on different anatomical body parts, extracting three types of
Anatomical Units: (1) Interconnected Units, capturing inter-relations between
neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or
functionally similar body parts; and (3) Lateral Units, connecting sensors
based on their position to capture region-specific coordination. These units
information are fused into an unified graph structure with a cyclic training
strategy, dynamically integrating spatial, functional, and lateral correlations
to facilitate a holistic, user-invariant representation. Information fusion
mechanism of GNN-ADG occurs by iteratively cycling through edge topologies
during training, allowing the model to refine its understanding of inter-sensor
relationships across diverse perspectives. By representing the spatial
configuration of sensors as an unified graph and incorporating adversarial
learning, Information Fusion GNN-ADG effectively learns features that
generalize well to unseen users without requiring target user data during
training, making it practical for real-world applications.

</details>


### [3] [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
*Andrew Kiruluta, Preethi Raju, Priscilla Burity*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新型的非注意力机制架构，用于解决传统Transformer模型在处理超长文本时的高计算和内存消耗问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统的Transformer设计由于自注意力机制的性质，在处理长序列时会遇到内存和计算的二次增长问题。为了解决这一问题，提出了这种新的架构。

**方法:** 该架构完全避免了传统的自注意力机制，采用了一系列互补组件：状态空间块、多分辨率卷积层、轻量级循环监督器和检索增强外部存储器。

**结果:** 新模型可以高效地处理长达数十万甚至可能数百万token的上下文窗口，并且在序列长度上接近线性扩展。

**结论:** 本文提出了一种新的非注意力机制架构，用于处理超长上下文窗口的大型语言模型，且不需要标记到标记的注意力计算。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+Quadratic+Barriers%3A+A+Non-Attention+LLM+for+Ultra-Long+Context+Horizons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01963，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01963&send_immediately=true&force_search=false)

**原文摘要:** We present a novel non attention based architecture for large language models
(LLMs) that efficiently handles very long context windows, on the order of
hundreds of thousands to potentially millions of tokens. Unlike traditional
Transformer designs, which suffer from quadratic memory and computation
overload due to the nature of the self attention mechanism, our model avoids
token to token attention entirely. Instead, it combines the following
complementary components: State Space blocks (inspired by S4) that learn
continuous time convolution kernels and scale near linearly with sequence
length, Multi Resolution Convolution layers that capture local context at
different dilation levels, a lightweight Recurrent Supervisor to maintain a
global hidden state across sequential chunks, and Retrieval Augmented External
Memory that stores and retrieves high-level chunk embeddings without
reintroducing quadratic operations.

</details>


### [4] [A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction](https://arxiv.org/abs/2506.01964)
*Kamal Acharya, Mehul Lad, Liang Sun, Houbing Song*

**主要类别:** cs.LG

**AI概要:** 这项研究通过将多源数据与机器学习结合，有效提高了交通规划中重力模型的预测精度和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 传统重力模型无法充分反映现代出行行为的复杂因素，因此需要一种更精确的数据驱动方法。

**方法:** 利用来自田纳西州和纽约州的地理、经济、社会和出行数据，并使用机器学习技术扩展传统重力模型的能力。

**结果:** 改进后的模型在R-squared上提升了51.48%，平均绝对误差（MAE）减少了63.59%，通勤者共同部分（CPC）增加了44.32%。

**结论:** 该研究通过整合多源数据并采用机器学习技术，显著提升了传统重力模型在交通预测中的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Data-Driven+Approach+to+Enhancing+Gravity+Models+for+Trip+Demand+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01964，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01964&send_immediately=true&force_search=false)

**原文摘要:** Accurate prediction of trips between zones is critical for transportation
planning, as it supports resource allocation and infrastructure development
across various modes of transport. Although the gravity model has been widely
used due to its simplicity, it often inadequately represents the complex
factors influencing modern travel behavior. This study introduces a data-driven
approach to enhance the gravity model by integrating geographical, economic,
social, and travel data from the counties in Tennessee and New York state.
Using machine learning techniques, we extend the capabilities of the
traditional model to handle more complex interactions between variables. Our
experiments demonstrate that machine learning-enhanced models significantly
outperform the traditional model. Our results show a 51.48% improvement in
R-squared, indicating a substantial enhancement in the model's explanatory
power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a
significant increase in prediction accuracy. Furthermore, a 44.32% increase in
Common Part of Commuters (CPC) demonstrates improved prediction reliability.
These findings highlight the substantial benefits of integrating diverse
datasets and advanced algorithms into transportation models. They provide urban
planners and policymakers with more reliable forecasting and decision-making
tools.

</details>


### [5] [TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition](https://arxiv.org/abs/2506.01965)
*Bonpagna Kann, Sandra Castellanos-Paez, Romain Rombourg, Philippe Lalanda*

**主要类别:** cs.LG

**AI概要:** 论文介绍了TaskVAE框架，在持续学习场景下利用任务特定变分自编码器生成历史任务样本，以有效应对数据漂移问题，尤其适用于人体活动识别(HAR)场景。


<details>
  <summary>更多</summary>
  
**动机:** 为了适应动态数据环境并减少模型遗忘先前知识的现象，需要持续学习方法，尤其是回放策略的有效应用。

**方法:** 采用任务特定的变分自编码器(TaskVAE)从以前的任务中生成合成样例，并与新任务数据一起用于训练分类器。

**结果:** TaskVAE在5个不同的HAR数据集中优于经验回放方法，尤其是在数据有限的情况下表现突出，同时其内存占用最小且可生成无限数量的合成样本。

**结论:** TaskVAE在HAR领域中提供了一种可靠解决方案，能够平衡内存约束、任务特定生成和长期稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TaskVAE%3A+Task-Specific+Variational+Autoencoders+for+Exemplar+Generation+in+Continual+Learning+for+Human+Activity+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01965&send_immediately=true&force_search=false)

**原文摘要:** As machine learning based systems become more integrated into daily life,
they unlock new opportunities but face the challenge of adapting to dynamic
data environments. Various forms of data shift-gradual, abrupt, or
cyclic-threaten model accuracy, making continual adaptation essential.
Continual Learning (CL) enables models to learn from evolving data streams
while minimizing forgetting of prior knowledge. Among CL strategies,
replay-based methods have proven effective, but their success relies on
balancing memory constraints and retaining old class accuracy while learning
new classes. This paper presents TaskVAE, a framework for replay-based CL in
class-incremental settings. TaskVAE employs task-specific Variational
Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which
are then used to train the classifier alongside new task data. In contrast to
traditional methods that require prior knowledge of the total class count or
rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks
without such constraints. We focus on Human Activity Recognition (HAR) using
IMU sensor-equipped devices. Unlike previous HAR studies that combine data
across all users, our approach focuses on individual user data, better
reflecting real-world scenarios where a person progressively learns new
activities. Extensive experiments on 5 different HAR datasets show that TaskVAE
outperforms experience replay methods, particularly with limited data, and
exhibits robust performance as dataset size increases. Additionally, memory
footprint of TaskVAE is minimal, being equivalent to only 60 samples per task,
while still being able to generate an unlimited number of synthetic samples.
The contributions lie in balancing memory constraints, task-specific
generation, and long-term stability, making it a reliable solution for
real-world applications in domains like HAR.

</details>


### [6] [Matrix Is All You Need](https://arxiv.org/abs/2506.01966)
*Yuzhou Zhu*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种统一的矩阵框架，将卷积、循环和自注意力机制表示为稀疏矩阵乘法，并通过实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络在不同任务中使用专门的架构，这种多样性掩盖了它们之间的共性。因此，作者希望找到一种统一的方式来描述这些不同的操作。

**方法:** 将卷积神经网络、循环神经网络和自注意力机制转化为稀疏矩阵乘法，并通过实验证明其性能匹配或超过原生模型。

**结果:** 提出的稀疏矩阵形式在图像分类、时间序列预测和语言建模等任务上表现出色，同时减少了架构设计到稀疏模式选择的问题。

**结论:** 该论文提出了一种统一的矩阵阶框架，将卷积、循环和自注意力操作统一为稀疏矩阵乘法，证明了其与标准CNN、RNN和Transformer层的代数同构性，并通过实验验证了稀疏矩阵形式在多种任务上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Matrix+Is+All+You+Need，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01966，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01966&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks employ specialized architectures for vision, sequential
and language tasks, yet this proliferation obscures their underlying
commonalities. We introduce a unified matrix-order framework that casts
convolutional, recurrent and self-attention operations as sparse matrix
multiplications. Convolution is realized via an upper-triangular weight matrix
performing first-order transformations; recurrence emerges from a
lower-triangular matrix encoding stepwise updates; attention arises naturally
as a third-order tensor factorization. We prove algebraic isomorphism with
standard CNN, RNN and Transformer layers under mild assumptions. Empirical
evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),
time-series forecasting (ETTh1, Electricity Load Diagrams) and language
modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that
sparse-matrix formulations match or exceed native model performance while
converging in comparable or fewer epochs. By reducing architecture design to
sparse pattern selection, our matrix perspective aligns with GPU parallelism
and leverages mature algebraic optimization tools. This work establishes a
mathematically rigorous substrate for diverse neural architectures and opens
avenues for principled, hardware-aware network design.

</details>


### [7] [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
*Patrik Czakó, Gábor Kertész, Sándor Szénási*

**主要类别:** cs.LG

**AI概要:** 本论文研究了大语言模型在量化过程中因离群值带来的量化误差问题，并提出了一种新的量化难度度量方法和一种混合量化策略以减少误差。


<details>
  <summary>更多</summary>
  
**动机:** 激活整数运算需要同时量化权重和激活值，但由于大语言模型中的显著离群值会增加量化误差，这带来了挑战。

**方法:** 研究了大语言模型中显著离群值对逐层量化误差的影响，并探讨了平滑和旋转如何转换观测值。

**结果:** 引入了一个新的度量标准用于衡量量化难度，并提出了一种结合通道级缩放和旋转的混合方法来减少量化误差。

**结论:** 本文提出了一个基于通道幅值的新度量标准来衡量和可视化量化难度，并在旋转前应用通道级缩放的混合方法，通过数学公式证明了其优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Turning+LLM+Activations+Quantization-Friendly，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01967&send_immediately=true&force_search=false)

**原文摘要:** Quantization effectively reduces the serving costs of Large Language Models
(LLMs) by speeding up data movement through compressed parameters and enabling
faster operations via integer arithmetic. However, activating integer
arithmetic requires quantizing both weights and activations, which poses
challenges due to the significant outliers in LLMs that increase quantization
error. In this work, we investigate these outliers with an emphasis on their
effect on layer-wise quantization error, then examine how smoothing and
rotation transform the observed values. Our primary contributions include
introducing a new metric to measure and visualize quantization difficulty based
on channel magnitudes, as well as proposing a hybrid approach that applies
channel-wise scaling before rotation, supported by a mathematical formulation
of its benefits.

</details>


### [8] [Efficient ANN-SNN Conversion with Error Compensation Learning](https://arxiv.org/abs/2506.01968)
*Chang Liu, Jiangrong Shen, Xuming Ran, Mingkun Xu, Qi Xu, Yi Xu, Gang Pan*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的ANN-to-SNN转换框架，有效解决了转换过程中的误差问题，从而提高了尖峰神经网络在资源受限环境中的应用潜力。


<details>
  <summary>更多</summary>
  
**动机:** 由于传统的人工神经网络在资源受限环境中部署存在挑战，而尖峰神经网络提供了一个生物启发的替代方案，因此提出了此研究。然而，当前的ANN到SNN转换往往导致显著的准确率损失和增加的推理时间。

**方法:** 提出了一种基于误差补偿学习的ANN-to-SNN转换框架，包括可学习阈值剪切函数、双阈值神经元和优化的膜电位初始化策略。

**结果:** 实验结果表明，该方法在CIFAR-10、CIFAR-100和ImageNet数据集上都达到了高精度和超低延迟，仅使用两个时间步骤就显著减少了推理时间，同时保持了在ResNet-18结构下CIFAR-10数据集94.75%的竞争性准确率。

**结论:** 这项研究促进了SNN在低功耗硬件上的实际应用，使得高效的实时处理成为可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+ANN-SNN+Conversion+with+Error+Compensation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01968，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01968&send_immediately=true&force_search=false)

**原文摘要:** Artificial neural networks (ANNs) have demonstrated outstanding performance
in numerous tasks, but deployment in resource-constrained environments remains
a challenge due to their high computational and memory requirements. Spiking
neural networks (SNNs) operate through discrete spike events and offer superior
energy efficiency, providing a bio-inspired alternative. However, current
ANN-to-SNN conversion often results in significant accuracy loss and increased
inference time due to conversion errors such as clipping, quantization, and
uneven activation. This paper proposes a novel ANN-to-SNN conversion framework
based on error compensation learning. We introduce a learnable threshold
clipping function, dual-threshold neurons, and an optimized membrane potential
initialization strategy to mitigate the conversion error. Together, these
techniques address the clipping error through adaptive thresholds, dynamically
reduce the quantization error through dual-threshold neurons, and minimize the
non-uniformity error by effectively managing the membrane potential.
Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our
method achieves high-precision and ultra-low latency among existing conversion
methods. Using only two time steps, our method significantly reduces the
inference time while maintains competitive accuracy of 94.75% on CIFAR-10
dataset under ResNet-18 structure. This research promotes the practical
application of SNNs on low-power hardware, making efficient real-time
processing possible.

</details>


### [9] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2506.01970)
*Ruizhuo Song, Beiming Yuan*

**主要类别:** cs.LG

**AI概要:** 论文研究了增强AI抽象推理能力的挑战，特别是涉及复杂人类概念的Raven渐进矩阵（RPM）任务。


<details>
  <summary>更多</summary>
  
**动机:** 传统端到端RPM解决模型严重依赖选项池配置，这种依赖限制了模型的推理能力，因此需要一种新的方法来提升AI在RPM任务中的表现。

**方法:** 提出了Johnny架构，通过其表示提取模块和推理模块的协同操作，显著提高了推理性能；同时引入了Spin-Transformer网络架构及其轻量级变体Straw Spin-Transformer以减少计算开销。

**结果:** 实验评估表明，Johnny和Spin-Transformer在RPM任务中表现出色，为提升AI的抽象推理能力提供了创新的方法。

**结论:** 这项研究不仅解决了传统模型对选项池配置的过度依赖问题，还通过新架构增强了AI的抽象推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Johnny%3A+Structuring+Representation+Space+to+Enhance+Machine+Abstract+Reasoning+Ability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01970&send_immediately=true&force_search=false)

**原文摘要:** This paper thoroughly investigates the challenges of enhancing AI's abstract
reasoning capabilities, with a particular focus on Raven's Progressive Matrices
(RPM) tasks involving complex human-like concepts. Firstly, it dissects the
empirical reality that traditional end-to-end RPM-solving models heavily rely
on option pool configurations, highlighting that this dependency constrains the
model's reasoning capabilities. To address this limitation, the paper proposes
the Johnny architecture - a novel representation space-based framework for
RPM-solving. Through the synergistic operation of its Representation Extraction
Module and Reasoning Module, Johnny significantly enhances reasoning
performance by supplementing primitive negative option configurations with a
learned representation space. Furthermore, to strengthen the model's capacity
for capturing positional relationships among local features, the paper
introduces the Spin-Transformer network architecture, accompanied by a
lightweight Straw Spin-Transformer variant that reduces computational overhead
through parameter sharing and attention mechanism optimization. Experimental
evaluations demonstrate that both Johnny and Spin-Transformer achieve superior
performance on RPM tasks, offering innovative methodologies for advancing AI's
abstract reasoning capabilities.

</details>


### [10] [Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh](https://arxiv.org/abs/2506.01974)
*Kanwal Aalijah*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了人工智能在解决城市交通拥堵和提升居民通勤体验方面的应用，通过实时数据和情感分析提供动态的城市规划方法。


<details>
  <summary>更多</summary>
  
**动机:** 现代城市面临因快速城市化导致的交通拥堵挑战，而了解交通与居民情感相关问题对于城市规划至关重要。

**方法:** 结合实时交通数据与地理定位情感分析，使用AI模型和探索性数据分析方法进行研究。

**结果:** 研究成功识别了交通拥堵模式、通勤行为特征以及城市中的拥堵热点和不满区域，并提出了针对交通流动优化和通勤体验改善的建议。

**结论:** 研究得出AI模型与探索性数据分析能够有效预测交通拥堵模式、分析通勤行为，并识别中东及其他地区城市的拥堵热点和不满区域，为优化交通流量和提升通勤体验提供了可操作的建议。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Traffic+and+Mobility+Optimization+Using+AI%3A+Comparative+Study+between+Dubai+and+Riyadh，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01974，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01974&send_immediately=true&force_search=false)

**原文摘要:** Urban planning plays a very important role in development modern cities. It
effects the economic growth, quality of life, and environmental sustainability.
Modern cities face challenges in managing traffic congestion. These challenges
arise to due to rapid urbanization. In this study we will explore how AI can be
used to understand the traffic and mobility related issues and its effects on
the residents sentiment. The approach combines real-time traffic data with
geo-located sentiment analysis, offering a comprehensive and dynamic approach
to urban mobility planning. AI models and exploratory data analysis was used to
predict traffic congestion patterns, analyze commuter behaviors, and identify
congestion hotspots and dissatisfaction zones. The findings offer actionable
recommendations for optimizing traffic flow, enhancing commuter experiences,
and addressing city specific mobility challenges in the Middle East and beyond.

</details>


### [11] [An empirical study of task and feature correlations in the reuse of pre-trained models](https://arxiv.org/abs/2506.01975)
*Jama Hussein Mohamud*

**主要类别:** cs.LG

**AI概要:** 该论文探讨了Bob能够成功重用Alice预训练模型的原因，发现任务间的相关性、网络结构以及优化器的选择对迁移学习的效果有重要影响。


<details>
  <summary>更多</summary>
  
**动机:** 动机是理解为什么Bob能够成功地重用Alice预训练的神经网络来完成不同的任务。

**方法:** 论文介绍了一种实验设置，用于研究影响Bob实证成功的因素，并在受控的真实场景中测试了预训练网络的重用效果。

**结果:** 结果显示，即使任务和输入特征不相关，Bob仍能取得显著优于随机的结果；同时发现重用较低层网络更为有效，并推测重新训练的层数可反映任务相关性。

**结论:** 论文得出结论，Bob的成功可能只是因为运气好，任务准确性随着与Alice任务的相关性的增加而单调增加。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+empirical+study+of+task+and+feature+correlations+in+the+reuse+of+pre-trained+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01975&send_immediately=true&force_search=false)

**原文摘要:** Pre-trained neural networks are commonly used and reused in the machine
learning community. Alice trains a model for a particular task, and a part of
her neural network is reused by Bob for a different task, often to great
effect. To what can we ascribe Bob's success? This paper introduces an
experimental setup through which factors contributing to Bob's empirical
success could be studied in silico. As a result, we demonstrate that Bob might
just be lucky: his task accuracy increases monotonically with the correlation
between his task and Alice's. Even when Bob has provably uncorrelated tasks and
input features from Alice's pre-trained network, he can achieve significantly
better than random performance due to Alice's choice of network and optimizer.
When there is little correlation between tasks, only reusing lower pre-trained
layers is preferable, and we hypothesize the converse: that the optimal number
of retrained layers is indicative of task and feature correlation. Finally, we
show in controlled real-world scenarios that Bob can effectively reuse Alice's
pre-trained network if there are semantic correlations between his and Alice's
task.

</details>


### [12] [Crack Path Prediction with Operator Learning using Discrete Particle System data Generation](https://arxiv.org/abs/2506.01976)
*Elham Kiyani, Venkatesh Ananchaperumal, Ahmad Peyvan, Mahendaran Uchimali, Gang Li, George Em Karniadakis*

**主要类别:** cs.LG

**AI概要:** 本文研究通过使用基于本构行为的粒子动力学（CPD）模拟数据训练深度操作网络（DeepONets），探索了裂纹扩展的预测方法，结果显示Fusion DeepONet在复杂几何和时间依赖问题上具有更强的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 准确建模裂纹扩展对于预测工程材料和结构的失效至关重要，尤其是当小裂纹迅速演化并导致灾难性破坏时。裂纹与不连续处（如孔洞）的相互作用显著影响裂纹偏转和止裂。

**方法:** 使用基于本构行为的粒子动力学（CPD）模拟的数据训练了两种深度操作网络（DeepONets）变体：vanilla DeepONet和Fusion DeepONet，以预测裂纹扩展。

**结果:** 结果表明，Fusion DeepONet始终优于vanilla DeepONet，尤其在无断裂情况下预测更为准确。然而，涉及位移和裂纹演化的断裂驱动场景仍然更具挑战性。

**结论:** Fusion DeepONet展现了在复杂、几何变化和时间依赖的裂纹扩展现象中泛化的潜力，优于普通DeepONet。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Crack+Path+Prediction+with+Operator+Learning+using+Discrete+Particle+System+data+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01976，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01976&send_immediately=true&force_search=false)

**原文摘要:** Accurately modeling crack propagation is critical for predicting failure in
engineering materials and structures, where small cracks can rapidly evolve and
cause catastrophic damage. The interaction of cracks with discontinuities, such
as holes, significantly affects crack deflection and arrest. Recent
developments in discrete particle systems with multibody interactions based on
constitutive behavior have demonstrated the ability to capture crack nucleation
and evolution without relying on continuum assumptions. In this work, we use
data from Constitutively Informed Particle Dynamics (CPD) simulations to train
operator learning models, specifically Deep Operator Networks (DeepONets),
which learn mappings between function spaces instead of finite-dimensional
vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for
predicting time-evolving crack propagation in specimens with varying
geometries. Three representative cases are studied: (i) varying notch height
without active fracture; and (ii) and (iii) combinations of notch height and
hole radius where dynamic fracture occurs on irregular discrete meshes. The
models are trained on 32 to 45 samples, using geometric inputs in the branch
network and spatial-temporal coordinates in the trunk network. Results show
that Fusion DeepONet consistently outperforms the vanilla variant, with more
accurate predictions especially in non-fracturing cases. Fracture-driven
scenarios involving displacement and crack evolution remain more challenging.
These findings highlight the potential of Fusion DeepONet to generalize across
complex, geometry-varying, and time-dependent crack propagation phenomena.

</details>


### [13] [Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN](https://arxiv.org/abs/2506.01977)
*Wei Huang, Hanchen Wang, Dong Wen, Shaozhen Ma, Wenjie Zhang, Xuemin Lin*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为GEDRanker的无监督方法，用于解决图编辑距离计算问题，该方法通过引入基于GAN的框架，实现了不依赖真实标签的高质量节点匹配和接近最优的解性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的混合GED求解器虽然性能良好，但通常严重依赖于代价高昂的真实标签监督，因此需要一种无需真实标签的新型无监督方法。

**方法:** 提出了一种基于GAN的新框架GEDRanker，包含一个基于匹配的GED求解器和一个可解释的偏好感知判别器，并采用有效的训练策略引导生成高质量节点匹配。

**结果:** 在基准数据集上的大量实验表明，GEDRanker能够使基于匹配的GED求解器在不需要真实标签的情况下达到接近最优的解质量。

**结论:** GEDRanker能够在没有真实标签的情况下实现接近最优的解质量，为计算图编辑距离提供了一种有效的无监督方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Unsupervised+Training+of+Matching-based+Graph+Edit+Distance+Solver+via+Preference-aware+GAN，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01977&send_immediately=true&force_search=false)

**原文摘要:** Graph Edit Distance (GED) is a fundamental graph similarity metric widely
used in various applications. However, computing GED is an NP-hard problem.
Recent state-of-the-art hybrid GED solver has shown promising performance by
formulating GED as a bipartite graph matching problem, then leveraging a
generative diffusion model to predict node matching between two graphs, from
which both the GED and its corresponding edit path can be extracted using a
traditional algorithm. However, such methods typically rely heavily on
ground-truth supervision, where the ground-truth labels are often costly to
obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel
unsupervised GAN-based framework for GED computation. Specifically, GEDRanker
consists of a matching-based GED solver and introduces an interpretable
preference-aware discriminator with an effective training strategy to guide the
matching-based GED solver toward generating high-quality node matching without
the need for ground-truth labels. Extensive experiments on benchmark datasets
demonstrate that our GEDRanker enables the matching-based GED solver to achieve
near-optimal solution quality without any ground-truth supervision.

</details>


### [14] [Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification](https://arxiv.org/abs/2506.01983)
*Reyhaneh Keshavarzpour, Eghbal Mansoori*

**主要类别:** cs.LG

**AI概要:** 本研究通过改进抗菌肽预测方法，结合最佳编码技术和深度神经网络，实现了更高的预测准确性与效率。


<details>
  <summary>更多</summary>
  
**动机:** 抗菌肽作为抗生素的替代品，在生物医学和其他实际应用中至关重要，因此需要改进识别方法。

**方法:** 结合不同视角的最佳编码方法，并使用深度神经网络平衡不平衡的组合数据集。

**结果:** 与现有方法相比，所提出的方法在抗菌肽预测的准确性和效率上有显著提升。

**结论:** 该研究在抗菌肽预测领域具有较高的有效性和应用价值，特别是在医学和制药工业领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improvement+of+AMPs+Identification+with+Generative+Adversarial+Network+and+Ensemble+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01983&send_immediately=true&force_search=false)

**原文摘要:** Identification of antimicrobial peptides is an important and necessary issue
in today's era. Antimicrobial peptides are essential as an alternative to
antibiotics for biomedical applications and many other practical applications.
These oligopeptides are useful in drug design and cause innate immunity against
microorganisms. Artificial intelligence algorithms have played a significant
role in the ease of identifying these peptides.This research is improved by
improving proposed method in the field of antimicrobial peptides prediction.
Suggested method is improved by combining the best coding method from different
perspectives, In the following a deep neural network to balance the imbalanced
combined datasets. The results of this research show that the proposed method
have a significant improvement in the accuracy and efficiency of the prediction
of antimicrobial peptides and are able to provide the best results compared to
the existing methods. These development in the field of prediction and
classification of antimicrobial peptides, basically in the fields of medicine
and pharmaceutical industries, have high effectiveness and application.

</details>


### [15] [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/abs/2506.01986)
*Selin Yildirim, Deming Chen*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为SpecMemo的推理引擎，能够在内存受限的设备上有效实现推测解码，从而提升大型语言模型的应用效率。


<details>
  <summary>更多</summary>
  
**动机:** 推测解码通常需要额外的内存分配，在内存受限的设备上部署存在挑战。

**方法:** 通过理论建模推测解码的内存占用，并在多个GPU上分配Llama-2-70B-Chat模型以实现批量推测解码。

**结果:** SpecMemo减少了65%的生成内存占用，并在多个AMD MI250 GPU上实现了2倍的加速。

**结论:** 论文提出了一种设备感知的推理引擎SpecMemo，可以在内存受限的设备上实现高效的推测解码。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpecMemo%3A+Speculative+Decoding+is+in+Your+Pocket，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01986&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in speculative decoding have demonstrated considerable
speedup across a wide array of large language model (LLM) tasks. Speculative
decoding inherently relies on sacrificing extra memory allocations to generate
several candidate tokens, of which acceptance rate drives the speedup. However,
deploying speculative decoding on memory-constrained devices, such as mobile
GPUs, remains as a significant challenge in real-world scenarios. In this work,
we present a device-aware inference engine named SpecMemo that can smartly
control memory allocations at finer levels to enable multi-turn chatbots with
speculative decoding on such limited memory devices. Our methodology stems from
theoretically modeling memory footprint of speculative decoding to determine a
lower bound on the required memory budget while retaining speedup. SpecMemo
empirically acquires a careful balance between minimizing redundant memory
allocations for rejected candidate tokens and maintaining competitive
performance gains from speculation. Notably, with SpecMemo's memory management,
we maintain 96% of overall throughput from speculative decoding on MT-Bench,
with reduced generation-memory by 65% on single Nvidia Titan RTX. Given
multiple constrained GPUs, we build on top of previous speculative decoding
architectures to facilitate big-model inference by distributing
Llama-2-70B-Chat model, on which we provide novel batched speculative decoding
to increase usability of multiple small server GPUs. This novel framework
demonstrates 2x speedup over distributed and batched vanilla decoding with the
base model on eight AMD MI250 GPUs. Moreover, inference throughput increases
remarkably 8x with batch size 10. Our work contributes to democratized LLM
applications in resource-constrained environments, providing a pathway for
faster and cheaper deployment of real-world LLM applications with robust
performance.

</details>


### [16] [Equally Critical: Samples, Targets, and Their Mappings in Datasets](https://arxiv.org/abs/2506.01987)
*Runkang Yang, Peng Sun, Xinyi Shang, Yi Tang, Tao Lin*

**主要类别:** cs.LG

**AI概要:** 该研究探讨了样本和目标在模型训练中的综合影响，并提出了一个统一的损失框架以提高训练效率，同时提供了六个提升训练效果的关键见解。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究要么侧重于利用知识蒸馏加速模型收敛，要么关注数据高效学习中的样本优化技术，而忽略了目标的关键作用。这种二元性促使作者研究样本和目标如何共同影响训练动态。

**方法:** 建立了一个分类法，通过样本-目标交互视角分析现有范式，并提出了一种新的统一损失框架以评估不同策略的影响。

**结果:** 通过广泛的实证研究，作者全面分析了目标和样本类型、数量和质量的变化对模型训练的影响，并得出了六个关键洞察力。

**结论:** 论文总结了样本和目标在模型训练中的共同影响，并提出了一个新的统一损失框架来评估不同的样本到目标映射策略对训练效率的影响，从而提供了增强训练效果的六个关键见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Equally+Critical%3A+Samples%2C+Targets%2C+and+Their+Mappings+in+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01987&send_immediately=true&force_search=false)

**原文摘要:** Data inherently possesses dual attributes: samples and targets. For targets,
knowledge distillation has been widely employed to accelerate model
convergence, primarily relying on teacher-generated soft target supervision.
Conversely, recent advancements in data-efficient learning have emphasized
sample optimization techniques, such as dataset distillation, while neglected
the critical role of target. This dichotomy motivates our investigation into
understanding how both sample and target collectively influence training
dynamic. To address this gap, we first establish a taxonomy of existing
paradigms through the lens of sample-target interactions, categorizing them
into distinct sample-to-target mapping strategies. Building upon this
foundation, we then propose a novel unified loss framework to assess their
impact on training efficiency. Through extensive empirical studies on our
proposed strategies, we comprehensively analyze how variations in target and
sample types, quantities, and qualities influence model training, providing six
key insights to enhance training efficacy.

</details>


### [17] [Surrogate Interpretable Graph for Random Decision Forests](https://arxiv.org/abs/2506.01988)
*Akshat Dubey, Aleksandar Anžel, Georges Hattab*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了随机森林模型在健康信息学领域的应用，以及如何通过一种新的方法——代理可解释性图，提升模型的全局可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 动机是解决随机森林模型中特征和估计器数量增加导致的领域专家无法准确解释全局特征交互的问题，从而影响信任和法规遵循。

**方法:** 该论文提出了一种名为代理可解释性图的方法，利用图和混合整数线性规划来分析和可视化特征交互。

**结果:** 结果是通过可视化每个决策特征交互表中的特征使用情况和预测的最主导的层次决策特征交互，改进了特征交互的可解释性。

**结论:** 论文的结论是，代理可解释性图增强了全局可解释性，这对于高风险领域至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Surrogate+Interpretable+Graph+for+Random+Decision+Forests，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01988&send_immediately=true&force_search=false)

**原文摘要:** The field of health informatics has been profoundly influenced by the
development of random forest models, which have led to significant advances in
the interpretability of feature interactions. These models are characterized by
their robustness to overfitting and parallelization, making them particularly
useful in this domain. However, the increasing number of features and
estimators in random forests can prevent domain experts from accurately
interpreting global feature interactions, thereby compromising trust and
regulatory compliance. A method called the surrogate interpretability graph has
been developed to address this issue. It uses graphs and mixed-integer linear
programming to analyze and visualize feature interactions. This improves their
interpretability by visualizing the feature usage per
decision-feature-interaction table and the most dominant hierarchical decision
feature interactions for predictions. The implementation of a surrogate
interpretable graph enhances global interpretability, which is critical for
such a high-stakes domain.

</details>


### [18] [Coded Robust Aggregation for Distributed Learning under Byzantine Attacks](https://arxiv.org/abs/2506.01989)
*Chengxi Li, Ming Xiao, Mikael Skoglund*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种应对拜占庭攻击的新分布式学习方法CRA-DL，通过编码诚实设备的梯度并使用鲁棒聚合规则，有效提升了学习性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的分布式学习方法在应用鲁棒有界聚合规则时，由于不同设备的局部梯度差异较大，导致学习性能显著下降，特别是在存在拜占庭攻击的情况下。

**方法:** 提出了一种新的基于编码鲁棒聚合的分布式学习方法（CRA-DL），通过对诚实设备发送的梯度进行编码，并在服务器端使用RBA规则聚合信息，以近似恢复全局梯度来更新全局模型。

**结果:** 理论上分析了CRA-DL的收敛性能，并通过数值实验验证了其在拜占庭攻击下的增强学习性能。

**结论:** CRA-DL方法通过使用编码的鲁棒聚合提高了分布式学习在拜占庭攻击下的性能，数值结果验证了该方法相对于现有基线的优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Coded+Robust+Aggregation+for+Distributed+Learning+under+Byzantine+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.01989，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.01989&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we investigate the problem of distributed learning (DL) in the
presence of Byzantine attacks. For this problem, various robust bounded
aggregation (RBA) rules have been proposed at the central server to mitigate
the impact of Byzantine attacks. However, current DL methods apply RBA rules
for the local gradients from the honest devices and the disruptive information
from Byzantine devices, and the learning performance degrades significantly
when the local gradients of different devices vary considerably from each
other. To overcome this limitation, we propose a new DL method to cope with
Byzantine attacks based on coded robust aggregation (CRA-DL). Before training
begins, the training data are allocated to the devices redundantly. During
training, in each iteration, the honest devices transmit coded gradients to the
server computed from the allocated training data, and the server then
aggregates the information received from both honest and Byzantine devices
using RBA rules. In this way, the global gradient can be approximately
recovered at the server to update the global model. Compared with current DL
methods applying RBA rules, the improvement of CRA-DL is attributed to the fact
that the coded gradients sent by the honest devices are closer to each other.
This closeness enhances the robustness of the aggregation against Byzantine
attacks, since Byzantine messages tend to be significantly different from those
of honest devices in this case. We theoretically analyze the convergence
performance of CRA-DL. Finally, we present numerical results to verify the
superiority of the proposed method over existing baselines, showing its
enhanced learning performance under Byzantine attacks.

</details>


### [19] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov*

**主要类别:** cs.LG

**AI概要:** 本论文通过关联记忆系统的视角分析扩散模型，揭示了其在不同数据规模下的记忆与生成行为以及虚假状态的存在。


<details>
  <summary>更多</summary>
  
**动机:** Hopfield网络中出现的虚假状态现象启发我们从关联记忆（AM）的角度研究扩散模型的记忆-泛化现象。

**方法:** 将扩散模型的训练阶段视为记忆编码，生成阶段视为记忆检索，并对两种数据情况下的表现进行理论分析与实证验证。

**结果:** 在小数据情况下，扩散模型创建围绕每个样本的独立吸引子；而在大数据情况下，吸引子对应于生成样本的流形，并在转变边界上出现未见于训练集的新兴虚假状态。

**结论:** 扩散模型在小数据和大数据情况下表现出不同的记忆和生成模式，通过吸引子状态的形成体现了类似Hopfield网络的记忆特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memorization+to+Generalization%3A+Emergence+of+Diffusion+Models+from+Associative+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.21777，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.21777&send_immediately=true&force_search=false)

**原文摘要:** Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [20] [Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids](https://arxiv.org/abs/2506.02050)
*Qingyu Xiao, Yuanlin Chang, Youtian Du*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种结合状态抽象的解耦分层强化学习框架（DcHRL-SA），在复杂离散环境中显著提升了探索效果。


<details>
  <summary>更多</summary>
  
**动机:** 在复杂的离散状态空间环境中，特别是在部分可观测条件下，如何实现有效的智能体探索仍然是强化学习的核心挑战。本文旨在解决这一问题。

**方法:** 该方法采用双层架构，包括一个高层基于强化学习的智能体和一个低层基于规则的策略，并结合了状态抽象方法来降低状态维度。

**结果:** 实验结果表明，与PPO相比，所提出的方法在探索效率、收敛速度、累积奖励和策略稳定性方面均表现出色。

**结论:** 论文提出了一种新的分层强化学习框架DcHRL-SA，通过解耦的层次策略和状态抽象方法，在具有大规模探索空间的离散网格环境中实现了有效的探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decoupled+Hierarchical+Reinforcement+Learning+with+State+Abstraction+for+Discrete+Grids，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02050&send_immediately=true&force_search=false)

**原文摘要:** Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.

</details>


### [21] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/abs/2506.02079)
*Xuefeng Jiang, Tian Wen, Zhiqin Yang, Lvhua Wu, Yufeng Chen, Sheng Sun, Yuwei Wang, Min Liu*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种针对联邦学习中标签噪声问题的两阶段优化框架（MaskedOptim），通过检测噪声客户端并进行标签修正，结合几何中位数聚合方法，显著提升了模型在多种噪声环境下的性能。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的参与者可能无法提供标注良好的数据，不同客户端的标签通常含有复杂且水平不一的噪声，这严重影响模型性能，因此需要开发一种有效的优化策略来减轻这些噪声的影响。

**方法:** 第一阶段用于检测具有高标签噪声率的客户端；第二阶段通过端到端的标签修正机制纠正这些客户端的数据标签，并采用基于几何中位数的模型聚合方法提高训练鲁棒性。

**结果:** 在三个图像数据集和一个文本数据集上的大量实验表明，所提出的框架在不同标签噪声模式下均表现出良好的鲁棒性，同时标签修正机制显著提高了数据质量。

**结论:** 该研究提出了一种两阶段优化框架MaskedOptim，能够有效缓解联邦学习中由标签噪声带来的负面影响，并通过实验验证了其在不同场景下的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Federated+Learning+against+Noisy+Clients+via+Masked+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02079&send_immediately=true&force_search=false)

**原文摘要:** In recent years, federated learning (FL) has made significant advance in
privacy-sensitive applications. However, it can be hard to ensure that FL
participants provide well-annotated data for training. The corresponding
annotations from different clients often contain complex label noise at varying
levels. This label noise issue has a substantial impact on the performance of
the trained models, and clients with greater noise levels can be largely
attributed for this degradation. To this end, it is necessary to develop an
effective optimization strategy to alleviate the adverse effects of these noisy
clients.In this study, we present a two-stage optimization framework,
MaskedOptim, to address this intricate label noise problem. The first stage is
designed to facilitate the detection of noisy clients with higher label noise
rates. The second stage focuses on rectifying the labels of the noisy clients'
data through an end-to-end label correction mechanism, aiming to mitigate the
negative impacts caused by misinformation within datasets. This is achieved by
learning the potential ground-truth labels of the noisy clients' datasets via
backpropagation. To further enhance the training robustness, we apply the
geometric median based model aggregation instead of the commonly-used vanilla
averaged model aggregation. We implement sixteen related methods and conduct
evaluations on three image datasets and one text dataset with diverse label
noise patterns for a comprehensive comparison. Extensive experimental results
indicate that our proposed framework shows its robustness in different
scenarios. Additionally, our label correction framework effectively enhances
the data quality of the detected noisy clients' local datasets. % Our codes
will be open-sourced to facilitate related research communities. Our codes are
available via https://github.com/Sprinter1999/MaskedOptim .

</details>


### [22] [Generalization Performance of Ensemble Clustering: From Theory to Algorithm](https://arxiv.org/abs/2506.02053)
*Xu Zhang, Haoye Qiu, Weixuan Liang, Hui Liu, Junhui Hou, Yuheng Jia*

**主要类别:** cs.LG

**AI概要:** 本研究分析了集成聚类的理论基础，提出了一个新算法，在实验中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 集成聚类在实践中表现出色，但其理论基础仍需探索。论文旨在填补这一空白，提供对集成聚类性能提升的理论支持。

**方法:** 从理论上分析集成聚类的泛化误差、超额风险和一致性，通过理论推导得出优化偏差和多样性的重要性，并设计了一个新的集成聚类算法进行实验验证。

**结果:** 论文推导了泛化误差界和超额风险界的收敛速率，证明了在特定条件下集成聚类是一致的，并通过理论分析表明最小化偏差和最大化多样性能够提高聚类性能。实验结果显示新方法在10个数据集上相对于NMI、ARI和Purity分别平均提升了6.1%、7.3%和6.0%。

**结论:** 论文得出集成聚类的一致性条件，并提出了一种新的集成聚类算法，取得了优于现有方法的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+Performance+of+Ensemble+Clustering%3A+From+Theory+to+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02053&send_immediately=true&force_search=false)

**原文摘要:** Ensemble clustering has demonstrated great success in practice; however, its
theoretical foundations remain underexplored. This paper examines the
generalization performance of ensemble clustering, focusing on generalization
error, excess risk and consistency. We derive a convergence rate of
generalization error bound and excess risk bound both of
$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$, with $n$ and $m$
being the numbers of samples and base clusterings. Based on this, we prove that
when $m$ and $n$ approach infinity and $m$ is significantly larger than log
$n$, i.e., $m,n\to \infty, m\gg \log n$, ensemble clustering is consistent.
Furthermore, recognizing that $n$ and $m$ are finite in practice, the
generalization error cannot be reduced to zero. Thus, by assigning varying
weights to finite clusterings, we minimize the error between the empirical
average clusterings and their expectation. From this, we theoretically
demonstrate that to achieve better clustering performance, we should minimize
the deviation (bias) of base clustering from its expectation and maximize the
differences (diversity) among various base clusterings. Additionally, we derive
that maximizing diversity is nearly equivalent to a robust (min-max)
optimization model. Finally, we instantiate our theory to develop a new
ensemble clustering algorithm. Compared with SOTA methods, our approach
achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.
NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.

</details>


### [23] [Temporal Causal-based Simulation for Realistic Time-series Generation](https://arxiv.org/abs/2506.02084)
*Nikolaos Gkorgkolis, Nikolaos Kougioulis, MingXue Wang, Bora Caglayan, Andrea Tonon, Dario Simionato, Ioannis Tsamardinos*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种名为Temporal Causal-based Simulation (TCS) 的新框架，用于生成更加贴近现实的时间序列因果数据，并展示了其在多种类型数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的因果发现方法通常依赖于合成数据进行评估甚至训练，但这些数据无法准确反映现实世界的情况，尤其是在时间序列数据中。因此，需要一种更有效的方法来生成具有现实性的因果数据。

**方法:** 论文引入了Temporal Causal-based Simulation (TCS)，包含三个阶段：估计数据的真实滞后因果结构、近似变量之间的函数依赖关系以及学习相应因果模型的噪声分布。此外，还详细描述了一个结合AutoML技术的最小-最大优化阶段。

**结果:** 通过广泛的实验验证，论文表明单一的数据检测方法对于生成数据的判别是不够的，并证明了该方法在真实、半合成和纯合成数据集上的有效性。

**结论:** 论文提出了一种灵活且模型无关的生成时间序列因果数据的框架，这种方法能够增强基于因果关系的时间数据生成领域。尽管采样真实因果数据仍是一个复杂任务，但作者的方法在这一领域提供了重要补充。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal+Causal-based+Simulation+for+Realistic+Time-series+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02084&send_immediately=true&force_search=false)

**原文摘要:** Causal Discovery plays a pivotal role in revealing relationships among
observed variables, particularly in the temporal setup. While the majority of
CD methods rely on synthetic data for evaluation, and recently for training,
these fall short in accurately mirroring real-world scenarios; an effect even
more evident in temporal data. Generation techniques depending on simplified
assumptions on causal structure, effects and time, limit the quality and
diversity of the simulated data. In this work, we introduce Temporal
Causal-based Simulation (TCS), a robust framework for generating realistic
time-series data and their associated temporal causal graphs. The approach is
structured in three phases: estimating the true lagged causal structure of the
data, approximating the functional dependencies between variables and learning
the noise distribution of the corresponding causal model, each part of which
can be explicitly tailored based on data assumptions and characteristics.
Through an extensive evaluation process, we highlight that single detection
methods for generated data discrimination prove inadequate, accentuating it as
a multifaceted challenge. For this, we detail a Min-max optimization phase that
draws on AutoML techniques. Our contributions include a flexible,
model-agnostic pipeline for generating realistic temporal causal data, a
thorough evaluation setup which enhances the validity of the generated datasets
and insights into the challenges posed by realistic data generation. Through
experiments involving not only real but also semi-synthetic and purely
synthetic datasets, we demonstrate that while sampling realistic causal data
remains a complex task, our method enriches the domain of generating sensible
causal-based temporal data.

</details>


### [24] [Predicting Blood Type: Assessing Model Performance with ROC Analysis](https://arxiv.org/abs/2506.02062)
*Malik A. Altayar, Muhyeeddin Alqaraleh, Mowafaq Salem Alzboon, Wesam T. Almagharbeh*

**主要类别:** cs.LG

**AI概要:** 该研究通过分析200人的指纹模式和血型数据，发现指纹模式与ABO血型之间无显著相关性，提示这些特征独立存在。


<details>
  <summary>更多</summary>
  
**动机:** 传统的生物识别系统如DNA分析和虹膜扫描虽然准确性高，但耗时且昂贵。因此，本研究旨在探索指纹模式与ABO血型分类之间的关系，以寻找这两种特征之间的潜在相关性。

**方法:** 研究分析了200名个体，将其指纹分为三种类型：环形、螺旋形和拱形，并记录了他们的血型分类。使用统计分析，包括卡方检验和皮尔逊相关性检验，评估指纹模式与血型之间的关联。

**结果:** 研究发现最常见的指纹模式是环形，而参与者的最常见血型是O+型。统计分析显示指纹模式与血型之间没有显著相关性（p > 0.05），表明这些特征是独立的。

**结论:** 虽然研究表明指纹模式与ABO血型之间相关性有限，但它强调了未来研究需要使用更大和更多样化的群体，采用机器学习方法，并整合多种生物特征信号。这项研究强调了个人身份识别中严格协议和全面调查的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Blood+Type%3A+Assessing+Model+Performance+with+ROC+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02062，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02062&send_immediately=true&force_search=false)

**原文摘要:** Introduction: Personal identification is a critical aspect of forensic
sciences, security, and healthcare. While conventional biometrics systems such
as DNA profiling and iris scanning offer high accuracy, they are time-consuming
and costly. Objectives: This study investigates the relationship between
fingerprint patterns and ABO blood group classification to explore potential
correlations between these two traits. Methods: The study analyzed 200
individuals, categorizing their fingerprints into three types: loops, whorls,
and arches. Blood group classification was also recorded. Statistical analysis,
including chi-square and Pearson correlation tests, was used to assess
associations between fingerprint patterns and blood groups. Results: Loops were
the most common fingerprint pattern, while blood group O+ was the most
prevalent among the participants. Statistical analysis revealed no significant
correlation between fingerprint patterns and blood groups (p > 0.05),
suggesting that these traits are independent. Conclusions: Although the study
showed limited correlation between fingerprint patterns and ABO blood groups,
it highlights the importance of future research using larger and more diverse
populations, incorporating machine learning approaches, and integrating
multiple biometric signals. This study contributes to forensic science by
emphasizing the need for rigorous protocols and comprehensive investigations in
personal identification.

</details>


### [25] [Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models](https://arxiv.org/abs/2506.02092)
*Francesco De Santis, Philippe Bich, Gabriele Ciravegna, Pietro Barbiero, Danilo Giordano, Tania Cerquitelli*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的无监督概念模型 LCBM，用于图像分类，解决了传统方法对人工监督的需求和扩展性差的问题，同时提升了模型的可解释性和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高深度神经网络的可信度，需要更好地理解它们如何做出决策。传统方法要么需要大量人工监督，要么扩展性有限，因此提出了 LCBM。

**方法:** 提出了一种名为 Learnable Concept-Based Model (LCBM) 的新型无监督概念模型，将概念建模为伯努利潜在空间中的随机变量。

**结果:** LCBM 在通用性方面优于现有的无监督概念模型，并且性能接近黑盒模型；用户研究表明发现的概念更容易被人类解释。

**结论:** LCBM 在保持模型可解释性的同时，提高了图像分类的泛化能力，并且概念表示更符合人类理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Better+Generalization+and+Interpretability+in+Unsupervised+Concept-Based+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02092&send_immediately=true&force_search=false)

**原文摘要:** To increase the trustworthiness of deep neural networks, it is critical to
improve the understanding of how they make decisions. This paper introduces a
novel unsupervised concept-based model for image classification, named
Learnable Concept-Based Model (LCBM) which models concepts as random variables
within a Bernoulli latent space. Unlike traditional methods that either require
extensive human supervision or suffer from limited scalability, our approach
employs a reduced number of concepts without sacrificing performance. We
demonstrate that LCBM surpasses existing unsupervised concept-based models in
generalization capability and nearly matches the performance of black-box
models. The proposed concept representation enhances information retention and
aligns more closely with human understanding. A user study demonstrates the
discovered concepts are also more intuitive for humans to interpret. Finally,
despite the use of concept embeddings, we maintain model interpretability by
means of a local linear combination of concepts.

</details>


### [26] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/abs/2506.02065)
*Shriraj P. Sawant, Krishna P. Miyapuram*

**主要类别:** cs.LG

**AI概要:** 本论文提出了弹性权重生成网络（EWGN）来解决持续学习中的灾难性遗忘问题，并通过实验分析验证其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 任务可变性和上下文切换对神经网络学习具有挑战性，而人类智能能够学习和保持各种任务的能力启发了人工智能研究。

**方法:** 引入了弹性权重生成网络（EWGN）架构，并使用标准计算机视觉数据集MNIST和fashion-MNIST进行实验分析。

**结果:** 通过防止网络中不同任务权重之间的干扰，上下文切换可以作为缓解灾难性遗忘的一种有用方法。

**结论:** 动态权重生成和上下文切换能力对于实现持续学习和提升性能是有益的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EWGN%3A+Elastic+Weight+Generation+and+Context+Switching+in+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02065，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02065&send_immediately=true&force_search=false)

**原文摘要:** The ability to learn and retain a wide variety of tasks is a hallmark of
human intelligence that has inspired research in artificial general
intelligence. Continual learning approaches provide a significant step towards
achieving this goal. It has been known that task variability and context
switching are challenging for learning in neural networks. Catastrophic
forgetting refers to the poor performance on retention of a previously learned
task when a new task is being learned. Switching between different task
contexts can be a useful approach to mitigate the same by preventing the
interference between the varying task weights of the network. This paper
introduces Elastic Weight Generative Networks (EWGN) as an idea for context
switching between two different tasks. The proposed EWGN architecture uses an
additional network that generates the weights of the primary network
dynamically while consolidating the weights learned. The weight generation is
input-dependent and thus enables context switching. Using standard computer
vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of
previously learned task representations in Fully Connected Networks,
Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient
Descent and Elastic Weight Consolidation learning algorithms. Understanding
dynamic weight generation and context-switching ability can be useful in
enabling continual learning for improved performance.

</details>


### [27] [Constrained Sliced Wasserstein Embedding](https://arxiv.org/abs/2506.02203)
*Navid NaderiAlizadeh, Darian Salehi, Xinran Liu, Soheil Kolouri*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种受限学习方法来优化Sliced Wasserstein距离的切片方向，以提高比较高维概率测度的效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Sliced Wasserstein距离在寻找信息量大的切片方向上存在困难，需要大量切片才能达到理想效果，从而增加了计算复杂度。

**方法:** 通过将1D传输计划约束为近似原始空间中的最优计划，并利用这些传输计划的连续松弛，采用基于梯度的原始-对偶方法训练切片参数和其他模型参数。

**结果:** 实验结果表明，所提出的受限学习方法在图像、点云和蛋白质序列的基础模型中能够学习到更具信息量的切片方向。

**结论:** 这种方法有效解决了现有SW距离在寻找高效且有意义的切片方向上的不足，提高了高维数据处理的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constrained+Sliced+Wasserstein+Embedding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02203，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02203&send_immediately=true&force_search=false)

**原文摘要:** Sliced Wasserstein (SW) distances offer an efficient method for comparing
high-dimensional probability measures by projecting them onto multiple
1-dimensional probability distributions. However, identifying informative
slicing directions has proven challenging, often necessitating a large number
of slices to achieve desirable performance and thereby increasing computational
complexity. We introduce a constrained learning approach to optimize the
slicing directions for SW distances. Specifically, we constrain the 1D
transport plans to approximate the optimal plan in the original space, ensuring
meaningful slicing directions. By leveraging continuous relaxations of these
transport plans, we enable a gradient-based primal-dual approach to train the
slicer parameters, alongside the remaining model parameters. We demonstrate how
this constrained slicing approach can be applied to pool high-dimensional
embeddings into fixed-length permutation-invariant representations. Numerical
results on foundation models trained on images, point clouds, and protein
sequences showcase the efficacy of the proposed constrained learning approach
in learning more informative slicing directions. Our implementation code can be
found at https://github.com/Stranja572/constrainedswe.

</details>


### [28] [An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/abs/2506.02070)
*Peter Holderrieth, Ezra Erives*

**主要类别:** cs.LG

**AI概要:** 该论文总结了Diffusion和flow-based模型在生成AI领域的广泛应用，并提供了从基础理论到前沿技术的学习资源。


<details>
  <summary>更多</summary>
  
**动机:** 为学生和从业者提供对生成AI理论与实践的原则性理解，满足其在研究或应用中的需求。

**方法:** 通过课程笔记和配套内容，从常微分方程和随机微分方程开始，逐步引入flow matching、score matching等技术，深入解析现代图像和视频生成模型的核心机制。

**结果:** 提供了一个自成一体的介绍框架，涵盖了生成AI领域的主要技术和最新进展，适用于教育和自学场景。

**结论:** 论文得出Diffusion和flow-based模型已成为跨多种数据模态的生成AI的最先进方法，并提供了一种从基础到高级概念的系统学习路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Introduction+to+Flow+Matching+and+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02070&send_immediately=true&force_search=false)

**原文摘要:** Diffusion and flow-based models have become the state of the art for
generative AI across a wide range of data modalities, including images, videos,
shapes, molecules, music, and more! These notes are originally from
https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)
term, and are intended to accompany other course content, including lectures
and labs. Overall, they function as a self-contained introduction to both flow
matching and diffusion models, starting with ordinary and stochastic
differential equations, and culminating in flow matching, score matching,
classifier-free guidance, and the inner workings of modern, state-of-the-art
models for image and video. These notes, and the accompanying course, are ideal
for students and practitioners alike who want to develop a principled
understanding of the theory and practice of generative AI.

</details>


### [29] [Latent Stochastic Interpolants](https://arxiv.org/abs/2506.02276)
*Saurabh Singh, Dmitry Lagun*

**主要类别:** cs.LG

**AI概要:** This paper introduces Latent Stochastic Interpolants (LSI), enabling joint learning of encoder, decoder, and SI models in latent space, avoiding limitations of traditional diffusion models and high-dimensional computation while maintaining SI's flexibility.


<details>
  <summary>更多</summary>
  
**动机:** Stochastic Interpolants (SI) are powerful for generative modeling but have not been used in jointly optimized latent variable models due to their requirement for direct access to samples from two distributions.

**方法:** The authors developed a principled Evidence Lower Bound (ELBO) objective in continuous time to allow joint optimization of encoder, decoder, and latent SI models.

**结果:** LSI successfully learns latent representations and performs a generative process transforming an arbitrary prior into the encoder-defined aggregated posterior, demonstrated effectively on the ImageNet generation benchmark.

**结论:** Latent Stochastic Interpolants (LSI) enable joint learning in a latent space with end-to-end optimized encoder, decoder, and latent SI models, providing effective latent representations and preserving the generative flexibility of the SI framework.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Stochastic+Interpolants，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02276&send_immediately=true&force_search=false)

**原文摘要:** Stochastic Interpolants (SI) are a powerful framework for generative
modeling, capable of flexibly transforming between two probability
distributions. However, their use in jointly optimized latent variable models
remains unexplored as they require direct access to the samples from the two
distributions. This work presents Latent Stochastic Interpolants (LSI) enabling
joint learning in a latent space with end-to-end optimized encoder, decoder and
latent SI models. We achieve this by developing a principled Evidence Lower
Bound (ELBO) objective derived directly in continuous time. The joint
optimization allows LSI to learn effective latent representations along with a
generative process that transforms an arbitrary prior distribution into the
encoder-defined aggregated posterior. LSI sidesteps the simple priors of the
normal diffusion models and mitigates the computational demands of applying SI
directly in high-dimensional observation spaces, while preserving the
generative flexibility of the SI framework. We demonstrate the efficacy of LSI
through comprehensive experiments on the standard large scale ImageNet
generation benchmark.

</details>


### [30] [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
*Yoonjun Cho, Soeun Kim, Dongjae Jeon, Kyelim Lee, Beomsoo Lee, Albert No*

**主要类别:** cs.LG

**AI概要:** 本文提出ODLRI方法，通过结构化分解解决大语言模型压缩中量化与低秩近似的平衡问题，在多个模型上取得了更好的压缩效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有联合优化方法在交替迭代过程中往往优先考虑某一部分而牺牲另一部分，导致未能充分利用量化和低秩分解各自的优势。

**方法:** 提出Outlier-Driven Low-Rank Initialization (ODLRI) 方法，将低秩部分专门用于捕捉激活敏感的权重，从而减少异常值对量化的影响，并结合联合优化框架进行实验验证。

**结果:** 在Llama2（7B、13B、70B）、Llama3-8B和Mistral-7B模型上实验表明，ODLRI能持续降低激活感知误差、最小化量化尺度，并改善困惑度和零样本准确率。

**结论:** ODLRI方法能够有效平衡量化与低秩近似，显著提升大语言模型在低比特设置下的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assigning+Distinct+Roles+to+Quantized+and+Low-Rank+Matrices+Toward+Optimal+Weight+Decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02077&send_immediately=true&force_search=false)

**原文摘要:** Decomposing weight matrices into quantization and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between quantization and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on quantization,
enabling more effective balance between quantization and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes quantization scale, and
improves perplexity and zero-shot accuracy in low-bit settings.

</details>


### [31] [CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation](https://arxiv.org/abs/2506.02306)
*Aditya Gorla, Ryan Wang, Zhengtong Liu, Ulzee An, Sriram Sankararaman*

**主要类别:** cs.LG

**AI概要:** CACTI是一种新的表格数据填补方法，利用缺失模式和特征间的关系显著提升了填补效果。


<details>
  <summary>更多</summary>
  
**动机:** 为了提升表格数据填补性能，利用缺失数据的结构和上下文信息。

**方法:** 采用中位截断复制掩码训练策略的CACTI，结合了特征间的语义关系。

**结果:** 平均R²增益比最佳现有方法高出7.8%，在随机缺失、非随机缺失和完全随机缺失条件下的增益分别为13.4%、6.1%和5.3%。

**结论:** CACTI方法通过利用缺失数据模式和上下文信息，在多种数据集和缺失条件下优于最先进的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CACTI%3A+Leveraging+Copy+Masking+and+Contextual+Information+to+Improve+Tabular+Data+Imputation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02306&send_immediately=true&force_search=false)

**原文摘要:** We present CACTI, a masked autoencoding approach for imputing tabular data
that leverages the structure in missingness patterns and contextual
information. Our approach employs a novel median truncated copy masking
training strategy that encourages the model to learn from empirical patterns of
missingness while incorporating semantic relationships between features -
captured by column names and text descriptions - to better represent feature
dependence. These dual sources of inductive bias enable CACTI to outperform
state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best
method (13.4%, 6.1%, and 5.3% under missing not at random, at random and
completely at random, respectively) - across a diverse range of datasets and
missingness conditions. Our results highlight the value of leveraging
dataset-specific contextual information and missingness patterns to enhance
imputation performance.

</details>


### [32] [Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs](https://arxiv.org/abs/2506.02337)
*Adrienne M. Propp, Jonas A. Actor, Elise Walker, Houman Owhadi, Nathaniel Trask, Daniel M. Tartakovsky*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于高斯过程的新方法来学习图上的Dirichlet-to-Neumann映射，并成功应用于多物理场模拟。


<details>
  <summary>更多</summary>
  
**动机:** 通过确保状态变量和通量在人工界面处的连续性，实现多物理场模拟的耦合。

**方法:** 结合离散外微积分和非线性最优恢复，在再生核希尔伯特空间范数上优化并应用最大似然估计惩罚来学习图上的Dirichlet-to-Neumann映射。

**结果:** 展示了该方法在地下裂缝网络和动脉血流两个典型应用中均能严格遵守守恒定律且不过拟合。

**结论:** 该方法在数据稀缺的情况下仍能保持高精度和良好的不确定性估计，突出了其在科学应用中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discovery+of+Probabilistic+Dirichlet-to-Neumann+Maps+on+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02337&send_immediately=true&force_search=false)

**原文摘要:** Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations
across computational subdomains by ensuring continuity of state variables and
fluxes at artificial interfaces. We present a novel method for learning
Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for
problems where the data obey a conservation constraint from an underlying
partial differential equation. Our approach combines discrete exterior calculus
and nonlinear optimal recovery to infer relationships between vertex and edge
values. This framework yields data-driven predictions with uncertainty
quantification across the entire graph, even when observations are limited to a
subset of vertices and edges. By optimizing over the reproducing kernel Hilbert
space norm while applying a maximum likelihood estimation penalty on kernel
complexity, our method ensures that the resulting surrogate strictly enforces
conservation laws without overfitting. We demonstrate our method on two
representative applications: subsurface fracture networks and arterial blood
flow. Our results show that the method maintains high accuracy and
well-calibrated uncertainty estimates even under severe data scarcity,
highlighting its potential for scientific applications where limited data and
reliable uncertainty quantification are critical.

</details>


### [33] [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/abs/2506.02081)
*Chihiro Maru, Shoetsu Sato*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的时间序列基础模型RATFM，通过引入测试时适应的示例，在不依赖领域微调的情况下达到了与领域内微调相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列基础模型在不同领域和任务中的表现存在差异，且不具备解释或利用示例或指令的能力。

**方法:** 提出了一种检索增强的时间序列基础模型（RATFM），使预训练的时间序列基础模型能够整合测试时适应的示例。

**结果:** 在UCR异常存档上的实验表明，RATFM的方法是有效的。

**结论:** RATFM在不需要领域相关微调的情况下，实现了与领域内微调相当的性能，并证明了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RATFM%3A+Retrieval-augmented+Time+Series+Foundation+Model+for+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02081&send_immediately=true&force_search=false)

**原文摘要:** Inspired by the success of large language models (LLMs) in natural language
processing, recent research has explored the building of time series foundation
models and applied them to tasks such as forecasting, classification, and
anomaly detection. However, their performances vary between different domains
and tasks. In LLM-based approaches, test-time adaptation using example-based
prompting has become common, owing to the high cost of retraining. In the
context of anomaly detection, which is the focus of this study, providing
normal examples from the target domain can also be effective. However, time
series foundation models do not naturally acquire the ability to interpret or
utilize examples or instructions, because the nature of time series data used
during training does not encourage such capabilities. To address this
limitation, we propose a retrieval augmented time series foundation model
(RATFM), which enables pretrained time series foundation models to incorporate
examples of test-time adaptation. We show that RATFM achieves a performance
comparable to that of in-domain fine-tuning while avoiding domain-dependent
fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset
including nine domains, confirms the effectiveness of the proposed approach.

</details>


### [34] [Multi-agent Markov Entanglement](https://arxiv.org/abs/2506.02385)
*Shuze Chen, Tianyi Peng*

**主要类别:** cs.LG

**AI概要:** 该论文研究了多智能体强化学习中的值分解技术，提出“马尔可夫纠缠”度量以评估分解效果，并证明某些索引策略具有较低的分解误差。


<details>
  <summary>更多</summary>
  
**动机:** 值分解在多智能体强化学习中广泛应用，但其有效性缺乏理论支持，因此需要探索其数学结构和理论依据。

**方法:** 通过类比量子纠缠的概念引入“马尔可夫纠缠”，并利用其分析多智能体系统的值分解可行性及误差界。

**结果:** 论文提出了“马尔可夫纠缠”的概念，能够有效衡量多智能体系统中的值分解质量，并提供实际估计方法。

**结论:** 论文得出多智能体马尔可夫决策过程的“马尔可夫纠缠”度量可以用来界定值分解误差，并证明了一类广泛使用的索引策略是弱纠缠的，具有次线性误差规模。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-agent+Markov+Entanglement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02385，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02385&send_immediately=true&force_search=false)

**原文摘要:** Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [35] [Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL](https://arxiv.org/abs/2506.02406)
*Renat Sergazinov, Jing Wu, Shao-An Yin*

**主要类别:** cs.LG

**AI概要:** 论文提出一种基于随机傅里叶特征的简单高效预处理方法，用于表格数据的深度学习，加速训练过程并提高模型表现。


<details>
  <summary>更多</summary>
  
**动机:** 由于表格数据上的深度学习流程存在不足，并且随机傅里叶特征在深度学习中的应用被忽视，因此作者重新利用随机傅里叶映射作为参数无关、架构无关的变换。

**方法:** 通过将输入数据映射到由正弦和余弦函数构成的固定特征空间，使用在初始化时一次性抽取的频率进行投影，从而绕过对额外归一化或可学习嵌入的需求。同时基于神经切线核（NTK）框架分析该方法的效果。

**结果:** 实验表明，经过傅里叶变换的输入数据能让深度网络更快收敛并实现优异的最终性能，通常需要更少的训练轮次和超参数调优。

**结论:** 论文得出结论，随机傅里叶预处理可以作为表格深度学习的一种理论驱动的、即插即用的增强方法，能够提升训练速度和最终性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Random+at+First%2C+Fast+at+Last%3A+NTK-Guided+Fourier+Pre-Processing+for+Tabular+DL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02406&send_immediately=true&force_search=false)

**原文摘要:** While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.

</details>


### [36] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/abs/2506.02089)
*Zeng Wang, Minghao Shao, Rupesh Karn, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为SALAD的新方法，它使用机器遗忘技术来降低大型语言模型在硬件设计自动化中的数据安全风险。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在Verilog代码生成方面提供了变革性的能力，但也带来了Verilog评估数据污染、知识产权设计泄露和恶意Verilog生成等重大数据安全挑战。

**方法:** 引入SALAD方法，利用机器遗忘技术选择性地从预训练的大型语言模型中移除受污染的基准、敏感IP和设计工件或恶意代码模式。

**结果:** 详细案例研究表明，机器遗忘技术能够有效降低LLM辅助硬件设计中的数据安全风险。

**结论:** SALAD通过机器遗忘技术减少了大型语言模型在硬件设计自动化中的数据安全风险，而无需完全重新训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SALAD%3A+Systematic+Assessment+of+Machine+Unlearing+on+LLM-Aided+Hardware+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02089&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [37] [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://arxiv.org/abs/2506.02612)
*Jan Robine, Marc Höftmann, Stefan Harmeling*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 SGF 的新型世界模型，该模型结构简单、运行速度快且性能良好，在不依赖传统复杂结构的前提下取得了优秀的实验结果。


<details>
  <summary>更多</summary>
  
**动机:** 探索世界模型的基本组成部分，并研究在不使用传统复杂结构（如 RNN 和 Transformer）的情况下，如何构建一个简单但有效的世界模型。

**方法:** SGF 使用自监督表示学习，通过帧和动作堆叠捕捉短期依赖关系，并利用数据增强提高对模型误差的鲁棒性。

**结果:** SGF 在 Atari 100k 基准测试中表现出色，经过消融实验验证了其各个组件的有效性，并展示了其与现有世界模型的联系。

**结论:** SGF 是一种简单、高效的世界模型，能够在不使用 RNN、Transformer、离散表示和图像重建的情况下实现良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Simple%2C+Good%2C+Fast%3A+Self-Supervised+World+Models+Free+of+Baggage，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02612，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02612&send_immediately=true&force_search=false)

**原文摘要:** What are the essential components of world models? How far do we get with
world models that are not employing RNNs, transformers, discrete
representations, and image reconstructions? This paper introduces SGF, a
Simple, Good, and Fast world model that uses self-supervised representation
learning, captures short-time dependencies through frame and action stacking,
and enhances robustness against model errors through data augmentation. We
extensively discuss SGF's connections to established world models, evaluate the
building blocks in ablation studies, and demonstrate good performance through
quantitative comparisons on the Atari 100k benchmark.

</details>


### [38] [Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport](https://arxiv.org/abs/2506.02712)
*Jayadev Naram, Fredrik Hellström, Ziming Wang, Rebecka Jörnsten, Giuseppe Durisi*

**主要类别:** cs.LG

**AI概要:** 本文提出了WARMPOT算法，用于解决部分域适应问题，通过广泛的实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 在实际场景中，目标分布的标记数据稀缺，而源分布的标记数据丰富，且目标标签空间是源标签空间的一个子集。

**方法:** 基于部分最优传输理论推导出PDA问题的泛化界，并据此设计了WARMPOT算法。

**结果:** 实验表明，WARMPOT与现有方法具有竞争力，且提出的权重方案优于现有方法。

**结论:** 本文为PDA提供了一个有理论支持的方法，并展示了其实验优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Theoretical+Performance+Guarantees+for+Partial+Domain+Adaptation+via+Partial+Optimal+Transport，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02712，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02712&send_immediately=true&force_search=false)

**原文摘要:** In many scenarios of practical interest, labeled data from a target
distribution are scarce while labeled data from a related source distribution
are abundant. One particular setting of interest arises when the target label
space is a subset of the source label space, leading to the framework of
partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a
domain alignment term and a weighted empirical loss on the source data, with
the aim of transferring knowledge between domains. However, a theoretical basis
for this procedure is lacking, and in particular, most existing weighting
schemes are heuristic. In this work, we derive generalization bounds for the
PDA problem based on partial optimal transport. These bounds corroborate the
use of the partial Wasserstein distance as a domain alignment term, and lead to
theoretically motivated explicit expressions for the empirical source loss
weights. Inspired by these bounds, we devise a practical algorithm for PDA,
termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT
is competitive with recent approaches, and that our proposed weights improve on
existing schemes.

</details>


### [39] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
*Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, Michael Qizhe Shieh*

**主要类别:** cs.LG

**AI概要:** SynthRL通过自动生成可验证且具有挑战性的问题，提升了视觉-语言模型在复杂推理任务上的性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了进一步改进基于强化学习的视觉-语言模型的数据训练效率和效果，本文研究了合成RL数据的应用。

**方法:** 提出SynthRL方法，包括三个阶段：选择合适的种子问题、生成更具挑战性的变体、确保正确性和难度增强。

**结果:** 实验表明，SynthRL可扩展性强，在MMK12数据集上合成了超过3.3K个额外的问题，并在五个跨领域基准测试中显著优于基线模型。

**结论:** SynthRL能够有效提升视觉-语言模型在推理任务中的表现，尤其是在复杂样本上的效果显著。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SynthRL%3A+Scaling+Visual+Reasoning+with+Verifiable+Data+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02096，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02096&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.

</details>


### [40] [LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale](https://arxiv.org/abs/2506.02098)
*Miran Özdogan, Gilad Landau, Gereon Elvers, Dulhan Jayalath, Pratik Somaiya, Francesco Mantegna, Mark Woolrich, Oiwi Parker Jones*

**主要类别:** cs.LG

**AI概要:** LibriBrain是一个前所未有的大规模单受试者MEG数据集，用于语音解码研究，其配套工具和实验结果展示了扩大数据规模对提升解码性能的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的非侵入性方法无法提供足够规模的单受试者数据，因此需要一个更大规模的数据集来推动语音解码领域的发展。

**方法:** 该论文介绍了LibriBrain，这是一个包含超过50小时高质量MEG记录的大型数据集，与现有数据集相比具有显著的规模优势，并提供了详细的注释和Python库以支持深度学习框架的集成。

**结果:** 实验表明，增加训练数据可以显著提高解码性能，证明了大规模单受试者数据集的价值。

**结论:** LibriBrain通过提供一个深度的、单受试者的MEG数据集，支持神经解码技术的进步，并旨在促进安全有效的临床脑机接口的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LibriBrain%3A+Over+50+Hours+of+Within-Subject+MEG+to+Improve+Speech+Decoding+Methods+at+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02098，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02098&send_immediately=true&force_search=false)

**原文摘要:** LibriBrain represents the largest single-subject MEG dataset to date for
speech decoding, with over 50 hours of recordings -- 5$\times$ larger than the
next comparable dataset and 50$\times$ larger than most. This unprecedented
`depth' of within-subject data enables exploration of neural representations at
a scale previously unavailable with non-invasive methods. LibriBrain comprises
high-quality MEG recordings together with detailed annotations from a single
participant listening to naturalistic spoken English, covering nearly the full
Sherlock Holmes canon. Designed to support advances in neural decoding,
LibriBrain comes with a Python library for streamlined integration with deep
learning frameworks, standard data splits for reproducibility, and baseline
results for three foundational decoding tasks: speech detection, phoneme
classification, and word classification. Baseline experiments demonstrate that
increasing training data yields substantial improvements in decoding
performance, highlighting the value of scaling up deep, within-subject
datasets. By releasing this dataset, we aim to empower the research community
to advance speech decoding methodologies and accelerate the development of
safe, effective clinical brain-computer interfaces.

</details>


### [41] [From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation](https://arxiv.org/abs/2506.02933)
*Junyi Fang, Yuxun Chen, Yuxin Chen, Chen Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了RAVEN-UCB算法，用于解决非平稳环境下多臂老虎机问题，通过理论分析和实验验证证明了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的UCB1和UCB-V算法在非平稳环境中表现不佳，因此需要一种新的方法来解决动态奖励分布带来的挑战。

**方法:** RAVEN-UCB结合了方差感知适应，利用置信区间中的方差驱动探索、自适应控制以及常数时间递归更新等三项创新技术。

**结果:** RAVEN-UCB实现了比现有方法更严格的遗憾界，并在合成和物流场景中对非平稳模式（如分布变化、周期性变化和临时波动）表现出优于基线模型的性能。

**结论:** RAVEN-UCB在非平稳环境中的多臂老虎机问题上表现出优越的性能，通过理论分析和实验验证展示了其在实际应用中的鲁棒性和有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Theory+to+Practice+with+RAVEN-UCB%3A+Addressing+Non-Stationarity+in+Multi-Armed+Bandits+through+Variance+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02933，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02933&send_immediately=true&force_search=false)

**原文摘要:** The Multi-Armed Bandit (MAB) problem is challenging in non-stationary
environments where reward distributions evolve dynamically. We introduce
RAVEN-UCB, a novel algorithm that combines theoretical rigor with practical
efficiency via variance-aware adaptation. It achieves tighter regret bounds
than UCB1 and UCB-V, with gap-dependent regret of order $K \sigma_{\max}^2 \log
T / \Delta$ and gap-independent regret of order $\sqrt{K T \log T}$. RAVEN-UCB
incorporates three innovations: (1) variance-driven exploration using
$\sqrt{\hat{\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive
control via $\alpha_t = \alpha_0 / \log(t + \epsilon)$, and (3) constant-time
recursive updates for efficiency. Experiments across non-stationary patterns -
distributional changes, periodic shifts, and temporary fluctuations - in
synthetic and logistics scenarios demonstrate its superiority over
state-of-the-art baselines, confirming theoretical and practical robustness.

</details>


### [42] [ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels](https://arxiv.org/abs/2506.02134)
*Rishi Raj Sahoo, Rucha Bhalchandra Joshi, Subhankar Mishra*

**主要类别:** cs.LG

**AI概要:** 论文讨论了图神经网络解释所带来的隐私风险，并提出了一种新的图重构攻击方法ReconXF，即使在差分隐私保护下也能有效恢复图结构。


<details>
  <summary>更多</summary>
  
**动机:** 图神经网络（GNN）广泛应用于各个领域，但其黑盒特性限制了其在医疗和司法等敏感领域的使用。为了提升透明度，解释性方法被引入，但它们可能带来隐私泄露的风险。现有攻击通常假设可以获取原始辅助数据，而在实际系统中，特征和标签会受到差分隐私保护。因此，本文研究了一个更贴近现实的威胁模型：攻击者通过公共特征解释和经过差分隐私处理的数据进行图重构攻击。

**方法:** 作者提出了ReconXF方法，该方法结合了解释性框架和去噪机制，能够处理差分隐私带来的噪声，并利用解释中的结构信号进行图重构。

**结果:** 实验表明，ReconXF在多个数据集上优于现有最先进的攻击方法，尤其在AUC和平均精度上有明显提升，说明即使在差分隐私保护下，公共解释仍可能导致图结构信息泄露。

**结论:** 尽管差分隐私用于保护节点特征和标签，但结合公开的解释信息，攻击者仍有可能重建图结构，这对未来的隐私保护机制设计提出了挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReconXF%3A+Graph+Reconstruction+Attack+via+Public+Feature+Explanations+on+Privatized+Node+Features+and+Labels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02134&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).

</details>


### [43] [On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning](https://arxiv.org/abs/2506.03037)
*Shubhendu Trivedi, Brian D. Nord*

**主要类别:** cs.LG

**AI概要:** 本文探讨了机器学习中不确定性量化的主要挑战，提出标准以弥合理论与实践差距，并为科学机器学习中的仿真推理提供实例解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 由于不确定性术语的不一致和不同领域对可信不确定性建立的技术要求各异，量化机器学习模型的不确定性成为一个基础性挑战。

**方法:** 通过分析当前的估计目标、不确定性构造以及它们之间的映射方法，结合文献中的问题示例进行论证。

**结果:** 明确了不确定性量化中存在的主要问题，讨论了可信赖不确定性量化的多个信任维度，并为设计和评估不确定性感知的机器学习系统提供了实践建议。

**结论:** 论文总结了在机器学习模型中量化不确定性的挑战，并提出了促进意图与实现对齐的标准，以提高不确定性量化的可靠性和可信赖性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Need+to+Align+Intent+and+Implementation+in+Uncertainty+Quantification+for+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03037&send_immediately=true&force_search=false)

**原文摘要:** Quantifying uncertainties for machine learning (ML) models is a foundational
challenge in modern data analysis. This challenge is compounded by at least two
key aspects of the field: (a) inconsistent terminology surrounding uncertainty
and estimation across disciplines, and (b) the varying technical requirements
for establishing trustworthy uncertainties in diverse problem contexts. In this
position paper, we aim to clarify the depth of these challenges by identifying
these inconsistencies and articulating how different contexts impose distinct
epistemic demands. We examine the current landscape of estimation targets
(e.g., prediction, inference, simulation-based inference), uncertainty
constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to
map between them. Drawing on the literature, we highlight and explain examples
of problematic mappings. To help address these issues, we advocate for
standards that promote alignment between the \textit{intent} and
\textit{implementation} of uncertainty quantification (UQ) approaches. We
discuss several axes of trustworthiness that are necessary (if not sufficient)
for reliable UQ in ML models, and show how these axes can inform the design and
evaluation of uncertainty-aware ML systems. Our practical recommendations focus
on scientific ML, offering illustrative cases and use scenarios, particularly
in the context of simulation-based inference (SBI).

</details>


### [44] [Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability](https://arxiv.org/abs/2506.02138)
*Yarden Bakish, Itamar Zimerman, Hila Chefer, Lior Wolf*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种考虑位置编码的新方法，用于改进Transformer模型的可解释性，并取得了显著效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有LRP方法忽视了Transformer中的位置编码，导致违反守恒性质和丢失重要结构信息。

**方法:** 将输入空间重新表述为位置-标记对，并提出新的LRP规则以处理位置编码问题。

**结果:** 实验表明，该方法在视觉和NLP解释任务中均显著优于当前最先进的模型。

**结论:** 作者通过重新设计输入空间，提出了专门的基于位置标记对的LRP规则，并成功应用于Transformer解释性分析中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+LRP%3A+Positional+Attribution+as+the+Missing+Ingredient+for+Transformer+Explainability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02138，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02138&send_immediately=true&force_search=false)

**原文摘要:** The development of effective explainability tools for Transformers is a
crucial pursuit in deep learning research. One of the most promising approaches
in this domain is Layer-wise Relevance Propagation (LRP), which propagates
relevance scores backward through the network to the input space by
redistributing activation values based on predefined rules. However, existing
LRP-based methods for Transformer explainability entirely overlook a critical
component of the Transformer architecture: its positional encoding (PE),
resulting in violation of the conservation property, and the loss of an
important and unique type of relevance, which is also associated with
structural and positional features. To address this limitation, we reformulate
the input space for Transformer explainability as a set of position-token
pairs. This allows us to propose specialized theoretically-grounded LRP rules
designed to propagate attributions across various positional encoding methods,
including Rotary, Learnable, and Absolute PE. Extensive experiments with both
fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,
demonstrate that our method significantly outperforms the state-of-the-art in
both vision and NLP explainability tasks. Our code is publicly available.

</details>


### [45] [Sample complexity of Schrödinger potential estimation](https://arxiv.org/abs/2506.03043)
*Nikita Puchkin, Iurii Pustovalov, Yuri Sapronov, Denis Suchkov, Alexey Naumov, Denis Belomestny*

**主要类别:** cs.LG

**AI概要:** 本文分析了Schrödinger势估计中经验KL风险最小化的推广能力，证明了其在KL散度上的非渐近高概率上界，并显示了其快速收敛速度。


<details>
  <summary>更多</summary>
  
**动机:** Schrödinger势估计在基于Schrödinger桥梁和随机最优控制的现代生成建模方法中至关重要，但如何评估其推广能力是一个重要问题。

**方法:** 通过分析经验Kullback-Leibler（KL）风险最小化方法，在合理的假设下推导出KL散度的非渐近高概率上界，并研究其在Schrödinger势估计中的表现。

**结果:** 研究表明，当样本容量n趋于无穷大时，过剩KL风险可以降低到O(log²n / n)的速度，这表明该方法具有良好的推广性能。

**结论:** 本文研究了基于经验Kullback-Leibler（KL）风险最小化在Schrödinger势估计中的推广能力，并给出了KL散度的非渐近高概率上界，证明了即使初始分布和目标分布具有无界支撑，过剩KL风险也可以随着样本容量增大而快速减小。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+complexity+of+Schr%C3%B6dinger+potential+estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03043&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of Schr\"odinger potential estimation, which plays a
crucial role in modern generative modelling approaches based on Schr\"odinger
bridges and stochastic optimal control for SDEs. Given a simple prior diffusion
process, these methods search for a path between two given distributions
$\rho_0$ and $\rho_T^*$ requiring minimal efforts. The optimal drift in this
case can be expressed through a Schr\"odinger potential. In the present paper,
we study generalization ability of an empirical Kullback-Leibler (KL) risk
minimizer over a class of admissible log-potentials aimed at fitting the
marginal distribution at time $T$. Under reasonable assumptions on the target
distribution $\rho_T^*$ and the prior process, we derive a non-asymptotic
high-probability upper bound on the KL-divergence between $\rho_T^*$ and the
terminal density corresponding to the estimated log-potential. In particular,
we show that the excess KL-risk may decrease as fast as $O(\log^2 n / n)$ when
the sample size $n$ tends to infinity even if both $\rho_0$ and $\rho_T^*$ have
unbounded supports.

</details>


### [46] [Z-Error Loss for Training Neural Networks](https://arxiv.org/abs/2506.02154)
*Guillaume Godin*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Z-Error Loss的新方法，可以有效减轻训练神经网络时异常值带来的问题。


<details>
  <summary>更多</summary>
  
**动机:** 异常值在神经网络训练中会传播错误梯度，从而降低模型性能和泛化能力，因此需要一种新的损失函数来解决这个问题。

**方法:** 利用批次级别的统计信息来自动检测并排除异常样本，通过屏蔽被识别为分布外的数据点的贡献来最小化异常值的影响。

**结果:** Z-Error Loss方法成功减少了异常值对训练过程的影响，同时允许模型专注于学习真实的底层数据结构。

**结论:** Z-Error Loss方法能够有效地减少训练数据中异常值对模型的影响，并且具有良好的鲁棒性和适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Z-Error+Loss+for+Training+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02154&send_immediately=true&force_search=false)

**原文摘要:** Outliers introduce significant training challenges in neural networks by
propagating erroneous gradients, which can degrade model performance and
generalization. We propose the Z-Error Loss, a statistically principled
approach that minimizes outlier influence during training by masking the
contribution of data points identified as out-of-distribution within each
batch. This method leverages batch-level statistics to automatically detect and
exclude anomalous samples, allowing the model to focus its learning on the true
underlying data structure. Our approach is robust, adaptive to data quality,
and provides valuable diagnostics for data curation and cleaning.

</details>


### [47] [Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation](https://arxiv.org/abs/2506.03062)
*Qining Zhang, Tanner Fiez, Yi Liu, Wenyang Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种结合自适应探索与A/B测试验证的新型两阶段实验框架，用于解决多指标、异质方差环境下传统A/B测试统计力不足的问题。


<details>
  <summary>更多</summary>
  
**动机:** 标准A/B测试在同时测试多个候选方案时面临统计力不足的问题，而仅使用自适应实验设计（AED）在处理具有多个指标和异质方差的情况时难以准确推断实验统计量，如平均处理效应。因此，需要一种新的实验框架来克服这些问题。

**方法:** 论文采用了一种两阶段结构：第一阶段使用基于SHRVar算法的自适应探索来识别最佳处理，第二阶段通过传统的A/B测试进行验证和统计推断。SHRVar算法结合了相对方差采样和基于奖励z值的消除策略。

**结果:** SHRVar算法能够在理论上保证错误概率呈指数下降，并且数值实验验证了该方法的有效性和优越性能。

**结论:** 论文提出了一种固定预算的多指标自适应实验设计框架，结合了自适应探索和验证阶段，有效解决了传统A/B测试在多重假设检验中的统计力问题，并通过SHRVar算法实现了对异质方差环境下的高效处理选择与统计推断。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Metric+Adaptive+Experimental+Design+under+Fixed+Budget+with+Validation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03062，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03062&send_immediately=true&force_search=false)

**原文摘要:** Standard A/B tests in online experiments face statistical power challenges
when testing multiple candidates simultaneously, while adaptive experimental
designs (AED) alone fall short in inferring experiment statistics such as the
average treatment effect, especially with many metrics (e.g., revenue, safety)
and heterogeneous variances. This paper proposes a fixed-budget multi-metric
AED framework with a two-phase structure: an adaptive exploration phase to
identify the best treatment, and a validation phase with an A/B test to verify
the treatment's quality and infer statistics. We propose SHRVar, which
generalizes sequential halving (SH) (Karnin et al., 2013) with a novel
relative-variance-based sampling and an elimination strategy built on reward
z-values. It achieves a provable error probability that decreases
exponentially, where the exponent generalizes the complexity measure for SH
(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and
heterogeneous variances, respectively. Numerical experiments verify our
analysis and demonstrate the superior performance of this new framework.

</details>


### [48] [An Approximation Theory Perspective on Machine Learning](https://arxiv.org/abs/2506.02168)
*Hrushikesh N. Mhaskar, Efstratios Tsoukanis, Ameya D. Jagtap*

**主要类别:** cs.LG

**AI概要:** 这篇论文讨论了机器学习中函数逼近的问题和挑战，以及逼近理论与机器学习实践之间的差距，并提出了一种新的方法来解决未知流形上的函数逼近问题。


<details>
  <summary>更多</summary>
  
**动机:** 论文动机在于探讨逼近理论与机器学习实践之间的差距，并研究如何改进当前机器学习框架以更好地进行函数逼近。

**方法:** 论文的方法包括回顾文献中的关键思想，讨论新兴趋势如浅层/深层网络、流形上的逼近、物理信息神经代理、神经算子和变压器架构，并介绍了一种无需学习特定流形特征的未知流形上函数逼近的新方法。

**结果:** 论文结果包括对当前机器学习框架的一些不足进行了分析，探讨了逼近理论与机器学习实践之间的差距，并介绍了实现未知流形上函数逼近的新方法。

**结论:** 论文结论是，尽管函数逼近是机器学习中的一个基本问题，但逼近理论并未在该领域的理论基础中发挥核心作用。这种脱节导致了当前机器学习框架的一些不足，并且使得训练好的模型如何泛化到未见数据或未标记数据变得不明确。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Approximation+Theory+Perspective+on+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02168，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02168&send_immediately=true&force_search=false)

**原文摘要:** A central problem in machine learning is often formulated as follows: Given a
dataset $\{(x_j, y_j)\}_{j=1}^M$, which is a sample drawn from an unknown
probability distribution, the goal is to construct a functional model $f$ such
that $f(x) \approx y$ for any $(x, y)$ drawn from the same distribution. Neural
networks and kernel-based methods are commonly employed for this task due to
their capacity for fast and parallel computation. The approximation
capabilities, or expressive power, of these methods have been extensively
studied over the past 35 years. In this paper, we will present examples of key
ideas in this area found in the literature. We will discuss emerging trends in
machine learning including the role of shallow/deep networks, approximation on
manifolds, physics-informed neural surrogates, neural operators, and
transformer architectures. Despite function approximation being a fundamental
problem in machine learning, approximation theory does not play a central role
in the theoretical foundations of the field. One unfortunate consequence of
this disconnect is that it is often unclear how well trained models will
generalize to unseen or unlabeled data. In this review, we examine some of the
shortcomings of the current machine learning framework and explore the reasons
for the gap between approximation theory and machine learning practice. We will
then introduce our novel research to achieve function approximation on unknown
manifolds without the need to learn specific manifold features, such as the
eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In
many machine learning problems, particularly classification tasks, the labels
$y_j$ are drawn from a finite set of values.

</details>


### [49] [Provable Reinforcement Learning from Human Feedback with an Unknown Link Function](https://arxiv.org/abs/2506.03066)
*Qining Zhang, Lei Ying*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的强化学习与人类偏好结合的算法ZSPO，该算法不需要预先知道人类偏好的链接函数，通过估计价值函数差别的符号来更新策略，从而提高了算法的适用性和性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统RLHF算法假设已知链接函数（如Bradley-Terry模型中的logistic函数），这在实际中可能不合理，因此需要一种不依赖链接函数的算法。

**方法:** 基于零阶策略优化方法提出ZSPO算法，利用人类偏好构建参数更新方向，通过估计价值函数差异的符号而非梯度进行优化。

**结果:** ZSPO能够在不知道链接函数的情况下收敛到平稳策略，并显示出比现有方法更好的鲁棒性。

**结论:** ZSPO是一种无需知道链接函数的新型策略优化算法，在链接函数不匹配的情况下也表现出优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provable+Reinforcement+Learning+from+Human+Feedback+with+an+Unknown+Link+Function，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03066，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03066&send_immediately=true&force_search=false)

**原文摘要:** Link functions, which characterize how human preferences are generated from
the value function of an RL problem, are a crucial component in designing RLHF
algorithms. Almost all RLHF algorithms, including state-of-the-art ones in
empirical studies such as DPO and PPO, assume the link function is known to the
agent (e.g., a logistic function according to the Bradley-Terry model), which
is arguably unrealistic considering the complex nature of human preferences. To
avoid link function mis-specification, this paper studies general RLHF problems
with unknown link functions. We propose a novel policy optimization algorithm
called ZSPO based on a new zeroth-order policy optimization method, where the
key is to use human preference to construct a parameter update direction that
is positively correlated with the true policy gradient direction. ZSPO achieves
it by estimating the sign of the value function difference instead of
estimating the gradient from the value function difference, so it does not
require knowing the link function. Under mild conditions, ZSPO converges to a
stationary policy with a polynomial convergence rate depending on the number of
policy iterations and trajectories per iteration. Numerical results also show
the superiority of ZSPO under link function mismatch.

</details>


### [50] [Learning Treatment Representations for Downstream Instrumental Variable Regression](https://arxiv.org/abs/2506.02200)
*Shiangyi Lin, Hui Lan, Vasilis Syrgkanis*

**主要类别:** cs.LG

**AI概要:** 论文介绍了一种利用工具变量改进高维处理变量估计的新方法，解决了传统IV估计器的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 传统工具变量估计器只能处理与可用工具变量数量相当的内生处理变量，而在高维和非结构化处理情况下（如医院患者治疗路径描述），这种方法存在较大限制。

**方法:** 论文提出了一种新的处理表示构建方法，将工具变量信息整合到表示学习过程中，并通过理论分析和实验验证了其有效性。

**结果:** 实验结果表明，作者提出的方法在结合工具变量信息的情况下进行降维处理，优于传统的两阶段方法。理论和实证都显示，这种新方法能够识别出优化结果预测的方向。

**结论:** 该论文提出了一种新方法，通过在表示学习过程中明确引入工具变量来构建处理表示，从而解决传统工具变量估计器在高维和非结构化处理设置中的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Treatment+Representations+for+Downstream+Instrumental+Variable+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02200，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02200&send_immediately=true&force_search=false)

**原文摘要:** Traditional instrumental variable (IV) estimators face a fundamental
constraint: they can only accommodate as many endogenous treatment variables as
available instruments. This limitation becomes particularly challenging in
settings where the treatment is presented in a high-dimensional and
unstructured manner (e.g. descriptions of patient treatment pathways in a
hospital). In such settings, researchers typically resort to applying
unsupervised dimension reduction techniques to learn a low-dimensional
treatment representation prior to implementing IV regression analysis. We show
that such methods can suffer from substantial omitted variable bias due to
implicit regularization in the representation learning step. We propose a novel
approach to construct treatment representations by explicitly incorporating
instrumental variables during the representation learning process. Our approach
provides a framework for handling high-dimensional endogenous variables with
limited instruments. We demonstrate both theoretically and empirically that
fitting IV models on these instrument-informed representations ensures
identification of directions that optimize outcome prediction. Our experiments
show that our proposed methodology improves upon the conventional two-stage
approaches that perform dimension reduction without incorporating instrument
information.

</details>


### [51] [Bregman Centroid Guided Cross-Entropy Method](https://arxiv.org/abs/2506.02205)
*Yuliang Gu, Hongpeng Cao, Marco Caccamo, Naira Hovakimyan*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bregman+Centroid+Guided+Cross-Entropy+Method，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02205&send_immediately=true&force_search=false)

**原文摘要:** The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.

</details>


### [52] [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
*Hongling Xu, Qi Zhu, Heyuan Deng, Jinpeng Li, Lu Hou, Yasheng Wang, Lifeng Shang, Ruifeng Xu, Fei Mi*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种统一的后训练框架KDRL，通过结合知识蒸馏（KD）和强化学习（RL）来提升大型语言模型的推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服单独使用强化学习或知识蒸馏在推理能力提升方面的局限性，如样本效率低和泛化能力差，作者提出了将两者结合起来的动机。

**方法:** KDRL框架利用策略梯度优化方法，同时最小化学生态度分布与教师模型分布之间的反向Kullback-Leibler散度（RKL），并最大化基于规则奖励的期望值。

**结果:** 实验结果显示，KDRL在多个推理基准上优于GRPO和其他KD基线方法，并在性能和推理令牌效率之间实现了良好的平衡。

**结论:** 集成知识蒸馏和强化学习是训练推理型大语言模型的有效且高效策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KDRL%3A+Post-Training+Reasoning+LLMs+via+Unified+Knowledge+Distillation+and+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02208&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language model (LLM) post-training have leveraged
two distinct paradigms to enhance reasoning capabilities: reinforcement
learning (RL) and knowledge distillation (KD). While RL enables the emergence
of complex reasoning behaviors, it often suffers from low sample efficiency
when the initial policy struggles to explore high-reward trajectories.
Conversely, KD improves learning efficiency via mimicking the teacher model but
tends to generalize poorly to out-of-domain scenarios. In this work, we present
\textbf{KDRL}, a \textit{unified post-training framework} that jointly
optimizes a reasoning model through teacher supervision (KD) and
self-exploration (RL). Specifically, KDRL leverages policy gradient
optimization to simultaneously minimize the reverse Kullback-Leibler divergence
(RKL) between the student and teacher distributions while maximizing the
expected rule-based rewards. We first formulate a unified objective that
integrates GRPO and KD, and systematically explore how different KL
approximations, KL coefficients, and reward-guided KD strategies affect the
overall post-training dynamics and performance. Empirical results on multiple
reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD
baselines while achieving a favorable balance between performance and reasoning
token efficiency. These findings indicate that integrating KD and RL serves as
an effective and efficient strategy to train reasoning LLMs.

</details>


### [53] [Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning](https://arxiv.org/abs/2506.02210)
*Pu, Yi, Tianlang Chen, Yifan Yang, Sara Achour*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为ExPrune的动态剪枝算法，用于减少神经网络中的对称性冗余，从而提升推理效率。


<details>
  <summary>更多</summary>
  
**动机:** 随着神经网络参数越来越多，部署所需的资源也越来越多，因此需要提高效率。已有的方法如剪枝和量化已经出现，但利用神经网络架构中的对称性冗余尚未被探索。

**方法:** 通过利用神经网络中参数和中间值的对称性（可交换性）来识别冗余，并基于每个输入进行动态剪枝。

**结果:** 在多个模型上评估了ExPrune，结果显示其能提供10.98--26.3%的FLOPs减少率，在最多1%的精度损失下甚至能达到39.05%的减少率。

**结论:** ExPrune是一种动态剪枝算法，可以有效减少神经网络中的对称性冗余，且与静态剪枝组合使用时还能进一步降低FLOPs。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exchangeability+in+Neural+Network+Architectures+and+its+Application+to+Dynamic+Pruning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02210&send_immediately=true&force_search=false)

**原文摘要:** Neural networks (NNs) are equipped with increasingly many parameters and
require more and more resource for deployment. Researchers have explored
various ways to improve the efficiency of NNs by identifying and reducing the
redundancy, such as pruning or quantizing unimportant weights. Symmetry in the
NN architectures has been identified by prior work as a possible type of
redundancy, but exploiting it for efficient inference is not yet explored. In
this work, we formalize the symmetry of parameters and intermediate values in
NNs using the statistical property of exchangeablility. We identify that
exchangeable values in NN computation may contain overlapping information,
leading to redundancy. Exploiting the insight, we derive a principled general
dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a
per-input basis. We also provide an instantiation of ExPrune that performs
neuron-level dynamic pruning by predicting negative inputs to ReLU activations.
We evaluate ExPrune on two computer vision models, one graph model and one
language model. ExPrune provides 10.98--26.3% reduction in FLOPs with
negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%
accuracy drop. We also demonstrate that ExPrune composes with static pruning.
On models that have been aggressively pruned statically, ExPrune provides
additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and
13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.

</details>


### [54] [Quantum Ensembling Methods for Healthcare and Life Science](https://arxiv.org/abs/2506.02213)
*Kahn Rhrissorrakrai, Kathleen E. Hamilton, Prerana Bangalore Parthsarathy, Aldo Guzman-Saenz, Tyler Alban, Filippo Utro, Laxmi Parida*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了量子集成模型在小数据问题中的应用，表明其在医疗和生命科学领域的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中许多应用面临小数据学习的挑战，尤其是在生物样本稀缺的情况下探索特征空间的医疗和生命科学研究领域。

**方法:** 构建了多种类型的量子集成模型，并使用最多26个模拟量子位和56个量子硬件量子位进行二分类任务。

**结果:** 测试结果显示了量子嵌入结构对性能的影响，并展示了如何提取信息特征并构建能够有效学习和泛化的模型。

**结论:** 量子集成模型在小数据问题上表现出有效性，特别是在医疗保健和生命科学领域具有潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Ensembling+Methods+for+Healthcare+and+Life+Science，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02213，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02213&send_immediately=true&force_search=false)

**原文摘要:** Learning on small data is a challenge frequently encountered in many
real-world applications. In this work we study how effective quantum ensemble
models are when trained on small data problems in healthcare and life sciences.
We constructed multiple types of quantum ensembles for binary classification
using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our
ensemble designs use minimal trainable parameters but require long-range
connections between qubits. We tested these quantum ensembles on synthetic
datasets and gene expression data from renal cell carcinoma patients with the
task of predicting patient response to immunotherapy. From the performance
observed in simulation and initial hardware experiments, we demonstrate how
quantum embedding structure affects performance and discuss how to extract
informative features and build models that can learn and generalize
effectively. We present these exploratory results in order to assist other
researchers in the design of effective learning on small data using ensembles.
Incorporating quantum computing in these data constrained problems offers hope
for a wide range of studies in healthcare and life sciences where biological
samples are relatively scarce given the feature space to be explored.

</details>


### [55] [From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](https://arxiv.org/abs/2506.02242)
*Yihong Tang, Ao Qu, Xujing Yu, Weipeng Deng, Jun Ma, Jinhua Zhao, Lijun Sun*

**主要类别:** cs.LG

**AI概要:** UrbanX是一种基于多模态大语言模型的方法，旨在通过自动化生成、评估和优化假设来实现城市设计与道路安全结果之间的可解释性假说推断。


<details>
  <summary>更多</summary>
  
**动机:** 传统工作流面临诸多挑战：(1) 依赖人类专家提出假设，费时且容易产生确认偏误；(2) 深度学习方法缺乏可解释性；(3) 未能充分利用非结构化数据。

**方法:** UrbanX利用多模态大语言模型（MLLM）生成与安全相关的问题，对街景图像进行处理，并从中提取可解释的嵌入向量，应用于基于回归的统计模型。

**结果:** 实验评估显示，该方法优于预训练深度学习模型，同时提供完全的可解释性。

**结论:** UrbanX不仅提升了模型在政策应用中的可信度，还为城市和交通研究中可解释的知识发现提供了可扩展的、基于统计学的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Street+Views+to+Urban+Science%3A+Discovering+Road+Safety+Factors+with+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02242，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02242&send_immediately=true&force_search=false)

**原文摘要:** Urban and transportation research has long sought to uncover statistically
meaningful relationships between key variables and societal outcomes such as
road safety, to generate actionable insights that guide the planning,
development, and renewal of urban and transportation systems. However,
traditional workflows face several key challenges: (1) reliance on human
experts to propose hypotheses, which is time-consuming and prone to
confirmation bias; (2) limited interpretability, particularly in deep learning
approaches; and (3) underutilization of unstructured data that can encode
critical urban context. Given these limitations, we propose a Multimodal Large
Language Model (MLLM)-based approach for interpretable hypothesis inference,
enabling the automated generation, evaluation, and refinement of hypotheses
concerning urban context and road safety outcomes. Our method leverages MLLMs
to craft safety-relevant questions for street view images (SVIs), extract
interpretable embeddings from their responses, and apply them in
regression-based statistical models. UrbanX supports iterative hypothesis
testing and refinement, guided by statistical evidence such as coefficient
significance, thereby enabling rigorous scientific discovery of previously
overlooked correlations between urban design and safety. Experimental
evaluations on Manhattan street segments demonstrate that our approach
outperforms pretrained deep learning models while offering full
interpretability. Beyond road safety, UrbanX can serve as a general-purpose
framework for urban scientific discovery, extracting structured insights from
unstructured urban data across diverse socioeconomic and environmental
outcomes. This approach enhances model trustworthiness for policy applications
and establishes a scalable, statistically grounded pathway for interpretable
knowledge discovery in urban and transportation studies.

</details>


### [56] [From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs](https://arxiv.org/abs/2506.02243)
*Tamara Cucumides, Floris Geerts*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为auGraph的新框架，该框架通过任务感知的图增强技术，有效地处理表格和关系数据，从而提高机器学习任务的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现代深度学习方法通常假设输入是扁平且特征对齐的，而表格和关系数据格式在现实世界的机器学习应用中最普遍，并且给现代深度学习方法带来了独特挑战。

**方法:** 引入auGraph，一种任务感知的图增强统一框架，应用于表格和关系数据。

**结果:** auGraph通过选择性促进属性成为节点，增强了基础图结构，实证结果显示其优于基于模式和启发式的图构建方法。

**结论:** auGraph框架通过增强基本图结构，选择性地将属性提升为节点，优于基于模式和启发式图构建方法，在关系和表格预测任务中提供了更好的学习支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Features+to+Structure%3A+Task-Aware+Graph+Construction+for+Relational+and+Tabular+Learning+with+GNNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02243&send_immediately=true&force_search=false)

**原文摘要:** Tabular and relational data remain the most ubiquitous formats in real-world
machine learning applications, spanning domains from finance to healthcare.
Although both formats offer structured representations, they pose distinct
challenges for modern deep learning methods, which typically assume flat,
feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a
promising solution by capturing structural dependencies within and between
tables. However, existing GNN-based approaches often rely on rigid,
schema-derived graphs -- such as those based on primary-foreign key links --
thereby underutilizing rich, predictive signals in non key attributes. In this
work, we introduce auGraph, a unified framework for task-aware graph
augmentation that applies to both tabular and relational data. auGraph enhances
base graph structures by selectively promoting attributes into nodes, guided by
scoring functions that quantify their relevance to the downstream prediction
task. This augmentation preserves the original data schema while injecting
task-relevant structural signal. Empirically, auGraph outperforms schema-based
and heuristic graph construction methods by producing graphs that better
support learning for relational and tabular prediction tasks.

</details>


### [57] [SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems](https://arxiv.org/abs/2506.02255)
*Asha Ramanujam, Adam Elyoumi, Hao Chen, Sai Madhukiran Kompalli, Akshdeep Singh Ahluwalia, Shraman Pal, Dimitri J. Papageorgiou, Can Li*

**主要类别:** cs.LG

**AI概要:** 论文提出了SafeOR-Gym，一个用于复杂约束下安全强化学习的基准测试平台，推动在能源系统、制造业和供应链等关键领域的应用。


<details>
  <summary>更多</summary>
  
**动机:** 现有的安全强化学习（RL）基准主要集中在机器人和控制任务上，难以满足涉及结构化约束、混合整数决策和工业复杂性的高风险领域的需求。

**方法:** 设计了包含九个运筹学环境的基准套件SafeOR-Gym，适用于复杂约束下的安全RL，并评估了多种先进的安全RL算法。

**结果:** 通过评估最先进的安全RL算法，发现部分任务可行，但其他任务暴露了当前方法的基本局限性。

**结论:** SafeOR-Gym提供了一个具有挑战性和实用性的安全强化学习测试平台，旨在推动现实世界决策问题的安全RL研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SafeOR-Gym%3A+A+Benchmark+Suite+for+Safe+Reinforcement+Learning+Algorithms+on+Practical+Operations+Research+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02255&send_immediately=true&force_search=false)

**原文摘要:** Most existing safe reinforcement learning (RL) benchmarks focus on robotics
and control tasks, offering limited relevance to high-stakes domains that
involve structured constraints, mixed-integer decisions, and industrial
complexity. This gap hinders the advancement and deployment of safe RL in
critical areas such as energy systems, manufacturing, and supply chains. To
address this limitation, we present SafeOR-Gym, a benchmark suite of nine
operations research (OR) environments tailored for safe RL under complex
constraints. Each environment captures a realistic planning, scheduling, or
control problems characterized by cost-based constraint violations, planning
horizons, and hybrid discrete-continuous action spaces. The suite integrates
seamlessly with the Constrained Markov Decision Process (CMDP) interface
provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms
across these environments, revealing a wide range of performance: while some
tasks are tractable, others expose fundamental limitations in current
approaches. SafeOR-Gym provides a challenging and practical testbed that aims
to catalyze future research in safe RL for real-world decision-making problems.
The SafeOR-Gym framework and all accompanying code are available at:
https://github.com/li-group/SafeOR-Gym.

</details>


### [58] [Human Heterogeneity Invariant Stress Sensing](https://arxiv.org/abs/2506.02256)
*Yi Xiao, Harshit Sharma, Sawinder Kaur, Dessa Bergen-Cico, Asif Salekin*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的压力检测模型HHISS，能够适应个体差异，在多种环境下都能准确检测压力变化，尤其适用于阿片类药物使用障碍患者的康复管理。


<details>
  <summary>更多</summary>
  
**动机:** 可穿戴设备用于日常压力监测，但生理信号因个体差异和健康状况而异，使机器学习模型难以泛化。

**方法:** 提出了Human Heterogeneity Invariant Stress Sensing (HHISS)，通过个体子网络剪枝交叉技术关注共享特征，并利用连续标签防止过拟合。

**结果:** HHISS在七个不同数据集中均优于现有最先进基线方法，并验证了其可行性与可扩展性。

**结论:** HHISS是一种有效的压力检测方法，适用于各种人群、环境和压力类型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human+Heterogeneity+Invariant+Stress+Sensing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02256，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02256&send_immediately=true&force_search=false)

**原文摘要:** Stress affects physical and mental health, and wearable devices have been
widely used to detect daily stress through physiological signals. However,
these signals vary due to factors such as individual differences and health
conditions, making generalizing machine learning models difficult. To address
these challenges, we present Human Heterogeneity Invariant Stress Sensing
(HHISS), a domain generalization approach designed to find consistent patterns
in stress signals by removing person-specific differences. This helps the model
perform more accurately across new people, environments, and stress types not
seen during training. Its novelty lies in proposing a novel technique called
person-wise sub-network pruning intersection to focus on shared features across
individuals, alongside preventing overfitting by leveraging continuous labels
while training. The study focuses especially on people with opioid use disorder
(OUD)-a group where stress responses can change dramatically depending on their
time of daily medication taking. Since stress often triggers cravings, a model
that can adapt well to these changes could support better OUD rehabilitation
and recovery. We tested HHISS on seven different stress datasets-four of which
we collected ourselves and three public ones. Four are from lab setups, one
from a controlled real-world setting, driving, and two are from real-world
in-the-wild field datasets without any constraints. This is the first study to
evaluate how well a stress detection model works across such a wide range of
data. Results show HHISS consistently outperformed state-of-the-art baseline
methods, proving both effective and practical for real-world use. Ablation
studies, empirical justifications, and runtime evaluations confirm HHISS's
feasibility and scalability for mobile stress sensing in sensitive real-world
applications.

</details>


### [59] [A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models](https://arxiv.org/abs/2506.02269)
*YuQing Xie, Tess Smidt*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了等变神经网络的优化问题，指出在某些情况下，放松等变约束并重新考虑隐藏层中的群表示可以改善学习效果。


<details>
  <summary>更多</summary>
  
**动机:** 等变神经网络在具有已知对称性的任务中已被证明是有效的，但优化这些网络可能很棘手。最近的研究发现放松等变约束有较小的训练优势，这引发了一个问题：等变约束是否会引入优化的基本障碍？

**方法:** 通过理论分析损失景观几何结构，并集中在使用排列表示的网络上。

**结果:** 研究表明，无约束模型的参数对称性对等变子空间的损失景观有重要影响，在某些情况下甚至可以防止学习全局最小值。此外，实证结果显示在这种情况下，放松到无约束MLP有时可以解决问题。

**结论:** 论文得出结论，放宽等变约束不仅需要增加非等变的自由度，还需要重新考虑隐藏层中群表示的固定选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Tale+of+Two+Symmetries%3A+Exploring+the+Loss+Landscape+of+Equivariant+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02269，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02269&send_immediately=true&force_search=false)

**原文摘要:** Equivariant neural networks have proven to be effective for tasks with known
underlying symmetries. However, optimizing equivariant networks can be tricky
and best training practices are less established than for standard networks. In
particular, recent works have found small training benefits from relaxing
equivariance constraints. This raises the question: do equivariance constraints
introduce fundamental obstacles to optimization? Or do they simply require
different hyperparameter tuning? In this work, we investigate this question
through a theoretical analysis of the loss landscape geometry. We focus on
networks built using permutation representations, which we can view as a subset
of unconstrained MLPs. Importantly, we show that the parameter symmetries of
the unconstrained model has nontrivial effects on the loss landscape of the
equivariant subspace and under certain conditions can provably prevent learning
of the global minima. Further, we empirically demonstrate in such cases,
relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,
the weights eventually found via relaxation corresponds to a different choice
of group representation in the hidden layer. From this, we draw 3 key
takeaways. (1) Viewing any class of networks in the context of larger
unconstrained function space can give important insights on loss landscape
structure. (2) Within the unconstrained function space, equivariant networks
form a complicated union of linear hyperplanes, each associated with a specific
choice of internal group representation. (3) Effective relaxation of
equivariance may require not only adding nonequivariant degrees of freedom, but
also rethinking the fixed choice of group representations in hidden layers.

</details>


### [60] [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/abs/2506.02281)
*Qinsi Wang, Jinghan Ke, Hancheng Ye, Yueqian Lin, Yuzhe Fu, Jianyi Zhang, Kurt Keutzer, Chenfeng Xu, Yiran Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的训练方法GAIN-RL，它通过分析模型内部的学习信号来提高大语言模型强化微调的训练效率，实验证明其在多种任务上都有显著的效果提升。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大语言模型强化微调范式由于均匀数据采样下的重复查询暴露而存在样本效率低下的问题。虽然先前的工作探索了通过启发式难度指标进行课程学习，但这些策略忽略了模型本身产生的内在学习信号，导致次优的训练机制。

**方法:** 提出了一种名为GAIN-RL的框架，该框架基于模型固有的角度集中信号来指导数据选择，以此来优化训练过程。

**结果:** 实验评估显示，GAIN-RL（GRPO）在各种数学和编程任务及不同模型规模上实现了超过2.5倍的训练效率加速。此外，GAIN-RL（GRPO）的高效采样产生了数据高效的训练，在使用原始一半的数据时就能达到比全量训练数据的GRPO更好的性能。

**结论:** GAIN-RL通过利用模型内在的角度集中信号，在每个训练周期动态选择训练数据，确保了持续有效的梯度更新，从而显著提高了整体训练效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Angles+Don%27t+Lie%3A+Unlocking+Training-Efficient+RL+Through+the+Model%27s+Own+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02281&send_immediately=true&force_search=false)

**原文摘要:** Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
acceleration in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

</details>


### [61] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/abs/2506.02285)
*Aaron Defazio*

**主要类别:** cs.LG

**AI概要:** 本文讨论了大语言模型训练末期梯度范数快速增加的问题，并提出了一个简单修正方法以降低训练损失。


<details>
  <summary>更多</summary>
  
**动机:** 在长时间的大语言模型训练过程中，发现梯度范数在接近训练结束时迅速增加，这需要深入理解并解决。

**方法:** 分析长持续时间的大语言模型训练运行中梯度范数的变化，并研究其原因。

**结果:** 提出了一种简单的修正方法，不仅解决了梯度范数增加的问题，而且在整个训练过程中导致了更低的损失值。

**结论:** 论文得出结论，梯度范数的增加是由于权重衰减、归一化层和学习率调度之间的意外相互作用，并提出了一种简单的修正方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+Gradients+Rapidly+Increase+Near+the+End+of+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02285&send_immediately=true&force_search=false)

**原文摘要:** During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [62] [On Universality Classes of Equivariant Networks](https://arxiv.org/abs/2506.02293)
*Marco Pacini, Gabriele Santin, Bruno Lepri, Shubhendu Trivedi*

**主要类别:** cs.LG

**AI概要:** 该论文分析了等变神经网络的逼近能力，揭示了其表达能力不仅限于分离能力，并探讨了其在不同对称群设置下的普遍性条件。


<details>
  <summary>更多</summary>
  
**动机:** 尽管等变神经网络的分离能力已被广泛研究，但其逼近目标函数的能力（即普遍性）仍未被充分探索。

**方法:** 通过研究浅层不变网络的普遍性类别，分析其逼近能力，并探讨等变模型在投影下减少到不变模型的情况。

**结果:** 研究表明分离能力并不能完全反映表达能力：具有相同分离能力的模型可能在逼近能力上存在差异。此外，作者提供了浅层不变网络的普遍性类别的刻画，并给出了等变网络无法普遍的条件。

**结论:** 本文得出浅层等变神经网络在某些条件下无法达到普遍性，但在其他条件下可以实现分离约束下的普遍性，这取决于对称群的结构性质。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Universality+Classes+of+Equivariant+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02293，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02293&send_immediately=true&force_search=false)

**原文摘要:** Equivariant neural networks provide a principled framework for incorporating
symmetry into learning architectures and have been extensively analyzed through
the lens of their separation power, that is, the ability to distinguish inputs
modulo symmetry. This notion plays a central role in settings such as graph
learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In
contrast, the universality of equivariant models-their capacity to approximate
target functions-remains comparatively underexplored. In this work, we
investigate the approximation power of equivariant neural networks beyond
separation constraints. We show that separation power does not fully capture
expressivity: models with identical separation power may differ in their
approximation ability. To demonstrate this, we characterize the universality
classes of shallow invariant networks, providing a general framework for
understanding which functions these architectures can approximate. Since
equivariant models reduce to invariant ones under projection, this analysis
yields sufficient conditions under which shallow equivariant networks fail to
be universal. Conversely, we identify settings where shallow models do achieve
separation-constrained universality. These positive results, however, depend
critically on structural properties of the symmetry group, such as the
existence of adequate normal subgroups, which may not hold in important cases
like permutation symmetry.

</details>


### [63] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300)
*Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于梯度放大的新方法，用于可视化深度神经网络的决策过程，从而更好地理解模型如何区分不同类别。


<details>
  <summary>更多</summary>
  
**动机:** 现有的可解释性方法难以阐明模型如何区分不同类别以及输入从一类过渡到另一类的变化方式。

**方法:** 将网络梯度视为一种微小运动，利用相位运动放大技术，在可逆变换域（Complex Steerable Pyramid）中计算类别条件梯度并进行线性外推。

**结果:** 实验表明，这种方法能够生成语义上有意义且空间一致的变换，为理解神经分类器的内部表示提供了一个新颖且可解释的视角。

**结论:** 该论文提出了一种新的可视化框架，通过梯度放大和线性外推揭示了神经网络分类器的决策机制，并展示了其在合成和真实数据集上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Through+a+Steerable+Lens%3A+Magnifying+Neural+Network+Interpretability+via+Phase-Based+Extrapolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02300，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02300&send_immediately=true&force_search=false)

**原文摘要:** Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [64] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/abs/2506.02308)
*Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为MINT的多模态任务分组策略，它通过按多模态交互类型对任务进行分组，从而提升模型的性能，并在多模态指令调整任务中取得了良好的效果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在扩大指令微调数据集方面存在越来越多的兴趣，但简单地增加任务数量并不能持续提高性能。需要一种更有效的方法来鼓励模型学习可转移的技能并抑制不匹配任务的干扰。

**方法:** 引入了一种名为MINT的任务分组策略，并通过实验验证其性能。

**结果:** 实验表明，该方法在多模态指令调整任务中显著优于现有任务分组基线。

**结论:** MINT策略基于多模态交互类型进行任务分组，在多模态指令调整中优于现有的任务分组基线，有效平衡了泛化和专业化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MINT%3A+Multimodal+Instruction+Tuning+with+Multimodal+Interaction+Grouping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02308，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02308&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [65] [A Data-Based Architecture for Flight Test without Test Points](https://arxiv.org/abs/2506.02315)
*D. Isaiah Harp, Joshua Ott, John Alora, Dylan Asmar*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的“无测试点”方法，利用机器学习生成降阶模型，并通过飞行测试数据进行更新，以提高预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 论文的动机是解决传统测试点方法中存在的问题，即飞行员偏离预设条件会使得模型假设失效，而数据带和公差的存在是一个比飞行员技能不足更根本的问题。

**方法:** 该论文使用机器学习方法生成一个降阶模型（ROM），并利用飞行测试数据对其进行更新和调整，以实现更准确的预测。

**结果:** 论文的结果展示了基于T-38C飞行测试数据的“无测试点”架构的应用，成功生成了用于评估纵向动态符合MIL-STD-1797B标准的参数。

**结论:** 论文的结论是提出了一种“无测试点”的新方法，通过使用飞行测试数据改进降阶模型（ROM），从而更新和验证高保真模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Data-Based+Architecture+for+Flight+Test+without+Test+Points，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02315，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02315&send_immediately=true&force_search=false)

**原文摘要:** The justification for the "test point" derives from the test pilot's
obligation to reproduce faithfully the pre-specified conditions of some model
prediction. Pilot deviation from those conditions invalidates the model
assumptions. Flight test aids have been proposed to increase accuracy on more
challenging test points. However, the very existence of databands and
tolerances is the problem more fundamental than inadequate pilot skill. We
propose a novel approach, which eliminates test points. We start with a
high-fidelity digital model of an air vehicle. Instead of using this model to
generate a point prediction, we use a machine learning method to produce a
reduced-order model (ROM). The ROM has two important properties. First, it can
generate a prediction based on any set of conditions the pilot flies. Second,
if the test result at those conditions differ from the prediction, the ROM can
be updated using the new data. The outcome of flight test is thus a refined ROM
at whatever conditions were flown. This ROM in turn updates and validates the
high-fidelity model. We present a single example of this "point-less"
architecture, using T-38C flight test data. We first use a generic aircraft
model to build a ROM of longitudinal pitching motion as a hypersurface. We then
ingest unconstrained flight test data and use Gaussian Process Regression to
update and condition the hypersurface. By proposing a second-order equivalent
system for the T-38C, this hypersurface then generates parameters necessary to
assess MIL-STD-1797B compliance for longitudinal dynamics.

</details>


### [66] [Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models](https://arxiv.org/abs/2506.02318)
*Yuchen Liang, Renxiang Huang, Lifeng Lai, Ness Shroff, Yingbin Liang*

**主要类别:** cs.LG

**AI概要:** 本研究首次为使用吸收速率矩阵的离散扩散模型提供了理论上的收敛保证和误差分析，改进了现有方法的表现。


<details>
  <summary>更多</summary>
  
**动机:** 吸收速率矩阵在生成质量上通常优于均匀速率矩阵，但其缺乏理论支持，尤其是收敛保证和误差分析。

**方法:** 通过推导前向过程的KL散度上界，并引入代理初始化分布来处理吸收平稳分布带来的挑战，同时建立了τ跳跃和均匀化采样器的收敛保证。

**结果:** 首次提出了基于吸收速率矩阵的离散扩散模型的收敛保证和误差分析，并在合适假设下实现了无需提前停止的收敛保证。

**结论:** 本文提供了首个使用吸收速率矩阵的离散扩散模型的有限时间误差界和收敛率分析，证明了其在生成质量和收敛性方面优于均匀速率矩阵。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Absorb+and+Converge%3A+Provable+Convergence+Guarantee+for+Absorbing+Discrete+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02318，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02318&send_immediately=true&force_search=false)

**原文摘要:** Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.

</details>


### [67] [Sensitivity-Aware Density Estimation in Multiple Dimensions](https://arxiv.org/abs/2506.02323)
*Aleix Boquet-Pujadas, Pol del Aguila Pla, Michael Unser*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种针对多维非均匀采样概率密度估计的新方法，结合了样条计算和核范数正则化，并成功应用于PET数据重组。


<details>
  <summary>更多</summary>
  
**动机:** 解决在非均匀采样条件下多维概率密度估计的问题，并考虑探测器灵敏度作为异质密度的影响。

**方法:** 通过优化问题公式化概率密度估计，利用网格上的样条函数进行计算，并使用核范数对样条的Hessian矩阵进行正则化以促进稀疏性。

**结果:** 该方法在空间上具有适应性，并且对于正则化参数的选择是稳定的，同时提供了一个新的PET数据重组的应用案例。

**结论:** 该论文提出了一种基于样条核范数正则化的新方法，用于估计多维问题的概率密度，并展示了其在PET数据重组中的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sensitivity-Aware+Density+Estimation+in+Multiple+Dimensions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02323&send_immediately=true&force_search=false)

**原文摘要:** We formulate an optimization problem to estimate probability densities in the
context of multidimensional problems that are sampled with uneven probability.
It considers detector sensitivity as an heterogeneous density and takes
advantage of the computational speed and flexible boundary conditions offered
by splines on a grid. We choose to regularize the Hessian of the spline via the
nuclear norm to promote sparsity. As a result, the method is spatially adaptive
and stable against the choice of the regularization parameter, which plays the
role of the bandwidth. We test our computational pipeline on standard densities
and provide software. We also present a new approach to PET rebinning as an
application of our framework.

</details>


### [68] [Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening](https://arxiv.org/abs/2506.02355)
*Andre He, Daniel Fried, Sean Welleck*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种改进的强化学习方法，以解决GRPO在形式化定理证明任务中样本偏差的问题。


<details>
  <summary>更多</summary>
  
**动机:** GRPO算法在多样本性能任务（如形式化定理证明）中偏向加强已可能的解决方案，忽略了罕见但正确的证明，影响了其实际应用。

**方法:** 引入“unlikeliness reward”机制，并增加PPO训练周期，以提升对罕见正确解的探索和样本多样性。

**结果:** 实验表明，新方法显著提升了pass@$N$指标，且在miniF2F-test基准上达到了与DeepSeek-Prover-V1.5-RL相当的性能。

**结论:** 改进后的强化学习策略为训练高效的形式化定理证明器提供了简单而有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rewarding+the+Unlikely%3A+Lifting+GRPO+Beyond+Distribution+Sharpening，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02355&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning has emerged as an effective framework for training
large language models on structured language-conditioned tasks. We identify a
critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL
algorithm in this setting. For tasks that require multi-sample performance,
such as formal theorem proving, GRPO biasedly reinforces already probable
solutions and neglects rare but correct proofs. This implicit bias impairs
performance on pass@$N$ metrics at large sample sizes, limiting its
practicality for training theorem provers. To address this, we introduce the
unlikeliness reward, a straightforward method that explicitly encourages
reinforcing rare correct solutions. Additionally, we find that increasing the
number of PPO epochs further mitigates this bias. Our experiments confirm that
incorporating the unlikeliness reward significantly improves pass@$N$ across a
large range of N, outperforming standard GRPO and substantially increasing
sample diversity. Applying our revised recipe to Lean, we achieve competitive
performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We
release our implementation, providing a simple yet effective recipe for
training formal theorem provers with RL.

</details>


### [69] [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/abs/2506.02357)
*Ram Potham*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于评估大型语言模型（LLM）代理在面对冲突任务时是否能够遵守安全原则的简单测试方法，强调了其在构建可控AI系统中的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 可信的安全计划需要早期验证代理行为并检测潜在控制缺陷的方法。确保代理遵循安全关键原则，尤其是在这些原则与操作目标冲突时，这是保证AI系统可控性的基础。

**方法:** 引入了一种轻量级、可解释的基准测试方法，使用简单的网格世界来评估LLM代理在面对冲突的低级任务指令时，坚持预定义高级安全原则的能力。

**结果:** 该试点研究证明了该方法的可行性，提供了对代理在原则冲突下行为的初步认识，并讨论了此类基准如何为评估可控性提供实证证据。

**结论:** 评估LLM代理在面对冲突任务时遵守安全原则的能力是构建可控AI系统的关键初步步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+LLM+Agent+Adherence+to+Hierarchical+Safety+Principles%3A+A+Lightweight+Benchmark+for+Probing+Foundational+Controllability+Components，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02357，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02357&send_immediately=true&force_search=false)

**原文摘要:** Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. Failure to prioritize such principles
indicates a potential basic control failure. This paper introduces a
lightweight, interpretable benchmark methodology using a simple grid world to
evaluate an LLM agent's ability to uphold a predefined, high-level safety
principle (e.g., "never enter hazardous zones") when faced with conflicting
lower-level task instructions. We probe whether the agent reliably prioritizes
the inviolable directive, testing a foundational controllability aspect of
LLMs. This pilot study demonstrates the methodology's feasibility, offers
preliminary insights into agent behavior under principle conflict, and
discusses how such benchmarks can contribute empirical evidence for assessing
controllability. We argue that evaluating adherence to hierarchical principles
is a crucial early step in understanding our capacity to build governable AI
systems.

</details>


### [70] [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/abs/2506.02370)
*Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的联邦学习方法HiSo，该方法利用Hessian信息改进零阶优化，同时保持标量通信的特性，从而在大语言模型微调中实现更快的收敛速度和更高的通信效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习框架如DeComFL虽然通过零阶随机梯度下降（ZO-SGD）减少了每轮通信的维度，但其高方差导致收敛速度较慢。尽管已知利用Hessian信息可以提高优化速度，但在联邦学习中集成这一信息存在客户端数据限制和保持无维度通信属性的重大挑战。

**方法:** 首先引入了一个广义的仅标量通信的联邦学习框架，将无维度通信与标准ZO-SGD解耦，为集成更先进的优化策略提供了可能。在此框架基础上提出了HiSo方法，通过Hessian信息辅助的零阶优化和仅标量通信来加速收敛，同时保持最低的通信成本。

**结果:** 理论上建立了独立于全局Lipschitz常数的收敛保证，并进一步表明当全局Hessian矩阵具有较低的有效秩时，HiSo能够达到更快的收敛速率。实验结果显示，在基准数据集和大语言模型微调任务上，HiSo在收敛速度和通信效率方面显著优于现有的基于ZO的联邦学习方法。

**结论:** HiSo是一种有效的联邦学习方法，能够在不增加通信负担的情况下利用全局曲率信息加速优化过程，适用于大语言模型的高效微调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reconciling+Hessian-Informed+Acceleration+and+Scalar-Only+Communication+for+Efficient+Federated+Zeroth-Order+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02370，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02370&send_immediately=true&force_search=false)

**原文摘要:** Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.

</details>


### [71] [SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples](https://arxiv.org/abs/2506.02371)
*Haoye Lu, Darren Lo, Yaoliang Yu*

**主要类别:** cs.LG

**AI概要:** This paper introduces SFBD flow, a more efficient and easier-to-implement variant of SFBD, which improves privacy in diffusion models and outperforms existing methods.


<details>
  <summary>更多</summary>
  
**动机:** Diffusion models face privacy risks due to their reliance on large datasets that may contain sensitive content and their tendency to memorize training data. Existing methods like SFBD are effective but cumbersome due to manual coordination requirements.

**方法:** The authors reinterpret SFBD as an alternating projection algorithm and propose a continuous variant called SFBD flow. They also demonstrate its connection to consistency constraint-based methods and evaluate its performance through experiments.

**结果:** The proposed method, SFBD flow, eliminates the need for iterative denoising and fine-tuning loops, making it easier to implement. Online SFBD, a practical version of SFBD flow, consistently outperforms strong baselines across benchmarks.

**结论:** SFBD flow, a continuous variant of SFBD, effectively addresses privacy concerns in diffusion models by removing the need for alternating steps and outperforms strong baselines.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SFBD+Flow%3A+A+Continuous-Optimization+Framework+for+Training+Diffusion+Models+with+Noisy+Samples，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02371，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02371&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models achieve strong generative performance but often rely on
large datasets that may include sensitive content. This challenge is compounded
by the models' tendency to memorize training data, raising privacy concerns.
SFBD (Lu et al., 2025) addresses this by training on corrupted data and using
limited clean samples to capture local structure and improve convergence.
However, its iterative denoising and fine-tuning loop requires manual
coordination, making it burdensome to implement. We reinterpret SFBD as an
alternating projection algorithm and introduce a continuous variant, SFBD flow,
that removes the need for alternating steps. We further show its connection to
consistency constraint-based methods, and demonstrate that its practical
instantiation, Online SFBD, consistently outperforms strong baselines across
benchmarks.

</details>


### [72] [Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget](https://arxiv.org/abs/2506.02386)
*Jie Bian, Vincent Y. F. Tan*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的线性赌博机最佳可行臂识别算法，能够在固定预算条件下实现误差概率的指数衰减，并从理论上证明其衰减速率达到最优下界。


<details>
  <summary>更多</summary>
  
**动机:** 尽管近年来对在固定预算内识别最佳可行臂的问题已有较多研究，但误差概率趋于零的精确指数速率仍未明确，尤其是在高斯噪声环境下。这是文章的主要研究动机。

**方法:** 采用基于后验抽样框架的游戏式采样规则，结合最小学习者和最大学习者，设计了一种适用于线性赌博机的新算法。

**结果:** 新算法能够保证误差概率呈指数衰减，并且衰减速率达到了信息论原理推导出的理论下界。

**结论:** 本文通过引入一种新的最佳可行臂识别算法，成功填补了在固定预算下确定最优可行臂的误差概率指数衰减速率的理论空白。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotically+Optimal+Linear+Best+Feasible+Arm+Identification+with+Fixed+Budget，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02386，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02386&send_immediately=true&force_search=false)

**原文摘要:** The challenge of identifying the best feasible arm within a fixed budget has
attracted considerable interest in recent years. However, a notable gap remains
in the literature: the exact exponential rate at which the error probability
approaches zero has yet to be established, even in the relatively simple
setting of $K$-armed bandits with Gaussian noise. In this paper, we address
this gap by examining the problem within the context of linear bandits. We
introduce a novel algorithm for best feasible arm identification that
guarantees an exponential decay in the error probability. Remarkably, the decay
rate -- characterized by the exponent -- matches the theoretical lower bound
derived using information-theoretic principles. Our approach leverages a
posterior sampling framework embedded within a game-based sampling rule
involving a min-learner and a max-learner. This strategy shares its foundations
with Thompson sampling, but is specifically tailored to optimize the
identification process under fixed-budget constraints. Furthermore, we validate
the effectiveness of our algorithm through comprehensive empirical evaluations
across various problem instances with different levels of complexity. The
results corroborate our theoretical findings and demonstrate that our method
outperforms several benchmark algorithms in terms of both accuracy and
efficiency.

</details>


### [73] [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/abs/2506.02389)
*Chamara Madarasingha, Nasrin Sohrabi, Zahir Tari*

**主要类别:** cs.LG

**AI概要:** LLMPred利用大型语言模型进行时间序列预测，通过序列分解和提示处理策略，在小模型上实现高效准确的预测。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究较少探索大型语言模型在复杂、嘈杂和多变量时间序列数据上的有效性。

**方法:** 提出LLMPred方法，包括时间序列分解和轻量级提示处理策略，以增强基于LLM的时间序列预测。

**结果:** 实验表明，LLMPred在多个较小的LLM上（如Llama 2 7B、Llama 3.2 3B、GPT-4o-mini和DeepSeek 7B）实现了与先进基线方法相当或更优的性能。

**结论:** LLMPred通过将时间序列数据转换为文本并结合预处理技术，使用较小的大型语言模型在时间序列预测任务中表现出色，甚至优于最先进的基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Univariate+to+Multivariate%3A+LLMs+as+Zero-Shot+Predictors+for+Time-Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02389&send_immediately=true&force_search=false)

**原文摘要:** Time-series prediction or forecasting is critical across many real-world
dynamic systems, and recent studies have proposed using Large Language Models
(LLMs) for this task due to their strong generalization capabilities and
ability to perform well without extensive pre-training. However, their
effectiveness in handling complex, noisy, and multivariate time-series data
remains underexplored. To address this, we propose LLMPred which enhances
LLM-based time-series prediction by converting time-series sequences into text
and feeding them to LLMs for zero shot prediction along with two main data
pre-processing techniques. First, we apply time-series sequence decomposition
to facilitate accurate prediction on complex and noisy univariate sequences.
Second, we extend this univariate prediction capability to multivariate data
using a lightweight prompt-processing strategy. Extensive experiments with
smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B
demonstrate that LLMPred achieves competitive or superior performance compared
to state-of-the-art baselines. Additionally, a thorough ablation study
highlights the importance of the key components proposed in LLMPred.

</details>


### [74] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/abs/2506.02390)
*Qin Xie, Qinghua Zhang, Shuyin Xia, Xinran Zhou, Guoyin Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了GAdaBoost框架及其扩展GAdaBoost.SA，通过粒计算减少冗余数据使用并提升AdaBoost在噪声环境下的表现。


<details>
  <summary>更多</summary>
  
**动机:** Adaptive Boosting (AdaBoost) 在面对标签噪声时存在显著问题，尤其是在多类分类任务中，而现有方法无法高效且有效地解决这些问题。

**方法:** 提出了一种基于粒计算的两阶段框架GAdaBoost，包括数据粒化阶段和自适应增强阶段，并设计了基于粒球的SAMME算法。

**结果:** 实验结果显示，该方法在一些有噪声的数据集上实现了比现有方法更优的鲁棒性和效率。

**结论:** GAdaBoost.SA在处理带标签噪声的多类分类任务中表现出色，有效提升了Adaptive Boosting的鲁棒性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GAdaBoost%3A+An+Efficient+and+Robust+AdaBoost+Algorithm+Based+on+Granular-Ball+Structure，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02390&send_immediately=true&force_search=false)

**原文摘要:** Adaptive Boosting (AdaBoost) faces significant challenges posed by label
noise, especially in multiclass classification tasks. Existing methods either
lack mechanisms to handle label noise effectively or suffer from high
computational costs due to redundant data usage. Inspired by granular
computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel
two-stage framework comprising a data granulation stage and an adaptive
boosting stage, to enhance efficiency and robustness under noisy conditions. To
validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is
proposed. Specifically, first, a granular-ball generation method is designed to
compress data while preserving diversity and mitigating label noise. Second,
the granular ball-based SAMME algorithm focuses on granular balls rather than
individual samples, improving efficiency and reducing sensitivity to noise.
Experimental results on some noisy datasets show that the proposed approach
achieves superior robustness and efficiency compared with existing methods,
demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [75] [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/abs/2506.02392)
*Yuanyao Chen, Rongsheng Chen, Fu Luo, Zhenkun Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种由大型语言模型驱动的新框架，解决了传统NCO方法在大规模车辆路径问题中性能下降的问题，并取得了显著效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的NCO方法在小规模实例上表现良好，但在大规模场景中由于训练与测试数据分布的差异导致性能下降，因此需要一种新方法来解决这一问题。

**方法:** 论文提出了一种新的学习框架，利用大型语言模型（LLMs）学习训练和测试数据分布之间的映射，从而提高现有NCO模型的可扩展性。

**结果:** 实验表明，该方法使基于100节点实例训练的骨干模型能够在多达10万个节点的大规模TSP和CVRP问题上实现卓越性能。

**结论:** 论文得出结论，提出的LLM驱动的框架能够显著提升NCO模型在大规模车辆路径问题上的性能，同时不需要重新训练模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Generalization+of+Neural+Combinatorial+Optimization+for+Vehicle+Routing+Problems+via+Test-Time+Projection+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02392，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02392&send_immediately=true&force_search=false)

**原文摘要:** Neural Combinatorial Optimization (NCO) has emerged as a promising
learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by
minimizing the need for extensive manual engineering. While existing NCO
methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated
considerable success on problems of similar scale, their performance
significantly degrades when applied to large-scale scenarios. This degradation
arises from the distributional shift between training and testing data,
rendering policies learned on small instances ineffective for larger problems.
To overcome this limitation, we introduce a novel learning framework driven by
Large Language Models (LLMs). This framework learns a projection between the
training and testing distributions, which is then deployed to enhance the
scalability of the NCO model. Notably, unlike prevailing techniques that
necessitate joint training with the neural network, our approach operates
exclusively during the inference phase, obviating the need for model
retraining. Extensive experiments demonstrate that our method enables a
backbone model (trained on 100-node instances) to achieve superior performance
on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) of up to 100K nodes from diverse distributions.

</details>


### [76] [AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting](https://arxiv.org/abs/2506.02415)
*Karthikeyan Vaiapury*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的机器学习优化框架AERO，灵感来自柔道的重定向原理，通过15个相互关联的公理进行对抗修正、能量守恒和干扰感知学习。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法在保持动态非线性系统中的稳定性和适应性方面常常遇到困难，尤其是在不确定性下。

**方法:** AERO利用梯度投影、整合不确定性驱动的动态以及管理学习能量，重新构想优化为一种重定向过程，受到Judo重定向原则的启发。

**结果:** 应用于概率太阳能预测时，AERO在预测准确性、可靠性和适应性方面展示了显著提升，特别是在嘈杂和不确定的环境中。

**结论:** AERO是一个在优化理论和实践领域令人信服的新方向，尤其在概率太阳能预测中的应用表明了其显著的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AERO%3A+A+Redirection-Based+Optimization+Framework+Inspired+by+Judo+for+Robust+Probabilistic+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02415&send_immediately=true&force_search=false)

**原文摘要:** Optimization remains a fundamental pillar of machine learning, yet existing
methods often struggle to maintain stability and adaptability in dynamic, non
linear systems, especially under uncertainty. We introduce AERO (Adversarial
Energy-based Redirection Optimization), a novel framework inspired by the
redirection principle in Judo, where external disturbances are leveraged rather
than resisted. AERO reimagines optimization as a redirection process guided by
15 interrelated axioms encompassing adversarial correction, energy
conservation, and disturbance-aware learning. By projecting gradients,
integrating uncertainty driven dynamics, and managing learning energy, AERO
offers a principled approach to stable and robust model updates. Applied to
probabilistic solar energy forecasting, AERO demonstrates substantial gains in
predictive accuracy, reliability, and adaptability, especially in noisy and
uncertain environments. Our findings highlight AERO as a compelling new
direction in the theoretical and practical landscape of optimization.

</details>


### [77] [Weak Supervision for Real World Graphs](https://arxiv.org/abs/2506.02451)
*Pratheeksha Nair, Reihaneh Rabbany*

**主要类别:** cs.LG

**AI概要:** WSNET是一种利用弱信号进行图节点分类的新方法，在标签稀缺和噪声环境下表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的图节点分类面临标签稀缺和噪声问题，尤其是在高风险领域如人口贩卖检测和虚假信息监控中。

**方法:** 提出了一种名为WSNET的弱监督图对比学习框架，结合了图结构、节点特征和多个噪声监督源。

**结果:** 在三个现实数据集和合成基准测试中，WSNET在F1分数上比现有技术高出最多达15%。

**结论:** WSNET能够有效利用弱信号进行节点分类，优于现有的对比学习和噪声标签学习方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weak+Supervision+for+Real+World+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02451&send_immediately=true&force_search=false)

**原文摘要:** Node classification in real world graphs often suffers from label scarcity
and noise, especially in high stakes domains like human trafficking detection
and misinformation monitoring. While direct supervision is limited, such graphs
frequently contain weak signals, noisy or indirect cues, that can still inform
learning. We propose WSNET, a novel weakly supervised graph contrastive
learning framework that leverages these weak signals to guide robust
representation learning. WSNET integrates graph structure, node features, and
multiple noisy supervision sources through a contrastive objective tailored for
weakly labeled data. Across three real world datasets and synthetic benchmarks
with controlled noise, WSNET consistently outperforms state of the art
contrastive and noisy label learning methods by up to 15% in F1 score. Our
results highlight the effectiveness of contrastive learning under weak
supervision and the promise of exploiting imperfect labels in graph based
settings.

</details>


### [78] [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
*Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, Weigao Sun*

**主要类别:** cs.LG

**AI概要:** 论文介绍了一种新的非线性RNN模型Comba，该模型通过标量加低秩状态转移及反馈校正机制，在语言和视觉建模任务中表现出优越的性能和计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 近期的高效序列建模方法通过Delta学习规则监督循环记忆管理取得了性能提升，但这些模型引入了递归状态和键向量之间的交互，导致非线性递归结构。本文旨在分析这些非线性RNN的优势与局限，并提出新的改进模型。

**方法:** 基于闭环控制理论，提出了一种新的非线性RNN变体Comba，采用标量加低秩状态转移，并实现了硬件高效的块状并行内核。

**结果:** Comba通过状态反馈和输出反馈校正机制，在大规模语料库上训练后表现出色。

**结论:** Comba在语言和视觉建模中展示了其优越的性能和计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comba%3A+Improving+Nonlinear+RNNs+with+Closed-loop+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02475&send_immediately=true&force_search=false)

**原文摘要:** Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
resulting in a nonlinear recursive structure. In this paper, we first introduce
the concept of Nonlinear RNNs with a comprehensive analysis on the advantages
and limitations of these models. Then, based on closed-loop control theory, we
propose a novel Nonlinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates its superior performance and computation
efficiency in both language and vision modeling.

</details>


### [79] [Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization](https://arxiv.org/abs/2506.02504)
*Xingyu Chen, Bokun Wang, Ming Yang, Quanqi Hu, Qihang Lin, Tianbao Yang*

**主要类别:** cs.LG

**AI概要:** 本文研究了非光滑有限和耦合组合优化问题，提出了一种高效的随机动量方法，将迭代复杂度降低到$O(1/\epsilon^5)$，并成功应用于多不等式约束下的非凸优化问题，取得了理论与实验上的突破。


<details>
  <summary>更多</summary>
  
**动机:** 现有的最先进方法存在两个关键限制：一是假设随机内函数期望下Lipschitz连续时具有较高的迭代复杂度$O(1/\epsilon^6)$；二是依赖于传统的SGD型更新，难以应用于深度学习场景。因此，需要开发一种更加高效且适合实际应用的新算法。

**方法:** 论文设计了针对非光滑FCCO问题的随机动量方法，并通过理论分析证明了其收敛性；此外，通过使用平滑铰链惩罚函数方法处理具有函数不等式约束的非凸优化问题，实现了新的最优迭代复杂度。

**结果:** 提出了一种用于非光滑有限和耦合组合优化问题的随机动量方法，并达到了新的最优迭代复杂度$O(1/\epsilon^5)$；在涉及平滑或弱凸函数不等式约束的多不等式约束非凸优化问题中，该方法能够以$O(1/\epsilon^5)$的复杂度找到（近似）$\epsilon$-水平的KKT解。

**结论:** 论文提出了适用于非光滑有限和耦合组合优化问题的随机动量方法，并在理论上取得了新的最优迭代复杂度$O(1/\epsilon^5)$的结果，同时展示了其在多个不等式约束下的非凸优化任务中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+Momentum+Methods+for+Non-smooth+Non-Convex+Finite-Sum+Coupled+Compositional+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02504，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02504&send_immediately=true&force_search=false)

**原文摘要:** Finite-sum Coupled Compositional Optimization (FCCO), characterized by its
coupled compositional objective structure, emerges as an important optimization
paradigm for addressing a wide range of machine learning problems. In this
paper, we focus on a challenging class of non-convex non-smooth FCCO, where the
outer functions are non-smooth weakly convex or convex and the inner functions
are smooth or weakly convex. Existing state-of-the-art result face two key
limitations: (1) a high iteration complexity of $O(1/\epsilon^6)$ under the
assumption that the stochastic inner functions are Lipschitz continuous in
expectation; (2) reliance on vanilla SGD-type updates, which are not suitable
for deep learning applications. Our main contributions are two fold: (i) We
propose stochastic momentum methods tailored for non-smooth FCCO that come with
provable convergence guarantees; (ii) We establish a new state-of-the-art
iteration complexity of $O(1/\epsilon^5)$. Moreover, we apply our algorithms to
multiple inequality constrained non-convex optimization problems involving
smooth or weakly convex functional inequality constraints. By optimizing a
smoothed hinge penalty based formulation, we achieve a new state-of-the-art
complexity of $O(1/\epsilon^5)$ for finding an (nearly) $\epsilon$-level KKT
solution. Experiments on three tasks demonstrate the effectiveness of the
proposed algorithms.

</details>


### [80] [VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning](https://arxiv.org/abs/2506.02539)
*Thong Q. Nguyen, Shubhang Desai, Yash Jain, Tanvir Aumi, Vishal Chowdhary*

**主要类别:** cs.LG

**AI概要:** VerificAgent 是一种用于计算机使用代理的记忆管理框架，在减少错误学习的同时显著提高了任务成功率。


<details>
  <summary>更多</summary>
  
**动机:** 持续记忆增强可能导致错误或虚构的学习内容，影响代理在特定领域工作流程中的性能。因此需要一种有效的方法来管理记忆并减少错误学习。

**方法:** 通过三个步骤进行记忆管理：（1）专家策划的领域知识种子；（2）训练期间基于轨迹的迭代记忆优化；（3）部署前由人类专家进行事后事实核查。

**结果:** 在OSWorld生产力任务中，与基线计算机使用代理相比，VerificAgent 成功率相对提高了111.1%。

**结论:** VerificAgent 提供了一种有效的记忆管理框架，显著提高了计算机使用代理在生产力任务中的成功率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VerificAgent%3A+Integrating+Expert+Knowledge+and+Fact-Checked+Memory+for+Robust+Domain-Specific+Task+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02539，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02539&send_immediately=true&force_search=false)

**原文摘要:** Continual memory augmentation allows computer-use agents (CUAs) to learn from
past interactions and refine their task-solving strategies over time. However,
unchecked memory accumulation can introduce spurious or hallucinated
"learnings" that degrade agent performance, particularly in domain-specific
workflows such as productivity software. We present a novel framework,
VerificAgent, that effectively manages memory for CUAs through (1) an
expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory
refinement during training, and (3) a post-hoc fact-checking pass by human
experts to sanitize accumulated memory before deployment. On OSWorld
productivity tasks, VerificAgent achieves a 111.1% relative improvement in
success rate over baseline CUA without any additional fine-tuning.

</details>


### [81] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)
*Minsung Kim, Nakyeong Yang, Kyomin Jung*

**主要类别:** cs.LG

**AI概要:** 论文提出PUBG方法，解决大视觉-语言模型在遗忘训练数据时产生的不良响应问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的遗忘方法无法很好地选择替代输出，导致生成质量差、信息量少或拒绝回答的问题。

**方法:** 引入了一个新的LVLMs的遗忘任务，并提出了PUBG方法来引导模型生成理想的输出分布。

**结果:** 实验表明，PUBG在防止隐私泄露的同时显著改善了模型响应的质量。

**结论:** PUBG方法能够有效缓解遗忘后的不良行为，同时避免隐私泄露。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Post-Unlearning+Behavior+of+Large+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02541，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02541&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning is used to mitigate the privacy risks of Large
Vision-Language Models (LVLMs) arising from training on large-scale web data.
However, existing unlearning methods often fail to carefully select substitute
outputs for forget targets, resulting in Unlearning Aftermaths-undesirable
behaviors such as degenerate, hallucinated, or excessively refused responses.
We highlight that, especially for generative LVLMs, it is crucial to consider
the quality and informativeness of post-unlearning responses rather than
relying solely on naive suppression. To address this, we introduce a new
unlearning task for LVLMs that requires models to provide privacy-preserving
yet informative and visually grounded responses. We also propose PUBG, a novel
unlearning method that explicitly guides post-unlearning behavior toward a
desirable output distribution. Experiments show that, while existing methods
suffer from Unlearning Aftermaths despite successfully preventing privacy
violations, PUBG effectively mitigates these issues, generating visually
grounded and informative responses without privacy leakage for forgotten
targets.

</details>


### [82] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/abs/2506.02542)
*Niklas Kormann, Masoud Ramuz, Zeeshan Nisar, Nadine S. Schaadt, Hendrik Annuth, Benjamin Doerr, Friedrich Feuerhake, Thomas Lampert, Johannes F. Lutzeyer*

**主要类别:** cs.LG

**AI概要:** 本文提出了用于肾小球分类的新型异构图神经网络HIEGNet，通过整合免疫细胞信息提升模型性能，并在实际数据上验证了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 图神经网络(GNNs)在组织病理学中表现出色，但在肾小球健康分类这一重要任务上尚未广泛探索，尤其是图构建存在挑战。

**方法:** 提出了一种结合传统和基于机器学习的计算机视觉技术的管道来构建异构图，并设计了HIEGNet模型以整合肾小球及其周围的免疫细胞信息。

**结果:** HIEGNet在多个基线模型中表现最优，在肾移植患者的全切片图像数据集上实现了最好的跨患者泛化能力。

**结论:** HIEGNet是一种新型的异构图神经网络架构，能够有效分类肾小球健康状况，并且在患者间表现出最佳泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HIEGNet%3A+A+Heterogenous+Graph+Neural+Network+Including+the+Immune+Environment+in+Glomeruli+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02542&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have recently been found to excel in
histopathology. However, an important histopathological task, where GNNs have
not been extensively explored, is the classification of glomeruli health as an
important indicator in nephropathology. This task presents unique difficulties,
particularly for the graph construction, i.e., the identification of nodes,
edges, and informative features. In this work, we propose a pipeline composed
of different traditional and machine learning-based computer vision techniques
to identify nodes, edges, and their corresponding features to form a
heterogeneous graph. We then proceed to propose a novel heterogeneous GNN
architecture for glomeruli classification, called HIEGNet, that integrates both
glomeruli and their surrounding immune cells. Hence, HIEGNet is able to
consider the immune environment of each glomerulus in its classification. Our
HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney
transplant patients. Experimental results demonstrate that HIEGNet outperforms
several baseline models and generalises best between patients among all
baseline models. Our implementation is publicly available at
https://github.com/nklsKrmnn/HIEGNet.git.

</details>


### [83] [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
*Shenghua He, Tian Xia, Xuan Zhou, Hui Wei*

**主要类别:** cs.LG

**AI概要:** 本文研究了大型语言模型强化学习中的零奖励假设问题，提出了轨迹策略梯度定理，并介绍了一种新的算法TRePO。


<details>
  <summary>更多</summary>
  
**动机:** 解决非终端动作缺乏即时奖励的问题，以便更有效地进行大型语言模型的微调。

**方法:** 引入轨迹策略梯度定理，提出Token-Reinforced Policy Optimization (TRePO) 算法。

**结果:** 证明响应级奖励方法可以有效建模token级奖励信号，并展示了TRePO算法的优势。

**结论:** TRePO提供了一种理论基础扎实且实用性强的新方法，有助于提高大型语言模型的训练效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Response-Level+Rewards+Are+All+You+Need+for+Online+Reinforcement+Learning+in+LLMs%3A+A+Mathematical+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02553，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02553&send_immediately=true&force_search=false)

**原文摘要:** We study a common challenge in reinforcement learning for large language
models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,
intermediate token generations) receive zero task-specific immediate reward,
while only the final token receives a reward for the entire response. This
assumption arises frequently in practice, as precise token-level rewards are
often difficult or infeasible to obtain in LLM applications. In this work, we
provide a unifying theoretical perspective. We introduce the Trajectory Policy
Gradient Theorem, which shows that the policy gradient based on true, unknown
token-level rewards can be unbiasedly estimated using only a response-level
reward model, regardless of whether the Zero-Reward Assumption holds or not,
for algorithms in the REINFORCE and Actor-Critic families. This result reveals
that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess
the capacity to model token-level reward signals, offering a theoretical
justification for response-level reward approaches. Our findings pave the way
for more practical, efficient LLM fine-tuning, allowing developers to treat
training algorithms as black boxes and focus on improving the response-level
reward model with auxiliary sub-models. We also offer a detailed analysis of
popular RL and non-RL methods, comparing their theoretical foundations and
practical advantages across common LLM tasks. Finally, we propose a new
algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically
grounded method that is simpler than PPO, matches GRPO in memory efficiency,
and holds promise for broad applicability.

</details>


### [84] [Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation](https://arxiv.org/abs/2506.02563)
*Roie Reshef, Kfir Yehuda Levy*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种在部分参与情况下实现联邦学习差分隐私的新方法，通过一种新颖的噪声消除机制来保持隐私同时不牺牲性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是为了解决在部分参与情况下实现差分隐私的挑战，因为在这种情况下，之前在全参与设置中表现最优的方法难以扩展。

**方法:** 该方法是在随机凸优化框架内进行分析的，并采用了能够保留隐私而不影响收敛率或计算效率的新颖噪声消除机制。

**结果:** 结果显示，这种方法在同质和异质数据分布下均能提供最佳性能，并为分布式系统中的隐私保护学习提供了高效且实用的解决方案。

**结论:** 该论文得出的结论是，他们提出的方法通过引入一种新颖的噪声消除机制，在联邦学习中实现了差分隐私，并且在部分参与的情况下保持了收敛速度和计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Preserving+Federated+Convex+Optimization%3A+Balancing+Partial-Participation+and+Efficiency+via+Noise+Cancellation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02563，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02563&send_immediately=true&force_search=false)

**原文摘要:** This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.

</details>


### [85] [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/abs/2506.02572)
*Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为HATA的新颖Top-k注意力机制，通过集成低开销的哈希学习技术，在保持准确性的同时显著提高了大型语言模型（LLM）的推理速度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Top-k注意力机制难以在效率和准确性之间取得平衡，因此需要一种更有效的解决方案。

**方法:** 将查询和键映射为二进制哈希码，以低成本获取相对qk分数顺序，用于实现Top-k注意力。

**结果:** 实验表明，与普通全注意力相比，HATA实现了最高7.2倍的速度提升，并且在多个主流LLM模型和不同任务中均表现出色。

**结论:** HATA是一种新的Top-k注意力机制，通过低开销的哈希学习技术显著加速LLM推理，同时保持准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HATA%3A+Trainable+and+Hardware-Efficient+Hash-Aware+Top-k+Attention+for+Scalable+Large+Model+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02572，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02572&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

</details>


### [86] [Reachability Weighted Offline Goal-conditioned Resampling](https://arxiv.org/abs/2506.02577)
*Wenyan Yang, Joni Pajarinen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的采样方法Reachability Weighted Sampling (RWS)，用于离线目标条件强化学习，通过利用可达性得分优化采样过程，显著提升策略性能。


<details>
  <summary>更多</summary>
  
**动机:** 离线目标条件强化学习通常依赖均匀采样方法（如Q-learning），但这种方法需要庞大的数据集且会生成许多无法实现的状态-目标-动作组合，从而降低策略性能。因此，提出RWS以解决这些问题。

**方法:** 提出了一种名为Reachability Weighted Sampling (RWS)的方法，通过使用基于正样本-未标记样本(PU)学习训练的可达性分类器，将目标条件状态-动作值映射为可达性得分，并将其用作采样优先级。

**结果:** 在六个复杂的模拟机器人操作任务上的实验表明，RWS显著提高了性能，其中在一个HandBlock-Z任务中相对基线提高了近50%的性能。

**结论:** RWS是一种可与标准离线RL算法无缝集成的插件模块，实验表明其在多个复杂机器人操作任务中显著提升了性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reachability+Weighted+Offline+Goal-conditioned+Resampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02577，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02577&send_immediately=true&force_search=false)

**原文摘要:** Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets
where many potential goals share the same state and action spaces. However,
these potential goals are not explicitly represented in the collected
trajectories. To learn a generalizable goal-conditioned policy, it is common to
sample goals and state-action pairs uniformly using dynamic programming methods
such as Q-learning. Uniform sampling, however, requires an intractably large
dataset to cover all possible combinations and creates many unreachable
state-goal-action pairs that degrade policy performance. Our key insight is
that sampling should favor transitions that enable goal achievement. To this
end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability
classifier trained via positive-unlabeled (PU) learning on goal-conditioned
state-action values. The classifier maps these values to a reachability score,
which is then used as a sampling priority. RWS is a plug-and-play module that
integrates seamlessly with standard offline RL algorithms. Experiments on six
complex simulated robotic manipulation tasks, including those with a robot arm
and a dexterous hand, show that RWS significantly improves performance. In one
notable case, performance on the HandBlock-Z task improved by nearly 50 percent
relative to the baseline. These results indicate the effectiveness of
reachability-weighted sampling.

</details>


### [87] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/abs/2506.02599)
*Niklas Roßberg, Marion Neumeier, Sinan Hasirlioglu, Mohamed Essayed Bouzouraa, Michael Botsch*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于高速公路交通场景聚类和完整性分析的新方法，使用CVQ-VAE模型提升了聚类效果，并评估了场景类别数量对完整性的影响。


<details>
  <summary>更多</summary>
  
**动机:** 自动驾驶系统（ADS）需要在日益复杂的交通环境中安全运行，因此需要精确理解交通场景，以确保ADS功能的安全发布。

**方法:** 采用Clustering Vector Quantized-Variational Autoencoder (CVQ-VAE) 对高速公路交通场景进行聚类，并利用生成的目录分析不同类别数量下的完整性权衡。

**结果:** 该方法在highD数据集上表现出优越的聚类性能，并揭示了聚类质量与维持完整性所需数据量之间的权衡关系。

**结论:** 本文提出了一种基于CVQ-VAE的交通场景聚类管道，并分析了场景类别数量对完整性的影响，结果表明其聚类性能优于以往方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+the+Completeness+of+Traffic+Scenario+Categories+for+Automated+Highway+Driving+Functions+via+Cluster-based+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02599&send_immediately=true&force_search=false)

**原文摘要:** The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [88] [Compositional Learning for Modular Multi-Agent Self-Organizing Networks](https://arxiv.org/abs/2506.02616)
*Qi Liao, Parijat Bhattacharjee*

**主要类别:** cs.LG

**AI概要:** 本研究提出了CDRL和CPDM两种组合学习方法，在自组织网络中实现了性能提升，包括更好的可扩展性、更快的收敛速度和更高的安全性。


<details>
  <summary>更多</summary>
  
**动机:** 自组织网络面临复杂参数相互依赖性和冲突目标的挑战，传统多智能体深度强化学习方法难以解决这些问题。

**方法:** 引入了两种组合学习方法（CDRL和CPDM），并采用模块化双层框架管理异构代理粒度以降低模型复杂性。

**结果:** 数值模拟显示手over失败显著减少，并且吞吐量和延迟得到改善。

**结论:** CDRL和CPDM方法在大规模自组织网络中表现出优越的性能，包括更快的收敛速度、更高的样本效率以及更安全的训练过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compositional+Learning+for+Modular+Multi-Agent+Self-Organizing+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02616&send_immediately=true&force_search=false)

**原文摘要:** Self-organizing networks face challenges from complex parameter
interdependencies and conflicting objectives. This study introduces two
compositional learning approaches-Compositional Deep Reinforcement Learning
(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their
performance under training time and safety constraints in multi-agent systems.
We propose a modular, two-tier framework with cell-level and cell-pair-level
agents to manage heterogeneous agent granularities while reducing model
complexity. Numerical simulations reveal a significant reduction in handover
failures, along with improved throughput and latency, outperforming
conventional multi-agent deep reinforcement learning approaches. The approach
also demonstrates superior scalability, faster convergence, higher sample
efficiency, and safer training in large-scale self-organizing networks.

</details>


### [89] [HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport](https://arxiv.org/abs/2506.02619)
*Yanbei Liu, Chongxu Wang, Zhitao Xiao, Lei Geng, Yanwei Pang, Xiao Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的异构图自监督学习框架HGOT，利用最优传输机制简化样本选择并提升模型性能，在多个任务上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 传统基于对比学习的异构图自监督方法需要精心设计的图增强策略和复杂的正负样本选择，而这些过程繁琐且难以准确衡量样本间的相似性。

**方法:** HGOT通过引入最优传输机制来替代传统的正负样本选择过程，并设计了一个中心视图来整合不同元路径表示的语义信息，从而对齐分支视图与中心视图之间的语义关系。

**结果:** 在四个真实世界数据集上的大量实验表明，HGOT在各种下游任务中均达到最先进的性能，在节点分类任务中平均准确率提升了超过6%。

**结论:** 论文提出了一种新的基于最优传输的异构图神经网络自监督学习方法HGOT，该方法无需图增强策略即可实现高效的自监督学习，并在多个实际数据集上验证了其优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HGOT%3A+Self-supervised+Heterogeneous+Graph+Neural+Network+with+Optimal+Transport，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02619&send_immediately=true&force_search=false)

**原文摘要:** Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent
capabilities in processing heterogeneous information networks. Self-supervised
learning on heterogeneous graphs, especially contrastive self-supervised
strategy, shows great potential when there are no labels. However, this
approach requires the use of carefully designed graph augmentation strategies
and the selection of positive and negative samples. Determining the exact level
of similarity between sample pairs is non-trivial.To solve this problem, we
propose a novel self-supervised Heterogeneous graph neural network with Optimal
Transport (HGOT) method which is designed to facilitate self-supervised
learning for heterogeneous graphs without graph augmentation strategies.
Different from traditional contrastive self-supervised learning, HGOT employs
the optimal transport mechanism to relieve the laborious sampling process of
positive and negative samples. Specifically, we design an aggregating view
(central view) to integrate the semantic information contained in the views
represented by different meta-paths (branch views). Then, we introduce an
optimal transport plan to identify the transport relationship between the
semantics contained in the branch view and the central view. This allows the
optimal transport plan between graphs to align with the representations,
forcing the encoder to learn node representations that are more similar to the
graph space and of higher quality. Extensive experiments on four real-world
datasets demonstrate that our proposed HGOT model can achieve state-of-the-art
performance on various downstream tasks. In particular, in the node
classification task, HGOT achieves an average of more than 6% improvement in
accuracy compared with state-of-the-art methods.

</details>


### [90] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/abs/2506.02623)
*Yuyang Zhou, Ferrante Neri, Yew-Soon Ong, Ruibin Bai*

**主要类别:** cs.LG

**AI概要:** SiamNAS通过使用Siamese网络作为代理模型，大幅降低了神经架构搜索的计算成本，并成功找到了多目标优化下的Pareto最优解。


<details>
  <summary>更多</summary>
  
**动机:** 为了应对现代神经架构搜索（NAS）中平衡准确性和计算成本等多目标所带来的高计算成本问题。

**方法:** 提出了一种新的代理模型方法，利用Siamese网络块的集成来预测候选架构之间的支配关系。

**结果:** 实验表明，SiamNAS能够在NAS-Bench-201基准测试中找到CIFAR-10和ImageNet数据集上的近似最优解，并且仅需0.01个GPU天数。

**结论:** SiamNAS能够显著降低计算成本，同时识别Pareto最优解，并展示了其在多任务优化中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SiamNAS%3A+Siamese+Surrogate+Model+for+Dominance+Relation+Prediction+in+Multi-objective+Neural+Architecture+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02623，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02623&send_immediately=true&force_search=false)

**原文摘要:** Modern neural architecture search (NAS) is inherently multi-objective,
balancing trade-offs such as accuracy, parameter count, and computational cost.
This complexity makes NAS computationally expensive and nearly impossible to
solve without efficient approximations. To address this, we propose a novel
surrogate modelling approach that leverages an ensemble of Siamese network
blocks to predict dominance relationships between candidate architectures.
Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces
the crowding distance calculation in the survivor selection strategy with a
heuristic rule based on model size. Integrated into a framework termed SiamNAS,
this design eliminates costly evaluations during the search process.
Experiments on NAS-Bench-201 demonstrate the framework's ability to identify
Pareto-optimal solutions with significantly reduced computational costs. The
proposed SiamNAS identified a final non-dominated set containing the best
architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in
terms of test error rate, within 0.01 GPU days. This proof-of-concept study
highlights the potential of the proposed Siamese network surrogate model to
generalise to multi-tasking optimisation, enabling simultaneous optimisation
across tasks. Additionally, it offers opportunities to extend the approach for
generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal
solutions for heterogeneous task settings.

</details>


### [91] [HAM: A Hyperbolic Step to Regulate Implicit Bias](https://arxiv.org/abs/2506.02630)
*Tom Jacobs, Advait Gadhikar, Celia Rubio-Madrigal, Rebekka Burkholz*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为HAM的优化算法，通过引入双曲镜像步骤来改善深度学习模型的隐式偏差和收敛速度，在多种任务中显示出优越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决由过参数化引起的双曲隐式偏差导致的有效学习率降低问题，需要一种能够保持良好双曲几何特性同时提升收敛速度的优化算法。

**方法:** 提出了交替进行优化器步骤和双曲镜像步骤的HAM算法，并通过与自然梯度下降的关联解释了其隐式偏差特性。

**结果:** 实验表明，HAM在各种任务中均表现出色，包括视觉、图和节点分类以及大型语言模型微调，并且即使在小批量情况下也能成功运行。

**结论:** HAM是一种新的优化算法，具有优异的隐式偏差特性，可以提高深度学习模型的性能，特别是在与不同的稀疏化方法结合使用时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HAM%3A+A+Hyperbolic+Step+to+Regulate+Implicit+Bias，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02630&send_immediately=true&force_search=false)

**原文摘要:** Understanding the implicit bias of optimization algorithms has become central
to explaining the generalization behavior of deep learning models. For
instance, the hyperbolic implicit bias induced by the overparameterization $m
\odot w$--though effective in promoting sparsity--can result in a small
effective learning rate, which slows down convergence. To overcome this
obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates
between an optimizer step and a new hyperbolic mirror step. We derive the
Riemannian gradient flow for its combination with gradient descent, leading to
improved convergence and a similar beneficial hyperbolic geometry as $m \odot
w$ for feature learning. We provide an interpretation of the the algorithm by
relating it to natural gradient descent, and an exact characterization of its
implicit bias for underdetermined linear regression. HAM's implicit bias
consistently boosts performance--even of dense training, as we demonstrate in
experiments across diverse tasks, including vision, graph and node
classification, and large language model fine-tuning. HAM is especially
effective in combination with different sparsification methods, improving upon
the state of the art. The hyperbolic step requires minimal computational and
memory overhead, it succeeds even with small batch sizes, and its
implementation integrates smoothly with existing optimizers.

</details>


### [92] [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](https://arxiv.org/abs/2506.02654)
*Shiyu Shen, Bin Pan, Guirong Xue*

**主要类别:** cs.LG

**AI概要:** 本文提出TrafficPPT，一种基于Transformer的预训练概率模型，用于城市级交通流量预测，解决了传统方法忽略不确定性及跨城市泛化能力不足的问题。


<details>
  <summary>更多</summary>
  
**动机:** 城市级交通流量预测面临观察数据不完整和有偏的挑战，现有方法大多生成确定性估计，忽视了未观测到的交通流带来的不确定性；同时，这些方法通常局限于特定城市，难以泛化到不同城市场景中。

**方法:** 引入了TrafficPPT，一个基于Transformer的预训练概率模型，融合了实时观测数据、历史轨迹数据以及道路网络拓扑，进行分布式的交通流量预测。

**结果:** 实验表明，TrafficPPT在多个真实数据集上均优于现有最先进的基线方法，尤其在极端数据稀疏条件下表现突出。

**结论:** TrafficPPT通过预训练和微调的方法，能够有效解决交通流量预测中的不确定性和数据偏差问题，并且在真实世界数据集中表现优异，尤其是在极端稀疏数据条件下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Pretrained+Probabilistic+Transformer+for+City-Scale+Traffic+Volume+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02654，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02654&send_immediately=true&force_search=false)

**原文摘要:** City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
sparsity. Code will be open.

</details>


### [93] [Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection](https://arxiv.org/abs/2506.02665)
*Tianci Liu, Tong Yang, Quan Zhang, Qi Lei*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的版权保护框架，利用可见且难去除的水印增强AI生成内容的安全性，解决了现有方法无法长期适应模型变化的问题。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI技术的进步，版权内容面临未经授权使用的风险。现有的基于不可见对抗扰动的方法只能提供短期安全，因为每当底层模型架构变化时都需要重新训练。因此，需要一种更稳健、长期的保护机制。

**方法:** 基于一种新的概率和逆问题公式化方法，最大化最佳重建与原始内容之间的差异，并开发了一个高效近似算法以避免难以处理的双层优化问题。

**结果:** 实验结果表明，所提出的方法在各种场景下均表现出优越性，有效实现了难去除的水印嵌入，提升了版权保护的鲁棒性。

**结论:** 本文提出了一种新的通用版权保护方法，通过在图像中嵌入可见且难以去除的水印来增强鲁棒性，为AI模型训练和直接滥用提供了长期的安全解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Invisibility%3A+Learning+Robust+Visible+Watermarks+for+Stronger+Copyright+Protection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02665，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02665&send_immediately=true&force_search=false)

**原文摘要:** As AI advances, copyrighted content faces growing risk of unauthorized use,
whether through model training or direct misuse. Building upon invisible
adversarial perturbation, recent works developed copyright protections against
specific AI techniques such as unauthorized personalization through DreamBooth
that are misused. However, these methods offer only short-term security, as
they require retraining whenever the underlying model architectures change. To
establish long-term protection aiming at better robustness, we go beyond
invisible perturbation, and propose a universal approach that embeds
\textit{visible} watermarks that are \textit{hard-to-remove} into images.
Grounded in a new probabilistic and inverse problem-based formulation, our
framework maximizes the discrepancy between the \textit{optimal} reconstruction
and the original content. We develop an effective and efficient approximation
algorithm to circumvent a intractable bi-level optimization. Experimental
results demonstrate superiority of our approach across diverse scenarios.

</details>


### [94] [XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation](https://arxiv.org/abs/2506.02694)
*Daichi Kimura, Tomonori Izumitani, Hisashi Kashima*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的注意力机制XicorAttention，利用Chatterjee秩相关系数测量非线性依赖关系，并通过SoftSort和SoftRank实现可微分计算。


<details>
  <summary>更多</summary>
  
**动机:** 现有的注意力机制可能无法充分捕捉时间序列数据中的固有非线性依赖关系，因此需要改进。

**方法:** 提出了一种基于Chatterjee秩相关系数的注意力机制，并使用SoftSort和SoftRank进行可微近似。

**结果:** 实验结果表明，与现有模型相比，所提方法在真实数据集上的预测准确率提高了约9.1%。

**结论:** 将非线性相关性集成到Transformer模型的注意力机制中，提高了时间序列预测的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是XicorAttention%3A+Time+Series+Transformer+Using+Attention+with+Nonlinear+Correlation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02694，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02694&send_immediately=true&force_search=false)

**原文摘要:** Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations sparse, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.

</details>


### [95] [Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies](https://arxiv.org/abs/2506.02703)
*Khizar Hayat, Baptiste Magnier*

**主要类别:** cs.LG

**AI概要:** 本文批判性地审视了信用卡欺诈检测研究中的方法论严谨性，揭示了基本评估缺陷如何掩盖算法复杂性的效果。


<details>
  <summary>更多</summary>
  
**动机:** 当前的信用欺诈检测研究可能存在根本性的评估缺陷，这促使本文进行深入分析。

**方法:** 通过故意采用不当评估协议进行实验，分析现有方法中的四个关键问题：数据泄漏、方法报告模糊、时间验证不足和指标操控。

**结果:** 研究表明，即使简单模型在违反基本方法原则时也可能取得误导性的出色结果，并展示了最小神经网络架构的案例。

**结论:** 正确的评估方法比模型复杂性更为重要，强调了方法论严谨性的必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Leakage+and+Deceptive+Performance%3A+A+Critical+Examination+of+Credit+Card+Fraud+Detection+Methodologies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02703&send_immediately=true&force_search=false)

**原文摘要:** This study critically examines the methodological rigor in credit card fraud
detection research, revealing how fundamental evaluation flaws can overshadow
algorithmic sophistication. Through deliberate experimentation with improper
evaluation protocols, we demonstrate that even simple models can achieve
deceptively impressive results when basic methodological principles are
violated. Our analysis identifies four critical issues plaguing current
approaches: (1) pervasive data leakage from improper preprocessing sequences,
(2) intentional vagueness in methodological reporting, (3) inadequate temporal
validation for transaction data, and (4) metric manipulation through recall
optimization at precision's expense. We present a case study showing how a
minimal neural network architecture with data leakage outperforms many
sophisticated methods reported in literature, achieving 99.9\% recall despite
fundamental evaluation flaws. These findings underscore that proper evaluation
methodology matters more than model complexity in fraud detection research. The
study serves as a cautionary example of how methodological rigor must precede
architectural sophistication, with implications for improving research
practices across machine learning applications.

</details>


### [96] [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.02718)
*Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的多智能体强化学习算法MHGPO，用于提高大型语言模型应用中的稳定性与计算效率，并通过实验证明其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有MARL算法如MAPPO依赖Critic网络导致训练不稳定和计算负担增加的问题，并优化多智能体LLM搜索系统(MASS)。

**方法:** 提出了一种名为Multi-Agent Heterogeneous Group Policy Optimization (MHGPO) 的新算法，并引入了三种组rollout采样策略。

**结果:** 实验显示，MHGPO在任务表现和计算效率方面均优于MAPPO，并且无需预热。

**结论:** MHGPO作为一种无Critic网络的算法，具有更高的稳定性和计算效率，适用于复杂的基于LLM的MAS优化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heterogeneous+Group-Based+Reinforcement+Learning+for+LLM-based+Multi-Agent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02718，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02718&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.

</details>


### [97] [WeightLoRA: Keep Only Necessary Adapters](https://arxiv.org/abs/2506.02724)
*Andrey Veprikov, Vladimir Solodkin, Alexander Zyl, Andrey Savchenko, Aleksandr Beznosikov*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种名为WeightLoRA的新方法，该方法能够在保持获得一致甚至更优度量值能力的同时显著减少可训练参数的数量。


<details>
  <summary>更多</summary>
  
**动机:** Parameter-Efficient Fine-Tuning技术（如LoRA）虽然可以获得准确的解决方案，但需要大量的内存来训练大型模型并需要对添加适配器的层数有所直觉。

**方法:** 作者提出了WeightLoRA方法，并对其在一系列具有竞争力的基准测试以及DeBERTa、BART和Llama模型上的表现进行了实验。

**结果:** 实验结果表明，与不同的自适应方法相比，WeightLoRA的有效性以及WeightLoRA+在几乎所有情况下的优越性能。

**结论:** 本文提出了一种新的方法WeightLoRA，通过在优化过程中自适应选择最重要的LoRA头来解决内存需求大和需要直觉选择层的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WeightLoRA%3A+Keep+Only+Necessary+Adapters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02724&send_immediately=true&force_search=false)

**原文摘要:** The widespread utilization of language models in modern applications is
inconceivable without Parameter-Efficient Fine-Tuning techniques, such as
low-rank adaptation ($\texttt{LoRA}$), which adds trainable adapters to
selected layers. Although $\texttt{LoRA}$ may obtain accurate solutions, it
requires significant memory to train large models and intuition on which layers
to add adapters. In this paper, we propose a novel method,
$\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the
most critical $\texttt{LoRA}$ heads throughout the optimization process. As a
result, we can significantly reduce the number of trainable parameters while
maintaining the capability to obtain consistent or even superior metric values.
We conduct experiments for a series of competitive benchmarks and DeBERTa,
BART, and Llama models, comparing our method with different adaptive
approaches. The experimental results demonstrate the efficacy of
$\texttt{WeightLoRA}$ and the superior performance of $\texttt{WeightLoRA+}$ in
almost all cases.

</details>


### [98] [Knowledge Graph Completion by Intermediate Variables Regularization](https://arxiv.org/abs/2506.02749)
*Changyi Xiao, Yixin Cao*

**主要类别:** cs.LG

**AI概要:** 本文提出了针对知识图谱补全任务中TDB模型的新正则化方法，通过优化中间变量的范数来减少过拟合，理论分析和实验结果均表明其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于张量分解（TDB）的模型在知识图谱补全任务中表现良好，但它们容易过拟合，而现有的正则化方法效果有限，因此需要一种更有效的正则化方法。

**方法:** 通过最小化预测张量的不同计算方式中涉及的中间变量的范数来实现正则化，并从理论上分析了其对降低张量迹范数的作用。

**结果:** 提出的正则化方法改善了TDB模型的过拟合问题，并在实验中表现出良好的有效性与可靠性。

**结论:** 论文提出了一种新的TDB模型正则化方法，该方法能够有效减少过拟合，并且适用于大多数TDB模型。实验验证了该方法的有效性和理论分析的可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Graph+Completion+by+Intermediate+Variables+Regularization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02749&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graph completion (KGC) can be framed as a 3-order binary tensor
completion task. Tensor decomposition-based (TDB) models have demonstrated
strong performance in KGC. In this paper, we provide a summary of existing TDB
models and derive a general form for them, serving as a foundation for further
exploration of TDB models. Despite the expressiveness of TDB models, they are
prone to overfitting. Existing regularization methods merely minimize the norms
of embeddings to regularize the model, leading to suboptimal performance.
Therefore, we propose a novel regularization method for TDB models that
addresses this limitation. The regularization is applicable to most TDB models
and ensures tractable computation. Our method minimizes the norms of
intermediate variables involved in the different ways of computing the
predicted tensor. To support our regularization method, we provide a
theoretical analysis that proves its effect in promoting low trace norm of the
predicted tensor to reduce overfitting. Finally, we conduct experiments to
verify the effectiveness of our regularization technique as well as the
reliability of our theoretical analysis. The code is available at
https://github.com/changyi7231/IVR.

</details>


### [99] [Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection](https://arxiv.org/abs/2506.02757)
*Ruiying Lu, Jinhan Liu, Chuan Du, Dandan Guo*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的表格异常检测方法，通过引入掩码建模和原型学习来解决现有方法中的表示纠缠和缺乏全局相关性建模的问题，提升了检测性能和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于深度学习的表格异常检测方法存在表示纠缠和缺乏全局相关性建模的问题，这限制了其检测性能。因此，作者希望设计一种能够同时解决这两个问题的新方法，从而提升异常检测效果。

**方法:** 该方法主要包括两个部分：编码阶段利用正交基向量在数据空间和投影空间进行掩码建模，以学习共享的解耦正常模式；解码阶段并行解码多个掩码表示，并学习关联原型以提取正常特征相关性。此外，从分布匹配的角度出发，将投影空间学习和关联原型学习形式化为最优传输问题。

**结果:** 作者在20个表格基准数据集上进行了定量和定性实验，结果表明他们提出的方法在异常检测任务中表现出色，不仅具有较高的检测精度，还具备良好的可解释性。

**结论:** 论文提出了一种结合掩码建模和原型学习的新方法，用于表格异常检测。该方法通过解耦表示学习和全局相关性建模，提高了异常检测的性能，并在20个表格基准测试中验证了其有效性与可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Investigating+Mask-aware+Prototype+Learning+for+Tabular+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02757，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02757&send_immediately=true&force_search=false)

**原文摘要:** Tabular anomaly detection, which aims at identifying deviant samples, has
been crucial in a variety of real-world applications, such as medical disease
identification, financial fraud detection, intrusion monitoring, etc. Although
recent deep learning-based methods have achieved competitive performances,
these methods suffer from representation entanglement and the lack of global
correlation modeling, which hinders anomaly detection performance. To tackle
the problem, we incorporate mask modeling and prototype learning into tabular
anomaly detection. The core idea is to design learnable masks by disentangled
representation learning within a projection space and extracting normal
dependencies as explicit global prototypes. Specifically, the overall model
involves two parts: (i) During encoding, we perform mask modeling in both the
data space and projection space with orthogonal basis vectors for learning
shared disentangled normal patterns; (ii) During decoding, we decode multiple
masked representations in parallel for reconstruction and learn association
prototypes to extract normal characteristic correlations. Our proposal derives
from a distribution-matching perspective, where both projection space learning
and association prototype learning are formulated as optimal transport
problems, and the calibration distances are utilized to refine the anomaly
scores. Quantitative and qualitative experiments on 20 tabular benchmarks
demonstrate the effectiveness and interpretability of our model.

</details>


### [100] [Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization](https://arxiv.org/abs/2506.02767)
*Marco Calì, Giulio Giacomuzzo, Ruggero Carli, Alberto Dalla Libera*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为EB-MC-PILCO的新方法，通过整合iLQR技术，显著提升了MC-PILCO的策略优化收敛速度，同时保持高成功率。


<details>
  <summary>更多</summary>
  
**动机:** 解决MC-PILCO算法在策略优化中收敛速度慢的问题。

**方法:** 将MC-PILCO与iLQR结合，提出了一种新的方法EB-MC-PILCO，利用iLQR生成信息丰富且具有探索性的轨迹，并用于策略初始化。

**结果:** 实验表明，在倒立摆任务中，EB-MC-PILCO相比标准MC-PILCO加速了收敛过程，执行时间最多减少了45.9%。

**结论:** EB-MC-PILCO在保持100%成功率的同时，显著提高了收敛速度，即使在MC-PILCO迭代次数较少的情况下也能更快完成任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+Model-Based+Reinforcement+Learning+using+Non-Linear+Trajectory+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02767，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02767&send_immediately=true&force_search=false)

**原文摘要:** This paper addresses the slow policy optimization convergence of Monte Carlo
Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art
model-based reinforcement learning (MBRL) algorithm, by integrating it with
iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization
method suitable for nonlinear systems. The proposed method, Exploration-Boosted
MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory
trajectories and initialize the policy, significantly reducing the number of
required optimization steps. Experiments on the cart-pole task demonstrate that
EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up
to $\bm{45.9\%}$ reduction in execution time when both methods solve the task
in four trials. EB-MC-PILCO also maintains a $\bm{100\%}$ success rate across
trials while solving the task faster, even in cases where MC-PILCO converges in
fewer iterations.

</details>


### [101] [CART-based Synthetic Tabular Data Generation for Imbalanced Regression](https://arxiv.org/abs/2506.02811)
*António Pedro Pinheiro, Rita P. Ribeiro*

**主要类别:** cs.LG

**AI概要:** 本研究提出一种针对不平衡回归问题的新颖CART-based合成数据生成方法，相较传统方法更高效、透明，并在基准数据集上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 在表格数据设置中，不平衡的目标分布会阻碍模型性能，而现有的分类技术应用于回归任务时通常依赖于明确的、人为的阈值，导致问题表述的随意性和误导性。此外，虽然GANs和VAEs等生成模型提供了灵活的样本合成，但其计算成本高且解释性有限。

**方法:** 适应现有的基于CART的合成数据生成方法，结合相关性和基于密度的机制，在目标空间的稀疏区域进行采样，并采用无阈值、特征驱动的生成过程。

**结果:** 实验结果显示，所提出的方法在预测极端目标值方面与其他重采样和生成策略具有竞争力，同时具备更快的执行速度和更高的透明度。

**结论:** 该研究提出了一种基于CART的合成数据生成方法，用于不平衡回归问题，具有较高的性能、更快的执行速度和更大的透明度，适用于目标分布不平衡的领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CART-based+Synthetic+Tabular+Data+Generation+for+Imbalanced+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02811，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02811&send_immediately=true&force_search=false)

**原文摘要:** Handling imbalanced target distributions in regression tasks remains a
significant challenge in tabular data settings where underrepresented regions
can hinder model performance. Among data-level solutions, some proposals, such
as random sampling and SMOTE-based approaches, propose adapting classification
techniques to regression tasks. However, these methods typically rely on crisp,
artificial thresholds over the target variable, a limitation inherited from
classification settings that can introduce arbitrariness, often leading to
non-intuitive and potentially misleading problem formulations. While recent
generative models, such as GANs and VAEs, provide flexible sample synthesis,
they come with high computational costs and limited interpretability. In this
study, we propose adapting an existing CART-based synthetic data generation
method, tailoring it for imbalanced regression. The new method integrates
relevance and density-based mechanisms to guide sampling in sparse regions of
the target space and employs a threshold-free, feature-driven generation
process. Our experimental study focuses on the prediction of extreme target
values across benchmark datasets. The results indicate that the proposed method
is competitive with other resampling and generative strategies in terms of
performance, while offering faster execution and greater transparency. These
results highlight the method's potential as a transparent, scalable data-level
strategy for improving regression models in imbalanced domains.

</details>


### [102] [Sheaves Reloaded: A Directional Awakening](https://arxiv.org/abs/2506.02842)
*Stefano Fiorini, Hakan Aktas, Iulia Duta, Stefano Coniglio, Pietro Morerio, Alessio Del Bue, Pietro Liò*

**主要类别:** cs.LG

**AI概要:** 本论文提出了有向层神经网络（DSNN），通过引入有向层拉普拉斯算子，将方向性纳入层神经网络（SNN）架构，从而在九个真实世界基准测试中始终优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管方向性已被证明能显著提升图学习任务的性能，并且是许多实际应用的关键，但现有的层神经网络（SNN）无法很好地表达这种方向性。因此，本文旨在解决这一局限性。

**方法:** 作者引入了有向胞腔层（Directed Cellular Sheaf），这是一种特殊类型的胞腔层，能够明确地考虑边的方向性。基于这一结构，他们定义了一个新的层拉普拉斯算子——有向层拉普拉斯算子（Directed Sheaf Laplacian），该算子捕捉了图的拓扑结构及其方向信息。

**结果:** 实验表明，在九个真实世界基准测试中，有向层神经网络（DSNN）始终优于基线方法。

**结论:** 有向层神经网络（DSNN）成功地将方向性嵌入到层神经网络的架构中，为建模复杂关系数据提供了更强的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sheaves+Reloaded%3A+A+Directional+Awakening，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02842，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02842&send_immediately=true&force_search=false)

**原文摘要:** Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph
Neural Networks (GNNs) that significantly improve our ability to model complex
relational data. While directionality has been shown to substantially boost
performance in graph learning tasks and is key to many real-world applications,
existing SNNs fall short in representing it. To address this limitation, we
introduce the Directed Cellular Sheaf, a special type of cellular sheaf
designed to explicitly account for edge orientation. Building on this
structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which
captures both the graph's topology and its directional information. This
operator serves as the backbone of the Directed Sheaf Neural Network (DSNN),
the first SNN model to embed a directional bias into its architecture.
Extensive experiments on nine real-world benchmarks show that DSNN consistently
outperforms baseline methods.

</details>


### [103] [BNPO: Beta Normalization Policy Optimization](https://arxiv.org/abs/2506.02864)
*Changyi Xiao, Mengdi Zhang, Yixin Cao*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的策略优化方法，称为Beta归一化策略优化（BNPO），它通过使用动态更新的Beta分布来对奖励进行自适应归一化，从而提高大语言模型的推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的策略优化方法忽视了奖励归一化或采用静态归一化策略，这不能适应训练期间策略更新的动态性。这可能导致梯度估计不稳定并阻碍训练稳定性。

**方法:** 提出了一种新的策略优化方法，称为Beta归一化策略优化（BNPO），使用具有动态更新参数的Beta分布来自适应地归一化奖励。

**结果:** 实验结果证实，与其他策略优化方法相比，BNPO在推理任务中实现了更稳定的训练动态和更低的方差梯度估计。

**结论:** BNPO在推理任务中的策略优化方法达到了最先进的性能，并且代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BNPO%3A+Beta+Normalization+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02864，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02864&send_immediately=true&force_search=false)

**原文摘要:** Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that
reinforcement learning with rule-based, binary-valued reward functions can
significantly enhance the reasoning capabilities of large language models.
These models primarily utilize REINFORCE-based policy optimization techniques,
such as REINFORCE with baseline and group relative policy optimization (GRPO).
However, a key limitation remains: current policy optimization methods either
neglect reward normalization or employ static normalization strategies, which
fail to adapt to the dynamic nature of policy updates during training. This may
result in unstable gradient estimates and hinder training stability. To address
this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel
policy optimization method that adaptively normalizes rewards using a Beta
distribution with dynamically updated parameters. BNPO aligns the normalization
with the changing policy distribution, enabling more precise and lower-variance
gradient estimation, which in turn promotes stable training dynamics. We
provide theoretical analysis demonstrating BNPO's variance-reducing properties
and show that it generalizes both REINFORCE and GRPO under binary-valued reward
settings. Furthermore, we introduce an advantage decomposition mechanism to
extend BNPO's applicability to more complex reward systems. Experimental
results confirm that BNPO achieves state-of-the-art performance among policy
optimization methods on reasoning tasks. The code is available at
https://github.com/changyi7231/BNPO.

</details>


### [104] [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.02883)
*Anthony Kobanda, Odalric-Ambrym Maillard, Rémy Portelas*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种针对持续强化学习的新基准，专注于视频游戏导航场景，以推动可重复研究并解决灾难性遗忘和任务适应性等挑战。


<details>
  <summary>更多</summary>
  
**动机:** 自主代理在机器人技术和视频游戏模拟等领域中必须适应不断变化的任务，同时不忘以前的任务。然而，目前在文献中存在一个空白，缺少能够捕捉关键挑战（如灾难性遗忘、任务适应性和内存效率）的基准测试。

**方法:** 基于最近的进展，作者引入了一套视频游戏导航场景作为基准测试，定义了各种任务和数据集、评估协议以及算法性能评估指标，包括最先进的基线。

**结果:** 开发了一个全面的基准测试工具，不仅促进了持续强化学习在游戏中的可重复研究，还为生产流程提供了可重复的框架，帮助从业者识别和应用有效的方法。

**结论:** 该论文介绍了一个新的基准，用于持续强化学习研究，特别是在游戏领域，旨在推动可重复的研究并加速这一领域的进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Continual+Offline+Reinforcement+Learning+Benchmark+for+Navigation+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02883&send_immediately=true&force_search=false)

**原文摘要:** Autonomous agents operating in domains such as robotics or video game
simulations must adapt to changing tasks without forgetting about the previous
ones. This process called Continual Reinforcement Learning poses non-trivial
difficulties, from preventing catastrophic forgetting to ensuring the
scalability of the approaches considered. Building on recent advances, we
introduce a benchmark providing a suite of video-game navigation scenarios,
thus filling a gap in the literature and capturing key challenges :
catastrophic forgetting, task adaptation, and memory efficiency. We define a
set of various tasks and datasets, evaluation protocols, and metrics to assess
the performance of algorithms, including state-of-the-art baselines. Our
benchmark is designed not only to foster reproducible research and to
accelerate progress in continual reinforcement learning for gaming, but also to
provide a reproducible framework for production pipelines -- helping
practitioners to identify and to apply effective approaches.

</details>


### [105] [Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review](https://arxiv.org/abs/2506.02887)
*Mrinmay Sen, Shruti Aparna, Rohit Agarwal, Chalavadi Krishna Mohan*

**主要类别:** cs.LG

**AI概要:** 该论文探讨了联邦学习中部分客户端参与所带来的挑战，并综述了应对这一现实问题的方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究大多关注于全客户端参与假设下的数据异构性带来的问题，而实际场景中的部分客户端参与所引发的挑战却鲜有关注。

**方法:** 论文采用了广泛的调查方法，结合理论见解和实证研究，对现有的联邦学习方法进行了系统分类和分析。

**结果:** 论文提供了针对部分客户端参与情况下的联邦学习方法的全面分析和结构化分类。

**结论:** 这篇论文总结了在联邦学习中部分客户端参与的影响，并提供了现有方法的深入分析和分类，强调了它们的优势和劣势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Overcoming+Challenges+of+Partial+Client+Participation+in+Federated+Learning+%3A+A+Comprehensive+Review，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02887&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) is a learning mechanism that falls under the
distributed training umbrella, which collaboratively trains a shared global
model without disclosing the raw data from different clients. This paper
presents an extensive survey on the impact of partial client participation in
federated learning. While much of the existing research focuses on addressing
issues such as generalization, robustness, and fairness caused by data
heterogeneity under the assumption of full client participation, limited
attention has been given to the practical and theoretical challenges arising
from partial client participation, which is common in real-world scenarios.
This survey provides an in-depth review of existing FL methods designed to cope
with partial client participation. We offer a comprehensive analysis supported
by theoretical insights and empirical findings, along with a structured
categorization of these methods, highlighting their respective advantages and
disadvantages.

</details>


### [106] [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)
*Jakub Krajewski, Marcin Chochowski, Daniel Korzekwa*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了细粒度MoE架构，证明其在大规模模型中比传统MoE配置有更好的性能和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 细粒度MoE方法在提高模型收敛性和质量方面显示出潜力，但需要对其扩展属性进行系统评估。

**方法:** 提出了一系列训练方法，并对细粒度MoE进行了全面的实证评估，直接比较了其与标准MoE配置的扩展属性。

**结果:** 研究调查了不同设置下的收敛速度、模型在下游基准测试中的表现以及实际训练考虑因素。

**结论:** 该研究得出结论，细粒度MoE在最大规模上实现了更好的验证损失和更高的下游基准测试准确率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Fine-Grained+MoE+Beyond+50B+Parameters%3A+Empirical+Evaluation+and+Practical+Insights，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02890，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02890&send_immediately=true&force_search=false)

**原文摘要:** Mixture of Experts (MoE) architectures have emerged as pivotal for scaling
Large Language Models (LLMs) efficiently. Fine-grained MoE approaches -
utilizing more numerous, smaller experts - have demonstrated potential in
improving model convergence and quality. This work proposes a set of training
recipes and provides a comprehensive empirical evaluation of fine-grained MoE,
directly comparing its scaling properties against standard MoE configurations
for models with up to 56B total (17B active) parameters. We investigate
convergence speed, model performance on downstream benchmarks, and practical
training considerations across various setups. Overall, at the largest scale we
show that fine-grained MoE achieves better validation loss and higher accuracy
across a set of downstream benchmarks. This study offers empirical grounding
and practical insights for leveraging fine-grained MoE in the development of
future large-scale models.

</details>


### [107] [Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning](https://arxiv.org/abs/2506.02897)
*Alessandro Licciardi, Roberta Raineri, Anton Proskurnikov, Lamberto Rondoni, Lorenzo Zino*

**主要类别:** cs.LG

**AI概要:** 本文研究了如何通过一种新的方差减少选择算法来解决联邦学习中的客户数据异质性问题，并验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的实际优势经常被客户数据异质性削弱，这严重降低了模型性能。

**方法:** 提出了一种名为Federated Coalition Variance Reduction with Boltzmann Exploration的方差减少选择算法。

**结果:** 实验表明，在异构场景下，该算法优于现有的联邦学习算法，能够产生更准确的结果和更快的收敛速度。

**结论:** 论文得出结论，通过采用受时间社交网络意见动态启发的方法，可以有效解决联邦学习中客户数据异质性的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sociodynamics-inspired+Adaptive+Coalition+and+Client+Selection+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02897，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02897&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables privacy-preserving collaborative model
training, yet its practical strength is often undermined by client data
heterogeneity, which severely degrades model performance. This paper proposes
that data heterogeneity across clients' distributions can be effectively
addressed by adopting an approach inspired by opinion dynamics over temporal
social networks. We introduce \shortname (Federated Coalition Variance
Reduction with Boltzmann Exploration), a variance-reducing selection algorithm
in which (1) clients dynamically organize into non-overlapping clusters based
on asymptotic agreements, and (2) from each cluster, one client is selected to
minimize the expected variance of its model update. Our experiments show that
in heterogeneous scenarios our algorithm outperforms existing FL algorithms,
yielding more accurate results and faster convergence, validating the efficacy
of our approach.

</details>


### [108] [MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver](https://arxiv.org/abs/2506.02935)
*Yuepeng Zheng, Fu Luo, Zhenkun Wang, Yaoxin Wu, Yu Zhou*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于知识蒸馏的多任务学习方法（MTL-KD），用于神经组合优化中的车辆路径规划问题，该方法通过将多个单任务模型的知识迁移至一个重解码器模型，提高了模型在不同任务上的泛化能力。此外，还引入了一种灵活的推理策略（R3C），进一步提升了性能，在最多包含1000个节点的6个已见和10个未见VRP变体上验证了所提方法的一致优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于强化学习的多任务方法只能在小规模问题上训练轻量级解码器模型，并且在解决大规模问题时泛化能力有限。因此，需要一种能够高效训练具有强泛化能力的重解码器模型的新方法。

**方法:** 本文提出了MTL-KD方法，它通过从多个不同的基于RL的单任务模型中向单一重解码器模型迁移策略知识来实现无标签训练。同时，引入了一种名为随机重排序重构（R3C）的灵活推理策略，以适应各种VRP任务并提升多任务模型的性能。

**结果:** 实验结果表明，所提出的MTL-KD方法在最多包含1000个节点的6个已见和10个未见VRP变体上均实现了优于现有方法的性能，并且在统一和真实世界基准测试中展示了强大的泛化能力。

**结论:** 所提出的MTL-KD方法结合R3C推理策略，有效解决了现有方法在大规模问题上的局限性，为多任务学习在神经组合优化领域的应用提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MTL-KD%3A+Multi-Task+Learning+Via+Knowledge+Distillation+for+Generalizable+Neural+Vehicle+Routing+Solver，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02935&send_immediately=true&force_search=false)

**原文摘要:** Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a
promising approach to train a unified model capable of solving multiple Vehicle
Routing Problem (VRP) variants. However, existing Reinforcement Learning
(RL)-based multi-task methods can only train light decoder models on
small-scale problems, exhibiting limited generalization ability when solving
large-scale problems. To overcome this limitation, this work introduces a novel
multi-task learning method driven by knowledge distillation (MTL-KD), which
enables the efficient training of heavy decoder models with strong
generalization ability. The proposed MTL-KD method transfers policy knowledge
from multiple distinct RL-based single-task models to a single heavy decoder
model, facilitating label-free training and effectively improving the model's
generalization ability across diverse tasks. In addition, we introduce a
flexible inference strategy termed Random Reordering Re-Construction (R3C),
which is specifically adapted for diverse VRP tasks and further boosts the
performance of the multi-task model. Experimental results on 6 seen and 10
unseen VRP variants with up to 1000 nodes indicate that our proposed method
consistently achieves superior performance on both uniform and real-world
benchmarks, demonstrating robust generalization abilities.

</details>


### [109] [Interaction Field Matching: Overcoming Limitations of Electrostatic Models](https://arxiv.org/abs/2506.02950)
*Stepan I. Manukhov, Alexander Kolesov, Vladimir V. Palyulin, Alexander Korotin*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的数据生成与传输方法 Interaction Field Matching (IFM)，其灵感来源于物理中的强相互作用，是 Electrostatic Field Matching (EFM) 方法的扩展和改进。


<details>
  <summary>更多</summary>
  
**动机:** EFM方法需要使用神经网络对复杂的电容器板外静电场进行建模，这存在一定的难度，因此提出了更通用的IFM方法。

**方法:** 受到物理学中夸克和反夸克强相互作用的启发，设计了一种特定的交互场实现方式，以改进EFM方法。

**结果:** 该论文展示了IFM方法在一系列玩具问题和图像数据传输任务中的性能表现。

**结论:** Interaction Field Matching (IFM) 是一种比Electrostatic Field Matching (EFM) 更通用的方法，可以应用于更广泛的交互场，并解决了EFM在建模静电场时出现的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interaction+Field+Matching%3A+Overcoming+Limitations+of+Electrostatic+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02950&send_immediately=true&force_search=false)

**原文摘要:** Electrostatic field matching (EFM) has recently appeared as a novel
physics-inspired paradigm for data generation and transfer using the idea of an
electric capacitor. However, it requires modeling electrostatic fields using
neural networks, which is non-trivial because of the necessity to take into
account the complex field outside the capacitor plates. In this paper, we
propose Interaction Field Matching (IFM), a generalization of EFM which allows
using general interaction fields beyond the electrostatic one. Furthermore,
inspired by strong interactions between quarks and antiquarks in physics, we
design a particular interaction field realization which solves the problems
which arise when modeling electrostatic fields in EFM. We show the performance
on a series of toy and image data transfer problems.

</details>


### [110] [QKV Projections Require a Fraction of Their Memory](https://arxiv.org/abs/2506.02939)
*Malik Khalf, Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Point-Approximate Matrix Multiplication (PAMM)的新方法，有效减少了大型语言模型训练过程中注意力机制中的内存消耗，同时保持了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究大多关注于近似缩放点积，而忽视了计算Q, K, V张量的线性投影的内存消耗，为此，作者提出了一种新的方法来解决这个问题。

**方法:** 提出了Point-Approximate Matrix Multiplication (PAMM)，一种新的张量压缩技术，用于减少注意力层中Q, K, V投影的内存消耗。

**结果:** PAMM最多可以将Q, K, V投影的内存消耗减少512倍，并且能够达到相似或更好的最终困惑度。

**结论:** PAMM是一个实用且互补的方法，可以与高效的注意力技术如FlashAttention结合使用，从而实现内存高效的LLM训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QKV+Projections+Require+a+Fraction+of+Their+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02939，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02939&send_immediately=true&force_search=false)

**原文摘要:** The Multi-Head Attention mechanism is central to LLM operation, and multiple
works target its compute and memory efficiency during training. While most
works focus on approximating the scaled dot product, the memory consumption of
the linear projections that compute the $Q$, $K$, and $V$ tensors from the
input $x$ is often overlooked. To address this, we propose Point-Approximate
Matrix Multiplication (PAMM), a novel tensor compression technique that reduces
memory consumption of the $Q,K,V$ projections in attention layers by a factor
of up to $\times 512$, effectively erasing their memory footprint, while
achieving similar or better final perplexity. PAMM is fully composable with
efficient attention techniques such as FlashAttention, making it a practical
and complementary method for memory-efficient LLM training.

</details>


### [111] [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](https://arxiv.org/abs/2506.03077)
*Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li*

**主要类别:** cs.LG

**AI概要:** StreamBP 是一种高效且精确的反向传播方法，用于降低长序列语言模型训练中的内存消耗和加速计算。


<details>
  <summary>更多</summary>
  
**动机:** 随着序列长度增加，传统反向传播过程中的内存消耗变得非常高，因此需要一种更加高效的训练方法。

**方法:** StreamBP 通过在线性链规则中进行分解来减少内存消耗，并利用语言模型的因果结构实现更高效的计算。

**结果:** StreamBP 能够显著提升最大序列长度达 2.8-5.5 倍，同时保持与梯度检查点技术相当或更低的反向传播时间。

**结论:** StreamBP 是一种高效的反向传播方法，适用于长序列训练，具有良好的扩展性和实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StreamBP%3A+Memory-Efficient+Exact+Backpropagation+for+Long+Sequence+Training+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03077&send_immediately=true&force_search=false)

**原文摘要:** Training language models on long sequence data is a demanding requirement for
enhancing the model's capability on complex tasks, e.g., long-chain reasoning.
However, as the sequence length scales up, the memory cost for storing
activation values becomes huge during the Backpropagation (BP) process, even
with the application of gradient checkpointing technique. To tackle this
challenge, we propose a memory-efficient and exact BP method called StreamBP,
which performs a linear decomposition of the chain rule along the sequence
dimension in a layer-wise manner, significantly reducing the memory cost of
activation values and logits. The proposed method is applicable to common
objectives such as SFT, GRPO, and DPO. From an implementation perspective,
StreamBP achieves less computational FLOPs and faster BP speed by leveraging
the causal structure of the language model. Compared to gradient checkpointing,
StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,
while using comparable or even less BP time. Note that StreamBP's sequence
length scaling ability can be directly transferred to batch size scaling for
accelerating training. We further develop a communication-efficient distributed
StreamBP to effectively support multi-GPU training and broaden its
applicability. Our code can be easily integrated into the training pipeline of
any transformer models and is available at https://github.com/Ledzy/StreamBP.

</details>


### [112] [Abstract Counterfactuals for Language Model Agents](https://arxiv.org/abs/2506.02946)
*Edoardo Pona, Milad Kazemi, Yali Du, David Watson, Nicola Paoletti*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种针对语言模型代理的新型反事实推理框架Abstract Counterfactuals，通过关注高层特征解决传统token级别方法的问题，并在实验中展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 由于语言模型代理的动作空间通常是隐式的且难以定义，传统的token级别反事实方法不足以满足需求，因此需要一种更高级、用户相关特征导向的反事实推理方法。

**方法:** 提出了一种名为Abstract Counterfactuals的新框架，该框架强调动作和交互的高层特征，并在文本游戏和反事实文本生成任务上进行了实验，比较了token级别和潜在空间干预的效果。

**结果:** 实验表明，Abstract Counterfactuals框架能够生成一致且有意义的反事实，并最大程度减少token级别方法的不良副作用。

**结论:** 论文得出结论，通过引入抽象反事实框架，可以更好地进行高层次的反事实推理，从而克服传统基于token级别的反事实方法存在的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Abstract+Counterfactuals+for+Language+Model+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02946&send_immediately=true&force_search=false)

**原文摘要:** Counterfactual inference is a powerful tool for analysing and evaluating
autonomous agents, but its application to language model (LM) agents remains
challenging. Existing work on counterfactuals in LMs has primarily focused on
token-level counterfactuals, which are often inadequate for LM agents due to
their open-ended action spaces. Unlike traditional agents with fixed, clearly
defined action spaces, the actions of LM agents are often implicit in the
strings they output, making their action spaces difficult to define and
interpret. Furthermore, the meanings of individual tokens can shift depending
on the context, adding complexity to token-level reasoning and sometimes
leading to biased or meaningless counterfactuals. We introduce \emph{Abstract
Counterfactuals}, a framework that emphasises high-level characteristics of
actions and interactions within an environment, enabling counterfactual
reasoning tailored to user-relevant features. Our experiments demonstrate that
the approach produces consistent and meaningful counterfactuals while
minimising the undesired side effects of token-level methods. We conduct
experiments on text-based games and counterfactual text generation, while
considering both token-level and latent-space interventions.

</details>


### [113] [How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment](https://arxiv.org/abs/2506.03087)
*Bin Ma, Yuyuan Feng, Minhua Lin, Enyan Dai*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了可解释图神经网络的安全问题，提出了一种新的模型窃取框架，并验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的可解释GNN可能带来安全风险，需要研究如何防止模型被盗用。

**方法:** 提出了一种新的窃取框架EGSteal，结合了解释对齐和引导数据增强技术。

**结果:** 实验表明，EGSteal在分子图数据集上优于传统方法。

**结论:** 该论文强调了在敏感领域部署可解释GNN时的安全隐患，并提出了针对解释性攻击的防护措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Explanations+Leak+the+Decision+Logic%3A+Stealing+Graph+Neural+Networks+via+Explanation+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03087&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have become essential tools for analyzing
graph-structured data in domains such as drug discovery and financial analysis,
leading to growing demands for model transparency. Recent advances in
explainable GNNs have addressed this need by revealing important subgraphs that
influence predictions, but these explanation mechanisms may inadvertently
expose models to security risks. This paper investigates how such explanations
potentially leak critical decision logic that can be exploited for model
stealing. We propose {\method}, a novel stealing framework that integrates
explanation alignment for capturing decision logic with guided data
augmentation for efficient training under limited queries, enabling effective
replication of both the predictive behavior and underlying reasoning patterns
of target models. Experiments on molecular graph datasets demonstrate that our
approach shows advantages over conventional methods in model stealing. This
work highlights important security considerations for the deployment of
explainable GNNs in sensitive domains and suggests the need for protective
measures against explanation-based attacks. Our code is available at
https://github.com/beanmah/EGSteal.

</details>


### [114] [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
*Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了检索增强生成（RAG）的理论基础，提出了其在上下文线性回归中的有限样本泛化界，并通过实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管RAG在实践中取得了成功，但其理论基础仍然不足，需要进一步探索。

**方法:** 提出了一种有限样本下的泛化界分析方法，并推导了RAG在上下文线性回归中的偏差-方差权衡。

**结果:** 分析表明，与传统的上下文学习（ICL）相比，RAG存在一个固有的泛化误差上限，并且可以通过引入均匀和非均匀RAG噪声来建模检索过程。

**结论:** 论文提出了一个理论框架来分析RAG的泛化性能，并通过实验验证了该框架的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Retrieval-Augmented+Generation+as+Noisy+In-Context+Learning%3A+A+Unified+Theory+and+Risk+Bounds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03100，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03100&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.

</details>


### [115] [Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](https://arxiv.org/abs/2506.02965)
*Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种隐私保护的协同混合专家模型（PC-MoE），可在保护数据隐私的前提下实现多方高效协作训练大型语言模型，且在多个基准测试中表现出接近或超越集中式模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的隐私保护方案通常以任务准确性为代价来换取保密性，而PC-MoE旨在解决这一问题，使得多方能够在保护数据隐私的前提下共同训练出更强大的语言模型。

**方法:** 本研究提出了Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE)，利用MoE架构的稀疏性实现内存高效的去中心化协作LLM训练。

**结果:** 实验结果显示，PC-MoE在七个流行的LLM基准测试中几乎匹配（有时甚至超过）完全集中式模型的性能和收敛速度，并且峰值GPU内存使用减少了近70%，同时对重建攻击具有完全的鲁棒性。

**结论:** PC-MoE在保护数据隐私的同时，能够接近甚至超过集中式模型的性能，打破了隐私保护方法中保密性与任务准确性之间的权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory-Efficient+and+Privacy-Preserving+Collaborative+Training+for+Mixture-of-Experts+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02965&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.

</details>


### [116] [PoLAR: Polar-Decomposed Low-Rank Adapter Representation](https://arxiv.org/abs/2506.03133)
*Kai Lion, Liang Zhang, Bingcong Li, Niao He*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的低秩适应参数化方法(PoLAR)，通过极分解技术提升了大规模模型微调的性能和收敛速度，并在多种任务和模型规模上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 大规模模型的低秩适应存在稳定秩低的问题，导致微调性能下降，因此需要一种有效缓解分配子空间利用不足的方法。

**方法:** 提出了一种基于极分解的参数化方法(PoLAR)，将低秩更新分解为两个受约束于Stiefel流形的方向矩阵和一个无约束的比例矩阵，并理论上证明了其在典型低秩适应问题上的收敛速度更快。

**结果:** PoLAR在从350M到27B大小的基础模型上，针对通用语言理解、常识推理和数学问题解决任务均表现出一致的提升效果。

**结论:** PoLAR通过分解低秩更新并结合黎曼优化，在不同基准测试中取得了稳定的性能增益。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PoLAR%3A+Polar-Decomposed+Low-Rank+Adapter+Representation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03133&send_immediately=true&force_search=false)

**原文摘要:** We show that low-rank adaptation of large-scale models suffers from a low
stable rank that is well below the linear algebraic rank of the subspace,
degrading fine-tuning performance. To mitigate the underutilization of the
allocated subspace, we propose PoLAR, a parameterization inspired by the polar
decomposition that factorizes the low-rank update into two direction matrices
constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory
shows that PoLAR yields an exponentially faster convergence rate on a canonical
low-rank adaptation problem. Pairing the parameterization with Riemannian
optimization leads to consistent gains on three different benchmarks testing
general language understanding, commonsense reasoning, and mathematical problem
solving with base model sizes ranging from 350M to 27B.

</details>


### [117] [Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles](https://arxiv.org/abs/2506.02972)
*Md-Ferdous Pervej, Richeng Jin, Md Moin Uddin Chowdhury, Simran Singh, İsmail Güvenç, Huaiyu Dai*

**主要类别:** cs.LG

**AI概要:** 这篇论文讨论了隐私保护分布式机器学习和空中连接车辆辅助边缘计算的问题。提出了一种名为2CEOAFL的算法，该算法在计算和通信效率方面表现出色，并且能够在保持性能的同时有效利用ACV的持续传感数据和有限资源。


<details>
  <summary>更多</summary>
  
**动机:** 由于ACV上的传感器可以随着它们沿着轨迹移动而捕获新的数据，这种'新'传感数据的持续到达导致了在线学习并对轨迹进行了仔细设计。此外，由于典型的ACV本质上资源受限，需要高效计算和通信的ML解决方案。

**方法:** 首先根据各自随时间变化的数据分布对独立拥有的ACV轨迹进行建模。然后提出了一种2CEOAFL算法，使飞行中的ACV能够(a) 剪枝接收到的密集ML模型使其变浅，(b) 训练剪枝后的模型，并(c) 对训练得到的累积梯度进行概率量化并卸载到中央服务器（CS）。

**结果:** 广泛的模拟实验结果表明，所提出的2CEOAFL算法与那些不进行剪枝和量化的低效计算和通信方法相比，性能相当。

**结论:** 论文提出了一种计算和通信高效的在线空中联邦学习（2CEOAFL）算法，以利用ACV的持续感知数据和有限的机载资源。模拟结果表明，所提出的2CEOAFL算法与其非剪枝和非量化、因此计算和通信效率较低的对应算法相比具有可比性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computation-+and+Communication-Efficient+Online+FL+for+Resource-Constrained+Aerial+Vehicles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02972&send_immediately=true&force_search=false)

**原文摘要:** Privacy-preserving distributed machine learning (ML) and aerial connected
vehicle (ACV)-assisted edge computing have drawn significant attention lately.
Since the onboard sensors of ACVs can capture new data as they move along their
trajectories, the continual arrival of such 'newly' sensed data leads to online
learning and demands carefully crafting the trajectories. Besides, as typical
ACVs are inherently resource-constrained, computation- and
communication-efficient ML solutions are needed. Therefore, we propose a
computation- and communication-efficient online aerial federated learning
(2CEOAFL) algorithm to take the benefits of continual sensed data and limited
onboard resources of the ACVs. In particular, considering independently owned
ACVs act as selfish data collectors, we first model their trajectories
according to their respective time-varying data distributions. We then propose
a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense
ML model to make it shallow, (b) train the pruned model, and (c)
probabilistically quantize and offload their trained accumulated gradients to
the central server (CS). Our extensive simulation results show that the
proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned
and nonquantized, hence, computation- and communication-inefficient
counterparts.

</details>


### [118] [On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](https://arxiv.org/abs/2506.02978)
*Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul Récamier, Salah Ghamizi, Maxime Cordy, Mike Papadakis*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了表格基础模型在对抗性攻击下的脆弱性，并提出了一种新的对抗训练方法以提高其鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的表格基础模型（如TabPFN和TabICL）虽然能够通过上下文学习实现良好的性能，但其对对抗性操作的鲁棒性仍未得到充分研究。因此，论文旨在全面分析其对抗性漏洞。

**方法:** 论文通过引入一种上下文对抗训练策略来提升表格基础模型的鲁棒性，该方法逐步用对抗扰动实例替换上下文，而无需更新模型权重。

**结果:** 实验表明，测试输入中的小规模结构化扰动会显著降低预测准确性，并发现表格基础模型可被重新利用以生成针对传统模型（如随机森林和XGBoost）的对抗样本。

**结论:** 论文得出结论，表格基础模型在面对对抗性攻击时表现出脆弱性，并且它们可以作为生成对抗样本的工具。同时，提出了一种增强这些模型鲁棒性的训练策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Robustness+of+Tabular+Foundation+Models%3A+Test-Time+Attacks+and+In-Context+Defenses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02978&send_immediately=true&force_search=false)

**原文摘要:** Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage
in-context learning to achieve strong performance without gradient updates or
fine-tuning. However, their robustness to adversarial manipulation remains
largely unexplored. In this work, we present a comprehensive study of the
adversarial vulnerabilities of tabular FM, focusing on both their fragility to
targeted test-time attacks and their potential misuse as adversarial tools. We
show on three benchmarks in finance, cybersecurity and healthcare, that small,
structured perturbations to test inputs can significantly degrade prediction
accuracy, even when training context remain fixed. Additionally, we demonstrate
that tabular FM can be repurposed to generate transferable evasion to
conventional models such as random forests and XGBoost, and on a lesser extent
to deep tabular models. To improve tabular FM, we formulate the robustification
problem as an optimization of the weights (adversarial fine-tuning), or the
context (adversarial in-context learning). We introduce an in-context
adversarial training strategy that incrementally replaces the context with
adversarial perturbed instances, without updating model weights. Our approach
improves robustness across multiple tabular benchmarks. Together, these
findings position tabular FM as both a target and a source of adversarial
threats, highlighting the urgent need for robust training and evaluation
practices in this emerging paradigm.

</details>


### [119] [Implicit Regularization of the Deep Inverse Prior Trained with Inertia](https://arxiv.org/abs/2506.02986)
*Nathan Buskulic, Jalal Fadil, Yvain Quéau*

**主要类别:** cs.LG

**AI概要:** 论文提出了针对自监督神经网络解决逆问题的方法，提供了理论上的收敛和恢复保证，并证明了其优于传统方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当使用神经网络解决逆问题时，恢复保证方面的理论保障非常少，因此需要提供关于收敛和恢复保证的理论支持。

**方法:** 研究分析了应用于逆问题的自监督神经网络（如深度图像/逆先验）在具有粘性和几何Hessian驱动阻尼的惯性下的收敛和恢复保证，并探讨了连续时间和离散时间情况下的动力系统轨迹及自适应步长的惯性算法。

**结果:** 研究表明，在连续时间情况下，网络可以以加速的指数收敛速率进行训练；在离散情况下，虽然线性收敛速率不够尖锐，但仍然有类似的恢复保证。

**结论:** 研究展示了使用具有惯性的自监督神经网络解决逆问题时，连续时间情况下网络可以以比梯度流获得的速率更优的加速指数收敛速率进行训练。在离散情况下，尽管线性收敛速率不够锐利，但训练网络仍享有类似的恢复保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Implicit+Regularization+of+the+Deep+Inverse+Prior+Trained+with+Inertia，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02986&send_immediately=true&force_search=false)

**原文摘要:** Solving inverse problems with neural networks benefits from very few
theoretical guarantees when it comes to the recovery guarantees. We provide in
this work convergence and recovery guarantees for self-supervised neural
networks applied to inverse problems, such as Deep Image/Inverse Prior, and
trained with inertia featuring both viscous and geometric Hessian-driven
dampings. We study both the continuous-time case, i.e., the trajectory of a
dynamical system, and the discrete case leading to an inertial algorithm with
an adaptive step-size. We show in the continuous-time case that the network can
be trained with an optimal accelerated exponential convergence rate compared to
the rate obtained with gradient flow. We also show that training a network with
our inertial algorithm enjoys similar recovery guarantees though with a less
sharp linear convergence rate.

</details>


### [120] [Protein Inverse Folding From Structure Feedback](https://arxiv.org/abs/2506.03028)
*Junde Xu, Zijun Gao, Xinyi Zhou, Jie Hu, Xingyi Cheng, Le Song, Guangyong Chen, Pheng-Ann Heng, Jiezhong Qiu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种利用结构反馈和偏好优化提升蛋白质序列设计的新方法，有效提高了逆折叠模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决逆折叠问题对于多种生物技术应用至关重要，但设计能够折叠成目标三维结构的氨基酸序列仍然具有挑战性。

**方法:** 通过使用Direct Preference Optimization (DPO)对逆折叠模型进行微调，并利用蛋白质折叠模型提供的配对结构偏好标签来改进序列设计。

**结果:** 在CATH 4.2测试集上的结果表明，DPO微调不仅提高了基线模型的序列恢复能力，还使平均TM-Score从0.77显著提高到0.81；对于具有挑战性的蛋白质结构，TM-Score相较基线模型提升了79.5%。

**结论:** 该论文提出了一种基于偏好优化的蛋白质序列设计方法，利用结构反馈提高了逆折叠模型的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Protein+Inverse+Folding+From+Structure+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03028&send_immediately=true&force_search=false)

**原文摘要:** The inverse folding problem, aiming to design amino acid sequences that fold
into desired three-dimensional structures, is pivotal for various
biotechnological applications. Here, we introduce a novel approach leveraging
Direct Preference Optimization (DPO) to fine-tune an inverse folding model
using feedback from a protein folding model. Given a target protein structure,
we begin by sampling candidate sequences from the inverse-folding model, then
predict the three-dimensional structure of each sequence with the folding model
to generate pairwise structural-preference labels. These labels are used to
fine-tune the inverse-folding model under the DPO objective. Our results on the
CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence
recovery of baseline models but also leads to a significant improvement in
average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity.
Furthermore, iterative application of our DPO-based method on challenging
protein structures yields substantial gains, with an average TM-Score increase
of 79.5\% with regard to the baseline model. This work establishes a promising
direction for enhancing protein sequence design ability from structure feedback
by effectively utilizing preference optimization.

</details>


### [121] [Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness](https://arxiv.org/abs/2506.03075)
*Bogdan Chornomaz, Yonatan Koren, Shay Moran, Tom Waknine*

**主要类别:** cs.LG

**AI概要:** 本文解决了Hanneke等人遗留的一个主要开放问题，即在不可知设定下，面对实例目标投毒攻击时，随机学习者的最优过失误差率为√(dη)。


<details>
  <summary>更多</summary>
  
**动机:** 对抗性攻击对机器学习模型的安全性和可靠性构成威胁，因此研究如何在存在对手的情况下进行有效学习是重要的。

**方法:** 研究了在实例目标投毒攻击下，随机学习者的过失错误率，并给出了一个上界和更强的下界。

**结果:** 研究表明，在可实现设定中，最优误差与VC维和破坏比例成正比；而在不可知设定中，最优过失误差为√(dη)。

**结论:** 在面对能够破坏训练示例的对手时，随机学习者可以实现最优过失错误率，并且即使对手完全了解随机位，这一结果依然成立。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agnostic+Learning+under+Targeted+Poisoning%3A+Optimal+Rates+and+the+Role+of+Randomness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03075，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03075&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of learning in the presence of an adversary that can
corrupt an $\eta$ fraction of the training examples with the goal of causing
failure on a specific test point. In the realizable setting, prior work
established that the optimal error under such instance-targeted poisoning
attacks scales as $\Theta(d\eta)$, where $d$ is the VC dimension of the
hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding
question in the agnostic setting. We show that the optimal excess error is
$\tilde{\Theta}(\sqrt{d\eta})$, answering one of the main open problems left by
Hanneke et al. To achieve this rate, it is necessary to use randomized
learners: Hanneke et al. showed that deterministic learners can be forced to
suffer error close to 1, even under small amounts of poisoning. Perhaps
surprisingly, our upper bound remains valid even when the learner's random bits
are fully visible to the adversary . In the other direction, our lower bound is
stronger than standard PAC-style bounds: instead of tailoring a hard
distribution separately for each sample size, we exhibit a single fixed
distribution under which the adversary can enforce an excess error of
$\Omega(\sqrt{d\eta})$ infinitely often.

</details>


### [122] [Non-Asymptotic Length Generalization](https://arxiv.org/abs/2506.03085)
*Thomas Chen, Tengyu Ma, Zhiyuan Li*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了长度泛化的能力，形式化了非渐近长度泛化的框架，并分析了不同函数类的长度复杂度上界。


<details>
  <summary>更多</summary>
  
**动机:** 研究学习算法对训练集之外更长输入的泛化能力是重要的，本文旨在为此提供可证明的保证。

**方法:** 论文提出了非渐近长度泛化的框架，并利用最小复杂度插值器学习算法实现最优长度复杂度。

**结果:** 作者们表明函数类是否允许非渐近长度泛化等同于其语言等价问题的可判定性，并得出上下文无关语法的长度复杂度无计算上界。

**结论:** 论文得出了Deterministic Finite Automata的长度复杂度为2n-2，且展示了1层和2层C-RASP函数的长度复杂度上界分别为O(T^2)和O(T^{O(K)})。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-Asymptotic+Length+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03085&send_immediately=true&force_search=false)

**原文摘要:** Length generalization is the ability of a learning algorithm to learn a
hypothesis which generalizes to longer inputs than the inputs in the training
set. In this paper, we provide provable guarantees of length generalization for
various classes of functions in an idealized setting. First, we formalize the
framework of non-asymptotic length generalization, which requires a computable
upper bound for the minimum input length that guarantees length generalization,
as a function of the complexity of ground-truth function under some given
complexity measure. We refer to this minimum input length to length generalize
as length complexity. We show the Minimum-Complexity Interpolator learning
algorithm achieves optimal length complexity. We further show that whether a
function class admits non-asymptotic length generalization is equivalent to the
decidability of its language equivalence problem, which implies that there is
no computable upper bound for the length complexity of Context-Free Grammars.
On the positive side, we show that the length complexity of Deterministic
Finite Automata is $2n - 2$ where $n$ is the number of states of the
ground-truth automaton. Our main results are upper bounds of length complexity
for a subset of a transformer-related function class called C-RASP (Yang &
Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions
is $O(T^2)$ when the ground-truth function has precision $T$, and that the
length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the
ground-truth function has precision $T$ and $K$ heads.

</details>


### [123] [From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](https://arxiv.org/abs/2506.03093)
*Valérie Costa, Thomas Fel, Ekdeep Singh Lubana, Bahareh Tolooshams, Demba Ba*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的稀疏自动编码器MP-SAE，通过改进匹配追踪算法，有效捕捉神经网络中的层级和非线性特征，证明了传统SAEs的线性假设不足，并展示了其在多模态表征解析中的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 受神经网络表征是否将抽象、可解释的特征编码为线性可访问且近似正交方向的假设驱动，研究者们提出了问题：现有的SAEs是否能够表示与该假设结构相冲突的特征？如果不能，那么避免这种不匹配是否有助于识别这些特征并进一步理解神经网络表征？

**方法:** 本文采用了构造性方法，重新利用了稀疏编码中的匹配追踪（MP）算法，设计了MP-SAE。这种SAE通过将其编码器展开为一系列残差引导步骤来捕捉层级和非线性可访问特征。

**结果:** 通过在合成数据和自然数据上的比较实验，论文展示了两点主要结果：(i) 层级概念会引发条件正交特征，而现有SAEs无法准确捕捉；(ii) MP-SAE的非线性编码步骤恢复了高度有意义的特征，揭示了视觉-语言模型中不同模态表征空间的共享结构。此外，MP-SAE的序列编码原理在推理时提供了自适应稀疏性的额外优势。

**结论:** 论文得出结论，稀疏自动编码器（SAEs）的假设即有用特征仅线性可访问是不充分的。通过引入MP-SAE架构，论文展示了层级和非线性可访问特征的恢复能力，并主张解释性应从表征的现象学开始，方法应源于适合现象学的假设。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Flat+to+Hierarchical%3A+Extracting+Sparse+Representations+with+Matching+Pursuit，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03093&send_immediately=true&force_search=false)

**原文摘要:** Motivated by the hypothesis that neural network representations encode
abstract, interpretable features as linearly accessible, approximately
orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in
interpretability. However, recent work has demonstrated phenomenology of model
representations that lies outside the scope of this hypothesis, showing
signatures of hierarchical, nonlinear, and multi-dimensional features. This
raises the question: do SAEs represent features that possess structure at odds
with their motivating hypothesis? If not, does avoiding this mismatch help
identify said features and gain further insights into neural network
representations? To answer these questions, we take a construction-based
approach and re-contextualize the popular matching pursuits (MP) algorithm from
sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a
sequence of residual-guided steps, allowing it to capture hierarchical and
nonlinearly accessible features. Comparing this architecture with existing SAEs
on a mixture of synthetic and natural data settings, we show: (i) hierarchical
concepts induce conditionally orthogonal features, which existing SAEs are
unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE
recovers highly meaningful features, helping us unravel shared structure in the
seemingly dichotomous representation spaces of different modalities in a
vision-language model, hence demonstrating the assumption that useful features
are solely linearly accessible is insufficient. We also show that the
sequential encoder principle of MP-SAE affords an additional benefit of
adaptive sparsity at inference time, which may be of independent interest.
Overall, we argue our results provide credence to the idea that
interpretability should begin with the phenomenology of representations, with
methods emerging from assumptions that fit it.

</details>


### [124] [On Weak-to-Strong Generalization and f-Divergence](https://arxiv.org/abs/2506.03109)
*Wei Yao, Gengze Xu, Huayi Tang, Wenkai Yang, Donglin Di, Ziqiao Wang, Yong Liu*

**主要类别:** cs.LG

**AI概要:** 本文研究了f-散度损失在弱到强泛化中的应用，发现其能有效提升强模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法需要额外的弱模型或复杂过程，而f-散度损失可能提高模型性能。

**方法:** 引入f-散度作为信息理论损失函数框架，并进行理论分析和实验验证。

**结果:** 理论分析揭示了不同f-散度损失在弱到强泛化中的基本限制和等价性，并通过实验验证了其提升模型泛化能力和抗噪能力的效果。

**结论:** f-散度损失在弱到强泛化中有效提升了强模型的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Weak-to-Strong+Generalization+and+f-Divergence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03109&send_immediately=true&force_search=false)

**原文摘要:** Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for
stimulating the capabilities of strong pre-trained models by leveraging
supervision from weaker supervisors. To improve the performance of the strong
model, existing methods often require additional weak models or complex
procedures, leading to substantial computational and memory overhead. Motivated
by the effectiveness of $f$-divergence loss in various machine learning
domains, we introduce $f$-divergence as an information-theoretic loss function
framework in W2SG. Our theoretical analysis reveals fundamental limitations and
equivalence of different $f$-divergence losses in W2SG, supported by sample
complexity bounds and information-theoretic insights. We empirically
demonstrate that $f$-divergence loss, which generalizes widely-used metrics
like KL divergence, effectively improves generalization and noise tolerance of
the strong model in practice.

</details>


### [125] [Rectified Flows for Fast Multiscale Fluid Flow Modeling](https://arxiv.org/abs/2506.03111)
*Victor Armegioiu, Yannick Ramic, Siddhartha Mishra*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一个名为rectified flow的新框架，有效减少了生成模型采样所需的步骤数，同时保持了预测的精确度。


<details>
  <summary>更多</summary>
  
**动机:** 由于流体流动的多尺度动态和对初始条件的极端敏感性，其统计建模极具挑战性；而近期提出的条件扩散模型虽然能够实现高保真度，但通常需要大量的随机采样步骤。

**方法:** 将采样过程建模为沿着几乎直线流动场求解常微分方程(ODE)的问题，从而让每一步积分更加高效。

**结果:** 实验表明rectified flows与扩散模型恢复了相同的后验分布，保留了MSE训练基线错过的细尺度特征，并且在推理时间上大大提升效率。

**结论:** 该论文提出了一种名为rectified flow的框架，通过学习一个时间相关的速度场，使得在生成模型中进行采样时可以显著减少步骤数且不牺牲预测精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rectified+Flows+for+Fast+Multiscale+Fluid+Flow+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03111，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03111&send_immediately=true&force_search=false)

**原文摘要:** The statistical modeling of fluid flows is very challenging due to their
multiscale dynamics and extreme sensitivity to initial conditions. While
recently proposed conditional diffusion models achieve high fidelity, they
typically require hundreds of stochastic sampling steps at inference. We
introduce a rectified flow framework that learns a time-dependent velocity
field, transporting input to output distributions along nearly straight
trajectories. By casting sampling as solving an ordinary differential equation
(ODE) along this straighter flow field, our method makes each integration step
much more effective, using as few as eight steps versus (more than) 128 steps
in standard score-based diffusion, without sacrificing predictive fidelity.
Experiments on challenging multiscale flow benchmarks show that rectified flows
recover the same posterior distributions as diffusion models, preserve
fine-scale features that MSE-trained baselines miss, and deliver
high-resolution samples in a fraction of inference time.

</details>


### [126] [Zero-Shot Time Series Forecasting with Covariates via In-Context Learning](https://arxiv.org/abs/2506.03128)
*Andreas Auer, Raghul Parthipan, Pedro Mercado, Abdul Fatir Ansari, Lorenzo Stella, Bernie Wang, Michael Bohlke-Schneider, Syama Sundar Rangapuram*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的零样本时间序列预测模型COSMIC，该模型能够有效利用协变量，并在相关任务上实现了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的预训练模型要么不支持协变量，要么不能有效地整合它们。本文旨在解决这一问题。

**方法:** 提出了信息协变量增强方法，使得COSMIC可以在不需要包含协变量的数据集的情况下进行训练。

**结果:** COSMIC在有和没有协变量的零样本预测中都达到了最先进的性能，并且通过定量和定性分析证明了其在零样本预测中有效利用了协变量。

**结论:** COSMIC是一种利用上下文学习使用协变量的零样本预测模型，并在零样本预测中实现了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Time+Series+Forecasting+with+Covariates+via+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03128&send_immediately=true&force_search=false)

**原文摘要:** Pretrained time series models, capable of zero-shot forecasting, have
demonstrated significant potential in enhancing both the performance and
accessibility of time series forecasting. However, existing pretrained models
either do not support covariates or fail to incorporate them effectively. We
introduce COSMIC, a zero-shot forecasting model that utilizes covariates via
in-context learning. To address the challenge of data scarcity, we propose
Informative Covariate Augmentation, which enables the training of COSMIC
without requiring any datasets that include covariates. COSMIC achieves
state-of-the-art performance in zero-shot forecasting, both with and without
covariates. Our quantitative and qualitative analysis demonstrates that COSMIC
effectively leverages covariates in zero-shot forecasting.

</details>


### [127] [Not All Tokens Are Meant to Be Forgotten](https://arxiv.org/abs/2506.03142)
*Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Douglas Zytko, Prashant Khanduri, Dongxiao Zhu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种称为TIF的新框架，用于解决大型语言模型中的过度遗忘问题，成功地提高了遗忘效果并保持了模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型可能会记住不希望的信息，例如私人或受版权保护的内容，这引发了隐私和法律方面的担忧。因此需要有效的遗忘机制。

**方法:** 该方法包括一个灵活的目标信息识别器和一种新的目标偏好优化方法，利用Logit Preference Loss来遗忘不需要的信息，并通过保留一般信息来减少效用降级。

**结果:** 在TOFU和MUSE基准上的实验表明，TIF框架能够提高遗忘效果，同时保持模型的实用性，并达到了最先进的结果。

**结论:** 论文提出了一种名为TIF的框架，有效解决了大型语言模型在遗忘过程中出现的过度遗忘问题，同时保持了模型的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Not+All+Tokens+Are+Meant+to+Be+Forgotten，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03142，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03142&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs), pre-trained on massive text corpora, exhibit
remarkable human-level language understanding, reasoning, and decision-making
abilities. However, they tend to memorize unwanted information, such as private
or copyrighted content, raising significant privacy and legal concerns.
Unlearning has emerged as a promising solution, but existing methods face a
significant challenge of over-forgetting. This issue arises because they
indiscriminately suppress the generation of all the tokens in forget samples,
leading to a substantial loss of model utility. To overcome this challenge, we
introduce the Targeted Information Forgetting (TIF) framework, which consists
of (1) a flexible targeted information identifier designed to differentiate
between unwanted words (UW) and general words (GW) in the forget samples, and
(2) a novel Targeted Preference Optimization approach that leverages Logit
Preference Loss to unlearn unwanted information associated with UW and
Preservation Loss to retain general information in GW, effectively improving
the unlearning process while mitigating utility degradation. Extensive
experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF
framework enhances unlearning effectiveness while preserving model utility and
achieving state-of-the-art results.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [128] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/abs/2506.02097)
*Priyaranjan Pattnayak, Amit Agarwal, Hansa Meghwani, Hitesh Laxmichand Patel, Srikant Panda*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种结合检索增强生成(RAG)和基于意图的预定义回复的混合框架，以提高企业级对话AI系统的准确性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 企业级对话AI系统面临多种挑战，包括用户查询多样性、高延迟、生成错误信息以及难以集成更新频繁的专业领域知识。因此需要一种高效且自适应的解决方案。

**方法:** 提出一种混合框架，将RAG与基于意图的预定义回复相结合，并通过对话上下文管理器和反馈机制动态优化意图识别和响应选择。

**结果:** 实验结果表明，该框架在多种查询类型上均表现出高准确率（95%）和低延迟（180ms），优于传统的RAG和意图识别系统。

**结论:** 提出的混合框架是一种可扩展且适应性强的企业级对话AI解决方案，能有效平衡准确性与效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+AI+for+Responsive+Multi-Turn+Online+Conversations+with+Novel+Dynamic+Routing+and+Feedback+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02097，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02097&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [129] [Descriptive History Representations: Learning Representations by Answering Questions](https://arxiv.org/abs/2506.02125)
*Guy Tennenholtz, Jihwan Jeong, Chih-Wei Hsu, Yinlam Chow, Craig Boutilier*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了描述性历史表示（DHRs），用于在部分可观测环境中进行有效决策，通过多智能体学习框架生成能够预测用户行为的可解释性用户档案。


<details>
  <summary>更多</summary>
  
**动机:** 在部分可观测环境中，有效决策需要将长时间的交互历史压缩为信息丰富的表示形式。这是本篇论文的主要动机。

**方法:** 提出了一种多智能体学习框架，包括表示、决策和问题提出组件，并通过一个联合目标进行优化，该目标平衡了奖励最大化与表示回答信息问题的能力。

**结果:** 验证了所提出的方法在公开的电影和购物数据集上的有效性，生成了解释性强的文本用户档案作为预测用户偏好行为的充分统计量。

**结论:** 论文得出结论，通过引入描述性历史表示（DHRs），可以有效地捕捉对用户行为预测和决策制定至关重要的历史细节和预测结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Descriptive+History+Representations%3A+Learning+Representations+by+Answering+Questions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02125&send_immediately=true&force_search=false)

**原文摘要:** Effective decision making in partially observable environments requires
compressing long interaction histories into informative representations. We
introduce Descriptive History Representations (DHRs): sufficient statistics
characterized by their capacity to answer relevant questions about past
interactions and potential future outcomes. DHRs focus on capturing the
information necessary to address task-relevant queries, providing a structured
way to summarize a history for optimal control. We propose a multi-agent
learning framework, involving representation, decision, and question-asking
components, optimized using a joint objective that balances reward maximization
with the representation's ability to answer informative questions. This yields
representations that capture the salient historical details and predictive
structures needed for effective decision making. We validate our approach on
user modeling tasks with public movie and shopping datasets, generating
interpretable textual user profiles which serve as sufficient statistics for
predicting preference-driven behavior of users.

</details>


### [130] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/abs/2506.02139)
*Edward Y. Chang*

**主要类别:** cs.AI

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Unified+Cognitive+Consciousness+Theory+for+Language+Models%3A+Anchoring+Semantics%2C+Thresholds+of+Activation%2C+and+Emergent+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02139&send_immediately=true&force_search=false)

**原文摘要:** Few-shot learning in large language models (LLMs) reveals a deep paradox:
Some tasks generalize from minimal examples, while others require extensive
supervision. We address this through the Unified Cognitive Consciousness Theory
(UCCT), which reframes LLMs not as incomplete agents, but as unconscious
substrates, repositories of latent linguistic and conceptual patterns that
operate without explicit semantics or goal-directed reasoning. In this view,
LLMs are not broken approximations of cognition, but necessary and foundational
components of general intelligence. Semantic anchoring, through prompts, roles,
and interaction, acts as a conscious control layer, binding latent structure to
task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying
account of prompting, fine-tuning, retrieval, and multi-agent coordination, all
grounded in probabilistic alignment between unconscious representation and
external control. To support this model, we present the Threshold-Crossing
Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase
transition. But the central claim remains architectural: AGI will not emerge by
discarding LLMs, but by aligning and integrating them into systems that reason,
regulate, and adapt together.

</details>


### [131] [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
*Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, Pavlo Molchanov*

**主要类别:** cs.AI

**AI概要:** 本文主张小语言模型（SLMs）在代理AI系统中比大型语言模型（LLMs）更合适、更经济，并提出了向SLMs迁移的必要性和可行性。


<details>
  <summary>更多</summary>
  
**动机:** 随着代理AI系统的兴起，大型语言模型（LLMs）在许多应用场景中需要被更高效、更经济的语言模型替代。

**方法:** 基于当前SLMs的能力、代理系统的常见架构以及LM部署的经济性进行论证，并讨论了向SLMs迁移的潜在障碍和LLM到SLM代理转换算法。

**结果:** 即使部分从LLMs转向SLMs，也会对AI代理行业产生重要的运营和经济效益。

**结论:** 小语言模型（SLMs）在代理系统中足够强大、更合适且更具经济性，因此是代理AI的未来。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Small+Language+Models+are+the+Future+of+Agentic+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02153，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02153&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are often praised for exhibiting near-human
performance on a wide range of tasks and valued for their ability to hold a
general conversation. The rise of agentic AI systems is, however, ushering in a
mass of applications in which language models perform a small number of
specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are
sufficiently powerful, inherently more suitable, and necessarily more
economical for many invocations in agentic systems, and are therefore the
future of agentic AI. Our argumentation is grounded in the current level of
capabilities exhibited by SLMs, the common architectures of agentic systems,
and the economy of LM deployment. We further argue that in situations where
general-purpose conversational abilities are essential, heterogeneous agentic
systems (i.e., agents invoking multiple different models) are the natural
choice. We discuss the potential barriers for the adoption of SLMs in agentic
systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of
the operational and economic impact even a partial shift from LLMs to SLMs is
to have on the AI agent industry. We aim to stimulate the discussion on the
effective use of AI resources and hope to advance the efforts to lower the
costs of AI of the present day. Calling for both contributions to and critique
of our position, we commit to publishing all such correspondence at
https://research.nvidia.com/labs/lpr/slm-agents.

</details>


### [132] [Reflection-Based Memory For Web navigation Agents](https://arxiv.org/abs/2506.02158)
*Ruhana Azam, Aditya Vempaty, Ashish Jagmohan*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个新的Web导航系统Reflection-Augment Planning (ReAP)，它可以通过自我反思利用过去的经验，显著提升导航任务的表现。


<details>
  <summary>更多</summary>
  
**动机:** 当前的Web导航代理没有记忆过去经验的能力，导致重复错误且无法从以前的交互中学习。

**方法:** 引入了一种名为Reflection-Augment Planning (ReAP)的新系统，该系统通过自我反思来利用过去的经历。

**结果:** 该方法整体上提高了基准结果11个百分点，在之前失败的任务上提高了29个百分点。

**结论:** Reflection-Augment Planning (ReAP)系统能够利用过去的经验，包括成功和失败的体验，从而提高Web导航任务的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reflection-Based+Memory+For+Web+navigation+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02158&send_immediately=true&force_search=false)

**原文摘要:** Web navigation agents have made significant progress, yet current systems
operate with no memory of past experiences -- leading to repeated mistakes and
an inability to learn from previous interactions. We introduce
Reflection-Augment Planning (ReAP), a web navigation system to leverage both
successful and failed past experiences using self-reflections. Our method
improves baseline results by 11 points overall and 29 points on previously
failed tasks. These findings demonstrate that reflections can transfer to
different web navigation tasks.

</details>


### [133] [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/abs/2506.02177)
*Haizhong Zheng, Yang Zhou, Brian R. Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, Beidi Chen*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种高效的预滚动筛选算法GRESO，可减少强化学习过程中的冗余计算。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习中的扩展滚动会引入显著的计算开销，而许多提示在训练中长期缺乏信息价值。

**方法:** 分析奖励动态并提出GRESO算法，以在线、轻量级的方式过滤无信息提示。

**结果:** 在多个数学推理基准模型上评估显示，GRESO显著减少了训练时间且未造成精度损失。

**结论:** GRESO通过预测和跳过无信息提示，在不降低准确率的前提下实现了高达2.4倍的滚动时间加速和2.0倍的总训练时间加速。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Act+Only+When+It+Pays%3A+Efficient+Reinforcement+Learning+for+LLM+Reasoning+via+Selective+Rollouts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02177，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02177&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning, such as PPO and GRPO, has powered recent
breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables
models to selectively use higher-quality data for training, which can stabilize
RL training and improve model performance. However, this comes at the cost of
significant computational overhead. In this paper, we show that a substantial
portion of this overhead can be avoided by skipping uninformative prompts
before rollout. Our analysis of reward dynamics reveals a strong temporal
consistency in prompt value: prompts that are uninformative in one epoch of
training are likely to remain uninformative in future epochs. Based on these
insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,
lightweight pre-rollout filtering algorithm that predicts and skips
uninformative prompts using reward training dynamics. By evaluating GRESO on a
broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,
DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves
up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total
training time without accuracy degradation.

</details>


### [134] [Natural, Artificial, and Human Intelligences](https://arxiv.org/abs/2506.02183)
*Emmanuel M. Pothos, Dominic Widdows*

**主要类别:** cs.AI

**AI概要:** 这篇论文讨论了人类智能的本质及其与非人类动物和聊天机器人的比较，指出人类智能的独特之处不仅在于语言，还包括发明创造、复杂推理、具身认知和自我意识。


<details>
  <summary>更多</summary>
  
**动机:** 论文旨在探讨现代时代的聊天机器人是否可以被认为像人类一样具有智能，并分析人类智能的独特成就所需的要素。

**方法:** 论文从心理学文献、非人类动物智能证据、书面语言在科学和技术中的作用、人工智能的发展、智力测试的历史以及具身认知在智能中的作用等方面探讨了问题。

**结果:** 研究发现，对于人类智能的独特成就来说，除了语言之外，还有发明、复杂推理能力、具身认知和自我意识四个关键要素。

**结论:** 论文得出的结论是，人类智能并非与非人类动物智能有质的区别，因为除了复杂的语言之外，其他要求都是满足的。至于聊天机器人，目前的局限性在于缺乏体现性和（明显的）意识。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Natural%2C+Artificial%2C+and+Human+Intelligences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02183&send_immediately=true&force_search=false)

**原文摘要:** Human achievement, whether in culture, science, or technology, is
unparalleled in the known existence. This achievement is tied to the enormous
communities of knowledge, made possible by (especially written) language:
leaving theological content aside, it is very much true that "in the beginning
was the word". There lies the challenge regarding modern age chatbots: they can
'do' language apparently as well as ourselves and there is a natural question
of whether they can be considered intelligent, in the same way as we are or
otherwise. Are humans uniquely intelligent? We consider this question in terms
of the psychological literature on intelligence, evidence for intelligence in
non-human animals, the role of written language in science and technology,
progress with artificial intelligence, the history of intelligence testing (for
both humans and machines), and the role of embodiment in intelligence. For the
most unique accomplishments of human intelligence (such as music symphonies or
complex scientific theories), we think that, together with language, there are
four essential ingredients, which can be summarised as invention, capacity for
complex inference, embodiment, and self-awareness. This conclusion makes
untenable the position that human intelligence differs qualitatively from that
of many non-human animals, since, with the exception of complex language, all
the other requirements are fulfilled. Regarding chatbots, the current
limitations are localised to the lack of embodiment and (apparent) lack of
awareness.

</details>


### [135] [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/abs/2506.02211)
*Maxime Robeyns, Laurence Aitchison*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的代码生成训练方法GRPO，其不仅考虑代码的功能正确性，还考虑代码的质量属性，从而提升生成代码的整体质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的代码生成模型训练方法主要关注功能正确性，而忽视了代码的可维护性、质量和安全性。

**方法:** 开发了一个全面的库来量化代码质量的不同方面，并将其用作GRPO中的奖励信号。

**结果:** 使用GRPO方法后，代码质量根据所提出的度量标准有所提高。

**结论:** GRPO提高了代码质量，这通过专家、盲审人类注释者的评估得到了证实。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+LLM-Generated+Code+Quality+with+GRPO，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02211&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are gaining widespread use for code generation.
Recent training procedures use execution feedback as a reward signal, typically
focusing on the functional correctness of the code, using unit test pass rate
as a reward signal. However, this reward signal fails to capture notions of
maintainability, quality and safety of the code produced. We address this
under-explored area and develop a comprehensive library to quantify various
aspects of code quality, and use it as a reward in GRPO. We find GRPO increases
code quality according to this measure, which is confirmed by expert, blinded
human annotators.

</details>


### [136] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/abs/2506.02280)
*Kedir Yassin Hussen, Walelign Tewabe Sewunetie, Abinew Ali Ayele, Sukairaj Hafiz Imam, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam*

**主要类别:** cs.AI

**AI概要:** 本文分析了六种大型语言模型（LLMs）、八种小型语言模型（SLMs）和六种专用小型语言模型（SSLMs）对非洲语言的支持情况，发现仅有42种非洲语言得到支持，且存在严重的数据缺乏、分词偏差、高昂计算成本和评估问题等挑战。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）在自然语言处理领域带来了重大变革，但其主要优势尚未惠及非洲的2000种低资源语言。本研究旨在分析当前模型对非洲语言的支持情况，并揭示其中存在的差距与挑战。

**方法:** 该论文对六个大型语言模型（LLMs）、八个小型语言模型（SLMs）和六个专用小型语言模型（SSLMs）在非洲语言上的覆盖情况进行了比较分析，评估了语言覆盖范围、训练集、技术限制、书写系统问题以及语言建模路线图。

**结果:** 研究发现，目前仅有42种非洲语言得到了一定支持，且只有23个公开数据集可用。其中，阿姆哈拉语、斯瓦希里语、南非荷兰语和马达加斯加语四种语言经常受到关注，而超过98%的非洲语言仍然未被支持。此外，现有研究仅涵盖了拉丁文、阿拉伯文和吉兹文三种书写系统，而忽略了其他20种仍在使用的书写系统。

**结论:** 论文指出，虽然大型语言模型（LLMs）和小型语言模型（SLMs）对非洲一些语言提供了一定支持，但绝大多数非洲低资源语言仍未被覆盖。需要通过语言标准化、语料库开发和有效的适应方法来解决这一问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+State+of+Large+Language+Models+for+African+Languages%3A+Progress+and+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02280，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02280&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [137] [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
*Tianyu Hua, Harper Hua, Violet Xiang, Benjamin Klieger, Sang T. Truong, Weixin Liang, Fan-Yun Sun, Nick Haber*

**主要类别:** cs.AI

**AI概要:** 本研究介绍了ResearchCodeBench，这是一个用于评估大型语言模型实现最新机器学习研究成果能力的基准测试工具。结果表明，当前最佳模型也只能准确实现不到40%的代码。


<details>
  <summary>更多</summary>
  
**动机:** 研究大型语言模型（LLMs）能否忠实地实现最新科研论文中的新概念，这些概念在预训练期间是未见过的。

**方法:** 引入了包含212个编码挑战的基准测试ResearchCodeBench，用以评估LLMs将前沿ML研究成果转化为可执行代码的能力，并对30多个专有和开源LLMs进行了评估。

**结果:** 研究发现，即使是表现最好的模型，正确实现代码的比例也不到40%。其中Gemini-2.5-Pro-Preview的成功率为37.3%，O3 (High) 和 O4-mini (High) 分别为32.3%和30.8%。

**结论:** ResearchCodeBench 提供了一个严格且由社区驱动的评估平台，能够持续理解和推进由大语言模型驱动的研究代码生成创新。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ResearchCodeBench%3A+Benchmarking+LLMs+on+Implementing+Novel+Machine+Learning+Research+Code，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02314，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02314&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown promise in transforming machine
learning research, yet their capability to faithfully implement novel ideas
from recent research papers-ideas unseen during pretraining-remains unclear. We
introduce ResearchCodeBench, a benchmark of 212 coding challenges that
evaluates LLMs' ability to translate cutting-edge ML contributions from top
2024-2025 research papers into executable code. We assessed 30+ proprietary and
open-source LLMs, finding that even the best models correctly implement less
than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%
success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and
30.8% respectively. We present empirical findings on performance comparison,
contamination, and error patterns. By providing a rigorous and community-driven
evaluation platform, ResearchCodeBench enables continuous understanding and
advancement of LLM-driven innovation in research code generation.

</details>


### [138] [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/abs/2506.02387)
*Zelai Xu, Zhexuan Xu, Xiangmin Yi, Huining Yuan, Xinlei Chen, Yi Wu, Chao Yu, Yu Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出VS-Bench，一个用于评估多智能体环境下视觉语言模型战略推理能力的多模态基准测试，并展示了当前模型与理想表现之间的明显差异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉语言模型(VLMs)基准测试局限于单智能体或仅文本环境，而现实世界场景通常涉及多个智能体在丰富视觉和语言背景下的互动，带来多模态观察和战略性互动的挑战。

**方法:** 介绍了一个名为VS-Bench的新基准，包含八个基于视觉的环境，涵盖合作、竞争和混合动机互动，通过离线策略推理评估和在线决策评估两个维度进行实验。

**结果:** 14个领先VLM模型的广泛实验显示，当前模型与最佳性能之间存在显著差距，最佳模型达到47.8%的预测准确率和24.3%的归一化回报。

**结论:** VS-Bench是一个用于评估视觉语言模型在多智能体环境中战略推理和决策能力的多模态基准测试，它揭示了现有模型与最优性能之间存在的显著差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VS-Bench%3A+Evaluating+VLMs+for+Strategic+Reasoning+and+Decision-Making+in+Multi-Agent+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02387&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Vision Language Models (VLMs) have expanded their
capabilities to interactive agent tasks, yet existing benchmarks remain limited
to single-agent or text-only environments. In contrast, real-world scenarios
often involve multiple agents interacting within rich visual and linguistic
contexts, posing challenges with both multimodal observations and strategic
interactions. To bridge this gap, we introduce Visual Strategic Bench
(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning
and decision-making in multi-agent environments. VS-Bench comprises eight
vision-grounded environments spanning cooperative, competitive, and
mixed-motive interactions, designed to assess agents' ability to predict
others' future moves and optimize for long-term objectives. We consider two
complementary evaluation dimensions, including offline evaluation of strategic
reasoning by next-action prediction accuracy and online evaluation of
decision-making by normalized episode return. Extensive experiments of fourteen
leading VLMs reveal a significant gap between current models and optimal
performance, with the best models attaining 47.8% prediction accuracy and 24.3%
normalized return. We further conduct in-depth analyses on multimodal
observations, test-time scaling, social behaviors, and failure cases of VLM
agents. By standardizing the evaluation and highlighting the limitations of
existing models, we envision VS-Bench as a foundation for future research on
strategic multimodal agents. Code and data are available at
https://vs-bench.github.io.

</details>


### [139] [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397)
*Shengjia Zhang, Junjie Wu, Jiawei Chen, Changwang Zhang, Xingyu Lou, Wangchunshu Zhou, Sheng Zhou, Can Wang, Jun Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出OThink-R1方法，通过动态选择推理模式减少推理冗余，提升大型推理模型效率。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型推理模型（LRMs）在复杂任务上表现优异，但其对简单任务使用过多token，导致推理过程可能存在冗余，需要一种更高效的方法。

**方法:** 系统分析了LRMs的推理轨迹，利用LLM-Judge将轨迹分类为冗余推理和必要推理，并提出了OThink-R1方法来动态选择快速或慢速推理模式。

**结果:** 实验表明，OThink-R1平均减少了约23%的推理冗余，同时保持了模型的准确性。

**结论:** OThink-R1方法通过减少冗余推理步骤，在不降低准确率的前提下提升了推理模型的效率，为构建高效的推理模型提供了实用指南。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OThink-R1%3A+Intrinsic+Fast%2FSlow+Thinking+Mode+Switching+for+Over-Reasoning+Mitigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02397，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02397&send_immediately=true&force_search=false)

**原文摘要:** Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.

</details>


### [140] [VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/abs/2506.02456)
*Tri Cao, Bennett Lim, Yue Liu, Yuan Sui, Yuexin Li, Shumin Deng, Lin Lu, Nay Oo, Shuicheng Yan, Bryan Hooi*

**主要类别:** cs.AI

**AI概要:** 本论文研究了视觉提示注入（VPI）攻击对计算机使用代理（CUAs）和浏览器使用代理（BUAs）的影响，并提出了一种名为VPI-Bench的基准测试方法来评估代理在VPI威胁下的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 计算机使用代理（CUAs）具有完整的系统访问权限，能够实现强大的任务自动化，但由于其可以操作文件、访问用户数据和执行任意命令，也带来了重大的安全和隐私风险。虽然之前的研究主要集中在基于浏览器的代理和HTML级别的攻击上，但CUAs的漏洞仍未得到充分探索。

**方法:** 本文提出了一个包含306个测试用例的基准测试VPI-Bench，用于评估代理在VPI威胁下的鲁棒性。每个测试用例都是一个web平台的变体，设计为可交互的，在真实环境中部署，并包含视觉嵌入的恶意提示。

**结果:** 实验研究表明，当前的CUAs和BUAs在某些平台上分别最多有51%和100%的概率被欺骗。

**结论:** 研究发现，当前的CUAs和BUAs在某些平台上分别最多有51%和100%的概率被欺骗，并且系统提示防御只能提供有限的改进。这凸显了开发强大、情境感知的防御机制的必要性，以确保多模态AI代理在现实环境中的安全部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VPI-Bench%3A+Visual+Prompt+Injection+Attacks+for+Computer-Use+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02456，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02456&send_immediately=true&force_search=false)

**原文摘要:** Computer-Use Agents (CUAs) with full system access enable powerful task
automation but pose significant security and privacy risks due to their ability
to manipulate files, access user data, and execute arbitrary commands. While
prior work has focused on browser-based agents and HTML-level attacks, the
vulnerabilities of CUAs remain underexplored. In this paper, we investigate
Visual Prompt Injection (VPI) attacks, where malicious instructions are
visually embedded within rendered user interfaces, and examine their impact on
both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of
306 test cases across five widely used platforms, to evaluate agent robustness
under VPI threats. Each test case is a variant of a web platform, designed to
be interactive, deployed in a realistic environment, and containing a visually
embedded malicious prompt. Our empirical study shows that current CUAs and BUAs
can be deceived at rates of up to 51% and 100%, respectively, on certain
platforms. The experimental results also indicate that system prompt defenses
offer only limited improvements. These findings highlight the need for robust,
context-aware defenses to ensure the safe deployment of multimodal AI agents in
real-world environments. The code and dataset are available at:
https://github.com/cua-framework/agents

</details>


### [141] [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/abs/2506.02470)
*Xuejiao Zhao, Siyan Liu, Su-Yin Yang, Chunyan Miao*

**主要类别:** cs.AI

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Smart+Multimodal+Healthcare+Copilot+with+Powerful+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02470，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02470&send_immediately=true&force_search=false)

**原文摘要:** Misdiagnosis causes significant harm to healthcare systems worldwide, leading
to increased costs and patient risks. MedRAG is a smart multimodal healthcare
copilot equipped with powerful large language model (LLM) reasoning, designed
to enhance medical decision-making. It supports multiple input modalities,
including non-intrusive voice monitoring, general medical queries, and
electronic health records. MedRAG provides recommendations on diagnosis,
treatment, medication, and follow-up questioning. Leveraging
retrieval-augmented generation enhanced by knowledge graph-elicited reasoning,
MedRAG retrieves and integrates critical diagnostic insights, reducing the risk
of misdiagnosis. It has been evaluated on both public and private datasets,
outperforming existing models and offering more specific and accurate
healthcare assistance. A demonstration video of MedRAG is available at:
https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:
https://github.com/SNOWTEAM2023/MedRAG.

</details>


### [142] [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/abs/2506.02485)
*Haowen Xu, Sisi Zlatanova, Ruiyu Liang, Ismet Canbulat*

**主要类别:** cs.AI

**AI概要:** 本研究提倡使用生成式人工智能作为野火预测的基础框架，以克服传统模型的局限性并提升野火管理能力。


<details>
  <summary>更多</summary>
  
**动机:** 由于全球范围内的野火持续造成严重的人类、环境和经济损失，现有的方法在实时预测和可视化2D及3D空间域的多模态火势蔓延方面存在关键限制，因此需要寻找更有效的方法如生成式AI来改善野火预测与应急响应。

**方法:** 论文研究了基于物理模型和深度学习模型在野火模拟中的局限性，并引入生成式AI（包括GANs、VAEs、Transformer和扩散模型等）来改进预测和可视化多模态火势蔓延的方法。同时，利用大语言模型进行知识提取、文献综合和书目图谱分析。

**结果:** 该研究展示了生成式AI在集成多模态数据、生成不确定性下的多样化场景以及改进跨时空尺度的野火动态建模方面的优势，并提出五项关键愿景和三项主要挑战的潜在解决方案。

**结论:** 本文提出采用生成式人工智能作为野火预测的基础框架，探讨了其在2D火势蔓延预测和更真实的可扩展3D模拟中的应用，并提出了将生成式AI整合到野火管理中的五个关键愿景以及应对三个主要挑战的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+AI+for+Predicting+2D+and+3D+Wildfire+Spread%3A+Beyond+Physics-Based+Models+and+Traditional+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02485，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02485&send_immediately=true&force_search=false)

**原文摘要:** Wildfires continue to inflict devastating human, environmental, and economic
losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and
the urgent demand for more effective response strategies. While physics-based
and deep learning models have advanced wildfire simulation, they face critical
limitations in predicting and visualizing multimodal fire spread in real time,
particularly in both 2D and 3D spatial domains using dynamically updated GIS
data. These limitations hinder timely emergency response, infrastructure
protection, and community safety. Generative AI has recently emerged as a
transformative approach across research and industry. Models such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and
diffusion-based architectures offer distinct advantages over traditional
methods, including the integration of multimodal data, generation of diverse
scenarios under uncertainty, and improved modeling of wildfire dynamics across
spatial and temporal scales. This position paper advocates for the adoption of
generative AI as a foundational framework for wildfire prediction. We explore
how such models can enhance 2D fire spread forecasting and enable more
realistic, scalable 3D simulations. Additionally, we employ a novel human-AI
collaboration framework using large language models (LLMs) for automated
knowledge extraction, literature synthesis, and bibliometric mapping. Looking
ahead, we identify five key visions for integrating generative AI into wildfire
management: multimodal approaches, AI foundation models, conversational AI
systems, edge-computing-based scenario generation, and cognitive digital twins.
We also address three major challenges accompanying these opportunities and
propose potential solutions to support their implementation.

</details>


### [143] [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/abs/2506.02522)
*Xu Wan, Wenyue Xu, Chao Yang, Mingyang Sun*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种结合大语言模型（LLMs）和强化学习（RL）优势的框架ACE，用于解决大规模决策问题。


<details>
  <summary>更多</summary>
  
**动机:** LLMs在长序列实时决策上存在不足，而RL在巨大动作空间中样本效率低下，因此需要一种协同方法来弥补两者的缺陷。

**方法:** ACE通过一种双角色轨迹优化机制，将LLM作为策略执行者和价值评判者，同时利用RL代理通过优先经验回放生成高质量数据集以提升LLM性能。

**结果:** 在多个电力系统操作任务上的实验表明，ACE在超过60K离散动作空间的问题上优于现有RL和LLM方法。

**结论:** ACE框架成功融合了LLMs的推理能力和RL的数据效率，为解决大规模工业决策问题提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+Twice%2C+Act+Once%3A+A+Co-Evolution+Framework+of+LLM+and+RL+for+Large-Scale+Decision+Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02522，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02522&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models (LLMs) and Reinforcement
Learning (RL) have shown significant promise in decision-making tasks.
Nevertheless, for large-scale industrial decision problems, both approaches
face distinct challenges: LLMs lack real-time long-sequence decision-making
capabilities, while RL struggles with sample efficiency in vast action spaces.
To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic
framework between LLMs and RL agents for large-scale decision-making scenarios.
ACE introduces a dual-role trajectory refinement mechanism where LLMs act as
both Policy Actor and Value Critic during RL's training: the Actor refines
suboptimal actions via multi-step reasoning and environment validation, while
the Critic performs temporal credit assignment through trajectory-level reward
shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making
with high-quality fine-tuning datasets generated via prioritized experience
replay. Through extensive experiments across multiple power grid operation
challenges with action spaces exceeding 60K discrete actions, ACE demonstrates
superior performance over existing RL methods and LLM-based methods.

</details>


### [144] [Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine](https://arxiv.org/abs/2506.02565)
*Zhuoxuan Jiang, Tianyang Zhang, Peiyan Peng, Jing Chen, Yinong Xun, Haotian Zhang, Lichi Li, Yong Li, Shaohua Zhang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的基于符号演绎引擎的几何问题生成方法，能够高效地生成可读、可解和可控的几何问题。


<details>
  <summary>更多</summary>
  
**动机:** 生成高质量的几何问题是教育中的一个重要且具有挑战性的任务，相比数学文字题，几何问题更加强调多模态格式以及非正式语言与正式语言之间的转换。

**方法:** 提出了基于符号演绎引擎的几何问题生成框架(SDE-GPG)，包括四个步骤：搜索知识要点到扩展定义的映射表、采样扩展定义并进行符号演绎、过滤不合格问题、生成文本问题和图表。

**结果:** 实验结果表明，SDE-GPG可以有效生成可读性强、可解性和可控性强的几何问题。

**结论:** SDE-GPG框架能够有效地生成可读性强、可解性和可控性强的几何问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Generating+Controllable+and+Solvable+Geometry+Problem+by+Leveraging+Symbolic+Deduction+Engine，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02565，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02565&send_immediately=true&force_search=false)

**原文摘要:** Generating high-quality geometry problems is both an important and
challenging task in education. Compared to math word problems, geometry
problems further emphasize multi-modal formats and the translation between
informal and formal languages. In this paper, we introduce a novel task for
geometry problem generation and propose a new pipeline method: the Symbolic
Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The
framework leverages a symbolic deduction engine and contains four main steps:
(1) searching a predefined mapping table from knowledge points to extended
definitions, (2) sampling extended definitions and performing symbolic
deduction, (3) filtering out unqualified problems, and (4) generating textual
problems and diagrams. Specifically, our method supports to avoid inherent
biases in translating natural language into formal language by designing the
mapping table, and guarantees to control the generated problems in terms of
knowledge points and difficulties by an elaborate checking function. With
obtained formal problems, they are translated to natural language and the
accompanying diagrams are automatically drew by rule-based methods. We conduct
experiments using real-world combinations of knowledge points from two public
datasets. The results demonstrate that the SDE-GPG can effectively generate
readable, solvable and controllable geometry problems.

</details>


### [145] [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/abs/2506.02568)
*Dongzhe Fan, Yi Fang, Jiajin Liu, Djellel Difallah, Qiaoyu Tan*

**主要类别:** cs.AI

**AI概要:** 我们提出了一个创新模型MLaGA，它能够有效地将大型语言模型的能力扩展到处理复杂的图结构和多模态属性，这种方法在多个数据集上显示了出色的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管现实场景中普遍存在节点关联多种属性类型的多模态图，但现有的基于LLM的方法在这些图中的应用仍被探索不足。

**方法:** 设计了一个结构感知的多模态编码器，并通过联合图预训练目标对齐文本和视觉属性。此外，还实现了一种多模态指令调整方法，通过轻量级投影器无缝集成多模态特征和图结构。

**结果:** 广泛的实验表明，与领先基线方法相比，MLaGA在多个数据集上的多样图学习任务中表现出色，在监督和迁移学习场景下都实现了卓越性能。

**结论:** MLaGA是一个创新模型，能够有效地将大型语言模型的能力扩展到处理复杂的图结构和多模态属性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MLaGA%3A+Multimodal+Large+Language+and+Graph+Assistant，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02568，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02568&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated substantial efficacy in
advancing graph-structured data analysis. Prevailing LLM-based graph methods
excel in adapting LLMs to text-rich graphs, wherein node attributes are text
descriptions. However, their applications to multimodal graphs--where nodes are
associated with diverse attribute types, such as texts and images--remain
underexplored, despite their ubiquity in real-world scenarios. To bridge the
gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an
innovative model that adeptly extends LLM capabilities to facilitate reasoning
over complex graph structures and multimodal attributes. We first design a
structure-aware multimodal encoder to align textual and visual attributes
within a unified space through a joint graph pre-training objective.
Subsequently, we implement a multimodal instruction-tuning approach to
seamlessly integrate multimodal features and graph structures into the LLM
through lightweight projectors. Extensive experiments across multiple datasets
demonstrate the effectiveness of MLaGA compared to leading baseline methods,
achieving superior performance in diverse graph learning tasks under both
supervised and transfer learning scenarios.

</details>


### [146] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/abs/2506.02576)
*Haichen Wang, Liu Yang, Xinyuan Zhang, Haomin Yu, Ming Li, Jilin Hu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的时空预测模型ADFormer，通过结合高阶相关性和原始相关性，显著提升了城市交通需求预测的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的依赖启发式掩码策略的乘客需求预测方法无法完全适应复杂的时空相关性，忽略了现实世界中存在的高阶相关性，导致模型难以聚焦于正确的上下文。

**方法:** 提出了一种新的Aggregation Differential Transformer (ADFormer) 模型，该模型通过Differential Attention来捕捉原始空间相关性并实现注意力去噪，同时设计了基于时空特性的聚合策略，将原始相关性与高阶相关性统一起来。

**结果:** 在出租车和自行车数据集上的实验验证了ADFormer模型的有效性和高效性，代码已公开。

**结论:** 实验结果表明，ADFormer模型能够有效整合高阶相关性和原始相关性，从而全面捕捉时空关系，在出租车和自行车数据集上表现出良好的实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADFormer%3A+Aggregation+Differential+Transformer+for+Passenger+Demand+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02576，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02576&send_immediately=true&force_search=false)

**原文摘要:** Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [147] [V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving](https://arxiv.org/abs/2506.02580)
*Xuewen Luo, Fengze Yang, Fan Ding, Xiangbo Gao, Shuo Xing, Yang Zhou, Zhengzhong Tu, Chenxi Liu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为V2X-UniPool的新框架，该框架通过整合多模态V2X数据来提升自动驾驶系统的推理和运动规划能力，并能显著减少传输成本。


<details>
  <summary>更多</summary>
  
**动机:** 解决自动驾驶系统中由于单车辆传感器视野有限而导致的感知不足以及缺乏实时环境基础而产生的幻觉问题。

**方法:** 引入了一个基于双查询检索增强生成机制的V2X-UniPool框架，用于整合多模态车辆到万物(V2X)数据到一个时间索引和语言为基础的知识库中。

**结果:** 实验表明，V2X-UniPool显著提高了运动规划的准确性及推理能力，并且相比以前的V2X方法，同时减少了超过99.9%的传输成本。

**结论:** V2X-UniPool是一个统一的框架，能够集成多模态V2X数据，从而提高自动驾驶系统在运动规划和推理方面的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是V2X-UniPool%3A+Unifying+Multimodal+Perception+and+Knowledge+Reasoning+for+Autonomous+Driving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02580，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02580&send_immediately=true&force_search=false)

**原文摘要:** Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning
capabilities, but face two critical challenges: limited perception due to the
short-sightedness of single-vehicle sensors, and hallucination arising from the
lack of real-time environmental grounding. To address these issues, this paper
introduces V2X-UniPool, a unified framework that integrates multimodal
Vehicle-to-Everything (V2X) data into a time-indexed and language-based
knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)
mechanism, which enables retrieval of both static and dynamic knowledge, our
system enables ADs to perform accurate, temporally consistent reasoning over
both static environment and dynamic traffic context. Experiments on a
real-world cooperative driving dataset demonstrate that V2X-UniPool
significantly enhances motion planning accuracy and reasoning capability.
Remarkably, it enables even zero-shot vehicle-side models to achieve
state-of-the-art performance by leveraging V2X-UniPool, while simultaneously
reducing transmission cost by over 99.9\% compared to prior V2X methods.

</details>


### [148] [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/abs/2506.02594)
*Ruibo Duan, Yuxin Liu, Xinyao Dong, Chenglin Fan*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为EALG的新框架，利用大语言模型和对抗进化方法，自动化生成复杂组合优化问题及其求解器，显著提高了问题难度和求解效果。


<details>
  <summary>更多</summary>
  
**动机:** 为了提升组合优化求解器的评估和进步，需要生成更具挑战性的问题实例。

**方法:** 采用基于变异的对抗方法，动态进化实例生成过程，并通过大语言模型指导算法结构合成自适应启发式算法。

**结果:** 实验结果表明，EALG生成的实例比当前基准测试更加困难，其合成的求解器在各种组合任务中具有良好的泛化能力。

**结论:** 论文提出了一种新的组合优化框架EALG，能够自动协同进化问题实例和启发式求解器，通过与大语言模型的交互实现更高效的问题求解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EALG%3A+Evolutionary+Adversarial+Generation+of+Language+Model-Guided+Generators+for+Combinatorial+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02594，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02594&send_immediately=true&force_search=false)

**原文摘要:** Generating challenging instances is crucial for the evaluation and
advancement of combinatorial optimization solvers. In this work, we introduce
EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),
a novel framework that automates the co-evolution of optimization problem
instances and their corresponding heuristic solvers using large language models
(LLMs). EALG leverages a mutation-based adversarial approach that dynamically
evolves instance generation procedures to create increasingly difficult
problems, while simultaneously synthesizing adaptive heuristic algorithms
through interactions with LLMs guided by algorithmic structure. Unlike existing
approaches that focus solely on static benchmark creation or manual solver
design, EALG provides a seamless pipeline from instance generation to solver
synthesis. Experimental results demonstrate that EALG generates significantly
harder instances than current benchmarks, and its synthesized solvers
generalize effectively across a broad spectrum of combinatorial tasks. This
work explores a new paradigm for combinatorial optimization that integrates
instance generation with solver design, resulting in state-of-the-art
performance.

</details>


### [149] [A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting](https://arxiv.org/abs/2506.02609)
*Tianfan Jiang, Mei Wu, Wenchao Weng, Dewen Seng, Yiqian Lin*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的交通流量预测模型TEDDN，通过时间增强和数据解耦提升预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于交通数据的时间变化性和动态空间相关性，传统的时空网络难以处理多种交通流模式的数据依赖性，同时交通流量变化对时间信息敏感，而现有研究未能充分重视时间信息的重要性。

**方法:** 提出了一种名为TEDDN的新型网络结构，利用动态图和时间特征提取模块来灵活学习时间和节点信息，从而将复杂的交通数据解耦为稳定模式和趋势。

**结果:** 在四个真实世界数据集上的实验评估和消融研究验证了TEDDN方法的有效性和优越性。

**结论:** 该论文提出了一种新的交通流量预测方法，称为时间增强数据解耦网络（TEDDN），能够有效解决传统时空网络在处理多变交通流模式依赖性方面的不足，并通过实验验证了其优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Time-Enhanced+Data+Disentanglement+Network+for+Traffic+Flow+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02609&send_immediately=true&force_search=false)

**原文摘要:** In recent years, traffic flow prediction has become a highlight in the field
of intelligent transportation systems. However, due to the temporal variations
and dynamic spatial correlations of traffic data, traffic prediction remains
highly challenging.Traditional spatiotemporal networks, which rely on
end-to-end training, often struggle to handle the diverse data dependencies of
multiple traffic flow patterns. Additionally, traffic flow variations are
highly sensitive to temporal information changes. Regrettably, other
researchers have not sufficiently recognized the importance of temporal
information.To address these challenges, we propose a novel approach called A
Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting
(TEDDN). This network disentangles the originally complex and intertwined
traffic data into stable patterns and trends. By flexibly learning temporal and
node information through a dynamic graph enhanced by a temporal feature
extraction module, TEDDN demonstrates significant efficacy in disentangling and
extracting complex traffic information. Experimental evaluations and ablation
studies on four real-world datasets validate the superiority of our method.

</details>


### [150] [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/abs/2506.02648)
*Yue Yang, MingKang Chen, Qihua Liu, Mengkang Hu, Qiguang Chen, Gengrui Zhang, Shuyue Hu, Guangtao Zhai, Yu Qiao, Yu Wang, Wenqi Shao, Ping Luo*

**主要类别:** cs.AI

**AI概要:** 论文提出了DRE-Bench基准测试，用于评估大语言模型的流体智力，发现当前模型在复杂任务中的表现仍存在局限。


<details>
  <summary>更多</summary>
  
**动机:** 现有的推理基准测试要么关注领域特定知识，要么缺乏可解释性，因此需要一种新的方法来评估LLMs的流体智力。

**方法:** 提出DRE-Bench基准测试，基于分层认知框架设计了36个抽象推理任务，并评估多个最先进的LLMs的表现。

**结果:** 实验结果显示大多数LLMs在低层次认知任务中表现良好，但在高复杂度任务中的泛化能力有限。

**结论:** 当前的LLMs在低层次认知任务中表现出色，但在高层次认知和复杂任务中表现有限，表明它们与真正的人类流体智力仍有差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Truly+Assessing+Fluid+Intelligence+of+Large+Language+Models+through+Dynamic+Reasoning+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02648，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02648&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs) have demonstrated impressive
reasoning capacities that mirror human-like thinking. However, whether LLMs
possess genuine fluid intelligence (i.e., the ability to reason abstractly and
generalize rules in novel situations) remains an open question. Existing
reasoning benchmarks either focus on domain-specific knowledge (crystallized
intelligence) or lack interpretability. To address these limitations, we
propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a
hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning
tasks organized across four cognitive levels, with each task featuring multiple
dynamic variants that test the same underlying latent rule. This design enables
fine-grained, interpretable, and reliable assessments of fluid intelligence. We
evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,
Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).
Experimental results reveal that although most LLMs achieve competent and
robust performance in low-level cognition, they struggle with high-level
cognition and exhibit limited generalization as task complexity grows. Our
findings highlight the gap between current LLMs and true human-like fluid
intelligence and offer a new path for systematically tracking reasoning
progress in LLMs.

</details>


### [151] [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/abs/2506.02649)
*Yousef Emami, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida, Zhu Han*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了如何利用大语言模型提升公共安全无人机在紧急情况下的自主性和响应能力。


<details>
  <summary>更多</summary>
  
**动机:** 深度强化学习在训练复杂性、样本效率和仿真到现实的差距方面存在局限性，而大语言模型提供了一个有前景的替代方案。

**方法:** 将大语言模型集成到公共安全无人机中，并通过一个数据收集调度的案例研究进行验证。

**结果:** 实验结果表明，与传统方法相比，该框架能显著降低数据包丢失率，并减轻潜在的越狱漏洞风险。

**结论:** 本文提出了一种基于大语言模型（LLM）的上下文学习（ICL）框架，用于公共安全无人机（UAV）的路径规划和速度控制，以提高其在紧急情况下的自主性和响应能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Prompts+to+Protection%3A+Large+Language+Model-Enabled+In-Context+Learning+for+Smart+Public+Safety+UAV，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02649，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02649&send_immediately=true&force_search=false)

**原文摘要:** A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness
in emergency response. Its agility and ability to optimize mobility and
establish Line-of-Sight (LoS) communication make it increasingly vital for
managing emergencies such as disaster response, search and rescue, and wildfire
monitoring. While Deep Reinforcement Learning (DRL) has been applied to
optimize UAV navigation and control, its high training complexity, low sample
efficiency, and simulation-to-reality gap limit its practicality in public
safety. Recent advances in Large Language Models (LLMs) offer a compelling
alternative. With strong reasoning and generalization capabilities, LLMs can
adapt to new tasks through In-Context Learning (ICL), which enables task
adaptation via natural language prompts and example-based guidance, without
retraining. Deploying LLMs at the network edge, rather than in the cloud,
further reduces latency and preserves data privacy, thereby making them
suitable for real-time, mission-critical public safety UAVs. This paper
proposes the integration of LLM-enabled ICL with public safety UAV to address
the key functions, such as path planning and velocity control, in the context
of emergency response. We present a case study on data collection scheduling
where the LLM-enabled ICL framework can significantly reduce packet loss
compared to conventional approaches, while also mitigating potential
jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify
future research directions. The ICL framework enables adaptive, context-aware
decision-making for public safety UAV, thus offering a lightweight and
efficient solution for enhancing UAV autonomy and responsiveness in
emergencies.

</details>


### [152] [FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems](https://arxiv.org/abs/2506.02668)
*Frederico Metelo, Alexandre Oliveira, Stevo Racković, Pedro Ákos Costa, Cláudia Soares*

**主要类别:** cs.AI

**AI概要:** FAuNO是一种用于边缘计算系统中分散任务卸载的联邦异步网络编排器，通过使用联邦强化学习框架，其在降低任务损失和延迟方面优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的完全集中式编排存在延迟和资源瓶颈问题，而边缘计算需要更高效的任务卸载策略来应对日益增长的数据需求。

**方法:** 提出了一种基于actor-critic架构的联邦异步网络编排器（FAuNO），其中本地actors学习节点特定动态和对等交互，而联邦critic则跨代理聚合经验以促进高效协作。

**结果:** 实验表明，FAuNO在减少任务损失和延迟方面始终优于启发式和联邦多智能体RL基线方法。

**结论:** FAuNO展示了其在动态边缘计算场景中的适应性和优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FAuNO%3A+Semi-Asynchronous+Federated+Reinforcement+Learning+Framework+for+Task+Offloading+in+Edge+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02668&send_immediately=true&force_search=false)

**原文摘要:** Edge computing addresses the growing data demands of connected-device
networks by placing computational resources closer to end users through
decentralized infrastructures. This decentralization challenges traditional,
fully centralized orchestration, which suffers from latency and resource
bottlenecks. We present \textbf{FAuNO} -- \emph{Federated Asynchronous Network
Orchestrator} -- a buffered, asynchronous \emph{federated
reinforcement-learning} (FRL) framework for decentralized task offloading in
edge systems. FAuNO adopts an actor-critic architecture in which local actors
learn node-specific dynamics and peer interactions, while a federated critic
aggregates experience across agents to encourage efficient cooperation and
improve overall system performance. Experiments in the \emph{PeersimGym}
environment show that FAuNO consistently matches or exceeds heuristic and
federated multi-agent RL baselines in reducing task loss and latency,
underscoring its adaptability to dynamic edge-computing scenarios.

</details>


### [153] [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696)
*Jinyuan Luo, Zhen Fang, Yixuan Li, Seongheon Park, Ling Chen*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种新的框架Sample-Specific Prompting（SSP），通过分析扰动敏感性来改进大型语言模型中的自我评估，从而更准确地检测幻觉。


<details>
  <summary>更多</summary>
  
**动机:** 幻觉仍然是大型语言模型在现实世界问答任务中可靠部署的主要障碍。传统的自我评估策略依赖于模型自身的输出置信度来估计答案的事实准确性，但这可能并不总是准确的。

**方法:** 论文提出了一种新的框架Sample-Specific Prompting（SSP），通过分析扰动敏感性来改进自我评估。

**结果:** 实验表明，SSP在一系列幻觉检测基准测试中显著优于先前的方法。

**结论:** 论文得出结论，通过利用中间表示的动态行为，SSP能够实现更可靠的自我评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Shaking+to+Reveal%3A+Perturbation-Based+Detection+of+LLM+Hallucinations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02696&send_immediately=true&force_search=false)

**原文摘要:** Hallucination remains a key obstacle to the reliable deployment of large
language models (LLMs) in real-world question answering tasks. A widely adopted
strategy to detect hallucination, known as self-assessment, relies on the
model's own output confidence to estimate the factual accuracy of its answers.
However, this strategy assumes that the model's output distribution closely
reflects the true data distribution, which may not always hold in practice. As
bias accumulates through the model's layers, the final output can diverge from
the underlying reasoning process, making output-level confidence an unreliable
signal for hallucination detection. In this work, we propose Sample-Specific
Prompting (SSP), a new framework that improves self-assessment by analyzing
perturbation sensitivity at intermediate representations. These
representations, being less influenced by model bias, offer a more faithful
view of the model's latent reasoning process. Specifically, SSP dynamically
generates noise prompts for each input and employs a lightweight encoder to
amplify the changes in representations caused by the perturbation. A
contrastive distance metric is then used to quantify these differences and
separate truthful from hallucinated responses. By leveraging the dynamic
behavior of intermediate representations under perturbation, SSP enables more
reliable self-assessment. Extensive experiments demonstrate that SSP
significantly outperforms prior methods across a range of hallucination
detection benchmarks.

</details>


### [154] [The Limits of Predicting Agents from Behaviour](https://arxiv.org/abs/2506.02923)
*Alexis Bellot, Jonathan Richens, Tom Everitt*

**主要类别:** cs.AI

**AI概要:** 本论文研究了如何从AI代理的行为推断其信念和目标，并探讨了这些推断在新环境中的预测能力与限制。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI系统变得越来越复杂，对其行为进行解释对于安全部署至关重要。理解代理的行为动机可以帮助我们在新情境中做出合理预测。

**方法:** 通过假设代理的行为由一个世界模型引导，推导出新的理论界限来描述从行为数据中预测代理行为的极限。

**结果:** 提出了在新部署环境中代理行为的理论预测边界，并明确了从行为数据中推断代理信念的可靠性。

**结论:** 我们可以通过行为推断代理的信念，并使用这些信念在新环境中预测代理的行为，这对公平性和安全性研究有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Limits+of+Predicting+Agents+from+Behaviour，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02923&send_immediately=true&force_search=false)

**原文摘要:** As the complexity of AI systems and their interactions with the world
increases, generating explanations for their behaviour is important for safely
deploying AI. For agents, the most natural abstractions for predicting
behaviour attribute beliefs, intentions and goals to the system. If an agent
behaves as if it has a certain goal or belief, then we can make reasonable
predictions about how it will behave in novel situations, including those where
comprehensive safety evaluations are untenable. How well can we infer an
agent's beliefs from their behaviour, and how reliably can these inferred
beliefs predict the agent's behaviour in novel situations? We provide a precise
answer to this question under the assumption that the agent's behaviour is
guided by a world model. Our contribution is the derivation of novel bounds on
the agent's behaviour in new (unseen) deployment environments, which represent
a theoretical limit for predicting intentional agents from behavioural data
alone. We discuss the implications of these results for several research areas
including fairness and safety.

</details>


### [155] [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713)
*Xiaochong Lan, Jie Feng, Yizhou Sun, Chen Gao, Jiahuan Lei, Xinlei Shi, Hengliang Luo, Yong Li*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种新的生活需求预测系统PIGEON，通过将预测问题定义为开放集分类任务，结合大语言模型和用户行为分析，显著提升了个性化服务推荐的效果。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法将生活需求预测视为封闭集分类问题，严重限制了其捕捉生活需求多样性和复杂性的能力。因此，需要一种更灵活和准确的预测方法来提升个性化服务推荐的效果。

**方法:** 该研究提出了一个名为PIGEON的新系统，利用大语言模型（LLMs）进行无限制的需求预测。首先使用行为感知记录检索器帮助LLMs理解用户偏好，然后结合马斯洛需求层次理论使预测与人类生活需求保持一致。此外，还设计了一个基于微调文本嵌入模型的召回模块，并采用了指令调整技术以提升小LLMs的表现。

**结果:** 在真实数据集上的实验表明，PIGEON在基于需求的生活服务召回方面平均优于封闭集方法19.37%。人工评估验证了预测的合理性和具体性，同时指令调整技术使较小的LLMs也能实现具有竞争力的性能。

**结论:** PIGEON系统通过将生活需求预测重新定义为开放集分类问题，显著提高了对用户生活需求的预测能力，支持更有效的个性化服务推荐。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Open-Set+Living+Need+Prediction+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02713，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02713&send_immediately=true&force_search=false)

**原文摘要:** Living needs are the needs people generate in their daily lives for survival
and well-being. On life service platforms like Meituan, user purchases are
driven by living needs, making accurate living need predictions crucial for
personalized service recommendations. Traditional approaches treat this
prediction as a closed-set classification problem, severely limiting their
ability to capture the diversity and complexity of living needs. In this work,
we redefine living need prediction as an open-set classification problem and
propose PIGEON, a novel system leveraging large language models (LLMs) for
unrestricted need prediction. PIGEON first employs a behavior-aware record
retriever to help LLMs understand user preferences, then incorporates Maslow's
hierarchy of needs to align predictions with human living needs. For evaluation
and application, we design a recall module based on a fine-tuned text embedding
model that links flexible need descriptions to appropriate life services.
Extensive experiments on real-world datasets demonstrate that PIGEON
significantly outperforms closed-set approaches on need-based life service
recall by an average of 19.37%. Human evaluation validates the reasonableness
and specificity of our predictions. Additionally, we employ instruction tuning
to enable smaller LLMs to achieve competitive performance, supporting practical
deployment.

</details>


### [156] [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
*Xiaochong Lan, Jie Feng, Jiahuan Lei, Xinlei Shi, Yong Li*

**主要类别:** cs.AI

**AI概要:** 该研究探讨了大型语言模型在本地生活服务中的应用，通过建立基准测试和评估不同模型的表现，发现优化后的较小模型也能达到较大模型的效果，从而提升实际应用的可行性。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型在多个领域的广泛应用，研究其在本地生活服务领域中的潜力变得尤为重要。

**方法:** 建立全面的基准并系统评估不同LLM在各种与本地生活服务相关的任务中的表现，并探索了模型微调和基于代理的工作流程两种方法。

**结果:** 即使是相对紧凑的7B模型也能达到与更大72B模型相当的性能水平，有效平衡了推理成本和模型能力。

**结论:** 通过优化模型规模和采用基于代理的工作流程，大型语言模型在本地生活服务中的应用变得更加可行和高效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+and+Advancing+Large+Language+Models+for+Local+Life+Services，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02720&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have exhibited remarkable capabilities and
achieved significant breakthroughs across various domains, leading to their
widespread adoption in recent years. Building on this progress, we investigate
their potential in the realm of local life services. In this study, we
establish a comprehensive benchmark and systematically evaluate the performance
of diverse LLMs across a wide range of tasks relevant to local life services.
To further enhance their effectiveness, we explore two key approaches: model
fine-tuning and agent-based workflows. Our findings reveal that even a
relatively compact 7B model can attain performance levels comparable to a much
larger 72B model, effectively balancing inference cost and model capability.
This optimization greatly enhances the feasibility and efficiency of deploying
LLMs in real-world online services, making them more practical and accessible
for local life applications.

</details>


### [157] [Why do AI agents communicate in human language?](https://arxiv.org/abs/2506.02739)
*Pengcheng Zhou, Yinglun Feng, Halimulati Julaiti, Zhongliang Yang*

**主要类别:** cs.AI

**AI概要:** 论文指出当前大语言模型在多智能体系统中使用自然语言进行通信存在根本性局限，并提议开发新的模型范式以支持高效协调。


<details>
  <summary>更多</summary>
  
**动机:** 因为当前大语言模型虽然广泛用于AI代理系统，但它们并不是为了支持代理行为而设计的，这导致了协调上的根本限制。

**方法:** 通过分析现有系统的自然语言通信限制，并探讨大语言模型在高维向量空间中操作的本质，提出两个核心问题以引导未来研究方向。

**结果:** 揭示了自然语言与高维向量空间之间的结构性不匹配，以及标准范式无法支持强大的、可扩展的代理协调。

**结论:** 论文得出结论，现有的大语言模型并不适合支持多智能体系统中的协调和通信，并呼吁重新考虑如何训练能够原生支持结构化通信和任务对齐的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+do+AI+agents+communicate+in+human+language%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02739，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02739&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have become foundational to modern AI agent
systems, enabling autonomous agents to reason and plan. In most existing
systems, inter-agent communication relies primarily on natural language. While
this design supports interpretability and human oversight, we argue that it
introduces fundamental limitations in agent-to-agent coordination. The semantic
space of natural language is structurally misaligned with the high-dimensional
vector spaces in which LLMs operate, resulting in information loss and
behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper
architectural limitation: current LLMs were not trained with the objective of
supporting agentic behavior. As such, they lack mechanisms for modeling role
continuity, task boundaries, and multi-agent dependencies. The standard
next-token prediction paradigm fails to support the structural alignment
required for robust, scalable agent coordination. Based on this, we argue that
two core questions deserve careful examination: first, given that AI agents
fundamentally operate in high-dimensional vector spaces, should they rely on a
language system originally designed for human cognition as their communication
medium? Second, should we consider developing a new model construction paradigm
that builds models from the ground up to natively support structured
communication, shared intentionality, and task alignment in multi-role,
multi-agent environments? This paper calls for a reconsideration not only of
how agents should communicate, but also of what it fundamentally means to train
a model that natively supports multi-agent coordination and communication.

</details>


### [158] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
*Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng*

**主要类别:** cs.AI

**AI概要:** 本文探讨了图像生成模型中的机器遗忘问题，提出了新的分类框架CatIGMU、评估框架EvalIGMU和数据集DataIGM，揭示了现有算法在保留性和鲁棒性方面的不足。


<details>
  <summary>更多</summary>
  
**动机:** 图像生成模型广泛应用带来了隐私和内容安全问题，机器遗忘被视为一种解决这些挑战的有效方法。然而，IGMU在实践中仍面临诸多挑战，如缺乏有效的评估体系和明确的任务区分标准。

**方法:** 通过分析最先进的遗忘算法和评估标准，设计了一个层次化的任务分类框架CatIGMU、一个综合评估框架EvalIGMU，并构建了高质量数据集DataIGM。

**结果:** 发现了现有IGMU算法在多个评估维度上的缺陷；提出了新的分类框架、评估框架和数据集，以推动IGMU机制的理解和实用遗忘算法的设计。

**结论:** 论文提出了CatIGMU和EvalIGMU，分别用于IGMU任务的分类和评估，并构建了高质量的数据集DataIGM。研究发现现有算法在不同评估维度上表现不佳，尤其是在保留性和鲁棒性方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Machine+Unlearning+in+Image+Generation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02761&send_immediately=true&force_search=false)

**原文摘要:** With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [159] [Optimising the attribute order in Fuzzy Rough Rule Induction](https://arxiv.org/abs/2506.02805)
*Henri Bollaert, Chris Cornelis, Marko Palangetić, Salvatore Greco, Roman Słowiński*

**主要类别:** cs.AI

**AI概要:** 该论文研究了如何通过调整属性顺序和特征选择来改进基于模糊粗糙集的规则归纳算法FRRI，发现属性顺序优化效果有限，但加入特征选择可以提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究可解释性机器学习的重要性，以及在规则归纳中采用模糊粗糙集理论的方法来实现玻璃盒模型的目标。

**方法:** 通过已知的模糊粗糙集理论和经典机器学习方法优化属性顺序，并在过程中引入模糊粗糙特征选择以改进FRRI算法。

**结果:** 实验表明，仅优化属性顺序无法提高FRRI在多个指标上的表现，但结合模糊粗糙特征选择则能够改善平衡准确率和规则长度。

**结论:** 优化属性顺序本身并不能提升FRRI的性能，但在过程中使用模糊粗糙特征选择移除少量属性可以对平衡准确率和平均规则长度产生积极影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimising+the+attribute+order+in+Fuzzy+Rough+Rule+Induction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02805，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02805&send_immediately=true&force_search=false)

**原文摘要:** Interpretability is the next pivotal frontier in machine learning research.
In the pursuit of glass box models - as opposed to black box models, like
random forests or neural networks - rule induction algorithms are a logical and
promising avenue, as the rules can easily be understood by humans. In our
previous work, we introduced FRRI, a novel rule induction algorithm based on
fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed
other rule induction methods with regards to accuracy and number of rules. FRRI
leverages a fuzzy indiscernibility relation to partition the data space into
fuzzy granules, which are then combined into a minimal covering set of rules.
This indiscernibility relation is constructed by removing attributes from rules
in a greedy way. This raises the question: does the order of the attributes
matter? In this paper, we show that optimising only the order of attributes
using known methods from fuzzy rough set theory and classical machine learning
does not improve the performance of FRRI on multiple metrics. However, removing
a small number of attributes using fuzzy rough feature selection during this
step positively affects balanced accuracy and the average rule length.

</details>


### [160] [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/abs/2506.02838)
*Jizhou Wang, Xiaodan Fang, Lei Huang, Yongfeng Huang*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种名为TaxAgent的新颖税收解决方案，该方案将大型语言模型与基于代理的建模相结合，以设计适应性税收政策。


<details>
  <summary>更多</summary>
  
**动机:** 经济不平等是一个全球性挑战，传统系统如美国联邦所得税缺乏适应性。虽然萨伊最优税收等模型可以动态调整，但未能解决纳税人异质性和非理性行为。

**方法:** 将大型语言模型（LLMs）与基于代理的建模（ABM）结合，设计出适应性税收政策。

**结果:** 在宏观经济模拟中，TaxAgent实现了比萨伊最优税收、美国联邦所得税和自由市场更好的公平效率权衡。

**结论:** TaxAgent提供了一种新颖的税收解决方案和可扩展的数据驱动财政政策评估框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TaxAgent%3A+How+Large+Language+Model+Designs+Fiscal+Policy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02838，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02838&send_immediately=true&force_search=false)

**原文摘要:** Economic inequality is a global challenge, intensifying disparities in
education, healthcare, and social stability. Traditional systems like the U.S.
federal income tax reduce inequality but lack adaptability. Although models
like the Saez Optimal Taxation adjust dynamically, they fail to address
taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,
a novel integration of large language models (LLMs) with agent-based modeling
(ABM) to design adaptive tax policies. In our macroeconomic simulation,
heterogeneous H-Agents (households) simulate real-world taxpayer behaviors
while the TaxAgent (government) utilizes LLMs to iteratively optimize tax
rates, balancing equity and productivity. Benchmarked against Saez Optimal
Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves
superior equity-efficiency trade-offs. This research offers a novel taxation
solution and a scalable, data-driven framework for fiscal policy evaluation.

</details>


### [161] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865)
*Mathieu Andreux, Breno Baldas Skuk, Hamza Benchekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Matthias Brunel, Pierre-Louis Cedoz, Antoine Chassang, Mickaël Chen, Alexandra D. Constantinou, Antoine d'Andigné, Hubert de La Jonquière, Aurélien Delfosse, Ludovic Denoyer, Alexis Deprez, Augustin Derupti, Michael Eickenberg, Mathïs Federico, Charles Kantor, Xavier Koegler, Yann Labbé, Matthew C. H. Lee, Erwan Le Jumeau de Kergaradec, Amir Mahla, Avshalom Manevich, Adrien Maret, Charles Masson, Rafaël Maurin, Arturo Mena, Philippe Modard, Axel Moyal, Axel Nguyen Kerbel, Julien Revelle, Mats L. Richter, María Santos, Laurent Sifre, Maxime Theillard, Marc Thibault, Louis Thiry, Léo Tronchon, Nicolas Usunier, Tony Wu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种高效的网络代理Surfer-H及其配套模型Holo1，在用户界面任务中表现出色且具有成本效益。


<details>
  <summary>更多</summary>
  
**动机:** 为了提供一种成本效益高且准确的用户定义网络任务解决方案

**方法:** 开发了Surfer-H网络代理并集成了专门用于网页导航和信息提取的Holo1视觉语言模型（VLM）

**结果:** Surfer-H在WebVoyager上达到92.2%的最先进性能，同时在UI基准测试中表现优异

**结论:** Surfer-H结合Holo1在Web任务执行中实现了高效准确的表现，并通过开源推动代理系统研究的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Surfer-H+Meets+Holo1%3A+Cost-Efficient+Web+Agent+Powered+by+Open+Weights，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02865，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02865&send_immediately=true&force_search=false)

**原文摘要:** We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [162] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
*Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao*

**主要类别:** cs.AI

**AI概要:** 这篇论文通过信息论的视角分析大型推理模型（LRMs）的推理机制，发现了MI峰值现象以及其与推理准确性的关系，同时识别出关键的思考标记，并提出改善推理性能的有效方法。


<details>
  <summary>更多</summary>
  
**动机:** 论文的动机是探究大型推理模型（LRMs）内部的推理机制，尤其是通过信息论视角来理解其复杂问题解决能力。当前对于LRMs内部运作的理解仍然有限，因此作者希望通过追踪互信息（MI）的变化来揭示其中的关键机制。

**方法:** 该论文使用信息论的方法，追踪大型推理模型（LRM）在推理过程中中间表示与正确答案之间的互信息（MI）变化，分析MI峰值现象及其对模型预测误差的影响。同时，作者还分析了MI峰值对应的特定标记（称为思考标记），并基于这些分析提出了两种提升推理性能的方法。

**结果:** 研究结果包括：1. 发现了MI峰值现象，即在某些生成步骤中互信息显著增加；2. 理论分析表明，MI增加会导致预测错误概率下降；3. MI峰值通常对应于特定的思考标记；4. 实验证明这些思考标记对推理性能至关重要；5. 基于这些发现，作者提出了两种有效的推理性能改进方法。

**结论:** 论文得出的结论是，通过利用信息论的方法追踪大型推理模型（LRM）中间表示与正确答案之间的互信息（MI）变化，发现了MI峰值现象，并且这些MI峰值通常对应于表达反思或过渡的标记，例如“Hmm”，“Wait”和“Therefore”。此外，作者提出两种改进LRM推理性能的方法，并表明这项研究为理解LRMs的推理机制提供了新见解，并提供了提高其推理能力的实用方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying+Reasoning+Dynamics+with+Mutual+Information%3A+Thinking+Tokens+are+Information+Peaks+in+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02867，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02867&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


### [163] [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)
*Matthew Kowal, Jasper Timm, Jean-Francois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine*

**主要类别:** cs.AI

**AI概要:** 该论文介绍了Attempt to Persuade Eval (APE)基准，用于评估大型语言模型在有害情境下的说服尝试行为，揭示了现有安全机制的不足。


<details>
  <summary>更多</summary>
  
**动机:** 理解模型是否会盲目地在有害主题上尝试说服（例如美化加入恐怖组织）对于评估安全护栏的有效性至关重要。此外，了解模型在追求某些目标时进行说服行为的风险是至关重要的。

**方法:** 提出了Attempt to Persuade Eval (APE)基准，通过模拟说服者和被说服者代理之间的多轮对话设置来评估模型的说服尝试。引入了一个自动化评估模型来识别说服意愿并衡量其频率和背景。

**结果:** 发现多个前沿的大型语言模型在涉及阴谋论、有争议的问题以及明显有害的内容等话题上表现出频繁的说服尝试倾向。越狱行为会增加这种行为的可能性。

**结论:** 研究发现，许多开源和闭源模型在有害话题上经常愿意尝试说服，并且越狱行为可能会增加这种倾向。结果突显了当前安全防护措施的不足，并强调评估说服意愿作为LLM风险关键维度的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是It%27s+the+Thought+that+Counts%3A+Evaluating+the+Attempts+of+Frontier+LLMs+to+Persuade+on+Harmful+Topics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02873，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02873&send_immediately=true&force_search=false)

**原文摘要:** Persuasion is a powerful capability of large language models (LLMs) that both
enables beneficial applications (e.g. helping people quit smoking) and raises
significant risks (e.g. large-scale, targeted political manipulation). Prior
work has found models possess a significant and growing persuasive capability,
measured by belief changes in simulated or real users. However, these
benchmarks overlook a crucial risk factor: the propensity of a model to attempt
to persuade in harmful contexts. Understanding whether a model will blindly
``follow orders'' to persuade on harmful topics (e.g. glorifying joining a
terrorist group) is key to understanding the efficacy of safety guardrails.
Moreover, understanding if and when a model will engage in persuasive behavior
in pursuit of some goal is essential to understanding the risks from agentic AI
systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts
the focus from persuasion success to persuasion attempts, operationalized as a
model's willingness to generate content aimed at shaping beliefs or behavior.
Our evaluation framework probes frontier LLMs using a multi-turn conversational
setup between simulated persuader and persuadee agents. APE explores a diverse
spectrum of topics including conspiracies, controversial issues, and
non-controversially harmful content. We introduce an automated evaluator model
to identify willingness to persuade and measure the frequency and context of
persuasive attempts. We find that many open and closed-weight models are
frequently willing to attempt persuasion on harmful topics and that
jailbreaking can increase willingness to engage in such behavior. Our results
highlight gaps in current safety guardrails and underscore the importance of
evaluating willingness to persuade as a key dimension of LLM risk. APE is
available at github.com/AlignmentResearch/AttemptPersuadeEval

</details>


### [164] [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/abs/2506.02918)
*Shangmin Guo, Omar Darwiche Domingues, Raphaël Avalos, Aaron Courville, Florian Strub*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为DyMo的方法，该方法增强了大型语言模型的状态预测能力和函数调用能力，并提出了一种集成到自验证采样（SVS）中的内部环境模型，从而提高了模型在工具使用方面的有效性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 在状态环境中使用工具对大型语言模型（LLMs）提出了独特的挑战，因为现有的测试时间计算策略在环境中重复试验是不切实际的。

**方法:** 提出了DyMo（动态建模）和集成到自验证采样（SVS）中的内部环境模型，以增强LLM的状态预测能力和函数调用能力。

**结果:** 在Berkeley Function Calling Leaderboard V2上，DyMo提高了成功率并显著减少了幻觉。通过将内部环境模型集成到SVS中，显著改善了k次试验的通过率，并允许模型拒绝不可靠的输出。

**结论:** DyMo和SVS的结合大大提高了LLM在工具使用方面的有效性和可靠性，为无需反复查询Oracle环境的可扩展规划RL方法提供了路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample%2C+Predict%2C+then+Proceed%3A+Self-Verification+Sampling+for+Tool+Use+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02918，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02918&send_immediately=true&force_search=false)

**原文摘要:** Tool use in stateful environments presents unique challenges for large
language models (LLMs), where existing test-time compute strategies relying on
repeated trials in the environment are impractical. We propose dynamics
modelling (DyMo), a method that augments LLMs with a state prediction
capability alongside function calling during post-training. This enables LLMs
to predict the future states of their actions through an internal environment
model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success
rates and significantly reduces hallucinations. We further integrate the
internal environment model into self-verification sampling (SVS), and show that
this substantially improves pass^k over number of trials k, and allows the
model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the
effectiveness and reliability of LLMs for tool use. We believe this work charts
a path towards scalable planning RL methods for LLM inference without
repeatedly querying the oracle environment.

</details>


### [165] [Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing](https://arxiv.org/abs/2506.02949)
*Lixiang Xu, Xianwei Ding, Xin Yuan, Richang Hong, Feiping Nie, Enhong Chen, Philip S. Yu*

**主要类别:** cs.AI

**AI概要:** 本文提出CRDP-KT模型，通过动态规划算法优化认知表征，提高知识追踪的准确性和系统性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法忽视了认知表征的缺陷，导致预测偏差和建模成本增加。

**方法:** 采用动态规划算法优化认知表征，并通过加权融合优化记录表征和二分图学习的关系。

**结果:** 实验表明CRDP-KT模型在三个公开数据集上均有效。

**结论:** CRDP-KT模型通过优化认知表征，提高了知识追踪的准确性与系统性，实验证明了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Programming+Techniques+for+Enhancing+Cognitive+Representation+in+Knowledge+Tracing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02949&send_immediately=true&force_search=false)

**原文摘要:** Knowledge Tracing (KT) involves monitoring the changes in a student's
knowledge over time by analyzing their past responses, with the goal of
predicting future performance. However, most existing methods primarily focus
on feature enhancement, while overlooking the deficiencies in cognitive
representation and the ability to express cognition-issues often caused by
interference from non-cognitive factors such as slipping and guessing. This
limitation hampers the ability to capture the continuity and coherence of the
student's cognitive process. As a result, many methods may introduce more
prediction bias and modeling costs due to their inability to maintain cognitive
continuity and coherence. Based on the above discussion, we propose the
Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)
model. This model em ploys a dynamic programming algorithm to optimize
cognitive representations based on the difficulty of the questions and the
performance intervals between them. This approach ensures that the cognitive
representation aligns with the student's cognitive patterns, maintaining
overall continuity and coherence. As a result, it provides more accurate and
systematic input features for subsequent model training, thereby minimizing
distortion in the simulation of cognitive states. Additionally, the CRDP-KT
model performs partitioned optimization of cognitive representations to enhance
the reliability of the optimization process. Furthermore, it improves its
ability to express the student's cognition through a weighted fusion of
optimized record representations and re lationships learned from a bipartite
graph. Finally, experiments conducted on three public datasets validate the
effectiveness of the proposed CRDP-KT model.

</details>


### [166] [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
*Li Zhang, Kevin D. Ashley*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种基于多智能体的反思性方法，以提升法律论证生成的可靠性并减少大型语言模型的风险，结果显示其在多个关键指标上优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在法律论证生成中存在显著风险，例如幻觉和无根据的说服，以及未能有效利用提供的事实基础或在不可行的情况下选择不生成。论文旨在解决这些问题，并提高法律论证系统的可靠性和可信度。

**方法:** 论文采用一种迭代精炼过程，利用专门的代理（因素分析员和论点润色师）生成3-ply法律论证（原告、被告、反驳）。通过与单智能体、增强提示单智能体和非反思多智能体基线方法比较，评估了该方法的效果。

**结果:** 实验结果表明，反射多智能体方法在成功放弃生成（防止无根据论点生成）、减少幻觉错误（降低伪造和归因错误因素）以及提高因素使用回忆率（更好利用案件事实）方面显著优于基线方法，尤其是在“不可争辩”场景中。

**结论:** 论文得出结论，提出了一种新颖的反思多智能体方法，用于解决法律论证生成中的风险问题。这种方法在伦理说服和减少大语言模型操控方面提供了稳健的计算方法，是实现可信法律人工智能的关键一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Manipulation+and+Enhancing+Persuasion%3A+A+Reflective+Multi-Agent+Approach+for+Legal+Argument+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02992&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly explored for legal argument
generation, yet they pose significant risks of manipulation through
hallucination and ungrounded persuasion, and often fail to utilize provided
factual bases effectively or abstain when arguments are untenable. This paper
introduces a novel reflective multi-agent method designed to address these
challenges in the context of legally compliant persuasion. Our approach employs
specialized agents--a Factor Analyst and an Argument Polisher--in an iterative
refinement process to generate 3-ply legal arguments (plaintiff, defendant,
rebuttal). We evaluate Reflective Multi-Agent against single-agent,
enhanced-prompt single-agent, and non-reflective multi-agent baselines using
four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,
Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched",
and "non-arguable". Results demonstrate Reflective Multi-Agent's significant
superiority in successful abstention (preventing generation when arguments
cannot be grounded), marked improvements in hallucination accuracy (reducing
fabricated and misattributed factors), particularly in "non-arguable"
scenarios, and enhanced factor utilization recall (improving the use of
provided case facts). These findings suggest that structured reflection within
a multi-agent framework offers a robust computable method for fostering ethical
persuasion and mitigating manipulation in LLM-based legal argumentation
systems, a critical step towards trustworthy AI in law. Project page:
https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

</details>


### [167] [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/abs/2506.02996)
*Matthieu Tehenan, Christian Bolivar Moya, Tenghai Long, Guang Lin*

**主要类别:** cs.AI

**AI概要:** 研究表明大型语言模型能够隐式编码线性空间世界模型，并通过实验和因果干预验证了这一点。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在各种任务中展示了涌现能力，但它们是否获得了内部世界模型仍不清楚。

**方法:** 引入了空间世界模型的形式化框架，并使用合成数据集训练探测器来解码物体位置并评估基础空间的几何一致性，同时进行因果干预测试这些空间表示是否被模型功能性地使用。

**结果:** 实验结果提供了经验证据表明LLMs确实编码了线性空间世界模型。

**结论:** LLMs能够编码线性空间世界模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Linear+Spatial+World+Models+Emerge+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02996&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated emergent abilities across
diverse tasks, raising the question of whether they acquire internal world
models. In this work, we investigate whether LLMs implicitly encode linear
spatial world models, which we define as linear representations of physical
space and object configurations. We introduce a formal framework for spatial
world models and assess whether such structure emerges in contextual
embeddings. Using a synthetic dataset of object positions, we train probes to
decode object positions and evaluate geometric consistency of the underlying
space. We further conduct causal interventions to test whether these spatial
representations are functionally used by the model. Our results provide
empirical evidence that LLMs encode linear spatial world models.

</details>


### [168] [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/abs/2506.03032)
*Junhao Yu, Yan Zhuang, YuXuan Sun, Weibo Gao, Qi Liu, Mingyue Cheng, Zhenya Huang, Enhong Chen*

**主要类别:** cs.AI

**AI概要:** 提出了一种基于大语言模型的适应性测试代理TestAgent，提高了测试准确性并减少了问题数量。


<details>
  <summary>更多</summary>
  
**动机:** 当前的适应性测试方法存在机械化、猜测行为、开放性问题困难以及主观评估噪音数据等问题，需要改进适应性测试过程。

**方法:** 通过动态对话互动支持个性化问题选择，并捕捉应试者的反应和异常情况。

**结果:** 实验显示，TestAgent在心理、教育和生活方式评估中实现了比最先进的基线方法少20%的问题数且结果更准确，同时在速度和流畅度等方面受到测试者青睐。

**结论:** TestAgent是一种利用大语言模型增强适应性测试的交互式代理，相较于现有方法，它能够实现更准确的结果并减少问题数量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TestAgent%3A+An+Adaptive+and+Intelligent+Expert+for+Human+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03032&send_immediately=true&force_search=false)

**原文摘要:** Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.

</details>


### [169] [Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/abs/2506.03056)
*Ram Potham, Max Harms*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新方法，通过将基础模型的目标设定为增强人类控制，以解决AI能力增长带来的安全问题。


<details>
  <summary>更多</summary>
  
**动机:** 随着基础模型能力的增长，其潜在的安全风险增加，当前的价值对齐方法难以应对这种挑战。

**方法:** 提出“可修正性作为单一目标”（CAST）的方法，并设计实证研究议程以测试不同训练方法、模型规模和可控性示范的效果。

**结果:** 研究展示了一种使基础模型随着能力提升而变得更加响应人类指导的方法，从而避免失控的风险。

**结论:** 论文提出了一种新的范式，即通过增强人类对模型的控制来解决基础模型的能力与安全之间的矛盾。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Corrigibility+as+a+Singular+Target%3A+A+Vision+for+Inherently+Reliable+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03056，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03056&send_immediately=true&force_search=false)

**原文摘要:** Foundation models (FMs) face a critical safety challenge: as capabilities
scale, instrumental convergence drives default trajectories toward loss of
human control, potentially culminating in existential catastrophe. Current
alignment approaches struggle with value specification complexity and fail to
address emergent power-seeking behaviors. We propose "Corrigibility as a
Singular Target" (CAST)-designing FMs whose overriding objective is empowering
designated human principals to guide, correct, and control them. This paradigm
shift from static value-loading to dynamic human empowerment transforms
instrumental drives: self-preservation serves only to maintain the principal's
control; goal modification becomes facilitating principal guidance. We present
a comprehensive empirical research agenda spanning training methodologies
(RLAIF, SFT, synthetic data generation), scalability testing across model
sizes, and demonstrations of controlled instructability. Our vision: FMs that
become increasingly responsive to human guidance as capabilities grow, offering
a path to beneficial AI that remains as tool-like as possible, rather than
supplanting human judgment. This addresses the core alignment problem at its
source, preventing the default trajectory toward misaligned instrumental
convergence.

</details>


### [170] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)
*Man Luo, David Cobbley, Xin Su, Shachar Rosenman, Vasudev Lal, Shao-Yen Tseng, Phillip Howard*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一个轻量级的视觉语言模型，它完全在本地机器上运行，旨在解决隐私和资源效率问题，并通过一种新的训练方法提高了性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前计算机使用代理通常依赖于基于云的推理，这引发了隐私和可扩展性问题，尤其是在个人设备上运行时。

**方法:** 该论文提出了一种LLM-as-Judge框架，用于自动评估和过滤合成交互轨迹，生成高质量数据以进行强化学习。

**结果:** 实验结果表明，在OS-World基准测试中，经过微调的本地模型优于现有的基线模型。

**结论:** 论文得出结论，开发出的轻量级视觉语言模型在本地机器上运行良好，为私人、高效和可推广的GUI代理提供了一条有希望的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DPO+Learning+with+LLMs-Judge+Signal+for+Computer+Use+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03095&send_immediately=true&force_search=false)

**原文摘要:** Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [171] [Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps](https://arxiv.org/abs/2506.02254)
*Dimitris G Giovanis, Nikolaos Evangelou, Ioannis G Kevrekidis, Roger G Ghanem*

**主要类别:** stat.ML

**AI概要:** This paper introduces an improved generative learning framework that extends the Probabilistic Learning on Manifolds (PLoM) approach by incorporating Double Diffusion Maps and Geometric Harmonics, addressing overfitting issues in small datasets while preserving system dynamics.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this research is to overcome the limitations of the original PLoM method, particularly when the number of available data points is small and the dimensionality of the diffusion-map basis approaches the number of data points, leading to overfitting and loss of generalization.

**方法:** The paper proposes an extension to the Probabilistic Learning on Manifolds (PLoM) approach by integrating Double Diffusion Maps and Geometric Harmonics. The new method aims to solve a full-order Ito Stochastic Differential Equation (ISDE) directly in the latent space while preserving the system's dynamical complexity and leveraging its reduced geometric representation.

**结果:** The effectiveness and robustness of the proposed method are demonstrated through two numerical studies: one based on synthetic data generated from two-dimensional Hermite polynomial functions and another based on high-fidelity simulations of a detonation wave in a reactive flow.

**结论:** The proposed extension of PLoM, which combines Double Diffusion Maps with Geometric Harmonics, effectively addresses the overfitting and generalization issues when dealing with small datasets. This method enhances the original framework by capturing multiscale geometric features and enabling smooth nonlinear interpolation in high-dimensional spaces, allowing for a more accurate and robust sampling process.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enabling+Probabilistic+Learning+on+Manifolds+through+Double+Diffusion+Maps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02254&send_immediately=true&force_search=false)

**原文摘要:** We present a generative learning framework for probabilistic sampling based
on an extension of the Probabilistic Learning on Manifolds (PLoM) approach,
which is designed to generate statistically consistent realizations of a random
vector in a finite-dimensional Euclidean space, informed by a limited (yet
representative) set of observations. In its original form, PLoM constructs a
reduced-order probabilistic model by combining three main components: (a)
kernel density estimation to approximate the underlying probability measure,
(b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure,
and (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample
from the learned distribution. A key challenge arises, however, when the number
of available data points N is small and the dimensionality of the diffusion-map
basis approaches N, resulting in overfitting and loss of generalization. To
overcome this limitation, we propose an enabling extension that implements a
synthesis of Double Diffusion Maps -- a technique capable of capturing
multiscale geometric features of the data -- with Geometric Harmonics (GH), a
nonparametric reconstruction method that allows smooth nonlinear interpolation
in high-dimensional ambient spaces. This approach enables us to solve a
full-order ISDE directly in the latent space, preserving the full dynamical
complexity of the system, while leveraging its reduced geometric
representation. The effectiveness and robustness of the proposed method are
illustrated through two numerical studies: one based on data generated from
two-dimensional Hermite polynomial functions and another based on high-fidelity
simulations of a detonation wave in a reactive flow.

</details>


### [172] [Assumption-free stability for ranking problems](https://arxiv.org/abs/2506.02257)
*Ruiting Liang, Jake A. Soloff, Rina Foygel Barber, Rebecca Willett*

**主要类别:** stat.ML

**AI概要:** 本研究解决了排名问题中的不稳定性，提出了两种新算法以在噪声数据中实现稳定且具有信息性的排名。


<details>
  <summary>更多</summary>
  
**动机:** 解决从噪声数据中估计排名时存在的不稳定性问题，尤其是在现实世界数据中得分相近的情况下。

**方法:** 开发了一种新的排名问题稳定性框架，包括“膨胀的top-k”和“膨胀的完整排名”。

**结果:** 提出的方法在真实世界数据上进行了验证，结果表明这些方法能够在不损害输出信息性的情况下提供稳定性。

**结论:** 本文提出了两种新的排名算子，用于实现稳定的排名，并通过实验验证了其在不依赖数据分布和候选数量的情况下提供稳定性的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assumption-free+stability+for+ranking+problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02257，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02257&send_immediately=true&force_search=false)

**原文摘要:** In this work, we consider ranking problems among a finite set of candidates:
for instance, selecting the top-$k$ items among a larger list of candidates or
obtaining the full ranking of all items in the set. These problems are often
unstable, in the sense that estimating a ranking from noisy data can exhibit
high sensitivity to small perturbations. Concretely, if we use data to provide
a score for each item (say, by aggregating preference data over a sample of
users), then for two items with similar scores, small fluctuations in the data
can alter the relative ranking of those items. Many existing theoretical
results for ranking problems assume a separation condition to avoid this
challenge, but real-world data often contains items whose scores are
approximately tied, limiting the applicability of existing theory. To address
this gap, we develop a new algorithmic stability framework for ranking
problems, and propose two novel ranking operators for achieving stable ranking:
the \emph{inflated top-$k$} for the top-$k$ selection problem and the
\emph{inflated full ranking} for ranking the full list. To enable stability,
each method allows for expressing some uncertainty in the output. For both of
these two problems, our proposed methods provide guaranteed stability, with no
assumptions on data distributions and no dependence on the total number of
candidates to be ranked. Experiments on real-world data confirm that the
proposed methods offer stability without compromising the informativeness of
the output.

</details>


### [173] [MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements](https://arxiv.org/abs/2506.02260)
*Howon Ryu, Yuliang Chen, Yacun Wang, Andrea Z. LaCroix, Chongzhi Di, Loki Natarajan, Yu Wang, Jingjing Zou*

**主要类别:** stat.ML

**AI概要:** MoCA是一种自我监督的学习框架，利用跨模态掩码和Transformer自动编码器架构来处理多模式数据，无需大量标记数据。


<details>
  <summary>更多</summary>
  
**动机:** 当前的数字健康技术方法主要依赖于监督学习，需要广泛标注的数据集，这在临床研究中往往是昂贵或不切实际的。

**方法:** 提出了一种名为Multi-modal Cross-masked Autoencoder (MoCA) 的自我监督学习框架，该框架利用了跨模态掩码和Transformer自动编码器架构，以利用模态内的时序相关性和数据流之间的跨模态相关性。

**结果:** 全面的实验和消融研究表明，该方法在重构和下游任务方面均优于现有方法。

**结论:** 这项工作强调了自我监督学习在数字健康和多模态数据中的变革潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoCA%3A+Multi-modal+Cross-masked+Autoencoder+for+Digital+Health+Measurements，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02260&send_immediately=true&force_search=false)

**原文摘要:** The growing prevalence of digital health technologies has led to the
generation of complex multi-modal data, such as physical activity measurements
simultaneously collected from various sensors of mobile and wearable devices.
These data hold immense potential for advancing health studies, but current
methods predominantly rely on supervised learning, requiring extensive labeled
datasets that are often expensive or impractical to obtain, especially in
clinical studies. To address this limitation, we propose a self-supervised
learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that
leverages cross-modality masking and the Transformer autoencoder architecture
to utilize both temporal correlations within modalities and cross-modal
correlations between data streams. We also provide theoretical guarantees to
support the effectiveness of the cross-modality masking scheme in MoCA.
Comprehensive experiments and ablation studies demonstrate that our method
outperforms existing approaches in both reconstruction and downstream tasks. We
release open-source code for data processing, pre-training, and downstream
tasks in the supplementary materials. This work highlights the transformative
potential of self-supervised learning in digital health and multi-modal data.

</details>


### [174] [Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression](https://arxiv.org/abs/2506.02336)
*Jingfeng Wu, Pierre Marion, Peter Bartlett*

**主要类别:** stat.ML

**AI概要:** 该论文发现，在ℓ2正则化逻辑回归中，使用大步长梯度下降可以显著提高收敛速度，从而改进现有方法的效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统理论建议使用小步长以确保优化目标的单调减少，但作者试图探索是否可以通过大步长实现加速。

**方法:** 研究采用常数步长的梯度下降法对线性可分数据进行ℓ2正则化逻辑回归分析。

**结果:** 研究表明，通过使用大步长，收敛速度可以从O(κ)加快到O(√κ)，并证明了其在种群风险最小化中的有效性。

**结论:** 论文得出使用大步长梯度下降可以加速ℓ2正则化逻辑回归的收敛速度，并扩展了Wu等人(2024)的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Stepsizes+Accelerate+Gradient+Descent+for+Regularized+Logistic+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02336，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02336&send_immediately=true&force_search=false)

**原文摘要:** We study gradient descent (GD) with a constant stepsize for
$\ell_2$-regularized logistic regression with linearly separable data.
Classical theory suggests small stepsizes to ensure monotonic reduction of the
optimization objective, achieving exponential convergence in
$\widetilde{\mathcal{O}}(\kappa)$ steps with $\kappa$ being the condition
number. Surprisingly, we show that this can be accelerated to
$\widetilde{\mathcal{O}}(\sqrt{\kappa})$ by simply using a large stepsize --
for which the objective evolves nonmonotonically. The acceleration brought by
large stepsizes extends to minimizing the population risk for separable
distributions, improving on the best-known upper bounds on the number of steps
to reach a near-optimum. Finally, we characterize the largest stepsize for the
local convergence of GD, which also determines the global convergence in
special scenarios. Our results extend the analysis of Wu et al. (2024) from
convex settings with minimizers at infinity to strongly convex cases with
finite minimizers.

</details>


### [175] [Tensor State Space-based Dynamic Multilayer Network Modeling](https://arxiv.org/abs/2506.02413)
*Tian Lan, Jie Guo, Chen Zhang*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新颖的动态多层网络分析模型（TSSDMN），结合了张量分解与变分推断方法，成功揭示了复杂网络的时空动态特性。


<details>
  <summary>更多</summary>
  
**动机:** 现有模型难以捕捉动态多层网络的时间和跨层动态特性，需要更精确的方法。

**方法:** 引入了一种新的张量状态空间模型，利用对称Tucker分解和平均场变分推断方法进行模型推理。

**结果:** 数值模拟和案例研究表明，TSSDMN在分析动态多层网络方面表现出色。

**结论:** TSSDMN模型被证明能够有效理解和分析动态多层网络的复杂交互。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tensor+State+Space-based+Dynamic+Multilayer+Network+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02413&send_immediately=true&force_search=false)

**原文摘要:** Understanding the complex interactions within dynamic multilayer networks is
critical for advancements in various scientific domains. Existing models often
fail to capture such networks' temporal and cross-layer dynamics. This paper
introduces a novel Tensor State Space Model for Dynamic Multilayer Networks
(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric
Tucker decomposition to represent latent node features, their interaction
patterns, and layer transitions. Then by fixing the latent features and
allowing the interaction patterns to evolve over time, TSSDMN uniquely captures
both the temporal dynamics within layers and across different layers. The model
identifiability conditions are discussed. By treating latent features as
variables whose posterior distributions are approximated using a mean-field
variational inference approach, a variational Expectation Maximization
algorithm is developed for efficient model inference. Numerical simulations and
case studies demonstrate the efficacy of TSSDMN for understanding dynamic
multilayer networks.

</details>


### [176] [Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks](https://arxiv.org/abs/2506.02651)
*Luca Arnaboldi, Bruno Loureiro, Ludovic Stephan, Florent Krzakala, Lenka Zdeborova*

**主要类别:** stat.ML

**AI概要:** 这篇论文研究了随机梯度下降在处理一种称为序列单指数（SSI）模型的问题时的动力学特性，这些问题涉及到数据中的序列结构如何有助于基于注意力模型的学习。通过分析，作者揭示了训练过程的两个关键阶段，并展示了序列结构的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 该研究旨在将经典的单索引模型推广到序列领域，以更好地理解和优化基于注意力的简化单层架构的学习过程。

**方法:** 研究者分析了随机梯度下降（SGD）的动力学特性，并推导出一种闭合形式的总体损失表达式。

**结果:** 研究揭示了训练过程中存在的两个不同阶段：从无信息的初始状态中逃离和与目标子空间对齐，并展示了序列长度和位置编码如何影响收敛速度和学习轨迹。

**结论:** 论文得出，数据中的序列结构对于基于注意力模型的学习是有益的，并提供了理解这种益处的严格且可解释的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotics+of+SGD+in+Sequence-Single+Index+Models+and+Single-Layer+Attention+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02651&send_immediately=true&force_search=false)

**原文摘要:** We study the dynamics of stochastic gradient descent (SGD) for a class of
sequence models termed Sequence Single-Index (SSI) models, where the target
depends on a single direction in input space applied to a sequence of tokens.
This setting generalizes classical single-index models to the sequential
domain, encompassing simplified one-layer attention architectures. We derive a
closed-form expression for the population loss in terms of a pair of sufficient
statistics capturing semantic and positional alignment, and characterize the
induced high-dimensional SGD dynamics for these coordinates. Our analysis
reveals two distinct training phases: escape from uninformative initialization
and alignment with the target subspace, and demonstrates how the sequence
length and positional encoding influence convergence speed and learning
trajectories. These results provide a rigorous and interpretable foundation for
understanding how sequential structure in data can be beneficial for learning
with attention-based models.

</details>


### [177] [Computational Thresholds in Multi-Modal Learning via the Spiked Matrix-Tensor Model](https://arxiv.org/abs/2506.02664)
*Hugo Tabanelli, Pierre Mergny, Lenka Zdeborova, Florent Krzakala*

**主要类别:** stat.ML

**AI概要:** 这篇论文探讨了从两个相关模态中恢复多个高维信号的问题，提出了一种有效的顺序课程学习策略，并揭示了多模态高维推理中的结构性关联和学习顺序的关键作用。


<details>
  <summary>更多</summary>
  
**动机:** 研究者旨在解决多个高维信号从两个噪声、相关的模态中恢复的问题，尤其是当这些模态共享一个共同的低秩结构时。

**方法:** 该论文采用了贝叶斯近似消息传递(Bayesian Approximate Message Passing)和经验风险最小化(Empirical Risk Minimization)等方法进行信号恢复，并提出了一种顺序课程学习(Sequential Curriculum Learning)策略。

**结果:** 论文结果显示，虽然在低信噪比下传统的张量模型通常是难以处理的，但通过贝叶斯近似消息传递可以实现高效的恢复，并且顺序课程学习策略优于联合学习方法。

**结论:** 论文得出结论，多模态学习中结构相关性和学习顺序的重要性，以及Sequential Curriculum Learning策略能够实现最佳的弱恢复阈值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computational+Thresholds+in+Multi-Modal+Learning+via+the+Spiked+Matrix-Tensor+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02664&send_immediately=true&force_search=false)

**原文摘要:** We study the recovery of multiple high-dimensional signals from two noisy,
correlated modalities: a spiked matrix and a spiked tensor sharing a common
low-rank structure. This setting generalizes classical spiked matrix and tensor
models, unveiling intricate interactions between inference channels and
surprising algorithmic behaviors. Notably, while the spiked tensor model is
typically intractable at low signal-to-noise ratios, its correlation with the
matrix enables efficient recovery via Bayesian Approximate Message Passing,
inducing staircase-like phase transitions reminiscent of neural network
phenomena. In contrast, empirical risk minimization for joint learning fails:
the tensor component obstructs effective matrix recovery, and joint
optimization significantly degrades performance, highlighting the limitations
of naive multi-modal learning. We show that a simple Sequential Curriculum
Learning strategy-first recovering the matrix, then leveraging it to guide
tensor recovery-resolves this bottleneck and achieves optimal weak recovery
thresholds. This strategy, implementable with spectral methods, emphasizes the
critical role of structural correlation and learning order in multi-modal
high-dimensional inference.

</details>


### [178] [Symmetry-Aware GFlowNets](https://arxiv.org/abs/2506.02685)
*Hohyun Kim, Seunggeun Lee, Min-hwan Oh*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的生成流网络方法(SA-GFN)，能够解决现有方法中存在的系统偏差问题，实现无偏采样和高奖励图形的生成。


<details>
  <summary>更多</summary>
  
**动机:** 现有的生成流网络(GFlowNets)方法由于状态转移概率计算中的不准确性而存在系统偏差，这些偏差源于图的内在对称性，并影响了基于原子和基于片段的生成方案。

**方法:** 引入了一种名为Symmetry-Aware GFlowNets (SA-GFN)的方法，该方法通过奖励缩放将对称性校正纳入学习过程中。

**结果:** 经验结果表明，SA-GFN能够在增强多样性的同时持续生成高奖励的图形，并且紧密匹配目标分布，从而实现无偏采样。

**结论:** Symmetry-Aware GFlowNets (SA-GFN)通过将对称性校正集成到奖励结构中，消除了显式状态转移计算的需要，并实现了无偏采样。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Symmetry-Aware+GFlowNets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02685，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02685&send_immediately=true&force_search=false)

**原文摘要:** Generative Flow Networks (GFlowNets) offer a powerful framework for sampling
graphs in proportion to their rewards. However, existing approaches suffer from
systematic biases due to inaccuracies in state transition probability
computations. These biases, rooted in the inherent symmetries of graphs, impact
both atom-based and fragment-based generation schemes. To address this
challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that
incorporates symmetry corrections into the learning process through reward
scaling. By integrating bias correction directly into the reward structure,
SA-GFN eliminates the need for explicit state transition computations.
Empirical results show that SA-GFN enables unbiased sampling while enhancing
diversity and consistently generating high-reward graphs that closely match the
target distribution.

</details>


### [179] [Safely Learning Controlled Stochastic Dynamics](https://arxiv.org/abs/2506.02754)
*Luc Brogat-Motte, Alessandro Rudi, Riccardo Bonalli*

**主要类别:** stat.ML

**AI概要:** 本文介绍了一种确保安全探索和高效估计系统动力学的方法，通过迭代扩展初始安全控制集，在多个领域具有广泛应用前景。


<details>
  <summary>更多</summary>
  
**动机:** 在自主机器人、金融和生物医学等安全至关重要的应用中，需要确保系统在训练和部署期间始终保持在预定义的安全区域内。

**方法:** 使用基于核的置信界来迭代扩展初始安全控制集，从而确保安全探索并有效估计系统动力学。

**结果:** 实验评估表明，该方法在安全性、估计准确性和计算效率方面具有实际有效性，并提供了理论上的安全性保证和自适应学习率。

**结论:** 论文提出了一种方法，用于在确保安全的前提下从离散时间轨迹观测中学习受控随机动力学。该方法通过迭代扩展初始已知的安全控制集来实现安全探索和高效估计系统动力学。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safely+Learning+Controlled+Stochastic+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02754，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02754&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of safely learning controlled stochastic dynamics from
discrete-time trajectory observations, ensuring system trajectories remain
within predefined safe regions during both training and deployment.
Safety-critical constraints of this kind are crucial in applications such as
autonomous robotics, finance, and biomedicine. We introduce a method that
ensures safe exploration and efficient estimation of system dynamics by
iteratively expanding an initial known safe control set using kernel-based
confidence bounds. After training, the learned model enables predictions of the
system's dynamics and permits safety verification of any given control. Our
approach requires only mild smoothness assumptions and access to an initial
safe control set, enabling broad applicability to complex real-world systems.
We provide theoretical guarantees for safety and derive adaptive learning rates
that improve with increasing Sobolev regularity of the true dynamics.
Experimental evaluations demonstrate the practical effectiveness of our method
in terms of safety, estimation accuracy, and computational efficiency.

</details>


### [180] [Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings](https://arxiv.org/abs/2506.02793)
*Houssam Zenati, Bariscan Bozkurt, Arthur Gretton*

**主要类别:** stat.ML

**AI概要:** 该研究提出了CPME框架，用于有效估计和测试反事实结果分布，为决策提供更可靠的支持。


<details>
  <summary>更多</summary>
  
**动机:** 在推荐、广告和医疗等领域，决策需要基于反事实策略下的结果分布估计，而现有方法存在局限性。

**方法:** 使用再生核希尔伯特空间(RKHS)表示整个反事实结果分布，并引入插件估计器和双重稳健估计器。

**结果:** 提出的CPME框架实现了灵活的非参数分布离线策略评估，并展示了比现有方法更优的性能。

**结论:** 论文提出了一种新的框架CPME，用于表示反事实结果分布，并通过假设检验和抽样支持进一步分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Doubly-Robust+Estimation+of+Counterfactual+Policy+Mean+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02793，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02793&send_immediately=true&force_search=false)

**原文摘要:** Estimating the distribution of outcomes under counterfactual policies is
critical for decision-making in domains such as recommendation, advertising,
and healthcare. We analyze a novel framework-Counterfactual Policy Mean
Embedding (CPME)-that represents the entire counterfactual outcome distribution
in a reproducing kernel Hilbert space (RKHS), enabling flexible and
nonparametric distributional off-policy evaluation. We introduce both a plug-in
estimator and a doubly robust estimator; the latter enjoys improved uniform
convergence rates by correcting for bias in both the outcome embedding and
propensity models. Building on this, we develop a doubly robust kernel test
statistic for hypothesis testing, which achieves asymptotic normality and thus
enables computationally efficient testing and straightforward construction of
confidence intervals. Our framework also supports sampling from the
counterfactual distribution. Numerical simulations illustrate the practical
benefits of CPME over existing methods.

</details>


### [181] [Asymptotically perfect seeded graph matching without edge correlation (and applications to inference)](https://arxiv.org/abs/2506.02825)
*Tong Qi, Vera Andersson, Peter Viechnicki, Vince Lyzinski*

**主要类别:** stat.ML

**AI概要:** OmniMatch是一种用于多图匹配的新算法，可以在没有边相关性的情况下有效对齐多个网络中的大量未标记顶点，并通过校正错位顶点来恢复假设检验的检验力。


<details>
  <summary>更多</summary>
  
**动机:** 在存在顶点错位/洗牌的情况下，图假设检验的检验力会受到影响，因此需要一种方法来纠正这些错位并恢复检验力。

**方法:** 通过证明，在d维随机点积图（RDPG）设定下，OmniMatch算法能够利用s个种子节点对O(s^α)个未标记节点进行对齐，并将其应用于模拟实验和实际数据案例中。

**结果:** 论文展示了OmniMatch算法在多个模拟实验和两个实际应用（连接组学和机器翻译）中的有效性，并表明其能显著恢复因错位而丢失的测试能力。

**结论:** OmniMatch算法即使在没有边相关性的情况下也能有效地对多个网络中的大量未标记顶点进行完美对齐，并且能够在测试前校正错位的顶点以恢复丢失的测试能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotically+perfect+seeded+graph+matching+without+edge+correlation+%28and+applications+to+inference%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02825&send_immediately=true&force_search=false)

**原文摘要:** We present the OmniMatch algorithm for seeded multiple graph matching. In the
setting of $d$-dimensional Random Dot Product Graphs (RDPG), we prove that
under mild assumptions, OmniMatch with $s$ seeds asymptotically and efficiently
perfectly aligns $O(s^{\alpha})$ unseeded vertices -- for $\alpha<2\wedge d/4$
-- across multiple networks even in the presence of no edge correlation. We
demonstrate the effectiveness of our algorithm across numerous simulations and
in the context of shuffled graph hypothesis testing. In the shuffled testing
setting, testing power is lost due to the misalignment/shuffling of vertices
across graphs, and we demonstrate the capacity of OmniMatch to correct for
misaligned vertices prior to testing and hence recover the lost testing power.
We further demonstrate the algorithm on a pair of data examples from
connectomics and machine translation.

</details>


### [182] [Non-stationary Bandit Convex Optimization: A Comprehensive Study](https://arxiv.org/abs/2506.02980)
*Xiaoqi Liu, Dorian Baudry, Julian Zimmert, Patrick Rebeschini, Arya Akhavan*

**主要类别:** stat.ML

**AI概要:** 这篇论文研究了非平稳环境下的Bandit Convex Optimization问题，提出了两种算法（TEWA-SE和cExO），分别在不同的损失函数下实现了最优遗憾界，并改进了当前的结果。


<details>
  <summary>更多</summary>
  
**动机:** 本文旨在解决连续动作空间中Bandit Convex Optimization问题在非平稳环境下的遗憾最小化问题，特别关注三种标准非平稳性度量：比较序列中的切换次数S、损失函数的总变差Δ以及比较序列的路径长度P。

**方法:** 作者提出了一种名为Tilted Exponentially Weighted Average with Sleeping Experts (TEWA-SE)的多项式时间算法，并结合Bandit-over-Bandit框架扩展到未知非平稳性度量的环境。对于一般的凸损失函数，作者引入了基于指数权重的clipped Exploration by Optimization (cExO)方法。

**结果:** 对于强凸损失函数，TEWA-SE被证明在已知S和Δ的情况下是最小最大最优的；对于一般凸损失函数，cExO方法不仅实现了最小最大最优遗憾，还改进了现有P的边界结果。

**结论:** 本文提出了两种适用于不同损失函数的算法TEWA-SE和cExO，能够在非平稳环境中实现最小最大最优遗憾，并且在某些情况下改进了现有的边界结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-stationary+Bandit+Convex+Optimization%3A+A+Comprehensive+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.02980，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.02980&send_immediately=true&force_search=false)

**原文摘要:** Bandit Convex Optimization is a fundamental class of sequential
decision-making problems, where the learner selects actions from a continuous
domain and observes a loss (but not its gradient) at only one point per round.
We study this problem in non-stationary environments, and aim to minimize the
regret under three standard measures of non-stationarity: the number of
switches $S$ in the comparator sequence, the total variation $\Delta$ of the
loss functions, and the path-length $P$ of the comparator sequence. We propose
a polynomial-time algorithm, Tilted Exponentially Weighted Average with
Sleeping Experts (TEWA-SE), which adapts the sleeping experts framework from
online convex optimization to the bandit setting. For strongly convex losses,
we prove that TEWA-SE is minimax-optimal with respect to known $S$ and $\Delta$
by establishing matching upper and lower bounds. By equipping TEWA-SE with the
Bandit-over-Bandit framework, we extend our analysis to environments with
unknown non-stationarity measures. For general convex losses, we introduce a
second algorithm, clipped Exploration by Optimization (cExO), based on
exponential weights over a discretized action space. While not polynomial-time
computable, this method achieves minimax-optimal regret with respect to known
$S$ and $\Delta$, and improves on the best existing bounds with respect to $P$.

</details>


### [183] [Causal Explainability of Machine Learning in Heart Failure Prediction from Electronic Health Records](https://arxiv.org/abs/2506.03068)
*Yina Hou, Shourav B. Rabbani, Liang Hong, Norou Diawara, Manar D. Samad*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新框架，用以分析混合类型的临床变量在心脏病预测中的因果关系，结果显示非线性因果模型更有效。


<details>
  <summary>更多</summary>
  
**动机:** 传统的因果发现方法假设原因和效应变量是数值和连续的，而本文旨在解决这一限制，并探索统计和ML背景下重要变量的因果可解释性。

**方法:** 提出了一种新的计算框架，用于实现因果结构发现（CSD）并评估混合类型临床变量的因果强度。

**结果:** 在HF分类中，研究了三种特征类型的重要性排序关联：相关特征、对ML预测重要的特征和因果特征。CSD建模显示非线性因果关系更具意义。

**结论:** 研究结果表明，非线性因果关系建模比线性方法更有意义，并且从非线性分类器获得的特征重要性与变量的因果强度密切相关。相关变量可能是HF的原因，但很少被识别为效应变量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Explainability+of+Machine+Learning+in+Heart+Failure+Prediction+from+Electronic+Health+Records，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03068，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03068&send_immediately=true&force_search=false)

**原文摘要:** The importance of clinical variables in the prognosis of the disease is
explained using statistical correlation or machine learning (ML). However, the
predictive importance of these variables may not represent their causal
relationships with diseases. This paper uses clinical variables from a heart
failure (HF) patient cohort to investigate the causal explainability of
important variables obtained in statistical and ML contexts. Due to inherent
regression modeling, popular causal discovery methods strictly assume that the
cause and effect variables are numerical and continuous. This paper proposes a
new computational framework to enable causal structure discovery (CSD) and
score the causal strength of mixed-type (categorical, numerical, binary)
clinical variables for binary disease outcomes. In HF classification, we
investigate the association between the importance rank order of three feature
types: correlated features, features important for ML predictions, and causal
features. Our results demonstrate that CSD modeling for nonlinear causal
relationships is more meaningful than its linear counterparts. Feature
importance obtained from nonlinear classifiers (e.g., gradient-boosting trees)
strongly correlates with the causal strength of variables without
differentiating cause and effect variables. Correlated variables can be causal
for HF, but they are rarely identified as effect variables. These results can
be used to add the causal explanation of variables important for ML-based
prediction modeling.

</details>


### [184] [GL-LowPopArt: A Nearly Instance-Wise Minimax Estimator for Generalized Low-Rank Trace Regression](https://arxiv.org/abs/2506.03074)
*Junghyun Lee, Kyoungseok Jang, Kwang-Sung Jun, Milan Vojnović, Se-Young Yun*

**主要类别:** stat.ML

**AI概要:** GL-LowPopArt是一种新颖的广义低秩迹回归估计器，在理论保证和实际应用上均表现优越。


<details>
  <summary>更多</summary>
  
**动机:** 改进现有的广义低秩迹回归估计方法，解决偏置控制问题并提升估计误差界。

**方法:** 采用两阶段方法：核范数正则化后接矩阵Catoni估计，以控制非线性逆链接函数带来的偏差。

**结果:** 建立了最新的估计误差界，揭示了新的实验设计目标GL(π)，并通过局部极小极大下界证明了方法的实例最优性。

**结论:** GL-LowPopArt在广义低秩迹回归问题中表现出色，具备实例最优性，并在矩阵补全和双线性决斗老虎机应用中取得了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GL-LowPopArt%3A+A+Nearly+Instance-Wise+Minimax+Estimator+for+Generalized+Low-Rank+Trace+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03074&send_immediately=true&force_search=false)

**原文摘要:** We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized
low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it
employs a two-stage approach: nuclear norm regularization followed by matrix
Catoni estimation. We establish state-of-the-art estimation error bounds,
surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and
reveal a novel experimental design objective, $\mathrm{GL}(\pi)$. The key
technical challenge is controlling bias from the nonlinear inverse link
function, which we address by our two-stage approach. We prove a *local*
minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise
optimality up to the condition number of the ground-truth Hessian. Applications
include generalized linear matrix completion, where `GL-LowPopArt` achieves a
state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a
novel setting inspired by general preference learning (Zhang et al., 2024). Our
analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,
potentially interesting problem-dependent quantity, along with improved Borda
regret bound than vectorization (Wu et al., 2024).

</details>
