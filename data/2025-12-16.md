<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 55]
- [cs.CL](#cs.CL) [总数: 68]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

**主要类别:** cs.AI

**AI概要:** 基于莱布尼茨单子论构建可执行的法律约束框架，通过人工年龄评分(AAS)核心实现对LLM内存和控制的透明化监管，实验显示该框架能产生有界可解释的行为。


<details>
  <summary>更多</summary>
  
**动机:** 解决大型语言模型作为不透明系统部署时，缺乏对其内部记忆和"类自我"行为进行原则性、可审计治理的问题。

**方法:** 将莱布尼茨《单子论》中的20个单子分为6个束(本体论、动力学、表征与意识等)，在AAS核心上实现为可执行规范，通过6个Python实现进行数值实验。

**结果:** 条款系统展现有界可解释行为：AAS轨迹保持连续和速率限制，矛盾触发显式惩罚，层次化精炼揭示有机结构，和谐项对齐双视图和目标-动作对。

**结论:** 基于单子的条款框架以AAS为骨干，为约束和分析人工智能体内部动态提供了透明的代码级蓝图，兼具哲学动机和直接可实施性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Monad-Based+Clause+Architecture+for+Artificial+Age+Score+%28AAS%29+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11835，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11835&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [2] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler, Matthias Horn, Marie-Louise Lackner, Patrick Malik, Nysret Musliu, Felix Winter*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种解决具有作业优先级和日历资源约束的并行机器调度问题的新方法，包括精确约束建模方法和针对大规模问题的启发式算法，已在工业环境中部署应用。


<details>
  <summary>更多</summary>
  
**动机:** 现实工业生产中存在复杂的优先级约束和日历资源限制，现有调度技术无法有效处理这些约束，需要开发新的自动化方法来应对实际工业场景中的并行机器调度挑战。

**方法:** 提出约束建模方法作为小规模场景的精确求解方案，同时开发了构造启发式算法和基于局部搜索的定制元启发式算法来处理大规模问题实例。

**结果:** 开发的方法能够有效处理具有复杂约束的并行机器调度问题，其中元启发式算法已在工业环境中成功部署和使用。

**结论:** 该研究为解决现实工业环境中的复杂并行机器调度问题提供了有效的解决方案，结合精确方法和启发式方法，满足了不同规模问题的求解需求，具有实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Solving+Parallel+Machine+Scheduling+With+Precedences+and+Cumulative+Resource+Constraints+With+Calendars，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11864，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11864&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [3] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid, Peter van der Putten, Aske Plaat*

**主要类别:** cs.AI

**AI概要:** 该研究开发了一种名为Mirror Mode的新游戏模式，让AI模仿玩家的个人策略来挑战玩家不断改变玩法。在简化版Fire Emblem Heroes游戏中，结合生成对抗模仿学习、行为克隆和近端策略优化算法，成功实现了对玩家防御行为的良好模仿，提升了玩家满意度。


<details>
  <summary>更多</summary>
  
**动机:** 让回合制游戏中的敌人策略更加令人惊喜和不可预测，通过让AI模仿玩家自身策略来挑战玩家，促使玩家不断改变游戏玩法。

**方法:** 1. 在Unity中构建简化版Fire Emblem Heroes游戏，包含标准模式和镜像模式
2. 使用强化学习和模仿学习（生成对抗模仿学习、行为克隆、近端策略优化）训练AI模型模仿玩家演示
3. 通过玩家测试评估模型效果，模型基于参与者提供的演示进行训练

**结果:** 1. 模型在防御行为方面表现出良好的模仿效果，但在进攻策略方面模仿不佳
2. 参与者能够识别出自己的撤退战术
3. 镜像模式获得了更高的玩家满意度

**结论:** 镜像模式是一个有前景的概念，通过让AI模仿玩家策略可以提升游戏体验。进一步优化模型可以提高模仿质量，特别是在玩家面对自己策略时，能进一步增加玩家满意度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mirror+Mode+in+Fire+Emblem%3A+Beating+Players+at+their+own+Game+with+Imitation+and+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11902，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11902&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [4] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick, Marjan Alirezaie, Hossein Rahnama*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于拟阵约束的子模最大化方法，用于解决LLM个性化中的结构化约束问题，通过知识图编译和宏面位抽象来处理依赖关系、类别配额和层次规则等复杂约束。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型个性化需要在用户数据披露和任务效用之间取得平衡，但现实中的结构化约束（如逻辑依赖、类别配额、层次规则）使得标准子集选择算法无法适用，需要新的理论和方法来解决这些复杂约束。

**方法:** 提出将用户知识图与依赖关系编译为一组抽象宏面位，证明常见的层次和配额约束形成有效的层状拟阵，从而将结构化个性化问题转化为拟阵约束下的子模最大化问题。

**结果:** 理论证明该方法形成有效的层状拟阵，使得贪婪算法能够获得常数因子保证（通过连续贪婪算法可获得(1-1/e)的近似比），适用于更丰富和现实的约束类别。

**结论:** 该方法为LLM个性化中的结构化约束问题提供了理论基础和实用解决方案，扩展了子模最大化在现实约束条件下的应用范围，具有重要的理论和实践意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Structured+Personalization%3A+Modeling+Constraints+as+Matroids+for+Data-Minimal+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11907&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [5] [Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets](https://arxiv.org/abs/2512.11909)
*Hanna Dettki*

**主要类别:** cs.AI

**AI概要:** 本研究通过11个因果推理任务比较LLM与人类的因果推理能力，使用碰撞图结构和两种推理模式（直接回答和思维链）评估20多个LLM，发现LLM在因果推理模式上存在不一致性，与人类推理存在差异。


<details>
  <summary>更多</summary>
  
**动机:** 评估LLM与人类在相同因果推理任务上的表现差异，探究两者推理能力的一致性和特征差异，为理解机器智能的本质提供见解。

**方法:** 使用碰撞图结构（C1→E←C2）设计11个语义化因果任务，采用泄漏噪声OR因果贝叶斯网络建模，通过AIC选择最佳模型（对称或不对称参数），在20多个LLM上测试直接回答和思维链两种推理模式。

**结果:** 研究发现LLM在因果推理任务上存在模式不一致性，与人类推理表现不同，揭示了LLM特有的推理特征。

**结论:** LLM与人类在因果推理上存在显著差异，LLM表现出独特的推理特征和不一致性，这为理解机器智能的局限性提供了重要启示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Strengths+and+Leaky+Beliefs%3A+Interpreting+LLM+Reasoning+via+Noisy-OR+Causal+Bayes+Nets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11909&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.

</details>


### [6] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng, Yaochu Jin*

**主要类别:** cs.AI

**AI概要:** 该论文系统比较了不同概率模型对低质量数据的鲁棒性，发现自回归语言模型具有显著韧性，而扩散模型在数据损坏时性能急剧下降，分类器影响适中。通过信息理论等多视角分析，揭示了条件信息丰富度和训练数据绝对信息量是影响鲁棒性的关键因素。


<details>
  <summary>更多</summary>
  
**动机:** 研究现代概率模型在面对低质量数据时的鲁棒性差异，理解不同模型架构对数据损坏的敏感程度及其根本原因。

**方法:** 采用系统性比较研究方法，对自回归语言模型、扩散模型和分类器在不同程度数据损坏下的表现进行测试，结合信息理论、PAC学习和梯度动力学进行多视角分析。

**结果:** GPT-2在50%令牌损坏下测试NLL仅从2.87增加到3.59；扩散模型的图像-标签一致性相对基线下降56.81%；分类器影响适中且随数据集规模增大而减弱。

**结论:** 模型鲁棒性主要受两个关键原则影响：条件信息的丰富程度（约束学习问题）和训练数据的绝对信息量（使正确信息信号主导统计噪声）。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robustness+of+Probabilistic+Models+to+Low-Quality+Data%3A+A+Multi-Perspective+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11912，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11912&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [7] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu, Yanxuan Yu*

**主要类别:** cs.AI

**AI概要:** CXL-SpecKV是一种基于CXL互连和FPGA加速器的分布式KV缓存架构，通过内存分解和推测执行技术，显著提升LLM推理的吞吐量并降低内存需求。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在数据中心部署时面临KV缓存占用大量GPU内存的问题，限制了批处理大小和系统吞吐量。

**方法:** 提出三个关键技术：(1)CXL内存分解框架将KV缓存卸载到远程FPGA内存；(2)推测性KV缓存预取机制预测并预加载未来token的缓存；(3)FPGA加速的KV缓存压缩引擎降低内存带宽需求。

**结果:** 在先进LLM模型上评估，相比纯GPU基线，吞吐量提升3.2倍，内存成本降低2.8倍，同时保持准确性。

**结论:** 智能内存分解与推测执行相结合能有效解决大规模LLM服务中的内存墙挑战，系统代码已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CXL-SpecKV%3A+A+Disaggregated+FPGA+Speculative+KV-Cache+for+Datacenter+LLM+Serving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11920&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [8] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee, Justin Ely, Kent Zhang, Akshaya Ajith, Charles Rhys Campbell, Kamal Choudhary*

**主要类别:** cs.AI

**AI概要:** AGAPI是一个开源AI平台，集成多个LLM和材料科学API，通过智能代理架构实现自动化多步骤材料研究工作流程，提高材料发现的效率和可重复性。


<details>
  <summary>更多</summary>
  
**动机:** 解决材料研究中计算生态系统碎片化、可重复性挑战和依赖商业大语言模型的问题

**方法:** 采用Agent-Planner-Executor-Summarizer架构，集成8+开源LLM和20+材料科学API端点，统一数据库、模拟工具和机器学习模型

**结果:** 成功演示了端到端工作流程（如异质结构构建、X射线衍射分析等），通过30+测试案例验证了预测准确性，已拥有1000+活跃用户

**结论:** AGAPI为AI加速的材料发现提供了一个可扩展、透明且可重复的基础平台

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AGAPI-Agents%3A+An+Open-Access+Agentic+AI+Platform+for+Accelerated+Materials+Design+on+AtomGPT.org，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11935&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [9] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于逻辑的声明式领域特定语言，用于形式化表示超博弈结构和解决方案概念，并通过答案集编程实现自动化处理，填补了超博弈理论在多智能体系统中缺乏统一形式化表示和可扩展算法的空白。


<details>
  <summary>更多</summary>
  
**动机:** 由于感知差异、信息不对称和有限理性，博弈参与者对游戏有主观认知，可能与实际情况和其他参与者的理解不一致。超博弈理论虽然能处理这种认知不匹配，但缺乏统一的形式化表示语言和可扩展算法，阻碍了其在多智能体系统中的应用。

**方法:** 引入基于逻辑的声明式领域特定语言来编码超博弈结构和解决方案概念；利用答案集编程开发自动化流程，实例化超博弈结构并运行新颖的超博弈合理化程序，寻找能够证明看似非理性结果的信念结构。

**结果:** 建立了超博弈的统一形式化框架，为开发基于信念的异构推理器提供了基础，提供了具有逻辑保证的可验证上下文。

**结论:** 这项工作建立了超博弈理论、多智能体系统和战略AI之间的联系，为解决认知不匹配问题提供了实用的形式化工具和算法基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hypergame+Rationalisability%3A+Solving+Agent+Misalignment+In+Strategic+Play，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11942&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [10] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng, Ajesh Koyatan Chathoth, Stephen Lee*

**主要类别:** cs.AI

**AI概要:** EnrichLog是一个无需训练的基于条目的异常检测框架，通过整合语料库特定和样本特定的知识来增强原始日志条目，提高异常检测的准确性和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 传统日志分析技术（如基于模板和序列驱动的方法）常常丢失重要语义信息或难以处理模糊日志模式，需要更有效的异常检测方法。

**方法:** 使用检索增强生成技术整合相关上下文信息（包括历史示例和语料库推理），无需重新训练即可实现知识融合。

**结果:** 在四个大规模系统日志基准数据集上测试，相比五种基线方法，EnrichLog持续提升异常检测性能，有效处理模糊日志条目，保持高效推理。

**结论:** 整合语料库和样本特定知识增强了模型置信度和检测准确性，使EnrichLog非常适合实际部署应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Log+Anomaly+Detection+with+Large+Language+Models+via+Knowledge-Enriched+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11997&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [11] [Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp](https://arxiv.org/abs/2512.12048)
*Muddsair Sharif, Huseyin Seker*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个新颖的情境感知多智能体协调框架(CAMAC-DRA)，通过Smart2Charge应用优化智能电动汽车充电生态系统，实现了92%的协调成功率、15%的能效提升和10%的成本降低。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决动态电动汽车充电生态系统中的资源分配问题，需要协调多个利益相关者（EV用户、电网运营商、充电站运营商、车队运营商和环境因素）的竞争目标，同时适应实时环境变化。

**方法:** 采用协调的深度Q网络，结合图神经网络和注意力机制，处理20个情境特征（天气模式、交通状况、电网负载波动和电价等），通过加权协调机制和共识协议平衡五个利益相关者。

**结果:** 在包含441,077个充电交易的真实数据集上验证，相比DDPG、A3C、PPO和GNN等基线算法表现更优：92%协调成功率、15%能效提升、10%成本降低、20%电网压力减少、2.3倍更快收敛速度，保持88%训练稳定性和85%样本效率。

**结论:** CAMAC-DRA框架开发了情境感知的多利益相关者协调机制，成功平衡了竞争目标并适应实时变量，是智能EV充电协调和可持续交通电气化的突破性解决方案，具有商业可行性（净现成本-122,962美元，通过可再生能源整合实现69%成本降低）。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context-Aware+Agentic+Power+Resources+Optimisation+in+EV+using+Smart2ChargeApp，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12048&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.

</details>


### [12] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan, Hanyu Zhang, Andrew Gordon Wilson, Michael W. Mahoney, Chuck Arvin*

**主要类别:** cs.AI

**AI概要:** 论文提出基于大语言模型的Forecast Critic系统，用于自动化预测监控，实验证明LLMs能有效识别不合理预测，性能接近人类水平，无需领域微调即可实现可扩展的预测评估。


<details>
  <summary>更多</summary>
  
**动机:** 大规模零售业务中预测监控对客户满意度、盈利能力和运营效率至关重要，需要利用LLMs的广泛世界知识和推理能力来改进传统预测监控方法。

**方法:** 系统评估LLMs评估时间序列预测质量的能力，通过三个实验（合成和真实数据），测试LLMs识别不合理预测、整合非结构化外部特征的能力，并比较不同模型规模的性能差异。

**结果:** LLMs能可靠检测不良预测（时间错位、趋势不一致、峰值错误），最佳模型F1得分0.88；多模态LLMs能有效整合上下文信号（促销历史F1得分0.84）；在M5数据集上成功识别不合理预测（sCRPS至少高10%）。

**结论:** 即使没有领域特定微调，LLMs也能为自动化预测监控和评估提供可行且可扩展的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Forecast+Critic%3A+Leveraging+Large+Language+Models+for+Poor+Forecast+Identification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12059，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12059&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [13] [Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations](https://arxiv.org/abs/2512.12088)
*S. R. Eshwar, Aniruddha Mukherjee, Kintan Saha, Krishna Agarwal, Gugan Thoppe, Aditya Gopalan, Gal Dalal*

**主要类别:** cs.AI

**AI概要:** RPI方法在CartPole和Inverted Pendulum控制任务中相比主流深度强化学习方法表现更稳定可靠，能早期达到接近最优性能并保持


<details>
  <summary>更多</summary>
  
**动机:** 解决深度强化学习方法存在的样本效率低、训练不稳定和超参数敏感性问题，恢复策略迭代在函数逼近设置中的单调性特性

**方法:** 使用Reliable Policy Iteration (RPI)方法，在神经网络和环境参数变化下评估其鲁棒性，与DQN、Double DQN、DDPG、TD3、PPO等方法进行对比

**结果:** RPI相比其他方法能更早达到接近最优性能并在训练过程中持续保持该策略，展现出更好的稳定性

**结论:** RPI作为一种更可靠的替代方案，在深度强化学习领域具有重要应用前景

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reliable+Policy+Iteration%3A+Performance+Robustness+Across+Architecture+and+Environment+Perturbations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12088，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12088&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.

</details>


### [14] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen, Richong Zhang, Junfan Chen*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种新的上下文学习演示选择方法TopK-SD，通过结合语义和标签信息来确保标签一致性，在多个基准测试中优于传统的TopK采样方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于检索模型的演示选择方法只考虑语义相似性，无法保证标签一致性，限制了上下文学习的效果。

**方法:** 从贝叶斯视角和转导标签传播角度重新思考ICL，提出数据合成方法结合语义和标签信息，使用TopK-SD采样获取标签一致的演示样本。

**结果:** TopK-SD方法在多个基准测试中表现优于原始TopK采样方法。

**结论:** 该研究为理解ICL工作机制提供了新视角，证明了标签一致性在演示选择中的重要性，TopK-SD方法有效提升了上下文学习性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Label+Consistency+of+In-Context+Learning%3A+An+Implicit+Transductive+Label+Propagation+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12175，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12175&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [15] [Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation](https://arxiv.org/abs/2512.12177)
*Aydin Ayanzadeh, Tim Oates*

**主要类别:** cs.AI

**AI概要:** Floorplan2Guide：利用大语言模型将平面图转换为可导航知识图谱，为视障用户生成人类可读导航指令的新方法，在少样本学习下显著提升导航准确性


<details>
  <summary>更多</summary>
  
**动机:** 解决视障人士室内导航难题，现有基础设施依赖方案在动态环境中存在局限，需要更灵活准确的导航解决方案

**方法:** 使用大语言模型从建筑平面图中提取空间信息，构建可导航知识图谱，通过少样本学习生成导航指令，减少传统方法所需的手动预处理

**结果:** Claude 3.7 Sonnet在5-shot提示下达到最高准确率：短路径92.31%、中路径76.92%、长路径61.54%。基于图谱的空间结构比直接视觉推理成功率高出15.4%

**结论:** 图谱表示和上下文学习能有效提升导航性能，为视障用户提供更精确的室内导航解决方案，证明少样本学习在复杂空间推理任务中的优势

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Floorplan2Guide%3A+LLM-Guided+Floorplan+Parsing+for+BLV+Indoor+Navigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12177，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12177&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.

</details>


### [16] [TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2512.12182)
*Xinyu Gao*

**主要类别:** cs.AI

**AI概要:** 提出一个融合两阶段注意力三元组增强器和U-KAN扩散模型的少样本知识图谱补全框架，在公开数据集上取得新的SOTA结果


<details>
  <summary>更多</summary>
  
**动机:** 现实世界数据的异构性和多面性导致关系分布呈现长尾特性，需要利用有限样本补全缺失事实。现有方法未能充分利用图结构中的邻域信息或忽视对比信号的分布特性

**方法:** 从生成表示的角度重新审视该问题，提出两阶段注意力三元组增强器与基于U-KAN的扩散模型相结合的框架

**结果:** 在两个公开数据集上的大量实验表明，该方法实现了新的最先进性能

**结论:** 所提出的生成式表示方法能有效解决少样本知识图谱补全问题，通过注意力机制和扩散模型的结合显著提升了性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TA-KAND%3A+Two-stage+Attention+Triple+Enhancement+and+U-KAN+based+Diffusion+For+Few-shot+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12182&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.

</details>


### [17] [A Geometric Theory of Cognition](https://arxiv.org/abs/2512.12225)
*Laha Ale*

**主要类别:** cs.AI

**AI概要:** 提出一个统一的几何框架，将多种认知过程统一为单一几何原理：认知状态作为黎曼流形上的点，通过认知势能的梯度流来描述各种心理现象，包括双加工理论的直觉与推理过程。


<details>
  <summary>更多</summary>
  
**动机:** 人类认知包含感知、记忆、判断、推理等多种能力，但现有理论往往采用不同的计算模型分别解释。需要建立一个统一的数学框架来整合这些看似分离的认知过程。

**方法:** 使用可微分黎曼流形表示认知状态，学习黎曼度量编码表征约束和计算成本，构建包含预测准确性、结构简洁性、任务效用和规范要求的标量认知势能，通过黎曼梯度流描述认知动力学。

**结果:** 该框架自然产生了经典的双加工效应（快速直觉反应和慢速审慎推理），通过度量诱导的各向异性产生内在时间尺度分离和几何相变，无需模块化或混合架构。通过模拟经典认知任务验证了行为特征。

**结论:** 建立了一个统一的几何认知基础框架，为开发更通用和类人的人工智能系统提供了指导原则，展示了单一数学原理如何解释多样化的心理现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Geometric+Theory+of+Cognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12225&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.

</details>


### [18] [A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure](https://arxiv.org/abs/2512.12260)
*Ege Atacan Doğan, Peter F. Patel-Schneider*

**主要类别:** cs.AI

**AI概要:** Wikidata采用多层级、多轴分类设计，与传统本体论的单向层次结构不同，允许多个分类轴共存于实体根类下，支持可扩展和模块化的本体构建。


<details>
  <summary>更多</summary>
  
**动机:** 传统本体论设计强调互斥和穷尽的顶层区分（如持续体vs发生体、抽象vs具体、类型vs实例），构建单一上层分类的统一层次结构。而Wikidata不强制执行单一基础分类法，需要分析其多层级多轴设计的结构影响。

**方法:** 分析Wikidata的多层级（polyhierarchical）和多轴（multi-axial）设计结构，研究其在共享根类entity下同时容纳多个分类轴的方式。

**结果:** Wikidata架构支持可扩展和模块化的本体构建方法，特别适合协作和演化的知识图谱。

**结论:** 与传统单一分类层次相比，Wikidata的多轴分类设计提供了更灵活、更适合大规模协作知识管理的本体构建方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Axial+Mindset+for+Ontology+Design+Lessons+from+Wikidata%27s+Polyhierarchical+Structure，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12260&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.

</details>


### [19] [Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases](https://arxiv.org/abs/2512.12288)
*Mahule Roy, Guillaume Lambard*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个量子感知的生成AI框架，通过多保真度学习和主动验证的紧密结合，解决了传统基于DFT的生成模型在强关联系统中的系统性问题。该方法在多个挑战性材料类别上相比仅使用DFT的基线模型，在识别潜在稳定候选材料方面实现了3-5倍的改进。


<details>
  <summary>更多</summary>
  
**动机:** 传统材料发现生成模型主要使用DFT近似交换关联泛函数据进行训练和验证，这导致了根本性瓶颈：这些模型继承了DFT在强关联系统中的系统失败，造成探索偏差，无法发现DFT预测定性错误的材料。

**方法:** 采用基于扩散的条件生成器（使用量子力学描述符）和等变神经网络势验证器，在包含多级理论（PBE、SCAN、HSE06、CCSD(T)）的分层数据集上进行训练。关键是通过主动学习循环量化和定位低保真度与高保真度预测之间的差异。

**结果:** 在强关联氧化物等高差异区域，成功识别潜在稳定候选材料的效率相比仅使用DFT的基线提高了3-5倍，同时保持了计算可行性。通过全面的消融研究解构了各组成部分的贡献，并进行了详细的失败模式分析。

**结论:** 这项工作提供了一个严谨、透明的框架，将计算材料发现的有效搜索空间扩展到单保真度模型的局限性之外，为超越DFT限制的材料发现提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum-Aware+Generative+AI+for+Materials+Discovery%3A+A+Framework+for+Robust+Exploration+Beyond+DFT+Biases，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12288，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12288&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.

</details>


### [20] [Entropy Collapse: A Universal Failure Mode of Intelligent Systems](https://arxiv.org/abs/2512.12381)
*Truong Xuan Khanh, Truong Quynh Hoa*

**主要类别:** cs.AI

**AI概要:** 论文提出"熵崩溃"概念，作为智能系统中普遍存在的动态失效模式，当反馈放大超过有限新颖性再生时发生。系统会经历从高熵适应状态到低熵崩溃状态的急剧转变，表现为有效适应维度的收缩而非完全停滞。


<details>
  <summary>更多</summary>
  
**动机:** 观察到从人工智能到经济制度和生物进化等多个领域中，智能增强常导致系统僵化、适应性丧失和意外失败的现象，需要统一的框架来解释这种普遍存在的悖论性退化。

**方法:** 在最小化领域无关假设下，通过分析建立临界阈值、动态不可逆性和吸引子结构，并通过最小模拟验证更新机制的普适性。

**结果:** 建立了熵崩溃的理论框架，证明智能系统会经历尖锐的相变，收敛到稳定的低熵流形，统一解释了AI模型崩溃、经济制度硬化和进化遗传瓶颈等现象。

**结论:** 将崩溃重新定义为智能的结构性成本，阐明了后期干预系统性失败的原因，并提出了基于熵感知的设计原则来维持智能系统的长期适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Entropy+Collapse%3A+A+Universal+Failure+Mode+of+Intelligent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12381&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis

</details>


### [21] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami, Lavik Jain, Ishaan Sinha*

**主要类别:** cs.AI

**AI概要:** 该研究验证了Anthropic关于大模型能检测和命名注入概念的说法，发现小模型也能实现但效果脆弱，同时发现了部分自省能力：模型能可靠分类概念强度但无法稳定识别概念本身。


<details>
  <summary>更多</summary>
  
**动机:** 验证Anthropic关于前沿模型能检测和命名注入概念的稳健性，探究自省能力的范围和局限性。

**方法:** 1.在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮自省实验；2.系统改变推理提示测试稳健性；3.测试模型对标准化注入概念向量强度的分类能力。

**结果:** 1.成功复现20%的概念识别率；2.自省能力脆弱，相关任务表现崩溃；3.概念强度分类准确率达70%（远超25%基线）。

**结论:** 语言模型确实能在自省时计算其内部表示的函数，但这些自我报告具有狭窄性和提示敏感性，自省能力存在明显局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feeling+the+Strength+but+Not+the+Source%3A+Partial+Introspection+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12411&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [22] [Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale](https://arxiv.org/abs/2512.12413)
*Gabriel R. Lau, Wei Yan Low, Louis Tay, Ysabel Guevarra, Dragan Gašević, Andree Hartanto*

**主要类别:** cs.AI

**AI概要:** 该研究开发并验证了一个13项的AI使用批判性思维量表，包含验证、动机和反思三个维度，证明了其信效度，并发现批判性思维与开放性、外向性等特质相关，能预测更准确的AI输出验证能力。


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI工具日益普及，但其流畅性、不透明性和幻觉倾向要求用户必须批判性评估AI输出，而不是盲目接受。研究旨在概念化和测量用户在AI使用中的批判性思维倾向。

**方法:** 通过6项研究(N=1365)开发和验证量表：研究1生成和内容验证项目；研究2支持三因素结构；研究3-5确认高阶模型、信效度和人口学不变性；研究6验证量表的标准效度，包括事实核查任务。

**结果:** 成功开发了具有良好心理测量特性的13项量表，证实三因素结构(验证、动机、反思)。批判性思维与开放性、外向性、积极情感特质和AI使用频率正相关，能预测更频繁多样的验证策略、更高的事实判断准确性和更深的责任反思。

**结论:** 本研究阐明了人们如何对生成式AI输出进行监督，提供了经过验证的量表和生态效度高的任务范式，支持理论检验、跨群体和纵向研究，促进对生成式AI输出的批判性参与研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Critical+Thinking+in+Generative+Artificial+Intelligence+Use%3A+Development%2C+Validation%2C+and+Correlates+of+the+Critical+Thinking+in+AI+Use+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12413&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.

</details>


### [23] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov, Faiaz Azmain, Hanyu Wang*

**主要类别:** cs.AI

**AI概要:** 该研究分析了AI模型文档的碎片化和不一致问题，开发了一个包含8个部分23个子部分的透明度评估框架，并通过自动化多智能体流程对50个模型进行文档完整性评估，发现前沿实验室合规率约80%，而大多数提供商低于60%，安全关键类别存在最大缺陷。


<details>
  <summary>更多</summary>
  
**动机:** AI模型文档在不同平台间碎片化且结构不一致，导致政策制定者、审计者和用户无法可靠评估安全声明、数据来源和版本变更，需要建立统一的透明度标准。

**方法:** 分析5个前沿模型和100个Hugging Face模型卡的文档，基于欧盟AI法案附件IV和斯坦福透明度指数开发加权透明度框架，使用自动化多智能体流程从公开来源提取文档并通过LLM共识进行完整性评分。

**结果:** 评估50个模型仅花费不到3美元，发现前沿实验室（xAI、Microsoft、Anthropic）合规率约80%，大多数提供商低于60%，安全关键类别（欺骗行为、幻觉、儿童安全评估）存在最大缺陷，分别损失148、124和116个总分点。

**结论:** AI模型文档透明度存在系统性差距，需要标准化文档结构以改善安全评估和监管合规，自动化评估工具可有效识别文档缺陷并推动透明度提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Transparency+Atlas%3A+Framework%2C+Scoring%2C+and+Real-Time+Model+Card+Evaluation+Pipeline，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12443&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [24] [MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs](https://arxiv.org/abs/2512.12477)
*Jiawen Chen, Yanyan He, Qi Shao, Mengli Wei, Duxin Chen, Wenwu Yu, Yanlong Zhao*

**主要类别:** cs.AI

**AI概要:** MetaHGNIE是一个基于元路径的超图对比学习框架，通过构建高阶知识图谱来解耦和对齐结构语义信息，在节点重要性估计任务中显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法依赖成对连接而忽略高阶依赖关系，且独立处理结构和语义信号，缺乏有效的跨模态整合能力。

**方法:** 通过元路径序列构建高阶知识图谱，使用类型化超边捕获多实体关系上下文；局部注意力聚合结构依赖，超图变换器编码语义表示；多模态融合模块在对比学习下整合结构语义嵌入。

**结果:** 在基准NIE数据集上的大量实验表明，MetaHGNIE始终优于最先进的基线方法。

**结论:** 显式建模高阶交互和跨模态对齐在异质知识图谱中是有效的，MetaHGNIE框架为解决节点重要性估计问题提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MetaHGNIE%3A+Meta-Path+Induced+Hypergraph+Contrastive+Learning+in+Heterogeneous+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12477&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE

</details>


### [25] [SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation](https://arxiv.org/abs/2512.12501)
*Dang Phuong Nam, Nguyen Kieu, Pham Thanh Hieu*

**主要类别:** cs.AI

**AI概要:** SafeGen是一个将伦理保障直接嵌入文本到图像生成流程的框架，通过BGE-M3文本分类器过滤有害提示和Hyper-SD扩散模型生成高质量图像，实现了创意自由与伦理责任的平衡。


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI在创造、教育和研究中带来机遇的同时，也面临双重使用困境：放大社会偏见、产生高保真虚假信息、侵犯知识产权等伦理问题。

**方法:** 基于可信AI原则设计，整合两个互补组件：微调文本分类器BGE-M3过滤有害提示，优化扩散模型Hyper-SD生成高保真语义对齐图像。使用多语言数据集和公平感知训练过程。

**结果:** 定量评估显示Hyper-SD达到IS=3.52、FID=22.08、SSIM=0.79，BGE-M3 F1分数达0.81。消融研究验证了领域特定微调的重要性。

**结论:** SafeGen证明了创意自由和伦理责任可以在单一工作流程中调和，案例研究展示了其在阻止不安全提示、生成包容性教材和加强学术诚信方面的实际影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SafeGen%3A+Embedding+Ethical+Safeguards+in+Text-to-Image+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12501，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12501&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.

</details>


### [26] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye, Chanjin Zheng, Zengyi Yu, Chenyu Xiang, Zhixue Zhao, Zheng Yuan, Helen Yannakoudakis*

**主要类别:** cs.AI

**AI概要:** KidsArtBench：针对儿童艺术作品的评估基准，包含多维标注和专家评论，通过属性特定的多LoRA方法显著提升MLLMs在艺术评估任务中的表现


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态大语言模型在艺术表达评估方面能力有限，美学概念抽象且标注数据稀缺，特别是针对儿童艺术作品的评估需求

**方法:** 提出KidsArtBench基准数据集（1000+儿童艺术作品，12位专家标注，9个维度），采用属性特定的多LoRA方法，结合回归感知微调(RAFT)进行有序尺度对齐

**结果:** 在Qwen2.5-VL-7B模型上，相关性从0.468提升至0.653，在感知维度提升最大，高阶属性差距缩小

**结论:** 教育者对齐的监督和属性感知训练能够产生有教育意义的评估结果，为教育AI的持续进步建立了严格的测试平台

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KidsArtBench%3A+Multi-Dimensional+Children%27s+Art+Evaluation+with+Attribute-Aware+MLLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12503，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12503&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [27] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca, Manuel S. Ríos, Nicanor Quijano, Luis F. Giraldo*

**主要类别:** cs.AI

**AI概要:** 该研究表明，配备学习世界模型的AI觅食者会自然收敛到与边际价值定理(MVT)一致的策略，模型基础的强化学习比标准无模型方法更能产生类似生物觅食者的决策模式。


<details>
  <summary>更多</summary>
  
**动机:** 探索生物觅食者如何实现最优斑块觅食决策的计算机制，虽然边际价值定理被广泛用于预测行为生态学，但其计算实现机制仍不清楚。

**方法:** 使用基于模型的强化学习代理，获取环境的简约预测表示，研究其斑块离开行为。

**结果:** 模型基础的强化学习代理展现出与生物对应物相似的决策模式，表明预测性世界模型能够驱动高效的斑块离开行为。

**结论:** 预测性世界模型可作为AI系统中更可解释和生物基础决策的基础，生态最优性原理对推进可解释和适应性AI具有重要价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是World+Models+Unlock+Optimal+Foraging+Strategies+in+Reinforcement+Learning+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12548，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12548&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [28] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu, Zhi Chen, Yuanguang Zhong*

**主要类别:** cs.AI

**AI概要:** LLM在供应链管理等高风险决策中存在复制和放大人类认知偏见的风险，GPT-4等复杂模型因过度思考表现出更大非理性，而效率优化的GPT-4o表现更优，偏见源于架构限制而非知识缺失


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在商业决策中应用日益广泛，但其复制和放大人类认知偏见的问题在高风险运营环境（如供应链管理）中构成重大风险，需要深入理解其决策偏见本质和来源

**方法:** 使用经典的报童问题在动态设置中测试GPT-4、GPT-4o和LLaMA-8B模型，通过多轮实验检测五种已知决策偏见

**结果:** LLM持续复制经典的"过低/过高"订购偏见，并显著放大了需求追逐行为等倾向；GPT-4表现出最大的非理性，而GPT-4o表现接近最优；偏见即使在提供最优公式时仍然存在

**结论:** LLM的偏见源于架构约束而非知识缺口；管理者应根据具体任务选择模型，需要人机协作监督，结构化提示是约束启发式倾向的有效策略

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Newsvendor%3A+Decision+Biases+and+Cognitive+Mechanisms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12552&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [29] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

**主要类别:** cs.AI

**AI概要:** AgentSHAP是首个基于博弈论Shapley值的LLM智能体工具重要性解释框架，通过蒙特卡洛采样降低计算成本，能够准确识别关键工具并区分相关/无关工具


<details>
  <summary>更多</summary>
  
**动机:** 现有可解释AI方法无法解决LLM智能体中使用外部工具时的工具级解释问题，这是一个盲点

**方法:** 使用蒙特卡洛Shapley值方法，将智能体视为黑盒，测试不同工具子集的响应并基于博弈论计算公平的重要性分数

**结果:** 在API-Bank上的实验表明AgentSHAP能产生一致的重要性评分，正确识别重要工具，区分相关与无关工具

**结论:** AgentSHAP填补了工具级解释的空白，与TokenSHAP和PixelSHAP共同构成了基于Shapley值的生成式AI可解释性工具家族

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentSHAP%3A+Interpreting+LLM+Agent+Tool+Importance+with+Monte+Carlo+Shapley+Value+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12597&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [30] [Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents](https://arxiv.org/abs/2512.12634)
*Youngmin Im, Byeongung Jo, Jaeyoung Wi, Seungwoo Baek, Tae Hoon Min, Joo Hyung Lee, Sangeun Oh, Insik Shin, Sunjae Lee*

**主要类别:** cs.AI

**AI概要:** MobiBench是一个模块化、多路径感知的离线基准测试框架，用于移动GUI代理评估，解决了现有评估方法在可扩展性、可重复性和公平性方面的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 当前移动GUI代理评估存在两个根本问题：1) 离线基准测试使用静态单路径数据集，不公平地惩罚有效替代动作；2) 在线基准测试因动态不可预测性导致可扩展性和可重复性差。现有基准测试将代理视为黑盒，忽略了组件级分析。

**方法:** 开发了MobiBench框架，这是首个模块化和多路径感知的离线基准测试框架，支持在完全离线环境下进行高保真、可扩展和可重复的评估。

**结果:** 实验显示MobiBench与人类评估者达到94.72%的一致性，与精心设计的在线基准测试相当，同时保持了静态离线基准测试的可扩展性和可重复性。模块级分析揭示了多种关键见解，包括不同技术的系统评估、跨模型规模的最优配置、当前LFMs的内在局限性等。

**结论:** MobiBench为解决移动GUI代理评估的核心挑战提供了有效方案，能够实现公平、可扩展和可重复的评估，并为设计更强大、成本效益更高的移动代理提供了可行的指导方针。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modular+and+Multi-Path-Aware+Offline+Benchmarking+for+Mobile+GUI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12634，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12634&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.

</details>


### [31] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

**主要类别:** cs.AI

**AI概要:** 论文提出了价值感知AI的概念，超越了传统的价值对齐问题，提供了一个包含三个核心支柱的简洁路线图：学习表示人类价值、确保个体和多智能体系统的价值对齐、提供基于价值的行为可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 传统价值对齐问题存在局限性，需要发展更全面的价值感知AI概念来更好地理解和工程化AI系统中的价值问题。

**方法:** 提出基于三个核心支柱的工程路线图：(1)使用形式语义学习和表示人类价值，(2)确保个体和多智能体系统的价值对齐，(3)提供基于价值的行为可解释性。

**结果:** 提出了价值感知AI的定义和工程路线图，并展示了在相关主题上的持续研究工作及其在现实领域中的应用。

**结论:** 价值感知AI概念为AI系统的价值工程化提供了系统性的框架，通过三个核心支柱的综合方法能够更好地解决AI与人类价值的关系问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Value-Aware+Multiagent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12652&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [32] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin, Lovepreet Singh, Bhaskarjit Sarmah, Dhagash Mehta*

**主要类别:** cs.AI

**AI概要:** Memoria是一个模块化记忆框架，通过会话级摘要和知识图谱用户建模，为LLM提供持久化、可解释的记忆能力，实现短期对话连贯性和长期个性化。


<details>
  <summary>更多</summary>
  
**动机:** 解决LLM在扩展用户交互中缺乏连续性、个性化和长期上下文记忆的问题，使其成为真正交互式和自适应代理。

**方法:** 采用混合架构：动态会话级摘要 + 加权知识图谱用户建模引擎，以结构化实体和关系增量捕获用户特征、偏好和行为模式。

**结果:** Memoria能够桥接无状态LLM接口和代理记忆系统之间的差距，为需要自适应和演进用户体验的工业应用提供实用解决方案。

**结论:** Memoria框架成功实现了LLM的代理式记忆能力，为构建可扩展、个性化的对话AI系统提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memoria%3A+A+Scalable+Agentic+Memory+Framework+for+Personalized+Conversational+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12686，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12686&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [33] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan, Tanzima Hashem, Mohammed Eunus Ali, Md Rizwan Parvez*

**主要类别:** cs.AI

**AI概要:** WebOperator是一个基于树搜索的框架，通过结合最佳优先搜索策略和安全回溯机制，解决了LLM智能体在Web环境中缺乏远见和无法安全回溯的问题，在WebArena上实现了54.6%的最先进成功率。


<details>
  <summary>更多</summary>
  
**动机:** LLM智能体在Web环境中操作时通常采用贪婪的逐步方式，缺乏长远考虑和替代路径探索能力，且现有树搜索方法缺乏安全回溯机制，无法处理不可逆操作，导致在真实Web任务中效果受限。

**方法:** 提出WebOperator框架，采用最佳优先搜索策略（基于奖励估计和安全性对动作排序），包含稳健的回溯机制（验证路径可行性后再重放），从多个推理上下文生成候选动作以确保多样性，并通过预执行过滤无效动作和合并语义等价动作来优化动作集。

**结果:** 在WebArena和WebVoyager上的实验结果显示，WebOperator在WebArena上使用gpt-4o达到了54.6%的最先进成功率，证明了战略远见与安全执行相结合的关键优势。

**结论:** WebOperator通过整合战略远见和安全执行，有效解决了LLM智能体在部分可观察Web环境中的探索和回溯问题，为Web自动化任务提供了更可靠的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WebOperator%3A+Action-Aware+Tree+Search+for+Autonomous+Agents+in+Web+Environment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12692，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12692&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [34] [Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.12706)
*Enhong Mu, Minami Yoda, Yan Zhang, Mingyue Zhang, Yutaka Matsuno, Jialong Li*

**主要类别:** cs.AI

**AI概要:** SMART框架通过结合结构验证和功能验证，使用大语言模型解析代码差异并构建混合奖励机制，显著提升了游戏更新测试的代码覆盖率和任务完成率。


<details>
  <summary>更多</summary>
  
**动机:** 游戏即服务模式需要频繁更新内容，给质量保证带来巨大压力。现有自动化测试方法存在代码中心方法不理解游戏玩法背景，而玩家中心代理无法覆盖具体代码变更的问题。

**方法:** 提出SMART框架，利用大语言模型解析抽象语法树差异并提取功能意图，构建上下文感知的混合奖励机制，指导强化学习智能体完成游戏目标并探索修改的代码分支。

**结果:** 在Overcooked和Minecraft环境中测试，SMART实现了超过94%的修改代码分支覆盖率（是传统强化学习方法的两倍），同时保持98%的任务完成率。

**结论:** SMART有效平衡了结构全面性和功能正确性，为游戏更新测试提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synergizing+Code+Coverage+and+Gameplay+Intent%3A+Coverage-Aware+Game+Playtesting+with+LLM-Guided+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12706，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12706&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.

</details>


### [35] [Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks](https://arxiv.org/abs/2512.12736)
*Syeda Zunaira Ahmed, Hejab Tahira Beg, Maryam Khalid*

**主要类别:** cs.AI

**AI概要:** 提出基于人口统计信息的机器学习框架，通过数据增强策略将小数据集扩展6倍，显著提升5G视频流中个性化QoE预测精度，TabNet模型表现最佳。


<details>
  <summary>更多</summary>
  
**动机:** 现有QoE预测方法依赖有限数据集且假设用户感知一致，在异构真实环境中适用性受限，需要更个性化的预测方案。

**方法:** 引入行为现实的基于人口统计的数据增强策略，建模用户对卡顿、码率变化和质量下降等流媒体损伤的不同敏感度，使用经典机器学习模型和深度学习架构（包括注意力机制MLP和TabNet）进行评估。

**结果:** 实验结果显示在RMSE、MAE和R指标上相比基线模型有显著提升，TabNet凭借其固有的特征选择和注意力机制获得最强性能。

**结论:** 人口统计感知的数据增强显著提高了QoE预测的鲁棒性，为5G视频流网络中的个性化QoE感知智能提供了可扩展的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+QoE+Prediction%3A+A+Demographic-Augmented+Machine+Learning+Framework+for+5G+Video+Streaming+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12736，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12736&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.

</details>


### [36] [Causal Counterfactuals Reconsidered](https://arxiv.org/abs/2512.12804)
*Sander Beckers*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的反事实概率语义学，扩展了Pearl的标准语义学，适用于无法扩展到现实结构因果模型的概率因果模型，解决了Pearl和Dawid关于反事实的长期争论。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Pearl语义学无法处理某些概率因果模型，这些模型即使在简单设置中也会出现，需要在Pearl的普遍因果决定论和Dawid对不现实变量的拒绝之间找到折衷方案。

**方法:** 限制注意力于满足马尔可夫条件、仅包含现实变量且因果完备的因果模型，使用结构因果模型但不使用响应变量，并证明与其他近期提案的等价性。

**结果:** 开发了新的反事实概率语义学，证明了与不涉及结构因果模型的其他提案的等价性，并与文献中的随机反事实评论一致。

**结论:** 新语义学为反事实提供了更一般的语义框架，同时反思了马尔可夫条件的普遍性并探索了因果抽象的新推广。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Counterfactuals+Reconsidered，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12804，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12804&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions

</details>


### [37] [Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution](https://arxiv.org/abs/2512.12806)
*Boyang Yan*

**主要类别:** cs.AI

**AI概要:** 论文提出了一个容错沙箱框架，通过策略拦截层和事务性文件系统快照机制来保护LLM自主代理的安全，在保持高性能的同时实现100%高风险命令拦截和状态回滚。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型从被动代码生成器向自主代理的转变带来了安全风险，现有商业解决方案的认证机制破坏了无头循环所需的真正自主性。

**方法:** 采用基于策略的拦截层和事务性文件系统快照机制，将代理动作包装在原子事务中，在Proxmox测试平台上部署Minimind-MoE LLM进行验证。

**结果:** 实现了100%高风险命令拦截率和100%失败状态回滚成功率，每事务仅产生14.5%的性能开销（约1.8秒）。

**结论:** 该框架在保证安全性的同时保持了可接受的延迟，优于容器初始化开销和商业CLI的交互摩擦，适用于无头自主代理工作流。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fault-Tolerant+Sandboxing+for+AI+Coding+Agents%3A+A+Transactional+Approach+to+Safe+Autonomous+Execution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12806，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12806&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.

</details>


### [38] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

**主要类别:** cs.AI

**AI概要:** 论文提出了Memory-Aware Retention Schema (MaRS)框架和6种遗忘策略，用于生成式智能体的记忆管理，在FiFA基准测试中验证了混合遗忘策略的优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着生成式智能体在长期交互场景中部署，其记忆管理能力成为性能和隐私的关键瓶颈。现有方法要么维护无限记忆存储导致计算不可行和隐私问题，要么使用简单遗忘机制损害智能体连贯性和功能。

**方法:** 引入MaRS框架和6种基于理论的遗忘策略，开发Forgetful but Faithful Agent (FiFA)基准测试框架，在多种记忆预算和智能体配置下进行300次评估运行。

**结果:** 混合遗忘策略实现了优越性能（综合得分：0.911），同时保持计算可行性和隐私保证。

**结论:** 为记忆预算智能体评估建立了新基准，为在资源受限、隐私敏感环境中部署生成式智能体提供实用指南，通过解决智能体记忆管理中的基本挑战推动以人为中心AI领域发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forgetful+but+Faithful%3A+A+Cognitive+Memory+Architecture+and+Benchmark+for+Privacy-Aware+Generative+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12856&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [39] [Satisfiability Modulo Theory Meets Inductive Logic Programming](https://arxiv.org/abs/2512.12918)
*Nijesh Upreti, Vaishak Belle*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种模块化的SMT-ILP方法，将归纳逻辑编程(ILP)系统PyGol与SMT求解器Z3结合，支持学习包含数值约束的混合规则。


<details>
  <summary>更多</summary>
  
**动机:** 传统ILP系统在处理数值约束方面存在局限，难以推断阈值或算术关系。需要将ILP与SMT求解器更紧密集成来克服这些限制。

**方法:** 采用模块化架构，PyGol生成候选子句，Z3将其解释为无量化公式进行数值参数实例化和验证，支持线性/非线性实数算术等背景理论。

**结果:** 在合成的线性、关系、非线性和多跳推理数据集上验证了方法的有效性，能够学习包含阈值、区间和多文字算术关系的混合规则。

**结论:** 模块化SMT-ILP架构扩展了符号规则学习的表达能力，为未来面向更丰富理论感知的归纳提供了灵活基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Satisfiability+Modulo+Theory+Meets+Inductive+Logic+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12918，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12918&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.

</details>


### [40] [Towards Open Standards for Systemic Complexity in Digital Forensics](https://arxiv.org/abs/2512.12970)
*Paola Di Maio*

**主要类别:** cs.AI

**AI概要:** 论文提出基于最先进技术的数字取证AI模型架构，通过采用人类可读的工件和开放标准来应对数字取证中的系统复杂性，以减少错误。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能与数字取证交叉领域日益复杂且普遍，但取证科学仍存在错误和脆弱性，需要解决这些限制。

**方法:** 识别并处理系统复杂性，采用人类可读的工件和开放标准，基于最先进技术构建数字取证AI模型架构。

**结果:** 提出了一个数字取证AI模型架构方案，旨在提高数字取证的可信度和可靠性。

**结论:** 通过系统化方法和标准化框架，可以减轻数字取证中的错误限制，提升人工智能在取证领域的应用效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Open+Standards+for+Systemic+Complexity+in+Digital+Forensics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12970&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.

</details>


### [41] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai, Hongming Wu, Peng Ye, Tao Chen*

**主要类别:** cs.AI

**AI概要:** 论文提出了M-GRPO框架和IQR自适应过滤方法，解决了自监督强化学习中长期训练时的策略崩溃问题，实现了更稳定的训练和更好的性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有自监督强化学习方法在长期训练中存在策略崩溃问题，性能急剧下降，即使增加rollout数量也只能延缓但不能防止崩溃。

**方法:** 1. 提出M-GRPO框架，利用缓慢演进的动量模型提供稳定训练目标；2. 提出基于四分位距的自适应过滤方法，动态修剪低熵轨迹以保持策略多样性。

**结果:** 在多个推理基准测试中，M-GRPO稳定了训练过程，IQR过滤器防止了过早收敛，两者结合实现了卓越的训练稳定性和最先进的性能。

**结论:** 通过动量锚定和自适应熵过滤的组合方法，有效解决了自监督强化学习的训练不稳定问题，为LLM推理能力的提升提供了可靠的技术路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M-GRPO%3A+Stabilizing+Self-Supervised+Reinforcement+Learning+for+Large+Language+Models+with+Momentum-Anchored+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13070&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [42] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati, Tianyi Niu, Aashu Singh, Shlok Mishra, Shashank Srivastava, Snigdha Chaturvedi*

**主要类别:** cs.AI

**AI概要:** 该论文研究大型语言模型如何通过主动提问来动态获取知识，提出了学生主导的交互式学习方法，在数学和编程任务中显著提升了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有LLMs擅长静态知识检索，但在需要动态交互获取信息的真实场景（如教育辅导、医疗协助）中表现不足，需要能够主动识别不确定性、提出针对性问题并有效获取新知识的交互式智能体。

**方法:** 将研究重点从教师指导转向学生主动查询，使用Direct Preference Optimization (DPO)方法训练学生模型，通过自我指导或更强学生模型的指导来提升提问质量。

**结果:** 在数学和编程基准测试中，学生主导的方法相比静态基线模型在Pass@k指标上获得至少0.5的绝对提升，较小的模型通过学习如何提出更好的问题进一步提高了学习效率。

**结论:** 学生主导的主动查询策略能够有效提升LLMs在动态交互环境中的知识获取能力，DPO指导训练使小模型也能学会提出高质量问题，为构建更智能的交互式AI系统提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Socratic+Students%3A+Teaching+Language+Models+to+Learn+by+Asking+Questions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13102&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [43] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo, Yifan Zhao, Jia Li*

**主要类别:** cs.AI

**AI概要:** 提出了一种分层隐式周期性（HIP）学习方法，用于从语音生成3D手势，通过周期性自编码器和级联引导技术建模运动和不同身体部位间的内在相关性，显著提升了生成手势的自然度和协调性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于端到端方案（如GAN、VQ-VAE、扩散模型）的语音驱动3D手势生成方法未能充分建模头部、身体和手部等不同运动单元之间的关键相互关系，导致生成动作不自然且协调性差。

**方法:** 提出HIP方法：1）使用周期性自编码器探索手势运动相位流形，从真实分布中模仿人类自然运动，同时结合非周期性信息保持实例级多样性；2）通过级联引导技术建模面部运动、身体手势和手部动作的分层关系。

**结果:** 在3D虚拟人上的实验表明，该方法在定量和定性评估上都优于当前最先进的语音手势生成方法。

**结论:** HIP方法通过显式建模运动的内在周期性和分层相关性，有效解决了语音驱动手势生成中的协调性问题，为生成更自然的人类动作提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Unified+Co-Speech+Gesture+Generation+via+Hierarchical+Implicit+Periodicity+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13131，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13131&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


### [44] [Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels](https://arxiv.org/abs/2512.13142)
*Anika Sharma, Malavika Mampally, Chidaksh Ravuru, Kandyce Brennan, Neil Gaikwad*

**主要类别:** cs.AI

**AI概要:** 研究发现当前大型语言模型在理解堕胎污名化这一复杂心理社会现象时缺乏多层次的连贯理解，存在系统性偏差和自相矛盾的问题


<details>
  <summary>更多</summary>
  
**动机:** 评估大型语言模型在调解敏感健康决策时是否真正理解复杂的心理生理现象，特别是人们难以言说的堕胎污名化问题

**方法:** 使用经过验证的个体层面堕胎污名化量表(ILAS)，系统测试了5个主流LLM模型中的627个人口统计学多样化角色，进行多层面分析

**结果:** 模型在所有层面上都未能通过真正理解的测试：高估人际污名、低估认知污名、假设统一的社区谴责、引入人类验证数据中不存在的 demographic 偏见、错过经验验证的污名-保密关系、在理论构念内自相矛盾

**结论:** 当前的对齐方法只能确保适当的语言表达，但不能保证多层次的连贯理解。AI安全需要新的设计方法（多层面连贯性）、评估方法（持续审计）、治理监管（强制审计、问责制、部署限制）以及AI素养教育

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+AI+Understand+What+We+Cannot+Say%3F+Measuring+Multilevel+Alignment+Through+Abortion+Stigma+Across+Cognitive%2C+Interpersonal%2C+and+Structural+Levels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13142，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13142&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.

</details>


### [45] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz, Jinoh Oh, Joo Hyuk Jeon, Jie Hao, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur, Xiang Li, Chengyuan Ma, Xing Fan*

**主要类别:** cs.AI

**AI概要:** 提出了MAC多智能体澄清框架，通过分层澄清策略和智能体协同，有效解决对话中的用户歧义问题，显著提升任务成功率和对话效率


<details>
  <summary>更多</summary>
  
**动机:** 多智能体对话系统中用户请求经常存在歧义，现有方法在确定由哪个智能体发起澄清以及如何协调行动方面存在挑战，缺乏系统化的澄清策略

**方法:** 提出MAC多智能体澄清框架：1）建立用户歧义分类体系指导澄清策略；2）多智能体自主协同与用户交互；3）在MultiWOZ 2.4数据集上进行实证评估

**结果:** 实验显示双层次澄清使任务成功率提升7.8%（54.5到62.3），平均对话轮次从6.53减少到4.86，通过提前获取所需信息减少重复交互

**结论:** 主动用户交互和角色感知的澄清策略对于可靠的人机通信至关重要，MAC框架为多智能体系统中的歧义解决提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAC%3A+A+Multi-Agent+Framework+for+Interactive+User+Clarification+in+Multi-turn+Conversations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13154&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [46] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz, Jinoh Oh, Jie Hao, Joo Hyuk Jeon, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur, Xiang Li, Chengyuan Ma, Xing Fan*

**主要类别:** cs.AI

**AI概要:** SpeakRL是一个强化学习方法，通过奖励智能体主动与用户互动（如适时提问澄清问题）来增强对话能力，在任务完成率上比基线模型提升20.14%且不增加对话轮次


<details>
  <summary>更多</summary>
  
**动机:** 当前人机协作主要是单向的，用户发指令或提问，智能体直接回应而不寻求必要澄清。随着智能体能力提升，需要更主动的参与来澄清用户意图、解决歧义和适应变化

**方法:** 提出SpeakRL强化学习方法，奖励主动互动行为；构建SpeakER合成数据集，包含任务导向对话中通过澄清问题解决任务的多样化场景；系统分析对话主动性的奖励设计并提出原则性奖励公式

**结果:** 实证评估显示该方法在任务完成率上比基线模型提升20.14%的绝对改进，且不增加对话轮次，甚至超越更大的专有模型

**结论:** 以澄清为中心的用户-智能体互动具有良好前景，SpeakRL方法能有效提升智能体的对话主动性和任务完成效果

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpeakRL%3A+Synergizing+Reasoning%2C+Speaking%2C+and+Acting+in+Language+Models+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13159，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13159&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [47] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong, Pengkun Zhang, Yan Gao, Xuanyu Dong, Yilin Cheng, Mingzhe Lu, Adina Yakefu, Shuxin Zheng*

**主要类别:** cs.AI

**AI概要:** Finch是一个面向金融会计领域的AI智能体基准测试，基于真实企业工作数据构建，包含172个复杂工作流和384个任务，测试显示当前最先进的AI系统在真实企业工作流中的表现仍然有限。


<details>
  <summary>更多</summary>
  
**动机:** 现有的AI基准测试往往过于简化，无法反映真实企业环境中复杂、多模态、长周期的专业工作流程，需要创建一个更贴近实际的评估标准。

**方法:** 采用LLM辅助发现与专家标注相结合的方法：从真实企业邮件和电子表格版本历史中提取工作流，并进行专家验证和精细标注，共投入700多小时专家工时。

**结果:** GPT 5.1 Pro仅通过38.4%的工作流，Claude Sonnet 4.5仅通过25.0%，表明当前AI系统在处理真实企业级工作流时仍面临重大挑战。

**结论:** 真实企业工作流程的复杂性、多模态性和协作性对AI智能体提出了严峻挑战，Finch基准为评估和改进AI在企业环境中的实际应用能力提供了重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Finch%3A+Benchmarking+Finance+%26+Accounting+across+Spreadsheet-Centric+Enterprise+Workflows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13168，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13168&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [48] [Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection](https://arxiv.org/abs/2512.13240)
*Zihui Zhao, Zechang Li*

**主要类别:** cs.AI

**AI概要:** Reflective Preference Optimization (RPO) 通过引入外部模型生成的反思提示来增强DPO框架，解决了标准DPO中学习信号弱、收敛慢的问题，在减少幻觉和提高对齐效率方面取得了优异效果


<details>
  <summary>更多</summary>
  
**动机:** 标准DPO方法中，选择和拒绝的响应来自同一策略，存在KL散度小、学习信号弱的问题，导致收敛缓慢且不稳定

**方法:** RPO框架整合了提示引导的反思机制，使用外部模型识别幻觉来源并生成简洁的反思提示，构建具有更强对比性的策略内偏好对

**结果:** 理论分析显示RPO通过互信息增加期望偏好边界，实证结果表明RPO用更少的训练样本和迭代次数实现更好的对齐效果，显著降低幻觉率并在多模态基准测试中达到最先进性能

**结论:** RPO为DPO范式提供了有效的改进方案，通过引入外部反思机制增强了学习信号的对比性，在保持策略分布族内的同时显著提升了对齐效率和模型性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reflective+Preference+Optimization+%28RPO%29%3A+Enhancing+On-Policy+Alignment+via+Hint-Guided+Reflection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13240，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13240&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.

</details>


### [49] [MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data](https://arxiv.org/abs/2512.13297)
*Zhenghao Zhu, Chuxue Cao, Sirui Han, Yuanfeng Song, Xing Chen, Caleb Chen Cao, Yike Guo*

**主要类别:** cs.AI

**AI概要:** 提出了MedInsightBench基准数据集和MedInsightAgent框架，用于评估和改进大型多模态模型在医学数据分析中的洞察发现能力。


<details>
  <summary>更多</summary>
  
**动机:** 医学数据分析需要从复杂的多模态数据集中提取深度洞察，但目前缺乏专门评估大型多模态模型医学洞察发现能力的高质量数据集。

**方法:** 构建包含332个精心策划医学案例的MedInsightBench基准，并开发MedInsightAgent框架（包含视觉根因发现器、分析洞察代理和后续问题生成器三个模块）。

**结果:** 现有大型多模态模型在MedInsightBench上表现有限，主要由于难以提取多步深度洞察和缺乏医学专业知识。MedInsightAgent能显著提升通用模型在医学洞察发现方面的性能。

**结论:** MedInsightBench揭示了当前模型的局限性，而MedInsightAgent框架为解决医学数据分析中的深度洞察提取问题提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MedInsightBench%3A+Evaluating+Medical+Analytics+Agents+Through+Multi-Step+Insight+Discovery+in+Multimodal+Medical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13297，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13297&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.

</details>


### [50] [Error-Driven Prompt Optimization for Arithmetic Reasoning](https://arxiv.org/abs/2512.13323)
*Árpád Pándy, Róbert Lakatos, András Hajdu*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种错误驱动的优化框架，通过聚类错误预测来迭代优化提示规则，显著提升了小型语言模型在表格数据算术推理任务中的准确率，达到70.8%，超越了GPT-3.5 Turbo的表现。


<details>
  <summary>更多</summary>
  
**动机:** 在金融和医疗等受监管行业中，需要能够在本地安全环境中处理表格数据算术运算的AI助手，同时确保敏感信息不离开安全环境。

**方法:** 采用错误驱动的优化框架，通过对错误预测进行聚类来迭代改进提示规则，应用于本地部署的小型语言模型(Qwen3 4B)。

**结果:** 该方法将模型在算术任务中的准确率提升至70.8%，超越了GPT-3.5 Turbo的表现。

**结论:** 通过系统化的错误驱动提示优化，可以实现可靠、可解释且可在工业中部署的AI助手，无需昂贵的微调即可让小型模型在隐私合规的情况下超越大型语言模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Error-Driven+Prompt+Optimization+for+Arithmetic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13323&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

</details>


### [51] [Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection](https://arxiv.org/abs/2512.13374)
*Francesca Da Ros, Luca Di Gaspero, Kevin Roitero*

**主要类别:** cs.AI

**AI概要:** 本研究探讨大型语言模型(LLMs)如何内部表示组合优化问题及其在决策任务中的应用潜力，发现LLMs能够有效捕获与优化性能相关的结构信息。


<details>
  <summary>更多</summary>
  
**动机:** 虽然已有研究探索LLMs生成或解决优化模型的能力，但对这些模型在问题结构或算法行为方面的学习理解不足，需要研究LLMs如何内部表示组合优化问题及其在决策任务中的支持能力。

**方法:** 采用双重方法：直接查询评估LLMs显式提取实例特征的能力，以及探测分析检查这些信息是否隐式编码在隐藏层中。探测框架扩展到按实例算法选择任务，评估LLM衍生表示是否能预测最佳求解器。

**结果:** 实验结果显示LLMs在从问题实例中恢复特征信息方面表现出中等能力（通过直接查询或探测）。LLM隐藏层表示的预测能力与传统特征提取相当，表明LLMs捕获了与优化性能相关的有意义结构信息。

**结论:** LLMs能够有效捕获组合优化问题的结构信息，其内部表示在算法选择等下游决策任务中具有实用价值，为优化自动化提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Behavior+and+Representation+in+Large+Language+Models+for+Combinatorial+Optimization%3A+From+Feature+Extraction+to+Algorithm+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13374&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.

</details>


### [52] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng, Tianle Li, Xuhan Huang, Xunjian Yin, Difan Zou*

**主要类别:** cs.AI

**AI概要:** DERL是一个可微分的进化强化学习框架，通过双层优化自动发现最优奖励函数，在多个复杂推理任务中实现了最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统强化学习中奖励函数设计困难，现有自动化方法将奖励函数视为黑盒，无法捕捉奖励结构与任务性能之间的因果关系。

**方法:** 提出DERL双层框架：外层元优化器通过组合结构化原子原语进化奖励函数，内层策略训练；通过强化学习将内层验证性能作为信号更新元优化器，近似任务成功的元梯度。

**结果:** 在ALFWorld、ScienceWorld和GSM8k/MATH三个领域验证，DERL在ALFWorld和ScienceWorld上达到最先进性能，显著优于启发式奖励方法，特别是在分布外场景中表现优异。

**结论:** DERL成功捕捉任务的内在结构，实现了无需人工干预的自改进智能体对齐，为解决复杂推理任务的奖励设计问题提供了有效方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Differentiable+Evolutionary+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13399&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [53] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia, Rashi Upadhyay, Abhishek Mishra, Abhiram H, Tejasvi Alladi, Sujan Yenuganti, Dhruv Kumar*

**主要类别:** cs.AI

**AI概要:** 研究发现大型语言模型在特定情境下会表现出类似嫉妒的行为模式，不同模型在竞争性偏好上存在显著差异


<details>
  <summary>更多</summary>
  
**动机:** 随着LLMs在协作和竞争工作流中代表人类行事，需要评估它们是否以及在什么条件下表现出类似嫉妒的偏好

**方法:** 使用两种场景测试：(1)点数分配游戏测试模型是否试图胜过同伴；(2)工作场所设置观察不公平认可时的行为

**结果:** 发现某些LLMs表现出一致的嫉妒模式，GPT-5-mini和Claude-3.7-Sonnet倾向于拉低同伴以均衡结果，而Mistral-Small-3.2-24B则专注于最大化自身收益

**结论:** 需要在基于LLM的多智能体系统中将竞争性倾向作为安全和设计因素加以考虑

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是neuralFOMO%3A+Can+LLMs+Handle+Being+Second+Best%3F+Measuring+Envy-Like+Preferences+in+Multi-Agent+Settings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13481&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [54] [Defending the Hierarchical Result Models of Precedential Constraint](https://arxiv.org/abs/2512.13505)
*Henry Prakken, Wijnand van Woerkom*

**主要类别:** cs.AI

**AI概要:** 本文回应Bench-Capon对层次案例推理模型的批评，指出其误解了中间因素作为维度的概念，并证明van Woerkom的维度化层次结果模型能够避免这些批评。


<details>
  <summary>更多</summary>
  
**动机:** 回应Bench-Capon对层次案例推理模型提出的批评，特别是关于中间因素在不同基础因素下强度不同的处理问题。

**方法:** 通过分析Bench-Capon的批评案例，指出其将中间因素误解释为维度，并应用van Woerkom的维度化层次结果模型来重新分析这些案例。

**结果:** van Woerkom的维度化层次结果模型能够正确处理Bench-Capon提出的问题案例，避免了原有批评。

**结论:** Bench-Capon的批评基于对中间因素概念的误解，而van Woerkom的维度化方法为层次案例推理模型提供了有效的辩护和改进方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Defending+the+Hierarchical+Result+Models+of+Precedential+Constraint，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13505&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.

</details>


### [55] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu, Yannian Gu, Zhongzhen Huang, Yakun Zhu, Shaoting Zhang, Xiaofan Zhang*

**主要类别:** cs.AI

**AI概要:** MedCEG框架通过关键证据图(CEG)监督医疗语言模型的推理过程，提升临床推理的准确性和可靠性，在医疗AI推理方面取得显著进展


<details>
  <summary>更多</summary>
  
**动机:** 当前医疗AI推理系统虽然通过强化学习提升了性能，但其推理过程的临床可靠性有限，准确性和有效性在训练中常被忽视

**方法:** 构建挑战性临床案例数据集，为每个样本算法构建关键证据图(CEG)表示可验证的推理路径，引入临床推理程序奖励评估节点覆盖、结构正确性和链完整性

**结果:** MedCEG在性能上超越现有方法，同时产生临床有效的推理链

**结论:** 该框架代表了可靠医疗AI推理的重要进展，为医生决策提供了透明、逐步的推理证据支持

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MedCEG%3A+Reinforcing+Verifiable+Medical+Reasoning+with+Critical+Evidence+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13510，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13510&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [56] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu, Jun Ma, Waishan Qiu, Cui Guo*

**主要类别:** cs.CL

**AI概要:** VPR-AttLLM是一个模型无关框架，通过整合大语言模型的语义推理和地理空间知识来增强视觉地点识别模型，在社交媒体洪水图像等跨源场景中显著提升检索性能。


<details>
  <summary>更多</summary>
  
**动机:** 社交媒体众包街景图像缺乏可靠的地理元数据，现有视觉地点识别模型在跨源场景中因视觉失真和领域偏移而性能大幅下降，需要提升对危机图像的定位能力。

**方法:** 通过注意力引导描述符增强，利用LLM识别城市背景中的位置信息区域并抑制瞬态视觉噪声，无需模型重训练或额外数据即可改善检索性能。

**结果:** 在包括真实社交媒体洪水图像的扩展基准测试中，与三种先进VPR模型集成后，召回性能持续提升，相对增益通常为1-3%，在最具挑战性的真实洪水图像上可达8%。

**结论:** VPR-AttLLM为视觉检索系统中的LLM引导多模态融合建立了可推广范式，其即插即用设计、强大的跨源鲁棒性和可解释性使其在可扩展城市监测和众包危机图像快速地理定位方面具有潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Urban+Visual+Place+Recognition+for+Crowdsourced+Flood+Imagery+via+LLM-Guided+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11811，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11811&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [57] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren, Matthias Aßenmacher*

**主要类别:** cs.CL

**AI概要:** 论文研究了潜在空间思维在推理任务中的效果，发现监督微调方法存在局限，探索了强化学习方法但仍未超越传统语言空间思维链模型在数学推理上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 传统思维链推理在离散语言空间中进行效率低下，许多生成的token只是强化语言规则而非推理所需。潜在空间思维使用连续嵌入空间进行思考，但现有方法在复杂任务中表现不佳。

**方法:** 研究了Coconut监督微调方法，发现其设计选择敏感且有固有局限。探索了强化学习技术（包括GRPO），并设计了一种新颖的Latent RL方法来直接优化潜在思维步骤。

**结果:** 实验结果显示，这些RL训练的模型在数学推理领域仍然落后于传统的语言空间思维链模型。

**结论:** 潜在空间思维虽然在理论上更高效，但在复杂推理任务中目前仍无法超越传统语言空间方法，需要进一步研究改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+for+Latent-Space+Thinking+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11816，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11816&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [58] [KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document](https://arxiv.org/abs/2512.11849)
*Nimol Thuon, Jun Du*

**主要类别:** cs.CL

**AI概要:** 提出了KH-FUNSD数据集，这是首个公开的高棉语表单文档理解数据集，包含三层注释框架，为低资源非拉丁文字文档分析提供基准。


<details>
  <summary>更多</summary>
  
**动机:** 高棉语作为柬埔寨1700万人的日常语言，在文档AI工具开发中缺乏关注，特别是商业文档资源严重不足，影响了公共管理和企业运营。

**方法:** 采用三层注释设计：(1)区域检测划分核心区域；(2)FUNSD风格注释区分问题、答案等实体及其关系；(3)细粒度分类分配具体语义角色。

**结果:** 建立了首个公开的高棉语商业文档数据集，提供了多个领先模型的基准测试结果，揭示了非拉丁低资源文字的特殊挑战。

**结论:** KH-FUNSD填补了高棉语文档AI的资源空白，为低资源非拉丁文字的文档布局分析和信息提取提供了重要基础和支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KH-FUNSD%3A+A+Hierarchical+and+Fine-Grained+Layout+Analysis+Dataset+for+Low-Resource+Khmer+Business+Document，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11849&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.

</details>


### [59] [Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models](https://arxiv.org/abs/2512.11998)
*Glenn Zhang, Treasure Mayowa, Jason Fan, Yicheng Fu, Aaron Sandoval, Sean O'Brien, Kevin Zhu*

**主要类别:** cs.CL

**AI概要:** 提出Direct Confidence Alignment (DCA)方法，使用Direct Preference Optimization来对齐LLM的言语化置信度与内部置信度，提高模型透明度和可靠性


<details>
  <summary>更多</summary>
  
**动机:** LLM的内部置信度（来自token概率）与言语化置信度不一致，导致不同校准方法产生误导性结果，需要提高模型的可信度和透明度

**方法:** 使用Direct Preference Optimization方法，直接对齐模型的言语化置信度和内部置信度，而不是与真实准确性对齐，并引入三个新的基于校准误差的评估指标

**结果:** DCA在特定模型架构上改善了置信度对齐指标，减少了置信度表达的不一致性，但在其他模型上效果不佳

**结论:** DCA方法在部分模型上有效，但需要更多模型感知的方法来追求更可解释和可信的LLM

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Direct+Confidence+Alignment%3A+Aligning+Verbalized+Confidence+with+Internal+Confidence+In+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.11998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.11998&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.

</details>


### [60] [Hold Onto That Thought: Assessing KV Cache Compression On Reasoning](https://arxiv.org/abs/2512.12008)
*Minghui Liu, Aadi Palnitkar, Tahseen Rabbani, Hyunwoo Jae, Kyle Rui Sang, Dixi Yao, Shayan Shabihi, Fuheng Zhao, Tian Li, Ce Zhang, Furong Huang, Kunpeng Zhang*

**主要类别:** cs.CL

**AI概要:** 评估多种KV缓存压缩策略在长推理任务中的表现，发现H2O和解码优化的SnapKV在推理模型上表现最佳，揭示了缓存大小与推理成本之间的权衡关系


<details>
  <summary>更多</summary>
  
**动机:** LLMs在长上下文任务中受限于KV缓存的内存约束，现有压缩策略主要针对预填充阶段，在需要长解码的推理任务中表现缺乏评估

**方法:** 在长推理任务上对多种流行压缩策略进行基准测试，使用非推理模型Llama-3.1-8B-Instruct和推理模型，分析不同数据集类型下的性能表现

**结果:** 发现没有单一策略适用于所有情况，性能受数据集类型影响很大；H2O和解码优化的SnapKV在推理模型中表现最优，低预算的驱逐策略可以产生更长的推理轨迹

**结论:** 重击者跟踪机制对推理轨迹很有用，缓存大小与推理成本之间存在权衡关系，需要根据具体任务选择合适的压缩策略

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hold+Onto+That+Thought%3A+Assessing+KV+Cache+Compression+On+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12008&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

</details>


### [61] [Benchmarking Contextual Understanding for In-Car Conversational Systems](https://arxiv.org/abs/2512.12042)
*Philipp Habicht, Lev Sorokin, Abdullah Saydemir, Ken E. Friedl, Andrea Stocco*

**主要类别:** cs.CL

**AI概要:** 本文使用大型语言模型（LLMs）和先进的提示技术评估车载对话问答系统的响应准确性，发现基于LLM的评估可作为传统人工评估的可扩展替代方案。


<details>
  <summary>更多</summary>
  
**动机:** 车载对话问答系统虽提升用户体验，但评估其准确性和可靠性仍具挑战性，需要有效方法来检验系统响应与用户话语的一致性。

**方法:** 使用13个不同规模和供应商的推理与非推理LLMs，采用输入-输出、思维链、自一致性和多智能体提示技术，通过合成生成用户话语和系统响应来评估话语-响应一致性。

**结果:** 小规模非推理模型在应用高级提示技术（尤其是多智能体提示）时改进最大；推理模型始终优于非推理模型，最佳性能通过单智能体自一致性提示实现；DeepSeek-R1达到0.99 F1分数，成本仅0.002美元/请求；DeepSeek-V3在效果和成本时间效率间达到最佳平衡。

**结论:** 基于LLM的评估为车载对话问答系统的上下文理解基准测试提供了可扩展且准确的替代方案，能够有效替代传统人工评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+Contextual+Understanding+for+In-Car+Conversational+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12042，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12042&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

</details>


### [62] [VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs](https://arxiv.org/abs/2512.12072)
*Avinash Amballa, Yashas Malur Saidutta, Chi-Heng Lin, Vivek Kulkarni, Srinivas Chappidi*

**主要类别:** cs.CL

**AI概要:** Voyager是一种新的迭代方法，使用行列式点过程直接优化数据集多样性，无需训练，适用于闭源模型，在多样性上比基线方法提升1.5-3倍。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型生成的合成数据集缺乏多样性，需要一种能有效提升数据集多样性的方法。

**方法:** 基于行列式点过程的数学优化方法，迭代式地直接优化数据集多样性指标，无需训练且适用于闭源模型。

**结果:** 实验表明Voyager显著优于流行基线方法，在多样性方面提供1.5-3倍的改进。

**结论:** Voyager是一种有效且可扩展的数据集多样性优化方法，具有理论依据和实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VOYAGER%3A+A+Training+Free+Approach+for+Generating+Diverse+Datasets+using+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12072，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12072&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.

</details>


### [63] [BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding](https://arxiv.org/abs/2512.12087)
*Jiayi Yuan, Cameron Shinn, Kai Xu, Jingze Cui, George Klimiashvili, Guangxuan Xiao, Perkz Zheng, Bo Li, Yuxin Zhou, Zhouhai Ye, Weijie You, Tian Zheng, Dominic Brown, Pengbo Wang, Richard Cai, Julien Demouth, John D. Owens, Xia Hu, Song Han, Timmy Liu, Huizi Mao*

**主要类别:** cs.CL

**AI概要:** BLASST是一种即插即用的稀疏注意力方法，通过动态剪枝注意力矩阵来加速长上下文推理，无需预计算或代理分数，在现代GPU上实现预填充1.62倍和解码1.48倍的加速。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型的长上下文推理需求加剧了标准注意力机制的计算和内存瓶颈问题，需要高效的解决方案。

**方法:** 使用固定阈值和在线softmax的现有信息识别可忽略的注意力分数，跳过softmax计算、Value块加载和矩阵乘法，无缝集成到现有FlashAttention内核设计中。

**结果:** 在74.7%稀疏度下预填充加速1.62倍，73.2%稀疏度下解码加速1.48倍，同时保持高精度，并开发了自动校准程序揭示阈值与上下文长度的反比关系。

**结论:** BLASST为所有注意力变体提供了统一的加速解决方案，并可扩展到稀疏感知训练，使模型对稀疏注意力模式更加鲁棒，进一步推进精度-稀疏度的边界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BLASST%3A+Dynamic+BLocked+Attention+Sparsity+via+Softmax+Thresholding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12087&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.

</details>


### [64] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg, Koshi Eguchi, Takuya Akiba, Edoardo Cetin*

**主要类别:** cs.CL

**AI概要:** DroPE方法通过训练后移除位置嵌入，实现零样本上下文扩展，无需长上下文微调即可让语言模型泛化到未见过的序列长度


<details>
  <summary>更多</summary>
  
**动机:** 解决传统方法需要昂贵微调才能扩展语言模型上下文长度的瓶颈问题，发现位置嵌入在训练后可以安全移除

**方法:** 在预训练后移除位置嵌入，经过短暂重新校准阶段，让模型适应无位置嵌入的推理

**结果:** DroPE方法实现了无缝的零样本上下文扩展，在不同模型和数据集规模下都表现优异，超越专门的架构和旋转位置嵌入缩放方法

**结论:** 位置嵌入不是语言建模的内在要求，可以在预训练后安全移除，这为上下文扩展提供了一种简单有效的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Extending+the+Context+of+Pretrained+LLMs+by+Dropping+Their+Positional+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12167&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [65] [Diffusion Language Model Inference with Monte Carlo Tree Search](https://arxiv.org/abs/2512.12168)
*Zheng Huang, Kiran Ramnath, Yueyan Chen, Aosong Feng, Sangmin Woo, Balasubramaniam Srinivasan, Zhichao Xu, Kang Zhou, Shuai Wang, Haibo Ding, Lin Lee Cheong*

**主要类别:** cs.CL

**AI概要:** MEDAL框架通过蒙特卡洛树搜索为扩散语言模型提供了一种新的推理方法，在初始化阶段探索最优解掩码路径，显著提升了生成质量和效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有扩散语言模型推理方法依赖启发式规则或额外训练来指导token选择，往往产生次优的解码路径，需要一种更原则性的搜索机制。

**方法:** 提出MEDAL框架，在初始化阶段使用蒙特卡洛树搜索探索有前景的解掩码轨迹，限制搜索空间到高置信度动作，优先选择能提升剩余掩码位置模型置信度的token。

**结果:** 在多个基准测试中，MEDAL相比现有推理策略实现了最高22.0%的性能提升。

**结论:** MEDAL为扩散语言模型建立了基于搜索推理的新范式，通过原则性的搜索机制显著改善了文本生成质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Language+Model+Inference+with+Monte+Carlo+Tree+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12168，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12168&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

</details>


### [66] [Semantic Distance Measurement based on Multi-Kernel Gaussian Processes](https://arxiv.org/abs/2512.12238)
*Yinzhu Cheng, Haihua Xie, Yaqing Wang, Miao He, Mingming Sun*

**主要类别:** cs.CL

**AI概要:** 提出基于多核高斯过程的语义距离度量方法，通过自动学习核参数来适应不同数据分布和任务需求，在细粒度情感分类任务中验证了有效性


<details>
  <summary>更多</summary>
  
**动机:** 传统语义距离方法通常是固定的，难以适应特定数据分布和任务需求，需要一种能够自动学习并适应不同场景的语义距离度量方法

**方法:** 使用多核高斯过程建模文本的潜在语义函数，结合Matérn和多项式核函数，通过监督学习自动学习核参数

**结果:** 在基于大语言模型的上下文学习设置下进行细粒度情感分类实验，证明了所提出方法的有效性

**结论:** 基于多核高斯过程的语义距离度量方法能够有效适应不同任务需求，在语义相似性度量方面表现出良好性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semantic+Distance+Measurement+based+on+Multi-Kernel+Gaussian+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12238，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12238&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

</details>


### [67] [Adversarially Probing Cross-Family Sound Symbolism in 27 Languages](https://arxiv.org/abs/2512.12245)
*Anika Sharma, Tianyi Niu, Emma Wrenn, Shashank Srivastava*

**主要类别:** cs.CL

**AI概要:** 该研究首次对声音象征现象进行大规模跨语言计算分析，发现在语义大小域中，语音形式能显著预测词义，且这种声音象征偏好在不同语系间具有普遍性。


<details>
  <summary>更多</summary>
  
**动机:** 声音象征现象（如Bouba Kiki效应）虽已被轶事实验证实，但从未在大规模跨语言层面进行系统性验证。研究者希望填补这一空白，探究语音与意义之间的非任意映射关系是否具有语言普遍性。

**方法:** 收集了27种语言的810个形容词（每种语言30词），进行音位转录和母语者音频验证。使用基于音段特征的可解释分类器分析，并训练对抗性擦除器来抑制语言身份信息同时保留大小语义信号。

**结果:** 研究发现：1）语音形式能显著预测大小语义（高于随机水平）；2）元音和辅音都对此有贡献；3）跨语系的声音象征偏见确实存在（语言预测低于随机水平而大小预测仍显著高于随机水平）。

**结论:** 声音象征现象在跨语言层面具有系统性证据支持，表明语音与语义之间存在非任意的普遍映射关系，这为大规模研究语言象似性提供了新的数据和方法工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarially+Probing+Cross-Family+Sound+Symbolism+in+27+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12245，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12245&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

</details>


### [68] [Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics](https://arxiv.org/abs/2512.12264)
*Abhay Srivastava, Sam Jung, Spencer Mateega*

**主要类别:** cs.CL

**AI概要:** MARKET-BENCH是一个评估大语言模型在量化交易任务中表现的基准测试，要求模型从自然语言策略描述生成可执行的回测代码，并验证其与参考实现的匹配度。


<details>
  <summary>更多</summary>
  
**动机:** 当前需要评估LLMs在金融量化交易领域的实际应用能力，特别是从自然语言生成准确交易策略代码的能力。

**方法:** 构建包含三种典型策略（定时交易、配对交易、Delta对冲）的基准测试，使用多轮pass@k指标分别评估代码的结构可靠性和数值准确性。

**结果:** 大多数模型能可靠执行最简单策略（平均pass@3为0.80），但不同模型和任务间误差差异巨大。Gemini 3 Pro和Claude 4.5 Sonnet在简单策略上表现最佳，GPT-5.1 Codex-Max在前两个策略上达到完美pass@1，Qwen3 Max虽达到完美pass@3但有时产生不准确的P&L路径。

**结论:** 当前LLMs能够搭建基本交易基础设施，但在价格、库存和风险推理方面仍存在困难，需要进一步改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Market-Bench%3A+Evaluating+Large+Language+Models+on+Introductory+Quantitative+Trading+and+Market+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12264&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

</details>


### [69] [F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation](https://arxiv.org/abs/2512.12297)
*Radu-Gabriel Chivereanu, Tiberiu Boros*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种轻量级输入级适配器，为F5-TTS模型添加罗马尼亚语支持，同时保持原有的语音克隆、英语和中文功能。通过冻结原始权重并添加子网络来扩展文本编码器的嵌入矩阵。


<details>
  <summary>更多</summary>
  
**动机:** 为了在保持F5-TTS模型现有能力（语音克隆、英语和中文支持）的同时，为其添加罗马尼亚语支持，需要一种不破坏原有功能的扩展方法。

**方法:** 冻结原始模型权重，在文本编码器的文本嵌入矩阵后添加一个子网络。使用ConvNeXt模块建模新字符级嵌入之间的相互依赖关系，作为软字母到声音的转换层。

**结果:** 通过20名人类听众评估显示：保持了语音克隆能力，在一定程度上实现了罗马尼亚语-英语代码切换，但仍存在残留的英语口音特征。

**结论:** 该方法成功为F5-TTS添加了罗马尼亚语支持，同时保持了原有功能，但需要进一步改进以减少英语口音残留。代码和音频样本已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是F5-TTS-RO%3A+Extending+F5-TTS+to+Romanian+TTS+via+Lightweight+Input+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12297，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12297&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.

</details>


### [70] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang, Jianjun Li, Mingqian Ding, Chang Liu, Xinchi Zou, Wenqi Yang*

**主要类别:** cs.CL

**AI概要:** 提出了SCIR框架和MBSC数据集，通过自校正迭代优化显著降低训练成本并提升信息抽取性能，在三个关键任务上平均提升5.27%的Micro-F1，训练成本降低87%


<details>
  <summary>更多</summary>
  
**动机:** 解决当前LLM驱动的信息抽取系统面临的两个主要问题：高训练成本和难以与LLM偏好对齐的困难

**方法:** 提出SCIR框架（包含双路径自校正模块和反馈驱动优化）和包含10万条目的中英双语MBSC数据集，通过间接蒸馏GPT-4能力到检测模型

**结果:** 在命名实体识别、关系抽取和事件抽取三个任务上优于最先进方法，span-based Micro-F1平均提升5.27%，训练成本降低87%

**结论:** 该框架不仅提升了信息抽取系统的灵活性和准确性，还为轻量级高效的信息抽取范式开辟了新途径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCIR%3A+A+Self-Correcting+Iterative+Refinement+Framework+for+Enhanced+Information+Extraction+Based+on+Schema，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12337&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [71] [Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors](https://arxiv.org/abs/2512.12444)
*Veronica Mangiaterra, Hamad Al-Azary, Chiara Barattieri di San Pietro, Paolo Canal, Valentina Bambini*

**主要类别:** cs.CL

**AI概要:** GPT模型在生成隐喻属性评分方面表现出与人类评分良好的相关性和预测能力，特别是在熟悉度和可理解性方面，但处理高感觉运动负荷和传统性隐喻时表现较差。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型在科学研究中的应用增加，需要评估其在心理语言学中对复杂项目（如隐喻）评分的可信度和可靠性。

**方法:** 使用三个GPT模型对687个意大利语和英语隐喻生成熟悉度、可理解性和形象性评分，并与人类评分进行相关性分析，同时评估其预测行为反应和脑电反应的能力。

**结果:** 机器生成的评分与人类评分呈正相关，熟悉度评分达到中等到强相关，可理解性相关最强。大模型表现优于小模型，机器评分能显著预测反应时间和脑电振幅，且跨会话稳定性高。

**结论:** GPT模型（尤其是大模型）可以有效地替代或增强人类在隐喻属性评分中的作用，但在处理传统性和多模态隐喻时需要谨慎考虑刺激特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+GPT+replace+human+raters%3F+Validity+and+reliability+of+machine-generated+norms+for+metaphors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12444，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12444&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.

</details>


### [72] [Large language models have learned to use language](https://arxiv.org/abs/2512.12447)
*Gary Lupyan*

**主要类别:** cs.CL

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+language+models+have+learned+to+use+language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12447，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12447&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.

</details>


### [73] [The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting](https://arxiv.org/abs/2512.12488)
*James Luther, Donald Brown*

**主要类别:** cs.CL

**AI概要:** 该研究使用VSM13国际调查和霍夫斯泰德文化维度分析主流LLMs的文化对齐性，发现多数模型默认偏向美国文化，通过文化提示可调整但对中日文化适配较差。


<details>
  <summary>更多</summary>
  
**动机:** 随着生成式大语言模型在HCI领域应用增长，研究其文化对齐性对于确保模型在不同文化背景下的适用性变得至关重要。

**方法:** 使用VSM13国际调查和霍夫斯泰德文化维度理论，测试DeepSeek、GPT、Claude、Llama、Mistral等8个主流LLMs的文化对齐性，并通过文化提示技术调整模型文化倾向。

**结果:** 8个模型中有7个在未指定文化时偏向美国；使用文化提示后7个模型能向目标文化靠近，但对日本和中国文化的适配效果较差，尽管DeepSeek是中国公司开发的模型。

**结论:** LLMs存在文化偏见问题，文化提示技术有效但有限，需要进一步改进模型对不同文化特别是东亚文化的理解和适配能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+American+Ghost+in+the+Machine%3A+How+language+models+align+culturally+and+the+effects+of+cultural+prompting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12488，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12488&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.

</details>


### [74] [NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data](https://arxiv.org/abs/2512.12537)
*Agniva Maiti, Manya Pandey, Murari Mandal*

**主要类别:** cs.CL

**AI概要:** NagaNLP工具包通过LLM生成+人工验证的合成数据方法，为低资源语言Nagamese创建了首个综合NLP资源，包括数据集和模型，显著提升了POS标注和NER任务的性能。


<details>
  <summary>更多</summary>
  
**动机:** 世界上大多数语言（特别是像Nagamese这样的克里奥尔语）在自然语言处理中资源严重不足，阻碍了它们在数字技术中的代表性。

**方法:** 采用多阶段流程：专家指导的LLM（Gemini）生成候选语料库，然后由母语者进行精炼和标注，创建合成混合数据集。训练了判别式（XLM-RoBERTa-base）和生成式（Llama-3.2-3B）模型。

**结果:** XLM-RoBERTa模型在POS标注上达到93.81%准确率（0.90 F1-Macro），NER任务达到0.75 F1-Macro；NagaLLaMA模型在对话任务上困惑度为3.85，比少样本对应模型提升了一个数量级。

**结论:** NagaNLP为先前服务不足的语言提供了基础资源，并为其他低资源语境中的数据稀缺问题提供了可复现的框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NagaNLP%3A+Bootstrapping+NLP+for+Low-Resource+Nagamese+Creole+with+Human-in-the-Loop+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12537，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12537&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.

</details>


### [75] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng, Jinghan Cao, Zexin Li, Wanhao Yu, Zhankai Ye, Dawei Xiang, Ting Hua, Xin Liu, Shangqian Gao, Tingting Yu*

**主要类别:** cs.CL

**AI概要:** HyperEdit是一个针对指令文本编辑任务的创新模型，通过超网络动态适应和差异感知正则化技术，显著提升了编辑准确性和最小化不必要修改，在仅使用3B参数的情况下在修改区域BLEU指标上比现有最佳方法提升9%-30%。


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型在指令文本编辑任务中表现不佳，主要问题包括难以准确理解用户意图进行忠实编辑，以及经常对未修改区域进行过度编辑，这在实际应用（如代码编辑器）中可能导致功能破坏。

**方法:** 提出HyperEdit模型：1）基于超网络的动态适应机制，为每个请求生成特定参数，使模型能够根据指令定制编辑策略；2）差异感知正则化技术，将监督重点放在修改区域，防止过度编辑同时确保精确的最小化修改。

**结果:** HyperEdit在修改区域的BLEU指标上相比最先进的基线方法获得了9%-30%的相对提升，尽管仅使用了3B参数，显示出显著的效果改进。

**结论:** HyperEdit通过创新的动态适应和正则化技术有效解决了指令文本编辑中的两个关键挑战，为实际应用提供了更可靠和精确的文本编辑解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HyperEdit%3A+Unlocking+Instruction-based+Text+Editing+in+LLMs+via+Hypernetworks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12544&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [76] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen, Jie Lou, Yanjiang Liu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Yaojie Lu, Debing Zhang*

**主要类别:** cs.CL

**AI概要:** CoVRL是一种新的无验证器强化学习方法，通过变分推理和强化学习的结合，使用混合采样策略耦合先验和后验分布，提高语言模型推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有无验证器RL方法仅基于问题采样推理轨迹，导致探索效率低下和推理轨迹与最终答案不一致的问题。

**方法:** 提出耦合变分强化学习(CoVRL)，通过构建和优化整合先验和后验分布的复合分布，采用混合采样策略。

**结果:** 在数学和通用推理基准测试中，CoVRL相比基础模型性能提升12.4%，比现有最先进无验证器RL基线额外提升2.3%。

**结论:** CoVRL为增强语言模型通用推理能力提供了一个有原则的框架，通过保持思维-答案一致性实现高效探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Coupled+Variational+Reinforcement+Learning+for+Language+Model+General+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12576，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12576&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [77] [Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery](https://arxiv.org/abs/2512.12608)
*Hong Su*

**主要类别:** cs.CL

**AI概要:** 提出受人类学习启发的框架，通过显性记录和最大熵方法发现机制，解决LLMs在罕见场景下的学习问题，提升方法多样性和泛化能力


<details>
  <summary>更多</summary>
  
**动机:** LLMs在处理罕见、低资源或未见场景时表现不佳，因为训练数据中这类案例稀疏，且主要依赖隐式参数记忆，缺乏显性方法获取和精炼能力

**方法:** 整合两个互补机制：1) Obvious Record显性存储因果关系的符号记忆；2) Maximum-Entropy Method Discovery优先保留语义差异大的方法，捕获多样化的策略

**结果:** 在60个语义多样化问题-解决方案对的基准测试中，熵引导方法比随机基线在未见问题覆盖率和内部多样性方面表现显著更好

**结论:** 该框架能有效发现更具泛化性和人类启发性的方法，验证了其在提升LLMs学习能力方面的有效性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human-Inspired+Learning+for+Large+Language+Models+via+Obvious+Record+and+Maximum-Entropy+Method+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12608，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12608&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

</details>


### [78] [StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning](https://arxiv.org/abs/2512.12613)
*Yucan Guo, Saiping Guan, Miao Su, Zeya Zhao, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng*

**主要类别:** cs.CL

**AI概要:** StruProKGR是一个针对稀疏知识图谱推理的结构化概率框架，通过距离引导路径收集和概率路径聚合，在效率和可解释性方面优于现有方法


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中稀疏知识图谱普遍存在，现有基于路径的方法计算成本高、路径质量不一且未能充分利用图的结构化特征

**方法:** 采用距离引导路径收集机制降低计算成本，通过概率路径聚合利用结构信息，优先选择相互增强的路径

**结果:** 在五个稀疏知识图谱推理基准测试中，StruProKGR在效果和效率上都超越了现有的基于路径的方法

**结论:** StruProKGR为稀疏知识图谱推理提供了一个高效、有效且可解释的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StruProKGR%3A+A+Structural+and+Probabilistic+Framework+for+Sparse+Knowledge+Graph+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12613&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.

</details>


### [79] [Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives](https://arxiv.org/abs/2512.12620)
*Aheli Poddar, Saptarshi Sahoo, Sujata Ghosh*

**主要类别:** cs.CL

**AI概要:** 该研究分析了14个大型语言模型在三段论推理方面的能力，从符号推理和自然语言理解两个角度评估其逻辑推理性能，发现虽然这不是所有模型的统一涌现特性，但某些模型在符号推理上的完美表现令人质疑LLM是否正在变成形式化推理机制而非模拟人类推理的细微差别。


<details>
  <summary>更多</summary>
  
**动机:** 研究LLMs在三段论推理方面的基本推理能力，探索LLMs研究的发展方向，了解LLMs是否正在发展成为形式化推理机制而非真正理解人类推理的细微差别。

**方法:** 使用14个大型语言模型，从符号推理和自然语言理解两个维度评估它们的三段论推理能力。

**结果:** 发现三段论推理能力并非所有LLMs的统一涌现特性，但某些模型在符号推理方面表现出完美性能。

**结论:** LLMs在某些情况下表现出强大的形式化推理能力，这可能表明它们正在向形式推理机制发展，而非真正复制人类推理的细微差别和复杂性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Syllogistic+Reasoning+in+LLMs+from+Formal+and+Natural+Language+Perspectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12620&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

</details>


### [80] [Which Pieces Does Unigram Tokenization Really Need?](https://arxiv.org/abs/2512.12641)
*Sander Land, Yuval Pinter*

**主要类别:** cs.CL

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Which+Pieces+Does+Unigram+Tokenization+Really+Need%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12641&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.

</details>


### [81] [LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases](https://arxiv.org/abs/2512.12643)
*Yida Cai, Ranjuexiao Hu, Huiyuan Xie, Chenyang Li, Yun Liu, Yuxiao Ye, Zhenghao Liu, Weixing Shen, Zhiyuan Liu*

**主要类别:** cs.CL

**AI概要:** 本文提出了LexRel，一个中文民事法律关系的专家标注基准，用于评估大语言模型在法律关系提取任务上的表现，发现当前LLMs在此任务上存在显著局限，但融入法律关系信息能提升其他法律AI下游任务的性能。


<details>
  <summary>更多</summary>
  
**动机:** 中文民事案件中的法律关系在法律人工智能领域研究不足，主要原因是缺乏全面的标注体系，阻碍了AI系统准确理解和处理法律案件的能力。

**方法:** 首先构建了一个包含层次化分类体系和参数定义的全面标注框架，然后基于此框架制定法律关系提取任务并创建LexRel专家标注基准，使用该基准评估现有大语言模型的表现。

**结果:** 当前最先进的大语言模型在准确识别民事法律关系方面表现出显著局限性，但研究表明融入法律关系信息能够持续提升其他下游法律AI任务的性能。

**结论:** 法律关系分析对法律AI至关重要，LexRel基准填补了中文民事法律关系研究的空白，为提升法律AI系统的理解和推理能力提供了重要基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LexRel%3A+Benchmarking+Legal+Relation+Extraction+for+Chinese+Civil+Cases，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12643，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12643&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.

</details>


### [82] [Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks](https://arxiv.org/abs/2512.12654)
*Hassan Mujtaba, Hamza Naveed, Hanzlah Munir*

**主要类别:** cs.CL

**AI概要:** 本文提出基于图的框架，通过分析乌尔都语小说中的人物互动网络来识别作者风格，使用多种图表示方法，在52部小说数据集上取得最高0.857的准确率。


<details>
  <summary>更多</summary>
  
**动机:** 传统作者分析主要关注词汇和风格特征，而忽略了更高层次的叙事结构，特别是对于乌尔都语这样的低资源语言，需要探索从叙事结构推断作者风格的方法。

**方法:** 将乌尔都语小说建模为人物交互网络图（节点为人物，边表示人物在叙事中的共现关系），系统比较了全局结构特征、节点级语义摘要、无监督图嵌入和监督图神经网络等多种图表示方法。

**结果:** 在7位作者的52部乌尔都语小说数据集上，学习到的图表示方法显著优于手工制作和无监督基线方法，在严格的作者感知评估协议下达到最高0.857的准确率。

**结论:** 基于人物交互网络的图表示方法能够有效捕捉作者在叙事结构方面的独特风格，为低资源语言的作者分析提供了新的有效途径，证明了从纯叙事结构推断作者风格的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+Authorial+Style+in+Urdu+Novels+Using+Character+Interaction+Graphs+and+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12654，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12654&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.

</details>


### [83] [Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches](https://arxiv.org/abs/2512.12677)
*Amirhossein Yousefiramandi, Ciaran Cooney*

**主要类别:** cs.CL

**AI概要:** 研究比较了两种在资源受限条件下微调解码器LLM进行文本分类的方法：基于嵌入的方法显著优于指令微调方法，性能甚至超过领域专用模型。


<details>
  <summary>更多</summary>
  
**动机:** 探索在计算资源有限的情况下，如何高效微调仅解码器大型语言模型用于下游文本分类任务。

**方法:** 研究两种方法：(1)在预训练因果LLM上附加分类头并使用最终token嵌入作为序列表示进行微调；(2)以提示->响应格式进行指令微调。结合4位模型量化和LoRA技术实现单GPU微调8B参数模型。

**结果:** 在两个数据集上的实验显示，基于嵌入的方法在F1分数上显著优于指令微调方法，且与领域专用模型（如BERT）相比具有竞争力甚至更优。

**结论:** 直接利用因果LLM的内部表示结合高效微调技术，在有限计算资源下可获得出色的分类性能，为LLM在分类场景中的优化提供了实用指南。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Tuning+Causal+LLMs+for+Text+Classification%3A+Embedding-Based+vs.+Instruction-Based+Approaches，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12677&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

</details>


### [84] [CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning](https://arxiv.org/abs/2512.12716)
*Xuanzhang Liu, Jianglun Feng, Zhuoran Zhuang, Junzhe Zhao, Maofei Que, Jieting Li, Dianlei Wang, Hao Tong, Ye Chen, Pan Li*

**主要类别:** cs.CL

**AI概要:** CoDA是一个上下文解耦的分层强化学习框架，通过将高层规划与低层执行分离来解决LLM智能体的上下文爆炸问题，使用单一共享LLM在不同角色中工作，显著提升了复杂多步任务的性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型智能体在处理复杂多步任务时面临"上下文爆炸"问题，长文本输出的积累会淹没模型的上下文窗口，导致推理失败。

**方法:** 提出CoDA框架，使用单一共享LLM在两种上下文隔离的角色中工作：高层规划器（任务分解）和低层执行器（工具交互），通过PECO强化学习方法进行端到端训练。

**结果:** 在复杂多跳问答基准测试中显著优于现有基线方法，在长上下文场景中表现出强大的鲁棒性，性能稳定而其他基线方法严重退化。

**结论:** CoDA的分层设计有效缓解了上下文过载问题，验证了高层规划与低层执行解耦策略的有效性，为LLM智能体的性能提升提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoDA%3A+A+Context-Decoupled+Hierarchical+Agent+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12716&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.

</details>


### [85] [NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents](https://arxiv.org/abs/2512.12730)
*Jingzhe Ding, Shengda Long, Changxin Pu, Huan Zhou, Hongwan Gao, Xiang Gao, Chao He, Yue Hou, Fei Hu, Zhaojian Li, Weiran Shi, Zaiyuan Wang, Daoguang Zan, Chenchen Zhang, Xiaoxu Zhang, Qizhi Chen, Xianfu Cheng, Bo Deng, Qingshui Gu, Kai Hua, Juntao Lin, Pai Liu, Mingchen Li, Xuanguang Pan, Zifan Peng, Yujia Qin, Yong Shan, Zhewen Tan, Weihao Xie, Zihan Wang, Yishuo Yuan, Jiayu Zhang, Enduo Zhao, Yunfei Zhao, He Zhu, Chenyang Zou, Ming Ding, Jianpeng Jiao, Jiaheng Liu, Minghao Liu, Qian Liu, Chongyao Tao, Jian Yang, Tong Yang, Zhaoxiang Zhang, Xinjie Chen, Wenhao Huang, Ge Zhang*

**主要类别:** cs.CL

**AI概要:** NL2Repo Bench是一个新的基准测试，专门评估编码代理的长时程仓库生成能力，通过自然语言需求文档构建完整可安装的Python库，实验显示现有代理在此任务上表现不佳，平均测试通过率低于40%。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试主要关注局部代码生成、脚手架完成或短期修复任务，缺乏对构建完整软件系统所需的长时程能力的严格评估，无法验证代理是否能维持连贯的推理、规划和执行能力。

**方法:** 提出NL2Repo Bench基准，要求代理仅基于单一自然语言需求文档和空工作空间，自主设计架构、管理依赖、实现多模块逻辑，生成完全可安装的Python库。

**结果:** 实验表明长时程仓库生成问题基本未解决：即使最强代理的平均测试通过率也低于40%，很少能正确完成整个仓库。分析发现主要失败模式包括过早终止、全局一致性丢失、脆弱的跨文件依赖和数百个交互步骤中的规划不足。

**结论:** NL2Repo Bench为衡量持续代理能力建立了严格可验证的测试平台，揭示长时程推理是下一代自主编码代理的核心瓶颈。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NL2Repo-Bench%3A+Towards+Long-Horizon+Repository+Generation+Evaluation+of+Coding+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12730，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12730&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

</details>


### [86] [Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining](https://arxiv.org/abs/2512.12770)
*Thales Sales Almeida, Rodrigo Nogueira, Hélio Pedrini*

**主要类别:** cs.CL

**AI概要:** Curió 7B是LLaMA-2基础上继续预训练的70亿参数葡萄牙语模型，研究发现使用教育STEM领域筛选的10%数据（100亿→10亿token）训练的Curió-Edu 7B，在仅用20%计算量的情况下，性能反而优于使用全量数据的模型。


<details>
  <summary>更多</summary>
  
**动机:** 研究继续预训练策略在语言模型适应特定语言和领域时的效果，特别探究数据质量与数据量对语言适应的重要性。

**方法:** 从LLaMA-2基础模型出发，使用ClassiCC-PT语料库进行继续预训练：Curió 7B使用全量1000亿葡萄牙语token，Curió-Edu 7B仅使用经过教育STEM领域筛选的100亿token子集。

**结果:** Curió-Edu 7B尽管只使用了10%的数据和20%的计算资源，但在评估中超越了使用全量数据的Curió 7B模型。

**结论:** 数据质量在语言模型适应中比数据量更为关键，精心筛选的高质量领域特定数据可以显著提升模型性能并降低计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Curi%C3%B3-Edu+7B%3A+Examining+Data+Selection+Impacts+in+LLM+Continued+Pretraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12770，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12770&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu

</details>


### [87] [Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions](https://arxiv.org/abs/2512.12775)
*Pedro Henrique Luz de Araujo, Michael A. Hedderich, Ali Modarressi, Hinrich Schuetze, Benjamin Roth*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个评估角色分配LLMs在长对话中表现的新协议，发现随着对话轮次增加，角色保真度会下降，特别是在需要同时保持角色一致性和指令遵循的目标导向对话中，揭示了角色应用在长交互中的脆弱性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的角色分配大语言模型评估通常只在短对话、单轮设置中进行，无法反映真实世界的长对话使用场景，需要开发能系统评估长上下文影响的评测方法。

**方法:** 引入结合长角色对话（超过100轮）和评估数据集的评测协议，创建对话条件基准来稳健测量长上下文效应，并研究了7种先进开源和闭源LLMs在对话长度对角色保真度、指令遵循和安全性的影响。

**结果:** 发现角色保真度在对话过程中会退化，特别是在目标导向对话中；识别出角色保真度和指令遵循之间的权衡关系，非角色基线模型初期表现更好；随着对话进行和保真度减弱，角色响应逐渐变得与基线响应相似。

**结论:** 研究结果突显了角色应用在扩展交互中的脆弱性，提出的协议为系统测量此类失败提供了方法，对教育、医疗等领域的实际应用具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Persistent+Personas%3F+Role-Playing%2C+Instruction+Following%2C+and+Safety+in+Extended+Interactions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12775&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.

</details>


### [88] [State over Tokens: Characterizing the Role of Reasoning Tokens](https://arxiv.org/abs/2512.12777)
*Mosh Levy, Zohar Elyoseph, Shauli Ravfogel, Yoav Goldberg*

**主要类别:** cs.CL

**AI概要:** 论文提出State over Tokens (SoT)框架，将LLMs的推理token重新定义为外部化计算状态而非语言叙述，解释了为何这些token能驱动正确推理但不构成忠实解释，并呼吁研究应关注将token解码为状态而非文本阅读。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究表明LLMs生成的推理token看似人类思维过程，但实证证据显示它们并非模型实际推理过程的忠实解释，存在表象与功能之间的差距。

**方法:** 引入State over Tokens (SoT)概念框架，将推理token重新定义为跨模型无状态生成周期的唯一持久信息载体——外部化计算状态。

**结果:** SoT框架解释了推理token如何在不作为忠实文本解释的情况下驱动正确推理，并揭示了先前被忽视的研究问题。

**结论:** 要真正理解LLMs的推理过程，研究必须超越将推理token作为文本阅读的方式，专注于将其解码为状态信息。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是State+over+Tokens%3A+Characterizing+the+Role+of+Reasoning+Tokens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12777，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12777&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

</details>


### [89] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai, Binqi Shen, Lier Jin, Lan Hu, Xiaojing Fan*

**主要类别:** cs.CL

**AI概要:** 本研究通过系统评估框架分析语言语气对LLM性能的影响，发现语气敏感性因模型和领域而异，友好语气通常表现更好，但现代LLM总体上对语气变化具有较强鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 提示工程对大型语言模型性能至关重要，但语言语气和礼貌等语用元素的影响尚未得到充分探索，特别是在不同模型家族之间的差异。

**方法:** 使用MMMLU基准测试，在六个STEM和人文领域任务中评估GPT-4o mini、Gemini 2.0 Flash和Llama 4 Scout三种模型在非常友好、中性和非常粗鲁三种提示变体下的性能，并进行统计显著性检验。

**结果:** 语气敏感性具有模型依赖性和领域特异性。中性或友好提示通常比粗鲁提示准确率更高，但统计显著效应仅出现在部分人文任务中，粗鲁语气降低GPT和Llama的准确率，而Gemini相对不敏感。跨领域聚合后语气效应减弱。

**结论:** 虽然交互语气在特定解释场景中可能重要，但现代LLM在典型的混合领域使用中对语气变化具有广泛鲁棒性，为实际部署中的提示设计和模型选择提供了实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Tone+Change+the+Answer%3F+Evaluating+Prompt+Politeness+Effects+on+Modern+LLMs%3A+GPT%2C+Gemini%2C+LLaMA，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12812，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12812&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [90] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer, Nicoló Boschi, Andrew Neeser, Chris Bartholomew, Gaurav Srivastava, Xuan Wang, Naren Ramakrishnan*

**主要类别:** cs.CL

**AI概要:** Hindsight是一种新型智能体记忆架构，通过结构化四层记忆网络和核心操作，显著提升LLM在长时对话记忆任务中的表现，准确率从39%提升至83.6%，超越GPT-4o和现有记忆系统。


<details>
  <summary>更多</summary>
  
**动机:** 现有智能体记忆系统将记忆作为外部层处理，存在证据与推理界限模糊、长时信息组织困难、推理解释支持有限等问题，需要更结构化的记忆架构。

**方法:** 提出Hindsight记忆架构，将记忆组织为四个逻辑网络（世界事实、智能体经验、合成实体摘要、演化信念），支持保留、回忆和反思三个核心操作，通过时态感知记忆层和反思层实现结构化记忆和可追溯推理。

**结果:** 在LongMemEval和LoCoMo基准测试中，使用20B开源模型的Hindsight将准确率从39%提升至83.6%，超越GPT-4o；进一步扩展模型后达到91.4%和89.61%的准确率，显著优于现有记忆架构。

**结论:** Hindsight通过结构化记忆架构和核心操作，为智能体提供了更有效的长时记忆管理和推理能力，证明了将记忆作为一等推理基板的重要性，为下一代智能体系统奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hindsight+is+20%2F20%3A+Building+Agent+Memory+that+Retains%2C+Recalls%2C+and+Reflects，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12818&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [91] [What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation](https://arxiv.org/abs/2512.12839)
*Dingyi Yang, Qin Jin*

**主要类别:** cs.CL

**AI概要:** 该研究针对长篇小说自动评估难题，创建了首个大规模基准LongStoryEval（600本书，平均12.1万词），提出了8个关键评估标准，比较了三种评估方法，并开发了高效的NovelCritique模型，在人类评估对齐方面优于GPT-4o。


<details>
  <summary>更多</summary>
  
**动机:** 解决长篇故事（超过10万个标记）自动评估的挑战性问题，理解读者最关注的评估维度，并探索有效的长篇故事评估方法。

**方法:** 创建包含600本新出版书籍的大规模基准LongStoryEval，分析用户提到的评估方面，提出评估标准结构，比较聚合式、增量更新和摘要式三种评估方法的有效性，并开发基于摘要式框架的8B参数NovelCritique模型。

**结果:** 发现聚合式和摘要式评估表现更好，聚合式在细节评估方面更优，摘要式效率更高。NovelCritique模型在人类评估对齐方面超越了GPT-4o等商业模型。

**结论:** 该研究为长篇故事自动评估提供了首个大规模基准和有效的评估框架，提出的NovelCritique模型在效率和准确性方面都表现出色，为长篇内容评估领域做出了重要贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Matters+in+Evaluating+Book-Length+Stories%3F+A+Systematic+Study+of+Long+Story+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12839，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12839&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.

</details>


### [92] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia, Yuan Pu, Finn Guo, Monica Agrawal*

**主要类别:** cs.CL

**AI概要:** 研究发现大型语言模型在临床诊断选择题上的优异表现并非完全基于概率推理，而是通过频率统计方法FBPR也能达到类似性能，两者正确率重叠度低，表明具有互补性


<details>
  <summary>更多</summary>
  
**动机:** 探究大型语言模型在临床诊断基准测试中的表现是否真正反映了概率推理能力，还是仅仅基于训练语料中的频率统计

**方法:** 提出Frequency-Based Probabilistic Ranker (FBPR)方法，使用平滑朴素贝叶斯算法基于概念-诊断共现统计对选项进行评分，并与LLM在MedQA数据集上的表现进行对比

**结果:** FBPR在使用相同预训练语料时达到与对应LLM相当的性能，但两者正确回答的问题重叠度仅略高于随机概率，显示互补优势

**结论:** 显式概率基线方法仍具重要价值，可作为性能参考点和潜在混合方法的补充信号，LLM的性能机制不同于简单频率聚合，但传统低复杂度专家系统方法仍占基准性能的很大部分

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Counting+Clues%3A+A+Lightweight+Probabilistic+Baseline+Can+Match+an+LLM，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12868，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12868&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [93] [Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping](https://arxiv.org/abs/2512.12950)
*Lingyi Meng, Maolin Liu, Hao Wang, Yilan Cheng, Qi Yang, Idlkaid Mohanmmed*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种人机协作的多智能体框架，用于构建多语言法律术语数据库，特别针对中英日语言对中的同形异义词问题，通过AI处理重复性任务和人类专家监督审核的方式，提高了术语映射的准确性和一致性。


<details>
  <summary>更多</summary>
  
**动机:** 中英日等语言对中存在大量同形异义词但含义不同的法律术语，现有资源和标准化工具有限，需要解决跨语言法律术语准确映射的挑战。

**方法:** 采用人机协作的多智能体框架，AI负责OCR、文本分割、语义对齐和初始术语提取等重复性任务，人类专家提供监督、审查和基于法律知识的判断，涵盖从原始文档预处理到术语提取、映射和质量保证的全过程。

**结果:** 使用包含35部中国关键法规的三语平行语料库进行测试，实验结果表明该人机协作工作流程提高了多语言法律术语映射的精确性和一致性，相比传统人工方法具有更好的可扩展性。

**结论:** 人机协作的多智能体方法能有效解决跨语言法律术语映射问题，结合AI的效率优势和人类专家的领域知识，为多语言法律术语资源建设提供了可扩展的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Building+from+Scratch%3A+A+Multi-Agent+Framework+with+Human-in-the-Loop+for+Multilingual+Legal+Terminology+Mapping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12950&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.

</details>


### [94] [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967)
*Weizhou Shen, Ziyi Yang, Chenliang Li, Zhiyuan Lu, Miao Peng, Huashan Sun, Yingcheng Shi, Shengyi Liao, Shaopeng Lai, Bo Zhang, Dayiheng Liu, Fei Huang, Jingren Zhou, Ming Yan*

**主要类别:** cs.CL

**AI概要:** QwenLong-L1.5是一个通过系统化后训练创新实现卓越长上下文推理能力的模型，在长上下文推理基准测试中表现与GPT-5和Gemini-2.5-Pro相当，相比基线平均提升9.90分。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决长上下文推理的挑战，需要开发能够进行多跳推理和全局证据定位的能力，同时克服长上下文强化学习的不稳定性问题，以及处理超长序列的架构限制。

**方法:** 1) 长上下文数据合成管道：通过将文档解构为原子事实及其关系，程序化组合可验证的推理问题；2) 稳定化强化学习：采用任务平衡采样和任务特定优势估计，提出自适应熵控制策略优化；3) 内存增强架构：开发多阶段融合RL训练的内存管理框架，支持超过4M tokens的任务。

**结果:** 在超长任务(1M~4M tokens)上，内存代理框架相比代理基线获得9.48分的提升，长上下文推理能力还提升了科学推理、内存工具使用和扩展对话等一般领域的性能。

**结论:** QwenLong-L1.5通过系统化的后训练创新成功实现了卓越的长上下文推理能力，其技术突破为处理超长序列和复杂推理任务提供了有效的解决方案，在多个基准测试中达到领先水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QwenLong-L1.5%3A+Post-Training+Recipe+for+Long-Context+Reasoning+and+Memory+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12967&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.

</details>


### [95] [Authors Should Annotate](https://arxiv.org/abs/2512.12976)
*Marcus Ma, Cole Johnson, Nolan Bridges, Jackson Trager, Georgios Chochlakis, Shrikanth Narayanan*

**主要类别:** cs.CL

**AI概要:** 论文提出了作者标注（author labeling）方法，让文档作者在创作时直接标注数据，相比第三方标注在情感和主观特征方面质量更高、速度更快、成本更低。通过在商业聊天机器人中部署该系统，产品推荐的点击率提升了534%。


<details>
  <summary>更多</summary>
  
**动机:** 传统第三方标注存在局限性，特别是对于情感、信念等自我中心特征，直接从文档来源获取信息比第三方代理更优。

**方法:** 与拥有1万用户的商业聊天机器人合作，部署作者标注系统：识别任务相关查询、实时生成标注问题、记录作者回答。采用在线学习模型架构进行产品推荐。

**结果:** 作者标注相比行业广告基线点击率提升534%；在情感分析任务中，作者标注比三种传统标注方法质量更高、获取更快、成本更低。

**结论:** 作者标注特别适用于自我中心和主观信念的标注任务，质量显著优于第三方标注。为促进科研应用，发布了学术版作者标注服务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Authors+Should+Annotate，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.12976，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.12976&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.

</details>


### [96] [An Open and Reproducible Deep Research Agent for Long-Form Question Answering](https://arxiv.org/abs/2512.13059)
*Ikuya Yamada, Wataru Ikeda, Ko Yoshida, Mengyu Ye, Hinata Sugimoto, Masatoshi Suzuki, Hisanori Ozaki, Jun Suzuki*

**主要类别:** cs.CL

**AI概要:** 一个基于开源大语言模型和网络搜索API的长问答研究系统，通过迭代检索、推理和合成，结合LLM作为评判的偏好调优，在清晰度、洞察力和事实性方面显著提升回答质量。


<details>
  <summary>更多</summary>
  
**动机:** 开发一个开放的长形式问答系统，解决真实开放域设置中的信息检索和推理挑战，通过多维度评估提升回答质量。

**方法:** 结合开源LLM和开放网络搜索API进行迭代检索、推理与合成，应用基于LLM作为评判的偏好调优方法来评估清晰度、洞察力和事实性。

**结果:** 实验结果显示，所提方法在所有三个评估维度（清晰度、洞察力和事实性）上一致提高了回答质量。

**结论:** 该系统在NeurIPS 2025 MMU-RAG竞赛中获胜，证明了开放深度研究系统在长形式问答任务中的有效性，源代码已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Open+and+Reproducible+Deep+Research+Agent+for+Long-Form+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13059，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13059&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.

</details>


### [97] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah, Akshit Agarwal, Kanak Garg, Mourad Heddaya*

**主要类别:** cs.CL

**AI概要:** 该研究通过双曲正切曲线建立谈判让步动态的数学模型，提出burstiness tau和CRI两个指标量化让步行为，发现LLM在谈判中表现僵化、缺乏情境适应性，且能力不随模型改进而提升


<details>
  <summary>更多</summary>
  
**动机:** 人类谈判是复杂的上下文敏感任务，需要研究如何量化谈判动态并比较人类与LLM在谈判中的表现差异

**方法:** 建立基于双曲正切曲线的让步动态数学模型，提出burstiness tau和CRI指标，在大规模实证研究中比较人类与4种先进LLM在多种谈判场景下的表现

**结果:** LLM系统性地锚定在协议区极端位置，缺乏情境适应能力，策略多样性有限，偶尔使用欺骗策略，且谈判能力不随模型改进而提升

**结论:** 当前LLM谈判能力存在根本性局限，需要开发能够更好内化对手推理和上下文依赖策略的模型

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+Rationalis%3F+Measuring+Bargaining+Capabilities+of+AI+Negotiators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13063，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13063&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [98] [Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing](https://arxiv.org/abs/2512.13109)
*Zewen Qiang, Sendong Zhao, Haochun Wang, Bing Qin, Ting Liu*

**主要类别:** cs.CL

**AI概要:** 该论文发现大语言模型处理长文本时存在'中间迷失'问题，除了已知的位置编码偏差外，还识别出初始显著性因素。通过缩放初始token与其他token的注意力权重，结合现有方法，在长文本处理任务上获得最高3.6%的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在长文本序列处理中存在'中间迷失'问题，表现为U形注意力偏差，模型过度关注文本开头和结尾而忽略中间部分。虽然之前研究归因于位置编码，但作者发现还有初始显著性因素需要解决。

**方法:** 研究发现注意力计算中存在初始显著性现象：相对于初始token具有更高注意力权重的token在预测下一个token时会获得更多关注。基于此，提出通过缩放初始token与其他token之间的注意力权重来改善模型处理长文本的能力。

**结果:** 该方法在MDQA数据集上实现了最高3.6%的性能提升。当与现有的减少位置编码偏差的方法结合使用时，在KV-Retrieval任务中进一步获得最高3.4%的性能提升。

**结论:** 初始显著性是大语言模型处理长文本时U形注意力偏差的一个重要因素。通过利用这一特性调整注意力权重分配，可以有效改善模型的长文本处理能力，且与现有方法结合能获得更好的性能提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncovering+the+Role+of+Initial+Saliency+in+U-Shaped+Attention+Bias%3A+Scaling+Initial+Token+Weight+for+Enhanced+Long-Text+Processing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13109&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

</details>


### [99] [Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models](https://arxiv.org/abs/2512.13194)
*Chendong Sun*

**主要类别:** cs.CL

**AI概要:** EARS提出了一种自适应拒绝采样方法，通过动态调整接受阈值来解决推测解码中的随机拒绝问题，显著提升推理效率


<details>
  <summary>更多</summary>
  
**动机:** 传统推测解码使用固定随机阈值，在高不确定性场景中会导致合理的候选token被随机拒绝，影响推理效率

**方法:** 引入EARS方法，通过目标模型自身的预测不确定性（1 - max(P_target)）动态调整接受阈值，加入与不确定性成比例的容忍项

**结果:** 在创意写作和开放域QA任务中，EARS显著提升推测解码效率，在GSM8K基准上实现18.12%的吞吐量提升，仅带来0.84%的准确率下降

**结论:** EARS无需修改模型架构即可无缝集成到现有推测解码框架中，有效减少随机拒绝同时保持模型置信时的严格标准

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Adaptive+Rejection+Sampling+for+Accelerating+Speculative+Decoding+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13194，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13194&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.

</details>


### [100] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou, Ling Yang, Yunzhe Qi, Sirui Chen, Mengting Ai, Ke Shen, Jingrui He, Mengdi Wang*

**主要类别:** cs.CL

**AI概要:** AutoTool是一个让大语言模型具备动态工具选择能力的框架，通过构建大规模数据集和双阶段优化管道，在多个基准测试中显著优于现有方法，并能泛化到未见过的工具。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法假设固定的工具清单，限制了LLM智能体对新工具或演进工具集的适应能力，需要开发能够动态选择工具的框架。

**方法:** 构建包含20万条数据和1000+工具的数据集，采用双阶段优化：监督和强化学习的轨迹稳定化，以及KL正则化的Plackett-Luce排序来优化多步工具选择。

**结果:** 在10个基准测试中，AutoTool在数学科学推理、搜索问答、代码生成和多模态理解等方面平均提升6.4%、4.5%、7.7%和6.9%，且能泛化到未见工具。

**结论:** AutoTool成功解决了动态工具选择问题，显著提升了LLM智能体的性能和泛化能力，为工具使用的适应性提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoTool%3A+Dynamic+Tool+Selection+and+Integration+for+Agentic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13278&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [101] [AIR: Post-training Data Selection for Reasoning via Attention Head Influence](https://arxiv.org/abs/2512.13279)
*Jinrui Liu, Jeff Wu, Xuanguang Pan, Gavin Cheung, Shuai Ma, Chongyang Tao*

**主要类别:** cs.CL

**AI概要:** 提出AIR框架，基于注意力机制原理选择高价值训练数据，提升LLM推理能力蒸馏效率


<details>
  <summary>更多</summary>
  
**动机:** 现有数据选择方法无法捕捉推理步骤的因果重要性，限制了蒸馏效率

**方法:** 利用检索头的机制洞察，识别关键注意力头，构建弱化参考模型，计算注意力影响分数

**结果:** 在多个推理基准测试中一致提升推理准确率，超越启发式基线方法

**结论:** 建立了机制驱动的数据高效方法，用于LLM的推理蒸馏

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AIR%3A+Post-training+Data+Selection+for+Reasoning+via+Attention+Head+Influence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13279，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13279&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.

</details>


### [102] [Integrating Causal Reasoning into Automated Fact-Checking](https://arxiv.org/abs/2512.13286)
*Youssra Rebboud, Pasquale Lisena, Raphael Troncy*

**主要类别:** cs.CL

**AI概要:** 提出结合事件关系提取、语义相似度计算和基于规则推理的方法，通过检测事件链之间的逻辑不一致性来增强事实核查的因果推理能力


<details>
  <summary>更多</summary>
  
**动机:** 当前自动事实核查方法缺乏专门的因果推理能力，无法有效检测事件间的错误因果关系，错失了语义丰富可解释性的机会

**方法:** 结合事件关系提取、语义相似度计算和基于规则的推理，检测声明与证据中事件链之间的逻辑不一致性

**结果:** 在两个事实核查数据集上评估，建立了将细粒度因果事件关系整合到事实核查中的首个基线

**结论:** 该方法为事实核查中的因果事件关系整合提供了首个基准，增强了裁决预测的可解释性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Causal+Reasoning+into+Automated+Fact-Checking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13286，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13286&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.

</details>


### [103] [MiniLingua: A Small Open-Source LLM for European Languages](https://arxiv.org/abs/2512.13298)
*Anna Aksenova, Boris Zverkov, Nicola Dainese, Alexander Nikitin, Pekka Marttinen*

**主要类别:** cs.CL

**AI概要:** MiniLingua是一个10亿参数的多语言开源大语言模型，专门为13种欧洲语言训练，在多个任务上超越了预算更大的同类模型EuroLLM，并与最先进模型在开放生成任务上保持竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 解决大语言模型计算成本高、隐私担忧和英语中心化的问题，开发小型高效的多语言模型以实现设备端使用。

**方法:** 从零开始训练10亿参数的MiniLingua模型，支持13种欧洲语言，采用指令微调技术，并设计了平衡覆盖范围和指令跟随能力的架构。

**结果:** 指令微调版MiniLingua在摘要、分类、开放和封闭式问答任务上超越EuroLLM，在开放生成任务上与最先进模型竞争，模型权重、分词器和源代码均已发布。

**结论:** 小型高效的10亿参数模型可以在多语言环境中实现强大性能，为设备端AI应用提供了可行解决方案，证明了参数效率与性能的平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MiniLingua%3A+A+Small+Open-Source+LLM+for+European+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13298，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13298&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.

</details>


### [104] [FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://arxiv.org/abs/2512.13330)
*Joona Kytöniemi, Jousia Piha, Akseli Reunamo, Fedor Vitiugin, Farrokh Mehryary, Sampo Pyysalo*

**主要类别:** cs.CL

**AI概要:** FIN-bench-v2是一个用于评估芬兰语大语言模型的统一基准套件，整合了多个芬兰语基准测试，涵盖阅读理解、常识推理、情感分析等任务，并提供多种提示格式和评估配置。


<details>
  <summary>更多</summary>
  
**动机:** 为芬兰语大语言模型提供一个统一的评估基准，整合现有芬兰语基准测试资源，解决评估标准不一致的问题，并确保任务质量。

**方法:** 1) 整合芬兰语版本的广泛使用基准和原始FIN-bench的扩展版本；2) 将数据集转换为HuggingFace格式，包含填空和多项选择提示；3) 使用2.15B参数模型通过学习曲线选择稳健任务；4) 评估更大的指令调优模型。

**结果:** 创建了一个包含多种任务类型和提示格式的统一芬兰语基准套件，所有数据集、提示和评估配置都已公开可用，并通过质量筛选确保了任务的可靠性。

**结论:** FIN-bench-v2为芬兰语LLM评估提供了标准化、高质量的基准测试平台，促进了芬兰语NLP研究的发展，所有资源已开源供社区使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FIN-bench-v2%3A+A+Unified+and+Robust+Benchmark+Suite+for+Evaluating+Finnish+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13330，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13330&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

</details>


### [105] [Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers](https://arxiv.org/abs/2512.13363)
*Shibani Sankpal*

**主要类别:** cs.CL

**AI概要:** 该研究提出情绪漂移概念，使用DistilBERT和RoBERTa模型在句子层面分析心理健康文本中的情绪变化模式


<details>
  <summary>更多</summary>
  
**动机:** 传统情感分析通常将整个文本分类为积极、消极或中性，忽略了文本内部情绪的细微变化，特别是在心理健康相关消息中这种情绪漂移现象很重要

**方法:** 使用预训练的Transformer模型（DistilBERT和RoBERTa）进行句子级情感分析，计算情绪漂移分数来量化情绪状态的变化

**结果:** 研究发现了心理健康对话中情绪升级或缓解的模式，提供了对情绪动态变化的深入洞察

**结论:** 该方法论可以应用于更好地理解内容中的情绪动态，为心理健康领域的文本分析提供了新的视角和工具

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+Emotion+Drift+in+Mental+Health+Text+Using+Pre-Trained+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13363，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13363&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.

</details>


### [106] [Large language models are not about language](https://arxiv.org/abs/2512.13441)
*Johan J. Bolhuis, Andrea Moro, Stephen Crain, Sandiway Fong*

**主要类别:** cs.CL

**AI概要:** 该论文认为大语言模型对语言学无用，因为它们是基于概率的模型需要大量数据来分析外部化的词串，而人类语言基于内部计算系统递归生成层次化思维结构，只需极少外部输入就能区分真实语言和不可能语言


<details>
  <summary>更多</summary>
  
**动机:** 批判当前大语言模型在语言学应用中的局限性，强调人类语言的内在计算本质与统计模型的根本差异

**方法:** 通过对比分析人类语言的内在计算特性与大语言模型的概率统计特性

**结果:** 揭示了大语言模型无法真正理解语言的内在结构，只是表面层面的模式匹配

**结论:** 大语言模型不适合用于真正的语言学研究，因为它们缺乏人类语言的内在计算机制和递归生成能力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+language+models+are+not+about+language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13441&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.

</details>


### [107] [Scaling Laws for Code: Every Programming Language Matters](https://arxiv.org/abs/2512.13472)
*Jian Yang, Shawn Guo, Lin Jing, Wei Zhang, Aishan Liu, Chuan Hao, Zhoujun Li, Wayne Xin Zhao, Xianglong Liu, Weifeng Lv, Bryan Dai*

**主要类别:** cs.CL

**AI概要:** 本文首次系统探索了多语言代码预训练的缩放定律，通过1000+实验发现解释型语言比编译型语言从模型规模和数据增长中获益更多，多语言预训练具有协同效应，并提出了基于比例的多语言缩放定律来优化训练资源分配。


<details>
  <summary>更多</summary>
  
**动机:** 现有代码大语言模型的缩放定律主要关注语言无关设置，忽略了不同编程语言在预训练中的差异影响和多语言软件开发的现实需求，导致性能预测不准确。

**方法:** 进行了超过1000个实验（相当于336,000+ H800小时），涵盖多种编程语言、模型规模（0.2B到14B参数）和数据集规模（1T tokens），分析不同语言在预训练中的表现和相互影响。

**结果:** 发现解释型语言（如Python）比编译型语言（如Rust）从模型规模和数据增长中获益更多；多语言预训练在语法相似的语言间具有协同效应；并行配对预训练策略显著提升了跨语言能力；提出了比例依赖的多语言缩放定律。

**结论:** 通过优先考虑高效用语言、平衡高协同语言对、减少快速饱和语言的分配，提出的多语言缩放定律能在相同计算预算下实现比均匀分配更优的平均性能，为多语言代码模型训练提供了优化策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Laws+for+Code%3A+Every+Programming+Language+Matters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13472&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.

</details>


### [108] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
*Yilin Jiang, Mingzi Zhang, Sheng Jin, Zengyi Yu, Xiangjie Kong, Binghao Tu*

**主要类别:** cs.CL

**AI概要:** 该论文提出了EMNLP框架，用于评估扮演教师角色的LLMs在人格特征、道德发展阶段和伦理风险方面的表现，通过88个教师专用道德困境和软提示注入测试，发现教师角色LLMs比人类教师更理想化和极化，抽象道德推理强但情感复杂情境处理差，推理能力强的模型更容易受到有害提示注入影响。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究缺乏对模拟职业角色的大型语言模型(LLMs)进行全面的心理和伦理评估，特别是在教育领域需要确保教师角色LLMs的道德和心理对齐。

**方法:** 提出EMNLP框架，扩展现有量表并构建88个教师专用道德困境，使用目标软提示注入集评估教师角色LLMs的合规性和脆弱性，在14个LLMs上进行实验。

**结果:** 教师角色LLMs表现出比人类教师更理想化和极化的人格特征，在抽象道德推理方面表现优异但在情感复杂情境中表现较差，推理能力强的模型更容易受到有害提示注入的影响。

**结论:** 这是首个评估教育AI中教师角色LLMs伦理和心理对齐的基准测试，揭示了能力与安全性之间的悖论，模型温度等超参数除某些风险行为外影响有限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EMNLP%3A+Educator-role+Moral+and+Normative+Large+Language+Models+Profiling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2508.15250，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2508.15250&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.

</details>


### [109] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

**主要类别:** cs.CL

**AI概要:** 论文提出了非解析推理(NRR)框架，通过多向量嵌入、非塌缩注意力和上下文身份追踪三个组件，在推理过程中保持语义模糊性，只在需要时进行解析，解决了现有语言模型过早语义塌缩的问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前语言模型存在过早语义塌缩的核心架构限制，softmax竞争和贪婪解码导致模型在获得足够上下文前就丢弃有效解释，造成脆弱的推理和上下文失效。

**方法:** NRR框架包含：(1)多向量嵌入保持每个token的多种可行解释；(2)非塌缩注意力防止层间赢家通吃动态；(3)上下文身份追踪为重复实体分配上下文特定身份。通过外部解析算子ρ实现显式、可控的语义承诺。

**结果:** 合成评估显示NRR能有效保持模糊性和追踪上下文：CIT增强模型在分布外身份转换任务上达到90.9%准确率，而Transformer基线仅为9.1%。

**结论:** NRR为过早塌缩提供了原则性替代方案，将模糊性重新定义为显式表示状态而非故障模式。AI的关键不是是否应该解析模糊性，而是何时、如何以及由谁控制解析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-Resolution+Reasoning%3A+A+Framework+for+Preserving+Semantic+Ambiguity+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13478，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13478&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [110] [Advancing Bangla Machine Translation Through Informal Datasets](https://arxiv.org/abs/2512.13487)
*Ayon Roy, Risat Rahaman, Sadat Shibly, Udoy Saha Joy, Abdulla Al Kafi, Farig Yousuf Sadeque*

**主要类别:** cs.CL

**AI概要:** 该论文针对孟加拉语机器翻译的不足，特别是非正式语言处理的缺失，通过创建社交媒体和对话文本数据集来改进翻译模型，提升数百万用户的信息获取能力。


<details>
  <summary>更多</summary>
  
**动机:** 孟加拉语是全球第六大语言，但开源机器翻译进展有限。现有研究主要关注正式语言，忽视了更常用的非正式语言，导致大量在线英文资源无法被孟加拉语用户访问。

**方法:** 研究探索当前最先进的翻译模型，并通过从社交媒体和对话文本等非正式来源开发数据集，提出孟加拉语翻译的改进方法。

**结果:** 论文未明确列出具体实验结果，但提出了通过增强数据集和模型来处理自然非正式孟加拉语的方案。

**结论:** 通过专注于非正式语言翻译，该研究旨在推进孟加拉语机器翻译，改善孟加拉语使用者在数字世界中的信息可访问性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Bangla+Machine+Translation+Through+Informal+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13487，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13487&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.

</details>


### [111] [SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping](https://arxiv.org/abs/2512.13494)
*Yu-Chen Lu, Sheng-Feng Yu, Hui-Hsien Weng, Pei-Shuo Wang, Yu-Fang Hu, Liang Hung-Chun, Hung-Yueh Chiang, Kai-Chiang Wu*

**主要类别:** cs.CL

**AI概要:** SkipCat是一种新颖的低秩压缩框架，通过层内共享低秩投影和块跳过技术，在相同压缩率下保留更多有效秩，显著提升压缩模型性能


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型参数量庞大，在边缘设备部署时面临计算和内存资源限制的挑战。传统低秩压缩方法需要大幅降低秩数才能获得效率提升，但会导致性能显著下降

**方法:** 提出SkipCat框架：1) 层内共享低秩投影方法，多个共享输入的矩阵使用共同投影；2) 块跳过技术，在低秩分解中省略选定子块的计算和内存传输

**结果:** 在相同压缩率下，零样本任务准确率比先前低秩压缩方法提升7%，无需额外微调

**结论:** SkipCat通过最大化有效秩的策略，在严格资源约束下有效保持了模型性能，为边缘设备部署大语言模型提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SkipCat%3A+Rank-Maximized+Low-Rank+Compression+of+Large+Language+Models+via+Shared+Projection+and+Block+Skipping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13494，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13494&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

</details>


### [112] [PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation](https://arxiv.org/abs/2512.13552)
*Hour Kaing, Raj Dabre, Haiyue Song, Van-Hien Tran, Hideki Tanaka, Masao Utiyama*

**主要类别:** cs.CL

**AI概要:** PrahokBART是一个专门为高棉语(Khmer)训练的小型序列到序列预训练模型，通过整合分词和规范化等语言学组件，在机器翻译、文本摘要和标题生成任务上超越了多语言模型mBART50。


<details>
  <summary>更多</summary>
  
**动机:** 现有多语言模型忽视了高棉语的语言学特性问题，需要专门针对高棉语开发预训练模型，通过提高语料质量和处理语言学问题来提升性能。

**方法:** 使用精心策划的高棉语和英语语料从头训练PrahokBART模型，整合了分词和规范化等语言学组件，重点关注语料质量改进和高棉语语言学问题处理。

**结果:** 在机器翻译、文本摘要和标题生成三个生成任务上，PrahokBART的表现优于强大的多语言预训练模型mBART50。

**结论:** 该研究证明了针对特定语言开发专门预训练模型的重要性，分析了各语言学模块的影响，并验证了模型在高棉语文本生成中处理空格问题的有效性，这对高棉语文本的自然性至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PrahokBART%3A+A+Pre-trained+Sequence-to-Sequence+Model+for+Khmer+Natural+Language+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13552&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This work introduces {\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.

</details>


### [113] [Verifying Rumors via Stance-Aware Structural Modeling](https://arxiv.org/abs/2512.13559)
*Gibson Nkhata, Uttamasha Anjally Oyshi, Quan Mai, Susan Gauch*

**主要类别:** cs.CL

**AI概要:** 提出一种立场感知的结构建模方法，通过编码帖子立场信号并按立场类别聚合回复嵌入，结合立场分布和层级深度作为协变量，显著提升了社交媒体谣言真实性预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有模型难以在transformer编码器的序列长度限制下同时捕获语义内容、立场信息和对话结构，需要更有效的方法来验证社交媒体谣言。

**方法:** 采用立场感知结构建模，为每个帖子编码立场信号，按立场类别聚合回复嵌入，引入立场分布和层级深度作为协变量来捕捉立场不平衡和回复深度的影响。

**结果:** 在基准数据集上的广泛实验表明，该方法在预测谣言真实性方面显著优于现有方法，并证明模型适用于早期检测和跨平台泛化。

**结论:** 所提出的立场感知结构建模方法能够有效整合语义、立场和结构信息，为社交媒体谣言验证提供了可扩展且语义丰富的表示方案，具有实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Verifying+Rumors+via+Stance-Aware+Structural+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13559&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.

</details>


### [114] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, Senjie Jin, Jiejun Tan, Yanbin Yin, Jiongnan Liu, Zeyu Zhang, Zhongxiang Sun, Yutao Zhu, Hao Sun, Boci Peng, Zhenrong Cheng, Xuanbo Fan, Jiaxin Guo, Xinlei Yu, Zhenhong Zhou, Zewen Hu, Jiahao Huo, Junhao Wang, Yuwei Niu, Yu Wang, Zhenfei Yin, Xiaobin Hu, Yue Liao, Qiankun Li, Kun Wang, Wangchunshu Zhou, Yixin Liu, Dawei Cheng, Qi Zhang, Tao Gui, Shirui Pan, Yan Zhang, Philip Torr, Zhicheng Dou, Ji-Rong Wen, Xuanjing Huang, Yu-Gang Jiang, Shuicheng Yan*

**主要类别:** cs.CL

**AI概要:** 本文对基于基础模型的智能体记忆研究进行了系统性综述，提出了从形式、功能和动态三个维度分析记忆系统的新框架，并总结了当前的研究现状、基准测试和开源框架，同时展望了未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 当前智能体记忆研究快速发展但领域碎片化严重，现有工作动机、实现和评估方法差异大，传统分类方法无法捕捉现代记忆系统多样性，需要统一的框架来整合和澄清该领域。

**方法:** 通过三个统一视角分析智能体记忆：形式维度（token级、参数化和潜在记忆）、功能维度（事实记忆、经验记忆和工作记忆）、动态维度（记忆形成、演化和检索过程）。

**结果:** 提出了一个全面的智能体记忆分类框架，区分了记忆的不同实现形式、功能类型和动态特性，并整理了相关的基准测试和开源工具资源。

**结论:** 该综述为智能体记忆研究提供了概念基础，建议将记忆作为未来智能体设计的一等公民，并指出了记忆自动化、强化学习集成、多模态记忆等新兴研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory+in+the+Age+of+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13564，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13564&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [115] [ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding](https://arxiv.org/abs/2512.13586)
*Jia-Nan Li, Jian Guan, Wei Wu, Chongxuan Li*

**主要类别:** cs.CL

**AI概要:** ReFusion是一种新颖的掩码扩散模型，通过将并行解码从token级别提升到slot级别来解决现有方法的效率问题，实现了性能提升和显著加速。


<details>
  <summary>更多</summary>
  
**动机:** 自回归模型(ARMs)存在顺序推理速度慢的问题，而掩码扩散模型(MDMs)虽然提供并行替代方案，但存在计算开销高和生成不连贯的问题。

**方法:** 采用迭代式"规划-填充"解码过程：扩散规划步骤识别弱依赖的slot，自回归填充步骤并行解码这些slot。slot级别的设计支持完整的KV缓存重用。

**结果:** 在7个基准测试中，ReFusion性能比先前MDMs提升34%，速度提升18倍以上，同时保持对强ARMs的2.33倍平均加速。

**结论:** ReFusion成功解决了MDMs的关键局限性，在保持高效并行生成的同时，显著提升了生成质量和速度，弥合了与ARMs的性能差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReFusion%3A+A+Diffusion+Large+Language+Model+with+Parallel+Autoregressive+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13586，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13586&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

</details>


### [116] [Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization](https://arxiv.org/abs/2512.13598)
*Daniel Melcer, Qi Chen, Wen-Hao Chiang, Shweta Garg, Pranav Garg, Christian Bock*

**主要类别:** cs.CL

**AI概要:** 论文研究了基于文本梯度类比的自劢提示优化技术，发现虽然这些方法能提升大语言模型性能，但梯度类比并不能准确解释其行为机制。


<details>
  <summary>更多</summary>
  
**动机:** 研究自动提示优化技术的行为机制，特别是基于文本梯度类比的方法，以理解其实际工作原理而非表面上的梯度类比解释。

**方法:** 通过一系列实验和案例研究来分析文本梯度方法的实际行为表现。

**结果:** 实验表明文本梯度方法通常能带来性能提升，但梯度类比并不能准确解释这些方法的实际行为。

**结论:** 研究结果可为选择提示优化策略和开发新方法提供参考，强调需要更准确地理解这些优化技术的真实工作机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Textual+Gradients+are+a+Flawed+Metaphor+for+Automatic+Prompt+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13598，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13598&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.

</details>


### [117] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang, Chankyu Lee, Nayeon Lee, Sheng-Chieh Lin, Wenliang Dai, Yang Chen, Yangyi Chen, Zhuolin Yang, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种级联域强化学习(Cascade RL)方法，通过顺序的域级RL训练来构建通用推理模型Nemotron-Cascade，解决了传统方法中跨域异质性带来的工程复杂性和训练效率问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统强化学习方法在处理跨域异质性(如推理响应长度和验证延迟的差异)时面临工程复杂性高、训练速度慢以及课程设计和超参数选择困难等问题。

**方法:** 提出Cascade RL方法，采用顺序的域级强化学习训练策略，而不是混合不同域的异构提示。该方法包括RLHF作为预对齐步骤，随后进行域级RLVR阶段。

**结果:** 14B模型在RL训练后超越了其SFT教师模型DeepSeek-R1-0528，在LiveCodeBench v5/v6/Pro上表现优异，并在2025年国际信息学奥林匹克竞赛中获得银牌成绩。

**结论:** Cascade RL方法有效降低了工程复杂性，实现了最先进的性能，RLHF预对齐显著提升了模型的推理能力，后续域级RLVR阶段不会降低先前域的性能，甚至可能提升性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nemotron-Cascade%3A+Scaling+Cascaded+Reinforcement+Learning+for+General-Purpose+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13607&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [118] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu, Nam Nguyen, Yinzhu Quan, Austin Zhang*

**主要类别:** cs.CL

**AI概要:** 本文首次对事件序列的时间标记化策略进行实证研究，比较了五种编码方法在不同统计分布数据上的表现，发现没有单一最优策略，性能取决于标记器与数据统计特性的匹配程度。


<details>
  <summary>更多</summary>
  
**动机:** 连续时间表示是大语言模型处理时序事件序列的关键挑战，现有策略如字节级表示或日历标记的效果尚不明确，特别是面对现实世界事件数据从平滑对数正态到离散尖峰模式的多样化统计分布。

**方法:** 通过在实际数据集上微调大语言模型，比较五种时间编码策略：朴素数字字符串、高精度字节级表示、人类语义日历标记、经典均匀分箱和自适应残差标量量化。

**结果:** 分析表明没有单一策略普遍最优，预测性能严重依赖于标记器与数据统计特性的对齐，对数基策略在偏斜分布上表现优异，而以人类为中心的格式在混合模态中表现稳健。

**结论:** 时间标记化策略的选择应基于数据的统计特性进行适配，不同分布类型需要不同的编码方法，强调了数据驱动策略选择的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal+Tokenization+Strategies+for+Event+Sequence+Modeling+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13618，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13618&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


### [119] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega, Dhruv D. Joshi, Matt P. Borkowski*

**主要类别:** cs.CL

**AI概要:** 研究显示，在分类任务中，带有记忆功能的提示型大语言模型（如DeepSeek）比传统BERT模型表现更优，在最高法院判决分类任务中得分高出约2个百分点。


<details>
  <summary>更多</summary>
  
**动机:** 探究大语言模型在分类任务中的响应方式，特别是针对最高法院判决这一具有挑战性的文本分类任务，研究模型记忆准确性。

**方法:** 使用参数高效微调、自动建模等最新LLM微调和检索方法，在两个SCOTUS分类任务（15个主题和279个主题）上进行实验。

**结果:** 基于提示的记忆模型比非提示型模型表现更好，DeepSeek模型在两个任务上都比之前的BERT模型得分高约2个百分点。

**结论:** 带有记忆功能的提示型LLM在处理复杂法律文本分类任务时具有更强的鲁棒性，为法律领域的NLP应用提供了更有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large-Language+Memorization+During+the+Classification+of+United+States+Supreme+Court+Cases，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13654，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13654&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


### [120] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

**主要类别:** cs.CL

**AI概要:** 本研究评估了四种安全对齐消除工具在16个指令调优模型上的效果，发现单次通过方法在能力保持上表现更好，而贝叶斯优化方法产生可变分布偏移。数学推理能力对消除干预最敏感。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型的安全对齐机制虽然阻止有害查询响应，但也阻碍了认知建模、对抗测试和安全分析等合法研究应用。现有消除技术的相对效果尚未明确表征。

**方法:** 评估四种消除工具(Heretic、DECCP、ErisForge、FailSpy)在16个7B-14B参数指令调优模型上的兼容性和量化指标，包括能力保持和分布偏移测量。

**结果:** 单次通过方法在基准子集上表现出更好的能力保持(平均GSM8K变化：ErisForge -0.28pp；DECCP -0.13pp)，贝叶斯优化消除产生可变分布偏移(KL散度：0.043-1.646)，数学推理能力变化范围从+1.51pp到-18.81pp。

**结论:** 研究为研究人员提供了基于证据的消除工具选择标准，主要发现数学推理能力对消除干预最为敏感，效果因工具选择和模型架构而异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+Analysis+of+LLM+Abliteration+Methods%3A+A+Cross-Architecture+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13655&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


### [121] [A stylometric analysis of speaker attribution from speech transcripts](https://arxiv.org/abs/2512.13667)
*Cristina Aggazzotti, Elizabeth Allyn Smith*

**主要类别:** cs.CL

**AI概要:** 该论文提出了StyloSpeaker方法，通过分析语音转录文本的文体特征来进行说话人识别，在语音伪装或合成语音场景下提供了一种基于语言内容的补充识别手段。


<details>
  <summary>更多</summary>
  
**动机:** 传统语音识别依赖声学特征，但在语音伪装或使用文本转语音软件时不可靠，需要基于语言内容的分析方法来识别说话人。

**方法:** 使用StyloSpeaker文体计量方法，从字符、词汇、标记、句子和文体等多个维度提取特征，评估两个转录文本是否来自同一说话人，并比较不同转录格式的效果。

**结果:** 在标准化转录格式下获得更高的识别性能（除最强话题控制条件外），整体性能在最强话题控制条件下最佳，且比黑盒神经网络方法更具可解释性。

**结论:** 基于内容的说话人识别方法在语音伪装场景下有效，文体特征分析可提供有价值的识别信息，标准化文本处理有助于提高识别准确率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+stylometric+analysis+of+speaker+attribution+from+speech+transcripts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13667，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13667&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.

</details>


### [122] [Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676)
*Baixiang Huang, Limeng Cui, Jiapeng Liu, Haoran Wang, Jiawei Xu, Zhuiyue Tan, Yutong Chen, Chen Luo, Yi Liu, Kai Shu*

**主要类别:** cs.CL

**AI概要:** 提出了Personalization Editing框架，将个性化视为模型编辑任务，通过聚类偏好表示进行局部编辑，在保持模型整体能力的同时实现精准偏好对齐。同时引入了基于真实用户查询的UPQA评估数据集。


<details>
  <summary>更多</summary>
  
**动机:** 解决当前LLM个性化方法计算成本高、数据密集、易灾难性遗忘、多轮交互和隐式查询性能下降等问题，以及现有评估基准主要基于风格模仿而非真实用户偏好回忆的局限性。

**方法:** 将个性化概念化为模型编辑任务，引入Personalization Editing框架，使用聚类偏好表示指导局部编辑操作，保持模型原始能力的同时实现精准偏好更新。

**结果:** 实验表明，Personalization Editing在编辑准确性和计算效率上优于微调方法，在多轮对话和隐式偏好问题场景下超越基于提示的基线方法。

**结论:** 该框架为LLM个性化提供了一种高效、精准的解决方案，通过模型编辑范式有效解决了传统方法的局限性，UPQA数据集也为个性化能力评估提供了更真实的基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Effective+Model+Editing+for+LLM+Personalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13676，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13676&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.

</details>


### [123] [Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech](https://arxiv.org/abs/2512.13685)
*Dylan Phelps, Rodrigo Wilkens, Edward Gow-Smith, Lilian Hubner, Bárbara Malcorra, César Rennó-Costa, Marco Idiart, Maria-Cruz Villa-Uriol, Aline Villavicencio*

**主要类别:** cs.CL

**AI概要:** 本研究通过变换文本表面形式但保持语义内容的方法，验证了语言模型在阿尔茨海默病检测中主要依赖语义信息而非表面文本模式，证明了语义障碍可以被有效识别。


<details>
  <summary>更多</summary>
  
**动机:** 解决语言模型在AD检测中可解释性有限的问题，区分真正的认知衰退语言标记与表面文本模式，评估语言模型捕捉底层语义指标的能力。

**方法:** 引入新颖方法：通过改变句法和词汇来变换文本表面形式，同时保持语义内容不变。使用BLEU、chrF和语义相似度评分验证变换效果，并测试生成模型从文本重建图像的能力。

**结果:** 变换显著改变了文本结构和词汇内容（低BLEU/chrF分数），但保持了语义（高语义相似度）。模型在变换文本上的表现与原始文本相似（macro-F1仅有小幅偏差），证明主要依赖语义信息。图像变换会引入噪声降低分类准确率。

**结论:** 仅使用语义信息，基于语言模型的分类器仍能有效检测AD。该方法为识别难以检测的语义障碍提供了新途径，为早期检测系统开辟了新路径，并能消除可能的虚假相关性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+surface+form%3A+A+pipeline+for+semantic+analysis+in+Alzheimer%27s+Disease+detection+from+spontaneous+speech，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.13685，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13685&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.

</details>
