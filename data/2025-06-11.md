<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 126]
- [cs.AI](#cs.AI) [总数: 34]
- [stat.ML](#stat.ML) [总数: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li, Song Liu, Weiguo Wu, Shiqiang Nie, Jinyu Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为KVmix的新型混合精度量化方法，用于优化大语言模型推理过程中KV Cache的高内存需求，通过动态分配不同层的量化精度和引入长上下文优化策略，在保持高质量生成的同时显著降低了内存使用并提升了推理速度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的KV Cache量化方法要么采用静态单一精度分配，要么无法在长上下文任务中动态优先处理关键KV对，导致内存-准确率-吞吐量之间的权衡。

**方法:** 提出了一种基于梯度重要性分析的混合精度量化方法KVmix，并引入了动态长上下文优化策略。

**结果:** 在Llama和Mistral等LLMs上，KVmix以极低的量化配置（Key 2.19bit，Value 2.38bit）实现了接近无损的推理性能，同时达到了4.9倍的内存压缩和5.3倍的推理吞吐量提升。

**结论:** KVmix在不显著影响推理性能的前提下，实现了高效的内存压缩和推理加速，为资源受限平台部署LLMs提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KVmix%3A+Gradient-Based+Layer+Importance-Aware+Mixed-Precision+Quantization+for+KV+Cache，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08018，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08018&send_immediately=true&force_search=false)

**原文摘要:** The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [2] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells, Geraldine Henningsen, Brice Bolane Tchinde Kengne*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于半监督学习的方法，用于将难民统计数据细化到地理网格单元，提高了对局部迁移模式的理解。


<details>
  <summary>更多</summary>
  
**动机:** 更深入地理解撒哈拉以南非洲25个国家内部流离失所的驱动因素需要高分辨率的数据集。现有的区域性和国家性统计数据无法提供足够的细节来识别局部迁移模式。

**方法:** 该研究采用了一种半监督的方法，利用标签传播算法将难民营统计数据从行政边界离散到0.5度的网格单元中。

**结果:** 该方法在将超过1000万难民观测值分配到适当网格单元时达到了92.9%的平均准确率，从而能够识别出之前被掩盖的局部迁移模式。

**结论:** 论文得出结论，通过集成UNHCR的ProGres注册数据与Google Open Buildings提供的卫星建筑轮廓和OpenStreetMap Populated Places的位置坐标，可以实现对难民营统计信息的高度细化分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gridding+Forced+Displacement+using+Semi-Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08019，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08019&send_immediately=true&force_search=false)

**原文摘要:** We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [3] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen, Chuan-Xian Ren, Hong Yan*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的Bi-level Unbalanced Optimal Transport (BUOT)模型，旨在解决Partial domain adaptation问题，在实验中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的加权框架只能描述样本关系而无法充分探索聚类结构，并且权重对不准确预测敏感可能导致对异常值类别的混淆。因此需要一种更有效的方法来处理PDA问题。

**方法:** 提出了一种新的Bi-level Unbalanced Optimal Transport (BUOT)模型，通过样本级和类别级传输的合作机制进行跨域样本对齐并区分异常值类别。

**结果:** 提出了BUOT模型并通过实验验证了其在基准数据集上的有效性与竞争力。

**结论:** 论文提出了一种新的Bi-level Unbalanced Optimal Transport (BUOT)模型，用于解决Partial domain adaptation (PDA)问题。通过结合样本级和类别级传输的合作机制以及标签感知的传输成本，该模型在基准数据集上的实验验证了其竞争力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bi-level+Unbalanced+Optimal+Transport+for+Partial+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08020，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08020&send_immediately=true&force_search=false)

**原文摘要:** Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [4] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou, Weibing Feng, Pin Wu*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的流场预测框架，结合大语言模型和适当正交分解技术，实现了高效的流体动力学预测，在多种条件下表现优异且计算速度大幅提升。


<details>
  <summary>更多</summary>
  
**动机:** 解决传统计算流体动力学方法计算成本高以及现有深度学习模型跨条件迁移能力有限的问题。

**方法:** 将适当正交分解（POD）降维与预训练大语言模型的微调策略相结合，通过POD实现流场特征的压缩表示，同时利用微调模型学习状态空间中系统动力学的编码；设计了面向流体力学的文本模板以增强模型对流场数据的适应性。

**结果:** 在少量样本学习场景下优于传统的Transformer模型，表现出卓越的泛化能力，可将预测时间从数小时减少到几秒，同时保持超过90%的准确率。消融实验揭示了FlowBERT架构中关键组件的贡献。

**结论:** 该研究提出了一种基于大语言模型知识迁移的通用流场预测框架，为快速流体动力学预测提供了新方向，并具有在空气动力学优化、流动控制等工程领域中的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlowBERT%3A+Prompt-tuned+BERT+for+variable+flow+field+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08021，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08021&send_immediately=true&force_search=false)

**原文摘要:** This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [5] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Tianyi Zhou, Heng Huang*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的多模态模型偏好优化方法MBPO，通过对抗扰动生成硬负例并结合在线数据生成，有效解决大型多模态模型中存在的模态不平衡问题，提高模型性能并减少幻觉。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LMM偏好优化方法未能关注LLM骨架网络内部偏置的抑制，且主要依赖离线数据，缺乏对训练过程中动态分布变化的适应能力。

**方法:** 本文提出了一种新的偏好学习框架，即模态平衡偏好优化（MBPO），通过生成硬负例和利用封闭式任务的易验证性来构建更有效的离线偏好数据集，并结合在线响应生成与GRPO进行训练。

**结果:** 实验表明，MBPO可以显著改善LMMs的多模态推理能力，缓解模态不平衡问题。

**结论:** MBPO能够提升LMM在具有挑战性的视觉-语言任务中的表现，并有效减少幻觉。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modality-Balancing+Preference+Optimization+of+Large+Multimodal+Models+by+Adversarial+Negative+Mining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08022，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08022&send_immediately=true&force_search=false)

**原文摘要:** The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [6] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra, Dusan Stosic, Simon Layton*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了如何通过改进舍入模式来提升低精度计算环境下大规模语言模型预训练的稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 为了在不牺牲准确性的前提下提升GPU效率，需要探索更精细的量化方法如MX格式。然而现有方法在实际应用中可能引起模型训练发散，因此需要找到更稳定的解决方案。

**方法:** 论文研究了NVIDIA Blackwell GPU中的Microscaling (MX)格式，并测试了不同舍入模式对模型训练稳定性的影响。

**结果:** 实验证明，传统的舍入模式可能导致训练过程中的发散，而新的round-to-infinity方法可以在MXFP8下成功完成8B模型的预训练。

**结论:** 论文得出结论，使用改进的舍入模式（round-to-infinity）可以成功实现MXFP8下的LLM预训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Recipes+for+Pre-training+LLMs+with+MXFP8，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08027，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08027&send_immediately=true&force_search=false)

**原文摘要:** Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [7] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi, Md Monzurul Islam, Anannya Ghosh Tusti, Shriyank Somvanshi, Subasish Das*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的时空图神经网络框架ST-GraphNet，用于预测自动驾驶汽车事故的严重性，并取得了显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 分析自动驾驶汽车事故严重性的时空动态对于提升城市交通安全性至关重要。

**方法:** 引入了一种时空图神经网络框架ST-GraphNet，并利用GCN、GAT和DSTGCN等不同架构进行模型评估。

**结果:** 提出的ST-GraphNet在粗粒度H3图上实现了97.74%的测试准确率，明显优于最佳细粒度模型（64.7%）。

**结论:** ST-GraphNet在预测自动驾驶汽车事故严重性方面表现出色，突出了空间聚合、动态信息传递和多模态特征集成的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ST-GraphNet%3A+A+Spatio-Temporal+Graph+Neural+Network+for+Understanding+and+Predicting+Automated+Vehicle+Crash+Severity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08051，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08051&send_immediately=true&force_search=false)

**原文摘要:** Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [8] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang, Hao Peng, Senzhang Wang, Haohua Du, Chunyang Liu, Jia Wu, Guanlin Wu*

**主要类别:** cs.LG

**AI概要:** 本文提出了STAMImputer，一种基于时空注意力混合专家网络的交通数据插补方法。通过引入MoE框架和LrSGAT机制，该方法能有效处理块缺失数据并提升非平稳交通数据的插补性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有时间到空间序列方法在处理块缺失数据时效果不佳，且静态图结构限制了模型对非平稳交通数据分布变化的适应性。

**方法:** 提出了一种新的SpatioTemporal Attention Mixture of Experts (STAMImputer) 模型，包含Mixture of Experts (MoE) 框架和Low-rank guided Sampling Graph ATtention (LrSGAT) 机制，用于动态生成反映实时空间相关性的图结构。

**结果:** 在四个交通数据集上的实验表明，与现有最先进的方法相比，STAMImputer取得了显著的性能提升。

**结论:** STAMImputer通过结合MoE框架和LrSGAT机制，有效解决了交通数据插补中的块缺失和非平稳性问题，展示了优越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STAMImputer%3A+Spatio-Temporal+Attention+MoE+for+Traffic+Data+Imputation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08054&send_immediately=true&force_search=false)

**原文摘要:** Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [9] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在不调整参数的前提下，如何通过推理时技术近似监督微调的效果，并提出了适用于实际场景的方法。


<details>
  <summary>更多</summary>
  
**动机:** 监督微调计算成本高昂，因此探索在不改变模型参数的情况下，是否能通过推理时技术来近似其能力。

**方法:** 研究基于Transformer的模型，分析了在上下文学习和有限资源条件下，如何用基础模型近似微调模型的行为，并提供了数据集大小与误差的关系公式。

**结果:** 论文证明了在理想情况下，可以通过在上下文学习中使用基础Transformer模型来近似监督微调的结果，并给出了不同任务下所需数据集大小的数学表达式。

**结论:** 论文得出结论，在理想化假设下，通过监督微调获得的能力可以使用推理时技术在不改变模型参数的情况下进行近似，并且这一结果扩展到了实际场景中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Eliciting+Fine-Tuned+Transformer+Capabilities+via+Inference-Time+Techniques，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08060&send_immediately=true&force_search=false)

**原文摘要:** Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [10] [GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity](https://arxiv.org/abs/2210.16402)
*Artavazd Maranjyan, Mher Safaryan, Peter Richtárik*

**主要类别:** cs.LG

**AI概要:** 该研究提出了GradSkip算法，允许客户端根据数据重要性灵活调整本地训练次数，从而减少计算资源消耗而不影响整体性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决现有分布式优化算法中所有客户端必须执行相同数量本地训练步骤的问题，以提高效率。

**方法:** 重新设计了ProxSkip方法，引入了GradSkip和其扩展版本GradSkip+，并进行了理论证明与实验验证。

**结果:** GradSkip在线性强凸条件下实现了线性收敛，且通信复杂度未受影响；通过实验验证了理论结果的有效性。

**结论:** 论文提出了一种新的分布式优化算法GradSkip，它在不牺牲通信复杂度的前提下允许客户端根据数据重要性进行不同数量的本地训练步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GradSkip%3A+Communication-Accelerated+Local+Gradient+Methods+with+Better+Computational+Complexity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2210.16402，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2210.16402&send_immediately=true&force_search=false)

**原文摘要:** We study a class of distributed optimization algorithms that aim to alleviate
high communication costs by allowing clients to perform multiple local
gradient-type training steps before communication. In a recent breakthrough,
Mishchenko et al. (2022) proved that local training, when properly executed,
leads to provable communication acceleration, and this holds in the strongly
convex regime without relying on any data similarity assumptions. However,
their ProxSkip method requires all clients to take the same number of local
training steps in each communication round. We propose a redesign of the
ProxSkip method, allowing clients with ``less important'' data to get away with
fewer local training steps without impacting the overall communication
complexity of the method. In particular, we prove that our modified method,
GradSkip, converges linearly under the same assumptions and has the same
accelerated communication complexity, while the number of local gradient steps
can be reduced relative to a local condition number. We further generalize our
method by extending the randomness of probabilistic alternations to arbitrary
unbiased compression operators and by considering a generic proximable
regularizer. This generalization, which we call GradSkip+, recovers several
related methods in the literature as special cases. Finally, we present an
empirical study on carefully designed toy problems that confirm our theoretical
claims.

</details>


### [11] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim, Jinho Lee, Jongmin Lee, Byung-Jun Lee*

**主要类别:** cs.LG

**AI概要:** 本文提出了FairDICE，这是首个能够直接优化非线性福利目标的离线多目标强化学习框架，并证明了其在公平性感知性能方面的优势。


<details>
  <summary>更多</summary>
  
**动机:** 现有的线性标量化方法无法捕捉需要非线性和非加性权衡的公平性目标，而针对特定公平目标的在线算法已经提出，但离线设置中缺乏一种统一的方法来优化非线性福利标准。

**方法:** FairDICE利用分布校正估计来联合考虑福利最大化和分布正则化，从而实现稳定且样本高效的学习。

**结果:** 在多个离线基准测试中，FairDICE展示了强大的公平性感知性能。

**结论:** FairDICE是一个离线多目标强化学习框架，能够直接优化非线性福利目标，在公平性感知性能方面优于现有基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FairDICE%3A+Fairness-Driven+Offline+Multi-Objective+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08062，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08062&send_immediately=true&force_search=false)

**原文摘要:** Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [12] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
*Diyuan Wu, Aleksandr Shevchenko, Samet Oymak, Marco Mondelli*

**主要类别:** cs.LG

**AI概要:** 这篇论文分析了通过梯度下降训练得到的token嵌入的结构，表明它们能够有效地捕捉token在数据集中的重要性，并通过实验验证了理论结果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管token嵌入在语言建模中起着关键作用，但其理论理解仍然有限，因此本文旨在填补这一空白。

**方法:** 研究者使用了一个单层softmax注意力模型和线性头进行二分类任务，并通过梯度下降训练来分析token嵌入的结构特性。

**结果:** 研究表明，在仅经过一次梯度训练步骤后，嵌入就能按照数据集中出现频率对token的重要性进行编码，并且在收敛后，softmax能选择出预测标签的重要token。

**结论:** 论文得出结论，通过梯度下降训练得到的token嵌入能够捕捉数据集中token的重要性，并且在实际数据集上的实验结果与理论分析相符。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention+with+Trained+Embeddings+Provably+Selects+Important+Tokens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17282&send_immediately=true&force_search=false)

**原文摘要:** Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [13] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu, Zeyi Liu, Xiao He*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Lite-RVFL的在线学习方法，可以快速高效地适应数据分布变化，无需进行漂移检测或模型再训练。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法通常需要模型再训练或漂移检测，这需要高计算成本且不适合实时应用。

**方法:** 提出了一种新的目标函数，并通过理论分析验证了其可行性，同时推导出高效的增量更新规则。

**结果:** 实验结果验证了该方法在适应概念漂移方面的效率和有效性，并具有捕捉时间模式的潜力。

**结论:** Lite-RVFL是一种轻量级的在线学习方法，能够在不进行漂移检测和重新训练的情况下适应概念漂移。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lite-RVFL%3A+A+Lightweight+Random+Vector+Functional-Link+Neural+Network+for+Learning+Under+Concept+Drift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08063，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08063&send_immediately=true&force_search=false)

**原文摘要:** The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [14] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li, Michael R. Metel, Akiko Takeda*

**主要类别:** cs.LG

**AI概要:** 本文研究了K-means算法的局部最优性，并提出了改进版本，保证局部最优解的实现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管K-means算法被广泛研究，但其局部最优性保证仍缺乏严格分析。

**方法:** 分析K-means算法达到局部最优解的条件，并提出对其进行简单修改以确保局部最优性。

**结果:** 提出的改进方法在实际中能够提供更优的局部解并减少聚类损失。

**结论:** 本文提出了改进的K-means算法，在保持原算法计算复杂度的同时确保局部最优解，并通过实验验证了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modified+K-means+Algorithm+with+Local+Optimality+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06990&send_immediately=true&force_search=false)

**原文摘要:** The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [15] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin, Hailun Xu, Wei Chee Yew, Qi Jia, Yang Luo, Kanchan Sarkar, Danhui Guan, Kai Wang, Yang You*

**主要类别:** cs.LG

**AI概要:** Info-Coevolution是一种新框架，通过在线选择性注释实现模型和数据的协同进化，从而在不损失性能的情况下减少标注和训练成本。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的数据增长迅速，如何高效构建数据集并进行有效训练是机器学习领域的一个重大挑战。传统的保留所有数据的方法效率不高，而主动学习方法则增加了管道复杂性和偏差。因此需要一种新的解决方案。

**方法:** 提出了一种名为Info-Coevolution的框架，利用任务特定模型（以及开源模型），通过在线选择性地标注和整合在线和网络数据来提高数据集效率，同时避免偏差。

**结果:** 在ImageNet-1K等真实数据集上，Info-Coevolution减少了32%的标注和训练成本且无性能损失，并能自动确定节省比例无需手动调整，结合半监督学习可进一步将标注比例降低至50%。

**结论:** Info-Coevolution为模型和数据的协同进化提供了一个高效、无偏差的解决方案，显著提升了数据使用效率，并具有实际应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Info-Coevolution%3A+An+Efficient+Framework+for+Data+Model+Coevolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08070&send_immediately=true&force_search=false)

**原文摘要:** Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [16] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali, Pietro Liò, Jamie Vicary*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的零参数方法来实现神经网络中的近似等变性，通过在损失函数中加入额外项来减少计算复杂性和参数数量，实验结果显示其性能优于或类似于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的等变神经网络方法计算复杂度高且参数多，因此需要一种更简单、高效的方法。

**方法:** 提出了一种零参数的方法，在损失函数中增加一个额外项来在潜在表示中实施近似的等变性，并通过实验学习群表示。

**结果:** 实验表明，该方法在多个数据集中表现良好，倾向于学习规则表示，并能有效减少参数数量同时保持性能。

**结论:** 论文得出结论，所提出的零参数方法通过在损失函数中施加近似等变性，能够以较少的参数实现与其他等变方法相似或更好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parameter-free+approximate+equivariance+for+tasks+with+finite+group+symmetry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08244&send_immediately=true&force_search=false)

**原文摘要:** Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [17] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi, Igor Tchappi, Gilbert Fridgen*

**主要类别:** cs.LG

**AI概要:** 研究发现，在电力价格预测中，传统双季节MSTL模型优于最新的时间序列基础模型。


<details>
  <summary>更多</summary>
  
**动机:** 为了评估新兴的时间序列基础模型在电力价格预测中的有效性，并找出最佳实践方法。

**方法:** 比较了多个最先进的预训练模型（如Chronos-Bolt、Chronos-T5、TimesFM等）与传统统计和机器学习方法在电力价格预测中的表现。

**结果:** Chronos-Bolt和Time-MoE是时间序列基础模型中最出色的，其表现与传统模型相当；然而，双季节MSTL模型在所有国家和评估指标中均显示出持续的优越性能。

**结论:** 在电力价格预测方面，尽管一些时间序列基础模型表现良好，但传统的双季节MSTL模型在多个国家和评估指标中表现出一致的优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+Pre-Trained+Time+Series+Models+for+Electricity+Price+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08113，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08113&send_immediately=true&force_search=false)

**原文摘要:** Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [18] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro, Suzana Vilas Boas de Oliveira, Thiago Henrique Segreto Silva, Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Leonardo André Ambrosio, Marcelo Becker*

**主要类别:** cs.LG

**AI概要:** 该研究通过评估多种特征缩放技术对不同机器学习模型的影响，揭示了某些模型对缩放器的选择高度敏感，而另一些则相对稳健，为从业者提供了选择合适缩放方法的实用指南。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是解决目前缺乏关于特征缩放全面研究的问题，提供有关不同缩放技术如何影响各种机器学习模型性能的深入见解。

**方法:** 研究方法包括系统性评估12种缩放技术，涵盖多个较少使用的转换方法，在14种不同的机器学习算法和16个数据集上进行分类和回归任务。此外，还分析了预测性能和计算成本的影响。

**结果:** 关键发现表明，虽然随机森林和梯度提升模型等集成方法表现稳健，不依赖于特征缩放，但其他常用模型如逻辑回归、支持向量机、TabNet和多层感知器在不同缩放器下表现出显著的性能差异。

**结论:** 论文的结论是，集成方法在很大程度上不受特征缩放的影响，而其他模型如逻辑回归、支持向量机、TabNet和MLPs则对选择的缩放器非常敏感。研究强调了为特定模型选择最佳特征缩放技术的重要性，并提供了模型特定的实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Impact+of+Feature+Scaling+In+Machine+Learning%3A+Effects+on+Regression+and+Classification+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08274&send_immediately=true&force_search=false)

**原文摘要:** This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [19] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu, Lang Cao, Yuanyi Ren, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, Dongmei Zhang*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为Bingo的强化学习框架，通过改进长度奖励设计，提高了大型语言模型的推理效率和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在提高推理效率时往往牺牲准确性，因此需要一种兼顾效率与准确性的新方法。

**方法:** 提出了一种新的强化学习框架Bingo，包含显著性感知长度奖励和动态长度奖励机制。

**结果:** 实验表明，Bingo在多个推理基准测试中优于传统奖励方法，实现了效率与准确性的良好平衡。

**结论:** Bingo框架在提升大型语言模型的推理效率和准确性方面表现出色，证明了高效推理训练的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bingo%3A+Boosting+Efficient+Reasoning+of+LLMs+via+Dynamic+and+Significance-based+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08125&send_immediately=true&force_search=false)

**原文摘要:** Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [20] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin, Nate Gruver, Andrew Gordon Wilson*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了离散扩散模型，特别是掩码扩散模型为何在实践中表现出色，并提出了一个通用的方法SCUD，通过将跳跃时间的已知分布结合到模型中，从而构建出性能更优的模型。


<details>
  <summary>更多</summary>
  
**动机:** 动机是为了解释为什么在实践中，不进行渐进去噪的掩码扩散模型表现最佳，并探索如何利用离散马尔可夫过程与连续马尔可夫过程之间的基本差异来提高模型性能。

**方法:** 研究提出了一种新的方法，称为时间表条件离散扩散（SCUD），该方法将跳跃时间的已知分布结合到离散扩散模型中，并将其应用于包含归纳偏置的数据上。

**结果:** 实验结果显示，应用了SCUD的模型在图像、文本和蛋白质数据上的表现超过了掩码扩散模型。

**结论:** 论文得出结论，通过将跳跃时间的已知分布融入到离散扩散模型中，可以构建出优于掩码扩散的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+Masking+Diffusion+Works%3A+Condition+on+the+Jump+Schedule+for+Improved+Discrete+Diffusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08316，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08316&send_immediately=true&force_search=false)

**原文摘要:** Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [21] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman, Mayte Suárez-Fariñas, Joseph T Colonel*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于注意力机制的可微k-NN回归层（NONA），显著提升了监督微调特征提取器后的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统预测算法（如k-NN）通常在SFT特征提取器后表现更好，但因其非可微性质未能直接整合到神经网络中，限制了性能进一步提升。

**方法:** 提出了Nearness of Neighbors Attention (NONA)回归层，利用神经网络注意力机制和新的学习注意力掩码方案，构建了k-NN回归算法的可微代理。

**结果:** 在多个非结构化数据集上验证了NONA方法的有效性，其预测性能优于现有的密集层和k-NN方法。

**结论:** 引入NONA回归层可以提升监督微调特征提取器后的预测性能，优于传统的密集层预测和k-NN方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nearness+of+Neighbors+Attention+for+Regression+in+Supervised+Finetuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08139&send_immediately=true&force_search=false)

**原文摘要:** It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [22] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi, Chenglin Fan*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一个简化的方法来分析扩散模型中的离散化误差，并表明高斯噪声可以用离散随机变量替代而不影响结果，这对于提高采样效率具有实际意义。


<details>
  <summary>更多</summary>
  
**动机:** 现有对扩散模型离散误差的分析往往依赖于复杂的概率工具，作者希望提供一种更简化的理论框架来分析这些模型。

**方法:** 论文利用Grönwall不等式分析了扩散模型中离散化的误差，并探讨了用离散随机变量代替高斯噪声的可能性。

**结果:** 论文得出在Lipschitz假设下，离散化的收敛速率为$\mathcal{O}(1/T^{1/2})$，并证明了高斯噪声可以被离散随机变量替代而不影响收敛保证。实验验证了错误按预测比例缩放、离散噪声能够达到与高斯噪声相当的样本质量，以及不正确的噪声缩放会降低性能。

**结论:** 该论文提出了一种简化理论框架，用于分析去噪扩散概率模型（DDPMs）中方差保持随机微分方程（VP-SDEs）的欧拉-丸山离散化。通过使用Grönwall不等式，在Lipschitz假设下推导出了收敛速率，并展示了高斯噪声可以用离散随机变量代替而不影响收敛保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Simple+Analysis+of+Discretization+Error+in+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08337&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [23] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li, Hanane Nour Moussa, Ziru Chen, Shijie Chen, Botao Yu, Mingyi Xue, Benjamin Burns, Tzu-Yao Chiu, Vishal Dey, Zitong Lu, Chen Wei, Qianheng Zhang, Tianyu Zhang, Song Gao, Xuhui Huang, Xia Ning, Nesreen K. Ahmed, Ali Payani, Huan Sun*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为AutoSDT的新方法，用于自动生成高质量的编码任务数据集，从而提高AI在科学发现中的表现，结果表明其性能接近GPT-4o。


<details>
  <summary>更多</summary>
  
**动机:** 由于缺乏高质量数据用于训练和评估，构建AI共同科学家面临挑战，因此提出了AutoSDT解决方案。

**方法:** 开发了AutoSDT自动管道，构建了AutoSDT-5K数据集，并训练了Qwen2.5-Coder-Instruct模型系列。

**结果:** 构造了包含5404个编码任务的数据集，专家反馈显示93%的任务具有生态有效性，且模型在基准测试中提升了17.4%的表现。

**结论:** AutoSDT-Coder在科学发现任务上表现出显著提升，接近GPT-4o的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoSDT%3A+Scaling+Data-Driven+Discovery+Tasks+Toward+Open+Co-Scientists，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08140，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08140&send_immediately=true&force_search=false)

**原文摘要:** Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [24] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar, Amandeep Singh, Rohitash Chandra*

**主要类别:** cs.LG

**AI概要:** 本文探讨了基于风速强度和空间坐标的深度学习、集成学习及数据增强框架在检测气旋快速增强中的应用，解决了类别不平衡问题并提升了检测效果。


<details>
  <summary>更多</summary>
  
**动机:** 气旋快速增强是一种罕见且极端的事件，其发生的相对稀有性导致了数据集中的类别不平衡问题，同时多种因素影响气旋发生快速增强的可能性，使传统机器学习模型难以处理。

**方法:** 本文使用深度学习和集成学习方法，并结合数据增强框架来解决气旋快速增强检测中的类别不平衡问题。利用深度学习模型生成模拟气旋的空间坐标和风速强度，并在数据增强框架中使用深度学习模型进行分类，以区分气旋中的快速和非快速增强事件。

**结果:** 研究表明，数据增强提高了气旋快速增强检测的效果，同时空间坐标在模型中起到了至关重要的作用。

**结论:** 数据增强改善了气旋快速增强检测的结果，空间坐标作为输入特征对模型起着关键作用。这为具有极端事件的时空数据的合成数据生成研究铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spatiotemporal+deep+learning+models+for+detection+of+rapid+intensification+in+cyclones，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08397，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08397&send_immediately=true&force_search=false)

**原文摘要:** Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [25] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin, Alex Lambert, Johan A. K. Suykens, Volkan Cevher*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种用于群体公平约束下谱聚类的新方法，在提高计算效率方面取得了显著进展。


<details>
  <summary>更多</summary>
  
**动机:** 随着决策算法公平性问题的重要性日益凸显，如何在保证性能的同时实现公平性成为研究重点。本文旨在解决谱聚类中的群体公平约束问题。

**方法:** 通过将公平谱聚类问题置于凸函数差（DC）框架中，并引入一种新颖的变量增广策略，采用适应于DC问题的交替方向乘子算法。

**结果:** 实验表明，与需要昂贵特征分解的先前方法相比，所提出的方法在计算效率上具有明显优势，尤其适用于大规模问题。

**结论:** 该论文提出了一种新的高效公平谱聚类方法，为现实应用中的公平聚类带来了显著的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+Spectral+Clustering+under+Fairness+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08143&send_immediately=true&force_search=false)

**原文摘要:** Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [26] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin, Jingfeng Wu, Peter L. Bartlett*

**主要类别:** cs.LG

**AI概要:** 本文研究了在数据受限的情况下，通过数据重用来改进缩放规律的可行性，并给出了理论证明和数值模拟验证。


<details>
  <summary>更多</summary>
  
**动机:** 当新数据不足时，现有的在线训练大语言模型的缩放规律可能不可持续。

**方法:** 推导了M维线性模型使用多遍随机梯度下降训练的测试误差界限，并假设数据协方差具有幂律谱。

**结果:** 多遍SGD实现了Θ(M^(1-b) + L^((1-b)/a))的测试误差，优于单遍SGD的Θ(M^(1-b) + N^((1-b)/a))。

**结论:** 通过数据重用，多遍SGD在数据受限的情况下可以实现改进的缩放规律。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+Scaling+Laws+in+Linear+Regression+via+Data+Reuse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08415，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08415&send_immediately=true&force_search=false)

**原文摘要:** Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [27] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç, Amirhossein Amiri-Hezaveh, Manuel K. Rausch, Grace N. Bechtel, Francisco Sahli Costabal, Adrian Buganza Tepole*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种利用神经微分方程和超网络技术识别非均质材料力学性质的新方法，无需依赖传统本构方程且具备高鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的逆向方法在处理非均质材料时受限于需要闭合形式的本构方程，而本文旨在开发一种更通用、无需明确本构关系的方法来提高识别精度和适用范围。

**方法:** 基于物理的数据驱动方法结合常微分神经方程（NODEs）和超网络技术，利用傅里叶特征训练神经网络以逼近应变场，并通过多目标损失函数优化超网络参数。

**结果:** 数值结果表明，该框架能够有效捕捉数据中的尖锐梯度，在存在噪声的情况下依然稳健，并成功应用于实验数据。

**结论:** 论文提出了一种无需闭合形式本构方程即可识别非均质材料力学性能的新框架，该方法在识别具有少量假设的非均质材料力学性能方面表现出鲁棒性和通用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fully+data-driven+inverse+hyperelasticity+with+hyper-network+neural+ODE+fields，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08146&send_immediately=true&force_search=false)

**原文摘要:** We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [28] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang, Ali Kavis, Aryan Mokhtari*

**主要类别:** cs.LG

**AI概要:** 论文提出了GALA方法，通过动态调整学习率提高深度学习优化器的性能，减少超参数调优需求。


<details>
  <summary>更多</summary>
  
**动机:** 优化器在大规模深度学习模型上的性能严重依赖于学习率的精细调整，通常需要广泛的网格搜索，因此提出了一种基于梯度对齐和局部曲率估计的动态学习率调整框架GALA。

**方法:** 通过跟踪连续梯度之间的对齐情况并使用局部曲率估计，将学习率选择问题建模为一维在线学习问题，并利用在线学习算法（如Follow-the-Regularized-Leader）生成自适应的学习率调度。

**结果:** 在平滑非凸环境下，理论分析表明结合GALA的归一化SGD具有数据自适应收敛率；实验显示，GALA增强了SGD和Adam等常见优化器的鲁棒性，使其在广泛初始学习率下均表现良好。

**结论:** GALA能够提升优化器在大规模深度学习模型中的性能，减少对学习率调整的依赖。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Learning-guided+Learning+Rate+Adaptation+via+Gradient+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08419，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08419&send_immediately=true&force_search=false)

**原文摘要:** The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [29] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh, Jinghan Jia, Zhiqi Bu, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Sijia Liu, Mingyi Hong*

**主要类别:** cs.LG

**AI概要:** 本文提出了BLUR算法，该算法基于双层优化的新公式化方法，解决了大型语言模型的知识遗忘问题，并在各种任务和模型中展示了优越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 使大型语言模型能够遗忘训练期间获得的知识和能力对于确保符合数据法规和促进生成式AI中的伦理实践至关重要。然而，如何最佳地制定遗忘问题仍不清楚。

**方法:** 论文提出了一种基于双层优化的新公式化方法，即Bi-Level UnleaRning（BLUR），其中下层目标专注于最小化遗忘损失，上层目标旨在保持模型效用。

**结果:** 广泛的实验表明，BLUR在各种遗忘任务、模型和指标上始终优于所有最先进的算法。

**结论:** 论文得出结论，通过使用提出的BLUR算法，可以有效地解决知识遗忘问题，并在各种任务和模型中表现出优于现有方法的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BLUR%3A+A+Bi-Level+Optimization+Approach+for+LLM+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08164，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08164&send_immediately=true&force_search=false)

**原文摘要:** Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [30] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu, Xinyi Zhong, Zhuoran Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种处理战略代理人的在线学习算法，通过结合多种技术实现了近最优性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决主代理人模型中由于代理人的非短视行为和私有类型带来的学习挑战。

**方法:** 结合延迟机制、奖励角度估计框架和悲观-乐观LinUCB算法的方法。

**结果:** 开发了首个可证明样本高效的算法，并建立了近最优的\tilde{O}(\sqrt{T})\后悔界。

**结论:** 论文得出了一种在线学习算法，能够在具有战略代理的广义委托-代理模型中实现近最优后悔界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Lead%3A+Incentivizing+Strategic+Agents+in+the+Dark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08438&send_immediately=true&force_search=false)

**原文摘要:** We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [31] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta, Nikita Jangid, Amit Sethi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的联邦学习框架UniVarFL，它通过直接在客户端级别模拟独立同分布训练动态，有效解决了非独立同分布数据带来的性能下降问题。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习在处理非独立同分布数据时常常遭遇严重的性能下降，传统的解决方法要么计算成本高，要么难以适应特征偏移。

**方法:** UniVarFL采用了两种互补的正则化策略进行本地训练：分类器方差正则化和超球面均匀性正则化。

**结果:** 大量实验表明，UniVarFL在多个基准数据集上的准确率优于现有方法。

**结论:** UniVarFL是一种高度可扩展且高效的解决方案，尤其适用于资源受限的现实世界FL部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UniVarFL%3A+Uniformity+and+Variance+Regularized+Federated+Learning+for+Heterogeneous+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08167&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [32] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian, Michael Kirchhof, Eugene Ndiaye, Louis Bethune, Michal Klein, Pierre Ablin, Marco Cuturi*

**主要类别:** cs.LG

**AI概要:** 论文表明从大型语言模型的激活中学习一个通用的“真相几何”来判断答案正确性是受限的，因为这种几何结构在不同任务间缺乏共享特征。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）表现出强大的泛化能力，但其实际应用受到可靠性的质疑，因此有研究试图通过检查推理时的激活来评估模型答案的正确性。本文旨在揭示基于这种“真相几何”的方法是否具有跨任务的可迁移性。

**方法:** 论文通过训练跨不同任务的线性分类器来分析大型语言模型（LLM）推理时产生的激活情况，并研究这些分类器的相似性和支持集的重叠程度。此外还尝试了更复杂的混合探针和任务的方法以期克服限制。

**结果:** 研究发现，针对不同任务训练的线性分类器几乎没有相似性，在使用稀疏正则化时，它们的支持集几乎不重叠。进一步尝试的复杂方法未能解决该问题，因为不同任务下的激活向量形成了明显分离的聚类。

**结论:** 论文得出结论，从不同任务中的激活向量中学习一个通用的“真相几何”是不可行的，因为这些几何结构在任务之间几乎没有相似性，并且使用混合探针和任务的更复杂方法也无法克服这一问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Geometries+of+Truth+Are+Orthogonal+Across+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08572，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08572&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [33] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang, Ryan Bausback, Feng Bao, Richard Archibald*

**主要类别:** cs.LG

**AI概要:** 该研究提出了一种基于随机神经网络的联邦学习新方法，用以应对客户端数据中的潜在噪声问题，并取得了良好的实验结果。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习由于客户端收集的数据可能存在潜在噪声（由测量能力有限或人为错误导致），因此需要一种能够有效应对这种噪声的方法。

**方法:** 通过在联邦学习框架中使用随机神经网络作为本地模型，以估计数据的真实底层状态并量化潜在噪声。

**结果:** 数值实验表明所提出的联邦随机神经网络方法在处理非独立同分布数据时具有出色的性能和有效性。

**结论:** 论文提出了一个结合随机神经网络的联邦学习方法，即联邦随机神经网络，用于处理本地数据集中的潜在噪声问题，并在处理非独立同分布数据方面表现出良好的性能和有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Learning+on+Stochastic+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08169&send_immediately=true&force_search=false)

**原文摘要:** Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [34] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen, Diego Klabjan*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在联邦学习框架下利用遗传算法构建个性化决策树的新方法，解决了现有方法在数据类型和性能上的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 目前联邦学习的研究主要集中于参数化的梯度模型，而非参数化模型如决策树的研究相对较少。现有的方法受限于差分隐私的要求，只能处理分类树和类别型数据。

**方法:** 该研究采用了遗传算法来促进个性化决策树的构建，从而克服现有方法在隐私保护方面的限制并提高性能。

**结果:** 实验表明，与仅使用本地数据训练的决策树和基准算法相比，所提出的方法具有更好的性能表现。

**结论:** 本文提出了一种基于遗传算法的联邦学习个性化决策树构建方法，适用于分类和回归任务，并且能够处理类别型和数值型数据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedGA-Tree%3A+Federated+Decision+Tree+using+Genetic+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08176，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08176&send_immediately=true&force_search=false)

**原文摘要:** In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [35] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang, Zachary T. Rewolinski, Abhineet Agarwal, Tiffany M. Tang, Bin Yu*

**主要类别:** cs.LG

**AI概要:** 本文提出了 LMDI+，一种改进的局部特征重要性方法，用于增强基于树的模型的局部可解释性和预测可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的局部特征重要性方法（如 LIME 和 TreeSHAP）依赖近似且忽略模型内部结构，导致不稳定；而全局方法 MDI+ 无法处理异质性个体特征。

**方法:** 提出 Local MDI+ (LMDI+) 方法，作为 MDI+ 框架在样本特定场景下的扩展。

**结果:** LMDI+ 在识别实例特定的信号特征上优于 LIME 和 TreeSHAP，平均提升了 10% 的下游任务性能，并展示了更高的稳定性及支持更多局部解释性应用场景。

**结论:** LMDI+ 提供了一种更稳定和准确的局部特征重要性方法，适用于基于树的模型，并支持局部可解释性的多种应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Local+MDI%2B%3A+Local+Feature+Importances+for+Tree-Based+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08928&send_immediately=true&force_search=false)

**原文摘要:** Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [36] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla, Jalaj Upadhyay, Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, Arun Ganesh, Monika Henzinger, Jonathan Katz, Ryan McKenna, H. Brendan McMahan, Keith Rush, Thomas Steinke, Abhradeep Thakurta*

**主要类别:** cs.LG

**AI概要:** 论文研究了差分隐私中相关噪声机制的设计与分析，以提高隐私保护效果并优化模型训练性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统差分隐私方法在每次梯度更新中加入独立噪声，但引入相关或反相关噪声可以在后续步骤中抵消之前添加的噪声，从而提升性能。

**方法:** 分析相关噪声机制，特别是矩阵机制、因式分解机制和DP-FTRL在机器学习中的应用。

**结果:** 研究表明，通过精心设计的相关噪声机制可以显著改进隐私保护与模型效用之间的平衡。

**结论:** 相关噪声机制在差分隐私中展现出重要的实际影响，并改善了隐私-效用权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Correlated+Noise+Mechanisms+for+Differentially+Private+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08201&send_immediately=true&force_search=false)

**原文摘要:** This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [37] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh, Kranthi Balusu, Ayoub Soulami*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于机器学习的残余应力生成器（RSG），可从有限测量数据中快速准确地推断全视场残余应力分布，显著减少了实验努力。


<details>
  <summary>更多</summary>
  
**动机:** 准确确定残余应力的全视场分布对于优化结构完整性和寿命至关重要，但现有的全视场表征实验工作量过大，难以实施。

**方法:** 基于U-Net架构的机器学习模型通过系统超参数调优进行训练，并利用大量工艺模拟数据集构建初始数据集以学习潜在结构。

**结果:** 该模型在预测模拟应力方面表现出优异的预测准确性，并展示了显著程度的泛化能力；最终测试验证了其在实际表征数据中的有效性。

**结论:** RSG方法能够从有限的测量数据中推断全视场残余应力分布，显著减少实验工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Machine+Learning+Approach+to+Generate+Residual+Stress+Distributions+using+Sparse+Characterization+Data+in+Friction-Stir+Processed+Parts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08205&send_immediately=true&force_search=false)

**原文摘要:** Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [38] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan, Guy Amir, Meirav Zehavi, Guy Katz*

**主要类别:** cs.LG

**AI概要:** 这篇论文通过计算复杂性理论研究集成模型的可解释性，发现基础模型的数量、大小和类型对解释难度有显著影响。结果表明，小型决策树的集成可以高效解释，而线性模型的集成仍难以处理。


<details>
  <summary>更多</summary>
  
**动机:** 尽管集成模型在机器学习领域被广泛认可，但它们的可解释性有限。论文旨在填补对什么因素使集成模型可解释或不可解释的数学理解空白，特别是基础模型的数量、大小和类型的影响。

**方法:** 该论文使用了计算复杂性理论的概念来分析各种集成配置的解释挑战。它利用标准复杂性假设（如P≠NP）来推导出关于不同因素如何影响解释难度的结论。

**结果:** 论文揭示了受不同因素影响的复杂的解释难度模式。例如，在标准复杂性假设下，即使基础模型的大小固定，集成模型的解释仍然是难以处理的。令人惊讶的是，当集成规模较小时，决策树的集成可以高效解释，而即使是少量的线性模型的集成也难以解释。

**结论:** 论文得出的结论是，通过计算复杂性理论的角度研究集成模型的可解释性能够提供更坚实的基础。论文发现基础模型的数量、大小和类型对可解释性有显著影响，并且小型决策树的集成可以高效解释，而线性模型的集成仍然难以处理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+makes+an+Ensemble+%28Un%29+Interpretable%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08216&send_immediately=true&force_search=false)

**原文摘要:** Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [39] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney, Kuei-Hsiang Huang, Aparna Chandramowlishwaran*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Mondrian的方法，通过将领域分解为子领域并在其上应用注意力机制，解决了基于Transformer的操作模型在高分辨率、多尺度领域的可扩展性问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于注意力机制的二次成本及其与离散化的耦合，扩展基于Transformer的操作模型到高分辨率、多尺度领域仍然存在挑战。

**方法:** Mondrian利用领域分解原理，在每个子领域内用表达力强的神经操作符替代标准层，并通过softmax-based inner products计算跨子领域的注意力。

**结果:** Mondrian在Allen-Cahn和Navier-Stokes PDEs上表现出了强大的性能，并且证明了无需再训练即可实现分辨率缩放。

**结论:** Mondrian通过分解领域和在子领域上应用注意力机制，成功解决了基于Transformer的操作模型在高分辨率、多尺度领域的可扩展性问题。这种方法具有可扩展性和通用性的神经操作符的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mondrian%3A+Transformer+Operators+via+Domain+Decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08226，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08226&send_immediately=true&force_search=false)

**原文摘要:** Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [40] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh, Kratarth Goel, Scott Ettinger, Carlos Fuertes, Ari Seff, Tim Shen, Cole Gulino, Chenjie Yang, Ghassen Jerfel, Dokook Choe, Rui Wang, Vinutha Kallem, Sergio Casas, Rami Al-Rfou, Benjamin Sapp, Dragomir Anguelov*

**主要类别:** cs.LG

**AI概要:** 该论文研究了Transformer模型在自动驾驶运动预测和规划任务中的扩展规律，揭示了模型性能随计算预算增加而提升的趋势及推理时的优化方法。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在理解模型性能随计算预算增加而提升的规律，并探索适合模型开发的度量标准以及解决机器人数据稀缺的问题。

**方法:** 研究使用了50万小时驾驶数据集，分析了编码器-解码器自回归Transformer模型在自动驾驶领域联合运动预测和规划任务中的表现。

**结果:** 研究发现模型性能以幂律函数形式随计算预算增长而提高，同时发现在推理时对较小模型采样和聚类输出可使其性能与较大模型相当。

**结论:** 论文得出结论，优化运动预测和规划模型的训练和推理扩展特性是提高其性能的关键手段。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Laws+of+Motion+Forecasting+and+Planning+--+A+Technical+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08228，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08228&send_immediately=true&force_search=false)

**原文摘要:** We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [41] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez, Nisha Singh, Lauren Dyson, Blythe Adamson, Qianyu Yuan, Megan W. Hildner, Erin Fidyk, Olive Mbah, Farhad Khan, Kathi Seidl-Rathkopf, Aaron B. Cohen*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个用于评估LLM提取临床数据质量的新框架，有助于提高AI在肿瘤学领域应用的可靠性和透明度。


<details>
  <summary>更多</summary>
  
**动机:** 由于LLMs在电子健康记录（EHRs）中提取临床数据的过程中面临可靠性、准确性和公平性的挑战，因此需要一个新的质量评估框架。

**方法:** 框架整合了变量级性能基准测试、自动化验证检查以及复制分析方法，以检测潜在错误并确认数据集的适用性。

**结果:** 该框架能够识别最需要改进的变量，系统地检测潜在错误，并通过分层指标支持偏差评估。

**结论:** 该论文提出了一种评估大型语言模型（LLMs）提取临床数据质量的综合框架，旨在提升肿瘤学研究和实践中AI生成证据的可信度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ensuring+Reliability+of+Curated+EHR-Derived+Data%3A+The+Validation+of+Accuracy+for+LLM%2FML-Extracted+Information+and+Data+%28VALID%29+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08231，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08231&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [42] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho, Rumi Chunara*

**主要类别:** cs.LG

**AI概要:** 该论文探讨了如何通过解决遗忘问题来提高随机数据增强的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 数据增强是增强分布外泛化的有希望的工具，其中关键是通过成本高昂的目标增强来产生多样、具有挑战性的源域变化以最大化其泛化效果。相反，随机增强成本低廉但因其有限的效果而被认为次优。

**方法:** 研究者重新审视了随机增强并探索了解决其缺点的方法，提出了一个改进随机增强的解决方案。

**结果:** 研究表明随机增强的随机本质可以产生一组碰撞增强，这会扭曲学习到的特征，类似于灾难性遗忘。

**结论:** 本文提出了一种通过解决遗忘问题来提升随机增强泛化效果的简单方法，并在多种单源域泛化基准测试中表现出强大的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dealing+with+the+Evil+Twins%3A+Improving+Random+Augmentation+by+Addressing+Catastrophic+Forgetting+of+Diverse+Augmentations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08240，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08240&send_immediately=true&force_search=false)

**原文摘要:** Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [43] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao, Artem Bisliouk, Rohith Reddy Nama, Ivan Ruchkin*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的结构化框架，利用信号时间逻辑（STL）改进大型语言模型在数学推理中的不确定估计，提高了可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型在数学推理任务中虽然表现良好，但由于会产生高度自信但错误的输出，在某些领域（如教育）中存在重大风险。因此需要一种更可靠的不确定性估计方法。

**方法:** 该论文的方法是将逐步置信度建模为时间信号，并使用信号时间逻辑（STL）评估其时间特性，同时引入了不确定性重塑策略以确保推理轨迹的平滑性、单调性和因果一致性。

**结果:** 实验表明，该方法在较准指标上显著优于传统置信度聚合和事后校准方法，提供了更可靠的不确定性估计。

**结论:** 该论文提出了一种基于信号时间逻辑（STL）的框架，能够通过建模逐步置信度来改善大型语言模型在数学推理任务中的不确定性估计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporalizing+Confidence%3A+Evaluation+of+Chain-of-Thought+Reasoning+with+Signal+Temporal+Logic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08243，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08243&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [44] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski, Łukasz Gorczyca, Piotr Helm, Kamil Książek, Przemysław Spurek*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为SHIELD的新方法，该方法结合了超网络和区间算术，以同时解决持续学习中的灾难性遗忘和对抗攻击问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统深度神经网络存在灾难性遗忘和对抗攻击脆弱性的问题，而目前尚无模型能同时解决这两个问题。

**方法:** SHIELD结合了基于超网络的持续学习方法和区间算术，通过超网络将可训练任务嵌入向量转化为针对特定数据的目标模型权重，并利用区间范围内的输入生成严格的安全保证。

**结果:** SHIELD能够为每个子任务动态生成独立网络，同时提供对区间范围内数据样本的严格安全保障。

**结论:** SHIELD方法在不牺牲网络适应性的情况下增强了安全性，解决了持续学习中的安全挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SHIELD%3A+Secure+Hypernetworks+for+Incremental+Expansion+Learning+Defense，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08255&send_immediately=true&force_search=false)

**原文摘要:** Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [45] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu, Blossom Metevier, Will Schwarzer, Austin Hoag, Scott Niekum, Philip S. Thomas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种高置信度的安全强化学习方法（HC-RLHF），在确保语言模型安全性的前提下提升其有用性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法通常将安全性与有用性对立，导致在敏感领域可能出现不可接受的响应。为解决此问题，需要一种能够提供高置信度安全保证的方法，同时最大化有用性。

**方法:** HC-RLHF 将人类偏好解耦为有用性和无害性，并分别训练奖励模型和成本模型。随后采用两步过程寻找安全解决方案：第一步是在悲观的成本约束下优化奖励函数；第二步是进行安全测试以验证模型是否符合实际成本约束的上置信界。

**结果:** 理论分析表明，HC-RLHF 不会返回不安全解决方案的概率高于用户指定阈值。实验结果显示，该方法在三种语言模型（Qwen2-1.5B, Qwen2.5-3B 和 LLaMa3.2-3B）上均能有效提升安全性和有用性。

**结论:** HC-RLHF 提供了高置信度的安全保障，并在保持或提升模型有用性方面优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+from+Human+Feedback+with+High-Confidence+Safety+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08266，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08266&send_immediately=true&force_search=false)

**原文摘要:** Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [46] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin, Majd Al Aawar, Antonio Ortega, Ajitesh Srivastava*

**主要类别:** cs.LG

**AI概要:** LIES是一种用于符号回归的新框架，它通过优化具有可解释激活函数的神经网络来发现数学表达式，表现出色并优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有SR方法在可扩展性和符号一致性方面存在困难，而LIES旨在解决这些问题。

**方法:** 通过训练具有特定过采样策略和定制损失函数的固定神经网络架构LIES，结合额外的剪枝策略来简化表达式。

**结果:** 实验表明，LIES框架成功地产生了紧凑的符号公式，并展示了每个设计组件的重要性。

**结论:** LIES框架能够有效地生成稀疏且准确的符号公式，并在SR基准测试中始终优于所有基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparse+Interpretable+Deep+Learning+with+LIES+Networks+for+Symbolic+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08267&send_immediately=true&force_search=false)

**原文摘要:** Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [47] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang, Mansooreh Montazerin, Ajitesh Srivastava*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种全新的神经网络设计方法，它能同时优化网络的结构和权重，并通过实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机源于传统的神经网络设计依赖于手动试错或先进行神经架构搜索再进行权重训练，前者费时费力，后者通常离散化架构搜索和权重优化。

**方法:** 论文的方法包括首先训练一个通用的多尺度自编码器，将架构和参数信息嵌入到连续潜在空间中，然后通过梯度下降法随机初始化并更新点以获得最优的神经网络，同时优化结构和权重。

**结果:** 实验结果表明，该方法能够在合成回归任务上有效发现稀疏且紧凑的神经网络，具有良好的性能表现。

**结论:** 论文的结论是提出了一种全新的神经网络设计方法，这种方法能够同时优化网络结构和权重，并通过实验验证了其在合成回归任务上发现稀疏且紧凑神经网络的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SWAT-NN%3A+Simultaneous+Weights+and+Architecture+Training+for+Neural+Networks+in+a+Latent+Space，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08270，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08270&send_immediately=true&force_search=false)

**原文摘要:** Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [48] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了基于通用微分方程（UDE）的科学机器学习方法，在智能电网系统中对节点特定的电池动力学进行高效建模，解决了太阳能输入随机性和负载配置差异带来的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 由于太阳能输入的随机性和家庭负载配置文件的可变性，对智能电网系统中的节点电池动力学进行建模仍然是一个挑战。传统方法通常难以泛化并捕捉未建模的残余动力学。

**方法:** 该研究通过将神经网络与物理微分方程相结合，提出了一种基于UDE的方法，用于学习节点特定的电池演化过程，并利用合成但现实的太阳能发电和负载需求数据模拟电池动态。

**结果:** 综合实验表明，训练后的UDE能够与真实的电池轨迹紧密对齐，表现出平滑的收敛行为，并在长期预测中保持稳定。

**结论:** 研究表明，基于UDE的SciML方法在分散能源网络中的电池建模中表现出色，为可再生能源集成智能电网的实时控制和优化提供了更广泛的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Differential+Equations+for+Scientific+Machine+Learning+of+Node-Wise+Battery+Dynamics+in+Smart+Grids，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08272&send_immediately=true&force_search=false)

**原文摘要:** Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [49] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, Bo Han*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的多语言模型协作框架ECON，通过不完全信息博弈理论中的纳什均衡来提高效率与性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于多智能体的LLM框架通常计算成本高且缺乏收敛保证，需要一种更高效的解决方案。

**方法:** 将多LLM协作重新定义为不完全信息博弈，并寻找贝叶斯纳什均衡；使用分层强化学习范式ECON进行分布式推理和集中输出。

**结果:** ECON在六个基准测试中平均优于现有方法11.2%，并具有更好的收敛性和扩展性。

**结论:** ECON是一个通过纳什均衡实现高效协调的新框架，能够在多LLM协作中取得更好的性能和可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Debate+to+Equilibrium%3A+Belief-Driven+Multi-Agent+LLM+Reasoning+via+Bayesian+Nash+Equilibrium，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08292，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08292&send_immediately=true&force_search=false)

**原文摘要:** Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [50] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou, Xiao Feng, Zhaocheng Zhu, Jiangchao Yao, Sanmi Koyejo, Bo Han*

**主要类别:** cs.LG

**AI概要:** 本文介绍AR-Bench，一种用于评估大型语言模型主动推理能力的新基准测试，发现当前模型在此方面存在显著不足，并指出未来需要改进的方向。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基准测试主要评估大型语言模型的被动推理能力，而对需要与外部系统交互获取缺失信息的主动推理能力研究较少，因此提出了AR-Bench来填补这一空白。

**方法:** 论文提出了一种新的基准测试AR-Bench，包括侦探案例、情境谜题和数字猜测三个任务家族，以评估LLM的主动推理能力。

**结果:** 实证评估表明，当代LLM在主动推理上表现不佳，难以获取或利用解决问题所需的信息，且即使采用高级策略也仅能取得适度提升。

**结论:** 论文得出结论，当前的LLM在主动推理方面存在显著困难，并强调了改进主动推理方法的必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Passive+to+Active+Reasoning%3A+Can+Large+Language+Models+Ask+the+Right+Questions+under+Incomplete+Information%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08295，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08295&send_immediately=true&force_search=false)

**原文摘要:** While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [51] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen, Heng Ping, Shixuan Li, Peiyu Zhang, Nikos Kanakaris, Nicholas Kotov, Paul Bogdan*

**主要类别:** cs.LG

**AI概要:** 本论文提出了H^2GFM，一种新的图基础模型框架，用于同时处理同质和异质文本属性图，通过统一的文本空间和上下文感知机制实现更强的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究主要集中在同质文本属性图（HoTAGs），而对包含多种节点/边类型的异质文本属性图（HeTAGs）探索不足，需要增强图基础模型的泛化能力。

**方法:** 提出了一个统一的框架H^2GFM，通过上下文感知的图变换器（CGT）和CGT专家混合机制来捕捉异质性和结构模式。

**结果:** 实验表明，H^2GFM在各种HoTAGs和HeTAGs以及学习场景中都具有有效性。

**结论:** H^2GFM在处理HoTAGs和HeTAGs方面表现出色，增强了图基础模型的能力和应用范围。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是H%24%5E2%24GFM%3A+Towards+unifying+Homogeneity+and+Heterogeneity+on+Text-Attributed+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08298，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08298&send_immediately=true&force_search=false)

**原文摘要:** The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [52] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu, Dongqi Fu, Zihao Li, Ross Maciejewski, Jingrui He*

**主要类别:** cs.LG

**AI概要:** 本文提出了L-STEP，一种通过可学习空间-时间位置编码进行时间链接预测的新方法，克服了现有位置编码的限制，并在多个实验中展示了卓越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的位置编码机制存在三个局限：预定义和固定函数难以适应复杂属性图；可学习的位置编码仅限于结构信息；大多数方法依赖于密集或关系注意力机制，在大规模数据上不可行。因此，需要一种更有效的解决方案。

**方法:** L-STEP提出了一种新的位置编码学习方案，该方案能够从空间-时间谱的角度保持图属性，并利用多层感知机（MLP）来充分利用编码的表达能力。

**结果:** L-STEP在13个经典数据集上使用三种不同的采样策略，在归纳和直推设置下与10种算法相比表现优异，并在最新的大规模TGB基准测试中获得领先性能。此外，其理论复杂度较低，实测运行时间少于现有最先进方法。

**结论:** L-STEP通过开发可学习的空间-时间位置编码，提供了一种有效且高效的时间链接预测模型，并在多个数据集和设置中展示了其优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learnable+Spatial-Temporal+Positional+Encoding+for+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08309&send_immediately=true&force_search=false)

**原文摘要:** Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [53] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González, Giulia Fanti, Aaditya Ramdas*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一个理论框架，用于解释Private Evolution (PE)在生成差分隐私合成数据时的实际行为，证明了其收敛性和1-Wasserstein距离的预期阶数，并通过仿真验证了理论结果的实用性。


<details>
  <summary>更多</summary>
  
**动机:** 由于PE在某些领域（如图像和文本）表现良好，但在其他领域（如表格数据）表现不稳定，且现有的理论分析依赖不现实的假设，因此需要一个新的理论框架来解释其实际行为并确定收敛条件。

**方法:** 开发了一个新的理论框架来解释PE的实际行为，证明了PE在敏感数据集上的1-Wasserstein距离的预期阶数，并将分析扩展到一般的Banach空间。

**结果:** 对于来自有界域的包含n个数据点的d维敏感数据集，PE生成的合成数据集具有$(\epsilon, \delta)$-DP性质，并且其1-Wasserstein距离的预期阶数为$\tilde{O}(d(n\epsilon)^{-1/d})$，从而建立了算法在$n \to \infty$时的最坏情况下的收敛性。

**结论:** 论文得出Private Evolution (PE)方法在生成差分隐私合成数据时具有理论收敛性，并连接了PE与Private Signed Measure Mechanism，展示了其实际相关性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Private+Evolution+Converges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08312&send_immediately=true&force_search=false)

**原文摘要:** Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [54] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu, Zehong Wang, Zihan Chen, Jiazheng Li, Yaochen Zhu, Zhenyu Lei, Cong Shen, Yanfang Ye, Chuxu Zhang, Jundong Li*

**主要类别:** cs.LG

**AI概要:** 本文系统性地综述了图提示学习的最新进展，包括预训练方法、提示技术设计、应用场景及未来挑战。


<details>
  <summary>更多</summary>
  
**动机:** 图学习模型通过大规模图数据学习表达能力强大的表示，在多种实际场景中表现出色。图提示方法作为一种有效的适应策略，能够在保持预训练模型不变的情况下，通过学习可训练提示来提升性能。

**方法:** 论文通过系统性综述的方法，介绍了图预训练方法、主流的图提示技术及其可学习提示的设计方式。

**结果:** 论文全面回顾了图提示学习的代表性预训练方法和技术，总结了其在现实世界中的应用，并提出了未来研究的潜在方向。

**结论:** 论文总结了图提示学习的最新进展，并讨论了其在不同领域的实际应用，以及未来可能的研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Prompting+for+Graph+Learning+Models%3A+Recent+Advances+and+Future+Directions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08326，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08326&send_immediately=true&force_search=false)

**原文摘要:** Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [55] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的优化框架，通过将控制权转移至策略，实现对策略参数的优化，而无需依赖传统的强化学习或动态规划技术。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习和近似动态规划方法过于复杂，需要更简单的优化策略参数的方法。

**方法:** 基于策略参数化和自主动力系统理论，提出一种新的优化算法。

**结果:** 开发了一种新的优化框架，能够计算与策略梯度、自然梯度等相同的量，并展示了其在多种任务中的适用性。

**结论:** 论文得出了一种新的优化框架，该框架简化了策略梯度方法，并且可以应用于生成式AI模型的调优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamical+System+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08340&send_immediately=true&force_search=false)

**原文摘要:** We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [56] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang, Haoteng Ying, Eli Chien, Rongzhe Wei, Pan Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种面向关系数据的差分隐私学习方法，解决了传统方法在多关系参与和复杂采样过程中的隐私保障问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于在涉及敏感领域的关系和网络结构数据中保护个体实体隐私的重要性日益增加，而现有差分隐私机制（如DP-SGD）难以直接应用于关系学习。

**方法:** 提供了一个严格的敏感性分析，并引入了一种自适应梯度裁剪方案，同时扩展了隐私放大结果以适用于一种可处理的耦合采样子类。

**结果:** 提出的框架能够有效控制实体级别的差分隐私风险，并在文本属性网络结构数据上展示了良好的隐私-效用平衡。

**结论:** 论文提出了一种针对关系数据的差分隐私学习框架，具有可证明的隐私保障，并通过实验验证了其在隐私与效用之间的良好权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Differentially+Private+Relational+Learning+with+Entity-level+Privacy+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08347&send_immediately=true&force_search=false)

**原文摘要:** Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [57] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为AdaAct的新优化算法，该算法通过调整学习率增强神经元输出的稳定性，从而达到更好的泛化能力，同时保持竞争力的执行时间。


<details>
  <summary>更多</summary>
  
**动机:** 为了弥补Adam的收敛速度和SGD的强大泛化能力之间的差距，并提高执行效率。

**方法:** AdaAct方法在训练过程中引入了神经元级别的自适应性，以调整学习率。

**结果:** 实验结果证明了AdaAct在标准图像分类基准测试中的竞争力性能，有效的结合了Adam的收敛速度和SGD的强泛化能力。

**结论:** AdaAct是一种新的优化算法，通过根据激活方差调整学习率来增强神经元输出的稳定性，从而实现更好的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Adaptive+Method+Stabilizing+Activations+for+Enhanced+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08353&send_immediately=true&force_search=false)

**原文摘要:** We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [58] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为NysAct的新优化方法，通过特征值位移的Nystrom方法近似激活协方差矩阵，以实现高效计算和良好的泛化性能之间的平衡。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自适应梯度方法虽然计算效率高且收敛快，但泛化能力差；而二阶方法虽能提升收敛和泛化性能，但计算和内存成本高。因此需要一种折中方案。

**方法:** NysAct利用特征值位移的Nystrom方法来近似激活协方差矩阵，作为预处理矩阵。

**结果:** 实验表明，与一阶和二阶方法相比，NysAct不仅提高了测试准确率，而且计算资源需求远低于现有二阶方法。

**结论:** NysAct是一种可扩展的一阶梯度预处理方法，可在最先进的优化方法之间取得平衡，并且在测试精度影响最小的情况下显著降低时间和内存复杂度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NysAct%3A+A+Scalable+Preconditioned+Gradient+Descent+using+Nystrom+Approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08360&send_immediately=true&force_search=false)

**原文摘要:** Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [59] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Siyuan Li, Yufei Huang, Stan Z. Li*

**主要类别:** cs.LG

**AI概要:** 本研究指出AlphaFold数据库结构存在系统性几何偏差，并提出一种新的去偏结构自编码器（DeSAE）框架，以提升基于结构的蛋白质设计和逆折叠任务的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管AlphaFold Protein Structure Database (AFDB) 提供了接近实验精度的广泛结构覆盖，但其在深度模型训练中的直接应用暴露了一个关键限制：它对精细原子几何敏感的任务（如逆折叠）表现出系统性几何偏差。这促使作者提出新的方法来解决这一问题。

**方法:** 研究人员使用了比较结构特征分布分析的方法来揭示AFDB与PDB之间的差异，并提出了一种去偏结构自编码器（DeSAE），该模型通过对故意破坏的骨架几何结构进行重建，学习恢复天然样式的构象。

**结果:** 研究发现，AFDB结构虽然更干净和理想化，但缺乏实验确定结构所具有的真实物理变异性和泛化能力。通过应用提出的DeSAE模型，研究人员成功地从AFDB结构中生成了去偏结构，在多个基准测试中显著提高了逆折叠性能。

**结论:** 论文得出结论，AlphaFold Protein Structure Database (AFDB) 结构存在系统性几何偏差，不能完全反映蛋白质的真实构象多样性。通过引入DeSAE模型，可以有效减轻这种偏差，从而显著提高基于结构的学习任务（如逆折叠）的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AlphaFold+Database+Debiasing+for+Robust+Inverse+Folding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08365&send_immediately=true&force_search=false)

**原文摘要:** The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [60] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan, Tengyang Xie*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为DPSDP的强化学习算法，用于训练大型语言模型以动态探索解决方案并整合反馈，从而提升推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的验证和改进方法常常受限于有限的反馈空间以及缺乏不同部分的协调训练，导致性能不佳。

**方法:** 将多轮改进过程建模为马尔可夫决策过程，并引入了一种强化学习算法DPSDP，该算法通过直接偏好学习来训练一个演员-评论家LLM系统以迭代优化答案。

**结果:** 在MATH 500基准测试中，基于Ministral的模型经过五次精化步骤后的多数投票使首次准确率从58.2%提高到63.2%。

**结论:** DPSDP可以匹配训练分布中任何策略的性能，并且通过实证研究展示了其在不同基准上的改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforce+LLM+Reasoning+through+Multi-Agent+Reflection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08379，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08379&send_immediately=true&force_search=false)

**原文摘要:** Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [61] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen, Rongbin Ye*

**主要类别:** cs.LG

**AI概要:** 该研究分析了机器学习技术在物联网恶意流量检测中的应用，发现适当不平衡处理技术和集成方法（如gcForest）的结合能够提高检测性能，为物联网环境的安全提供支持。


<details>
  <summary>更多</summary>
  
**动机:** 随着物联网网络的迅速扩展，实时检测恶意流量已成为关键的网络安全挑战。为了开发更智能高效的自动化威胁检测系统，以保护关键基础设施免受复杂网络攻击，同时优化计算资源的使用。

**方法:** 通过三种重采样策略解决数据集中的显著类别不平衡问题，并实现和比较了一些机器学习技术。

**结果:** 研究结果表明，将适当的不平衡处理技术与集成方法结合使用可以实现更好的检测性能。

**结论:** 论文得出结论，将适当的不平衡处理技术与集成方法（特别是gcForest）结合使用，在IoT网络中的恶意流量检测方面优于传统方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Network+Threat+Detection%3A+Addressing+Class+Imbalanced+Data+with+Deep+Forest，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08383&send_immediately=true&force_search=false)

**原文摘要:** With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [62] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin, Tianyu Zhao, Yujin Tang*

**主要类别:** cs.LG

**AI概要:** 本研究介绍了一种新的基于强化学习的教师模型（RLTs）框架，用于解决推理语言模型在强化学习训练中的探索难题，并提升下游蒸馏的效果。


<details>
  <summary>更多</summary>
  
**动机:** 传统上使用强化学习( RL )训练推理语言模型( LMs )依赖于LM在初始状态下探索和解决问题的能力，同时RLMs的主要用途是作为教师模型来指导新学生模型的蒸馏和冷启动后续的RL迭代。

**方法:** 提出了一种新的Reinforcement-Learned Teachers (RLTs)模型类别，通过密集奖励机制进行训练，该机制将每个解释输入学生模型并测试其对问题解决方案的理解。

**结果:** 实验表明，一个7B RLT的原始输出在竞赛和研究生级别的任务上比现有的蒸馏和冷启动流程提供更高的最终性能。

**结论:** RLTs在训练较大规模的学生模型和应用于分布外任务时保持有效性，为RL推理框架解锁了新的效率和可重用性水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+Teachers+of+Test+Time+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08388，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08388&send_immediately=true&force_search=false)

**原文摘要:** Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [63] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu, Song Jiang, Zijie Huang, Xiao Luo, Shichang Zhang, Adrian Chen, Yizhou Sun*

**主要类别:** cs.LG

**AI概要:** 本文提出了FUSE，一种新的集合表示学习方法，通过使用模糊集的体积近似来实现信息保留和高效学习，并在分类学扩展任务中实现了高达23%的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有工作通常将集合建模为向量或几何对象，但它们在集合操作下不闭合，而模糊集能够结合不确定性和语义概念内的信息度量，更适合于概念建模。

**方法:** 提出了一种新的嵌入框架Fuzzy Set Embedding (FUSE)，其基于体积近似作为模糊集来进行集合表示学习。

**结果:** FUSE在分类学扩展任务中表现出了高达23%的显著改进。

**结论:** FUSE是一种基于模糊集的集合表示学习的新方法，它在信息保留方面表现出色，并且可以高效地进行学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FUSE%3A+Measure-Theoretic+Compact+Fuzzy+Set+Representation+for+Taxonomy+Expansion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08409，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08409&send_immediately=true&force_search=false)

**原文摘要:** Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [64] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali, Aleksandr Khizhik, Stepan Svirin, Artem Ryzhikov, Denis Derkach*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于无监督学习的数据增强方法（SGDA），用于三相发动机的智能故障诊断，通过结合机器学习和物理模型显著提升了诊断效果。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法依赖于签名分析，在诊断性能和准确性方面存在提升空间，而集成先进的机器学习技术可以弥补这一不足。

**方法:** 提出了一种名为Signature-Guided Data Augmentation (SGDA) 的无监督框架，该框架在电流信号频域中综合生成物理上合理的故障，并利用Motor Current Signature Analysis指导数据增强过程。

**结果:** 研究结果表明，这种结合了有监督机器学习和无监督签名分析的混合方法实现了更优的诊断准确性和可靠性，并具有广泛的工业应用潜力。

**结论:** 结合机器学习算法和基于物理模型的无监督异常生成方法可以显著提高三相发动机智能诊断的准确性和可靠性，为工业应用提供了稳健高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Hear+Broken+Motors%3A+Signature-Guided+Data+Augmentation+for+Induction-Motor+Diagnostics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08412&send_immediately=true&force_search=false)

**原文摘要:** The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [65] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao, Zhichao Lei, Tianyuan Chen, Ziyue Yuan, Xuefan Chen, Jianxiang Liu, Faguo Wu, Xiao Zhang*

**主要类别:** cs.LG

**AI概要:** 本论文提出了SQOG方法，通过SBO解决离线RL中Q函数在分布外区域的过度保守问题，实现了更精确的Q值估计并在实际任务中取得良好性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的离线强化学习方法在处理分布外动作时往往过于保守，导致Q函数泛化受限，影响策略改进。

**方法:** 提出了一种称为Smooth Bellman Operator (SBO) 的新方法，用于平滑分布外动作的Q值，从而增强Q函数在分布外区域的泛化能力。

**结果:** SQOG在理论上展示了对CHN内的样本内和分布外动作都能进行准确的Q值估计，并且在D4RL基准测试中表现优异。

**结论:** SQOG通过引入SBO改进了Q值估计，解决了离线RL中的分布外泛化问题，并在D4RL基准测试中优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Offline+RL+with+Smooth+OOD+Generalization+in+Convex+Hull+and+its+Neighborhood，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08417，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08417&send_immediately=true&force_search=false)

**原文摘要:** Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [66] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin, Zhe Chen, Xianhao Chen, Wei Ni, Yue Gao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的异构感知Split Federated Learning框架HASFL，用于解决边缘设备上因资源异构导致的训练效率低下问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Split Federated Learning方法由于边缘设备能力异构而受到显著的拖尾效应影响，需要解决资源异构带来的根本性挑战。

**方法:** 通过推导SFL的收敛界来量化不同批量大小和模型分割对学习性能的影响，并基于此设计HASFL算法。

**结果:** 大量使用各种数据集的实验验证了HASFL的有效性，并证明其优于最先进的基准方法。

**结论:** 论文提出了一种异构感知的Split Federated Learning框架HASFL，能够自适应控制批量大小和模型分割以平衡通信计算延迟和训练收敛性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HASFL%3A+Heterogeneity-aware+Split+Federated+Learning+over+Edge+Computing+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08426&send_immediately=true&force_search=false)

**原文摘要:** Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [67] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan, Fuyi Wang, Cen Chen, Jianying Zhou*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了联邦学习中的隐私问题，开发了一种名为FedLeak的新技术，可以有效地重构客户端数据，揭示了联邦学习系统的重大漏洞。


<details>
  <summary>更多</summary>
  
**动机:** 最近关于联邦学习中的隐私风险是否实际存在的争论激发了我们的研究，我们旨在证明即使在现实环境中，客户的数据仍可能被有效重建。

**方法:** 开发了FedLeak，引入了部分梯度匹配和梯度正则化两种新技术，并制定了基于广泛联邦学习文献和行业实践的实用评估协议。

**结果:** FedLeak在实用评估协议下仍能实现高保真数据重建。

**结论:** 联邦学习系统中存在显著的漏洞，需要更有效的防御方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Boosting+Gradient+Leakage+Attacks%3A+Data+Reconstruction+in+Realistic+FL+Settings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08435，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08435&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [68] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu, Sanghyun Son, Ming Lin*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的基于模型的学习方法——时间感知世界模型（TAWM），它通过考虑时间步长的变化来提升控制任务中的性能与数据利用效率。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决常规模型在处理具有多样时间动态特性的控制问题时表现不佳的问题，研究者试图开发一种能够同时捕捉高、低频率动态特性的方法。

**方法:** 通过基于信息理论的方法引入时间感知世界模型（TAWM），明确地结合时间动态特性。该方法通过对不同的时间步长Δt进行训练，而不是固定的时间步长采样来学习高低频任务动态。

**结果:** 实验结果显示，在使用相同数量的训练样本和迭代次数的情况下，TAWM在不同观测率下的一系列控制任务中始终优于传统模型。

**结论:** TAWM在各种控制任务中都表现出优于传统模型的性能，证明了其时间感知公式在处理不同采样率问题上的有效性和数据效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time-Aware+World+Model+for+Adaptive+Prediction+and+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08441&send_immediately=true&force_search=false)

**原文摘要:** In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [69] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo, Yu Yang, Pan Xu, Anqi Liu*

**主要类别:** cs.LG

**AI概要:** MOBODY是一种基于模型的off-dynamics离线强化学习算法，通过生成目标域的新合成转移并结合Q加权行为克隆损失，实现了比现有方法更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的off-dynamics离线RL方法受限于目标域中可用转移的有限性，无法超越离线数据集进行探索，因此需要一种能够增强目标域探索能力的新方法。

**方法:** MOBODY通过表示学习发现跨领域的状态和转移共享潜在表征，结合Q加权行为克隆损失来稳定训练，并利用目标域中的离线数据、增强源数据和模型rollout数据进行策略学习。

**结果:** MOBODY在MuJoCo基准测试中显著优于最先进的基线方法，特别是在挑战性场景中效果更明显。

**结论:** MOBODY算法在离线强化学习中表现出优于现有方法的性能，尤其是在挑战性场景中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MOBODY%3A+Model+Based+Off-Dynamics+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08460，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08460&send_immediately=true&force_search=false)

**原文摘要:** We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [70] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu, Yu Yang, Ruhan Wang, Pan Xu, Dongruo Zhou*

**主要类别:** cs.LG

**AI概要:** 本文提出了Reinforced RCSL，一种改进的监督学习方法，用于顺序决策问题，解决了传统RCSL方法的局限性，并在多个测试中表现优越。


<details>
  <summary>更多</summary>
  
**动机:** RCSL虽然简单稳定，但因缺乏拼接属性而受限于生成离线数据集策略的质量，因此需要一种更优的方法来提升性能。

**方法:** 提出了一种名为Reinforced RCSL的框架，其核心创新是利用当前状态识别数据集内最佳可实现未来回报的机制，避免了复杂回报增强技术的需求。

**结果:** 理论分析表明Reinforced RCSL能够持续超越标准RCSL方法，实验结果进一步验证了这一结论，在多个基准测试中展示了显著的性能提升。

**结论:** Reinforced RCSL通过引入分布内最优回报机制，解决了RCSL缺乏拼接属性的问题，从而在理论上和实证上都显著优于标准RCSL方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+to+Provably+Improve+Return+Conditioned+Supervised+Learning%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08463，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08463&send_immediately=true&force_search=false)

**原文摘要:** In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [71] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为MAC的新优化方法，该方法通过有效逼近KFAC中的FIM组件来提高计算效率，并首次将Kronecker分解应用于transformer中的注意力层的FIM。


<details>
  <summary>更多</summary>
  
**动机:** 二阶优化方法如KFAC由于计算负担高而面临挑战，因此需要一种计算效率高的优化方法。

**方法:** 基于对KFAC中使用的FIM的两个组成部分的经验观察，提出了一种有效的近似方法。

**结果:** 实验评估表明，在各种网络架构和数据集上，所提出的方法在准确性、端到端训练时间和内存使用方面均优于KFAC和其他最先进方法。

**结论:** MAC是一种新的优化方法，优于KFAC和其他最先进的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAC%3A+An+Efficient+Gradient+Preconditioning+using+Mean+Activation+Approximated+Curvature，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08464，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08464&send_immediately=true&force_search=false)

**原文摘要:** Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [72] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang, Qihui Zhang, Yuyang Liu, Yue Huang, Xiaojun Jia, Kunpeng Ning, Jiayu Yao, Jigang Wang, Hailiang Dai, Yibing Song, Li Yuan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 AsFT 的新方法，通过利用对齐方向来提升大型语言模型在微调过程中的安全性，并在多个实验中证明了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在微调过程中容易受到恶意或无害数据的影响，导致安全性下降，因此需要一种能够维持模型安全性的微调方法。

**方法:** 基于对齐方向的概念，提出了一种新的正则化项，以抑制在有害方向上的更新，从而将微调限制在狭窄的安全区域内。

**结果:** AsFT 在多个数据集上进行了广泛的实验，结果表明其在减少有害行为方面提高了7.60%，模型性能提高了3.44%，并且在各种实验设置中保持了稳健的性能。

**结论:** AsFT 是一种用于提高模型安全性的微调方法，在多个实验中表现出优于 Safe LoRA 的性能，同时减少了有害行为并提升了模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AsFT%3A+Anchoring+Safety+During+LLM+Fine-Tuning+Within+Narrow+Safety+Basin，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08473，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08473&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [73] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan, Yizhak Yisrael Elboher, Tobias Ladner, Matthias Althoff, Guy Katz*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的抽象-细化技术，用于高效地生成具有形式保证的神经网络预测解释，并展示了其在不同抽象层级上的细粒度解释能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管后验解释技术取得了显著进展，但许多现有方法依赖于启发式算法，不能提供正式可证明的解释保证。最近的工作表明，使用神经网络验证技术可以找到具有形式保证的解释，但其计算面临显著的可扩展性挑战。

**方法:** 通过构建一个大幅减少的网络来抽象原始大型神经网络，在这个缩减网络中得到的充分解释同样适用于原始网络，从而显著加快验证过程。如果在简化网络上的解释不足，则通过逐步增加网络规模来进行迭代优化，直到收敛。

**结果:** 实验表明，所提出的方法提高了获取可证明充分解释的效率，并且在不同抽象层次上提供了对网络预测的细粒度解释。

**结论:** 作者提出了一种新的抽象-细化技术，用于高效计算神经网络预测的可证明充分解释。这种方法在不同抽象层次上提供了对网络预测的细粒度解释，并提高了获取可证明充分解释的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explaining%2C+Fast+and+Slow%3A+Abstraction+and+Refinement+of+Provable+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08505&send_immediately=true&force_search=false)

**原文摘要:** Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [74] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He, Yeonjong Shin, Anthony Gruber, Sohyeon Jung, Kookjin Lee, Youngsoo Choi*

**主要类别:** cs.LG

**AI概要:** 提出了一个高效的热力学感知的潜在空间动力学识别框架（tLaSDI），用于参数非线性动力系统的降阶建模，结合了自动编码器和新开发的参数GENERIC形式主义神经网络（pGFINNs），并采用物理信息主动学习策略提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高参数非线性动力系统建模的效率与准确性，同时保持关键的热力学原理。

**方法:** 结合自动编码器进行维度约简和参数GENERIC形式主义神经网络进行潜在动力学学习，并采用基于残差误差指标的贪心主动学习策略优化训练数据采样。

**结果:** 在Burgers方程和1D/1V Vlasov-Poisson方程上的实验表明，该方法实现了高达3,528倍的加速，相对误差为1-3%，并在训练和推理成本上有显著降低。

**结论:** tLaSDI框架能够高效准确地建模参数非线性动力系统，并揭示系统的潜在热力学行为，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thermodynamically+Consistent+Latent+Dynamics+Identification+for+Parametric+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08475&send_immediately=true&force_search=false)

**原文摘要:** We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [75] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde, Alexandra Gianzina, Hanno Gottschalk, Andreas Ebert*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于进化算法的多目标网络架构搜索方法（EMNAS），用于自主驾驶的强化学习模型优化，具有更高的性能和更少的参数。


<details>
  <summary>更多</summary>
  
**动机:** 为了在不牺牲性能的前提下提升自主驾驶强化学习模型的奖励表现并减小模型规模，需要一种自动化的网络架构优化方法。

**方法:** EMNAS采用遗传算法自动化设计网络结构，并结合并行化技术和师生教学方法，以提升大规模强化学习的效率和稳定性。

**结果:** 实验结果表明，定制化的EMNAS优于人工设计的模型，在参数更少的情况下获得了更高的奖励。

**结论:** EMNAS为自主驾驶中的强化学习提供了一个有效的网络架构搜索方法，能够实现高性能、轻量化的模型优化，有助于推动该领域向更适用于现实场景的方向发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Evolutionary+Multi-Objective+Network+Architecture+Search+for+Reinforcement+Learning+%28EMNAS-RL%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08533&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [76] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland, Chris Sweet, Adam Czakja*

**主要类别:** cs.LG

**AI概要:** 本文提出了SHAM和DiffGradCAM，分别用于测试和提升类激活映射方法在对抗环境中的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 传统CAM及其梯度变体通常关注单个logits，而忽略了softmax函数对logits差异的依赖，这导致其容易受到被动欺骗攻击。

**方法:** 引入Salience-Hoax Activation Maps (SHAMs)作为对抗条件下评估CAM鲁棒性的基准，同时提出DiffGradCAM，一种新型、轻量级且具有对比性的类激活映射方法。

**结果:** 提出的SHAM能够作为一种对抗条件下的基准工具，而DiffGradCAM则能够在非对抗情况下匹配标准CAM方法的输出，同时具备抵抗被动欺骗的能力。

**结论:** SHAM和DiffGradCAM共同建立了一个用于探究和改进基于显著性解释的鲁棒性的新框架，并且在多类任务中验证了它们的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiffGradCAM%3A+A+Universal+Class+Activation+Map+Resistant+to+Adversarial+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08514，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08514&send_immediately=true&force_search=false)

**原文摘要:** Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [77] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson, Francesca Cairoli, Luca Bortolussi, Davide Russo, Francesca Zanello*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种利用生成式人工智能和扩散模型增强污水系统情境预测的新方法，通过处理多变量时间序列数据并结合保形推理技术，实现了在极端天气条件下的高精度和可靠预测。


<details>
  <summary>更多</summary>
  
**动机:** 本研究的动机是提高污水系统中情境预测的准确性，特别是在极端天气事件期间。传统方法可能无法有效捕捉多样化的环境信号之间的复杂相关性，因此需要一种更强大的深度学习方法。

**方法:** 该论文提出了一种基于扩散模型的新方法，专门处理多变量时间序列数据，并引入了针对概率时间序列数据定制的保形推理技术来校准预测结果。

**结果:** 实验结果显示，所提出的模型在真实污水系统数据上表现出色，能够可靠地进行情境预测，并且即使在严重天气条件下也能维持准确性。此外，它能够以期望的置信水平覆盖真实目标值。

**结论:** 论文得出结论，他们开发的基于扩散模型的方法结合了生成人工智能的力量，并通过保形推理技术校准预测，以确保统计可靠性。这种方法在极端天气条件下也能保持准确性，为污水系统中的情境预测提供了一个强有力的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion-based+Time+Series+Forecasting+for+Sewerage+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08577，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08577&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [78] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi, David Danan, Milad Leyli-Abadi, Ahmed Mazari, Jean-Patrick Brunet, Abbas Kabalan, Fabien Casenave, Yuxin Ma, Giovanni Catalani, Jean Fesquet, Jacob Helwig, Xuan Zhang, Haiyang Yu, Xavier Bertrand, Frederic Tost, Michael Baurheim, Joseph Morlier, Shuiwang Ji*

**主要类别:** cs.LG

**AI概要:** ML4CFD竞赛成功验证了机器学习模型在计算流体动力学模拟中的优越性能，并为未来研究提供方向。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服机器学习模型在实际科学领域部署中的准确性和泛化能力等挑战。

**方法:** 通过组织ML4CFD竞赛，采用多标准评估框架对参赛方法进行系统性基准测试。

**结果:** 超过240支队伍参与，一些方法在综合评估得分上超过了基线，排名第一的方法在聚合指标上表现优于原始OpenFOAM求解器。

**结论:** 本次比赛展示了ML模型在计算流体动力学中替代传统求解器的潜力，并为未来的科学机器学习挑战提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeurIPS+2024+ML4CFD+Competition%3A+Results+and+Retrospective+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08516，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08516&send_immediately=true&force_search=false)

**原文摘要:** The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [79] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan, Qiang Liu, Alberto Guardone, Nils Thuerey*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的生成模型Physics-Based Flow Matching(PBFM)，其能够将物理约束显式嵌入到训练过程中，从而在代理建模、不确定性量化和加速仿真等物理和工程任务中取得更高的准确性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的生成机器学习方法通常从数据中隐式学习基础物理特性，缺乏对显式物理约束的考虑。

**方法:** 提出了一种新的生成框架Physics-Based Flow Matching(PBFM)，将物理约束嵌入到flow matching目标中，并在训练时引入时间展开以提高最终预测的准确性。

**结果:** 通过三个具有代表性的PDE问题的广泛基准测试表明，与现有方法相比，该方法在分布准确性和物理残差方面表现更优，物理残差最多减少8倍。

**结论:** PBFM提供了一个有原则且高效的框架，用于物理和工程应用中的代理建模、不确定性量化和加速仿真。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flow+Matching+Meets+PDEs%3A+A+Unified+Framework+for+Physics-Constrained+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08604，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08604&send_immediately=true&force_search=false)

**原文摘要:** Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [80] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González, Miguel C. Soriano, Lucas Lacasa*

**主要类别:** cs.LG

**AI概要:** 论文指出，在训练人工神经网络时，适当引入混沌动力学区域可显著提升学习效率，而最佳学习率位于从利用型向探索-利用平衡过渡的临界点。


<details>
  <summary>更多</summary>
  
**动机:** 传统的神经网络优化方法通常基于纯利用型动力学（如梯度下降），但这种方法可能效率较低，因此作者试图探索大学习率下的动态特性以寻找提升学习效率的可能性。

**方法:** 研究使用梯度下降（GD）算法在非传统大学习率条件下训练人工神经网络的动力学轨迹，并分析其敏感性依赖初始条件的表现和最大Lyapunov指数等指标。

**结果:** 研究发现，在特定学习率区域内，GD优化从纯利用型转向探索-利用平衡状态，同时网络的最大Lyapunov指数为正值且测试集上的训练时间达到最小值，表明此时学习效率最高。

**结论:** 论文得出结论，在训练人工神经网络时，通过定位到混沌状态的临界点，可以加速网络的学习过程。这表明在GD优化中引入探索-利用平衡机制能够提高学习效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+chaos+in+the+training+of+artificial+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08523，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08523&send_immediately=true&force_search=false)

**原文摘要:** Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [81] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan, Hakan Akgün, Kenji Kawaguchi, N. Duane Loh, Ching Hua Lee*

**主要类别:** cs.LG

**AI概要:** 论文提出了HSG-12M，这是一个大规模的空间多重图数据集，用于解决现有图基准无法保留几何轨迹的问题，并开发了相应的转换工具Poly2Graph。


<details>
  <summary>更多</summary>
  
**动机:** 现有图基准假设非空间、简单边，将物理上不同的路径折叠成单一链接；需要能够保留多几何轨迹的数据集。

**方法:** 引入了HSG-12M，一个大规模的空间多重图数据集，并开发了Poly2Graph工具将任意1-D晶体哈密顿量映射为谱图。

**结果:** HSG-12M包含1160万静态和510万动态哈密顿谱图，跨越1401个特征多项式类，揭示了从多项式、向量和矩阵到图的新的代数-图链接。

**结论:** HSG-12M为几何感知的图学习奠定了基础，并为凝聚态物理学及其他领域的数据驱动科学发现提供了新机会。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HSG-12M%3A+A+Large-Scale+Spatial+Multigraph+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08618，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08618&send_immediately=true&force_search=false)

**原文摘要:** Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [82] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann, Quentin Bouniot, Vasilii Feofanov, Ievgen Redko, Zeynep Akata*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为TiViT的框架，通过将时间序列转换为图像以利用大规模预训练视觉转换器的能力，从而在时间序列分类任务中取得优异表现。


<details>
  <summary>更多</summary>
  
**动机:** 由于公开可用的时间序列数据集稀缺，时间序列基础模型（TSFMs）的发展受到限制，因此提出了新的解决方案TiViT。

**方法:** 首先，通过理论分析2D patching方法来解释为什么可以将时间序列转换为图像；其次，利用OpenCLIP模型的隐藏表示进行实验验证；最后，评估TiViT与TSFM表示空间的对齐性并结合特征提升性能。

**结果:** TiViT在标准时间序列分类基准测试中实现了最先进的性能，同时发现中间层具有高内在维度的表示最有效，并且结合TSFM特征可进一步提升性能。

**结论:** 研究发现，通过将时间序列转化为图像并利用预训练的视觉转换器，提出的方法TiViT在时间序列分类任务上表现卓越，并揭示了在非视觉领域重复使用视觉表示的另一方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+Series+Representations+for+Classification+Lie+Hidden+in+Pretrained+Vision+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08641&send_immediately=true&force_search=false)

**原文摘要:** Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [83] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, Fangxin Wang*

**主要类别:** cs.LG

**AI概要:** 本文介绍了DeepForm，这是第一个专门用于自动化通信系统建模的推理大语言模型（LLM），并提出了一个两阶段的训练策略以及一个新的大规模开源数据集CSFRC。


<details>
  <summary>更多</summary>
  
**动机:** 现有的通用大语言模型（LLM）缺乏适应通信系统建模所需的专门领域知识、细微推理能力和高质量的领域特定训练数据。

**方法:** 提出了一种两阶段训练策略，包括监督微调（SFT）与思维链（CoT）数据以及基于ReMax的新型基于规则的强化学习算法C-ReMax。

**结果:** 开发了首个专门用于自动通信系统建模的推理LLM DeepForm和一个名为CSFRC的大规模开源数据集。

**结论:** DeepForm是第一个专门用于自动化通信系统建模的推理LLM，并通过实验展示了其在不同场景下显著优于较大的专有LLM的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepForm%3A+Reasoning+Large+Language+Model+for+Communication+System+Formulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08551，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08551&send_immediately=true&force_search=false)

**原文摘要:** Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [84] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的基于旅程的Transformer架构JoFormer，它通过可学习的方向变换来表示相对位置，并在Tiny Shakespeare字符级语言建模任务中展示了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Transformers在序列建模方面取得了巨大成功，但如何有效结合位置信息仍然是一个具有挑战性和活跃的研究领域。

**方法:** 从第一原理推导出JoFormer注意力机制，并将其与RoFormer基线在Tiny Shakespeare字符级语言建模任务中进行比较。

**结果:** 结果表明，JoFormer一致地实现了更低的困惑度和更快的收敛速度，突显了基于旅程的位置处理方式的优势。

**结论:** JoFormer为将位置信息整合到Transformer架构中提供了一个原则性的新方法，并展示了其作为更表达力更强的架构的前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JoFormer+%28Journey-based+Transformer%29%3A+Theory+and+Empirical+Analysis+on+the+Tiny+Shakespeare+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08652&send_immediately=true&force_search=false)

**原文摘要:** Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [85] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang, Hyungjin Park, Jinmyeong Choi, Taesup Kim*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为ChannelTokenFormer的基于Transformer的预测模型，以解决现实世界时间序列数据中的多通道依赖、异步采样和缺失值问题。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的时间序列数据通常是多变量的，具有复杂的跨通道依赖性，并且由于各种实际操作限制，每个通道的采样周期不同且容易缺失数据，而现有的架构通常无法处理这些问题。

**方法:** 设计了一个灵活的Transformer结构，能够显式地捕捉跨通道交互，适应各通道的异步采样，并有效处理缺失值。

**结果:** 在三个基准数据集和一个真实世界的工业数据集上的广泛实验表明，ChannelTokenFormer在具有挑战性的现实条件下表现出卓越的鲁棒性和准确性。

**结论:** ChannelTokenFormer是一种有效的模型，可以应对实际时间序列预测任务中多通道数据所带来的复杂问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Robust+Real-World+Multivariate+Time+Series+Forecasting%3A+A+Unified+Framework+for+Dependency%2C+Asynchrony%2C+and+Missingness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08660&send_immediately=true&force_search=false)

**原文摘要:** Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [86] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi, Matteo Metaldi, Michal Bechny, Irina Filchenko, Julia van der Meer, Markus H. Schmidt, Claudio L. A. Bassetti, Athina Tzovara, Francesca D. Faraci, Luigi Fiorillo*

**主要类别:** cs.LG

**AI概要:** 本文提出SLEEPYLAND框架和SOMNUS集成模型，显著提升了睡眠分期模型的公平性和泛化能力，表现优于当前最先进的方法和人类评分员。


<details>
  <summary>更多</summary>
  
**动机:** 由于深度学习在自动睡眠分期方面的进展在临床应用中受到限制，主要问题包括模型评估不公平、跨数据集泛化能力不足、模型偏差以及人工标注的变异性。

**方法:** 介绍了一种基于软投票机制的集成模型SOMNUS，并使用超过220,000小时的ID数据和84,000小时的OOD数据对其进行评估。

**结果:** SOMNUS在24个不同数据集中表现出稳健性能，宏观F1分数在68.7%到87.2%之间，在94.9%的情况下优于单个模型。

**结论:** SLEEPYLAND和SOMNUS提供了一个强大且公平的睡眠分期评估框架，具有超越现有方法和人类评分员的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SLEEPYLAND%3A+trust+begins+with+fair+evaluation+of+automatic+sleep+staging+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08574&send_immediately=true&force_search=false)

**原文摘要:** Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [87] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski, Michael Schäfer, Heiko Schwarz, Jonathan Pfaff, Detlev Marpe, Thomas Wiegand*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的深度学习图像压缩训练方法，通过在推理阶段重新训练网络来优化量化过程，从而提升压缩效率，特别是在熵约束量化中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 当前图像压缩方法中，量化过程导致了零导数问题，需要通过可微分近似来解决。然而，这些近似方法未能正确建模量化噪声，导致网络性能次优。因此，作者提出了新的训练策略以解决这一问题。

**方法:** 提出了一种额外的微调训练步骤，即在网络推理阶段获得量化潜在变量后对部分网络进行再训练。

**结果:** 实验结果显示，在Kodak测试集上平均比特率节省1%到2%，在TecNick测试集上最高可达2.2%的Bjøntegaard-Delta比特率节省，且未增加推理复杂度。

**结论:** 通过在推理阶段重新训练网络，可以显著提高压缩效率，尤其是在使用Trellis-Coded量化器的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Learned+Image+Compression+on+Scalar+and+Entropy-Constraint+Quantization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08662，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08662&send_immediately=true&force_search=false)

**原文摘要:** The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [88] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han, Menglin Xia, Daniel Madrigal Diaz, Samuel Kessler, Ankur Mallick, Xuchao Zhang, Mirian Del Carmen Hipolito Garcia, Jin Xu, Victor Rühle, Saravan Rajmohan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种利用LLM生成蓝图和提示模板搜索机制的新框架，有效提升了SLM的推理能力并减少了对提示变化的敏感性。


<details>
  <summary>更多</summary>
  
**动机:** 小型语言模型（SLMs）由于容量有限，推理能力和对提示变化的敏感性受到限制，因此需要一种高效的解决方案来提升其性能。

**方法:** 提出了一种通过LLM生成蓝图增强SLM推理能力的新框架，并整合了提示模板搜索机制以减少对提示变化的敏感性。

**结果:** 该框架在多种任务中展示了改进的SLM性能，包括数学（GSM8K）、编码（MBPP）和逻辑推理（BBH）。

**结论:** 该框架在不增加模型规模或需要额外训练的情况下提升了SLM的推理能力，为设备端或资源受限环境提供了轻量级且易于部署的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Reasoning+Capabilities+of+Small+Language+Models+with+Blueprints+and+Prompt+Template+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08669，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08669&send_immediately=true&force_search=false)

**原文摘要:** Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [89] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera, Shun Arakawa, Yuta Sato*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个基于Transformer模型的用户友好型Python库（CALT），旨在帮助非专家用户训练符号计算任务的深度学习模型。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能的最新进展展示了通过端到端深度学习实现符号计算的可学习性，这为符号计算社区带来了新的挑战和研究方向。

**方法:** 使用Transformer模型进行序列到序列的学习，通过大量示例训练模型以模拟目标计算。

**结果:** 引入了CALT，一个用户友好的Python库，帮助非深度学习专家进行符号计算任务的模型训练。

**结论:** CALT是一个有助于非深度学习专家训练用于符号计算任务的模型的用户友好型Python库。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CALT%3A+A+Library+for+Computer+Algebra+with+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08600，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08600&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [90] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie, Tangtang Xie*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于变分自编码器的模型VAE-LF，用于解决高维且不完整的电力负载监测数据问题，并在实验中表现出优越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 高维且不完整的电力负载监测数据对电力负载预测模型性能提出了挑战。

**方法:** 基于变分自编码器（VAE）的潜在表征模型VAE-LF，利用编码器-解码器结构学习数据的低维潜在表征。

**结果:** 实验结果显示，VAE-LF在5%和10%稀疏性测试案例中均显著优于其他基准模型，RMSE和MAE更低，尤其适用于低稀疏比数据。

**结论:** VAE-LF方法在电力负载管理中提供了高效的数据补全解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Autoencoder-Based+Approach+to+Latent+Feature+Analysis+on+Efficient+Representation+of+Power+Load+Monitoring+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08698&send_immediately=true&force_search=false)

**原文摘要:** With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [91] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand, Rohit Mehra, Priyavanshi Pathania, Nikhil Bamby, Vibhu Saujanya Sharma, Vikrant Kaulgud, Sanjay Podder, Adam P. Burden*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的方法R-ICE，用于估计提示级别的推理碳排放，并展示了其在推理排放估计方面的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 论文的动机是当前监控和估算能源消耗工具存在高输入数据点、侵入性强、误差大等问题，而利用新兴的LLM基准和相关数据点有助于克服这些挑战并平衡排放估算的准确性。

**方法:** 论文提出了一个演进框架R-ICE，通过利用现有的最先进的基准来估计提示级别的推理碳排放。

**结果:** 论文的验证结果表明，基于基准的建模在推理排放估计方面具有良好的潜力，并提供了一种更实用且非侵入性的方式来支持动态LLM路由、碳核算等新兴用例。

**结论:** 论文得出结论，基于基准的建模在推理排放估计方面具有巨大潜力，并值得科学界进一步探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+the+ICE%3A+Exploring+promises+and+challenges+of+benchmarks+for+Inference+Carbon+%26+Energy+estimation+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08727，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08727&send_immediately=true&force_search=false)

**原文摘要:** While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [92] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit, V Venktesh, Sourangshu Bhattacharya, Avishek Anand*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种高效的示例选择方法CASE，该方法显著降低了LLM调用次数和运行时间，同时保持了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 在上下文长度受限的情况下，如何选择有效的少样本示例以构建高效的提示是LLM应用中的关键问题。

**方法:** 将示例选择任务建模为top-m最佳臂识别问题，并提出了一种新的高效选择策略——CASE（Challenger Arm Sampling for Exemplar selection）；通过维护一个候选臂短列表来减少样本复杂度，并使用参数化线性评分函数对示例子集进行建模，从而形成随机线性赌博机设置。

**结果:** 实验表明，与现有方法相比，CASE在保持性能的同时减少了87%的LLM调用次数，并且运行速度提高了7倍。

**结论:** CASE实现了显著的效率提升，相较于最先进的示例选择方法，在不牺牲性能的情况下减少了7倍的LLM调用次数，并减少了7倍的运行时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Efficient+Demonstration+Selection+for+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08607&send_immediately=true&force_search=false)

**原文摘要:** The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [93] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma, Guoji Fu, Zhengding Luo, Jiele Wu, Tze-Yun Leong*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的强化学习探索策略RRP，通过增加环境奖励的噪声提高策略多样性，增强了探索效果，并且与现有的探索策略兼容，具有较高的通用性和低实现成本。


<details>
  <summary>更多</summary>
  
**动机:** 为了提升现有RL算法在各种任务中的样本效率以及避免陷入局部最优解，作者提出了RRP方法。

**方法:** 论文提出了一种名为随机奖励扰动（RRP）的方法，并进行了理论分析和实验验证。

**结果:** 实验表明，RRP显著提升了Proximal Policy Optimization和Soft Actor-Critic的性能，在稀疏和密集奖励场景下都实现了更高的样本效率并能够逃离局部最优解。

**结论:** RRP是一种用于强化学习的新型探索策略，它通过向环境奖励添加零均值噪声来有效增强训练期间的策略多样性，从而扩展探索范围。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+by+Random+Reward+Perturbation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08737&send_immediately=true&force_search=false)

**原文摘要:** We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [94] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 SeerAttention-R 的稀疏注意力框架，用于提高推理模型长解码的效率，并通过优化内核实现了显著的速度提升。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高推理模型长解码的效率并减少计算资源消耗，提出了 SeerAttention-R 稀疏注意力框架。

**方法:** 从 SeerAttention 扩展而来，保留了通过自蒸馏门控机制学习注意力稀疏性的设计，同时移除了查询池化以适应自回归解码，并通过轻量级插件门控实现灵活性和易集成性。

**结果:** SeerAttention-R 在仅训练 0.4B 标记的情况下，能够在 4K 标记预算下保持接近无损的推理准确性，并在 H100 GPU 上实现了接近理论上的速度提升。

**结论:** SeerAttention-R 在稀疏注意力块大小为 64/128 时，在 AIME 基准测试中保持接近无损的推理准确性，并且通过 TileLang 开发的高度优化的稀疏解码内核实现了比 FlashAttention-3 高达 9 倍的速度提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SeerAttention-R%3A+Sparse+Attention+Adaptation+for+Long+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08889，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08889&send_immediately=true&force_search=false)

**原文摘要:** We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [95] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng, Seohong Park, Sergey Levine, Benjamin Eysenbach*

**主要类别:** cs.LG

**AI概要:** 本文提出了InFOM方法，通过流匹配和用户意图建模，在强化学习的预训练中取得了显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习中的长期依赖性和适应不同任务的需求促使我们提出了一种新的预训练方法。

**方法:** 使用流匹配构建了一个概率模型来预测代理在遥远未来访问的状态，并引入了捕捉用户意图的隐变量。

**结果:** 实验表明，该方法在36个基于状态和4个基于图像的基准任务上表现优异。

**结论:** InFOM方法在预训练中实现了1.8倍的中位回报率提升，并将成功率提高了36%。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Intention-Conditioned+Flow+Occupancy+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08902，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08902&send_immediately=true&force_search=false)

**原文摘要:** Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [96] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim, JunHo Seo, Jongmin Lee, Byung-Jun Lee*

**主要类别:** cs.LG

**AI概要:** 论文发现近期增强DICE框架离线强化学习性能的方法削弱了其进行可靠离线策略评估的能力，为此提出了一种新方法解决这一问题。


<details>
  <summary>更多</summary>
  
**动机:** 最近旨在增强DICE框架的离线RL性能的方法无意中削弱了其执行OPE的能力，使它们不适合受限RL场景。

**方法:** 基于对半梯度优化依赖导致成本估计失败的根本原因的分析，提出了一种新方法来实现OPE和受限RL。

**结果:** 新方法确保准确的成本估计，并在离线受限RL基准测试DSRL上取得先进表现。

**结论:** 本文提出了通过半梯度DICE实现OPE和受限强化学习的新方法，该方法在离线受限RL基准DSRL上实现了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semi-gradient+DICE+for+Offline+Constrained+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08644，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08644&send_immediately=true&force_search=false)

**原文摘要:** Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [97] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang, Huaping Liu*

**主要类别:** cs.LG

**AI概要:** 论文探讨了深度强化学习代理在环境状态扰动下的脆弱性，并提出了一种新的攻击和防御方法，即Boosted Adversarial Training（BAT），以提升代理的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 论文旨在解决深度强化学习中较少被研究的环境状态扰动问题，特别是在具身场景中自然存在的扰动。

**方法:** 论文提出了一种初步的非目标攻击方法，并提出了BAT防御框架，该框架首先通过监督学习调整代理以避免灾难性故障，然后使用强化学习进行对抗训练。

**结果:** 实验结果表明，主流的深度强化学习代理在环境状态扰动下是脆弱的，而所提出的攻击方法有效。此外，现有的鲁棒强化学习算法可能不适用，而BAT框架则表现出显著增强的鲁棒性。

**结论:** 论文得出结论，Boosted Adversarial Training（BAT）框架能够显著提高深度强化学习代理在面对环境状态扰动时的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Robust+Deep+Reinforcement+Learning+against+Environmental+State+Perturbation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08961，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08961&send_immediately=true&force_search=false)

**原文摘要:** Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [98] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu, Jingwei Zhang, Farzan Farnia*

**主要类别:** cs.LG

**AI概要:** 本文提出RP-KrossFuse方法，通过基于随机投影的Kronecker积融合跨模态和单模态嵌入，既保持了跨模态对齐能力，又提升了模态特定任务的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的跨模态嵌入（如CLIP、BLIP）在跨模态对齐上表现良好，但在特定模态任务上可能不如最先进的单模态嵌入；而单模态嵌入虽然在各自领域表现出色，但缺乏跨模态对齐能力。因此需要一种能同时兼顾模态特定性能和跨模态对齐的方法。

**方法:** RP-KrossFuse方法利用基于随机投影的Kronecker积整合跨模态嵌入与单模态嵌入，融合样本对相似性得分，并通过随机Fourier特征在指定核空间中高效运行，支持可扩展实现。

**结果:** 通过多个数值实验，将CLIP嵌入与单模态图像和文本嵌入结合，结果表明RP-KrossFuse在保留跨模态对齐能力的同时，达到了具有竞争力的模态特定性能。

**结论:** RP-KrossFuse方法有效地结合了跨模态和单模态嵌入，在保持跨模态对齐的同时实现了有竞争力的模态特定性能，从而弥合了跨模态和单模态嵌入之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fusing+Cross-modal+and+Uni-modal+Representations%3A+A+Kronecker+Product+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08645，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08645&send_immediately=true&force_search=false)

**原文摘要:** Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [99] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao, Huiyu Bai, Xuejiao Zhao*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的数据增强框架，通过改进的偏好关系精炼、评分机制和优化方法，使少量数据下训练的奖励模型表现接近于大量数据训练的效果。


<details>
  <summary>更多</summary>
  
**动机:** 提升在少量数据情况下训练高性能奖励模型的能力，以增强基于人类反馈的强化学习（RLHF）的效率和可扩展性。

**方法:** 引入了一种数据增强和扩展框架，包括偏好关系精炼、基于困惑度的评分机制以及多级直接偏好优化（M-DPO）方法。

**结果:** 实验结果表明，所提出的方法显著提高了数据效率和模型性能，使得在少量数据情况下训练的奖励模型能够达到与大规模数据训练相当的结果。

**结论:** 该研究强调了数据高效策略在奖励模型优化中的潜力，为资源有限的RLHF应用提供了强有力的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GFRIEND%3A+Generative+Few-shot+Reward+Inference+through+EfficieNt+DPO，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08965&send_immediately=true&force_search=false)

**原文摘要:** The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [100] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier, Malte Schilling*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的时间序列预测方法TimeFlex，并通过一个新颖的合成数据集来分析模型性能与时间序列特征之间的关系。


<details>
  <summary>更多</summary>
  
**动机:** 为了揭示时间序列特性与特定模型之间的明确联系，特别是在模型适应性和准确性方面。

**方法:** 研究引入了一个基于高斯过程的新型数据集，用于展示明确且已知的特征，并提出了TimeFlex模型，其模块化架构旨在处理多样化的时间动态。

**结果:** 研究结果展示了TimeFlex模型相较于当前最先进的模型在目标评估中的优越性能。

**结论:** TimeFlex模型在处理具有不同时间动态（如趋势和周期性模式）的时间序列方面表现出色，为模型性能在不同时间序列条件下的表现提供了更深入的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tailored+Architectures+for+Time+Series+Forecasting%3A+Evaluating+Deep+Learning+Models+on+Gaussian+Process-Generated+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08977&send_immediately=true&force_search=false)

**原文摘要:** Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [101] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek, Jan Luxemburk, Richard Plny, Josef Koumar, Jaroslav Pesek, Karel Hynek*

**主要类别:** cs.LG

**AI概要:** 本文研究了在流量分类中一个简单的k-NN方法为何能与复杂模型媲美，发现数据集中大量冗余样本影响了模型性能评估，并指出当前机器学习实践可能不适合此类任务。


<details>
  <summary>更多</summary>
  
**动机:** 近期发现使用数据包序列元数据的简单k-NN方法可以与更复杂的模型表现相当甚至更好，因此进一步研究这一现象背后的原因。

**方法:** 论文通过在12个数据集和15个TC任务上评估k-NN基线方法，并分析其性能优异的原因。

**结果:** 分析表明，大多数数据集中包含超过50%的冗余样本，这会导致模型性能被高估，并降低理论最大准确率。

**结论:** 论文得出结论，标准的机器学习实践可能不适合流量分类（TC），并提出了新的任务制定和评估方向以应对这些挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Simple+Model+Just+Works%3A+Is+Network+Traffic+Classification+in+Crisis%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08655&send_immediately=true&force_search=false)

**原文摘要:** Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [102] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk, Jaap Jumelet, Willem Zuidema*

**主要类别:** cs.LG

**AI概要:** 论文探讨了三种神经架构在基于命题逻辑任务中的泛化能力，发现它们在处理未见模式时存在显著局限性，尤其是在涉及否定的情况下。


<details>
  <summary>更多</summary>
  
**动机:** 研究神经网络如何获取和表示符号规则是当前研究和争论的关键话题，尤其是大型语言模型在推理任务中的失败情况。

**方法:** 引入了现有数据集的平衡扩展，并评估三种关键神经架构在命题逻辑任务中的泛化行为。

**结果:** 所有模型在分布内表现良好，但对未见模式（特别是涉及否定）的泛化仍然是一个重大挑战。除非引入结构偏差，否则Transformer无法组合应用否定。

**结论:** 标准架构在学习逻辑运算符的系统表示方面存在持续限制，这表明需要更强的归纳偏置以支持基于规则的稳健推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Propositional+Logic+for+Probing+Generalization+in+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08978&send_immediately=true&force_search=false)

**原文摘要:** The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [103] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi, Brian Karrer, Itai Gat, Ricky T. Q. Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Edit Flows的非自回归模型，通过编辑操作解决传统模型在变长序列生成中的限制，并展示了其在多个任务上的优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的非自回归模型难以生成变长序列，且通常施加严格的逐token结构，而自回归模型天然支持变长序列生成。因此，研究者提出了Edit Flows以解决这些问题，更贴近序列数据的内在结构。

**方法:** 通过在序列空间内定义离散流并利用编辑操作（插入、删除、替换）建模，在连续时间马尔可夫链中进行序列生成，并使用带有辅助变量的扩展状态空间进行高效训练。

**结果:** 实验结果表明，Edit Flows在图像描述生成任务上优于自回归模型和掩码模型，在文本和代码生成方面也显著优于掩码构建方法。

**结论:** Edit Flows是一种克服自回归模型和掩码模型局限性的新型非自回归模型，能够在序列空间中通过编辑操作实现灵活的位置相关生成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Edit+Flows%3A+Flow+Matching+with+Edit+Operations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09018，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09018&send_immediately=true&force_search=false)

**原文摘要:** Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [104] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang*

**主要类别:** cs.LG

**AI概要:** 本论文介绍了一种新型的零阶优化算法FZOO，用于解决大型语言模型微调中的内存瓶颈问题，实现了比现有方法更快的收敛速度和更高的内存效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统的Adam优化器在微调大型语言模型时存在GPU内存瓶颈，而现有的零阶优化方法如MeZO需要更多的迭代次数才能收敛。

**方法:** 提出了一种新的零阶优化算法FZOO，利用批量单侧估计和Rademacher随机向量扰动加速计算，并基于批量损失的标准差调整步长。

**结果:** FZOO相比MeZO平均提升了3%的准确率，同时所需的前向传播次数减少了3倍；对于RoBERTa-large，在准确率上平均提高了5.6%，前向传播次数减少了18倍。

**结论:** FZOO能够在减少内存使用的同时实现接近Adam的收敛速度，为大规模模型的微调提供了高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FZOO%3A+Fast+Zeroth-Order+Optimizer+for+Fine-Tuning+Large+Language+Models+towards+Adam-Scale+Speed，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09034&send_immediately=true&force_search=false)

**原文摘要:** Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [105] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma, Chenyang Lin, Yao Zhang, Volker Tresp, Yunpu Ma*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 Agentic Neural Network 的新框架，将多智能体协作视为分层神经网络架构，实现更高的准确性和适应性。


<details>
  <summary>更多</summary>
  
**动机:** 当前多智能体配置依赖静态、手动设计，难以应对复杂高维任务，需要一种更灵活、自动化的解决方案。

**方法:** 提出 Agentic Neural Network 框架，采用两阶段优化策略：前向阶段动态分解任务并构建合作团队；反向阶段通过迭代反馈优化协作。

**结果:** 在四个基准数据集上，ANN 在相同配置下超越了领先的多智能体基线方法，表现出一致的性能提升。

**结论:** Agentic Neural Network (ANN) 提供了一个可扩展的、数据驱动的多智能体系统框架，结合了大语言模型的协作能力和神经网络的效率和灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+Neural+Networks%3A+Self-Evolving+Multi-Agent+Systems+via+Textual+Backpropagation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09046&send_immediately=true&force_search=false)

**原文摘要:** Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [106] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty, Kushagra Chatterjee, Debarati Das, Tien Long Nguyen, Romina Nobahari*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了如何在保证公平性的同时，将多个输入聚类聚合为一个代表数据结构的聚类。


<details>
  <summary>更多</summary>
  
**动机:** 这项工作的动机是通过结合Chierichetti等人引入的公平聚类理念，确保数据集中每个受保护组的比例代表性，从而找到一个不仅具有代表性而且公平的共识聚类。

**方法:** 研究者使用了近似算法来解决公平共识聚类问题，并开发了一种最优算法用于群体表示相等的数据集，以及一种近线性时间的常数因子近似算法用于更一般的情况。

**结果:** 该研究的结果包括一种常数因子近似算法，用于公平共识聚类问题，以及在需要公平表示的许多聚类应用中必不可少的后处理步骤的最小修改现有聚类的方法。此外，作者证明了对于两个大小不等的组来说，这个问题是NP难的。

**结论:** 这篇论文首次尝试解决公平共识聚类问题，并提供了常数因子近似算法。作者认为他们的结果可能对其他聚类问题有更广泛的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Fair+Representation%3A+Clustering+and+Consensus，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08673&send_immediately=true&force_search=false)

**原文摘要:** Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [107] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen, Ngoc-Hieu Nguyen, Duy H. M. Nguyen, Anji Liu, An Mai, Binh T. Nguyen, Daniel Sonntag, Khoa D. Doan*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种名为IS-DAAs的重要性采样方法，用于解决Direct Alignment Algorithms中的过优化问题，实验证明其效果显著。


<details>
  <summary>更多</summary>
  
**动机:** Direct Alignment Algorithms（DAAs）容易受到过优化的影响，导致训练过程中性能下降，需要一种方法来缓解这个问题。

**方法:** 通过将DAA目标与考虑参考策略分布的重要性比率相乘，并对重要性比率进行剪裁以避免高方差问题。

**结果:** 实验表明，IS-DAAs可以有效缓解过优化问题，尤其是在低正则化强度情况下表现优于其他现有方法。

**结论:** IS-DAAs是一种新的重要性采样方法，能够有效缓解离线DAA的过优化问题，并在低正则化强度下表现更优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Reward+Over-optimization+in+Direct+Alignment+Algorithms+with+Importance+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08681，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08681&send_immediately=true&force_search=false)

**原文摘要:** Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [108] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar, Shuvom Sadhuka, Bonnie Berger, Emma Pierson, Nikhil Garg*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种利用政府评分和众包报告数据的城市事件潜在状态预测方法，通过多视图多输出GNN模型解决了数据稀疏性和偏差问题，并公开了一个大规模纽约市事件数据集。


<details>
  <summary>更多</summary>
  
**动机:** 政府官员需要了解城市街区中如坑洼或啮齿类动物问题的发生情况，但评分数据稀疏，而众包报告可能存在偏差，因此需要一种综合分析方法。

**方法:** 提出了基于多视图、多输出的GNN模型，结合了无偏的政府评分数据和有偏的众包报告数据以预测潜在的事件状态。

**结果:** 通过真实和半合成数据实验表明，该模型相比仅使用报告数据或评分数据的模型能更好地预测潜在状态，特别是在评分数据稀疏且报告对评分具有预测性时。同时量化了众包报告中的社会经济偏差。

**结论:** 论文得出结论，提出的方法能够更准确地预测潜在的事件状态，并展示了在异构、稀疏和有偏数据下广泛适用的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Urban+Incident+Prediction+with+Graph+Neural+Networks%3A+Integrating+Government+Ratings+and+Crowdsourced+Reports，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08740，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08740&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [109] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun, Soufiane Hayou, Hanan Salam, Mohamed El Amine Seddik, Pierre Youssef*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一个深度神经网络的通用稳定性定理，适用于稀疏性和弱相关权重情况，为现代神经网络初始化方案提供了理论支持。


<details>
  <summary>更多</summary>
  
**动机:** 解决深度增加时深层神经网络出现的梯度爆炸或消失问题，并超越先前仅限于全连接网络的分析限制。

**方法:** 提出了一种适用于稀疏性和非独立同分布权重的深度神经网络稳定性定理。

**结果:** 建立了一个更广泛的网络模型类别中的谱稳定性严格保证。

**结论:** 作者通过使用随机矩阵理论的最新进展，为具有结构化和依赖随机性的现代神经网络提供了初始化方案的理论基础扩展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Stability+of+the+Jacobian+Matrix+in+Deep+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08764，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08764&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [110] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner, Beat Buesser Ana-Maria Creţu, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tramèr, Václav Volhejn*

**主要类别:** cs.LG

**AI概要:** 该研究针对大型语言模型驱动的AI代理面临的提示注入攻击威胁，提出了一套具有理论保证的防御设计模式，并通过案例研究验证了其实用性和安全性。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型（LLMs）驱动的AI代理变得越来越多功能化并能够处理广泛的任务，确保其安全性成为一个关键挑战，尤其是面对提示注入攻击时。

**方法:** 系统性分析和案例研究方法被用于评估所提出的设计模式的有效性和实际可行性。

**结果:** 提出了一系列原则性设计模式，用以构建能够抵御提示注入攻击的AI代理，并通过案例研究验证了这些模式的实用性与安全性。

**结论:** 本文提出了一套构建对提示注入攻击具有可证明抵抗力的AI代理的原则设计模式，并系统地分析了这些模式在实用性和安全性方面的权衡，通过案例研究展示了它们的实际应用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Design+Patterns+for+Securing+LLM+Agents+against+Prompt+Injections，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08837，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08837&send_immediately=true&force_search=false)

**原文摘要:** As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [111] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun, David Antony Selby, Yunchuan Huang, Sebastian Vollmer, Seth Flaxman, Anisoara Calinescu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了IMAGIC-500数据集和基准测试，用于评估不同缺失机制和缺失比例下的插补方法效果，以推动稳健插补算法的发展并促进可重复的社会科学研究。


<details>
  <summary>更多</summary>
  
**动机:** 缺失数据插补在数据科学和机器学习中是一个重要挑战，尤其是在社会经济研究领域。然而，现实的社会经济数据通常受到严格的数据保护协议限制，这使得数据共享变得困难，从而限制了插补方法的系统评估。

**方法:** 利用世界银行公开的合成数据集，从中衍生出IMAGIC-500数据集，并在不同的缺失机制和缺失比例下进行插补基准测试。

**结果:** 结果展示了统计学、传统机器学习和深度学习插补技术（包括最新的扩散方法）的优势和劣势。

**结论:** IMAGIC-500数据集和基准测试旨在促进稳健插补算法的发展，并推动可重复的社会科学研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IMAGIC-500%3A+IMputation+benchmark+on+A+Generative+Imaginary+Country+%28500k+samples%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08844&send_immediately=true&force_search=false)

**原文摘要:** Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [112] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan, Akramul Azim, Qusay Mahmoud*

**主要类别:** cs.LG

**AI概要:** 为了解决边缘计算环境中软实时应用任务调度面临的复杂性和动态条件挑战，本研究提出了敏捷强化学习方法（aRL）。这种方法通过有指导的探索和动作掩码提高了预测能力和收敛速度，从而在命中率和收敛速度方面优于传统方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的启发式和元启发式算法难以应对边缘计算环境中的任务调度复杂性，而强化学习算法需要较长的学习时间来适应新环境并解决中大规模问题。

**方法:** 提出了一种敏捷强化学习方法（aRL），其中RL代理进行有指导的探索，并仅执行相关动作。结合了有指导探索和动作掩码方法。

**结果:** 实验表明，结合有指导探索和动作掩码方法使aRL实现了比基线方法更高的命中率和更快的收敛速度。

**结论:** Agile Reinforcement learning (aRL) 提高了预测能力，加快了适应和收敛速度，使其成为边缘计算中软实时应用程序任务调度的合适候选方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agile+Reinforcement+Learning+for+Real-Time+Task+Scheduling+in+Edge+Computing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08850，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08850&send_immediately=true&force_search=false)

**原文摘要:** Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [113] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio, Madeline Navarro, Samuel Rey, Santiago Segarra, Antonio G. Marques*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的图神经网络架构SG-GNN，通过利用节点的结构属性创建新的图结构，从而改善传统GNN在异配性数据上的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的GNN在处理异配性数据时表现不佳，因为它们通常假设同质性并依赖局部消息传递。为了解决这个问题，作者提出了一种新的方法，利用节点的结构属性来创建更优的图结构。

**方法:** 提出了Structure-Guided GNN (SG-GNN)，通过链接具有相似结构属性的节点来创建替代图结构，并自适应地学习权衡原始图和新图的贡献。

**结果:** 实验表明，SG-GNN在各种基准数据集上，尤其是在具有异配性特征的数据集上，达到了最先进的或高度竞争性的性能。

**结论:** SG-GNN通过利用结构信息指导图神经网络，在处理异配性数据方面表现出色，证明了利用结构属性创建新图以提高GNN性能的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+to+Heterophilic+Graph+Data+with+Structure-Guided+Neighbor+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08871&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [114] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis, Themistoklis Sarantakos, Ioannis Chatzigiannakis, Georgios Mylonas*

**主要类别:** cs.LG

**AI概要:** 本文探讨了如何利用数据填补技术提升智能水表在供水管网监测中的数据质量和应用效果。


<details>
  <summary>更多</summary>
  
**动机:** 由于技术问题导致的数据缺失可能严重影响运营决策和效率，因此需要探索近期的数据填补技术以增强供水管网的监测和管理。

**方法:** 通过比较多种填补方法，如k-最近邻、MissForest、Transformer和循环神经网络。

**结果:** 研究结果表明，使用合适的数据填补技术能够显著提升智能水表数据的质量及其可靠性。

**结论:** 有效的数据填补可以显著提高从用水数据中得出的见解质量，为漏损检测和预测性维护调度等应用提供解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Filling+in+the+Blanks%3A+Applying+Data+Imputation+in+incomplete+Water+Metering+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08882&send_immediately=true&force_search=false)

**原文摘要:** In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [115] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang, Shujian Yu*

**主要类别:** cs.LG

**AI概要:** InfoDPCCA是一种新型的动态概率CCA框架，通过信息理论方法提取共享和特定于序列的潜在表示，提高了潜在表示学习的性能和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 从高维序列数据中提取有意义的潜在表示是机器学习中的一个关键挑战，而现有的动态CCA模型未能充分平衡表示压缩与预测充分性。

**方法:** InfoDPCCA利用一种新的信息理论目标，提取共享潜在表示，并学习单独编码每个序列信息的潜在成分；采用两步训练方案和残差连接机制增强训练稳定性。

**结果:** 通过在合成数据和医学fMRI数据上的实验，InfoDPCCA在潜在表示学习方面表现出色。

**结论:** InfoDPCCA是一种新的动态概率CCA框架，能够从高维序列数据中提取有意义的潜在表示，具有良好的解释性和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InfoDPCCA%3A+Information-Theoretic+Dynamic+Probabilistic+Canonical+Correlation+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08884，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08884&send_immediately=true&force_search=false)

**原文摘要:** Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [116] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel, John T. Nardini, Kevin B. Flores, Erica M. Rutter, Suzanne S. Sindi, Alexandria Volkening*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种改进的方程学习方法Multi-experiment equation learning（ME-EQL），能够显著提升从基于主体的生物系统建模中获得的连续模型的泛化能力和准确性。


<details>
  <summary>更多</summary>
  
**动机:** Agent-based modeling（ABM）虽然强大，但计算成本高且难以解析处理；而传统的方程学习（EQL）方法通常需要大量模拟，影响其泛化能力。因此，需要新方法来减少计算负担并提高模型的泛化和解析能力。

**方法:** 引入了两种新的方程学习（EQL）方法：一次一个参数集的多实验方程学习（OAT ME-EQL），通过插值连接不同参数集的个体模型；嵌入结构的多实验方程学习（ES ME-EQL），构建跨参数的统一模型库。

**结果:** 两种ME-EQL方法显著降低了从基于主体的模拟中恢复参数的相对误差，并且OAT ME-EQL在参数空间上的泛化能力更优。

**结论:** Multi-experiment equation learning (ME-EQL)方法在从基于主体的模型数据中学习连续模型方面具有潜力，尤其是在提升模型的泛化能力和可解释性方面。OAT ME-EQL在参数空间的泛化能力上表现更好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+generalizability+of+model+discovery+across+parameter+space+with+multi-experiment+equation+learning+%28ME-EQL%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08916&send_immediately=true&force_search=false)

**原文摘要:** Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [117] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa, Artem Moskale, Pushpak Pati, Tommaso Mansi, Mangal Prakash, Rui Liao*

**主要类别:** cs.LG

**AI概要:** BioLangFusion是一种将DNA、mRNA和蛋白质语言模型整合为统一分子表示的简单有效的方法。


<details>
  <summary>更多</summary>
  
**动机:** 受分子生物学中心法则（信息从基因到转录本再到蛋白质的流动）的驱动，希望确保直接的跨模态对应关系。

**方法:** BioLangFusion在密码子级别对齐每个模态的嵌入，并研究了三种标准融合技术：(i) 密码子级别的嵌入连接，(ii) 受多重实例学习启发的熵正则化注意力池化，(iii) 跨模态多头注意力。

**结果:** BioLangFusion在五个分子属性预测任务中均优于强单模态基线方法。

**结论:** BioLangFusion能够通过简单的预训练模型融合捕捉具有最小开销的互补多组学信息。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BioLangFusion%3A+Multimodal+Fusion+of+DNA%2C+mRNA%2C+and+Protein+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08936，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08936&send_immediately=true&force_search=false)

**原文摘要:** We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [118] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye, Gaoxiang Duan, Haoran Zeng, Yangxin Zhu, Lingxue Meng, Xiaoying Zheng, Yongxin Zhu*

**主要类别:** cs.LG

**AI概要:** 本文提出了KARMA，一种用于多变量长期高效时间序列预测的模型，结合了自适应分解模块和基于Mamba的处理模块，在多个真实数据集上验证了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的时间序列分解方法单一且固定，无法有效挖掘复杂时间序列中的潜在信息，而基于Transformer的模型因计算复杂度高难以有效建模长序列和复杂的动态关系。

**方法:** 引入了KARMA模型，包含自适应时间通道分解模块（ATCD）用于动态提取趋势和季节成分，以及混合频域-时域分解模块（HFTD）进一步将序列分解为频域和时域成分，并通过多尺度Mamba-based KarmaBlock模块高效处理全局与局部信息。

**结果:** 在八个来自不同领域的实际数据集上的实验表明，KARMA在预测精度和计算效率方面均显著优于主流基线方法。

**结论:** KARMA模型通过结合自适应分解与高效信息处理模块，有效解决了复杂时间序列预测中的关键问题，具有较高的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KARMA%3A+A+Multilevel+Decomposition+Hybrid+Mamba+Framework+for+Multivariate+Long-Term+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08939，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08939&send_immediately=true&force_search=false)

**原文摘要:** Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [119] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev, Akim Kotelnikov, Nikolay Kartashev*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了TabPFNv2表格基础模型的微调方法及其对模型内部机制的影响，证明了完全微调是最有效的解决方案，并显示微调能提高模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管TabPFNv2在小规模数据集上表现优异，但如何最佳地微调表格基础模型以及这种微调如何改变其内部机制仍然是未被充分探索的问题。

**方法:** 作者系统地评估了各种微调策略，并研究了微调对TabPFNv2内部机制的影响，将其与检索增强模型进行类比。

**结果:** 研究发现，完全微调是最有效和时间高效的解决方案，并且微调后的TabPFNv2能够更好地逼近目标依赖性。此外，在具有独立同分布拆分的学术数据集上，微调允许TabPFNv2实现最先进的结果。

**结论:** 论文得出结论，完全微调是TabPFNv2的最实用解决方案，并揭示了微调如何通过改进查询表示和键表示之间的点积来提升模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Finetuning+Tabular+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08982&send_immediately=true&force_search=false)

**原文摘要:** Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [120] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, Weizhu Chen*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的问题合成框架SwS，通过模型自我识别弱点并针对性训练，提高了强化学习的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的合成数据集缺乏高质量、可验证答案的问题，且合成策略不够高效，因此需要一种新的方法来提高强化学习的有效性和效率。

**方法:** 提出了一种Self-aware Weakness-driven problem Synthesis框架(SwS)，用于系统地识别模型缺陷并利用这些缺陷进行问题增强训练。

**结果:** 在7B和32B模型上，该方法在八个主流推理基准测试中分别实现了平均10.0%和7.7%的性能提升。

**结论:** 论文得出结论，SwS框架能够在不依赖外部知识蒸馏的情况下实现强化学习中模型的自我识别和弱点加强，从而带来性能提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwS%3A+Self-aware+Weakness-driven+Problem+Synthesis+in+Reinforcement+Learning+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08989，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08989&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [121] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang, Yinuo Zhang, Alexander Tong, Pranam Chatterjee*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的生成建模方法BranchSBM，用於解決從一個初始分佈到多個目標分佈的中間軌跡預測問題。


<details>
  <summary>更多</summary>
  
**动机:** 現有方法（如流匹配和薛定諤橋匹配）只能建模單一隨機路徑，無法捕捉從同一源頭分支到多個不同結果的演化過程。

**方法:** 引入Branched Schr"odinger Bridge Matching (BranchSBM)，通過參數化多個時間相關的速度場和增長過程來學習分支的薛定諤橋。

**结果:** BranchSBM能夠有效表示群體層級的分歧，適用於多路徑表面導航、細胞命運分歧建模以及對干擾的細胞反應模擬。

**结论:** BranchSBM是一種更具表現力且必要的框架，可用於處理多路徑演化問題。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Branched+Schr%C3%B6dinger+Bridge+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09007，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09007&send_immediately=true&force_search=false)

**原文摘要:** Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [122] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt, Prasanga Dhungel, Christoffer Löffler, Björn Nieth, Stephan Günnemann, Leo Schwinn*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的数据修剪框架，能够通过仅训练一小部分数据来预测整个数据集的样本重要性，从而提高计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决传统数据修剪技术需要完整训练过程从而导致效率低下的问题，本研究旨在开发一种新的重要性分数外推框架。

**方法:** 该研究提出了基于k近邻和图神经网络的两种方法来预测整个数据集中样本的重要性。

**结果:** 研究结果表明，该框架在多个数据集和训练范式中都表现良好，证明了其有效性和广泛适用性。

**结论:** 论文得出结论，他们的新框架在减少训练数据量的同时保持了模型性能，并且可以推广到不同的任务和数据集上。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Effective+Data+Pruning+through+Score+Extrapolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09010，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09010&send_immediately=true&force_search=false)

**原文摘要:** Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [123] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang, Daman Arora, Song Mei, Andrea Zanette*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为SPEED的新方法，通过选择性地使用中间难度的训练样例来提高大型语言模型的强化学习效率。


<details>
  <summary>更多</summary>
  
**动机:** 由于低效的均匀提示采样，使用强化学习训练大型语言模型在计算上是昂贵的。

**方法:** 引入了选择性提示和难度估计效率（SPEED），这是一种在线RL课程方法，选择具有中间难度的训练样例以最大化学习效率。

**结果:** 理论上，确定了中间难度的提示可以提高梯度估计器的信噪比，从而加速收敛；经验上，其高效实现导致训练速度加快2到6倍，且不会降低准确性。

**结论:** SPEED是一种高效的RL课程方法，可以加速训练过程，并且无需手动调整，能无缝集成到标准RL算法中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SPEED-RL%3A+Faster+Training+of+Reasoning+Models+via+Online+Curriculum+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09016，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09016&send_immediately=true&force_search=false)

**原文摘要:** Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [124] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar*

**主要类别:** cs.LG

**AI概要:** 本文提出e3方法，通过链式技能、负梯度探索和课程设计，提高LLM在测试时的推理能力，使其在多个指标上达到最佳表现并实现外推。


<details>
  <summary>更多</summary>
  
**动机:** 现有的推理模型在测试时扩展计算资源的情况下难以有效外推到更复杂的问题，需要一种新的训练方法以提升LLM在超出训练标记预算情况下的推理能力。

**方法:** 提出了一种名为e3的方法，包括链式操作、利用负梯度进行探索以及设计特定课程来耦合任务难度与训练标记预算。

**结果:** 使用e3训练的1.7B模型在AIME'25和HMMT'25评分中表现最佳，不仅提高了pass@1分数，还显著提升了pass@k分数，并且能够外推至2倍的训练标记预算。

**结论:** e3方法通过链式技能、利用错误轨迹的负梯度和结合任务难度与训练标记预算三个关键要素，实现了LLM在测试时扩展推理能力的有效训练，取得了1.7B模型的最佳成绩，并能外推到2倍的训练标记预算。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是e3%3A+Learning+to+Explore+Enables+Extrapolation+of+Test-Time+Compute+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09026&send_immediately=true&force_search=false)

**原文摘要:** Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [125] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino, Thomas Kehrenberg, Jose A. Lozano, Novi Quadrianto*

**主要类别:** cs.LG

**AI概要:** 论文探讨了可执行预测中的分布变化问题，通过可视化方法提供实用见解，并提出了新的应用场景。


<details>
  <summary>更多</summary>
  
**动机:** 文献中关于可执行预测的理论视角已经给出数学保证，但缺乏实际洞察，因此需要通过可视化损失景观来获得实践见解。

**方法:** 通过可视化损失景观来补充理论进展，使用两步过程进行解耦风险可视化，关注模型参数和数据参数。

**结果:** 引入了解耦风险可视化方法并提出新的兴趣点属性，研究了现有算法如何遍历风险景观，并在更现实的情况下表现。此外，还引入了扩展的可执行预测新场景。

**结论:** 本文提出了一个简单的解耦风险可视化方法，并介绍了扩展的可执行预测的新设置，以反映代理通常无法完全访问部署模型的现实情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Decoupled+Risk+Landscape+in+Performative+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09044&send_immediately=true&force_search=false)

**原文摘要:** Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [126] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong, Jiachen Jiang, Zhihui Zhu, Xia Ning*

**主要类别:** cs.LG

**AI概要:** 论文提出线性组合猜想解释任务向量机制，通过多种方法验证其有效性，并探讨任务向量在上下文学习中的作用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管任务向量在上下文学习中加速推理方面表现出色，但其背后原理尚不明确，因此需要深入研究其机制。

**方法:** 论文采用损失景观分析、显著性分析和参数可视化方法，预测任务向量在表示高秩映射时的失败并验证其猜想。

**结果:** 论文显示任务向量自然出现在经过三元组格式提示训练的线性变换器中，并确认任务向量在实际LLMs中的表现。

**结论:** 论文得出任务向量通过线性组合原始上下文示例形成单一上下文示例的线性组合猜想，并通过理论和实证支持该猜想。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Task+Vectors+in+In-Context+Learning%3A+Emergence%2C+Functionality%2C+and+Limitations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09048&send_immediately=true&force_search=false)

**原文摘要:** Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [127] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

**主要类别:** cs.AI

**AI概要:** TIP-Search是一个为了实时市场预测而设计的时间可预测推理调度框架，在不确定的工作负载下具有稳健的低延迟金融推断能力。


<details>
  <summary>更多</summary>
  
**动机:** 高频金融系统中严格的延迟需求促使了TIP-Search的提出。

**方法:** TIP-Search通过离线分析延迟和泛化性能，然后在线执行任务感知选择，从异构池中动态选择深度学习模型。

**结果:** TIP-Search在三个真实世界的限价订单簿数据集上进行了评估，并证明其比静态基线有高达8.5%的准确率提升和100%的截止时间满足率。

**结论:** TIP-Search在不确定工作负载下能够有效进行稳健的低延迟金融推断。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TIP-Search%3A+Time-Predictable+Inference+Scheduling+for+Market+Prediction+under+Uncertain+Load，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08026&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [128] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)
*Akash Vishwakarma, Hojin Lee, Mohith Suresh, Priyam Shankar Sharma, Rahul Vishwakarma, Sparsh Gupta, Yuvraj Anupam Chauhan*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一个名为Cognitive Weave的新颖记忆框架，旨在解决当前记忆系统在结构灵活性、时间感知和高层见解合成方面的局限性，通过多层时空共振图和语义Oracle接口管理信息，实现了显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于大型语言模型（LLM）的智能体需要超越单纯数据存储的记忆架构，以实现持续学习、精细推理和动态适应。然而，现有的记忆系统存在结构性灵活性、时间感知能力和从原始交互数据合成高层见解的能力等基本限制。

**方法:** 提出了一种新的记忆框架Cognitive Weave，使用多层时空共振图（STRG）管理信息，并通过语义Oracle接口（SOI）动态丰富洞察粒子（IPs）。

**结果:** 实验结果显示，Cognitive Weave在长期规划任务、演化问答场景和多会话对话一致性方面比现有方法有显著提升，任务完成率平均提高了34%，查询延迟减少了42%。

**结论:** Cognitive Weave是一个以多层时空共振图为中心的记忆框架，它通过认知提炼过程和洞察力聚合来增强长期任务的规划能力，并在与现有方法相比时表现出显著改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cognitive+Weave%3A+Synthesizing+Abstracted+Knowledge+with+a+Spatio-Temporal+Resonance+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08098，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08098&send_immediately=true&force_search=false)

**原文摘要:** The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [129] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)
*Subhrangshu Nandi, Arghya Datta, Nikhil Vichare, Indranil Bhattacharya, Huzefa Raja, Jing Xu, Shayan Ray, Giuseppe Carenini, Abhi Srivastava, Aaron Chan, Man Ho Woo, Amar Kandola, Brandon Theresa, Francesco Carbone*

**主要类别:** cs.AI

**AI概要:** 本论文介绍了为解决大型语言模型在执行复杂标准操作程序中的不足而构建的合成数据生成框架和SOP-Bench基准测试，并评估了现有代理架构的表现。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型难以执行需要严格遵守标准操作程序的复杂、长期的工作流程，而现实工业自动化中对此有关键需求。此外，目前缺乏反映标准操作程序复杂性和领域细微差别的公共基准。

**方法:** 开发了一个合成数据生成框架，并创建了SOP-Bench基准测试，包含10个工业领域的超过1800个任务。

**结果:** 评估了两种主要代理架构：函数调用和ReAct代理，在SOP-Bench上的平均成功率仅为27%和48%。当工具注册表远大于必要时，代理几乎100%调用了错误的工具。

**结论:** 当前基于LLM的代理在自动化真实世界的标准操作程序方面存在显著不足，需要针对特定领域进行基准测试和架构选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SOP-Bench%3A+Complex+Industrial+SOPs+for+Evaluating+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08119&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [130] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)
*Qiyao Wei, Samuel Holt, Jing Yang, Markus Wulfmeier, Mihaela van der Schaar*

**主要类别:** cs.AI

**AI概要:** 该论文探讨了机器学习领域中AI辅助同行评审的必要性，以应对当前评审体系因投稿量激增而面临的质量和效率问题。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习领域的顶级会议如NeurIPS、ICML和ICLR的手稿提交量呈指数级增长，超出了合格审稿人的有限能力，导致对审稿质量、一致性和审稿疲劳的担忧。

**方法:** 该论文提出了一种基于大型语言模型（LLMs）的AI增强生态系统，作为作者、审稿人和领域主席的复杂协作者，并讨论了AI在事实验证、审稿人表现指导、作者质量改进以及支持决策方面的具体角色。

**结果:** 提出了一个研究议程，包括示例性实验，以开发和验证这些AI助手，并讨论了重要的技术和伦理挑战。

**结论:** 论文得出结论，AI辅助的同行评审必须成为紧急的研究和基础设施优先事项，以解决当前机器学习领域同行评审中的规模危机。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+AI+Imperative%3A+Scaling+High-Quality+Peer+Review+in+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08134&send_immediately=true&force_search=false)

**原文摘要:** Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [131] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker, Pedro Cabalar, Martin Diéguez, Javier Romero, Susana Hahn, Torsten Schaub*

**主要类别:** cs.AI

**AI概要:** 论文介绍了一种用于度量ASP的新方法，解决时间约束下可扩展性的问题，通过引入差分约束实现时间外部化处理。


<details>
  <summary>更多</summary>
  
**动机:** 传统ASP在处理包含时间持续和截止等定量时间约束的问题时，由于细粒度时间约束会显著加剧其基础瓶颈，因此需要一种更具可扩展性的解决方案。

**方法:** 利用带有差分约束的ASP扩展来表达时间相关的定量约束，并将时间处理外部化以减少对时间精度的依赖。

**结果:** 新方法能够高效地处理时间相关约束，并且不受时间精度影响，从而保持了良好的可扩展性。

**结论:** 该论文提出了一种处理度量答案集编程（ASP）的新计算方法，通过将时间相关部分与时间粒度解耦，有效解决了细粒度时间约束带来的可扩展性问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Compiling+Metric+Temporal+Answer+Set+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08150，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08150&send_immediately=true&force_search=false)

**原文摘要:** We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [132] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/abs/2506.08306)
*Tuan Truong, Rithwik Sudharsan, Yibo Yang, Peter Xiangyuan Ma, Ruihan Yang, Stephan Mandt, Joshua S. Bloom*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种针对天体物理数据的神经压缩挑战赛，并展示了神经压缩技术在提升天文数据传输效率方面的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 由于天文观测站的远程物理位置导致数据传输能力受限，改进无损数据压缩技术有潜力显著增加科学数据的获取量。

**方法:** 论文通过引入AstroCompress：一个用于天体物理数据的神经压缩挑战赛，提供了四个新数据集和一个遗留数据集，并评估了七种无损压缩方法的效果。

**结果:** 研究结果表明，无损神经压缩技术相较于传统方法具有优势，并能有效利用天文图像的空间、时间和波长结构特征。

**结论:** 论文得出结论，无损神经压缩技术可以增强天文台的数据收集能力，并为科学应用中神经压缩的采用提供指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AstroCompress%3A+A+benchmark+dataset+for+multi-purpose+compression+of+astronomical+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08306&send_immediately=true&force_search=false)

**原文摘要:** The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [133] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel, Rayna Bhattacharyya, Thomas Lu, Arnav Mehta, Niels Voss, Narges Norouzi, Gireeja Ranade*

**主要类别:** cs.AI

**AI概要:** LeanTutor是一个基于大型语言模型的用于数学证明教学的辅导系统，它能够将学生的自然语言证明转化为形式化语言，验证准确性，并提供有效的学习反馈。


<details>
  <summary>更多</summary>
  
**动机:** 为了帮助学生在学习数学证明时获得即时且准确的指导，研究者开发了一个基于大型语言模型的辅导系统LeanTutor。

**方法:** 该研究提出了一个包含自动形式化/验证器、下一步生成器和自然语言反馈生成器三个模块的系统，利用大型语言模型（LLM）进行候选生成和搜索以完成任务。

**结果:** 实验表明，LeanTutor的自动形式化模块能正确形式化57%的正确证明中的步骤，并在30%的错误证明中识别出错误步骤；此外，在生成自然语言提示方面，其性能优于简单基线模型。

**结论:** LeanTutor能够通过模块化设计，有效地将学生自然语言的数学证明转换为Lean形式化语言，并提供有效的反馈和下一步建议。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LeanTutor%3A+A+Formally-Verified+AI+Tutor+for+Mathematical+Proofs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08321，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08321&send_immediately=true&force_search=false)

**原文摘要:** We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [134] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)
*Amur Ghose, Andrew B. Kahng, Sayak Kundu, Zhiang Wang*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种基于大语言模型的迭代优化代理ORFS-agent，用于自动化硬件设计流程中的参数调优。


<details>
  <summary>更多</summary>
  
**动机:** 现代集成电路设计流程涉及大量参数配置，小的参数变化会对设计性能、功耗和面积产生重大影响，而近期大型语言模型（LLMs）的发展为这种高维优化任务提供了新机会。

**方法:** 引入了ORFS-agent，这是一个基于LLM的迭代优化代理，它通过自然语言目标来权衡某些指标以实现多目标优化，并自适应地探索参数配置。

**结果:** 实验表明，ORFS-agent在资源效率和最终设计指标方面优于标准贝叶斯优化方法，在两个不同的技术节点和一系列电路基准测试中，ORFS-agent在路由线长和有效时钟周期上均提升了超过13%，同时优化迭代次数减少了40%。

**结论:** ORFS-agent是一个模块化且与模型无关的优化代理，可以插入任何前沿的LLM而无需进一步微调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ORFS-agent%3A+Tool-Using+Agents+for+Chip+Design+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08332&send_immediately=true&force_search=false)

**原文摘要:** Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [135] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/abs/2506.08363)
*Jun Yin, Jing Zhong, Pengyu Zeng, Peilin Li, Miao Zhang, Ran Luo, Shuai Lu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为FloorplanMAE的自监督学习框架，用于从不完整的平面图中重建完整的建筑平面图。


<details>
  <summary>更多</summary>
  
**动机:** 建筑设计师在设计过程中需要反复修改平面图，能够根据部分平面图预测完整的设计可以提高效率并减少重复工作。

**方法:** 作者提出了FloorplanMAE模型，基于Masked Autoencoders（MAE）和轻量级Vision Transformer（ViT），通过遮蔽平面图的部分区域进行训练，并创建了专门的FloorplanNet数据集。

**结果:** 实验表明，FloorplanMAE能够高效地生成高质量的完整平面图，并与现有先进方法进行了比较验证，同时使用真实建筑设计草图进行了测试。

**结论:** FloorplanMAE提供了一个可扩展的解决方案，具有广泛的应用前景，特别是在辅助建筑设计领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FloorplanMAE%3AA+self-supervised+framework+for+complete+floorplan+generation+from+partial+inputs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08363，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08363&send_immediately=true&force_search=false)

**原文摘要:** In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [136] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)
*Leheng Sheng, An Zhang, Zijian Wu, Weixiang Zhao, Changshuo Shen, Yi Zhang, Xiang Wang, Tat-Seng Chua*

**主要类别:** cs.AI

**AI概要:** 本文揭示了大型推理模型如何通过预规划的激活状态和方向向量来调控推理强度，从而影响推理长度和任务表现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已观察到大型推理模型能根据问题难度自动分配推理资源，但其背后机制尚不明确，因此需要深入研究这一现象的根源。

**方法:** 从模型激活的角度出发，使用线性探测方法分析问题激活与推理令牌数量之间的关系，并探索方向向量对推理长度和性能的影响。

**结果:** 研究发现模型在生成前就已通过激活状态预判推理强度，且该强度受一个可调节的方向向量控制，干预该向量可以显著改变推理行为和性能。

**结论:** 研究揭示了大型推理模型在处理问题时能够预先规划推理强度，并通过特定的方向向量控制推理行为，为理解和控制模型的推理机制提供了新方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Reasoning+Strength+Planning+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08390&send_immediately=true&force_search=false)

**原文摘要:** Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [137] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma, Zhanhui Zhou, Chao Yang, Chaochao Lu*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种名为SafeCoT的新方法，通过使用基于规则的思维链监督来改进视觉-语言模型在处理安全敏感任务时的行为。


<details>
  <summary>更多</summary>
  
**动机:** 确保视觉-语言模型（VLMs）产生安全和适当的响应仍然是一个关键挑战，尤其是在高风险或模糊场景中。

**方法:** SafeCoT是一个轻量级、可解释的框架，利用基于规则的思维链（CoT）监督来改善VLMs的拒绝行为。

**结果:** 实验结果表明，即使训练数据有限，SafeCoT也能显著减少过度拒绝并增强泛化能力。

**结论:** SafeCoT提供了一种可扩展的解决方案，用于使视觉-语言模型与安全关键目标保持一致。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SafeCoT%3A+Improving+VLM+Safety+with+Minimal+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08399&send_immediately=true&force_search=false)

**原文摘要:** Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [138] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/abs/2506.08401)
*Runze Li, Di Jin, Xiaobao Wang, Dongxiao He, Bingdao Feng, Zhen Wang*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种新的图推荐系统攻击方式，通过插入一个虚假用户节点来提高特定目标项目的可见性，同时尽量减少对整个系统的影响。


<details>
  <summary>更多</summary>
  
**动机:** 由于现有的shilling攻击方法存在低隐蔽性和高破坏性的挑战，因此需要提出一种更有效的图推荐系统的攻击策略。

**方法:** 设计了一个单节点触发生成器，并在目标节点与无关节点之间引入约束条件，以减少假节点对推荐系统性能的影响。

**结果:** 实验结果表明，99%的目标用户中，目标项目的曝光率不低于50%，而对推荐系统性能的影响控制在约5%以内。

**结论:** 本文提出了一种新颖的图后门攻击方法，该方法能够在不影响其他无关节点的情况下，以隐蔽的方式增加目标用户对目标项目的曝光度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Single-Node+Trigger+Backdoor+Attacks+in+Graph-Based+Recommendation+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08401，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08401&send_immediately=true&force_search=false)

**原文摘要:** Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [139] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)
*Ikkei Itoku, David Theil, Evelyn Eichelsdoerfer Uehara, Sreyoshi Bhaduri, Junnosuke Kuroda, Toshi Yumoto, Alex Gil, Natalie Perez, Rajesh Cherukuri, Naumaan Nayyar*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的自动化分类对齐框架，结合了大型语言模型与专家校准及迭代提示优化，在性能上显著超越人工基准。


<details>
  <summary>更多</summary>
  
**动机:** 传统手动分类对齐方法依赖专家评审，但在规模上变得昂贵且耗时，而现有自动化方法在处理细微语义关系和保持跨领域一致性方面面临局限性。

**方法:** 整合专家标记的示例、多阶段提示工程和人工验证来指导大型语言模型生成分类链接和支持性原理。

**结果:** 在评估该框架在概念必要性领域的特定任务时，达到了0.97的F1分数，远超0.68的人工基准。

**结论:** 结合大型语言模型与专家校准及迭代提示优化的新型框架在自动化分类对齐方面表现出色，显著优于人工基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transforming+Expert+Knowledge+into+Scalable+Ontology+via+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08422，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08422&send_immediately=true&force_search=false)

**原文摘要:** Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [140] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/abs/2506.08424)
*Yong Liang Goh, Zhiguang Cao, Yining Ma, Jianan Zhou, Mohammad Haroon Dupty, Wee Sun Lee*

**主要类别:** cs.AI

**AI概要:** 本文提出了SHIELD模型，通过结合稀疏性和层次结构设计，有效解决了多任务多分布车辆路径问题，展现出良好的泛化性能和效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的路由问题基础模型忽略了复杂的真实客户分布，因此需要一种更现实且具有挑战性的设置来提升模型的适应性和泛化能力。

**方法:** 作者基于更深层次的解码器架构，引入了Mixture-of-Depths（MoD）技术以实现稀疏性，并开发了一种基于上下文的聚类层来利用问题中的层次结构。

**结果:** 实验结果表明，SHIELD在9个真实地图和每个地图上的16种VRP变体中均优于现有方法。

**结论:** 论文得出结论，SHIELD模型通过利用稀疏性和层次性原则，在处理多任务多分布车辆路径问题（MTMDVRP）时表现出优越的泛化能力和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SHIELD%3A+Multi-task+Multi-distribution+Vehicle+Routing+Solver+with+Sparsity+and+Hierarchy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08424，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08424&send_immediately=true&force_search=false)

**原文摘要:** Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [141] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, Yang Yu*

**主要类别:** cs.AI

**AI概要:** 这篇论文综述了大型语言模型在数学推理能力方面的发展，探讨了提升推理能力的方法，并指出了未来的研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 数学推理一直是人工智能研究中最基础且具有挑战性的前沿之一，近年来大型语言模型在此领域取得了显著进展，因此有必要对其进行系统回顾与总结。

**方法:** 本文采用了综述的方式，对大型语言模型在数学推理方面的进展进行了系统的分析与总结。

**结果:** 文章回顾了从预训练策略到生成答案的各个阶段的发展，讨论了从无训练提示到微调方法等多种提升数学推理能力的技术，并指出了当前仍存在的基本挑战。

**结论:** 本文总结了大型语言模型在数学推理领域的发展，并提出了未来研究的方向，包括更先进的预训练和知识增强技术、形式化推理框架以及通过原则性学习范式实现的元泛化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+on+Large+Language+Models+for+Mathematical+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08446，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08446&send_immediately=true&force_search=false)

**原文摘要:** Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [142] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji, Sebastian W. Pattinson*

**主要类别:** cs.AI

**AI概要:** 提出CIPHER框架，实现工业控制中具备人类推理能力的自主系统，无需大量标注数据即可执行复杂任务。


<details>
  <summary>更多</summary>
  
**动机:** 传统AI控制系统依赖大量标记数据，难以适应复杂多变且数据稀缺的工业环境，而通用模型又无法满足工程应用对定量精度的要求。

**方法:** 引入Control and Interpretation of Production via Hybrid Expertise and Reasoning (CIPHER)框架，采用视觉-语言-动作(VLA)模型，结合过程专家知识、回归模型与检索增强生成技术。

**结果:** CIPHER展现出强大的跨任务泛化能力，能够解释决策并自动生成精确的机器指令，支持透明通信和物理推理。

**结论:** CIPHER通过结合过程专家、回归模型和检索增强生成技术，实现了在工业控制中具有强泛化能力的自主系统，为工业环境中安全可靠的部署提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Reasoning+for+Perception%2C+Explanation%2C+and+Autonomous+Action+in+Manufacturing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08462，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08462&send_immediately=true&force_search=false)

**原文摘要:** Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [143] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)
*Rahatara Ferdousi, M Anwar Hossain*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了Responsible Health Twin (RHealthTwin) 框架，该框架通过Responsible Prompt Engine (RPE) 动态提取预定义槽位来结构化输入，解决传统LLM配置的风险问题，并在多个消费者健康领域取得优秀表现，同时强调了在健康与福祉领域负责任地使用LLM的应用基础。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）在医疗保健数字孪生中的应用带来了新的可能性，但在消费者健康环境中部署这些系统存在诸如幻觉、偏见、缺乏透明度和伦理滥用等重大问题。因此需要一种有原则的框架来构建和管理AI驱动的数字孪生以提供健康辅助服务。

**方法:** 提出Responsible Health Twin (RHealthTwin) 框架，并通过多模态输入引导专注于健康的LLM生成安全、相关和可解释的响应，核心是Responsible Prompt Engine (RPE)，其通过预定义槽位动态提取来结构化输入，从而指导语言模型生成响应。

**结果:** 在四个消费者健康领域中对RHealthTwin进行了评估，包括心理健康支持、症状分诊、营养规划和活动指导。RPE在基准数据集中取得了BLEU = 0.41, ROUGE-L = 0.63, BERTScore = 0.89的成果，并且在伦理合规性和指令遵循指标上使用LLM作为评判达到了超过90%的表现，优于基线策略。

**结论:** RHealthTwin框架为基于LLM的健康和福祉应用提供了负责任的基础，展示了在消费者健康领域的广泛应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RHealthTwin%3A+Towards+Responsible+and+Multimodal+Digital+Twins+for+Personalized+Well-being，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08486，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08486&send_immediately=true&force_search=false)

**原文摘要:** The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [144] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/abs/2506.08747)
*Boyang Sun, Yu Yao, Xinshuai Dong, Zongfang Liu, Tongliang Liu, Yumou Qiu, Kun Zhang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种高效的条件独立性检验方法，该方法不需要对观测数据进行二值化处理，从而保留了更多信息并提高了测试性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于测量限制，在许多现实场景中感兴趣的变量通常被表示为离散值。直接对这些离散数据应用条件独立性（CI）测试可能会导致错误的结论。因此需要一种新的条件独立性检验方法。

**方法:** 通过解决广义矩方法（GMM）中的过度识别限制问题，建立潜在连续变量的独立性关系，并利用节点回归推导出适当的检验统计量并建立其渐近分布。

**结果:** 理论发现和跨各种数据集的实证结果表明所提测试方法的优越性和有效性。

**结论:** 本文提出的样本高效条件独立性检验方法避免了信息损失，提高了测试性能，适用于多种实际场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Sample+Efficient+Conditional+Independence+Test+in+the+Presence+of+Discretization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08747，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08747&send_immediately=true&force_search=false)

**原文摘要:** In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [145] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta, Nikita Jangid, Shounak Das, Amit Sethi*

**主要类别:** cs.AI

**AI概要:** FedTAIL通过梯度一致性和锐度最小化等技术，在解决域泛化问题上取得了优异表现。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决现有方法在长尾类别分布和优化目标冲突下的不足，提出了一种新的联邦域泛化框架。

**方法:** FedTAIL引入了梯度一致性正则化、类别级锐度最小化和曲率感知动态加权方案，并结合熵正则化进行条件分布对齐。

**结果:** 实验表明FedTAIL在标准域泛化基准上达到了最先进的性能，特别是在存在域转移和标签不平衡的情况下。

**结论:** FedTAIL是一种联邦域泛化框架，解决了类别不平衡和优化目标冲突的问题，在集中式和联邦设置中都表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FEDTAIL%3A+Federated+Long-Tailed+Domain+Generalization+with+Sharpness-Guided+Gradient+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08518&send_immediately=true&force_search=false)

**原文摘要:** Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [146] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)
*Yanwei Gong, Xiaolin Chang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种结合深度强化学习和大语言模型推理的无人机轨迹规划框架，以解决低空经济背景下复杂城市环境中无人机路径规划的安全性、合规性和经济性问题，并通过实验验证了该方法的有效性和优越性。


<details>
  <summary>更多</summary>
  
**动机:** 随着低空经济的快速发展，无人机（UAV）得到了广泛应用，但其在复杂城市环境中的轨迹规划面临新挑战。现有研究通常忽略了城市空域限制和经济效益等关键因素，而深度强化学习虽然被视为有前景的解决方案，但其实用化仍受限于较低的学习效率。

**方法:** 论文提出了一种新的无人机轨迹规划框架，将深度强化学习（DRL）与大语言模型（LLM）推理相结合，旨在实现安全、合规且经济可行的路径规划。

**结果:** 实验结果表明，所提出的方法在数据采集率、避障、成功着陆、法规遵从性和能效等多个指标上均显著优于现有基线方法。

**结论:** 论文得出结论，提出的结合深度强化学习和大语言模型推理的无人机轨迹规划框架在多个指标上显著优于现有基线方法，有效应对了低空经济网络中的关键挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safe+and+Economical+UAV+Trajectory+Planning+in+Low-Altitude+Airspace%3A+A+Hybrid+DRL-LLM+Approach+with+Compliance+Awareness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08532&send_immediately=true&force_search=false)

**原文摘要:** The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [147] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/abs/2506.08580)
*Yang Lv, Jinlong Lei, Peng Yi*

**主要类别:** cs.AI

**AI概要:** 本文提出HGformer框架，通过图Transformer与分层决策模型优化大规模对抗环境下的资源分配策略，解决了两阶段Colonel Blotto游戏中的复杂决策问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法难以解决两阶段Colonel Blotto游戏中由于阶段间依赖性和图拓扑复杂约束导致的全局最优策略获取问题。

**方法:** 提出了一种层次化图Transformer框架HGformer，包括增强的图Transformer编码器、双智能体层次决策模型以及逐层反馈的强化学习算法。

**结果:** 实验结果表明，HGformer在资源分配效率和对抗收益方面表现优越，提升了大规模对抗环境中的策略生成效率，并有效协调了两个决策阶段。

**结论:** HGformer在复杂的动态博弈场景中显著提高了资源分配效率和对抗收益，优于现有的分层决策或图神经网络方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HGFormer%3A+A+Hierarchical+Graph+Transformer+Framework+for+Two-Stage+Colonel+Blotto+Games+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08580，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08580&send_immediately=true&force_search=false)

**原文摘要:** Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [148] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/abs/2506.08627)
*Douwe Geurtjens, Xixi Lu*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一种名为FoldA的新技术，通过使用有向Petri网展开进行实时部分有序对齐计算，解决了现有过程挖掘中一致性检查的状态爆炸和并发行为表示不足的问题。尽管FoldA需要更多的计算时间，但它能更准确地表示并发行为并减少状态数量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的对齐方法由于探索状态空间会引发状态爆炸问题，同时由于对齐本质上是顺序结构，因此无法完全表示现实世界流程中的并发行为。

**方法:** 提出了一种新的技术FoldA，用于实时计算基于有向Petri网展开的部分有序对齐。

**结果:** 在485个合成模型日志对上进行了评估，并与Astar和Dijkstra对齐在13个实际模型日志对和6个基准对上进行了比较，结果表明FoldA能够减少排队状态数量并提供更准确的并发表示。

**结论:** 论文得出结论，FoldA虽然需要更多的计算时间，但通常减少了排队状态的数量，并提供了对并发性更准确的表示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FoldA%3A+Computing+Partial-Order+Alignments+Using+Directed+Net+Unfoldings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08627，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08627&send_immediately=true&force_search=false)

**原文摘要:** Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [149] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/abs/2506.08630)
*Laurens Engwegen, Daan Brinks, Wendelin Böhmer*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种适用于任何机器人形态的通用控制器设计方法，通过模块化循环架构提升了对未见机器人的泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 通用控制器能够显著提高计算和数据效率，但对新形态机器人的泛化仍是一个挑战，因此需要探索新的方法。

**方法:** 实现了一种模块化循环架构，并在MuJoCo环境中测试其对不同动力学、运动学和拓扑结构的机器人泛化能力。

**结果:** 结果表明该方法在四个不同环境中对未见过的机器人形态表现出显著改进的性能。

**结论:** 论文得出结论，通过利用机器人属性的上下文信息并采用模块化循环架构，可以有效提升对未见机器人的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modular+Recurrence+in+Contextual+MDPs+for+Universal+Morphology+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08630&send_immediately=true&force_search=false)

**原文摘要:** A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [150] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao*

**主要类别:** cs.AI

**AI概要:** 本研究提出 CoVo，一种新的自我奖励强化学习框架，通过利用不同推理路径间中间状态的一致性来增强大语言模型的推理能力，且无需外部监督。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习方法在复杂推理任务中通常依赖于外部监督，限制了其广泛应用。

**方法:** 引入了基于一致性与波动性的内在奖励机制 CoVo，并结合好奇心奖励以促进多样化的探索。

**结果:** 实验表明 CoVo 在多种推理基准上的表现可媲美甚至超越有监督的强化学习方法。

**结论:** CoVo 提供了一种无需外部监督的强化学习方法，使大型语言模型能够通过自我奖励机制进行推理学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Consistent+Paths+Lead+to+Truth%3A+Self-Rewarding+Reinforcement+Learning+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08745&send_immediately=true&force_search=false)

**原文摘要:** Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [151] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti, Michael Färber*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种结合知识图谱和大语言模型的新方法，显著提升了因果关系推理的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于大语言模型的方法在推断变量对之间的因果关系时结果不稳定且不一致，影响了其可靠性，因此需要一种更有效的方法。

**方法:** 该方法首先在知识图谱中识别信息量大的元路径子图，并使用基于排序学习的模型进一步优化这些子图的选择；然后将排名最高的子图纳入零样本提示，以增强LLMs推断因果关系的能力。

**结果:** 在生物医学和开放领域的数据集上进行的广泛实验表明，该方法在F1分数上比大多数基线方法高出44.4分。

**结论:** 论文提出了一种新的方法，通过整合知识图谱（KGs）和大语言模型（LLMs），提高了基于知识的因果发现的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Paths+to+Causality%3A+Finding+Informative+Subgraphs+Within+Knowledge+Graphs+for+Knowledge-Based+Causal+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08771&send_immediately=true&force_search=false)

**原文摘要:** Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [152] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini, José Hernández-Orallo, Lorenzo Pacchiardi*

**主要类别:** cs.AI

**AI概要:** 该论文综述了大型语言模型在数据科学中的应用评估，揭示了当前研究在活动范围、协作模式和任务转型方面的不足。


<details>
  <summary>更多</summary>
  
**动机:** 数据科学旨在从数据中提取洞察以支持决策过程，而近年来大型语言模型（LLMs）被广泛应用于这一领域。然而，对于这些模型在数据科学中自动化能力的评估尚不充分，因此需要系统性地审视其潜力与局限。

**方法:** 本文采用调查研究的方法，分析了当前关于大型语言模型作为数据科学助手和代理的应用现状。

**结果:** 研究发现，现有评估存在以下问题：1）主要集中在一小部分目标导向活动中，忽略了数据管理和探索性活动；2）只关注纯辅助角色或全自主代理，未考虑人机协作的不同层次；3）强调取代人类工作，而未能探索任务转型带来的更高自动化可能性。

**结论:** 论文总结指出当前对数据科学中大型语言模型（LLMs）助手和代理的评估存在三个主要问题：对目标导向活动的小范围关注、仅考虑纯辅助或完全自主代理而忽略中间的人机协作水平，以及强调人类替代而忽视通过任务转换实现更高自动化水平的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Data+Science+Automation%3A+A+Survey+of+Evaluation+Tools+for+AI+Assistants+and+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08800，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08800&send_immediately=true&force_search=false)

**原文摘要:** Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [153] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)
*Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, Pattie Maes*

**主要类别:** cs.AI

**AI概要:** 该论文研究了大型语言模型（LLM）在辅助写作时对认知和行为的影响，发现LLM使用虽然提供便利，但可能导致大脑活动减弱和自我创作归属感降低。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探讨大型语言模型（LLM）辅助写作对神经和行为的影响，以及与其他工具（如搜索引擎）或无工具条件下的写作差异。

**方法:** 将参与者分为三组（LLM、搜索引擎和仅大脑），每组完成三个相同条件的会话。在第四次会话中，LLM用户转为Brain-only组，而Brain-only用户转为LLM条件。研究使用脑电图（EEG）评估写作过程中的认知负荷，并利用自然语言处理（NLP）分析文章内容，同时由教师和AI裁判评分。

**结果:** 1. EEG结果显示，不同组别的大脑连接性存在显著差异：仅大脑组表现出最强且分布最广的网络，搜索引擎用户显示中等参与度，而LLM用户的大脑连接性最弱。
2. 第四阶段中，从LLM转向仅大脑的参与者显示出较低的α和β波连接性，表明参与度不足；而从仅大脑转向LLM的用户则表现出更高的记忆召回和顶枕部及前额叶区域的激活。
3. 自我报告的文章归属感最低出现在LLM组，最高出现在仅大脑组。
4. LLM用户在准确引用自己的作品方面也遇到困难。
5. 在四个月内，LLM用户在神经、语言学和行为层面上的表现始终较差。

**结论:** 研究表明，尽管LLM在写作中提供了即时的便利，但其使用可能导致认知成本，并引发了关于LLM依赖对长期教育影响的担忧。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Your+Brain+on+ChatGPT%3A+Accumulation+of+Cognitive+Debt+when+Using+an+AI+Assistant+for+Essay+Writing+Task，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08872，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08872&send_immediately=true&force_search=false)

**原文摘要:** This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [154] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/abs/2506.08898)
*Mingfeng Fan, Jianan Zhou, Yifeng Zhang, Yaoxin Wu, Jinbiao Chen, Guillaume Adrien Sartoretti*

**主要类别:** cs.AI

**AI概要:** 本文提出了POCCO框架，通过自适应选择模型结构和偏好驱动优化算法，在解决多目标组合优化问题上取得了显著成效。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度强化学习方法通常平等地对待所有子问题并使用单一模型解决，这限制了对解空间的有效探索，导致性能不理想。

**方法:** 设计了一个条件计算模块来将子问题路由到专门的神经架构，并提出了一种基于偏好信号而非显式奖励值的偏好驱动优化算法。

**结果:** 实验结果表明，POCCO在四个经典的MOCOP基准测试中表现出显著的优势和强大的泛化能力。

**结论:** 论文提出了一种新框架POCCO，用于解决多目标组合优化问题，并证明其在多个基准测试中的优越性和广泛应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preference-Driven+Multi-Objective+Combinatorial+Optimization+with+Conditional+Computation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08898&send_immediately=true&force_search=false)

**原文摘要:** Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [155] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan, Rahul Sengupta, Anand Rangarajan, Sanjay Ranka*

**主要类别:** cs.AI

**AI概要:** 本文提出一种结合交通信号信息的多头自注意力轨迹预测模型，用于提升数据驱动交通模拟的真实性与效率，并在仿真环境中进行了验证。


<details>
  <summary>更多</summary>
  
**动机:** 传统的基于规则的交通模拟器难以真实还原实际驾驶行为，而交通交叉口是事故高发区域并影响道路整体运行效率，因此需要一种更精确的数据驱动模拟方法。

**方法:** 提出了一种基于多头自注意力机制的轨迹预测模型，并结合交通信号信息进行优化，同时设计了针对交通工程的评估指标和仿真测试流程。

**结果:** 所提出的模型在评估指标上优于之前的模型，并成功集成到微仿真环境中进行实时测试，验证了其在模仿宏观与微观驾驶行为上的有效性。

**结论:** 研究得出，通过使用多头自注意力机制结合信号信息的生成轨迹预测模型，可以有效提高交通交叉口驾驶行为模拟的真实性与效率，并为未来数据驱动的交通仿真提供了新方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IntTrajSim%3A+Trajectory+Prediction+for+Simulating+Multi-Vehicle+driving+at+Signalized+Intersections，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08957，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08957&send_immediately=true&force_search=false)

**原文摘要:** Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [156] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/abs/2506.08963)
*Yash Ranjan, Rahul Sengupta, Anand Rangarajan, Sanjay Ranka*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种全面的分析工具，用于从交通工程角度更好地理解交通动态模型的表现。


<details>
  <summary>更多</summary>
  
**动机:** 目前，交通信号交叉口的深度生成模型主要基于轨迹重建误差等计算指标进行评估，缺乏在“实时”微观仿真场景中的验证。并且现有指标未能充分考虑交通工程方面的关注点，如闯红灯、非法停车等。

**方法:** 该论文训练了一个最先进的多车辆轨迹预测模型，并在一个通过校准的真实城市交叉口场景的大数据集上运行。接着，在微观模拟器中对预测模型的性能进行在线评估。

**结果:** 研究结果显示，即便在输入理想行为轨迹且轨迹重建误差较低的情况下，生成的轨迹仍会出现违反交通规则的情况。为解决此问题，作者引入了新的评估指标。

**结论:** 论文得出结论，尽管使用了理想行为轨迹作为输入并实现了低轨迹重建误差，生成的轨迹仍然表现出违反交通规则的行为，并引入了新的指标来评估这些不良行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Generative+Vehicle+Trajectory+Models+for+Traffic+Intersection+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08963，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08963&send_immediately=true&force_search=false)

**原文摘要:** Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [157] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/abs/2506.08970)
*Jiyao Wei, Saiping Guan, Da Li, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng*

**主要类别:** cs.AI

**AI概要:** 本文首次系统性地综述了N-ary知识图谱中的链接预测方法，对其进行了分类与评估，并指出未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** N-ary知识图谱（NKGs）能够高效表示包含多个实体的复杂现实世界事实，而传统知识图谱只能表示两个实体之间的关系。因此，对NKGs中的链接预测进行系统性研究对于完善NKGs并提升下游应用性能具有重要意义。

**方法:** 对现有的NKGs中的链接预测方法进行了系统分类，并分析了它们的性能和应用场景。

**结果:** 本文提供了NKGs中链接预测任务的全面综述，包括方法分类、性能分析以及应用前景探讨。

**结论:** 本文提出了首个关于NKGs中链接预测的全面调查，概述了该领域的研究进展，并为未来的研究指明了方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+Link+Prediction+in+N-ary+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08970&send_immediately=true&force_search=false)

**原文摘要:** N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [158] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)
*Polina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri, Samuel J. Bell*

**主要类别:** cs.AI

**AI概要:** 研究介绍了一个名为AbstentionBench的基准，用于评估大型语言模型在面对无法回答的问题时的表现，并发现现有模型在这方面仍存在显著问题。


<details>
  <summary>更多</summary>
  
**动机:** 为了可靠地部署大型语言模型，了解何时不回答同样重要。然而，目前缺乏一个系统性的框架来评估现代大型语言模型的 abstention 能力。

**方法:** 引入了AbstentionBench，这是一个大规模基准，涵盖20个多样化数据集，用于全面评估 abstention 表现。研究团队评估了20个前沿的LLM，并分析了其 abstention 表现。

**结果:** 研究发现，abstention 是一个尚未解决的问题，且模型规模的扩大对此帮助不大。此外，尽管最近的推理模型在复杂问题解决方面表现出色，但它们在 abstention 上的表现反而下降了。

**结论:** 当前的大型语言模型在 abstention 方面仍面临挑战，需要进一步的研究来提升模型对不确定性进行推理的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AbstentionBench%3A+Reasoning+LLMs+Fail+on+Unanswerable+Questions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09038&send_immediately=true&force_search=false)

**原文摘要:** For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [159] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin*

**主要类别:** cs.AI

**AI概要:** 本文提出了VIKI-Bench和VIKI-R，用于评估和提升多智能体在动态环境中的视觉驱动协作能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于VLM的方法在支持多样化的具身类型方面仍有限，需要更有效的多智能体合作策略评估与训练框架。

**方法:** 提出了VIKI-Bench这一分层基准测试，并开发了VIKI-R框架，通过两阶段微调VLM并结合强化学习进行任务优化。

**结果:** VIKI-R在所有任务层级上显著优于基线方法，且强化学习促使异构智能体间产生复杂的协作模式。

**结论:** VIKI-Bench和VIKI-R为基于视觉的多智能体协作提供了统一的测试平台和方法，推动了具身AI系统的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VIKI-R%3A+Coordinating+Embodied+Multi-Agent+Cooperation+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09049&send_immediately=true&force_search=false)

**原文摘要:** Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [160] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/abs/2506.09050)
*Yuki Imajuku, Kohki Horie, Yoichi Iwata, Kensho Aoki, Naohiro Takahashi, Takuya Akiba*

**主要类别:** cs.AI

**AI概要:** 本文介绍了ALE-Bench，一个用于评估AI系统在基于评分的算法编程竞赛中的表现的新基准。


<details>
  <summary>更多</summary>
  
**动机:** 需要评估AI系统在解决硬优化问题上的表现，并推动AI技术的发展。

**方法:** 通过从AtCoder启发式竞赛中提取真实任务，开发了支持交互式代理架构的软件框架，并进行了前沿LLM的评估。

**结果:** 尽管前沿LLM在特定问题上表现出色，但在跨问题一致性和长视野问题解决能力方面与人类相比仍有显著差距。

**结论:** ALE-Bench提供了一个有价值的工具，以促进未来AI系统的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ALE-Bench%3A+A+Benchmark+for+Long-Horizon+Objective-Driven+Algorithm+Engineering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09050&send_immediately=true&force_search=false)

**原文摘要:** How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [161] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu, Weijia Zhang, Hao Liu*

**主要类别:** stat.ML

**AI概要:** TelePiT 是一种用于次季节至季节 (S2S) 气候预测的新型深度学习架构，通过整合多尺度物理过程和遥相关模式显著提高了预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的 S2S 预测方法未能显式建模对这些时间尺度至关重要的潜在物理过程和遥相关关系，因此需要开发更准确和可靠的预测模型。

**方法:** 引入 TelePiT 模型，包含三个关键组件：(1) 球面调和嵌入；(2) 多尺度物理信息神经微分方程；(3) 遥相关感知变压器，以增强全球 S2S 预测能力。

**结果:** 实验表明，TelePiT 显著优于现有数据驱动基线和运行中的数值天气预报系统，在多个大气变量上取得了显著改进，例如相比之前最佳模型，2 米温度的 RMSE 降低了 57.7%。

**结论:** TelePiT 提供了一种创新的方法来改进全球 S2S 气候预测，为未来的气候预测研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Informed+Teleconnection-Aware+Transformer+for+Global+Subseasonal-to-Seasonal+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08049&send_immediately=true&force_search=false)

**原文摘要:** Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [162] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin, Evgenia Romanenkova, Alexey Zaytsev*

**主要类别:** stat.ML

**AI概要:** 本文研究了高维数据流中的变点检测问题，提出了一种基于Wasserstein距离的集成聚合方法WWAggr，并解决了变点检测中阈值选择的问题。


<details>
  <summary>更多</summary>
  
**动机:** 高维数据流中变点检测具有挑战性，传统方法和独立深度神经网络未能达到理想效果，而集成方法虽然性能更强，但其预测聚合技术仍有不足。

**方法:** 提出了WWAggr方法，该方法利用Wasserstein距离进行集成聚合，适用于各种深度变点检测模型，并解决了变点检测中长期存在的决策阈值选择问题。

**结果:** WWAggr方法在多个实验中表现出色，比现有的集成聚合方法更优，并有效解决了阈值选择问题。

**结论:** WWAggr为高维数据流的变点检测提供了更加鲁棒和有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WWAggr%3A+A+Window+Wasserstein-based+Aggregation+for+Ensemble+Change+Point+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08066，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08066&send_immediately=true&force_search=false)

**原文摘要:** Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [163] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone, Emilie Kaufmann, Laura Richert*

**主要类别:** stat.ML

**AI概要:** 本文研究了在多变量老虎机设置中，在满足线性可行性约束的情况下识别Pareto集的问题，并提出了一个高效的算法，其性能优于现有方法且理论分析表明其样本复杂性接近最优。


<details>
  <summary>更多</summary>
  
**动机:** 传统的多臂老虎机问题通常关注单一目标优化，但在实际应用中往往需要考虑多个目标以及一些已知的可行性约束。因此，作者希望能够在满足这些约束的同时高效地识别出非支配解集（即Pareto集）。

**方法:** 提出了一种新算法，用于在多臂老虎机中识别受线性可行性约束的Pareto集，并与传统竞速类算法和直观的两阶段方法进行了比较。

**结果:** 新算法显著优于现有的竞速类算法和两阶段方法，并且理论上证明了任何受约束的Pareto集识别算法的样本复杂性的信息论下界，同时验证了所提算法的近最优性。

**结论:** 作者提出了一种新的算法，该算法在固定置信度下识别受约束的Pareto集，并证明了其样本复杂性接近最优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constrained+Pareto+Set+Identification+with+Bandit+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08127&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [164] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena, Rahul Ghosal, Pavlo Mozharovskyi, Oscar Hernan Madrid Padilla, Jukka-Pekka Onnela*

**主要类别:** stat.ML

**AI概要:** 本论文提出一种新的模型无关的不确定性量化方法，用于构建预测区域，并通过理论分析与实际案例证明了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度度量具有良好的理论性质，但它们在回归建模中的整合以提供预测区域的研究仍较为不足，因此本文旨在填补这一空白。

**方法:** 提出了一种基于条件深度度量的不确定性量化算法，结合核均值嵌入和共形预测方法，在可分希尔伯特空间中构建预测区域。

**结果:** 新算法不仅提供了预测和容忍区域，还实现了更快的收敛速度，并通过广泛的模拟研究和实际应用验证了性能。

**结论:** 论文提出了基于条件深度度量的新算法，能够定义预测和容忍区域，并通过模拟研究和数字健康应用验证了其有效性和实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-Free+Kernel+Conformal+Depth+Measures+Algorithm+for+Uncertainty+Quantification+in+Regression+Models+in+Separable+Hilbert+Spaces，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08325，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08325&send_immediately=true&force_search=false)

**原文摘要:** Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [165] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala, Erwan Scornet, Charles Tillier, Olivier Wintenberger*

**主要类别:** stat.ML

**AI概要:** 本文研究了在不平衡数据上使用再平衡和去偏方法训练随机森林的效果，并从理论上证明了其优势。


<details>
  <summary>更多</summary>
  
**动机:** 许多分类任务涉及不平衡数据，这促使研究者研究如何通过创建再平衡数据集来改善分类器性能。

**方法:** 论文采用了理论分析的方法，并基于重要性抽样（IS）提出了称为IS-ICRF的去偏估计方法。

**结果:** 论文证明了中心极限定理（CLT），揭示了偏差的存在并通过适当技术去除偏差后可以获得更优的预测结果。

**结论:** 论文得出的结论是，在处理不平衡数据时，通过在再平衡数据集上训练并使用去偏估计方法，可以有效减少方差，从而优于直接使用原始数据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotic+Normality+of+Infinite+Centered+Random+Forests+-Application+to+Imbalanced+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08548，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08548&send_immediately=true&force_search=false)

**原文摘要:** Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [166] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan, Yu-Ching Shih, Dong Yang, Amol Salunkhe*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种无需真实标签的概念漂移检测算法，在计算受限情况下表现出色，并设计了一个新的框架来在无标签环境下进行漂移检测。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器学习模型在各个领域的广泛应用，确保其性能至关重要。尤其是在没有即时真实标签的场景下，如何控制误报并及时检测概念漂移成为一大挑战，这促使了对无标签设置下的概念漂移检测方法的研究。

**方法:** 该论文提出了一种灵活且高效的无标签概念漂移检测算法，利用经典统计过程控制进行准确的概念漂移检测，并引入了新的漂移检测框架以在给定先前检测结果的情况下建模无标签的漂移检测场景。

**结果:** 通过数值模拟验证了该方法的有效性，在计算资源受限的情况下，所提方法在统计能力上优于已有方法，并展示了其在新提出的漂移检测框架中的有效集成。

**结论:** 论文得出结论，在计算受限的情况下，所提出的无标签概念漂移检测算法比已知方法具有更好的统计能力，并且可以有效地集成到新的漂移检测框架中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flexible+and+Efficient+Drift+Detection+without+Labels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.08734，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.08734&send_immediately=true&force_search=false)

**原文摘要:** Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>
