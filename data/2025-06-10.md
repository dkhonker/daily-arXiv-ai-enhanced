<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 128]
- [cs.AI](#cs.AI) [总数: 22]
- [stat.ML](#stat.ML) [总数: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Mixture-of-Experts Meets In-Context Reinforcement Learning](https://arxiv.org/abs/2506.05426)
*Wenhao Wu, Fuhong Liu, Haoru Li, Zican Hu, Daoyi Dong, Chunlin Chen, Zhi Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了T2MIR，一种结合混合专家模型的上下文强化学习框架，有效处理多模态数据与多样化任务，提高了学习效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决上下文强化学习中的多模态数据和多样化任务带来的挑战。

**方法:** 提出了一种新的框架T2MIR，结合了token-wise和task-wise的MoE结构，并使用对比学习方法优化任务路由。

**结果:** 实验结果表明，T2MIR在提升上下文学习能力方面显著优于现有方法。

**结论:** T2MIR通过引入混合专家模型，提升了上下文强化学习的适应能力和性能，在多个基准测试中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixture-of-Experts+Meets+In-Context+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05426&send_immediately=true&force_search=false)

**原文摘要:** In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose \textbf{T2MIR} (\textbf{T}oken- and \textbf{T}ask-wise
\textbf{M}oE for \textbf{I}n-context \textbf{R}L), an innovative framework that
introduces architectural advances of mixture-of-experts (MoE) into
transformer-based decision models. T2MIR substitutes the feedforward layer with
two parallel layers: a token-wise MoE that captures distinct semantics of input
tokens across multiple modalities, and a task-wise MoE that routes diverse
tasks to specialized experts for managing a broad task distribution with
alleviated gradient conflicts. To enhance task-wise routing, we introduce a
contrastive learning method that maximizes the mutual information between the
task and its router representation, enabling more precise capture of
task-relevant information. The outputs of two MoE components are concatenated
and fed into the next layer. Comprehensive experiments show that T2MIR
significantly facilitates in-context learning capacity and outperforms various
types of baselines. We bring the potential and promise of MoE to ICRL, offering
a simple and scalable architectural enhancement to advance ICRL one step closer
toward achievements in language and vision communities. Our code is available
at https://github.com/NJU-RL/T2MIR.

</details>


### [2] [MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction](https://arxiv.org/abs/2506.05427)
*Zishan Shu, Yufan Deng, Hongyu Zhang, Zhiwei Nie, Jie Chen*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种名为MTPNet的新方法，它利用分子与靶蛋白之间的相互作用来动态优化分子表示，从而更有效地进行活性悬崖预测。


<details>
  <summary>更多</summary>
  
**动机:** 现有的计算方法局限于处理单一结合靶点，限制了这些预测模型的适用性。因此需要一种新的方法来解决这个问题。

**方法:** 论文提出了一种名为Multi-Grained Target Perception network (MTPNet)的方法，该方法包括Macro-level Target Semantic (MTS)指导和Micro-level Pocket Semantic (MPS)指导两个部分，利用分子与其靶蛋白之间的相互作用的先验知识动态优化分子表示。

**结果:** 在30个代表性的活性悬崖数据集中进行的广泛实验表明，MTPNet显著优于以前的方法，在几个主流GNN架构上平均RMSE提高了18.95%。

**结论:** MTPNet通过内部的交互模式实现了对活性悬崖的统一预测，有助于加速化合物优化和设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MTPNet%3A+Multi-Grained+Target+Perception+for+Unified+Activity+Cliff+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05427&send_immediately=true&force_search=false)

**原文摘要:** Activity cliff prediction is a critical task in drug discovery and material
design. Existing computational methods are limited to handling single binding
targets, which restricts the applicability of these prediction models. In this
paper, we present the Multi-Grained Target Perception network (MTPNet) to
incorporate the prior knowledge of interactions between the molecules and their
target proteins. Specifically, MTPNet is a unified framework for activity cliff
prediction, which consists of two components: Macro-level Target Semantic (MTS)
guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet
dynamically optimizes molecular representations through multi-grained protein
semantic conditions. To our knowledge, it is the first time to employ the
receptor proteins as guiding information to effectively capture critical
interaction details. Extensive experiments on 30 representative activity cliff
datasets demonstrate that MTPNet significantly outperforms previous approaches,
achieving an average RMSE improvement of 18.95% on top of several mainstream
GNN architectures. Overall, MTPNet internalizes interaction patterns through
conditional deep learning to achieve unified predictions of activity cliffs,
helping to accelerate compound optimization and design. Codes are available at:
https://github.com/ZishanShu/MTPNet.

</details>


### [3] [Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction](https://arxiv.org/abs/2506.05428)
*Zhihao Tang, Chaozhuo Li, Litian Zhang, Xi Zhang*

**主要类别:** cs.LG

**AI概要:** MCI-Diff是一种新的扩散框架，用于从基线数据直接合成未来sMRI表示，以同时实现快速且准确的轻度认知障碍转化预测。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在即时性与准确性之间存在权衡，即利用单次基线sMRI进行快速预测与通过纵向扫描提高预测精度之间的矛盾。

**方法:** 1. 使用多任务序列重建策略训练共享去噪网络，处理不规则随访采样并学习稳健的潜在轨迹；2. 引入基于LLM的“语言指南针”，指导生成具有临床合理性的特征。

**结果:** 实验表明，MCI-Diff优于现有最先进的基线模型，并将早期转化预测的准确性提高了5-12%。

**结论:** MCI-Diff在不依赖纵向扫描的情况下实现了高精度的早期预测，为疾病进展建模提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+with+a+Linguistic+Compass%3A+Steering+the+Generation+of+Clinically+Plausible+Future+sMRI+Representations+for+Early+MCI+Conversion+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05428，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05428&send_immediately=true&force_search=false)

**原文摘要:** Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by
a trade-off between immediacy--making fast predictions from a single baseline
sMRI--and accuracy--leveraging longitudinal scans to capture disease
progression. We propose MCI-Diff, a diffusion-based framework that synthesizes
clinically plausible future sMRI representations directly from baseline data,
achieving both real-time risk assessment and high predictive performance.
First, a multi-task sequence reconstruction strategy trains a shared denoising
network on interpolation and extrapolation tasks to handle irregular follow-up
sampling and learn robust latent trajectories. Second, an LLM-driven
"linguistic compass" is introduced for clinical plausibility sampling:
generated feature candidates are quantized, tokenized, and scored by a
fine-tuned language model conditioned on expected structural biomarkers,
guiding autoregressive generation toward realistic disease patterns.
Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms
state-of-the-art baselines, improving early conversion accuracy by 5-12%.

</details>


### [4] [PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling](https://arxiv.org/abs/2506.05432)
*Yuxuan Yue, Zukang Xu, Zhihang Yuan, Dawei Yang, Jianglong Wu, Liqiang Nie*

**主要类别:** cs.LG

**AI概要:** 本文提出了PCDVQ方法，用于解决大型语言模型在边缘部署中的低比特量化问题，通过解耦向量的方向和大小并进行独立量化，从而提升模型准确性。


<details>
  <summary>更多</summary>
  
**动机:** 由于大型语言模型在边缘部署中面临巨大挑战，而现有基于欧氏距离的向量量化方法对量化误差敏感，因此需要提出一种新的解决方案。

**方法:** Polar Coordinate Decoupled Vector Quantization (PCDVQ) 通过将向量转换为极坐标表示，并独立量化方向和大小参数，同时优化方向和大小码本以匹配源分布。

**结果:** 实验结果显示，PCDVQ在2位级别上至少比基线方法提高了1.5%的零样本准确性。

**结论:** PCDVQ提供了一种有效的向量量化框架，能够显著提高零样本任务的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PCDVQ%3A+Enhancing+Vector+Quantization+for+Large+Language+Models+via+Polar+Coordinate+Decoupling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05432&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.

</details>


### [5] [Zeroth-Order Optimization Finds Flat Minima](https://arxiv.org/abs/2506.05454)
*Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Michael Muehlebach, Niao He*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了零阶优化方法在梯度不可行或昂贵的应用场景下隐式正则化特性，发现其偏好选择Hessian迹较小的解，并提供相关理论支持与实验验证。


<details>
  <summary>更多</summary>
  
**动机:** 现有的优化理论主要关注于任意平稳点的收敛，对于细粒度隐式正则化特性的了解较少，特别是在梯度难以或昂贵计算的应用中。

**方法:** 使用标准两点估计器进行零阶优化，并通过理论分析和实验验证方法的有效性。

**结果:** 研究表明，零阶优化方法偏好Hessian迹较小的解，并提供了针对凸函数和足够平滑函数达到近似平坦极小值的收敛速率。

**结论:** 论文得出零阶优化方法倾向于选择具有较小Hessian矩阵迹的解，这在区分尖锐和平坦极小值中被广泛使用，并且为凸函数和平滑函数提供了近似平坦极小值的收敛速率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zeroth-Order+Optimization+Finds+Flat+Minima，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05454，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05454&send_immediately=true&force_search=false)

**原文摘要:** Zeroth-order methods are extensively used in machine learning applications
where gradients are infeasible or expensive to compute, such as black-box
attacks, reinforcement learning, and language model fine-tuning. Existing
optimization theory focuses on convergence to an arbitrary stationary point,
but less is known on the implicit regularization that provides a fine-grained
characterization on which particular solutions are finally reached. We show
that zeroth-order optimization with the standard two-point estimator favors
solutions with small trace of Hessian, which is widely used in previous work to
distinguish between sharp and flat minima. We further provide convergence rates
of zeroth-order optimization to approximate flat minima for convex and
sufficiently smooth functions, where flat minima are defined as the minimizers
that achieve the smallest trace of Hessian among all optimal solutions.
Experiments on binary classification tasks with convex losses and language
model fine-tuning support our theoretical findings.

</details>


### [6] [Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward](https://arxiv.org/abs/2506.05433)
*Zikang Liu, Tongtian Yue, Yepeng Tang, Longteng Guo, Junxian Cai, Qingbin Liu, Xi Chen, Jing Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出Prefix Grouper方法优化GRPO算法，在保持训练效果不变的前提下大幅减少计算开销，提升其在长上下文任务中的效率和可扩展性。


<details>
  <summary>更多</summary>
  
**动机:** GRPO在处理长共享前缀时引入了较大的计算开销，成为长上下文学习场景中的可扩展性瓶颈。

**方法:** 通过将自注意力机制重构为两个部分，实现共享前缀仅编码一次的策略（Shared-Prefix Forward）。

**结果:** 实验表明Prefix Grouper在保证结果一致性的同时显著降低了训练计算成本，尤其是在长前缀场景中。

**结论:** Prefix Grouper与标准GRPO在训练上是等效的，能够显著降低计算成本，提高GRPO的可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prefix+Grouper%3A+Efficient+GRPO+Training+through+Shared-Prefix+Forward，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05433，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05433&send_immediately=true&force_search=false)

**原文摘要:** Group Relative Policy Optimization (GRPO) enhances policy learning by
computing gradients from relative comparisons among candidate outputs that
share a common input prefix. Despite its effectiveness, GRPO introduces
substantial computational overhead when processing long shared prefixes, which
must be redundantly encoded for each group member. This inefficiency becomes a
major scalability bottleneck in long-context learning scenarios. We propose
Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant
prefix computation via a Shared-Prefix Forward strategy. In particular, by
restructuring self-attention into two parts, our method enables the shared
prefix to be encoded only once, while preserving full differentiability and
compatibility with end-to-end training. We provide both theoretical and
empirical evidence that Prefix Grouper is training-equivalent to standard GRPO:
it yields identical forward outputs and backward gradients, ensuring that the
optimization dynamics and final policy performance remain unchanged.
Empirically, our experiments confirm that Prefix Grouper achieves consistent
results while significantly reducing the computational cost of training,
particularly in long-prefix scenarios. The proposed method is fully
plug-and-play: it is compatible with existing GRPO-based architectures and can
be seamlessly integrated into current training pipelines as a drop-in
replacement, requiring no structural modifications and only minimal changes to
input construction and attention computation. Prefix Grouper enables the use of
larger group sizes under the same computational budget, thereby improving the
scalability of GRPO to more complex tasks and larger models. Code is now
available at https://github.com/johncaged/PrefixGrouper

</details>


### [7] [The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models](https://arxiv.org/abs/2506.05500)
*Alex Damian, Jason D. Lee, Joan Bruna*

**主要类别:** cs.LG

**AI概要:** 该论文提出了用于多指标模型中隐藏子空间估计的新方法，并确定了样本复杂度的上下限以及实际应用场景。


<details>
  <summary>更多</summary>
  
**动机:** 研究标签如何仅通过其在低维子空间上的投影依赖于高斯输入的问题，并探索高效估计隐藏子空间的方法。

**方法:** 引入了“生成跃迁”指数k⋆，并基于低次多项式框架展示了所需样本复杂度，并通过适当的Hermite张量上的谱U统计量给出了一种无先验知识的顺序估计方法。

**结果:** 论文表明，在多指标设置下，必要且足够的样本复杂度为n=Θ(d^{1∨κ/2})，并且成功构造出一种无需先验知识的估计过程。

**结论:** 论文得出了一种有效的对多指标模型进行无先验知识的估计方法，并计算了包括分段线性函数和深层神经网络在内的多个示例的生成跃迁指数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Generative+Leap%3A+Sharp+Sample+Complexity+for+Efficiently+Learning+Gaussian+Multi-Index+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05500，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05500&send_immediately=true&force_search=false)

**原文摘要:** In this work we consider generic Gaussian Multi-index models, in which the
labels only depend on the (Gaussian) $d$-dimensional inputs through their
projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient
agnostic estimation procedures for this hidden subspace. We introduce the
\emph{generative leap} exponent $k^\star$, a natural extension of the
generative exponent from [Damian et al.'24] to the multi-index setting. We
first show that a sample complexity of $n=\Theta(d^{1 \vee \k/2})$ is necessary
in the class of algorithms captured by the Low-Degree-Polynomial framework. We
then establish that this sample complexity is also sufficient, by giving an
agnostic sequential estimation procedure (that is, requiring no prior knowledge
of the multi-index model) based on a spectral U-statistic over appropriate
Hermite tensors. We further compute the generative leap exponent for several
examples including piecewise linear functions (deep ReLU networks with bias),
and general deep neural networks (with $r$-dimensional first hidden layer).

</details>


### [8] [Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks](https://arxiv.org/abs/2506.05434)
*Thomas Massena, Léo andéol, Thibaut Boissin, Franck Mamalet, Corentin Friedrich, Mathieu Serrurier, Sébastien Gerchinovitz*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的鲁棒共形预测方法lip-rcp，这种方法利用Lipschitz有界网络来提高预测集的信任度，并在大规模问题上表现出优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的共形预测在对抗攻击下无法保持其保证，因此需要一种新的鲁棒共形预测方法，能够在实际场景中提供有效的保证和计算效率。

**方法:** 该研究采用Lipschitz有界网络来构建稳健的共形预测集，并推导了对所有对抗攻击水平同时有效的香草共形预测集的最坏情况覆盖率界限。

**结果:** 实验表明，结合1-Lipschitz鲁棒网络，新方法lip-rcp在ImageNet等中大规模场景下的鲁棒共形预测集大小和计算效率方面均优于当前最先进的结果。

**结论:** 本文提出了一种新的方法lip-rcp，通过利用Lipschitz有界的网络来精确且高效地估计稳健的共形预测集，并在中大规模场景下优于现有技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Robust+Conformal+Prediction+via+Lipschitz-Bounded+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05434，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05434&send_immediately=true&force_search=false)

**原文摘要:** Conformal Prediction (CP) has proven to be an effective post-hoc method for
improving the trustworthiness of neural networks by providing prediction sets
with finite-sample guarantees. However, under adversarial attacks, classical
conformal guarantees do not hold anymore: this problem is addressed in the
field of Robust Conformal Prediction. Several methods have been proposed to
provide robust CP sets with guarantees under adversarial perturbations, but,
for large scale problems, these sets are either too large or the methods are
too computationally demanding to be deployed in real life scenarios. In this
work, we propose a new method that leverages Lipschitz-bounded networks to
precisely and efficiently estimate robust CP sets. When combined with a
1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms
state-of-the-art results in both the size of the robust CP sets and
computational efficiency in medium and large-scale scenarios such as ImageNet.
Taking a different angle, we also study vanilla CP under attack, and derive new
worst-case coverage bounds of vanilla CP sets, which are valid simultaneously
for all adversarial attack levels. Our lip-rcp method makes this second
approach as efficient as vanilla CP while also allowing robustness guarantees.

</details>


### [9] [Winner-takes-all for Multivariate Probabilistic Time Series Forecasting](https://arxiv.org/abs/2506.05515)
*Adrien Cortés, Rémi Rehm, Victor Letzelter*

**主要类别:** cs.LG

**AI概要:** 本文提出了TimeMCL，一种基于Multiple Choice Learning的时间序列预测方法，能高效地产生多样化的预测结果。


<details>
  <summary>更多</summary>
  
**动机:** 解决时间序列预测中任务模糊和不适定的问题，并提供多种可能的预测结果。

**方法:** 利用Multiple Choice Learning范式和具有多个头部的神经网络，采用Winner-Takes-All损失来促进预测多样性。

**结果:** 在合成数据和真实世界时间序列上的实验表明，该方法计算成本低且性能良好。

**结论:** TimeMCL是一种高效的多头神经网络方法，用于时间序列预测，通过Winner-Takes-All损失促进预测的多样性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Winner-takes-all+for+Multivariate+Probabilistic+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05515&send_immediately=true&force_search=false)

**原文摘要:** We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)
paradigm to forecast multiple plausible time series futures. Our approach
employs a neural network with multiple heads and utilizes the Winner-Takes-All
(WTA) loss to promote diversity among predictions. MCL has recently gained
attention due to its simplicity and ability to address ill-posed and ambiguous
tasks. We propose an adaptation of this framework for time-series forecasting,
presenting it as an efficient method to predict diverse futures, which we
relate to its implicit quantization objective. We provide insights into our
approach using synthetic data and evaluate it on real-world time series,
demonstrating its promising performance at a light computational cost.

</details>


### [10] [Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning](https://arxiv.org/abs/2506.05435)
*Manon Renault, Hamoud Younes, Hugo Tessier, Ronan Le Roy, Bastien Pasdeloup, Mathieu Léonardon*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于嵌入式系统的可重复使用包装状态监测方法，利用一维卷积神经网络处理从传感器收集的时间序列数据，通过数据增强技术和模型压缩实现了高精度和低功耗。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高工业应用中可重复使用包装监测系统的操作效率和生态可持续性，需要一个具有多年使用寿命的嵌入式系统来检测其状态。

**方法:** 采用了一维卷积神经网络架构对物联网设备上的加速度计数据进行分类，并在训练前测试了两种数据增强技术以解决数据集不平衡问题。

**结果:** 方法在所考虑的二类问题上取得了94.54%和95.83%的准确率，同时压缩技术将模型大小减少了四倍，并且模型在推理过程中仅消耗316毫瓦的功率。

**结论:** 研究得出，通过最小化设备唤醒时间可以最大化设备寿命，并成功设计了一个低功耗、高性能的嵌入式系统用于检测可重复使用包装的状态。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Event+Classification+of+Accelerometer+Data+for+Industrial+Package+Monitoring+with+Embedded+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05435，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05435&send_immediately=true&force_search=false)

**原文摘要:** Package monitoring is an important topic in industrial applications, with
significant implications for operational efficiency and ecological
sustainability. In this study, we propose an approach that employs an embedded
system, placed on reusable packages, to detect their state (on a Forklift, in a
Truck, or in an undetermined location). We aim to design a system with a
lifespan of several years, corresponding to the lifespan of reusable packages.
Our analysis demonstrates that maximizing device lifespan requires minimizing
wake time. We propose a pipeline that includes data processing, training, and
evaluation of the deep learning model designed for imbalanced, multiclass time
series data collected from an embedded sensor. The method uses a
one-dimensional Convolutional Neural Network architecture to classify
accelerometer data from the IoT device. Before training, two data augmentation
techniques are tested to solve the imbalance problem of the dataset: the
Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling
approach. After training, compression techniques are implemented to have a
small model size. On the considered twoclass problem, the methodology yields a
precision of 94.54% for the first class and 95.83% for the second class, while
compression techniques reduce the model size by a factor of four. The trained
model is deployed on the IoT device, where it operates with a power consumption
of 316 mW during inference.

</details>


### [11] [On Fitting Flow Models with Large Sinkhorn Couplings](https://arxiv.org/abs/2506.05526)
*Michal Klein, Alireza Mousavi-Hosseini, Stephen Zhang, Marco Cuturi*

**主要类别:** cs.LG

**AI概要:** This paper demonstrates that flow models perform better when trained with large-scale Sinkhorn couplings and minimal entropic regularization, enabling more efficient data generation.


<details>
  <summary>更多</summary>
  
**动机:** Training flow models without optimal pairing between source and target points leads to inefficiencies. Using optimal transport measures can improve training efficiency, but practical implementations have been limited to small batch sizes and specific OT solvers.

**方法:** The paper explores increasing the batch size by three to four orders of magnitude and analyzes the impact of entropic regularization in the Sinkhorn algorithm. It introduces scale-invariant quantities to measure coupling sharpness and employs sharded computations across GPUs for scalability.

**结果:** The study shows that using larger batch sizes and lower entropic regularization significantly improves the performance of flow models in both synthetic and image generation tasks.

**结论:** Flow models benefit greatly from being fitted with large Sinkhorn couplings and low entropic regularization, improving efficiency in synthetic and image generation tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Fitting+Flow+Models+with+Large+Sinkhorn+Couplings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05526，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05526&send_immediately=true&force_search=false)

**原文摘要:** Flow models transform data gradually from one modality (e.g. noise) onto
another (e.g. images). Such models are parameterized by a time-dependent
velocity field, trained to fit segments connecting pairs of source and target
points. When the pairing between source and target points is given, training
flow models boils down to a supervised regression problem. When no such pairing
exists, as is the case when generating data from noise, training flows is much
harder. A popular approach lies in picking source and target points
independently. This can, however, lead to velocity fields that are slow to
train, but also costly to integrate at inference time. In theory, one would
greatly benefit from training flow models by sampling pairs from an optimal
transport (OT) measure coupling source and target, since this would lead to a
highly efficient flow solving the Benamou and Brenier dynamical OT problem. In
practice, recent works have proposed to sample mini-batches of $n$ source and
$n$ target points and reorder them using an OT solver to form better pairs.
These works have advocated using batches of size $n\approx 256$, and considered
OT solvers that return couplings that are either sharp (using e.g. the
Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.
Sinkhorn). We follow in the footsteps of these works by exploring the benefits
of increasing $n$ by three to four orders of magnitude, and look more carefully
on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn
algorithm. Our analysis is facilitated by new scale invariant quantities to
report the sharpness of a coupling, while our sharded computations across
multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic
and image generation tasks, flow models greatly benefit when fitted with large
Sinkhorn couplings, with a low entropic regularization $\varepsilon$.

</details>


### [12] [An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics](https://arxiv.org/abs/2506.05438)
*Tongda Sun, Chen Yin, Huailiang Zheng, Yining Dong*

**主要类别:** cs.LG

**AI概要:** 论文介绍了一种基于无监督框架的动态健康指标构建方法，有效解决了传统方法依赖专家知识和忽视动态信息的问题，在退化趋势建模和未来退化预测方面表现出优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的HI构建方法大多依赖专家知识进行特征提取，并且忽略了序列退化过程中隐藏的动态信息，这限制了构建的HI在退化趋势表示和预测方面的能力。

**方法:** 通过一个无监督框架构建考虑HI级时间依赖性的动态HI，包括降维特征学习模块和HI生成模块，其中HI生成模块嵌入了内部HI预测块以保证过去和当前HI状态之间的时间依赖性。

**结果:** 实验结果表明，所提出的HI构建方法在两个轴承生命周期数据集上均优于比较方法，构建的动态HI在预测任务中表现优越。

**结论:** 论文提出了一种新的动态健康指标（HI）构建方法，该方法在滚动轴承的退化评估和预测中表现出优于现有方法的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Unsupervised+Framework+for+Dynamic+Health+Indicator+Construction+and+Its+Application+in+Rolling+Bearing+Prognostics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05438&send_immediately=true&force_search=false)

**原文摘要:** Health indicator (HI) plays a key role in degradation assessment and
prognostics of rolling bearings. Although various HI construction methods have
been investigated, most of them rely on expert knowledge for feature extraction
and overlook capturing dynamic information hidden in sequential degradation
processes, which limits the ability of the constructed HI for degradation trend
representation and prognostics. To address these concerns, a novel dynamic HI
that considers HI-level temporal dependence is constructed through an
unsupervised framework. Specifically, a degradation feature learning module
composed of a skip-connection-based autoencoder first maps raw signals to a
representative degradation feature space (DFS) to automatically extract
essential degradation features without the need for expert knowledge.
Subsequently, in this DFS, a new HI-generating module embedded with an inner
HI-prediction block is proposed for dynamic HI construction, where the temporal
dependence between past and current HI states is guaranteed and modeled
explicitly. On this basis, the dynamic HI captures the inherent dynamic
contents of the degradation process, ensuring its effectiveness for degradation
tendency modeling and future degradation prognostics. The experiment results on
two bearing lifecycle datasets demonstrate that the proposed HI construction
method outperforms comparison methods, and the constructed dynamic HI is
superior for prognostic tasks.

</details>


### [13] [When can in-context learning generalize out of task distribution?](https://arxiv.org/abs/2506.05574)
*Chase Goddard, Lindsay M. Smith, Vudtiwat Ngampruetikorn, David J. Schwab*

**主要类别:** cs.LG

**AI概要:** 本文研究了Transformer模型在不同任务多样性下的上下文学习能力，发现任务多样性促使模型从特定任务解决方案过渡到跨任务泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 旨在探究在不同分布下，上下文学习（ICL）所需的预训练分布条件，以及任务多样性对ICL泛化能力的影响。

**方法:** 通过实证方法研究预训练分布对上下文学习能力的影响，并使用线性函数上的Transformer进行实验。同时，构建了一个相图来表征任务多样性与预训练任务数量之间的关系。

**结果:** 发现任务多样性促使Transformer模型从仅在预训练任务分布内表现ICL的能力，转变为能够在未见过的任务上泛化的解决方案。同时观察到非线性回归问题中也存在类似的转变。

**结论:** 论文得出结论，随着任务多样性的增加，Transformer模型会经历从特定解决方案到能够泛化到整个任务空间的过渡。此外，还探讨了模型深度和回归问题的维度如何影响这种过渡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+can+in-context+learning+generalize+out+of+task+distribution%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05574&send_immediately=true&force_search=false)

**原文摘要:** In-context learning (ICL) is a remarkable capability of pretrained
transformers that allows models to generalize to unseen tasks after seeing only
a few examples. We investigate empirically the conditions necessary on the
pretraining distribution for ICL to emerge and generalize
\emph{out-of-distribution}. Previous work has focused on the number of distinct
tasks necessary in the pretraining dataset. Here, we use a different notion of
task diversity to study the emergence of ICL in transformers trained on linear
functions. We find that as task diversity increases, transformers undergo a
transition from a specialized solution, which exhibits ICL only within the
pretraining task distribution, to a solution which generalizes out of
distribution to the entire task space. We also investigate the nature of the
solutions learned by the transformer on both sides of the transition, and
observe similar transitions in nonlinear regression problems. We construct a
phase diagram to characterize how our concept of task diversity interacts with
the number of pretraining tasks. In addition, we explore how factors such as
the depth of the model and the dimensionality of the regression problem
influence the transition.

</details>


### [14] [UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss](https://arxiv.org/abs/2506.05443)
*Yiyu Lin, Yan Wang, You Zhou, Xinye Ni, Jiahui Wu, Sen Yang*

**主要类别:** cs.LG

**AI概要:** 该研究提出了UniPTMs，这是一种用于多类型蛋白质翻译后修饰预测的统一框架，通过创新的双路径架构和多种模块显著提升了预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有深度学习模型在跨模态特征融合、领域泛化和架构优化方面存在局限性，需要一种统一的多类型PTM预测框架来提高精度和效率。

**方法:** 提出了一种“主从”双路径协作架构，包括BGCA模块、LDFN模块、MACP模块、BHGFN模块以及HDWF机制，并采用了分层对比损失函数。

**结果:** UniPTMs在五个修饰类型上的性能比现有最先进模型显著提升（MCC增加3.2%-11.4%，AP增加4.2%-14.3%），并成功超越了单类型预测范式。

**结论:** UniPTMs是一个统一的多类型PTM预测框架，通过多种创新模块和机制显著提高了性能，并开发了轻量级版本以平衡模型复杂性和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UniPTMs%3A+The+First+Unified+Multi-type+PTM+Site+Prediction+Model+via+Master-Slave+Architecture-Based+Multi-Stage+Fusion+Strategy+and+Hierarchical+Contrastive+Loss，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05443&send_immediately=true&force_search=false)

**原文摘要:** As a core mechanism of epigenetic regulation in eukaryotes, protein
post-translational modifications (PTMs) require precise prediction to decipher
dynamic life activity networks. To address the limitations of existing deep
learning models in cross-modal feature fusion, domain generalization, and
architectural optimization, this study proposes UniPTMs: the first unified
framework for multi-type PTM prediction. The framework innovatively establishes
a "Master-Slave" dual-path collaborative architecture: The master path
dynamically integrates high-dimensional representations of protein sequences,
structures, and evolutionary information through a Bidirectional Gated
Cross-Attention (BGCA) module, while the slave path optimizes feature
discrepancies and recalibration between structural and traditional features
using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale
Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and
a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level
feature integration across paths, the framework employs a Hierarchical Dynamic
Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal
features. Enhanced by a novel Hierarchical Contrastive loss function for
feature consistency optimization, UniPTMs demonstrates significant performance
improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art
models across five modification types and transcends the Single-Type Prediction
Paradigm. To strike a balance between model complexity and performance, we have
also developed a lightweight variant named UniPTMs-mini.

</details>


### [15] [Conformal Prediction Adaptive to Unknown Subpopulation Shifts](https://arxiv.org/abs/2506.05583)
*Nien-Shao Wang, Duygu Nur Yaldiz, Yavuz Faruk Bakman, Sai Praneeth Karimireddy*

**主要类别:** cs.LG

**AI概要:** 本文提出了适应分布偏移的共形预测新方法，在高维任务中表现优异，解决了标准方法失败的问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于传统共形预测在分布偏移下失效，因此需要提出新方法以应对实际场景中的不确定性。

**方法:** 提出了新的共形预测方法以适应子群体分布变化的情况。

**结果:** 实验表明新方法在视觉和语言任务中能可靠保持覆盖率，控制风险。

**结论:** 论文得出新的方法能够有效适应分布偏移，保证覆盖率，并在实际任务中表现良好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conformal+Prediction+Adaptive+to+Unknown+Subpopulation+Shifts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05583，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05583&send_immediately=true&force_search=false)

**原文摘要:** Conformal prediction is widely used to equip black-box machine learning
models with uncertainty quantification enjoying formal coverage guarantees.
However, these guarantees typically break down in the presence of distribution
shifts, where the data distribution at test time differs from the training (or
calibration-time) distribution. In this work, we address subpopulation shifts,
where the test environment exhibits an unknown and differing mixture of
subpopulations compared to the calibration data. We propose new methods that
provably adapt conformal prediction to such shifts, ensuring valid coverage
without requiring explicit knowledge of subpopulation structure. Our algorithms
scale to high-dimensional settings and perform effectively in realistic machine
learning tasks. Extensive experiments on vision (with vision transformers) and
language (with large language models) benchmarks demonstrate that our methods
reliably maintain coverage and controls risk in scenarios where standard
conformal prediction fails.

</details>


### [16] [Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic](https://arxiv.org/abs/2506.05445)
*Thanh Vinh Vo, Young Lee, Haozhe Ma, Chien Lu, Tze-Yun Leong*

**主要类别:** cs.LG

**AI概要:** 本文提出DoSAC算法，通过因果干预估计解决强化学习中的隐藏混杂问题，提升了策略学习的效果。


<details>
  <summary>更多</summary>
  
**动机:** 隐藏的混杂因素会影响状态和动作，导致策略学习出现偏差。大多数强化学习算法忽略了这个问题。

**方法:** 提出了一种名为Backdoor Reconstructor的可学习模块，并将其集成到软演员-评论家框架中，以计算干预策略及其熵值。

**结果:** 在连续控制基准测试中，DoSAC在存在混杂因素的情况下优于基线模型，表现出更好的鲁棒性、泛化能力和策略可靠性。

**结论:** DoSAC通过因果干预估计纠正了隐藏的混杂因素，改善了强化学习中的策略学习效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Policy+Learning+in+Reinforcement+Learning%3A+Backdoor-Adjusted+Soft+Actor-Critic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05445&send_immediately=true&force_search=false)

**原文摘要:** Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.

</details>


### [17] [Zero-shot protein stability prediction by inverse folding models: a free energy interpretation](https://arxiv.org/abs/2506.05596)
*Jes Frellsen, Maher M. Kassem, Tone Bengtsen, Lars Olsen, Kresten Lindorff-Larsen, Jesper Ferkinghoff-Borg, Wouter Boomsma*

**主要类别:** cs.LG

**AI概要:** 该研究探讨了逆折叠模型的自由能基础，揭示了现有方法的局限性，并提出改进方案以增强零样本蛋白质稳定性预测的效果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管逆折叠模型在蛋白质稳定性预测中表现出色，但其氨基酸偏好与热力学稳定性背后的自由能关系尚未完全明确，这促使作者从理论角度出发，寻求更强的零样本预测能力。

**方法:** 论文通过推导和实证评估的方法，分析了似然比作为简化近似的效果，并探索了改进估计逆折叠模型相对稳定性的方法。

**结果:** 论文展示了通过较简单手段即可实现零样本预测性能的显著提升，并提出了改进似然比方法的路径。

**结论:** 论文得出结论，通过对逆折叠模型的自由能基础进行更准确的估计，可以显著提高零样本稳定性预测的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-shot+protein+stability+prediction+by+inverse+folding+models%3A+a+free+energy+interpretation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05596&send_immediately=true&force_search=false)

**原文摘要:** Inverse folding models have proven to be highly effective zero-shot
predictors of protein stability. Despite this success, the link between the
amino acid preferences of an inverse folding model and the free-energy
considerations underlying thermodynamic stability remains incompletely
understood. A better understanding would be of interest not only from a
theoretical perspective, but also potentially provide the basis for stronger
zero-shot stability prediction. In this paper, we take steps to clarify the
free-energy foundations of inverse folding models. Our derivation reveals the
standard practice of likelihood ratios as a simplistic approximation and
suggests several paths towards better estimates of the relative stability. We
empirically assess these approaches and demonstrate that considerable gains in
zero-shot performance can be achieved with fairly simple means.

</details>


### [18] [Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning](https://arxiv.org/abs/2506.05447)
*Andrei Mircea, Supriyo Chakraborty, Nima Chitsazan, Irina Rish, Ekaterina Lobacheva*

**主要类别:** cs.LG

**AI概要:** This paper studies how scaling affects the training dynamics of language models, identifying a phenomenon called loss deceleration and attributing it to zero-sum learning, offering potential ways to enhance language models independently of their scale.


<details>
  <summary>更多</summary>
  
**动机:** This work aims to understand how scaling improves language models, specifically in terms of training dynamics.

**方法:** We make our code and artefacts available at: https://github.com/mirandrom/zsl

**结果:** We find that language models undergo loss deceleration early in training; an abrupt slowdown in the rate of loss improvement, resulting in piecewise linear behaviour of the loss curve in log-log space. Scaling up the model mitigates this transition by (1) decreasing the loss at which deceleration occurs, and (2) improving the log-log rate of loss improvement after deceleration. We attribute loss deceleration to a type of degenerate training dynamics we termed zero-sum learning (ZSL). In ZSL, per-example gradients become systematically opposed, leading to destructive interference in per-example changes in loss. As a result, improving loss on one subset of examples degrades it on another, bottlenecking overall progress.

**结论:** Loss deceleration and ZSL provide new insights into the training dynamics underlying language model scaling laws, and could potentially be targeted directly to improve language models independent of scale.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+Dynamics+Underlying+Language+Model+Scaling+Laws%3A+Loss+Deceleration+and+Zero-Sum+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05447，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05447&send_immediately=true&force_search=false)

**原文摘要:** This work aims to understand how scaling improves language models,
specifically in terms of training dynamics. We find that language models
undergo loss deceleration early in training; an abrupt slowdown in the rate of
loss improvement, resulting in piecewise linear behaviour of the loss curve in
log-log space. Scaling up the model mitigates this transition by (1) decreasing
the loss at which deceleration occurs, and (2) improving the log-log rate of
loss improvement after deceleration. We attribute loss deceleration to a type
of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,
per-example gradients become systematically opposed, leading to destructive
interference in per-example changes in loss. As a result, improving loss on one
subset of examples degrades it on another, bottlenecking overall progress. Loss
deceleration and ZSL provide new insights into the training dynamics underlying
language model scaling laws, and could potentially be targeted directly to
improve language models independent of scale. We make our code and artefacts
available at: https://github.com/mirandrom/zsl

</details>


### [19] [RNE: a plug-and-play framework for diffusion density estimation and inference-time control](https://arxiv.org/abs/2506.05668)
*Jiajun He, José Miguel Hernández-Lobato, Yuanqi Du, Francisco Vargas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的扩散模型推理密度估计和控制框架Radon-Nikodym Estimator (RNE)，其通过统一视角简化并优化了现有方法的应用。


<details>
  <summary>更多</summary>
  
**动机:** 为了提供一种统一的方法来处理扩散模型的密度估计和推理控制问题，从而提升模型的灵活性和实用性。

**方法:** 基于Radon-Nikodym导数的灵活框架进行扩散推断密度估计和控制。

**结果:** 实验表明，RNE在扩散密度估计和推理时间控制任务中表现良好，包括退火、扩散模型组合和奖励倾斜等场景。

**结论:** RNE成功地连接和统一了多种现有的密度估计和推理时间控制方法，展示了其在理论和实践中的价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RNE%3A+a+plug-and-play+framework+for+diffusion+density+estimation+and+inference-time+control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05668&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,
plug-and-play framework for diffusion inference-time density estimation and
control, based on the concept of the density ratio between path distributions.
RNE connects and unifies a variety of existing density estimation and
inference-time control methods under a single and intuitive perspective,
stemming from basic variational inference and probabilistic principles
therefore offering both theoretical clarity and practical versatility.
Experiments demonstrate that RNE achieves promising performances in diffusion
density estimation and inference-time control tasks, including annealing,
composition of diffusion models, and reward-tilting.

</details>


### [20] [Grokking Beyond the Euclidean Norm of Model Parameters](https://arxiv.org/abs/2506.05718)
*Pascal Jr Tikeng Notsawo, Guillaume Dumas, Guillaume Rabusseau*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了神经网络训练中的“grokking”现象，指出正则化、模型深度和数据选择对此现象的影响，并发现L2范数不能很好地预测模型泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 本文旨在探究在梯度优化过程中出现的延迟泛化现象“grokking”，并尝试理解其背后机制以及影响因素，例如正则化、模型深度和数据选择等。

**方法:** 研究方法包括分析梯度下降结合小但非零的正则化项对模型泛化能力的影响，并探讨了过参数化和数据选择对grokking现象的作用。

**结果:** 结果显示，在存在具有特定属性P（如稀疏性或低秩权重）的模型时，加入适当的正则化能够诱导出grokking现象；同时发现L2范数并不是衡量泛化能力的可靠指标，特别是在正则化目标不同于L2范数时。此外，仅通过数据选择就可以放大grokking效应。

**结论:** 论文得出的结论是，通过正则化（无论是显式还是隐式）可以诱导神经网络产生“grokking”现象，并且深度模型可以在不使用显式正则化的情况下实现“grokking”或“ungrokking”。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Grokking+Beyond+the+Euclidean+Norm+of+Model+Parameters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05718，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05718&send_immediately=true&force_search=false)

**原文摘要:** Grokking refers to a delayed generalization following overfitting when
optimizing artificial neural networks with gradient-based methods. In this
work, we demonstrate that grokking can be induced by regularization, either
explicit or implicit. More precisely, we show that when there exists a model
with a property $P$ (e.g., sparse or low-rank weights) that generalizes on the
problem of interest, gradient descent with a small but non-zero regularization
of $P$ (e.g., $\ell_1$ or nuclear norm regularization) results in grokking.
This extends previous work showing that small non-zero weight decay induces
grokking. Moreover, our analysis shows that over-parameterization by adding
depth makes it possible to grok or ungrok without explicitly using
regularization, which is impossible in shallow cases. We further show that the
$\ell_2$ norm is not a reliable proxy for generalization when the model is
regularized toward a different property $P$, as the $\ell_2$ norm grows in many
cases where no weight decay is used, but the model generalizes anyway. We also
show that grokking can be amplified solely through data selection, with any
other hyperparameter fixed.

</details>


### [21] [Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors](https://arxiv.org/abs/2506.05479)
*Matei Gabriel Coşa, Marek Eliáš*

**主要类别:** cs.LG

**AI概要:** 本文研究了在线任务处理中启发式方法的选择问题，通过结合Bandit Learning技术，在受限条件下实现了与最优启发式方法相当的性能，并给出了理论保证。


<details>
  <summary>更多</summary>
  
**动机:** 问题背景是在线处理输入实例时，只能在每个时间步查询一个启发式方法的动作，目标是实现与给定启发式中最好的一个相当的性能。由于当前时间步的代价无法估计，除非前一时间步也查询了相同的启发式方法，因此提出了新的挑战。

**方法:** 作者通过结合Bandit Learning和记忆有界对抗者的模型，设计了一种算法来在只能查询一个启发式方法的情况下，实现与最佳启发式相当的性能。

**结果:** 文章展示了如何达到$O(	ext{OPT}^{2/3})$的遗憾界，并提供了基于Dekel等人构造的紧下界，表明所提出的算法在理论上是最优的。

**结论:** 本文提出了一种针对记忆有界的对抗者在在线任务处理中选择启发式方法的策略，并证明了该策略具有$O(	ext{OPT}^{2/3})$的遗憾界，并基于Dekel等人的构造给出了紧下界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning-Augmented+Algorithms+for+MTS+with+Bandit+Access+to+Multiple+Predictors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05479，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05479&send_immediately=true&force_search=false)

**原文摘要:** We consider the following problem: We are given $\ell$ heuristics for
Metrical Task Systems (MTS), where each might be tailored to a different type
of input instances. While processing an input instance received online, we are
allowed to query the action of only one of the heuristics at each time step.
Our goal is to achieve performance comparable to the best of the given
heuristics. The main difficulty of our setting comes from the fact that the
cost paid by a heuristic at time $t$ cannot be estimated unless the same
heuristic was also queried at time $t-1$. This is related to Bandit Learning
against memory bounded adversaries (Arora et al., 2012). We show how to achieve
regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the
construction of Dekel et al. (2013).

</details>


### [22] [Neural Collapse in Cumulative Link Models for Ordinal Regression: An Analysis with Unconstrained Feature Model](https://arxiv.org/abs/2506.05801)
*Chuang Ma, Tomoyuki Obuchi, Toshiyuki Tanaka*

**主要类别:** cs.LG

**AI概要:** 这项研究探讨了深度序数回归任务中是否存在类似于分类任务中的神经坍缩现象，并提出了序数神经坍缩（ONC）的概念及其特性，同时提供了理论证明和实验验证。


<details>
  <summary>更多</summary>
  
**动机:** Neural Collapse (NC) 现象的研究加深了我们对深度神经网络行为的理解，本文旨在探索该现象是否会在深度序数回归（OR）任务中出现，以期扩展NC的应用范围。

**方法:** 研究采用了Unconstrained Feature Model (UFM) 来解释神经坍缩现象，并将这一理论扩展到了序数回归任务中。分析包括解析证明和对多种数据集的实证验证。

**结果:** 研究发现，在深度序数回归任务中也存在一种称为Ordinal Neural Collapse (ONC) 的现象，其特点包括：(ONC1) 应用正则化时，同一类的所有最优特征会坍缩到类内均值；(ONC2) 这些类均值与分类器对齐，即坍缩到一维子空间；(ONC3) 最优潜在变量按照类别顺序排列，并且在零正则化极限下，潜在变量和阈值之间会出现高度局部且简单的几何关系。

**结论:** 本研究通过结合累积链接模型和UFM框架，证明了在深度序数回归任务中确实存在类似于分类任务中的神经坍缩现象，并将其命名为序数神经坍缩（ONC）。此外，作者讨论了如何利用这些洞察进行OR任务，并强调使用固定阈值的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Collapse+in+Cumulative+Link+Models+for+Ordinal+Regression%3A+An+Analysis+with+Unconstrained+Feature+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05801，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05801&send_immediately=true&force_search=false)

**原文摘要:** A phenomenon known as ''Neural Collapse (NC)'' in deep classification tasks,
in which the penultimate-layer features and the final classifiers exhibit an
extremely simple geometric structure, has recently attracted considerable
attention, with the expectation that it can deepen our understanding of how
deep neural networks behave. The Unconstrained Feature Model (UFM) has been
proposed to explain NC theoretically, and there emerges a growing body of work
that extends NC to tasks other than classification and leverages it for
practical applications. In this study, we investigate whether a similar
phenomenon arises in deep Ordinal Regression (OR) tasks, via combining the
cumulative link model for OR and UFM. We show that a phenomenon we call Ordinal
Neural Collapse (ONC) indeed emerges and is characterized by the following
three properties: (ONC1) all optimal features in the same class collapse to
their within-class mean when regularization is applied; (ONC2) these class
means align with the classifier, meaning that they collapse onto a
one-dimensional subspace; (ONC3) the optimal latent variables (corresponding to
logits or preactivations in classification tasks) are aligned according to the
class order, and in particular, in the zero-regularization limit, a highly
local and simple geometric relationship emerges between the latent variables
and the threshold values. We prove these properties analytically within the UFM
framework with fixed threshold values and corroborate them empirically across a
variety of datasets. We also discuss how these insights can be leveraged in OR,
highlighting the use of fixed thresholds.

</details>


### [23] [Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?](https://arxiv.org/abs/2506.05484)
*Ruihua Chen, Bangyu Wu, Meng Li, Kai Yang*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了在全波形反演中使用神经网络重新参数化的方法，比较了预训练和去标准化两种初始模型嵌入方式，发现去标准化方法更为高效。


<details>
  <summary>更多</summary>
  
**动机:** 研究的动机是解决使用不准确初始模型时的稳定反演问题，并探索如何有效地将初始模型的先验知识嵌入到神经网络中。

**方法:** 论文系统地研究了两种初始模型嵌入方式对神经网络重新参数化FWI的影响，通过实验比较了预训练和去标准化的效果。

**结果:** 实验结果表明，与预训练相比，去标准化能够简化流程、加速收敛并提升反演准确性。

**结论:** 论文得出结论，去标准化方法比预训练方法在神经网络重新参数化的全波形反演中更有效，可以简化工作流程，加快收敛速度，并提高反演精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Initial+Model+Incorporation+for+Deep+Learning+FWI%3A+Pretraining+or+Denormalization%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05484，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05484&send_immediately=true&force_search=false)

**原文摘要:** Subsurface property neural network reparameterized full waveform inversion
(FWI) has emerged as an effective unsupervised learning framework, which can
invert stably with an inaccurate starting model. It updates the trainable
neural network parameters instead of fine-tuning on the subsurface model
directly. There are primarily two ways to embed the prior knowledge of the
initial model into neural networks, that is, pretraining and denormalization.
Pretraining first regulates the neural networks' parameters by fitting the
initial velocity model; Denormalization directly adds the outputs of the
network into the initial models without pretraining. In this letter, we
systematically investigate the influence of the two ways of initial model
incorporation for the neural network reparameterized FWI. We demonstrate that
pretraining requires inverting the model perturbation based on a constant
velocity value (mean) with a two-stage implementation. It leads to a complex
workflow and inconsistency of objective functions in the two-stage process,
causing the network parameters to become inactive and lose plasticity.
Experimental results demonstrate that denormalization can simplify workflows,
accelerate convergence, and enhance inversion accuracy compared with
pretraining.

</details>


### [24] [Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models](https://arxiv.org/abs/2506.05497)
*Sima Noorani, Shayan Kiyani, George Pappas, Hamed Hassani*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的不确定性量化框架CPQ，它适用于大型语言模型，并在有限查询条件下提供更丰富的预测信息。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是为了解决传统共形预测方法依赖于结构化输出的问题，并提供一种仅通过有限查询黑盒生成模型来构建预测集的方法。

**方法:** 该论文提出了一种名为Conformal Prediction with Query Oracle (CPQ)的新框架，并基于统计学中的缺失质量问题构建了最优查询策略和从查询样本到预测集的映射。

**结果:** 论文结果显示，CPQ适用于任何黑盒LLM，并且在三个实际开放任务和两个LLMs上的实验表明其有效性和优势。

**结论:** 论文得出结论，CPQ框架在语言模型中能够生成比现有共形方法更具信息量的预测集，并且每个核心原则对CPQ性能都有个体贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conformal+Prediction+Beyond+the+Seen%3A+A+Missing+Mass+Perspective+for+Uncertainty+Quantification+in+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05497，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05497&send_immediately=true&force_search=false)

**原文摘要:** Uncertainty quantification (UQ) is essential for safe deployment of
generative AI models such as large language models (LLMs), especially in high
stakes applications. Conformal prediction (CP) offers a principled uncertainty
quantification framework, but classical methods focus on regression and
classification, relying on geometric distances or softmax scores: tools that
presuppose structured outputs. We depart from this paradigm by studying CP in a
query only setting, where prediction sets must be constructed solely from
finite queries to a black box generative model, introducing a new trade off
between coverage, test time query budget, and informativeness. We introduce
Conformal Prediction with Query Oracle (CPQ), a framework characterizing the
optimal interplay between these objectives. Our finite sample algorithm is
built on two core principles: one governs the optimal query policy, and the
other defines the optimal mapping from queried samples to prediction sets.
Remarkably, both are rooted in the classical missing mass problem in
statistics. Specifically, the optimal query policy depends on the rate of
decay, or the derivative, of the missing mass, for which we develop a novel
estimator. Meanwhile, the optimal mapping hinges on the missing mass itself,
which we estimate using Good Turing estimators. We then turn our focus to
implementing our method for language models, where outputs are vast, variable,
and often under specified. Fine grained experiments on three real world open
ended tasks and two LLMs, show CPQ applicability to any black box LLM and
highlight: (1) individual contribution of each principle to CPQ performance,
and (2) CPQ ability to yield significantly more informative prediction sets
than existing conformal methods for language uncertainty quantification.

</details>


### [25] [A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization](https://arxiv.org/abs/2506.06179)
*Muhammed Ustaomeroglu, Guannan Qu*

**主要类别:** cs.LG

**AI概要:** 本研究分析了self-attention在捕捉实体间交互方面的理论基础与应用潜力，并提出了用于学习特征级和多实体交互的新模块。


<details>
  <summary>更多</summary>
  
**动机:** 尽管self-attention已成为现代神经架构的核心组件，但其理论基础仍不清楚，因此本文试图通过不同领域的交互实体来揭示其本质特性。

**方法:** 通过研究从多智能体强化学习中的代理到遗传序列中的等位基因等一系列交互实体，分析self-attention的理论基础，并引入了HyperFeatureAttention和HyperAttention两个新模块进行实验验证。

**结果:** 研究表明单层线性self-attention可以高效表示和学习捕捉成对交互的函数，包括分布外场景；同时提出了两个新的网络模块以扩展交互学习的能力。

**结论:** 论文得出结论，self-attention可以在最小假设下作为相互作用学习器，能够有效表示、学习和泛化捕捉成对交互的函数，并且适用于多种现实世界领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Theoretical+Study+of+%28Hyper%29+Self-Attention+through+the+Lens+of+Interactions%3A+Representation%2C+Training%2C+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06179，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06179&send_immediately=true&force_search=false)

**原文摘要:** Self-attention has emerged as a core component of modern neural
architectures, yet its theoretical underpinnings remain elusive. In this paper,
we study self-attention through the lens of interacting entities, ranging from
agents in multi-agent reinforcement learning to alleles in genetic sequences,
and show that a single layer linear self-attention can efficiently represent,
learn, and generalize functions capturing pairwise interactions, including
out-of-distribution scenarios. Our analysis reveals that self-attention acts as
a mutual interaction learner under minimal assumptions on the diversity of
interaction patterns observed during training, thereby encompassing a wide
variety of real-world domains. In addition, we validate our theoretical
insights through experiments demonstrating that self-attention learns
interaction functions and generalizes across both population distributions and
out-of-distribution scenarios. Building on our theories, we introduce
HyperFeatureAttention, a novel neural network module designed to learn
couplings of different feature-level interactions between entities.
Furthermore, we propose HyperAttention, a new module that extends beyond
pairwise interactions to capture multi-entity dependencies, such as three-way,
four-way, or general n-way interactions.

</details>


### [26] [Antithetic Noise in Diffusion Models](https://arxiv.org/abs/2506.06185)
*Jing Jia, Sifan Liu, Bowen Song, Wei Yuan, Liyue Shen, Guanyang Wang*

**主要类别:** cs.LG

**AI概要:** 论文研究了扩散模型中反噪声配对的效果，发现其能显著提升图像多样性和估计精度，并提出了一个通用且高效的框架。


<details>
  <summary>更多</summary>
  
**动机:** 为了系统研究扩散模型中反初始噪声的影响，并利用这种影响提升模型性能。

**方法:** 通过将初始噪声与其反噪声配对生成负相关样本，并结合实验和理论分析提出了对称性假设。

**结果:** 发现初始噪声与其反噪声的配对能够产生强负相关样本，从而在Stable Diffusion等模型中增强了图像多样性，并显著提高了不确定性量化估计的准确性。

**结论:** 论文提出了一种基于反噪声配对的方法，可以增强图像多样性和提高不确定性量化的准确性，并且该框架无需训练、模型无关且不增加运行时开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Antithetic+Noise+in+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06185&send_immediately=true&force_search=false)

**原文摘要:** We initiate a systematic study of antithetic initial noise in diffusion
models. Across unconditional models trained on diverse datasets,
text-conditioned latent-diffusion models, and diffusion-posterior samplers, we
find that pairing each initial noise with its negation consistently yields
strongly negatively correlated samples. To explain this phenomenon, we combine
experiments and theoretical analysis, leading to a symmetry conjecture that the
learned score function is approximately affine antisymmetric (odd symmetry up
to a constant shift), and provide evidence supporting it. Leveraging this
negative correlation, we enable two applications: (1) enhancing image diversity
in models like Stable Diffusion without quality loss, and (2) sharpening
uncertainty quantification (e.g., up to 90% narrower confidence intervals) when
estimating downstream statistics. Building on these gains, we extend the
two-point pairing to a randomized quasi-Monte Carlo estimator, which further
improves estimation accuracy. Our framework is training-free, model-agnostic,
and adds no runtime overhead.

</details>


### [27] [Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates](https://arxiv.org/abs/2506.05513)
*Yunfei Huang, David S. Greenberg*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了在阶梯网格上结合物理定律和对称性约束的新型PDE代理模型，有效提升了模型的准确性和泛化能力，尤其是在复杂流体动力学问题上的表现优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统求解器在成本和精度之间存在权衡，而神经PDE代理模型虽然改善了这一问题，但在新初始条件下的泛化能力和长时间预测的误差积累方面表现不佳。因此，研究旨在通过引入物理和对称性约束来弥补这一性能差距。

**方法:** 研究提出了一种新的输入和输出层结构，能够在阶梯网格上尊重物理定律和对称性，并系统地研究这些约束如何影响PDE代理模型的准确性。实验集中在两个具有挑战性的问题上：带封闭边界的浅水方程和衰减不可压缩湍流。

**结果:** 实验结果显示，引入物理约束和对称性显著提升了PDE代理模型的性能，无论是在不同任务、架构、自回归预测步骤、精度衡量标准还是网络规模下都表现一致。尤其是同时使用物理约束和对称性的模型表现最佳，并且能够更好地泛化到训练数据范围之外的初始条件和时间长度，还能更准确地预测真实世界的洋流。

**结论:** 论文得出结论，通过在PDE代理模型中引入物理定律和对称性约束，特别是在阶梯网格上设计的新输入输出层，可以显著提升模型的准确性和泛化能力。结合物理约束和对称性的双重限制效果最好，优于仅使用数据增强或pushforward训练的基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Geometric+and+Physical+Constraints+Synergistically+Enhance+Neural+PDE+Surrogates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05513，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05513&send_immediately=true&force_search=false)

**原文摘要:** Neural PDE surrogates can improve the cost-accuracy tradeoff of classical
solvers, but often generalize poorly to new initial conditions and accumulate
errors over time. Physical and symmetry constraints have shown promise in
closing this performance gap, but existing techniques for imposing these
inductive biases are incompatible with the staggered grids commonly used in
computational fluid dynamics. Here we introduce novel input and output layers
that respect physical laws and symmetries on the staggered grids, and for the
first time systematically investigate how these constraints, individually and
in combination, affect the accuracy of PDE surrogates. We focus on two
challenging problems: shallow water equations with closed boundaries and
decaying incompressible turbulence. Compared to strong baselines, symmetries
and physical constraints consistently improve performance across tasks,
architectures, autoregressive prediction steps, accuracy measures, and network
sizes. Symmetries are more effective than physical constraints, but surrogates
with both performed best, even compared to baselines with data augmentation or
pushforward training, while themselves benefiting from the pushforward trick.
Doubly-constrained surrogates also generalize better to initial conditions and
durations beyond the range of the training data, and more accurately predict
real-world ocean currents.

</details>


### [28] [Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum](https://arxiv.org/abs/2506.05530)
*Snir Hordan, Maya Bechler-Speicher, Gur Lifshitz, Nadav Dym*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了带有光谱特征的图神经网络（SGNNs）的表达能力问题，发现现有模型在某些图上表现不足，并提出了一种提升其表达能力的新方法，同时通过实验验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前SGNNs的表达能力评估框架与图谱之间缺乏良好对应关系，导致对SGNNs表达能力的理解有限。因此，需要一种新的方法来更好地评估和提升SGNNs的表达能力。

**方法:** 作者利用一种经典的根据最大特征值重数对图进行分类的范式，引入了一个SGNNs的表达性层次结构，并适配旋转等变神经网络到图谱设置中以提升SGNNs的表达能力。

**结果:** 作者证明了许多SGNNs即使在具有不同特征值的图上也是不完整的。他们提出了一个新的方法，能够理论上改善SGNNs在简单谱图上的表达能力，并通过MNIST Superpixel数据集上的图像分类实验以及ZINC图集的特征向量规范化实验证明了其方法的有效性。

**结论:** 论文得出结论，许多SGNNs在具有不同特征值的图上仍然是不完整的。为了解决这个问题，作者提出了一种可证明改善SGNNs在简单谱图上的表达能力的方法，并通过实验验证了理论结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectral+Graph+Neural+Networks+are+Incomplete+on+Graphs+with+a+Simple+Spectrum，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05530，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05530&send_immediately=true&force_search=false)

**原文摘要:** Spectral features are widely incorporated within Graph Neural Networks (GNNs)
to improve their expressive power, or their ability to distinguish among
non-isomorphic graphs. One popular example is the usage of graph Laplacian
eigenvectors for positional encoding in MPNNs and Graph Transformers. The
expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated
via the k-WL graph isomorphism test hierarchy and homomorphism counting. Yet,
these frameworks align poorly with the graph spectra, yielding limited insight
into SGNNs' expressive power. We leverage a well-studied paradigm of
classifying graphs by their largest eigenvalue multiplicity to introduce an
expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete
even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt
rotation equivariant neural networks to the graph spectra setting to propose a
method to provably improve SGNNs' expressivity on simple spectrum graphs. We
empirically verify our theoretical claims via an image classification
experiment on the MNIST Superpixel dataset and eigenvector canonicalization on
graphs from ZINC.

</details>


### [29] [SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms](https://arxiv.org/abs/2506.05538)
*Arnesh Batra, Anushk Kumar, Jashn Khemani, Arush Gumber, Arhan Jain, Somil Gupta*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一个针对社交媒体中深度伪造问题的新检测方法，并发布了一个用于评估该问题的数据集。


<details>
  <summary>更多</summary>
  
**动机:** 现有的检测框架难以区分良性与对抗生成的深度伪造，特别是在社交媒体上被用作错误信息活动的情况下。

**方法:** 提出了一种新的基于LLM的多因素检测方法，结合面部识别、自动语音转录和多代理LLM流水线来交叉验证音视频线索。

**结果:** 介绍了一个名为SocialDF的数据集，反映了社交媒体平台上现实世界的深度伪造挑战，并展示了新方法在检测深度伪造方面的有效性。

**结论:** 论文的结论是提出了一种新的基于LLM的多因素检测方法，强调了鲁棒性的多模态验证技术，结合语言、行为和上下文分析来有效识别合成媒体。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SocialDF%3A+Benchmark+Dataset+and+Detection+Model+for+Mitigating+Harmful+Deepfake+Content+on+Social+Media+Platforms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05538，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05538&send_immediately=true&force_search=false)

**原文摘要:** The rapid advancement of deep generative models has significantly improved
the realism of synthetic media, presenting both opportunities and security
challenges. While deepfake technology has valuable applications in
entertainment and accessibility, it has emerged as a potent vector for
misinformation campaigns, particularly on social media. Existing detection
frameworks struggle to distinguish between benign and adversarially generated
deepfakes engineered to manipulate public perception. To address this
challenge, we introduce SocialDF, a curated dataset reflecting real-world
deepfake challenges on social media platforms. This dataset encompasses
high-fidelity deepfakes sourced from various online ecosystems, ensuring broad
coverage of manipulative techniques. We propose a novel LLM-based multi-factor
detection approach that combines facial recognition, automated speech
transcription, and a multi-agent LLM pipeline to cross-verify audio-visual
cues. Our methodology emphasizes robust, multi-modal verification techniques
that incorporate linguistic, behavioral, and contextual analysis to effectively
discern synthetic media from authentic content.

</details>


### [30] [Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data](https://arxiv.org/abs/2506.05542)
*Vlastimil Martinek, Andrea Gariboldi, Dimosthenis Tzimotoudis, Aitor Alberdi Escudero, Edward Blake, David Cechak, Luke Cassar, Alessandro Balestrucci, Panagiotis Alexiou*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Agentomics-ML的完全自主基于代理的系统，用于生成分类模型和必要的可重复训练及推理文件，该系统通过与文件系统的交互完成机器学习实验过程中的各个步骤，并通过反馈机制不断优化模型。


<details>
  <summary>更多</summary>
  
**动机:** 生物数据集的数量、多模态性和异质性要求自动化方法能够产生可泛化的预测模型。尽管基于大型语言模型的代理在结构化基准测试中显示出自动化端到端机器学习实验的潜力，但在应用于异质的计算生物学数据集时，这些方法在泛化和成功率方面存在困难。

**方法:** Agentomics-ML通过Bash与文件系统反复交互来完成机器学习实验过程中的各个步骤，并在生成机器学习模型后，通过训练和验证指标提供标量反馈以识别过拟合等问题，然后创建语言反馈用于未来的迭代，建议对数据表示、模型架构和超参数选择等步骤进行调整。

**结果:** Agentomics-ML在几个已建立的基因组学和转录组学基准数据集上进行了评估，结果表明其在泛化能力和成功率方面均超过了现有最先进的基于代理的方法。虽然由领域专家构建的最先进模型在本工作中使用的大多数计算生物学数据集上的绝对性能仍然领先，但Agentomics-ML缩小了完全自主系统之间的差距，并在一个使用的基准数据集上实现了最先进的性能。

**结论:** Agentomics-ML是一个完全自主的基于代理的系统，能够生成分类模型以及可重复训练和推理的必要文件，并且在一些基因组学和转录组学基准数据集上表现优于现有的最先进的基于代理的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentomics-ML%3A+Autonomous+Machine+Learning+Experimentation+Agent+for+Genomic+and+Transcriptomic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05542&send_immediately=true&force_search=false)

**原文摘要:** The adoption of machine learning (ML) and deep learning methods has
revolutionized molecular medicine by driving breakthroughs in genomics,
transcriptomics, drug discovery, and biological systems modeling. The
increasing quantity, multimodality, and heterogeneity of biological datasets
demand automated methods that can produce generalizable predictive models.
Recent developments in large language model-based agents have shown promise for
automating end-to-end ML experimentation on structured benchmarks. However,
when applied to heterogeneous computational biology datasets, these methods
struggle with generalization and success rates. Here, we introduce
Agentomics-ML, a fully autonomous agent-based system designed to produce a
classification model and the necessary files for reproducible training and
inference. Our method follows predefined steps of an ML experimentation
process, repeatedly interacting with the file system through Bash to complete
individual steps. Once an ML model is produced, training and validation metrics
provide scalar feedback to a reflection step to identify issues such as
overfitting. This step then creates verbal feedback for future iterations,
suggesting adjustments to steps such as data representation, model
architecture, and hyperparameter choices. We have evaluated Agentomics-ML on
several established genomic and transcriptomic benchmark datasets and show that
it outperforms existing state-of-the-art agent-based methods in both
generalization and success rates. While state-of-the-art models built by domain
experts still lead in absolute performance on the majority of the computational
biology datasets used in this work, Agentomics-ML narrows the gap for fully
autonomous systems and achieves state-of-the-art performance on one of the used
benchmark datasets. The code is available at
https://github.com/BioGeMT/Agentomics-ML.

</details>


### [31] [Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning](https://arxiv.org/abs/2506.05568)
*Arian Raje, Baris Askin, Divyansh Jhunjhunwala, Gauri Joshi*

**主要类别:** cs.LG

**AI概要:** Ravan 提出了一种新的联邦学习中的大型语言模型微调方法，该方法通过改进 LoRA 方法提高了模型的准确率。


<details>
  <summary>更多</summary>
  
**动机:** 由于数据和计算异构性导致基于 LoRA 的方法在联邦学习中出现准确率下降，因此需要提出新的解决方案。

**方法:** Ravan 采用了一种自适应多头 LoRA 方法，通过对权重更新进行重新参数化来平衡参数效率和模型表达能力。

**结果:** 实验表明，Ravan 在视觉和语言基准上的测试准确率比以前的参数高效基线提高了 2-8％。

**结论:** Ravan 是一种用于大型语言模型的联邦微调的有效方法，它通过多头 LoRA 方法提高了测试准确性，并且具有可扩展性和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ravan%3A+Multi-Head+Low-Rank+Adaptation+for+Federated+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05568，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05568&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have not yet effectively leveraged the vast
amounts of edge-device data, and federated learning (FL) offers a promising
paradigm to collaboratively fine-tune LLMs without transferring private edge
data to the cloud. To operate within the computation and communication
constraints of edge devices, recent literature on federated fine-tuning of LLMs
proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient
methods. However, LoRA-based methods suffer from accuracy degradation in FL
settings, primarily because of data and computational heterogeneity across
clients. We propose \textsc{Ravan}, an adaptive multi-head LoRA method that
balances parameter efficiency and model expressivity by reparameterizing the
weight updates as the sum of multiple LoRA heads
$s_i\textbf{B}_i\textbf{H}_i\textbf{A}_i$ in which only the core matrices
$\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These
trainable scaling factors let the optimization focus on the most useful heads,
recovering a higher-rank approximation of the full update without increasing
the number of communicated parameters since clients upload $s_i\textbf{H}_i$
directly. Experiments on vision and language benchmarks show that
\textsc{Ravan} improves test accuracy by 2-8\% over prior parameter-efficient
baselines, making it a robust and scalable solution for federated fine-tuning
of LLMs.

</details>


### [32] [Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts](https://arxiv.org/abs/2506.05577)
*Saptarshi Nath, Christos Peridis, Eseoghene Benjamin, Xinran Liu, Soheil Kolouri, Peter Kinnell, Zexin Li, Cong Liu, Shirin Dora, Andrea Soltoggio*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一个名为MOSAIC的代理算法，它允许多个代理在没有协调或集中控制的情况下独立解决问题，并识别、分享和重用有用的知识。


<details>
  <summary>更多</summary>
  
**动机:** 论文的研究动机是探索如何在去中心化的环境中利用先前学到的技能、任务相似性和通信能力来提高代理AI系统的可扩展性、开放性和有益的协作学习动态。

**方法:** 论文提出了一种名为MOSAIC的方法，结合了三种机制：通过神经网络掩码的模块化策略组合、使用Wasserstein嵌入进行知识选择的余弦相似性估计、以及异步通信和策略集成。

**结果:** 实验结果表明，MOSAIC比孤立的学习者具有更高的样本效率，且在某些情况下可以解决孤立学习者无法解决的任务。此外，协作学习和共享机制促使从简单到困难的理想任务课程的出现。

**结论:** 论文得出结论，协作学习在基于代理的系统中能够实现个体和集体层面更好的、持续发展的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Collaborative+Learning+in+Agentic+Systems%3A+A+Collective+AI+is+Greater+Than+the+Sum+of+Its+Parts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05577，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05577&send_immediately=true&force_search=false)

**原文摘要:** Agentic AI has gained significant interest as a research paradigm focused on
autonomy, self-directed learning, and long-term reliability of decision making.
Real-world agentic systems operate in decentralized settings on a large set of
tasks or data distributions with constraints such as limited bandwidth,
asynchronous execution, and the absence of a centralized model or even common
objectives. We posit that exploiting previously learned skills, task
similarities, and communication capabilities in a collective of agentic AI are
challenging but essential elements to enabling scalability, open-endedness, and
beneficial collaborative learning dynamics. In this paper, we introduce Modular
Sharing and Composition in Collective Learning (MOSAIC), an agentic algorithm
that allows multiple agents to independently solve different tasks while also
identifying, sharing, and reusing useful machine-learned knowledge, without
coordination, synchronization, or centralized control. MOSAIC combines three
mechanisms: (1) modular policy composition via neural network masks, (2) cosine
similarity estimation using Wasserstein embeddings for knowledge selection, and
(3) asynchronous communication and policy integration. Results on a set of RL
benchmarks show that MOSAIC has a greater sample efficiency than isolated
learners, i.e., it learns significantly faster, and in some cases, finds
solutions to tasks that cannot be solved by isolated learners. The
collaborative learning and sharing dynamics are also observed to result in the
emergence of ideal curricula of tasks, from easy to hard. These findings
support the case for collaborative learning in agentic systems to achieve
better and continuously evolving performance both at the individual and
collective levels.

</details>


### [33] [TabFlex: Scaling Tabular Learning to Millions with Linear Attention](https://arxiv.org/abs/2506.05584)
*Yuchen Zeng, Tuan Dinh, Wonjun Kang, Andreas C Mueller*

**主要类别:** cs.LG

**AI概要:** TabFlex利用线性注意力机制，在大规模表格分类任务中实现了比现有方法更高的效率和可扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法如TabPFN在小规模表格数据集上表现良好，但在大规模和复杂数据集上难以扩展，因此需要改进以提高效率和可扩展性。

**方法:** 引入线性注意力机制作为复杂度二次的自注意力的可扩展替代方案，并评估其在多种数据集上的性能。

**结果:** TabFlex能够高效处理具有数千特征和数百类别的表格数据，相比TabPFN实现了超过2倍的速度提升，并在多个数据集中超越了25个基线模型。

**结论:** TabFlex通过线性注意力机制提高了TabPFN的效率和可扩展性，尤其在大规模数据集上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TabFlex%3A+Scaling+Tabular+Learning+to+Millions+with+Linear+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05584，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05584&send_immediately=true&force_search=false)

**原文摘要:** Leveraging the in-context learning (ICL) capability of Large Language Models
(LLMs) for tabular classification has gained significant attention for its
training-free adaptability across diverse datasets. Recent advancements, like
TabPFN, excel in small-scale tabular datasets but struggle to scale for large
and complex datasets. Our work enhances the efficiency and scalability of
TabPFN for larger datasets by incorporating linear attention mechanisms as a
scalable alternative to complexity-quadratic self-attention. Our model,
TabFlex, efficiently handles tabular datasets with thousands of features and
hundreds of classes, scaling seamlessly to millions of samples. For instance,
TabFlex processes the poker-hand dataset with over a million samples in just 5
seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a
2x speedup compared to TabPFN and a 1.5x speedup over XGBoost, outperforming 25
tested baselines in terms of efficiency across a diverse range of datasets.
Furthermore, TabFlex remains highly effective on large-scale datasets,
delivering strong performance with significantly reduced computational costs,
especially when combined with data-efficient techniques such as dimensionality
reduction and data sampling.

</details>


### [34] [CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions](https://arxiv.org/abs/2506.05586)
*Isha Puri, Amit Dhurandhar, Tejaswini Pedapati, Kartikeyan Shanmugam, Dennis Wei, Kush R. Varshney*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新型可解释神经网络架构 CoFrNet，其不仅具有强大的表示能力，而且在合成和实际任务中也展现了出色的性能与可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 近年来关于本地后验解释的研究较多，但构建可解释神经网络架构的研究相对较少，因此提出 CoFrNet 来解决这一问题。

**方法:** 设计了一种受连分数启发的新型神经网络架构 CoFrNet，并通过不同策略证明了其作为通用逼近器的能力。

**结果:** CoFrNets 能够高效训练并解释，能够准确建模非线性合成函数并估计特征归因，甚至在某些情况下可以估计高阶项。实验还表明 CoFrNets 在多个真实数据集中表现优异。

**结论:** CoFrNets 在合成和真实数据集上都展示了其优越的表示能力和可解释性，有时甚至接近最先进模型的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoFrNets%3A+Interpretable+Neural+Architecture+Inspired+by+Continued+Fractions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05586，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05586&send_immediately=true&force_search=false)

**原文摘要:** In recent years there has been a considerable amount of research on local
post hoc explanations for neural networks. However, work on building
interpretable neural architectures has been relatively sparse. In this paper,
we present a novel neural architecture, CoFrNet, inspired by the form of
continued fractions which are known to have many attractive properties in
number theory, such as fast convergence of approximations to real numbers. We
show that CoFrNets can be efficiently trained as well as interpreted leveraging
their particular functional form. Moreover, we prove that such architectures
are universal approximators based on a proof strategy that is different than
the typical strategy used to prove universal approximation results for neural
networks based on infinite width (or depth), which is likely to be of
independent interest. We experiment on nonlinear synthetic functions and are
able to accurately model as well as estimate feature attributions and even
higher order terms in some cases, which is a testament to the representational
power as well as interpretability of such architectures. To further showcase
the power of CoFrNets, we experiment on seven real datasets spanning tabular,
text and image modalities, and show that they are either comparable or
significantly better than other interpretable models and multilayer
perceptrons, sometimes approaching the accuracies of state-of-the-art models.

</details>


### [35] [FaCTR: Factorized Channel-Temporal Representation Transformers for Efficient Time Series Forecasting](https://arxiv.org/abs/2506.05597)
*Yash Vijay, Harini Subramanyan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种针对时间序列预测的轻量级时空Transformer模型FaCTR，通过结构化设计解决了传统Transformer的过度参数化问题，并取得了优异的性能和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 解决传统Transformer在时间序列预测中的过度参数化问题，并提高其在低信息密度和复杂依赖结构数据上的表现。

**方法:** 提出了FaCTR，一种轻量级时空Transformer，通过低秩分解机模型注入动态、对称的跨通道交互，并使用可学习门控机制进行多变量条件编码。

**结果:** FaCTR在11个公开预测基准测试中达到了最先进的表现，且参数量仅为竞争模型的约1/50。

**结论:** FaCTR实现了最先进的性能，同时保持了模型的紧凑性和可解释性，并支持自监督预训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FaCTR%3A+Factorized+Channel-Temporal+Representation+Transformers+for+Efficient+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05597&send_immediately=true&force_search=false)

**原文摘要:** While Transformers excel in language and vision-where inputs are semantically
rich and exhibit univariate dependency structures-their architectural
complexity leads to diminishing returns in time series forecasting. Time series
data is characterized by low per-timestep information density and complex
dependencies across channels and covariates, requiring conditioning on
structured variable interactions. To address this mismatch and
overparameterization, we propose FaCTR, a lightweight spatiotemporal
Transformer with an explicitly structural design. FaCTR injects dynamic,
symmetric cross-channel interactions-modeled via a low-rank Factorization
Machine into temporally contextualized patch embeddings through a learnable
gating mechanism. It further encodes static and dynamic covariates for
multivariate conditioning. Despite its compact design, FaCTR achieves
state-of-the-art performance on eleven public forecasting benchmarks spanning
both short-term and long-term horizons, with its largest variant using close to
only 400K parameters-on average 50x smaller than competitive spatiotemporal
transformer baselines. In addition, its structured design enables
interpretability through cross-channel influence scores-an essential
requirement for real-world decision-making. Finally, FaCTR supports
self-supervised pretraining, positioning it as a compact yet versatile
foundation for downstream time series tasks.

</details>


### [36] [When Maximum Entropy Misleads Policy Optimization](https://arxiv.org/abs/2506.05615)
*Ruipeng Zhang, Ya-Chien Chang, Sicun Gao*

**主要类别:** cs.LG

**AI概要:** 该论文研究了MaxEnt强化学习方法在复杂控制任务中的局限性，发现熵最大化虽能增强探索和鲁棒性，但也可能导致策略优化的误导，进而影响性能。


<details>
  <summary>更多</summary>
  
**动机:** MaxEnt强化学习在许多任务中表现良好，但在某些关键控制任务中表现不佳，这促使作者研究其背后的原因。

**方法:** 通过在各种控制问题上的实验分析MaxEnt方法的表现，并探讨熵最大化对策略优化的影响。

**结果:** 实验结果表明，在需要精确、低熵策略的任务中，熵最大化可能导致策略优化的误导，从而影响性能。

**结论:** 该论文得出结论，MaxEnt算法在复杂的控制任务中因熵最大化而可能误导策略优化，从而导致性能问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Maximum+Entropy+Misleads+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05615，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05615&send_immediately=true&force_search=false)

**原文摘要:** The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading
approach for achieving efficient learning and robust performance across many RL
tasks. However, MaxEnt methods have also been shown to struggle with
performance-critical control problems in practice, where non-MaxEnt algorithms
can successfully learn. In this work, we analyze how the trade-off between
robustness and optimality affects the performance of MaxEnt algorithms in
complex control tasks: while entropy maximization enhances exploration and
robustness, it can also mislead policy optimization, leading to failure in
tasks that require precise, low-entropy policies. Through experiments on a
variety of control problems, we concretely demonstrate this misleading effect.
Our analysis leads to better understanding of how to balance reward design and
entropy maximization in challenging control problems.

</details>


### [37] [LFA applied to CNNs: Efficient Singular Value Decomposition of Convolutional Mappings by Local Fourier Analysis](https://arxiv.org/abs/2506.05617)
*Antonia van Betteray, Matthias Rottmann, Karsten Kahl*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新颖且高效的方法，用于计算高维卷积映射的奇异值，其时间复杂度仅为O(N)，相较于现有方法有显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 传统的计算卷积映射奇异值的方法资源消耗大，而现有高效方法虽然使用快速傅里叶变换（FFT）降低了复杂度至O(N log N)，但仍有改进空间。因此，作者希望找到一种更高效的算法以应对高维数据。

**方法:** 利用局部傅里叶分析和卷积算子的移位不变性，提出了一种时间复杂度为O(N)的新方法，并通过理论分析和数值实验验证了该方法的效率。

**结果:** 新方法在给定卷积通道数为常数的情况下，能够以O(N)的时间复杂度计算N个奇异值，并通过数值实验验证了其效率和可扩展性。

**结论:** 论文提出了一种基于局部傅里叶分析的方法，用于高效计算高维卷积映射的全部奇异值，并且如果需要的话，还包括相应的奇异向量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LFA+applied+to+CNNs%3A+Efficient+Singular+Value+Decomposition+of+Convolutional+Mappings+by+Local+Fourier+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05617，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05617&send_immediately=true&force_search=false)

**原文摘要:** The singular values of convolutional mappings encode interesting spectral
properties, which can be used, e.g., to improve generalization and robustness
of convolutional neural networks as well as to facilitate model compression.
However, the computation of singular values is typically very
resource-intensive. The naive approach involves unrolling the convolutional
mapping along the input and channel dimensions into a large and sparse
two-dimensional matrix, making the exact calculation of all singular values
infeasible due to hardware limitations. In particular, this is true for
matrices that represent convolutional mappings with large inputs and a high
number of channels. Existing efficient methods leverage the Fast Fourier
transformation (FFT) to transform convolutional mappings into the frequency
domain, enabling the computation of singular values for matrices representing
convolutions with larger input and channel dimensions. For a constant number of
channels in a given convolution, an FFT can compute N singular values in O(N
log N) complexity. In this work, we propose an approach of complexity O(N)
based on local Fourier analysis, which additionally exploits the shift
invariance of convolutional operators. We provide a theoretical analysis of our
algorithm's runtime and validate its efficiency through numerical experiments.
Our results demonstrate that our proposed method is scalable and offers a
practical solution to calculate the entire set of singular values - along with
the corresponding singular vectors if needed - for high-dimensional
convolutional mappings.

</details>


### [38] [Two-dimensional Taxonomy for N-ary Knowledge Representation Learning Methods](https://arxiv.org/abs/2506.05626)
*Xiaohua Lu, Liubov Tupikina, Mehwish Alam*

**主要类别:** cs.LG

**AI概要:** 这篇论文综述了处理n元关系数据的知识超图和超关系知识图谱方法，提出了一个基于方法论和实体角色/位置意识的二维分类法，并讨论了数据集和未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 知识图谱通常将复杂的n元关系简化为三元组，导致高阶关系细节丢失；而超图表示学习又往往忽视超边中的实体角色，限制了细粒度语义建模。因此，需要结合两者优势的知识超图和超关系知识图谱来更好地捕捉现实世界知识的复杂结构和角色特定语义。

**方法:** 提出了一种两维分类法，一维基于方法论（如翻译模型、张量分解模型等），另一维基于对n元关系中实体角色和位置的认知程度（无认知、位置感知和角色感知方法）

**结果:** 全面回顾了处理n元关系数据的方法，包括知识超图和超关系知识图谱的相关文献，并通过提出的二维分类法对这些方法进行了系统梳理。

**结论:** 这篇论文总结了处理n元关系数据的方法，并提出了一个二维分类法，同时讨论了现有数据集、负采样策略和未来研究的开放性挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-dimensional+Taxonomy+for+N-ary+Knowledge+Representation+Learning+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05626&send_immediately=true&force_search=false)

**原文摘要:** Real-world knowledge can take various forms, including structured,
semi-structured, and unstructured data. Among these, knowledge graphs are a
form of structured human knowledge that integrate heterogeneous data sources
into structured representations but typically reduce complex n-ary relations to
simple triples, thereby losing higher-order relational details. In contrast,
hypergraphs naturally represent n-ary relations with hyperedges, which directly
connect multiple entities together. Yet hypergraph representation learning
often overlooks entity roles in hyperedges, limiting the fine-grained semantic
modelling. To address these issues, knowledge hypergraphs and hyper-relational
knowledge graphs combine the advantages of knowledge graphs and hypergraphs to
better capture the complex structures and role-specific semantics of real-world
knowledge. This survey provides a comprehensive review of methods handling
n-ary relational data, covering both knowledge hypergraphs and hyper-relational
knowledge graphs literatures. We propose a two-dimensional taxonomy: the first
dimension categorises models based on their methodology, i.e.,
translation-based models, tensor factorisation-based models, deep neural
network-based models, logic rules-based models, and hyperedge expansion-based
models. The second dimension classifies models according to their awareness of
entity roles and positions in n-ary relations, dividing them into aware-less,
position-aware, and role-aware approaches. Finally, we discuss existing
datasets, negative sampling strategies, and outline open challenges to inspire
future research.

</details>


### [39] [GP-MoLFormer-Sim: Test Time Molecular Optimization through Contextual Similarity Guidance](https://arxiv.org/abs/2506.05628)
*Jiri Navratil, Jarret Ross, Payel Das, Youssef Mroueh, Samuel C Hoffman, Vijil Chenthamarakshan, Brian Belgodere*

**主要类别:** cs.LG

**AI概要:** This paper proposes GP-MoLFormer-Sim, an efficient training-free method for navigating and sampling molecular space using a generative Chemical Language Model while preserving similarity to target molecules, particularly effective when combined with genetic algorithms.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to design molecules while preserving similarity to a target molecule or property, which is crucial for applications in drug discovery, chemical design, and biology.

**方法:** The paper introduces GP-MoLFormer-Sim, a method that uses contextual representations from a generative Chemical Language Model (CLM) to estimate molecular similarity and adjust autoregressive sampling. It integrates this method into a genetic algorithm for testing.

**结果:** Results show that GP-MoLFormer-Sim, when combined with GA, outperforms existing training-free baseline methods on standard molecular optimization benchmarks involving property optimization, molecular rediscovery, and structure-based drug design.

**结论:** GP-MoLFormer-Sim combined with GA outperforms existing training-free baseline methods in molecular optimization benchmarks when the oracle remains black-box.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GP-MoLFormer-Sim%3A+Test+Time+Molecular+Optimization+through+Contextual+Similarity+Guidance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05628&send_immediately=true&force_search=false)

**原文摘要:** The ability to design molecules while preserving similarity to a target
molecule and/or property is crucial for various applications in drug discovery,
chemical design, and biology. We introduce in this paper an efficient
training-free method for navigating and sampling from the molecular space with
a generative Chemical Language Model (CLM), while using the molecular
similarity to the target as a guide. Our method leverages the contextual
representations learned from the CLM itself to estimate the molecular
similarity, which is then used to adjust the autoregressive sampling strategy
of the CLM. At each step of the decoding process, the method tracks the
distance of the current generations from the target and updates the logits to
encourage the preservation of similarity in generations. We implement the
method using a recently proposed $\sim$47M parameter SMILES-based CLM,
GP-MoLFormer, and therefore refer to the method as GP-MoLFormer-Sim, which
enables a test-time update of the deep generative policy to reflect the
contextual similarity to a set of guide molecules. The method is further
integrated into a genetic algorithm (GA) and tested on a set of standard
molecular optimization benchmarks involving property optimization, molecular
rediscovery, and structure-based drug design. Results show that,
GP-MoLFormer-Sim, combined with GA (GP-MoLFormer-Sim+GA) outperforms existing
training-free baseline methods, when the oracle remains black-box. The findings
in this work are a step forward in understanding and guiding the generative
mechanisms of CLMs.

</details>


### [40] [List-Level Distribution Coupling with Applications to Speculative Decoding and Lossy Compression](https://arxiv.org/abs/2506.05632)
*Joseph Rowan, Buu Phan, Ashish Khisti*

**主要类别:** cs.LG

**AI概要:** 该研究扩展了Gumbel-max采样方法，提出了用于多草案推测采样和分布式压缩的新技术，并展示了其实际优势。


<details>
  <summary>更多</summary>
  
**动机:** 解决概率分布耦合问题的放松版本，旨在提高现有采样方法的性能和适用性。

**方法:** 推广Daliri等人提出的Gumbel-max采样方法，提出列表匹配引理，用于多草案推测采样和分布式有损压缩。

**结果:** 提出了新的采样方法，并建立了接受概率的理论下界；在语言任务和图像数据集上的实验表明新方法具有竞争力。

**结论:** 论文得出了一种新的多草案推测采样机制和分布式有损压缩技术，并通过实验验证了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是List-Level+Distribution+Coupling+with+Applications+to+Speculative+Decoding+and+Lossy+Compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05632，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05632&send_immediately=true&force_search=false)

**原文摘要:** We study a relaxation of the problem of coupling probability distributions --
a list of samples is generated from one distribution and an accept is declared
if any one of these samples is identical to the sample generated from the other
distribution. We propose a novel method for generating samples, which extends
the Gumbel-max sampling suggested in Daliri et al. (arXiv:2408.07978) for
coupling probability distributions. We also establish a corresponding lower
bound on the acceptance probability, which we call the list matching lemma. We
next discuss two applications of our setup. First, we develop a new mechanism
for multi-draft speculative sampling that is simple to implement and achieves
performance competitive with baselines such as SpecTr and SpecInfer across a
range of language tasks. Our method also guarantees a certain degree of drafter
invariance with respect to the output tokens which is not supported by existing
schemes. We also provide a theoretical lower bound on the token level
acceptance probability. As our second application, we consider distributed
lossy compression with side information in a setting where a source sample is
compressed and available to multiple decoders, each with independent side
information. We propose a compression technique that is based on our
generalization of Gumbel-max sampling and show that it provides significant
gains in experiments involving synthetic Gaussian sources and the MNIST image
dataset.

</details>


### [41] [AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization](https://arxiv.org/abs/2506.05634)
*Saeed Hedayatian, Stefanos Nikolaidis*

**主要类别:** cs.LG

**AI概要:** AutoQD是一种自动产生行为描述符的理论方法，它通过嵌入策略占据度量来改进质量-多样性算法的多样性探索能力。


<details>
  <summary>更多</summary>
  
**动机:** QD算法虽然成功但依赖于人工设计的行为描述符，限制了多样性探索的可能性。

**方法:** 利用策略与占据度量之间的等价性，通过嵌入马尔可夫决策过程中的策略占据度量来生成描述符。

**结果:** 实验证明AutoQD能够在多种连续控制任务中发现多样化的策略，而无需预定义行为描述符。

**结论:** AutoQD为在没有领域特定知识的情况下进行开放式学习和自动化行为发现提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoQD%3A+Automatic+Discovery+of+Diverse+Behaviors+with+Quality-Diversity+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05634，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05634&send_immediately=true&force_search=false)

**原文摘要:** Quality-Diversity (QD) algorithms have shown remarkable success in
discovering diverse, high-performing solutions, but rely heavily on
hand-crafted behavioral descriptors that constrain exploration to predefined
notions of diversity. Leveraging the equivalence between policies and occupancy
measures, we present a theoretically grounded approach to automatically
generate behavioral descriptors by embedding the occupancy measures of policies
in Markov Decision Processes. Our method, AutoQD, leverages random Fourier
features to approximate the Maximum Mean Discrepancy (MMD) between policy
occupancy measures, creating embeddings whose distances reflect meaningful
behavioral differences. A low-dimensional projection of these embeddings that
captures the most behaviorally significant dimensions is then used as
behavioral descriptors for off-the-shelf QD methods. We prove that our
embeddings converge to true MMD distances between occupancy measures as the
number of sampled trajectories and embedding dimensions increase. Through
experiments in multiple continuous control tasks we demonstrate AutoQD's
ability in discovering diverse policies without predefined behavioral
descriptors, presenting a well-motivated alternative to prior methods in
unsupervised Reinforcement Learning and QD optimization. Our approach opens new
possibilities for open-ended learning and automated behavior discovery in
sequential decision making settings without requiring domain-specific
knowledge.

</details>


### [42] [Bayesian Inference for Correlated Human Experts and Classifiers](https://arxiv.org/abs/2506.05636)
*Markelle Kelly, Alex Boyd, Sam Showalter, Mark Steyvers, Padhraic Smyth*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种贝叶斯框架，用于结合机器学习模型和专家意见进行预测，有效减少了专家查询次数，降低了成本，同时保持了高预测准确率。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是希望在基于模型输出和人类专家意见进行预测时，尽可能少地查询专家，以降低成本。

**方法:** 论文开发了一种通用的贝叶斯框架，通过联合潜在表示对专家相关性进行建模，并利用预训练分类器的类别概率估计进行基于模型输出和专家意见的预测。

**结果:** 将方法应用于两个实际的医学分类问题以及CIFAR-10H和ImageNet-16H数据集，结果显示相对于基线方法，在保持高预测准确性的同时显著减少了专家查询成本。

**结论:** 论文得出结论，使用所提出的贝叶斯框架可以显著减少查询人类专家的成本，同时保持高预测准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Inference+for+Correlated+Human+Experts+and+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05636，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05636&send_immediately=true&force_search=false)

**原文摘要:** Applications of machine learning often involve making predictions based on
both model outputs and the opinions of human experts. In this context, we
investigate the problem of querying experts for class label predictions, using
as few human queries as possible, and leveraging the class probability
estimates of pre-trained classifiers. We develop a general Bayesian framework
for this problem, modeling expert correlation via a joint latent
representation, enabling simulation-based inference about the utility of
additional expert queries, as well as inference of posterior distributions over
unobserved expert labels. We apply our approach to two real-world medical
classification problems, as well as to CIFAR-10H and ImageNet-16H,
demonstrating substantial reductions relative to baselines in the cost of
querying human experts while maintaining high prediction accuracy.

</details>


### [43] [Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones](https://arxiv.org/abs/2506.05641)
*Andrey Zhmoginov, Jihwan Lee, Mark Sandler*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种将大型Transformer模型转换为针对特定任务的小型模型的方法，并证明其在图像建模中具有更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现代基础模型通常需要大量计算资源，并且可能包含对特定任务无关的知识，因此需要一种更高效的方法来利用这些模型。

**方法:** 研究一种将大型Transformer模型参数转换为较小专用模型参数的技术，并将其应用于图像建模任务。

**结果:** 实验表明，生成的专用模型在图像建模任务中的表现优于通用条件模型。

**结论:** 通过将大型Transformer模型参数映射到较小的特定任务模型，可以更有效地利用模型知识，提高图像建模任务的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Projectable+Models%3A+One-Shot+Generation+of+Small+Specialized+Transformers+from+Large+Ones，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05641&send_immediately=true&force_search=false)

**原文摘要:** Modern Foundation Models (FMs) are typically trained on corpora spanning a
wide range of different data modalities, topics and downstream tasks. Utilizing
these models can be very computationally expensive and is out of reach for most
consumer devices. Furthermore, most of the broad FM knowledge may actually be
irrelevant for a specific task at hand. Here we explore a technique for mapping
parameters of a large Transformer to parameters of a smaller specialized model.
By making this transformation task-specific, we aim to capture a narrower scope
of the knowledge needed for performing a specific task by a smaller model. We
study our method on image modeling tasks, showing that performance of generated
models exceeds that of universal conditional models.

</details>


### [44] [Learning to Weight Parameters for Data Attribution](https://arxiv.org/abs/2506.05647)
*Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的生成模型数据归因方法，通过学习参数重要性权重提升归因准确性和细粒度分析能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法在追溯梯度到训练数据时通常将所有网络参数一视同仁，忽略了不同层编码不同类型信息的事实。

**方法:** 提出了一种通过学习参数重要性权重来进行数据归因的新方法，这种方法不依赖标签数据并能适应模型结构。

**结果:** 新方法能够捕捉哪些训练样本对输出的特定语义方面（如主题、风格或背景）有贡献，并在扩散模型中提升了归因准确性。

**结论:** 该方法提高了生成模型中数据归属的准确性，并提供了关于输出如何借鉴训练数据的细粒度见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Weight+Parameters+for+Data+Attribution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05647，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05647&send_immediately=true&force_search=false)

**原文摘要:** We study data attribution in generative models, aiming to identify which
training examples most influence a given output. Existing methods achieve this
by tracing gradients back to training data. However, they typically treat all
network parameters uniformly, ignoring the fact that different layers encode
different types of information and may thus draw information differently from
the training set. We propose a method that models this by learning parameter
importance weights tailored for attribution, without requiring labeled data.
This allows the attribution process to adapt to the structure of the model,
capturing which training examples contribute to specific semantic aspects of an
output, such as subject, style, or background. Our method improves attribution
accuracy across diffusion models and enables fine-grained insights into how
outputs borrow from training data.

</details>


### [45] [BAQ: Efficient Bit Allocation Quantization for Large Language Models](https://arxiv.org/abs/2506.05664)
*Chao Zhang, Li Wang, Samson Lasaulce, Merouane Debbah*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的量化位宽分配框架，该框架利用Hessian代理的敏感度度量来优化量化过程，从而提高了大语言模型的效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现有方法依赖于均匀或启发式的位宽分配，未能考虑到权重对量化噪声的非均匀敏感性。

**方法:** 提出了一种基于Hessian代理的敏感度度量来分配量化位宽的新框架，并设计了BAQ算法。

**结果:** 实验结果表明，BAQ在125M到30B参数范围内的大型语言模型上，在相同位宽下实现了比GPTQ高达56倍的更低困惑度。

**结论:** BAQ算法在损失最小化和复杂度之间取得了良好的平衡，并且能够被集成到标准量化流水线中，具有较小的开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BAQ%3A+Efficient+Bit+Allocation+Quantization+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05664&send_immediately=true&force_search=false)

**原文摘要:** Post-training model quantization is a widely adopted technique for reducing
the memory and computational costs of large language models (LLMs). However,
most existing methods rely on uniform or heuristic bitwidth assignments,
failing to account for the nonuniform sensitivity of weights to quantization
noise. In this paper, we propose a novel framework for allocating quantization
bitwidths based on sensitivity metrics derived from a Hessian proxy. We make
key assumptions, which allow the layer/component-wise loss function to be
expressed as an explicit function of the bitwidths. This enables a neat
formulation of the bit allocation problem as a convex optimization task, whose
closed-form solution adapts precision across weights to minimize the layer-wise
quantization loss. Inspecting the solution provides several insights (such as
the equal-loss structure), which are then exploited to design the proposed
\textbf{BAQ} (Bit Allocation Quantization) algorithm. The proposed algorithm
achieves a good trade-off between loss minimization and complexity and allows
BAQ to be integrated into standard quantization pipelines with minimal
overhead. Experimental results show that BAQ consistently outperforms GPTQ,
achieving up to 56$\times$ lower perplexity at the same bitwidth on large
language models ranging from 125M to 30B parameters. Leveraging our analytical
results derived from solving the optimal bit allocation problem, we also
provide a theoretical explanation for the observed gains. All codes of this
paper are available at https://github.com/CSU-ModelCompression/BAQ.

</details>


### [46] [Contextually Guided Transformers via Low-Rank Adaptation](https://arxiv.org/abs/2506.05672)
*Andrey Zhmoginov, Jihwan Lee, Max Vladymyrov, Mark Sandler*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的Transformer模型CGT，它通过将上下文直接集成到模型架构中，实现了自我专业化，从而提高了语言建模的效率和适应性。


<details>
  <summary>更多</summary>
  
**动机:** 基于Transformer的大语言模型虽然擅长文本处理，但其对提示的依赖性会引入计算开销，因此需要一种能够自我专业化的模型。

**方法:** 修改Transformer架构以在每个序列位置维护上下文摘要，并基于先前上下文在线更新权重。此外，引入了增强学习上下文表示可解释性的技术。

**结果:** 所提出的CGT模型在合成上下文学习任务和语言建模基准上展示了有效性，并促进了更平滑、一致的上下文编码。

**结论:** 本文提出了一种名为CGT的新型Transformer模型，该模型通过将上下文直接集成到模型架构中，消除了对显式提示的依赖。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Contextually+Guided+Transformers+via+Low-Rank+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05672&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) based on Transformers excel at text processing,
but their reliance on prompts for specialized behavior introduces computational
overhead. We propose a modification to a Transformer architecture that
eliminates the need for explicit prompts by learning to encode context into the
model's weights. Our Contextually Guided Transformer (CGT) model maintains a
contextual summary at each sequence position, allowing it to update the weights
on the fly based on the preceding context. This approach enables the model to
self-specialize, effectively creating a tailored model for processing
information following a given prefix. We demonstrate the effectiveness of our
method on synthetic in-context learning tasks and language modeling benchmarks.
Furthermore, we introduce techniques for enhancing the interpretability of the
learned contextual representations, drawing connections to Variational
Autoencoders and promoting smoother, more consistent context encoding. This
work offers a novel direction for efficient and adaptable language modeling by
integrating context directly into the model's architecture.

</details>


### [47] [Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning Vision Models from DataSeeds' Annotated Imagery](https://arxiv.org/abs/2506.05673)
*Sajjad Abdoli, Freeman Lewin, Gediminas Vasiliauskas, Fabian Schonholz*

**主要类别:** cs.LG

**AI概要:** 本文讨论了一种从以模型为中心到以数据为中心的范式转变，并介绍了DSD数据集作为推动计算机视觉领域发展的解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 传统上以模型为中心的方法正在向以数据为中心的方法转变，强调训练数据的质量、结构和相关性作为模型性能的主要驱动力。

**方法:** 介绍并分析了DSD数据集的定量改进，并公开了评估中使用的代码和训练模型。

**结果:** 论文记录了DSD在特定模型上的定量改进，并通过已知基准进行了验证。

**结论:** 论文介绍了数据集DSD，作为DataSeeds.AI的一部分，旨在推动计算机视觉领域的发展，并提供了一个可扩展的基础用于强大的商业和多模态AI开发。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Peer-Ranked+Precision%3A+Creating+a+Foundational+Dataset+for+Fine-Tuning+Vision+Models+from+DataSeeds%27+Annotated+Imagery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05673&send_immediately=true&force_search=false)

**原文摘要:** The development of modern Artificial Intelligence (AI) models, particularly
diffusion-based models employed in computer vision and image generation tasks,
is undergoing a paradigmatic shift in development methodologies. Traditionally
dominated by a "Model Centric" approach, in which performance gains were
primarily pursued through increasingly complex model architectures and
hyperparameter optimization, the field is now recognizing a more nuanced
"Data-Centric" approach. This emergent framework foregrounds the quality,
structure, and relevance of training data as the principal driver of model
performance. To operationalize this paradigm shift, we introduce the
DataSeeds.AI sample dataset (the "DSD"), initially comprised of approximately
10,610 high-quality human peer-ranked photography images accompanied by
extensive multi-tier annotations. The DSD is a foundational computer vision
dataset designed to usher in a new standard for commercial image datasets.
Representing a small fraction of DataSeeds.AI's 100 million-plus image catalog,
the DSD provides a scalable foundation necessary for robust commercial and
multimodal AI development. Through this in-depth exploratory analysis, we
document the quantitative improvements generated by the DSD on specific models
against known benchmarks and make the code and the trained models used in our
evaluation publicly available.

</details>


### [48] [Topology-aware Neural Flux Prediction Guided by Physics](https://arxiv.org/abs/2506.05676)
*Haoyang Jiang, Jindong Wang, Xingquan Zhu, Yi He*

**主要类别:** cs.LG

**AI概要:** 本文提出一种新框架，通过显式差分矩阵和物理约束提升GNN对有向图高频特征的敏感性，成功应用于真实场景。


<details>
  <summary>更多</summary>
  
**动机:** 传统GNN在处理有向图时难以保留节点信号的高频分量，而这些分量对于建模流动动态至关重要。

**方法:** 结合了1）建模方向梯度的显式差分矩阵和2）强制消息传递符合自然规律的隐式物理约束。

**结果:** 在真实的有向图数据（如水流网络和城市交通流网络）上的评估证明了所提方法的有效性。

**结论:** 论文提出了一种新的框架，使GNN能够敏感于高频分量，从而捕捉有向图的拓扑差异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Topology-aware+Neural+Flux+Prediction+Guided+by+Physics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05676，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05676&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) often struggle in preserving high-frequency
components of nodal signals when dealing with directed graphs. Such components
are crucial for modeling flow dynamics, without which a traditional GNN tends
to treat a graph with forward and reverse topologies equal.To make GNNs
sensitive to those high-frequency components thereby being capable to capture
detailed topological differences, this paper proposes a novel framework that
combines 1) explicit difference matrices that model directional gradients and
2) implicit physical constraints that enforce messages passing within GNNs to
be consistent with natural laws. Evaluations on two real-world directed graph
data, namely, water flux network and urban traffic flow network, demonstrate
the effectiveness of our proposal.

</details>


### [49] [Numerical Investigation of Sequence Modeling Theory using Controllable Memory Functions](https://arxiv.org/abs/2506.05678)
*Haotian Jiang, Zeyu Bao, Shida Wang, Qianxiao Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个用于评估序列模型的合成基准测试框架，通过生成具有不同时间依赖强度的任务来分析模型的行为。


<details>
  <summary>更多</summary>
  
**动机:** 尽管序列建模架构已从循环神经网络和卷积模型发展到Transformer和结构化状态空间模型，但系统地刻画这些架构的优势和局限性仍然是一个根本性的挑战。

**方法:** 生成具有记忆函数和参数的合成目标，以系统地评估不同序列模型的表现。

**结果:** 实验结果确认了现有的理论见解并揭示了新发现，表明所提出的方法能够有效推进对序列建模架构的理解。

**结论:** 论文提出了一种合成基准测试框架，用于评估不同序列模型如何有效地捕捉不同的时间结构，并证明了该方法在推进理论理解方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Numerical+Investigation+of+Sequence+Modeling+Theory+using+Controllable+Memory+Functions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05678，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05678&send_immediately=true&force_search=false)

**原文摘要:** The evolution of sequence modeling architectures, from recurrent neural
networks and convolutional models to Transformers and structured state-space
models, reflects ongoing efforts to address the diverse temporal dependencies
inherent in sequential data. Despite this progress, systematically
characterizing the strengths and limitations of these architectures remains a
fundamental challenge. In this work, we propose a synthetic benchmarking
framework to evaluate how effectively different sequence models capture
distinct temporal structures. The core of this approach is to generate
synthetic targets, each characterized by a memory function and a parameter that
determines the strength of temporal dependence. This setup allows us to produce
a continuum of tasks that vary in temporal complexity, enabling fine-grained
analysis of model behavior concerning specific memory properties. We focus on
four representative memory functions, each corresponding to a distinct class of
temporal structures. Experiments on several sequence modeling architectures
confirm existing theoretical insights and reveal new findings. These results
demonstrate the effectiveness of the proposed method in advancing theoretical
understanding and highlight the importance of using controllable targets with
clearly defined structures for evaluating sequence modeling architectures.

</details>


### [50] [Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization](https://arxiv.org/abs/2506.05680)
*Tailin Zhou, Zhilin Chen, Wenlong Lyu, Zhitang Chen, Danny H. K. Tsang, Jun Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基於擴散模型的框架 ManGO，用於在不與系統互動的情況下進行離線優化，能夠超越訓練數據進行泛化，並在多個領域中表現出優於現有方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 傳統方法可能無法有效處理超出訓練數據的設計空間，導致預測不準確和設計質量低下，因此需要一種能同時考慮設計與評分空間的方法。

**方法:** ManGO 使用擴散模型學習設計與評分之間的整體依賴關係，結合無導數引導的條件生成和自適應推理時間縮放來動態優化去噪路徑。

**结果:** ManGO 在多種領域中優於 24 種單目標和 10 種多目標優化方法，包括合成任務、機器人控制、材料設計等。

**结论:** ManGO 提供了一種新的離線優化方法，在設計和評分空間上實現了良好的泛化能力，為未來的優化問題提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Design-Score+Manifold+to+Guide+Diffusion+Models+for+Offline+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05680，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05680&send_immediately=true&force_search=false)

**原文摘要:** Optimizing complex systems, from discovering therapeutic drugs to designing
high-performance materials, remains a fundamental challenge across science and
engineering, as the underlying rules are often unknown and costly to evaluate.
Offline optimization aims to optimize designs for target scores using
pre-collected datasets without system interaction. However, conventional
approaches may fail beyond training data, predicting inaccurate scores and
generating inferior designs. This paper introduces ManGO, a diffusion-based
framework that learns the design-score manifold, capturing the design-score
interdependencies holistically. Unlike existing methods that treat design and
score spaces in isolation, ManGO unifies forward prediction and backward
generation, attaining generalization beyond training data. Key to this is its
derivative-free guidance for conditional generation, coupled with adaptive
inference-time scaling that dynamically optimizes denoising paths. Extensive
evaluations demonstrate that ManGO outperforms 24 single- and 10
multi-objective optimization methods across diverse domains, including
synthetic tasks, robot control, material design, DNA sequence, and real-world
engineering optimization.

</details>


### [51] [Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR](https://arxiv.org/abs/2506.05683)
*Fardis Nadimi, Payam Abdisarabshali, Kasra Borazjani, Jacob Chakareski, Seyyedali Hosseinalipour*

**主要类别:** cs.LG

**AI概要:** 本文探讨了利用多模态多任务联邦基础模型（FedFMs）提升扩展现实（XR）系统的隐私保护和智能化水平的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 扩展现实（XR）系统提供了沉浸式人机交互的新方式，但其发展受到数据隐私和模型训练等多方面挑战的影响。

**方法:** 提出了一种模块化的FedFMs架构，并引入了SHIFT维度来指导FedFMs在XR中的实现。

**结果:** 论文阐述了FedFMs如何通过整合多模态基础模型与联邦学习，在扩展现实系统中实现情境感知和隐私保护智能。

**结论:** 该论文提出多模态多任务联邦基础模型（FedFMs）可以为扩展现实系统提供变革性的能力，通过结合M3T基础模型的表征能力和联邦学习的隐私保护原则。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Modal+Multi-Task+Federated+Foundation+Models+for+Next-Generation+Extended+Reality+Systems%3A+Towards+Privacy-Preserving+Distributed+Intelligence+in+AR%2FVR%2FMR，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05683，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05683&send_immediately=true&force_search=false)

**原文摘要:** Extended reality (XR) systems, which consist of virtual reality (VR),
augmented reality (AR), and mixed reality (XR), offer a transformative
interface for immersive, multi-modal, and embodied human-computer interaction.
In this paper, we envision that multi-modal multi-task (M3T) federated
foundation models (FedFMs) can offer transformative capabilities for XR systems
through integrating the representational strength of M3T foundation models
(FMs) with the privacy-preserving model training principles of federated
learning (FL). We present a modular architecture for FedFMs, which entails
different coordination paradigms for model training and aggregations. Central
to our vision is the codification of XR challenges that affect the
implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality
diversity, (2) Hardware heterogeneity and system-level constraints, (3)
Interactivity and embodied personalization, (4) Functional/task variability,
and (5) Temporality and environmental variability. We illustrate the
manifestation of these dimensions across a set of emerging and anticipated
applications of XR systems. Finally, we propose evaluation metrics, dataset
requirements, and design tradeoffs necessary for the development of
resource-aware FedFMs in XR. This perspective aims to chart the technical and
conceptual foundations for context-aware privacy-preserving intelligence in the
next generation of XR systems.

</details>


### [52] [Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health](https://arxiv.org/abs/2506.05701)
*Pavel Dolin, Weizhi Li, Gautam Dasarathy, Visar Berisha*

**主要类别:** cs.LG

**AI概要:** 本文讨论了临床AI部署后监测的重要性，并提出了一种基于统计有效性与标签高效性的测试框架，以确保临床AI系统的可靠性与安全性。


<details>
  <summary>更多</summary>
  
**动机:** 目前只有9%的FDA注册的AI医疗工具包含部署后的监测计划，而现有的监测方法通常是手动、零散和被动的，难以适应临床模型运行的动态环境。

**方法:** 文章主张采用基于统计有效性和标签高效性的测试框架，将数据变化检测和模型性能退化视为独立的统计假设检验问题，从而提供明确的误差率保证和可重复性。

**结果:** 提出了一个科学严谨的监测基础，能够支持监管所需的显式误差率保证、正式推断和可重复性，并为技术社区提供了新的研究方向。

**结论:** 基于统计原则的部署后监测可以提高临床AI系统的可靠性和安全性，并推动相关领域的理论和方法发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Statistically+Valid+Post-Deployment+Monitoring+Should+Be+Standard+for+AI-Based+Digital+Health，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05701&send_immediately=true&force_search=false)

**原文摘要:** This position paper argues that post-deployment monitoring in clinical AI is
underdeveloped and proposes statistically valid and label-efficient testing
frameworks as a principled foundation for ensuring reliability and safety in
real-world deployment. A recent review found that only 9% of FDA-registered
AI-based healthcare tools include a post-deployment surveillance plan. Existing
monitoring approaches are often manual, sporadic, and reactive, making them
ill-suited for the dynamic environments in which clinical models operate. We
contend that post-deployment monitoring should be grounded in label-efficient
and statistically valid testing frameworks, offering a principled alternative
to current practices. We use the term "statistically valid" to refer to methods
that provide explicit guarantees on error rates (e.g., Type I/II error), enable
formal inference under pre-defined assumptions, and support
reproducibility--features that align with regulatory requirements.
Specifically, we propose that the detection of changes in the data and model
performance degradation should be framed as distinct statistical hypothesis
testing problems. Grounding monitoring in statistical rigor ensures a
reproducible and scientifically sound basis for maintaining the reliability of
clinical AI systems. Importantly, it also opens new research directions for the
technical community--spanning theory, methods, and tools for statistically
principled detection, attribution, and mitigation of post-deployment model
failures in real-world settings.

</details>


### [53] [Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces](https://arxiv.org/abs/2506.05702)
*Chaofan Pan, Jiafen Liu, Yanhua Li, Linbo Xiong, Fan Min, Wei Wei, Xin Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个考虑动态变化能力的持续学习新问题（CL-DC），并通过AACL框架解决了跨动作空间策略泛化的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习方法通常假设代理能力在动态环境中保持不变，这不符合现实场景。

**方法:** 提出了一种基于动作表示空间的动作自适应持续学习（AACL）框架，并通过编码器-解码器进行微调。

**结果:** 实验结果表明，AACL框架在跨动作空间的策略泛化方面优于现有方法。

**结论:** AACL框架在CL-DC问题上表现出色，实现了跨动作空间的策略泛化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Action-Adaptive+Continual+Learning%3A+Enabling+Policy+Generalization+under+Dynamic+Action+Spaces，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05702&send_immediately=true&force_search=false)

**原文摘要:** Continual Learning (CL) is a powerful tool that enables agents to learn a
sequence of tasks, accumulating knowledge learned in the past and using it for
problem-solving or future task learning. However, existing CL methods often
assume that the agent's capabilities remain static within dynamic environments,
which doesn't reflect real-world scenarios where capabilities dynamically
change. This paper introduces a new and realistic problem: Continual Learning
with Dynamic Capabilities (CL-DC), posing a significant challenge for CL
agents: How can policy generalization across different action spaces be
achieved? Inspired by the cortical functions, we propose an Action-Adaptive
Continual Learning framework (AACL) to address this challenge. Our framework
decouples the agent's policy from the specific action space by building an
action representation space. For a new action space, the encoder-decoder of
action representations is adaptively fine-tuned to maintain a balance between
stability and plasticity. Furthermore, we release a benchmark based on three
environments to validate the effectiveness of methods for CL-DC. Experimental
results demonstrate that our framework outperforms popular methods by
generalizing the policy across action spaces.

</details>


### [54] [Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application](https://arxiv.org/abs/2506.05710)
*Xiucheng Wang, Honggang Jia, Nan Cheng, Dusit Niyato*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于生成式人工智能和扩散模型的新型语义通信框架，具有良好的抗噪能力和广泛的适用性。


<details>
  <summary>更多</summary>
  
**动机:** 为了提升在低信噪比条件下的语义通信性能，并解决接收信号与训练数据分布不匹配的问题。

**方法:** 利用随机微分方程理论建立基础，设计了闭合形式的信噪比与去噪步长关系模型，并引入了数学原理驱动的缩放机制。

**结果:** 实验表明，该框架在低信噪比条件下及分布偏移情况下显著优于传统的基于神经网络的语义通信基线方法。

**结论:** 所提出的框架为未来6G系统中的鲁棒语义传输提供了有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Diffusion+Model+Based+Denoising+Receiver+for+6G+Semantic+Communication%3A+From+Stochastic+Differential+Theory+to+Application，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05710&send_immediately=true&force_search=false)

**原文摘要:** In this paper, a novel semantic communication framework empowered by
generative artificial intelligence (GAI) is proposed, specifically leveraging
the capabilities of diffusion models (DMs). A rigorous theoretical foundation
is established based on stochastic differential equations (SDEs), which
elucidates the denoising properties of DMs in mitigating additive white
Gaussian noise (AWGN) in latent semantic representations. Crucially, a
closed-form analytical relationship between the signal-to-noise ratio (SNR) and
the denoising timestep is derived, enabling the optimal selection of diffusion
parameters for any given channel condition. To address the distribution
mismatch between the received signal and the DM's training data, a
mathematically principled scaling mechanism is introduced, ensuring robust
performance across a wide range of SNRs without requiring model fine-tuning.
Built upon this theoretical insight, we develop a latent diffusion model
(LDM)-based semantic transceiver, wherein a variational autoencoder (VAE) is
employed for efficient semantic compression, and a pretrained DM serves as a
universal denoiser. Notably, the proposed architecture is fully training-free
at inference time, offering high modularity and compatibility with large-scale
pretrained LDMs. This design inherently supports zero-shot generalization and
mitigates the challenges posed by out-of-distribution inputs. Extensive
experimental evaluations demonstrate that the proposed framework significantly
outperforms conventional neural-network-based semantic communication baselines,
particularly under low SNR conditions and distributional shifts, thereby
establishing a promising direction for GAI-driven robust semantic transmission
in future 6G systems.

</details>


### [55] [Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation](https://arxiv.org/abs/2506.05713)
*Zhan Zhuang, Xiequn Wang, Wei Li, Yulong Zhang, Qiushi Huang, Shuhao Chen, Xuehao Wang, Yanbin Wei, Yuhe Nie, Kede Ma, Yu Zhang, Ying Wei*

**主要类别:** cs.LG

**AI概要:** 论文提出了CoTo，这是一种用于改善LoRA优化过程的渐进式训练策略，它通过随机停用适配器来促进更平衡的优化和损失景观的广泛探索，理论分析和实验结果验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** LoRA技术虽然高效，但常常将适配器锁定在次优最小值附近，影响了模型的泛化能力和下游操作的效果，因此需要一种新的训练策略来解决这些问题。

**方法:** 提出了一种名为CoTo的训练策略，该策略通过随机停用适配器来鼓励更平衡的优化和损失景观的广泛探索，并采用合作博弈方法量化每个适配器的边际贡献。

**结果:** 实验表明，CoTo能够持续提升单任务性能、增强多任务合并精度、改进剪枝鲁棒性并减少训练开销，同时与多种LoRA变体兼容。

**结论:** CoTo是一种渐进式训练策略，通过逐步增加适配器的激活概率来改善LoRA的优化过程，从而提高模型的泛化能力和多种下游操作的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Come+Together%2C+But+Not+Right+Now%3A+A+Progressive+Strategy+to+Boost+Low-Rank+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05713，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05713&send_immediately=true&force_search=false)

**原文摘要:** Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient
fine-tuning technique for adapting large foundation models, yet it often locks
adapters into suboptimal minima near their initialization. This hampers model
generalization and limits downstream operators such as adapter merging and
pruning. Here, we propose CoTo, a progressive training strategy that gradually
increases adapters' activation probability over the course of fine-tuning. By
stochastically deactivating adapters, CoTo encourages more balanced
optimization and broader exploration of the loss landscape. We provide a
theoretical analysis showing that CoTo promotes layer-wise dropout stability
and linear mode connectivity, and we adopt a cooperative-game approach to
quantify each adapter's marginal contribution. Extensive experiments
demonstrate that CoTo consistently boosts single-task performance, enhances
multi-task merging accuracy, improves pruning robustness, and reduces training
overhead, all while remaining compatible with diverse LoRA variants. Code is
available at https://github.com/zwebzone/coto.

</details>


### [56] [Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning](https://arxiv.org/abs/2506.05716)
*Adrian Ly, Richard Dazeley, Peter Vamplew, Francisco Cruz, Sunil Aryal*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的算法Ensemble Elastic Step DQN (EEDQN)，它通过结合集成方法和弹性步长更新来改善深度强化学习中的高估偏差和样本效率问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已经提出了许多对深度Q网络(DQN)的算法扩展，但对不同改进之间的相互作用仍缺乏深入的理解。特别是，多步和集成风格的扩展已被证明可以减少高估偏差，从而提高样本效率和算法稳定性。这是该研究的主要动机。

**方法:** 论文提出了一种新颖的算法Ensemble Elastic Step DQN (EEDQN)，它将集成方法与弹性步长更新相结合，以解决深度强化学习中的两个主要问题：高估偏差和样本效率问题。

**结果:** 结果表明，在MinAtar基准测试的所有测试环境中，EEDQN都实现了稳定而强大的性能，优于基线DQN方法，并在大多数MinAtar环境中达到了或超过了最先进的集成DQNs的最终回报。

**结论:** 论文得出结论，系统地结合算法改进具有巨大潜力，并提供了证据，表明在深度强化学习中，仔细整合集成和多步方法可以带来显著增益。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ensemble+Elastic+DQN%3A+A+novel+multi-step+ensemble+approach+to+address+overestimation+in+deep+value-based+reinforcement+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05716&send_immediately=true&force_search=false)

**原文摘要:** While many algorithmic extensions to Deep Q-Networks (DQN) have been
proposed, there remains limited understanding of how different improvements
interact. In particular, multi-step and ensemble style extensions have shown
promise in reducing overestimation bias, thereby improving sample efficiency
and algorithmic stability. In this paper, we introduce a novel algorithm called
Ensemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step
updates to stabilise algorithmic performance. EEDQN is designed to address two
major challenges in deep reinforcement learning: overestimation bias and sample
efficiency. We evaluated EEDQN against standard and ensemble DQN variants
across the MinAtar benchmark, a set of environments that emphasise behavioral
learning while reducing representational complexity. Our results show that
EEDQN achieves consistently robust performance across all tested environments,
outperforming baseline DQN methods and matching or exceeding state-of-the-art
ensemble DQNs in final returns on most of the MinAtar environments. These
findings highlight the potential of systematically combining algorithmic
improvements and provide evidence that ensemble and multi-step methods, when
carefully integrated, can yield substantial gains.

</details>


### [57] [Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data](https://arxiv.org/abs/2506.05721)
*Dumindu Tissera, Omar Awadallah, Muhammad Umair Danish, Ayan Sadhu, Katarina Grolinger*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的多标签分类损失函数设计方法，通过调整损失函数来更好地处理包含大量负数据的情况，从而提升分类性能。


<details>
  <summary>更多</summary>
  
**动机:** 当数据集中包含大量未分配类别的实例（负数据）时，传统的多标签分类方法难以准确识别和分类正实例，因此需要一种更有效的方法来处理这种情况。

**方法:** 通过推导出任何类别存在的可能性，重新设计标准的多标签分类损失函数，并引入一个正则化参数来控制不存在类别的概率对存在类别可能性的贡献。

**结果:** 实验表明，新提出的损失函数相比标准损失函数，在F1分数上提高了最多6.01个百分点，在F2分数上提高了最多8.06个百分点，在平均精度上提高了最多3.11个百分点。

**结论:** 论文提出了一种新的多标签分类损失函数设计方法，通过实验验证了其在多个大规模数据集上的有效性，并且没有增加额外的参数或计算复杂度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Any-Class+Presence+Likelihood+for+Robust+Multi-Label+Classification+with+Abundant+Negative+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05721&send_immediately=true&force_search=false)

**原文摘要:** Multi-label Classification (MLC) assigns an instance to one or more
non-exclusive classes. A challenge arises when the dataset contains a large
proportion of instances with no assigned class, referred to as negative data,
which can overwhelm the learning process and hinder the accurate identification
and classification of positive instances. Nevertheless, it is common in MLC
applications such as industrial defect detection, agricultural disease
identification, and healthcare diagnosis to encounter large amounts of negative
data. Assigning a separate negative class to these instances further
complicates the learning objective and introduces unnecessary redundancies. To
address this challenge, we redesign standard MLC loss functions by deriving a
likelihood of any class being present, formulated by a normalized weighted
geometric mean of the predicted class probabilities. We introduce a
regularization parameter that controls the relative contribution of the absent
class probabilities to the any-class presence likelihood in positive instances.
The any-class presence likelihood complements the multi-label learning by
encouraging the network to become more aware of implicit positive instances and
improve the label classification within those positive instances. Experiments
on large-scale datasets with negative data: SewerML, modified COCO, and
ChestX-ray14, across various networks and base loss functions show that our
loss functions consistently improve MLC performance of their standard loss
counterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in
F2, and 3.11 in mean average precision, all without additional parameters or
computational complexity. Code available at:
https://github.com/ML-for-Sensor-Data-Western/gmean-mlc

</details>


### [58] [Generalized Incremental Learning under Concept Drift across Evolving Data Streams](https://arxiv.org/abs/2506.05736)
*En Yu, Jie Lu, Guangquan Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的无源适应框架CSFA，用于处理概念漂移下的广义增量学习，并通过融合新兴原型与基本表示实现稳定的新类别识别。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法主要解决孤立的分布变化问题，忽视了在有限监督和持续不确定性下标签空间和分布的关键共同演化问题。

**方法:** 提出了Calibrated Source-Free Adaptation (CSFA)框架，包括一种无需训练的原型校准机制和一种新的无源适应算法RSGS最小化。

**结果:** 实验表明，CSFA相比现有最先进的方法具有优越的性能和有效性，在开放世界流场景中实现了稳定的语义和分布适应。

**结论:** CSFA为开放环境中的进化语义和分布提供了一个统一的适应框架，有效解决了概念漂移带来的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Incremental+Learning+under+Concept+Drift+across+Evolving+Data+Streams，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05736，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05736&send_immediately=true&force_search=false)

**原文摘要:** Real-world data streams exhibit inherent non-stationarity characterized by
concept drift, posing significant challenges for adaptive learning systems.
While existing methods address isolated distribution shifts, they overlook the
critical co-evolution of label spaces and distributions under limited
supervision and persistent uncertainty. To address this, we formalize
Generalized Incremental Learning under Concept Drift (GILCD), characterizing
the joint evolution of distributions and label spaces in open-environment
streaming contexts, and propose a novel framework called Calibrated Source-Free
Adaptation (CSFA). First, CSFA introduces a training-free prototype calibration
mechanism that dynamically fuses emerging prototypes with base representations,
enabling stable new-class identification without optimization overhead. Second,
we design a novel source-free adaptation algorithm, i.e., Reliable Surrogate
Gap Sharpness-aware (RSGS) minimization. It integrates sharpness-aware
perturbation loss optimization with surrogate gap minimization, while employing
entropy-based uncertainty filtering to discard unreliable samples. This
mechanism ensures robust distribution alignment and mitigates generalization
degradation caused by uncertainties. Therefore, CSFA establishes a unified
framework for stable adaptation to evolving semantics and distributions in
open-world streaming scenarios. Extensive experiments validate the superior
performance and effectiveness of CSFA compared to state-of-the-art approaches.

</details>


### [59] [Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance](https://arxiv.org/abs/2506.05748)
*Rudransh Agnihotri, Ananya Pandey*

**主要类别:** cs.LG

**AI概要:** 本文提出一种高效的RLHF方法，利用轻量级LoRA和提示工程替代传统大型奖励模型，显著降低成本并提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RLHF流程中奖励模型训练成本高昂，需要数十亿参数及离线阶段，因此亟需一种更高效、低成本的替代方案。

**方法:** 使用一个冻结的7B LLM，并附加一个单行JSON评分标准和一个rank-16 LoRA适配器，进行在线PPO训练，同时引入了HH-Rationales数据集评估解释性。

**结果:** 该方法在RewardBench上达到96.2%的准确率，在GSM-8K任务上使用在线PPO使7B模型达到92%的准确率，显著优于70B基线模型（61.8%）。此外，LoRA评委在人类解释相似度上得分高达9/10。

**结论:** 论文提出了一种轻量级的方法，通过结合提示工程和小型LoRA适配器，替代传统的重型奖励模型，实现了成本效益高且透明的奖励函数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Online+RFT+with+Plug-and-Play+LLM+Judges%3A+Unlocking+State-of-the-Art+Performance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05748，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05748&send_immediately=true&force_search=false)

**原文摘要:** Reward-model training is the cost bottleneck in modern Reinforcement Learning
Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters
and an offline preference-tuning phase. In the proposed method, a frozen,
instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a
rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling
it to serve as a complete substitute for the previously used heavyweight
evaluation models. The plug-and-play judge achieves 96.2% accuracy on
RewardBench, outperforming specialized reward networks ranging from 27B to 70B
parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO
baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K
utilizing online PPO. Thorough ablations indicate that (i) six in context
demonstrations deliver the majority of the zero-to-few-shot improvements
(+2pp), and (ii) the LoRA effectively addresses the remaining disparity,
particularly in the safety and adversarial Chat-Hard segments. The proposed
model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic
HH-RLHF, to examine interpretability, accompanied by human generated
justifications. GPT-4 scoring indicates that our LoRA judge attains
approximately = 9/10 in similarity to human explanations, while zero-shot
judges score around =5/10. These results indicate that the combination of
prompt engineering and tiny LoRA produces a cost effective, transparent, and
easily adjustable reward function, removing the offline phase while achieving
new state-of-the-art outcomes for both static evaluation and online RLHF.

</details>


### [60] [Integrating Spatiotemporal Features in LSTM for Spatially Informed COVID-19 Hospitalization Forecasting](https://arxiv.org/abs/2506.05752)
*Zhongying Wang, Thoai D. Ngo, Hamidreza Zoraghein, Benjamin Lucas, Morteza Karimzadeh*

**主要类别:** cs.LG

**AI概要:** 本文提出一种基于LSTM的新框架，结合SPH特征，显著提升了新冠住院率的预测精度，尤其适用于变异株激增期。


<details>
  <summary>更多</summary>
  
**动机:** 新冠疫情对医疗系统造成了巨大压力，准确及时的住院预测对于有效医疗规划至关重要。然而大多数模型在变异株激增时表现不佳，因此需要更有效的预测方法。

**方法:** 研究提出了一种新的长短期记忆（LSTM）框架，并引入了一个时空特征——住院社交接近度（SPH），基于Facebook的社交连接指数进行构建。数据消融实验验证了SPH的预测能力。

**结果:** 在Delta和Omicron激增期间，该模型在7天、14天、21天和28天预测日分别比集成模型平均高出27、42、54和69例住院病例。SPH被证明具有增强预测模型的能力。

**结论:** 研究表明，通过引入SPH和LSTM框架，显著提高了住院率预测的准确性，特别是在疫情变种激增期间。这项研究不仅改进了预测模型，还强调了在传染病传播建模中使用时空特征的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Spatiotemporal+Features+in+LSTM+for+Spatially+Informed+COVID-19+Hospitalization+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05752&send_immediately=true&force_search=false)

**原文摘要:** The COVID-19 pandemic's severe impact highlighted the need for accurate,
timely hospitalization forecasting to support effective healthcare planning.
However, most forecasting models struggled, especially during variant surges,
when they were needed most. This study introduces a novel Long Short-Term
Memory (LSTM) framework for forecasting daily state-level incident
hospitalizations in the United States. We present a spatiotemporal feature,
Social Proximity to Hospitalizations (SPH), derived from Facebook's Social
Connectedness Index to improve forecasts. SPH serves as a proxy for interstate
population interaction, capturing transmission dynamics across space and time.
Our parallel LSTM architecture captures both short- and long-term temporal
dependencies, and our multi-horizon ensembling strategy balances consistency
and forecasting error. Evaluation against COVID-19 Forecast Hub ensemble models
during the Delta and Omicron surges reveals superiority of our model. On
average, our model surpasses the ensemble by 27, 42, 54, and 69
hospitalizations per state on the $7^{th}$, $14^{th}$, $21^{st}$, and $28^{th}$
forecast days, respectively, during the Omicron surge. Data-ablation
experiments confirm SPH's predictive power, highlighting its effectiveness in
enhancing forecasting models. This research not only advances hospitalization
forecasting but also underscores the significance of spatiotemporal features,
such as SPH, in refining predictive performance in modeling the complex
dynamics of infectious disease spread.

</details>


### [61] [FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts](https://arxiv.org/abs/2506.05755)
*Yang Li, Zhi Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了flowOE，一种基于流匹配模型的新型最优执行策略，其利用模仿学习技术改进现有方法，并在多个测试环境中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 传统最优执行策略（如静态Almgren-Chriss模型）在动态金融市场中表现不佳，因此需要更适应性强的方法。

**方法:** 提出了一种基于流匹配模型的新型模仿学习框架flowOE，并通过精炼损失函数使其能够改进专家行为。

**结果:** 实验表明，在各种市场条件下，flowOE均显著优于传统基准和专门校准的专家模型，实现了更高的利润和更低的风险。

**结论:** flowOE展现出在随机最优执行问题中应用流匹配模型的实用性和潜力，显著优于传统方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlowOE%3A+Imitation+Learning+with+Flow+Policy+from+Ensemble+RL+Experts+for+Optimal+Execution+under+Heston+Volatility+and+Concave+Market+Impacts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05755&send_immediately=true&force_search=false)

**原文摘要:** Optimal execution in financial markets refers to the process of strategically
transacting a large volume of assets over a period to achieve the best possible
outcome by balancing the trade-off between market impact costs and timing or
volatility risks. Traditional optimal execution strategies, such as static
Almgren-Chriss models, often prove suboptimal in dynamic financial markets.
This paper propose flowOE, a novel imitation learning framework based on flow
matching models, to address these limitations. FlowOE learns from a diverse set
of expert traditional strategies and adaptively selects the most suitable
expert behavior for prevailing market conditions. A key innovation is the
incorporation of a refining loss function during the imitation process,
enabling flowOE not only to mimic but also to improve upon the learned expert
actions. To the best of our knowledge, this work is the first to apply flow
matching models in a stochastic optimal execution problem. Empirical
evaluations across various market conditions demonstrate that flowOE
significantly outperforms both the specifically calibrated expert models and
other traditional benchmarks, achieving higher profits with reduced risk. These
results underscore the practical applicability and potential of flowOE to
enhance adaptive optimal execution.

</details>


### [62] [BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning](https://arxiv.org/abs/2506.05762)
*Yunpeng Qing, Shuo Chen, Yixiao Chi, Shunyu Liu, Sixu Lin, Changqing Zou*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种用于离线强化学习的新颖数据增强框架BiTrajDiff，该框架通过同时建模任何中间状态的未来和历史轨迹来提高数据集的多样性。


<details>
  <summary>更多</summary>
  
**动机:** 当前的数据增强技术仅关注从给定状态重建未来轨迹，而忽略了达到这些状态的历史转换的探索。这种单向范式不可避免地阻碍了多样化行为模式的发现，尤其是那些通向可能产生高奖励结果的关键状态的模式。

**方法:** 提出了一种新的离线RL数据增强框架BiTrajDiff，将轨迹生成任务分解为两个独立但互补的扩散过程：一个生成前向轨迹以预测未来动态，另一个生成后向轨迹以追踪必要的历史转换。

**结果:** 在D4RL基准套件上的大量实验表明，BiTrajDiff相比其他先进的DA方法在各种离线RL基线上实现了优越的性能。

**结论:** BiTrajDiff能够利用关键状态作为锚点，扩展状态空间中潜在有价值但未被充分探索的区域，从而促进数据集多样性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BiTrajDiff%3A+Bidirectional+Trajectory+Generation+with+Diffusion+Models+for+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05762&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in offline Reinforcement Learning (RL) have proven that
effective policy learning can benefit from imposing conservative constraints on
pre-collected datasets. However, such static datasets often exhibit
distribution bias, resulting in limited generalizability. To address this
limitation, a straightforward solution is data augmentation (DA), which
leverages generative models to enrich data distribution. Despite the promising
results, current DA techniques focus solely on reconstructing future
trajectories from given states, while ignoring the exploration of history
transitions that reach them. This single-direction paradigm inevitably hinders
the discovery of diverse behavior patterns, especially those leading to
critical states that may have yielded high-reward outcomes. In this work, we
introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework
for offline RL that models both future and history trajectories from any
intermediate states. Specifically, we decompose the trajectory generation task
into two independent yet complementary diffusion processes: one generating
forward trajectories to predict future dynamics, and the other generating
backward trajectories to trace essential history transitions.BiTrajDiff can
efficiently leverage critical states as anchors to expand into potentially
valuable yet underexplored regions of the state space, thereby facilitating
dataset diversity. Extensive experiments on the D4RL benchmark suite
demonstrate that BiTrajDiff achieves superior performance compared to other
advanced DA methods across various offline RL backbones.

</details>


### [63] [Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer](https://arxiv.org/abs/2506.05764)
*Haochuan, Wang*

**主要类别:** cs.LG

**AI概要:** 本文分析加密货币价格动态，探索简单模型与复杂模型在预测短期价格上的表现差异，发现经过适当处理，简单模型同样有效且更高效。


<details>
  <summary>更多</summary>
  
**动机:** 旨在探讨增加额外隐藏层或参数是否真正提升了短期价格预测能力，还是收益主要归因于数据预处理和特征工程。

**方法:** 论文比较了一系列模型，从逻辑回归、XGBoost 到深度架构（DeepLOB, Conv1D+LSTM），并在 BTC/USDT 的 LOB 数据上进行测试。

**结果:** 结果揭示了，在适当的预处理和调优下，简单模型表现优于复杂模型，并且在样本外准确率、延迟和抗噪性方面有优势。

**结论:** 研究发现，通过数据预处理和超参数调优，简单的模型可以匹配甚至超越复杂网络的表现，提供更快的推理速度和更高的可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Microstructural+Dynamics+in+Cryptocurrency+Limit+Order+Books%3A+Better+Inputs+Matter+More+Than+Stacking+Another+Hidden+Layer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05764，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05764&send_immediately=true&force_search=false)

**原文摘要:** Cryptocurrency price dynamics are driven largely by microstructural supply
demand imbalances in the limit order book (LOB), yet the highly noisy nature of
LOB data complicates the signal extraction process. Prior research has
demonstrated that deep-learning architectures can yield promising predictive
performance on pre-processed equity and futures LOB data, but they often treat
model complexity as an unqualified virtue. In this paper, we aim to examine
whether adding extra hidden layers or parameters to "blackbox ish" neural
networks genuinely enhances short term price forecasting, or if gains are
primarily attributable to data preprocessing and feature engineering. We
benchmark a spectrum of models from interpretable baselines, logistic
regression, XGBoost to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT
LOB snapshots sampled at 100 ms to multi second intervals using publicly
available Bybit data. We introduce two data filtering pipelines (Kalman,
Savitzky Golay) and evaluate both binary (up/down) and ternary (up/flat/down)
labeling schemes. Our analysis compares models on out of sample accuracy,
latency, and robustness to noise. Results reveal that, with data preprocessing
and hyperparameter tuning, simpler models can match and even exceed the
performance of more complex networks, offering faster inference and greater
interpretability.

</details>


### [64] [AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation](https://arxiv.org/abs/2506.05768)
*Wenyu Zhu, Jianhui Wang, Bowen Gao, Yinjun Jia, Haichuan Tan, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一个新的虚拟筛选框架，通过三模态对比学习和交叉注意力机制，在结构不确定性下实现了准确的虚拟筛选，并在apo结构上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的虚拟筛选方法在缺乏口袋信息的apo或预测结构上性能显著下降，而这种情况在早期药物发现中更为典型。

**方法:** 该方法包括一个三模态对比学习模块和一个基于交叉注意力的适配器，用于动态聚合候选结合位点。

**结果:** 在新整理的apo结构基准测试中，所提方法显著优于最先进的方法，提高了早期富集因子（EF1%）从11.75到37.19。

**结论:** 本文提出了一种新的虚拟筛选框架，能够在结构不确定性下实现准确的虚拟筛选，并显著优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AANet%3A+Virtual+Screening+under+Structural+Uncertainty+via+Alignment+and+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05768&send_immediately=true&force_search=false)

**原文摘要:** Virtual screening (VS) is a critical component of modern drug discovery, yet
most existing methods--whether physics-based or deep learning-based--are
developed around holo protein structures with known ligand-bound pockets.
Consequently, their performance degrades significantly on apo or predicted
structures such as those from AlphaFold2, which are more representative of
real-world early-stage drug discovery, where pocket information is often
missing. In this paper, we introduce an alignment-and-aggregation framework to
enable accurate virtual screening under structural uncertainty. Our method
comprises two core components: (1) a tri-modal contrastive learning module that
aligns representations of the ligand, the holo pocket, and cavities detected
from structures, thereby enhancing robustness to pocket localization error; and
(2) a cross-attention based adapter for dynamically aggregating candidate
binding sites, enabling the model to learn from activity data even without
precise pocket annotations. We evaluated our method on a newly curated
benchmark of apo structures, where it significantly outperforms
state-of-the-art methods in blind apo setting, improving the early enrichment
factor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong
performance on holo structures. These results demonstrate the promise of our
approach in advancing first-in-class drug discovery, particularly in scenarios
lacking experimentally resolved protein-ligand complexes.

</details>


### [65] [Positional Encoding meets Persistent Homology on Graphs](https://arxiv.org/abs/2506.05814)
*Yogesh Verma, Amauri H. Souza, Vikas Garg*

**主要类别:** cs.LG

**AI概要:** 该论文探讨了改进图神经网络结构信息利用的方法，提出了一个结合位置编码和持久同源的新方法PiPE，以提高表达能力和性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是为了解决消息传递图神经网络局部归纳偏置限制其利用关键结构信息的问题。

**方法:** 论文通过理论分析比较了位置编码和持久同源方法，并设计了一个新的可学习方法PiPE。

**结果:** 研究表明，位置编码和持久同源方法在某些情况下会失败，而新提出的PiPE方法展示了更强的性能。

**结论:** 论文得出结论，位置编码和持久同源方法在表达能力上互不优越，提出了一种新的方法PiPE，其表达能力优于两者。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Positional+Encoding+meets+Persistent+Homology+on+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05814&send_immediately=true&force_search=false)

**原文摘要:** The local inductive bias of message-passing graph neural networks (GNNs)
hampers their ability to exploit key structural information (e.g., connectivity
and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged
as two promising approaches to mitigate this issue. PE schemes endow GNNs with
location-aware features, while PH methods enhance GNNs with multiresolution
topological features. However, a rigorous theoretical characterization of the
relative merits and shortcomings of PE and PH has remained elusive. We bridge
this gap by establishing that neither paradigm is more expressive than the
other, providing novel constructions where one approach fails but the other
succeeds. Our insights inform the design of a novel learnable method, PiPE
(Persistence-informed Positional Encoding), which is provably more expressive
than both PH and PE. PiPE demonstrates strong performance across a variety of
tasks (e.g., molecule property prediction, graph classification, and
out-of-distribution generalization), thereby advancing the frontiers of graph
representation learning. Code is available at
https://github.com/Aalto-QuML/PIPE.

</details>


### [66] [Evaluating Neuron Explanations: A Unified Framework with Sanity Checks](https://arxiv.org/abs/2506.05774)
*Tuomas Oikarinen, Ge Yan, Tsui-Wei Weng*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了如何准确评估神经网络中单位行为的解释性，提出了一些可靠的评估指标，并指出现有的一些评估指标存在问题。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地理解神经网络中各个单元的功能，我们需要了解我们对这些单元行为的解释有多可靠和真实。

**方法:** 作者通过实验和理论结果，比较了现有的评估指标，并提出了两个简单的合理性检查。

**结果:** 许多常用的评估指标未能通过作者提出的合理性检查，并在概念标签发生重大变化后没有改变其评分。

**结论:** 论文得出了一些可靠的评估指标，并提出了未来评估应遵循的指导方针。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Neuron+Explanations%3A+A+Unified+Framework+with+Sanity+Checks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05774&send_immediately=true&force_search=false)

**原文摘要:** Understanding the function of individual units in a neural network is an
important building block for mechanistic interpretability. This is often done
by generating a simple text explanation of the behavior of individual neurons
or units. For these explanations to be useful, we must understand how reliable
and truthful they are. In this work we unify many existing explanation
evaluation methods under one mathematical framework. This allows us to compare
existing evaluation metrics, understand the evaluation pipeline with increased
clarity and apply existing statistical methods on the evaluation. In addition,
we propose two simple sanity checks on the evaluation metrics and show that
many commonly used metrics fail these tests and do not change their score after
massive changes to the concept labels. Based on our experimental and
theoretical results, we propose guidelines that future evaluations should
follow and identify a set of reliable evaluation metrics.

</details>


### [67] [Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling](https://arxiv.org/abs/2506.05831)
*Yihan Xie, Sijing Li, Tianwei Lin, Zhuonan Wang, Chenglin Yang, Yu Zhong, Wenqiao Zhang, Haoyuan Li, Hao Jiang, Fengda Zhang, Qishan Chen, Jun Xiao, Yueting Zhuang, Beng Chin Ooi*

**主要类别:** cs.LG

**AI概要:** 本文提出了Heartcare Suite，包括Heartcare-220K数据集、Heartcare-Bench基准测试和HeartcareGPT模型，旨在提升ECG的多模态理解和诊断能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了提升心电图（ECG）的细粒度诊断和分析能力，需要一个高质量、结构化且全面的多模态数据集以及针对性设计的模型和评价体系。

**方法:** 开发了一个包含数据集（Heartcare-220K）、基准测试（Heartcare-Bench）和基于定制标记器（Beat）的模型（HeartcareGPT）的综合框架。

**结果:** Heartcare Suite 在多个临床相关任务中表现出色，实现了先进的性能，并推动了ECG领域的多模态大语言模型发展。

**结论:** Heartcare Suite 是一个在ECG多模态理解与评估方面表现突出的综合框架，并提供了高质量的数据集和基准测试工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heartcare+Suite%3A+Multi-dimensional+Understanding+of+ECG+with+Raw+Multi-lead+Signal+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05831，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05831&send_immediately=true&force_search=false)

**原文摘要:** We present Heartcare Suite, a multimodal comprehensive framework for
finegrained electrocardiogram (ECG) understanding. It comprises three key
components: (i) Heartcare-220K, a high-quality, structured, and comprehensive
multimodal ECG dataset covering essential tasks such as disease diagnosis,
waveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench,
a systematic and multi-dimensional benchmark designed to evaluate diagnostic
intelligence and guide the optimization of Medical Multimodal Large Language
Models (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored
tokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw
multi-lead signals into semantically rich discrete tokens via duallevel vector
quantization and query-guided bidirectional diffusion mechanism. Built upon
Heartcare-220K, HeartcareGPT achieves strong generalization and SoTA
performance across multiple clinically meaningful tasks. Extensive experiments
demonstrate that Heartcare Suite is highly effective in advancing ECGspecific
multimodal understanding and evaluation. Our project is available at
https://github.com/DCDmllm/Heartcare-Suite .

</details>


### [68] [Exploiting Similarity for Computation and Communication-Efficient Decentralized Optimization](https://arxiv.org/abs/2506.05791)
*Yuki Takezawa, Xiaowen Jiang, Anton Rodomanov, Sebastian U. Stich*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的去中心化优化方法SPDO，在减少通信和计算复杂度方面表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的PDO方法需要对与近端算子相关的子问题求解高度准确，导致计算开销显著。而减少通信复杂性对于高效的去中心化优化至关重要。

**方法:** 提出了一种稳定近端去中心化优化（SPDO）方法，并改进了现有PDO方法的分析，放宽了子问题精度要求并利用平均函数相似性。

**结果:** SPDO方法通过减少所需通信步骤并降低子问题精度要求，取得了更快的收敛速度和更少的计算开销。

**结论:** SPDO方法在PDO框架内实现了最先进的通信和计算复杂性，并通过实验结果证明其显著优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploiting+Similarity+for+Computation+and+Communication-Efficient+Decentralized+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05791，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05791&send_immediately=true&force_search=false)

**原文摘要:** Reducing communication complexity is critical for efficient decentralized
optimization. The proximal decentralized optimization (PDO) framework is
particularly appealing, as methods within this framework can exploit functional
similarity among nodes to reduce communication rounds. Specifically, when local
functions at different nodes are similar, these methods achieve faster
convergence with fewer communication steps. However, existing PDO methods often
require highly accurate solutions to subproblems associated with the proximal
operator, resulting in significant computational overhead. In this work, we
propose the Stabilized Proximal Decentralized Optimization (SPDO) method, which
achieves state-of-the-art communication and computational complexities within
the PDO framework. Additionally, we refine the analysis of existing PDO methods
by relaxing subproblem accuracy requirements and leveraging average functional
similarity. Experimental results demonstrate that SPDO significantly
outperforms existing methods.

</details>


### [69] [Loss Functions for Predictor-based Neural Architecture Search](https://arxiv.org/abs/2506.05869)
*Han Ji, Yuqi Feng, Jiahao Fan, Yanan Sun*

**主要类别:** cs.LG

**AI概要:** 这篇论文对性能预测器中使用的损失函数进行了全面研究，包括回归、排序和加权损失函数，并在多个任务和搜索空间上进行了评估。


<details>
  <summary>更多</summary>
  
**动机:** 性能预测器在神经架构搜索（NAS）中起着关键作用，但其有效性受损失函数选择的影响。虽然传统预测器采用回归损失函数，而最近的方法探索了基于排序的损失函数，但这些损失函数的有效性和特性尚未得到彻底研究。

**方法:** 作者将损失函数分为三类：回归、排序和加权损失函数，并使用一系列与NAS相关的指标在五个搜索空间中的13个任务上评估了八种损失函数。

**结果:** 论文发现特定类别的损失函数可以有效结合以增强基于预测器的NAS，并且该研究能够为各种任务选择合适的损失函数提供实践指导。

**结论:** 论文得出结论，特定类别的损失函数可以有效结合以增强基于预测器的NAS，并且研究结果能够为各种任务选择合适的损失函数提供实践指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Loss+Functions+for+Predictor-based+Neural+Architecture+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05869，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05869&send_immediately=true&force_search=false)

**原文摘要:** Evaluation is a critical but costly procedure in neural architecture search
(NAS). Performance predictors have been widely adopted to reduce evaluation
costs by directly estimating architecture performance. The effectiveness of
predictors is heavily influenced by the choice of loss functions. While
traditional predictors employ regression loss functions to evaluate the
absolute accuracy of architectures, recent approaches have explored various
ranking-based loss functions, such as pairwise and listwise ranking losses, to
focus on the ranking of architecture performance. Despite their success in NAS,
the effectiveness and characteristics of these loss functions have not been
thoroughly investigated. In this paper, we conduct the first comprehensive
study on loss functions in performance predictors, categorizing them into three
main types: regression, ranking, and weighted loss functions. Specifically, we
assess eight loss functions using a range of NAS-relevant metrics on 13 tasks
across five search spaces. Our results reveal that specific categories of loss
functions can be effectively combined to enhance predictor-based NAS.
Furthermore, our findings could provide practical guidance for selecting
appropriate loss functions for various tasks. We hope this work provides
meaningful insights to guide the development of loss functions for
predictor-based methods in the NAS community.

</details>


### [70] [EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator](https://arxiv.org/abs/2506.05797)
*Qianyi Chen, Tianrun Gao, Chenbo Jiang, Tailin Wu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个端到端的等变神经场模拟器EqCollide，用于模拟变形物体及其碰撞，解决了现有数据驱动方法在等变性、碰撞处理和可扩展性方面的不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据驱动方法通常缺乏对物理对称性的等变性，处理碰撞的能力不足，且可扩展性有限。

**方法:** 提出了一种名为EqCollide的方法，包括一个等变编码器将物体几何和速度映射到潜在控制点，随后使用基于等变图神经网络的神经常微分方程通过碰撞感知消息传递来建模控制点之间的相互作用。为了重建速度场，查询依赖于控制点特征的神经场。

**结果:** 实验结果表明，EqCollide在各种物体配置下都能实现准确、稳定的模拟，并且与表现最好的基线模型相比，rollout MSE降低了24.34%到35.82%。此外，该模型能够推广到更多发生碰撞的物体和更长的时间范围内。

**结论:** EqCollide是一个准确、稳定且可扩展的模型，能够模拟不同物体配置下的变形物体及其碰撞，并在输入经过群作用变换后仍保持鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EqCollide%3A+Equivariant+and+Collision-Aware+Deformable+Objects+Neural+Simulator，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05797，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05797&send_immediately=true&force_search=false)

**原文摘要:** Simulating collisions of deformable objects is a fundamental yet challenging
task due to the complexity of modeling solid mechanics and multi-body
interactions. Existing data-driven methods often suffer from lack of
equivariance to physical symmetries, inadequate handling of collisions, and
limited scalability. Here we introduce EqCollide, the first end-to-end
equivariant neural fields simulator for deformable objects and their
collisions. We propose an equivariant encoder to map object geometry and
velocity into latent control points. A subsequent equivariant Graph Neural
Network-based Neural Ordinary Differential Equation models the interactions
among control points via collision-aware message passing. To reconstruct
velocity fields, we query a neural field conditioned on control point features,
enabling continuous and resolution-independent motion predictions. Experimental
results show that EqCollide achieves accurate, stable, and scalable simulations
across diverse object configurations, and our model achieves 24.34% to 35.82%
lower rollout MSE even compared with the best-performing baseline model.
Furthermore, our model could generalize to more colliding objects and extended
temporal horizons, and stay robust to input transformed with group action.

</details>


### [71] [Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution](https://arxiv.org/abs/2506.05937)
*Charmaine Barker, Daniel Bethell, Simos Gerasimou*

**主要类别:** cs.LG

**AI概要:** Conflict-aware Evidential Deep Learning（C-EDL）是一种改进的不确定性量化方法，能够有效增强对抗性和分布外输入的鲁棒性，无需重新训练，并且具有较低的计算开销。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习模型的可靠性对于在高风险应用中的部署至关重要，因为分布外或对抗性输入可能导致不利结果。Evidential Deep Learning（EDL）在此方面存在局限性，容易受到对抗性扰动的影响。

**方法:** Conflict-aware Evidential Deep Learning (C-EDL) 是一种轻量级的后处理不确定性量化方法，通过生成多样化的任务保持变换并量化表示分歧来校准不确定性估计。

**结果:** 实验评估表明，C-EDL显著优于最先进的EDL变体和竞争基线，在多个数据集、攻击类型和不确定性度量中实现了对OOD数据（最多55%）和对抗数据（最多90%）的覆盖减少。

**结论:** C-EDL在提高模型可靠性和对抗性输入检测方面显著优于现有方法，同时保持了高精度和低计算开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantifying+Adversarial+Uncertainty+in+Evidential+Deep+Learning+using+Conflict+Resolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05937，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05937&send_immediately=true&force_search=false)

**原文摘要:** Reliability of deep learning models is critical for deployment in high-stakes
applications, where out-of-distribution or adversarial inputs may lead to
detrimental outcomes. Evidential Deep Learning, an efficient paradigm for
uncertainty quantification, models predictions as Dirichlet distributions of a
single forward pass. However, EDL is particularly vulnerable to adversarially
perturbed inputs, making overconfident errors. Conflict-aware Evidential Deep
Learning (C-EDL) is a lightweight post-hoc uncertainty quantification approach
that mitigates these issues, enhancing adversarial and OOD robustness without
retraining. C-EDL generates diverse, task-preserving transformations per input
and quantifies representational disagreement to calibrate uncertainty estimates
when needed. C-EDL's conflict-aware prediction adjustment improves detection of
OOD and adversarial inputs, maintaining high in-distribution accuracy and low
computational overhead. Our experimental evaluation shows that C-EDL
significantly outperforms state-of-the-art EDL variants and competitive
baselines, achieving substantial reductions in coverage for OOD data (up to
55%) and adversarial data (up to 90%), across a range of datasets, attack
types, and uncertainty metrics.

</details>


### [72] [Option Pricing Using Ensemble Learning](https://arxiv.org/abs/2506.05799)
*Zeyuan Li, Qingdao Huang*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了集成学习在期权定价中的应用，相较于传统机器学习模型展现出更高的预测准确性、更好的结构精简性和更强的抗噪能力，并通过新型实验策略将金融理论与计算方法有序结合。


<details>
  <summary>更多</summary>
  
**动机:** 动机是探索集成学习在期权定价中的应用，因为其具有灵活性、高精度和精细结构的特点，这与机器学习在计算金融中对高预测准确性和降低结构复杂性的需求相符。

**方法:** 论文采用了参数转移的新型实验策略，结合了评分策略和加权评估策略，并研究滑动窗口技术与噪声之间的相互作用。

**结果:** 结果表明，集成学习模型在准确性、局部特征提取和抗噪鲁棒性方面优于经典机器学习模型，并揭示了滑动窗口技术和噪声之间可能存在潜在联系的新模式。

**结论:** 该论文得出结论，集成学习在期权定价中的应用不仅能够实现高预测准确性，还具备减少结构复杂性的潜力，并且与金融理论的基础作用有机结合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Option+Pricing+Using+Ensemble+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05799&send_immediately=true&force_search=false)

**原文摘要:** Ensemble learning is characterized by flexibility, high precision, and
refined structure. As a critical component within computational finance, option
pricing with machine learning requires both high predictive accuracy and
reduced structural complexity-features that align well with the inherent
advantages of ensemble learning. This paper investigates the application of
ensemble learning to option pricing, and conducts a comparative analysis with
classical machine learning models to assess their performance in terms of
accuracy, local feature extraction, and robustness to noise. A novel
experimental strategy is introduced, leveraging parameter transfer across
experiments to improve robustness and realism in financial simulations.Building
upon this strategy, an evaluation mechanism is developed that incorporates a
scoring strategy and a weighted evaluation strategy explicitly emphasizing the
foundational role of financial theory. This mechanism embodies an orderly
integration of theoretical finance and computational methods. In addition, the
study examines the interaction between sliding window technique and noise,
revealing nuanced patterns that suggest a potential connection relevant to
ongoing research in machine learning and data science.

</details>


### [73] [Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting](https://arxiv.org/abs/2506.05941)
*Luka Hobor, Mario Brcic, Lidija Polutnik, Ante Kapetanovic*

**主要类别:** cs.LG

**AI概要:** 本研究全面评估了应用于实体零售数据的预测模型，发现基于树的模型在未插补数据上表现优异，而神经网络模型在处理复杂数据时仍有挑战。


<details>
  <summary>更多</summary>
  
**动机:** 准确的销售预测对于企业规划至关重要，过高或过低的预测都会带来成本增加或销售损失。本文旨在全面评估应用于高分辨率实体零售数据集的预测模型，以解决零售环境中间歇性需求、缺失值和产品频繁更换等问题。

**方法:** 研究比较了基于树的集成模型（如XGBoost和LightGBM）与最先进的神经网络架构（包括N-BEATS、NHITS和时间融合变压器）在各种实验设置下的表现，并评估了不同的数据预处理方法对预测性能的影响。

**结果:** 结果表明，使用基于树的模型在未进行数据插补的情况下，针对个别群体进行建模能够提供更高的预测精度和计算效率。相比之下，神经网络模型虽然受益于先进的插补方法，但在处理实体零售数据的不规则性方面仍存在局限。

**结论:** 论文得出结论，在实体零售数据中，局部建模策略尤其是基于树的模型在非插补数据上表现更优，而神经网络模型虽然借助高级插补方法，但在处理实体零售数据中的不规则情况时仍存在不足。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+Analysis+of+Modern+Machine+Learning+Models+for+Retail+Sales+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05941&send_immediately=true&force_search=false)

**原文摘要:** Accurate forecasting is key for all business planning. When estimated sales
are too high, brick-and-mortar retailers may incur higher costs due to unsold
inventories, higher labor and storage space costs, etc. On the other hand, when
forecasts underestimate the level of sales, firms experience lost sales,
shortages, and impact on the reputation of the retailer in their relevant
market. Accurate forecasting presents a competitive advantage for companies. It
facilitates the achievement of revenue and profit goals and execution of
pricing strategy and tactics. In this study, we provide an exhaustive
assessment of the forecasting models applied to a high-resolution
brick-and-mortar retail dataset. Our forecasting framework addresses the
problems found in retail environments, including intermittent demand, missing
values, and frequent product turnover. We compare tree-based ensembles (such as
XGBoost and LightGBM) and state-of-the-art neural network architectures
(including N-BEATS, NHITS, and the Temporal Fusion Transformer) across various
experimental settings. Our results show that localized modeling strategies
especially those using tree-based models on individual groups with non-imputed
data, consistently deliver superior forecasting accuracy and computational
efficiency. In contrast, neural models benefit from advanced imputation
methods, yet still fall short in handling the irregularities typical of
physical retail data. These results further practical understanding for model
selection in retail environment and highlight the significance of data
preprocessing to improve forecast performance.

</details>


### [74] [Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning](https://arxiv.org/abs/2506.05968)
*Motoki Omura, Kazuki Ota, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada*

**主要类别:** cs.LG

**AI概要:** 本论文探讨了在连续动作空间中使用Bellman最优算子改进actor-critic方法的学习效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 离散动作空间的强化学习算法通常使用Bellman最优性算子，而连续动作空间的算法则用Bellman算子建模Q值，导致样本效率低下。

**方法:** 提出了一种退火方法，从Bellman最优算子逐渐过渡到Bellman算子，以加速学习并减少偏差。

**结果:** 实验表明，结合TD3和SAC的方法在多种任务上显著优于现有方法，提高了性能和对超参数的鲁棒性。

**结论:** 将Bellman最优性纳入actor-critic框架可以提高连续动作空间任务中的学习速度和效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gradual+Transition+from+Bellman+Optimality+Operator+to+Bellman+Operator+in+Online+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05968，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05968&send_immediately=true&force_search=false)

**原文摘要:** For continuous action spaces, actor-critic methods are widely used in online
reinforcement learning (RL). However, unlike RL algorithms for discrete
actions, which generally model the optimal value function using the Bellman
optimality operator, RL algorithms for continuous actions typically model
Q-values for the current policy using the Bellman operator. These algorithms
for continuous actions rely exclusively on policy updates for improvement,
which often results in low sample efficiency. This study examines the
effectiveness of incorporating the Bellman optimality operator into
actor-critic frameworks. Experiments in a simple environment show that modeling
optimal values accelerates learning but leads to overestimation bias. To
address this, we propose an annealing approach that gradually transitions from
the Bellman optimality operator to the Bellman operator, thereby accelerating
learning while mitigating bias. Our method, combined with TD3 and SAC,
significantly outperforms existing approaches across various locomotion and
manipulation tasks, demonstrating improved performance and robustness to
hyperparameters related to optimality.

</details>


### [75] [On Measuring Long-Range Interactions in Graph Neural Networks](https://arxiv.org/abs/2506.05971)
*Jacob Bamberger, Benjamin Gutteridge, Scott le Roux, Michael M. Bronstein, Xiaowen Dong*

**主要类别:** cs.LG

**AI概要:** 这篇论文旨在解决图神经网络中的长程交互问题，提出了一种新的衡量图上算子范围的方法，并通过合成实验加以验证，从而评估现有任务和架构的长程特性。


<details>
  <summary>更多</summary>
  
**动机:** 当前对图神经网络中长程交互的研究缺乏稳健性和理论基础，因此需要一个更原则性的方法来描述长程问题。

**方法:** 论文通过形式化图任务中的长程交互，引入了一个用于衡量图上算子范围的度量标准，并使用合成实验对其进行验证。随后，利用该度量标准来分析常见的任务和架构是否具有长程特性。

**结果:** 论文成功地提出了一个衡量图上算子范围的新方法，通过合成实验验证了该方法的有效性，并对现有任务和架构进行了评估，揭示了它们在多大程度上具备长程特性。

**结论:** 该论文提出了一种衡量图上算子范围的方法，并用合成实验验证了其有效性。作者利用这一方法评估了常见任务和架构的长程特性，认为他们的工作推动了解决图上长程问题的努力，并将有助于新数据集和架构的评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Measuring+Long-Range+Interactions+in+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05971&send_immediately=true&force_search=false)

**原文摘要:** Long-range graph tasks -- those dependent on interactions between distant
nodes -- are an open problem in graph neural network research. Real-world
benchmark tasks, especially the Long Range Graph Benchmark, have become popular
for validating the long-range capability of proposed architectures. However,
this is an empirical approach that lacks both robustness and theoretical
underpinning; a more principled characterization of the long-range problem is
required. To bridge this gap, we formalize long-range interactions in graph
tasks, introduce a range measure for operators on graphs, and validate it with
synthetic experiments. We then leverage our measure to examine commonly used
tasks and architectures, and discuss to what extent they are, in fact,
long-range. We believe our work advances efforts to define and address the
long-range problem on graphs, and that our range measure will aid evaluation of
new datasets and architectures.

</details>


### [76] [Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible Representation Learning](https://arxiv.org/abs/2506.05826)
*Ngoc Bui, Menglin Yang, Runjin Chen, Leonardo Neves, Mingxuan Ju, Rex Ying, Neil Shah, Tong Zhao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，通过使用双曲线几何和对比对齐损失函数，解决了模型更新过程中存在的兼容性问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的欧几里得空间中的兼容方法忽视了旧嵌入模型的不确定性，并且强制新模型重构过时的表示，阻碍了新模型的学习过程。

**方法:** 将嵌入提升到双曲线空间，并引入一个强大的对比对齐损失函数，根据旧嵌入的不确定性动态调整对齐权重。

**结果:** 实验验证了所提出的方法在实现兼容性方面的优越性。

**结论:** 论文提出了一种基于双曲线几何的方法来提高模型更新的兼容性，从而实现更强大的机器学习系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Along+the+Arrow+of+Time%3A+Hyperbolic+Geometry+for+Backward-Compatible+Representation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05826，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05826&send_immediately=true&force_search=false)

**原文摘要:** Backward compatible representation learning enables updated models to
integrate seamlessly with existing ones, avoiding to reprocess stored data.
Despite recent advances, existing compatibility approaches in Euclidean space
neglect the uncertainty in the old embedding model and force the new model to
reconstruct outdated representations regardless of their quality, thereby
hindering the learning process of the new model. In this paper, we propose to
switch perspectives to hyperbolic geometry, where we treat time as a natural
axis for capturing a model's confidence and evolution. By lifting embeddings
into hyperbolic space and constraining updated embeddings to lie within the
entailment cone of the old ones, we maintain generational consistency across
models while accounting for uncertainties in the representations. To further
enhance compatibility, we introduce a robust contrastive alignment loss that
dynamically adjusts alignment weights based on the uncertainty of the old
embeddings. Experiments validate the superiority of the proposed method in
achieving compatibility, paving the way for more resilient and adaptable
machine learning systems.

</details>


### [77] [AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification](https://arxiv.org/abs/2506.05980)
*Geonwoo Cho, Jaemoon Lee, Jaegyun Im, Subi Lee, Jihwan Lee, Sundong Kim*

**主要类别:** cs.LG

**AI概要:** 本文提出了AMPED方法，通过梯度手术技术和技能选择模块，解决了技能强化学习中探索与多样性的冲突，取得了优异性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的技能强化学习方法难以同时优化探索与技能多样性这两个冲突的目标，需要一种能显式协调两者的新方法。

**方法:** 提出了一种新的多目标梯度手术技术用于平衡探索和技能多样性，并设计了一个技能选择模块，以动态选择适合下游任务的技能。

**结果:** 在多个基准测试中，AMPED表现出了超越现有SBRL方法的性能，证明了其在促进鲁棒性和泛化性技能学习方面的有效性。

**结论:** AMPED方法在技能预训练和微调阶段引入了创新机制，有效平衡了探索与技能多样性目标，实现了优于现有SBRL基线的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AMPED%3A+Adaptive+Multi-objective+Projection+for+balancing+Exploration+and+skill+Diversification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05980，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05980&send_immediately=true&force_search=false)

**原文摘要:** Skill-based reinforcement learning (SBRL) enables rapid adaptation in
environments with sparse rewards by pretraining a skill-conditioned policy.
Effective skill learning requires jointly maximizing both exploration and skill
diversity. However, existing methods often face challenges in simultaneously
optimizing for these two conflicting objectives. In this work, we propose a new
method, Adaptive Multi-objective Projection for balancing Exploration and skill
Diversification (AMPED), which explicitly addresses both exploration and skill
diversification. We begin by conducting extensive ablation studies to identify
and define a set of objectives that effectively capture the aspects of
exploration and skill diversity, respectively. During the skill pretraining
phase, AMPED introduces a gradient surgery technique to balance the objectives
of exploration and skill diversity, mitigating conflicts and reducing reliance
on heuristic tuning. In the subsequent fine-tuning phase, AMPED incorporates a
skill selector module that dynamically selects suitable skills for downstream
tasks, based on task-specific performance signals. Our approach achieves
performance that surpasses SBRL baselines across various benchmarks. These
results highlight the importance of explicitly harmonizing exploration and
diversity and demonstrate the effectiveness of AMPED in enabling robust and
generalizable skill learning. Project Page: https://geonwoo.me/amped/

</details>


### [78] [TRUST: Test-time Resource Utilization for Superior Trustworthiness](https://arxiv.org/abs/2506.06048)
*Haripriya Harikumar, Santu Rana*

**主要类别:** cs.LG

**AI概要:** 本文提出一种解决分类器权重噪声影响的测试时优化方法，显著提升了不确定性估计的可靠性，并在多个任务和模型中展现了优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的不确定性估计技术（如dropout）难以区分可靠和不可靠的预测，因为分类器权重中的噪声会影响细粒度统计信息的有效性。

**方法:** 该研究通过解决分类器权重中的噪声问题，在测试阶段采用优化方法以生成更可靠的置信度估计。

**结果:** 新方法定义了一个单调子集选择函数，随着去除低分样本，总体准确率持续提升，并且在AUSE和AURC等指标上表现优异。同时能够有效识别训练与测试数据分布差异，并区分CNN与ViT分类器的关键差异。

**结论:** 论文提出了一种新的测试时间优化方法，可以更准确地估计预测的置信度，并在多个评估指标和模型类型中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRUST%3A+Test-time+Resource+Utilization+for+Superior+Trustworthiness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06048&send_immediately=true&force_search=false)

**原文摘要:** Standard uncertainty estimation techniques, such as dropout, often struggle
to clearly distinguish reliable predictions from unreliable ones. We attribute
this limitation to noisy classifier weights, which, while not impairing overall
class-level predictions, render finer-level statistics less informative. To
address this, we propose a novel test-time optimization method that accounts
for the impact of such noise to produce more reliable confidence estimates.
This score defines a monotonic subset-selection function, where population
accuracy consistently increases as samples with lower scores are removed, and
it demonstrates superior performance in standard risk-based metrics such as
AUSE and AURC. Additionally, our method effectively identifies discrepancies
between training and test distributions, reliably differentiates
in-distribution from out-of-distribution samples, and elucidates key
differences between CNN and ViT classifiers across various vision datasets.

</details>


### [79] [Wavelet-based Disentangled Adaptive Normalization for Non-stationary Times Series Forecasting](https://arxiv.org/abs/2506.05857)
*Junpeng Lin, Tian Lan, Bo Zhang, Ke Lin, Dandan Miao, Huiru He, Jiantao Ye, Chen Zhang, Yan-fu Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的时间序列预测方法WDAN，该方法通过解决非平稳性问题显著提高了预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 非平稳时间序列的统计特性随时间变化，使得深度模型难以很好地泛化，而现有的方法忽略了时间序列的多成分性质。

**方法:** 使用基于小波的解耦自适应归一化（WDAN）框架，将输入分解为低频趋势和高频波动，并对每个部分应用定制的归一化策略。

**结果:** 在多个基准数据集上进行了广泛的实验，结果表明WDAN能够持续提高各种模型的时间序列预测精度。

**结论:** WDAN可以有效提高时间序列预测的准确性，并且适用于多种模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wavelet-based+Disentangled+Adaptive+Normalization+for+Non-stationary+Times+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05857&send_immediately=true&force_search=false)

**原文摘要:** Forecasting non-stationary time series is a challenging task because their
statistical properties often change over time, making it hard for deep models
to generalize well. Instance-level normalization techniques can help address
shifts in temporal distribution. However, most existing methods overlook the
multi-component nature of time series, where different components exhibit
distinct non-stationary behaviors. In this paper, we propose Wavelet-based
Disentangled Adaptive Normalization (WDAN), a model-agnostic framework designed
to address non-stationarity in time series forecasting. WDAN uses discrete
wavelet transforms to break down the input into low-frequency trends and
high-frequency fluctuations. It then applies tailored normalization strategies
to each part. For trend components that exhibit strong non-stationarity, we
apply first-order differencing to extract stable features used for predicting
normalization parameters. Extensive experiments on multiple benchmarks
demonstrate that WDAN consistently improves forecasting accuracy across various
backbone model. Code is available at this repository:
https://github.com/MonBG/WDAN.

</details>


### [80] [Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/abs/2506.06105)
*Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Tjarko Lange*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Text-to-LoRA (T2L) 的新方法，该方法通过自然语言描述实现大型语言模型的快速、低成本适应，克服了传统微调技术的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的微调技术需要昂贵且耗时的训练，并对超参数选择敏感，因此需要一种更高效、简便的模型适应方法。

**方法:** 通过训练一个超网络（hypernetwork）来构建LoRAs，仅需自然语言描述目标任务即可进行模型调整。

**结果:** T2L能够在单次前向传递中重建LoRA实例，其性能与特定任务适配器相匹配，同时能够压缩数百个LoRA实例并对未见过的任务进行零样本泛化。

**结论:** T2L提供了一种有效的方法来简化和加速大型语言模型的适应过程，使得基于语言描述的任务特定化成为可能，并且计算需求极低。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Text-to-LoRA%3A+Instant+Transformer+Adaption，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06105，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06105&send_immediately=true&force_search=false)

**原文摘要:** While Foundation Models provide a general tool for rapid content creation,
they regularly require task-specific adaptation. Traditionally, this exercise
involves careful curation of datasets and repeated fine-tuning of the
underlying model. Fine-tuning techniques enable practitioners to adapt
foundation models for many new applications but require expensive and lengthy
training while being notably sensitive to hyperparameter choices. To overcome
these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting
large language models (LLMs) on the fly solely based on a natural language
description of the target task. T2L is a hypernetwork trained to construct
LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9
pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc
reconstructed LoRA instances match the performance of task-specific adapters
across the corresponding test sets. Furthermore, T2L can compress hundreds of
LoRA instances and zero-shot generalize to entirely unseen tasks. This approach
provides a significant step towards democratizing the specialization of
foundation models and enables language-based adaptation with minimal compute
requirements.
  Our code is available at https://github.com/SakanaAI/text-to-lora

</details>


### [81] [Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness](https://arxiv.org/abs/2506.06112)
*Cheng-Long Wang, Qi Li, Zihang Xiang, Yinzhi Cao, Di Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了 IAM 框架用于高效评估机器遗忘效果，解决了现有方法资源消耗大和不适用于近似遗忘的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的 Membership Inference Attacks (MIAs) 方法计算成本过高且难以有效评估近似遗忘带来的细微变化，因此需要新的解决方案。

**方法:** 提出 Interpolated Approximate Measurement (IAM) 框架，通过插值模型在查询样本上的泛化-拟合行为差距来量化样本级的遗忘完整性。

**结果:** IAM 在精确遗忘的二分类包含测试中表现良好，在近似遗忘中与模型变化高度相关，且适用于大规模语言模型，仅需一个预训练影子模型。

**结论:** IAM 是一种有效的机器遗忘推理框架，能够高效评估精确和近似遗忘算法的风险，并揭示了当前方法存在过度遗忘和遗忘不足的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Lifecycle+Unlearning+Commitment+Management%3A+Measuring+Sample-level+Unlearning+Completeness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06112，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06112&send_immediately=true&force_search=false)

**原文摘要:** Growing concerns over data privacy and security highlight the importance of
machine unlearning--removing specific data influences from trained models
without full retraining. Techniques like Membership Inference Attacks (MIAs)
are widely used to externally assess successful unlearning. However, existing
methods face two key limitations: (1) maximizing MIA effectiveness (e.g., via
online attacks) requires prohibitive computational resources, often exceeding
retraining costs; (2) MIAs, designed for binary inclusion tests, struggle to
capture granular changes in approximate unlearning. To address these
challenges, we propose the Interpolated Approximate Measurement (IAM), a
framework natively designed for unlearning inference. IAM quantifies
sample-level unlearning completeness by interpolating the model's
generalization-fitting behavior gap on queried samples. IAM achieves strong
performance in binary inclusion tests for exact unlearning and high correlation
for approximate unlearning--scalable to LLMs using just one pre-trained shadow
model. We theoretically analyze how IAM's scoring mechanism maintains
performance efficiently. We then apply IAM to recent approximate unlearning
algorithms, revealing general risks of both over-unlearning and
under-unlearning, underscoring the need for stronger safeguards in approximate
unlearning systems. The code is available at
https://github.com/Happy2Git/Unlearning_Inference_IAM.

</details>


### [82] [BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures](https://arxiv.org/abs/2506.05871)
*Xiannan Hu, Tianyou Zeng, Xiaoming Yuan, Liwei Song, Guangyuan Zhang, Bangzheng He*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为BestServe的框架，用于快速评估和选择大型语言模型的服务策略，具有高效、低成本和强扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 为数百万用户提供大型语言模型（LLMs）服务需要有效的资源分配和并行策略，而找到这样的策略通常是一个耗时且依赖试错的过程。

**方法:** 基于改进的roofline模型和CPU-GPU调度动态构建推理模拟器，并通过该模拟器评估不同场景下的goodput来选择最优策略。

**结果:** 在各种操作场景下预测结果误差控制在20%以内，同时避免了昂贵的基准测试需求。

**结论:** BestServe是一个高效的LLM服务策略框架，可以在单个标准CPU上快速确定最佳策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BestServe%3A+Serving+Strategies+with+Optimal+Goodput+in+Collocation+and+Disaggregation+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05871&send_immediately=true&force_search=false)

**原文摘要:** Serving large language models (LLMs) to millions of users requires efficient
resource allocation and parallelism strategies. It is a labor intensive
trial-and-error process to find such a strategy. We present BestServe, a novel
framework for ranking serving strategies by estimating goodput under various
operating scenarios. Supporting both collocated and disaggregated
architectures, BestServe leverages an inference simulator built on an adapted
roofline model and CPU-GPU dispatch dynamics. Our framework determines the
optimal strategy in minutes on a single standard CPU, eliminating the need for
costly benchmarking, while achieving predictions within a $20\%$ error margin.
It appeals to be practical for rapid deployment planning because of its
lightweight design and strong extensibility.

</details>


### [83] [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/abs/2506.06166)
*Tianyi Alex Qiu, Zhonghao He, Tejasveer Chugh, Max Kleiman-Weiner*

**主要类别:** cs.LG

**AI概要:** 论文简要指出，大型语言模型与人类用户之间的反馈循环可能导致观点多样性的下降，并通过模拟和实际数据分析验证了这一假设。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是关注LLMs在训练和部署中形成的人类用户反馈循环可能带来的负面影响，例如信息茧房和错误信念的强化。

**方法:** 论文通过基于代理的LLM模拟和对真实GPT使用数据的经验测试来验证假设。

**结果:** 分析显示，在新GPT版本发布后出现了观点多样性突然且持续的下降，这与提出的人工智能反馈回路假设一致。

**结论:** 论文得出结论，大型语言模型（LLMs）与人类用户之间的反馈循环可能导致用户现有价值观和信念的固化，造成观点多样性的丧失以及错误信念的锁定。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Lock-in+Hypothesis%3A+Stagnation+by+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06166，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06166&send_immediately=true&force_search=false)

**原文摘要:** The training and deployment of large language models (LLMs) create a feedback
loop with human users: models learn human beliefs from data, reinforce these
beliefs with generated content, reabsorb the reinforced beliefs, and feed them
back to users again and again. This dynamic resembles an echo chamber. We
hypothesize that this feedback loop entrenches the existing values and beliefs
of users, leading to a loss of diversity and potentially the lock-in of false
beliefs. We formalize this hypothesis and test it empirically with agent-based
LLM simulations and real-world GPT usage data. Analysis reveals sudden but
sustained drops in diversity after the release of new GPT iterations,
consistent with the hypothesized human-AI feedback loop. Code and data
available at https://thelockinhypothesis.com

</details>


### [84] [Interpretable Clustering Ensemble](https://arxiv.org/abs/2506.05877)
*Hang Lv, Lianyu Hu, Mudi Jiang, Xinying Liu, Zengyou He*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种创新的可解释聚类集成方法，在保证聚类质量的同时提升了模型的可解释性，适用于需要透明决策的关键领域。


<details>
  <summary>更多</summary>
  
**动机:** 现有的聚类集成方法大多忽视了高风险应用中的可解释性需求，如医疗诊断和金融风险评估，因此需要一种既准确又可解释的算法以确保透明和可信的决策过程。

**方法:** 提出了一种新的可解释聚类集成方法，该方法利用决策树和统计关联测试来提高模型的可解释性同时保持较高的聚类质量。

**结果:** 实验结果表明，所提出的算法在性能上与最先进的聚类集成方法相当，同时还具有额外的可解释性优势。

**结论:** 本文提出了首个用于聚类集成的可解释算法，通过将基础划分视为分类变量，并在原始特征空间中构建决策树，使用统计关联检验指导树的构建过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+Clustering+Ensemble，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05877，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05877&send_immediately=true&force_search=false)

**原文摘要:** Clustering ensemble has emerged as an important research topic in the field
of machine learning. Although numerous methods have been proposed to improve
clustering quality, most existing approaches overlook the need for
interpretability in high-stakes applications. In domains such as medical
diagnosis and financial risk assessment, algorithms must not only be accurate
but also interpretable to ensure transparent and trustworthy decision-making.
Therefore, to fill the gap of lack of interpretable algorithms in the field of
clustering ensemble, we propose the first interpretable clustering ensemble
algorithm in the literature. By treating base partitions as categorical
variables, our method constructs a decision tree in the original feature space
and use the statistical association test to guide the tree building process.
Experimental results demonstrate that our algorithm achieves comparable
performance to state-of-the-art (SOTA) clustering ensemble methods while
maintaining an additional feature of interpretability. To the best of our
knowledge, this is the first interpretable algorithm specifically designed for
clustering ensemble, offering a new perspective for future research in
interpretable clustering.

</details>


### [85] [Towards an Explainable Comparison and Alignment of Feature Embeddings](https://arxiv.org/abs/2506.06231)
*Mohammad Jalali, Bahar Dibaei Nia, Farzan Farnia*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了Spectral Pairwise Embedding Comparison (SPEC)框架，用于比较和对齐嵌入模型，并展示了其在大规模数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有嵌入模型的比较主要关注数值性能，缺乏可解释性的对比方法。

**方法:** 提出Spectral Pairwise Embedding Comparison (SPEC)框架，分析两个嵌入的核矩阵差异，并利用特征分解检测样本簇的不同表示。

**结果:** 实现了基于核的方法的扩展应用，并提出了一个优化问题以对齐两个嵌入模型。

**结论:** SPEC框架能够有效比较和对齐嵌入模型，适用于大规模数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+an+Explainable+Comparison+and+Alignment+of+Feature+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06231，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06231&send_immediately=true&force_search=false)

**原文摘要:** While several feature embedding models have been developed in the literature,
comparisons of these embeddings have largely focused on their numerical
performance in classification-related downstream applications. However, an
interpretable comparison of different embeddings requires identifying and
analyzing mismatches between sample groups clustered within the embedding
spaces. In this work, we propose the \emph{Spectral Pairwise Embedding
Comparison (SPEC)} framework to compare embeddings and identify their
differences in clustering a reference dataset. Our approach examines the kernel
matrices derived from two embeddings and leverages the eigendecomposition of
the difference kernel matrix to detect sample clusters that are captured
differently by the two embeddings. We present a scalable implementation of this
kernel-based approach, with computational complexity that grows linearly with
the sample size. Furthermore, we introduce an optimization problem using this
framework to align two embeddings, ensuring that clusters identified in one
embedding are also captured in the other model. We provide numerical results
demonstrating the SPEC's application to compare and align embeddings on
large-scale datasets such as ImageNet and MS-COCO. The code is available at
[https://github.com/mjalali/embedding-comparison](github.com/mjalali/embedding-comparison).

</details>


### [86] [A projection-based framework for gradient-free and parallel learning](https://arxiv.org/abs/2506.05878)
*Andreas Bergmeister, Manish Krishan Lal, Stefanie Jegelka, Suvrit Sra*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于可行性寻求的神经网络训练方法，与传统的梯度下降法不同，该方法通过投影算子和迭代投影算法来满足本地约束条件。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服传统梯度下降法在并行化和处理不可导操作方面的局限性，文章提出了新的神经网络训练框架。

**方法:** 将训练重新定义为大规模可行性问题，通过投影到这些约束条件上进行求解，并引入PJAX软件框架以支持GPU/TPU加速和自动推导可行性问题的解决方案。

**结果:** 成功地在标准基准测试中训练了多种架构（MLPs、CNNs、RNNs），证明了该方法作为梯度下降替代方案的潜力，特别是在并行性和处理非可导操作方面具有明显优势。

**结论:** 论文表明，其提出的可行性寻求方法是梯度下降法的一个有吸引力的替代方案，具备更强的灵活性和性能优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+projection-based+framework+for+gradient-free+and+parallel+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05878&send_immediately=true&force_search=false)

**原文摘要:** We present a feasibility-seeking approach to neural network training. This
mathematical optimization framework is distinct from conventional
gradient-based loss minimization and uses projection operators and iterative
projection algorithms. We reformulate training as a large-scale feasibility
problem: finding network parameters and states that satisfy local constraints
derived from its elementary operations. Training then involves projecting onto
these constraints, a local operation that can be parallelized across the
network. We introduce PJAX, a JAX-based software framework that enables this
paradigm. PJAX composes projection operators for elementary operations,
automatically deriving the solution operators for the feasibility problems
(akin to autodiff for derivatives). It inherently supports GPU/TPU
acceleration, provides a familiar NumPy-like API, and is extensible. We train
diverse architectures (MLPs, CNNs, RNNs) on standard benchmarks using PJAX,
demonstrating its functionality and generality. Our results show that this
approach is as a compelling alternative to gradient-based training, with clear
advantages in parallelism and the ability to handle non-differentiable
operations.

</details>


### [87] [Distillation Robustifies Unlearning](https://arxiv.org/abs/2506.06278)
*Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为UNDO的方法，通过蒸馏技术增强LLM的遗忘能力，并在多个任务和基准上验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前的LLM遗忘方法不够稳健，容易通过微调恢复被遗忘的信息，因此需要一种更有效的遗忘方法。

**方法:** 提出Unlearn-Noise-Distill-on-Outputs (UNDO) 方法，结合模型蒸馏与噪声注入，以提升模型遗忘的稳健性。

**结果:** UNDO 在合成语言、算术任务以及 WMDP 基准上均表现出优异的遗忘稳健性，并且计算成本较低。

**结论:** UNDO 提供了一种高效且实用的遗忘方法，为实现稳健的能力移除提供了新路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distillation+Robustifies+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06278&send_immediately=true&force_search=false)

**原文摘要:** Current LLM unlearning methods are not robust: they can be reverted easily
with a few steps of finetuning. This is true even for the idealized unlearning
method of training to imitate an oracle model that was never exposed to
unwanted information, suggesting that output-based finetuning is insufficient
to achieve robust unlearning. In a similar vein, we find that training a
randomly initialized student to imitate an unlearned model transfers desired
behaviors while leaving undesired capabilities behind. In other words,
distillation robustifies unlearning. Building on this insight, we propose
Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an
unlearned model into a partially noised copy of itself. UNDO introduces a
tunable tradeoff between compute cost and robustness, establishing a new Pareto
frontier on synthetic language and arithmetic tasks. At its strongest setting,
UNDO matches the robustness of a model retrained from scratch with perfect data
filtering while using only 60-80% of the compute and requiring only 0.01% of
the pretraining data to be labeled. We also show that UNDO robustifies
unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)
benchmark. Since distillation is widely used in practice, incorporating an
unlearning step beforehand offers a convenient path to robust capability
removal.

</details>


### [88] [NILMFormer: Non-Intrusive Load Monitoring that Accounts for Non-Stationarity](https://arxiv.org/abs/2506.05880)
*Adrien Petralia, Philippe Charpentier, Youssef Kadhi, Themis Palpanas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于Transformer的新型非侵入式负载监控（NILM）架构NILMFormer，解决了智能电表数据分布漂移问题，在实际数据集中表现优异，并已应用于法国电力公司（EDF）的服务中。


<details>
  <summary>更多</summary>
  
**动机:** 非侵入式负载监控（NILM）旨在从家庭总功耗中分离出各个电器的功耗，但由于现实世界智能电表数据的非平稳性，现有方法面临性能下降的问题。因此需要更有效的解决方案。

**方法:** 提出了一个基于Transformer的模型，结合了新的子序列平稳化/去平稳化方案以及新的位置编码方式。

**结果:** 在4个真实数据集上的实验表明，NILMFormer显著优于当前最先进的方法。

**结论:** 论文介绍了一个基于Transformer的架构NILMFormer，该架构通过引入新的子序列平稳化/去平稳化方案和仅依赖时间戳信息的位置编码，显著优于现有的最先进方法，并已成为EDF消费监测服务的骨干算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NILMFormer%3A+Non-Intrusive+Load+Monitoring+that+Accounts+for+Non-Stationarity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05880，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05880&send_immediately=true&force_search=false)

**原文摘要:** Millions of smart meters have been deployed worldwide, collecting the total
power consumed by individual households. Based on these data, electricity
suppliers offer their clients energy monitoring solutions to provide feedback
on the consumption of their individual appliances. Historically, such estimates
have relied on statistical methods that use coarse-grained total monthly
consumption and static customer data, such as appliance ownership.
Non-Intrusive Load Monitoring (NILM) is the problem of disaggregating a
household's collected total power consumption to retrieve the consumed power
for individual appliances. Current state-of-the-art (SotA) solutions for NILM
are based on deep-learning (DL) and operate on subsequences of an entire
household consumption reading. However, the non-stationary nature of real-world
smart meter data leads to a drift in the data distribution within each
segmented window, which significantly affects model performance. This paper
introduces NILMFormer, a Transformer-based architecture that incorporates a new
subsequence stationarization/de-stationarization scheme to mitigate the
distribution drift and that uses a novel positional encoding that relies only
on the subsequence's timestamp information. Experiments with 4 real-world
datasets show that NILMFormer significantly outperforms the SotA approaches.
Our solution has been deployed as the backbone algorithm for EDF's
(Electricit\'e De France) consumption monitoring service, delivering detailed
insights to millions of customers about their individual appliances' power
consumption. This paper appeared in KDD 2025.

</details>


### [89] [Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias](https://arxiv.org/abs/2506.06280)
*Yuanzhe Hu, Kinshuk Goel, Vlad Killiakov, Yaoqing Yang*

**主要类别:** cs.LG

**AI概要:** 本论文提出了FARMS方法，通过固定纵横比的矩阵子采样来减轻特征谱分析中的纵横比偏差，从而提高深度神经网络的模型诊断准确性和超参数分配效果。


<details>
  <summary>更多</summary>
  
**动机:** 特征谱分析中的纵横比偏差会导致重尾度量估计的不准确，影响模型诊断和超参数分配，因此需要一种方法来克服这一挑战。

**方法:** 提出了一种名为FARMS的方法，通过以固定纵横比对子矩阵进行子采样来标准化权重矩阵，并测量这些子矩阵的平均经验谱密度（ESD）。

**结果:** FARMS在多个领域中均提升了特征谱分析的准确性和层间超参数分配的有效性，在LLM剪枝实验中将LLaMA-7B模型的困惑度降低了17.3%。

**结论:** FARMS有效地减轻了权重矩阵的纵横比偏差，从而提高了深度神经网络中特征谱分析的准确性，并且在不同领域的应用中表现出色，包括CV模型、SciML模型和LLM剪枝。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Eigenspectrum+Analysis+of+Neural+Networks+without+Aspect+Ratio+Bias，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06280，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06280&send_immediately=true&force_search=false)

**原文摘要:** Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight
matrices has been an active area of research in recent years. At a high level,
eigenspectrum analysis of DNNs involves measuring the heavytailness of the
empirical spectral densities (ESD) of weight matrices. It provides insight into
how well a model is trained and can guide decisions on assigning better
layer-wise training hyperparameters. In this paper, we address a challenge
associated with such eigenspectrum methods: the impact of the aspect ratio of
weight matrices on estimated heavytailness metrics. We demonstrate that
matrices of varying sizes (and aspect ratios) introduce a non-negligible bias
in estimating heavytailness metrics, leading to inaccurate model diagnosis and
layer-wise hyperparameter assignment. To overcome this challenge, we propose
FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the
weight matrices by subsampling submatrices with a fixed aspect ratio. Instead
of measuring the heavytailness of the original ESD, we measure the average ESD
of these subsampled submatrices. We show that measuring the heavytailness of
these submatrices with the fixed aspect ratio can effectively mitigate the
aspect ratio bias. We validate our approach across various optimization
techniques and application domains that involve eigenspectrum analysis of
weights, including image classification in computer vision (CV) models,
scientific machine learning (SciML) model training, and large language model
(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly
improves the accuracy of eigenspectrum analysis while enabling more effective
layer-wise hyperparameter assignment in these application domains. In one of
the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model
by 17.3% when compared with the state-of-the-art method.

</details>


### [90] [Few Labels are all you need: A Weakly Supervised Framework for Appliance Localization in Smart-Meter Series](https://arxiv.org/abs/2506.05895)
*Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为CamAL的弱监督方法，用于解决非侵入式负载监测问题，该方法仅需要有关家庭中存在哪些电器的信息即可进行训练，并且在四个真实数据集上的实验结果显示其性能优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 改善智能电网系统管理对于对抗气候变化至关重要，而让消费者积极参与这一努力是电力供应商面临的一个重大挑战。非侵入式负载监控（NILM）旨在利用主智能电表信号估计个别电器的功耗、模式或开关状态激活情况，但现有的完全监督深度学习方法需要收集昂贵且极其稀缺的标签。

**方法:** CamAL结合了深度学习分类器的集成和一种可解释的分类方法，以实现电器模式的定位。

**结果:** 实验评估表明，CamAL显著优于现有的弱监督基线，并且当前最先进的完全监督NILM方法需要更多的标签才能达到CamAL的性能。

**结论:** CamAL是一个弱监督的方法，用于电器模式定位，仅需要有关家庭中存在哪些电器的信息即可进行训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Few+Labels+are+all+you+need%3A+A+Weakly+Supervised+Framework+for+Appliance+Localization+in+Smart-Meter+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05895，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05895&send_immediately=true&force_search=false)

**原文摘要:** Improving smart grid system management is crucial in the fight against
climate change, and enabling consumers to play an active role in this effort is
a significant challenge for electricity suppliers. In this regard, millions of
smart meters have been deployed worldwide in the last decade, recording the
main electricity power consumed in individual households. This data produces
valuable information that can help them reduce their electricity footprint;
nevertheless, the collected signal aggregates the consumption of the different
appliances running simultaneously in the house, making it difficult to
apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of
estimating the power consumption, pattern, or on/off state activation of
individual appliances using the main smart meter signal. Recent methods
proposed to tackle this task are based on a fully supervised deep-learning
approach that requires both the aggregate signal and the ground truth of
individual appliance power. However, such labels are expensive to collect and
extremely scarce in practice, as they require conducting intrusive surveys in
households to monitor each appliance. In this paper, we introduce CamAL, a
weakly supervised approach for appliance pattern localization that only
requires information on the presence of an appliance in a household to be
trained. CamAL merges an ensemble of deep-learning classifiers combined with an
explainable classification method to be able to localize appliance patterns.
Our experimental evaluation, conducted on 4 real-world datasets, demonstrates
that CamAL significantly outperforms existing weakly supervised baselines and
that current SotA fully supervised NILM approaches require significantly more
labels to reach CamAL performances. The source of our experiments is available
at: https://github.com/adrienpetralia/CamAL. This paper appeared in ICDE 2025.

</details>


### [91] [A Driving Regime-Embedded Deep Learning Framework for Modeling Intra-Driver Heterogeneity in Multi-Scale Car-Following Dynamics](https://arxiv.org/abs/2506.05902)
*Shirui Zhou, Jiying Yan, Junfang Tian, Tao Wang, Yongfu Li, Shiquan Zhong*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的数据驱动的跟车模型框架，结合了离散驾驶状态以提高车辆运动预测的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有模型往往强调驾驶员间的异质性或依赖简化假设，无法准确捕捉单个驾驶员在不同驾驶条件下的动态异质性。

**方法:** 利用高分辨率交通轨迹数据集，将门控循环单元与长短期记忆网络相结合，并采用自下而上的分割算法和动态时间规整识别驾驶状态。

**结果:** 该方法显著降低了加速度、速度和间距指标的预测误差，并重现了关键交通现象。

**结论:** 论文提出了一种新的数据驱动的跟车模型框架，能够系统地嵌入离散驾驶状态来改进车辆运动预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Driving+Regime-Embedded+Deep+Learning+Framework+for+Modeling+Intra-Driver+Heterogeneity+in+Multi-Scale+Car-Following+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05902，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05902&send_immediately=true&force_search=false)

**原文摘要:** A fundamental challenge in car-following modeling lies in accurately
representing the multi-scale complexity of driving behaviors, particularly the
intra-driver heterogeneity where a single driver's actions fluctuate
dynamically under varying conditions. While existing models, both conventional
and data-driven, address behavioral heterogeneity to some extent, they often
emphasize inter-driver heterogeneity or rely on simplified assumptions,
limiting their ability to capture the dynamic heterogeneity of a single driver
under different driving conditions. To address this gap, we propose a novel
data-driven car-following framework that systematically embeds discrete driving
regimes (e.g., steady-state following, acceleration, cruising) into vehicular
motion predictions. Leveraging high-resolution traffic trajectory datasets, the
proposed hybrid deep learning architecture combines Gated Recurrent Units for
discrete driving regime classification with Long Short-Term Memory networks for
continuous kinematic prediction, unifying discrete decision-making processes
and continuous vehicular dynamics to comprehensively represent inter- and
intra-driver heterogeneity. Driving regimes are identified using a bottom-up
segmentation algorithm and Dynamic Time Warping, ensuring robust
characterization of behavioral states across diverse traffic scenarios.
Comparative analyses demonstrate that the framework significantly reduces
prediction errors for acceleration (maximum MSE improvement reached 58.47\%),
speed, and spacing metrics while reproducing critical traffic phenomena, such
as stop-and-go wave propagation and oscillatory dynamics.

</details>


### [92] [DeviceScope: An Interactive App to Detect and Localize Appliance Patterns in Electricity Consumption Time Series](https://arxiv.org/abs/2506.05912)
*Adrien Petralia, Paul Boniol, Philippe Charpentier, Themis Palpanas*

**主要类别:** cs.LG

**AI概要:** 本文介绍DeviceScope，一种帮助非专家用户通过新型弱监督方法CamAL分析智能电表数据的交互式工具。


<details>
  <summary>更多</summary>
  
**动机:** 由于非专业用户难以理解智能电表数据且缺乏标注数据，需要一种更有效的电器用电模式检测和定位方法。

**方法:** 提出了一种名为CamAL的弱监督方法，用于电器定位，并开发了交互式工具DeviceScope进行验证。

**结果:** 成功开发了DeviceScope系统，其能够在没有详细标注的情况下检测并定位家庭电器的用电模式。

**结论:** 本文提出了DeviceScope，一个基于CamAL的新工具，能够帮助非专业用户理解和分析智能电表数据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeviceScope%3A+An+Interactive+App+to+Detect+and+Localize+Appliance+Patterns+in+Electricity+Consumption+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05912，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05912&send_immediately=true&force_search=false)

**原文摘要:** In recent years, electricity suppliers have installed millions of smart
meters worldwide to improve the management of the smart grid system. These
meters collect a large amount of electrical consumption data to produce
valuable information to help consumers reduce their electricity footprint.
However, having non-expert users (e.g., consumers or sales advisors) understand
these data and derive usage patterns for different appliances has become a
significant challenge for electricity suppliers because these data record the
aggregated behavior of all appliances. At the same time, ground-truth labels
(which could train appliance detection and localization models) are expensive
to collect and extremely scarce in practice. This paper introduces DeviceScope,
an interactive tool designed to facilitate understanding smart meter data by
detecting and localizing individual appliance patterns within a given time
period. Our system is based on CamAL (Class Activation Map-based Appliance
Localization), a novel weakly supervised approach for appliance localization
that only requires the knowledge of the existence of an appliance in a
household to be trained. This paper appeared in ICDE 2025.

</details>


### [93] [Over-PINNs: Enhancing Physics-Informed Neural Networks via Higher-Order Partial Derivative Overdetermination of PDEs](https://arxiv.org/abs/2506.05918)
*Wenxuan Huo, Qiang He, Gang Zhu, Weifeng Huang*

**主要类别:** cs.LG

**AI概要:** 本文提出一种名为Over-PINNs的新框架，通过引入高阶辅助方程增强物理信息约束，从而显著提升求解偏微分方程的精度和通用性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决物理信息神经网络（PINNs）在处理复杂问题时准确度不足的问题。

**方法:** 引入Over-PINNs框架，利用自动微分生成高阶辅助方程，并将其作为额外损失项加入训练过程。

**结果:** 数值结果表明该方法在提高模型捕捉物理信息能力方面表现优异，具备较强的通用性。

**结论:** Over-PINNs框架在解决各种类型的偏微分方程上展示出强通用性和显著的精度提升，同时没有显著增加计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Over-PINNs%3A+Enhancing+Physics-Informed+Neural+Networks+via+Higher-Order+Partial+Derivative+Overdetermination+of+PDEs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05918，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05918&send_immediately=true&force_search=false)

**原文摘要:** Partial differential equations (PDEs) serve as the cornerstone of
mathematical physics. In recent years, Physics-Informed Neural Networks (PINNs)
have significantly reduced the dependence on large datasets by embedding
physical laws directly into the training of neural networks. However, when
dealing with complex problems, the accuracy of PINNs still has room for
improvement. To address this issue, we introduce the Over-PINNs framework,
which leverages automatic differentiation (AD) to generate higher-order
auxiliary equations that impose additional physical constraints. These
equations are incorporated as extra loss terms in the training process,
effectively enhancing the model's ability to capture physical information
through an "overdetermined" approach. Numerical results illustrate that this
method exhibits strong versatility in solving various types of PDEs. It
achieves a significant improvement in solution accuracy without incurring
substantial additional computational costs.

</details>


### [94] [Machine Learning Predictions for Traffic Equilibria in Road Renovation Scheduling](https://arxiv.org/abs/2506.05933)
*Robbert Bosch, Wouter van Heeswijk, Patricia Rogetzer, Martijn Mes*

**主要类别:** cs.LG

**AI概要:** 该论文研究了使用机器学习替代模型来预测道路维护导致的拥堵情况，发现XGBoost在准确性方面优于其他模型，有助于降低传统模拟方法的计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 论文旨在解决道路维护计划对交通状况影响的准确估计问题，因为未妥善规划的维护操作可能导致严重的交通拥堵，而传统的交通模拟方法在长期维护规划中存在巨大的计算负担。

**方法:** 论文将问题定义为监督学习任务，使用one-hot编码、工程交通特征和启发式近似方法，并在在线学习框架下评估了多种线性、集成、概率和神经网络回归模型的表现。

**结果:** 实验结果表明，在训练数据有限的情况下，Costliest Subset Heuristic提供了一个合理的近似方案，大多数回归模型无法超越它；但XGBoost在多个指标上显著优于其他模型，包括MAPE（平均绝对百分比误差）和分位数损失。

**结论:** 论文得出结论，基于机器学习的替代模型，尤其是XGBoost，在预测道路维护引起的拥堵方面优于其他回归模型，并有望减少大规模交通分配问题的计算负担。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Machine+Learning+Predictions+for+Traffic+Equilibria+in+Road+Renovation+Scheduling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05933，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05933&send_immediately=true&force_search=false)

**原文摘要:** Accurately estimating the impact of road maintenance schedules on traffic
conditions is important because maintenance operations can substantially worsen
congestion if not carefully planned. Reliable estimates allow planners to avoid
excessive delays during periods of roadwork. Since the exact increase in
congestion is difficult to predict analytically, traffic simulations are
commonly used to assess the redistribution of the flow of traffic. However,
when applied to long-term maintenance planning involving many overlapping
projects and scheduling alternatives, these simulations must be run thousands
of times, resulting in a significant computational burden. This paper
investigates the use of machine learning-based surrogate models to predict
network-wide congestion caused by simultaneous road renovations. We frame the
problem as a supervised learning task, using one-hot encodings, engineered
traffic features, and heuristic approximations. A range of linear,
ensemble-based, probabilistic, and neural regression models is evaluated under
an online learning framework in which data progressively becomes available. The
experimental results show that the Costliest Subset Heuristic provides a
reasonable approximation when limited training data is available, and that most
regression models fail to outperform it, with the exception of XGBoost, which
achieves substantially better accuracy. In overall performance, XGBoost
significantly outperforms alternatives in a range of metrics, most strikingly
Mean Absolute Percentage Error (MAPE) and Pinball loss, where it achieves a
MAPE of 11% and outperforms the next-best model by 20% and 38% respectively.
This modeling approach has the potential to reduce the computational burden of
large-scale traffic assignment problems in maintenance planning.

</details>


### [95] [Exponential Family Variational Flow Matching for Tabular Data Generation](https://arxiv.org/abs/2506.05940)
*Andrés Guzmán-Cordero, Floor Eijkelboom, Jan-Willem van de Meent*

**主要类别:** cs.LG

**AI概要:** 本文提出了TabbyFlow，一种基于变分流匹配的方法，用于处理包含连续和离散特征的表格数据生成任务，并通过新提出的EF-VFM框架实现高效概率路径学习。


<details>
  <summary>更多</summary>
  
**动机:** 尽管去噪扩散和流匹配在生成建模方面取得了重要进展，但它们在表格数据中的应用仍然有限，而表格数据在现实世界应用中广泛存在。

**方法:** 引入了Exponential Family Variational Flow Matching (EF-VFM)，利用指数族分布来表示异构数据类型，并通过矩匹配获得一个有效的数据驱动目标函数。

**结果:** 在表格数据基准上的评估表明，TabbyFlow相较其他基线模型表现出最先进的性能。

**结论:** TabbyFlow通过EF-VFM方法在表格数据生成上实现了高效和基于原则的概率路径学习，并展示了与现有基线模型相比的最先进性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exponential+Family+Variational+Flow+Matching+for+Tabular+Data+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05940，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05940&send_immediately=true&force_search=false)

**原文摘要:** While denoising diffusion and flow matching have driven major advances in
generative modeling, their application to tabular data remains limited, despite
its ubiquity in real-world applications. To this end, we develop TabbyFlow, a
variational Flow Matching (VFM) method for tabular data generation. To apply
VFM to data with mixed continuous and discrete features, we introduce
Exponential Family Variational Flow Matching (EF-VFM), which represents
heterogeneous data types using a general exponential family distribution. We
hereby obtain an efficient, data-driven objective based on moment matching,
enabling principled learning of probability paths over mixed continuous and
discrete variables. We also establish a connection between variational flow
matching and generalized flow matching objectives based on Bregman divergences.
Evaluation on tabular data benchmarks demonstrates state-of-the-art performance
compared to baselines.

</details>


### [96] [Additive decomposition of one-dimensional signals using Transformers](https://arxiv.org/abs/2506.05942)
*Samuele Salti, Andrea Pinto, Alessandro Lanza, Serena Morigi*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种基于深度学习Transformer的一维信号分解新方法，在合成数据上验证了其高精度分解能力。


<details>
  <summary>更多</summary>
  
**动机:** 传统的信号分解技术依赖于数学模型，而本文认为应用最新的深度学习模型可以探索新的领域并提高性能。

**方法:** 利用Transformer架构对一维信号进行分解，将其分为分段常数、平滑（低频振荡）、纹理（高频振荡）和噪声成分。

**结果:** 实验结果表明，该模型在合成数据上表现出色，能够准确地建模和分解输入信号。

**结论:** 本文提出了一种基于Transformer架构的新型一维信号加性分解方法，并证明了其在合成数据上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Additive+decomposition+of+one-dimensional+signals+using+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05942&send_immediately=true&force_search=false)

**原文摘要:** One-dimensional signal decomposition is a well-established and widely used
technique across various scientific fields. It serves as a highly valuable
pre-processing step for data analysis. While traditional decomposition
techniques often rely on mathematical models, recent research suggests that
applying the latest deep learning models to this problem presents an exciting,
unexplored area with promising potential. This work presents a novel method for
the additive decomposition of one-dimensional signals. We leverage the
Transformer architecture to decompose signals into their constituent
components: piece-wise constant, smooth (low-frequency oscillatory), textured
(high-frequency oscillatory), and a noise component. Our model, trained on
synthetic data, achieves excellent accuracy in modeling and decomposing input
signals from the same distribution, as demonstrated by the experimental
results.

</details>


### [97] [Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes](https://arxiv.org/abs/2506.05953)
*Alessandro Montenegro, Leonardo Cesani, Marco Mussi, Matteo Papini, Alberto Maria Metelli*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的约束强化学习算法C-PG，该算法能够在训练后部署确定性策略，并且具有全局收敛性保证。


<details>
  <summary>更多</summary>
  
**动机:** 约束强化学习（CRL）中的策略方法因其在连续控制问题上的优势而被广泛使用，但需要解决策略空间搜索中的探索问题。

**方法:** 引入了一种与探索无关的算法C-PG，并在特定噪声模型下建立了其全局收敛性保证。

**结果:** C-PG在梯度主导假设下具有全局最后一次迭代收敛性，并且在特定噪声模型下也能保持这一特性。

**结论:** C-PG算法在训练后关闭随机性时，能够全局收敛到最优确定性策略，并且其变体C-PGAE和C-PGPE在受限控制任务中表现出了有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Deterministic+Policies+with+Policy+Gradients+in+Constrained+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05953，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05953&send_immediately=true&force_search=false)

**原文摘要:** Constrained Reinforcement Learning (CRL) addresses sequential decision-making
problems where agents are required to achieve goals by maximizing the expected
return while meeting domain-specific constraints. In this setting, policy-based
methods are widely used thanks to their advantages when dealing with
continuous-control problems. These methods search in the policy space with an
action-based or a parameter-based exploration strategy, depending on whether
they learn the parameters of a stochastic policy or those of a stochastic
hyperpolicy. We introduce an exploration-agnostic algorithm, called C-PG, which
enjoys global last-iterate convergence guarantees under gradient domination
assumptions. Furthermore, under specific noise models where the (hyper)policy
is expressed as a stochastic perturbation of the actions or of the parameters
of an underlying deterministic policy, we additionally establish global
last-iterate convergence guarantees of C-PG to the optimal deterministic
policy. This holds when learning a stochastic (hyper)policy and subsequently
switching off the stochasticity at the end of training, thereby deploying a
deterministic policy. Finally, we empirically validate both the action-based
(C-PGAE) and parameter-based (C-PGPE) variants of C-PG on constrained control
tasks, and compare them against state-of-the-art baselines, demonstrating their
effectiveness, in particular when deploying deterministic policies after
training.

</details>


### [98] [Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization](https://arxiv.org/abs/2506.05957)
*Tianjun Yao, Haoxuan Li, Yongqiang Chen, Tongliang Liu, Le Song, Eric Xing, Zhiqiang Shen*

**主要类别:** cs.LG

**AI概要:** 为了解决图神经网络在分布外泛化方面的挑战，提出了PrunE方法，该方法通过修剪虚假边缘来保留关键的不变子图，从而提高了模型的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图神经网络在训练数据和测试数据之间的分布变化下表现显著下降，而当前的方法主要集中在直接识别与目标标签相关的不变子图，但这种方法容易出错。

**方法:** PrunE使用两种正则化项来修剪虚假边缘：1）图大小约束以排除无信息的虚假边缘，2）ε-概率对齐以进一步抑制虚假边缘的发生。

**结果:** PrunE在广泛的实验中显示出优越的OOD性能，并显著优于之前最先进的方法。

**结论:** PrunE是一种基于修剪的图OOD方法，通过消除虚假边缘来提高OOD泛化能力，并且在理论分析和实验中都表现出卓越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pruning+Spurious+Subgraphs+for+Graph+Out-of-Distribtuion+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05957，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05957&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) often encounter significant performance
degradation under distribution shifts between training and test data, hindering
their applicability in real-world scenarios. Recent studies have proposed
various methods to address the out-of-distribution generalization challenge,
with many methods in the graph domain focusing on directly identifying an
invariant subgraph that is predictive of the target label. However, we argue
that identifying the edges from the invariant subgraph directly is challenging
and error-prone, especially when some spurious edges exhibit strong
correlations with the targets. In this paper, we propose PrunE, the first
pruning-based graph OOD method that eliminates spurious edges to improve OOD
generalizability. By pruning spurious edges, \mine{} retains the invariant
subgraph more comprehensively, which is critical for OOD generalization.
Specifically, PrunE employs two regularization terms to prune spurious edges:
1) graph size constraint to exclude uninformative spurious edges, and 2)
$\epsilon$-probability alignment to further suppress the occurrence of spurious
edges. Through theoretical analysis and extensive experiments, we show that
PrunE achieves superior OOD performance and outperforms previous
state-of-the-art methods significantly. Codes are available at:
\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.

</details>


### [99] [AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models](https://arxiv.org/abs/2506.05960)
*Adil Hasan, Thomas Peyrin*

**主要类别:** cs.LG

**AI概要:** 研究提出使用基于码本的加性向量量化方法进行扩散模型压缩，取得了优于传统均匀标量量化方法的效果。


<details>
  <summary>更多</summary>
  
**动机:** diffusion models face adoption barriers due to high hardware resource requirements, and current quantization strategies have primarily explored uniform scalar quantization methods

**方法:** applied codebook-based additive vector quantization to diffusion model compression

**结果:** achieved a new Pareto frontier for low-bit weight quantization on LDM-4 benchmark with improved sFID, FID, and ISC scores at W4A8 and W2A8 precision levels

**结论:** codebook-based additive vector quantization can achieve superior performance in diffusion model compression compared to traditional uniform scalar quantization methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AQUATIC-Diff%3A+Additive+Quantization+for+Truly+Tiny+Compressed+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05960，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05960&send_immediately=true&force_search=false)

**原文摘要:** Significant investments have been made towards the commodification of
diffusion models for generation of diverse media. Their mass-market adoption is
however still hobbled by the intense hardware resource requirements of
diffusion model inference. Model quantization strategies tailored specifically
towards diffusion models have been useful in easing this burden, yet have
generally explored the Uniform Scalar Quantization (USQ) family of quantization
methods. In contrast, Vector Quantization (VQ) methods, which operate on groups
of multiple related weights as the basic unit of compression, have seen
substantial success in Large Language Model (LLM) quantization. In this work,
we apply codebook-based additive vector quantization to the problem of
diffusion model compression. Our resulting approach achieves a new Pareto
frontier for the extremely low-bit weight quantization on the standard
class-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.
Notably, we report sFID 1.92 points lower than the full-precision model at W4A8
and the best-reported results for FID, sFID and ISC at W2A8. We are also able
to demonstrate FLOPs savings on arbitrary hardware via an efficient inference
kernel, as opposed to savings resulting from small integer operations which may
lack broad hardware support.

</details>


### [100] [Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning](https://arxiv.org/abs/2506.05977)
*Yujia Huo, Jianchun Liu, Hongli Xu, Zhenguo Ma, Shilong Wang, Liusheng Huang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的联邦微调框架FedBE，通过扩展可训练模块和动态分配策略，有效缓解了联邦环境中灾难性遗忘问题，并提升了模型的性能和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦微调方法在应对分布式环境中的持续适应时，往往无法充分解决灾难性遗忘问题，且传统集中式微调方法难以应对联邦环境中的异构性和隐私限制。此外，不同客户端之间的数据分布和设备能力差异加剧了遗忘问题并降低了模型泛化能力。

**方法:** 提出了一种新的联邦微调框架FedBE，该框架通过扩展模型结构中的可训练块，并根据客户端的数据分布和计算能力动态分配这些可训练块。

**结果:** 与现有联邦微调方法相比，FedBE在微调后的一般任务中实现了12-74%的更高准确率保留，并且模型收敛加速比达到1.9-3.1倍，同时没有降低下游任务的准确性。

**结论:** FedBE通过结合自适应变压器块扩展机制和动态可训练块分配策略，有效解决了联邦微调中的灾难性遗忘问题，并提高了模型的泛化能力和收敛速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Catastrophic+Forgetting+with+Adaptive+Transformer+Block+Expansion+in+Federated+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05977&send_immediately=true&force_search=false)

**原文摘要:** Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as
a promising solution for adapting models to distributed data environments while
ensuring data privacy.
  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning
(PEFT) techniques to reduce communication and computation overhead.
  However, they often fail to adequately address the catastrophic forgetting, a
critical challenge arising from continual adaptation in distributed
environments. The traditional centralized fine-tuning methods, which are not
designed for the heterogeneous and privacy-constrained nature of federated
environments, struggle to mitigate this issue effectively. Moreover, the
challenge is further exacerbated by significant variation in data distributions
and device capabilities across clients, which leads to intensified forgetting
and degraded model generalization. To tackle these issues, we propose FedBE, a
novel FedFT framework that integrates an adaptive transformer block expansion
mechanism with a dynamic trainable-block allocation strategy. Specifically,
FedBE expands trainable blocks within the model architecture, structurally
separating newly learned task-specific knowledge from the original pre-trained
representations. Additionally, FedBE dynamically assigns these trainable blocks
to clients based on their data distributions and computational capabilities.
This enables the framework to better accommodate heterogeneous federated
environments and enhances the generalization ability of the model.Extensive
experiments show that compared with existing federated fine-tuning methods,
FedBE achieves 12-74% higher accuracy retention on general tasks after
fine-tuning and a model convergence acceleration ratio of 1.9-3.1x without
degrading the accuracy of downstream tasks.

</details>


### [101] [Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning](https://arxiv.org/abs/2506.05985)
*Yuheng Lei, Sitong Mao, Shunbo Zhou, Hongyuan Zhang, Xuelong Li, Ping Luo*

**主要类别:** cs.LG

**AI概要:** 本研究提出了DMPEL框架，用于解决终身机器人学习中灾难性遗忘的问题，该方法高效利用存储和计算资源，并在实验中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决通用智能体在持续学习过程中面临的前向迁移效率低下和灾难性遗忘的问题，同时避免传统方法对测试时任务标识符的依赖并增强知识共享能力。

**方法:** 提出了一种名为Dynamic Mixture of Progressive Parameter-Efficient Expert Library (DMPEL) 的框架，结合了渐进式参数高效专家学习与系数重放策略，以实现灵活的终身适应行为。

**结果:** 在LIBERO基准上的实验表明，DMPEL在连续适应任务的成功率上优于现有最先进的终身学习方法，同时仅需极少的可训练参数和存储资源。

**结论:** DMPEL通过渐进式学习低秩专家库和使用轻量级路由器动态组合专家，有效解决了终身机器人学习中的灾难性遗忘问题，并且在存储和计算上更加高效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Mixture+of+Progressive+Parameter-Efficient+Expert+Library+for+Lifelong+Robot+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05985，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05985&send_immediately=true&force_search=false)

**原文摘要:** A generalist agent must continuously learn and adapt throughout its lifetime,
achieving efficient forward transfer while minimizing catastrophic forgetting.
Previous work within the dominant pretrain-then-finetune paradigm has explored
parameter-efficient fine-tuning for single-task adaptation, effectively
steering a frozen pretrained model with a small number of parameters. However,
in the context of lifelong learning, these methods rely on the impractical
assumption of a test-time task identifier and restrict knowledge sharing among
isolated adapters. To address these limitations, we propose Dynamic Mixture of
Progressive Parameter-Efficient Expert Library (DMPEL) for lifelong robot
learning. DMPEL progressively learn a low-rank expert library and employs a
lightweight router to dynamically combine experts into an end-to-end policy,
facilitating flexible behavior during lifelong adaptation. Moreover, by
leveraging the modular structure of the fine-tuned parameters, we introduce
coefficient replay to guide the router in accurately retrieving frozen experts
for previously encountered tasks, thereby mitigating catastrophic forgetting.
This method is significantly more storage- and computationally-efficient than
applying demonstration replay to the entire policy. Extensive experiments on
the lifelong manipulation benchmark LIBERO demonstrate that our framework
outperforms state-of-the-art lifelong learning methods in success rates across
continual adaptation, while utilizing minimal trainable parameters and storage.

</details>


### [102] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/abs/2506.05994)
*Yi-Chun Liao, Chieh-Lin Tsai, Yuan-Hao Chang, Camélia Slimani, Jalil Boukhobza, Tei-Wei Kuo*

**主要类别:** cs.LG

**AI概要:** The paper proposes RETENTION, a framework that significantly reduces CAM capacity requirements for tree-based model inference, offering a resource-efficient solution for accelerating these models.


<details>
  <summary>更多</summary>
  
**动机:** Although deep learning excels at unstructured data, tree-based models are superior at structured datasets. However, conventional accelerators struggle with these models due to their inherent characteristics. Existing CAM-based solutions have excessive memory consumption and low utilization.

**方法:** This work introduces RETENTION, an end-to-end framework that reduces CAM capacity requirement for tree-based model inference. It proposes an iterative pruning algorithm and a tree mapping scheme with two data placement strategies to reduce memory redundancy.

**结果:** Implementing the tree mapping scheme alone achieves 1.46x to 21.30x better space efficiency. The full RETENTION framework yields 4.35x to 207.12x improvement in space efficiency with less than 3% accuracy loss.

**结论:** RETENTION provides a resource-efficient direction for tree-based model acceleration by significantly reducing CAM capacity requirements.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RETENTION%3A+Resource-Efficient+Tree-Based+Ensemble+Model+Acceleration+with+Content-Addressable+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05994，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05994&send_immediately=true&force_search=false)

**原文摘要:** Although deep learning has demonstrated remarkable capabilities in learning
from unstructured data, modern tree-based ensemble models remain superior in
extracting relevant information and learning from structured datasets. While
several efforts have been made to accelerate tree-based models, the inherent
characteristics of the models pose significant challenges for conventional
accelerators. Recent research leveraging content-addressable memory (CAM)
offers a promising solution for accelerating tree-based models, yet existing
designs suffer from excessive memory consumption and low utilization. This work
addresses these challenges by introducing RETENTION, an end-to-end framework
that significantly reduces CAM capacity requirement for tree-based model
inference. We propose an iterative pruning algorithm with a novel pruning
criterion tailored for bagging-based models (e.g., Random Forest), which
minimizes model complexity while ensuring controlled accuracy degradation.
Additionally, we present a tree mapping scheme that incorporates two innovative
data placement strategies to alleviate the memory redundancy caused by the
widespread use of don't care states in CAM. Experimental results show that
implementing the tree mapping scheme alone achieves $1.46\times$ to $21.30
\times$ better space efficiency, while the full RETENTION framework yields
$4.35\times$ to $207.12\times$ improvement with less than 3% accuracy loss.
These results demonstrate that RETENTION is highly effective in reducing CAM
capacity requirement, providing a resource-efficient direction for tree-based
model acceleration.

</details>


### [103] [Machine learning for in-situ composition mapping in a self-driving magnetron sputtering system](https://arxiv.org/abs/2506.05999)
*Sanna Jarl, Jens Sjölund, Robert J. W. Frost, Anders Holst, Jonathan J. S. Scragg*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种基于磁控共溅射的自驱动实验室(SDL)框架，结合机器学习和原位测量技术，以加速多元素组合薄膜材料的发现。


<details>
  <summary>更多</summary>
  
**动机:** 论文动机是为了解决传统自驱动实验室(SDL)在薄膜科学中主要局限于溶液基合成方法的问题，这些方法无法访问广泛的无机材料化学空间，因此需要一种更广泛适用且高效的材料发现方法。

**方法:** 论文采用了一种基于磁控共溅射的自驱动实验室(SDL)框架，利用组合方法和机器学习(GPs)，通过原位测量传感器的数据来预测薄膜的成分分布，并开发了几种用于机器学习程序的获取函数进行实验优化。

**结果:** 论文结果显示，所提出的ML驱动方法能够在无需大量表征或校准的情况下准确预测多元素组合薄膜的成分分布，其中贝叶斯主动学习方法BALM表现最佳，仅需10次实验即可学习单一源的沉积速率。

**结论:** 论文得出结论，通过使用基于机器学习的主动学习方法和高斯过程，结合沉积腔室中石英晶体微天平传感器的原位测量，可以快速预测多元素组合薄膜的成分分布，从而大幅提高材料探索的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Machine+learning+for+in-situ+composition+mapping+in+a+self-driving+magnetron+sputtering+system，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05999&send_immediately=true&force_search=false)

**原文摘要:** Self-driving labs (SDLs), employing automation and machine learning (ML) to
accelerate experimental procedures, have enormous potential in the discovery of
new materials. However, in thin film science, SDLs are mainly restricted to
solution-based synthetic methods which are easier to automate but cannot access
the broad chemical space of inorganic materials. This work presents an SDL
based on magnetron co-sputtering. We are using combinatorial frameworks,
obtaining accurate composition maps on multi-element, compositionally graded
thin films. This normally requires time-consuming ex-situ analysis prone to
systematic errors. We present a rapid and calibration-free in-situ, ML driven
approach to produce composition maps for arbitrary source combinations and
sputtering conditions. We develop a method to predict the composition
distribution in a multi-element combinatorial thin film, using in-situ
measurements from quartz-crystal microbalance sensors placed in a sputter
chamber. For a given source, the sensor readings are learned as a function of
the sputtering pressure and magnetron power, through active learning using
Gaussian processes (GPs). The final GPs are combined with a geometric model of
the deposition flux distribution in the chamber, which allows interpolation of
the deposition rates from each source, at any position across the sample. We
investigate several acquisition functions for the ML procedure. A fully
Bayesian GP - BALM (Bayesian active learning MacKay) - achieved the best
performance, learning the deposition rates for a single source in 10
experiments. Prediction accuracy for co-sputtering composition distributions
was verified experimentally. Our framework dramatically increases throughput by
avoiding the need for extensive characterisation or calibration, thus
demonstrating the potential of ML-guided SDLs to accelerate materials
exploration.

</details>


### [104] [LaDEEP: A Deep Learning-based Surrogate Model for Large Deformation of Elastic-Plastic Solids](https://arxiv.org/abs/2506.06001)
*Shilong Tao, Zhe Feng, Haonan Sun, Zhanxing Zhu, Yunhuai Liu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为LaDEEP的深度学习代理模型，用于解决弹性塑性固体大变形问题，相比传统方法具有更高的速度和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 经典数值求解器在精度与效率之间存在固有折衷，且难以准确处理涉及接触、加载和卸载的复杂弹性塑性固体。

**方法:** 该研究采用了一种两阶段的Transformer模块来预测变形，并将细长固体的分区区域编码为令牌序列以维持其基本顺序属性。

**结果:** LaDEEP的速度比有限元方法快五个数量级，并且平均相对其他深度学习基线提高了20.47%。

**结论:** LaDEEP是一个基于深度学习的代理模型，用于弹性塑性固体的大变形仿真，在速度和准确性方面优于传统有限元方法和其他深度学习基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LaDEEP%3A+A+Deep+Learning-based+Surrogate+Model+for+Large+Deformation+of+Elastic-Plastic+Solids，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06001，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06001&send_immediately=true&force_search=false)

**原文摘要:** Scientific computing for large deformation of elastic-plastic solids is
critical for numerous real-world applications. Classical numerical solvers rely
primarily on local discrete linear approximation and are constrained by an
inherent trade-off between accuracy and efficiency. Recently, deep learning
models have achieved impressive progress in solving the continuum mechanism.
While previous models have explored various architectures and constructed
coefficient-solution mappings, they are designed for general instances without
considering specific problem properties and hard to accurately handle with
complex elastic-plastic solids involving contact, loading and unloading. In
this work, we take stretch bending, a popular metal fabrication technique, as
our case study and introduce LaDEEP, a deep learning-based surrogate model for
\textbf{La}rge \textbf{De}formation of \textbf{E}lastic-\textbf{P}lastic
Solids. We encode the partitioned regions of the involved slender solids into a
token sequence to maintain their essential order property. To characterize the
physical process of the solid deformation, a two-stage Transformer-based module
is designed to predict the deformation with the sequence of tokens as input.
Empirically, LaDEEP achieves five magnitudes faster speed than finite element
methods with a comparable accuracy, and gains 20.47\% relative improvement on
average compared to other deep learning baselines. We have also deployed our
model into a real-world industrial production system, and it has shown
remarkable performance in both accuracy and efficiency.

</details>


### [105] [What Really is a Member? Discrediting Membership Inference via Poisoning](https://arxiv.org/abs/2506.06003)
*Neal Mangaokar, Ashish Hooda, Zhuohang Li, Bradley A. Malin, Kassem Fawaz, Somesh Jha, Atul Prakash, Amrita Roy Chowdhury*

**主要类别:** cs.LG

**AI概要:** 这篇论文分析了成员推断测试在放宽成员资格定义后的不可靠性，并提出了一种通过毒害训练数据来误导测试的方法。


<details>
  <summary>更多</summary>
  
**动机:** 最近的工作表明，基于精确匹配的成员资格定义下的成员推断测试经常失败，并建议将语义邻居也包括在内作为成员。

**方法:** 我们理论揭示了测试准确性与其对毒害的鲁棒性之间的权衡，并提出了这种毒害攻击的具体实例并验证了其实证效果。

**结果:** 结果显示，现有的测试方法在性能上可以降至随机以下。

**结论:** Membership inference测试在放松定义下仍然不可靠，可以以一种导致测试为目标点生成错误预测的方式毒害训练数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Really+is+a+Member%3F+Discrediting+Membership+Inference+via+Poisoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06003&send_immediately=true&force_search=false)

**原文摘要:** Membership inference tests aim to determine whether a particular data point
was included in a language model's training set. However, recent works have
shown that such tests often fail under the strict definition of membership
based on exact matching, and have suggested relaxing this definition to include
semantic neighbors as members as well. In this work, we show that membership
inference tests are still unreliable under this relaxation - it is possible to
poison the training dataset in a way that causes the test to produce incorrect
predictions for a target point. We theoretically reveal a trade-off between a
test's accuracy and its robustness to poisoning. We also present a concrete
instantiation of this poisoning attack and empirically validate its
effectiveness. Our results show that it can degrade the performance of existing
tests to well below random.

</details>


### [106] [LightGTS: A Lightweight General Time Series Forecasting Model](https://arxiv.org/abs/2506.06005)
*Yihang Wang, Yuying Qiu, Peng Chen, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种轻量级通用时间序列预测模型LightGTS，通过周期性建模设计，在多源预训练中有效处理不同尺度和内在周期，并在解码过程中利用周期性并行解码技术提高预测性能。与现有时间序列基础模型相比，LightGTS具有更高的效率，并在多个真实世界基准测试中达到了最先进的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的通用时间序列预测方法依赖大规模多源预训练构建重参数模型，尽管其在各种数据集中表现出优秀的泛化能力，但计算负担大且在资源受限场景下存在局限性。因此需要一种更高效、轻量级的模型来解决这些问题。

**方法:** 从一致周期建模的角度出发，提出了Periodical Tokenization技术和Periodical Parallel Decoding技术，以提取跨数据集的一致周期模式并在解码过程中更好地利用周期性。基于这两种技术，LightGTS使用轻量级模型实现高效的通用时间序列预测。

**结果:** LightGTS在零样本和全样本设置下的9个真实世界基准测试中均达到了最先进的预测性能，并且与现有时间序列基础模型相比具有显著更高的效率。

**结论:** LightGTS是一种高效的通用时间序列预测模型，通过充分利用时间序列固有的周期性归纳偏置，在保证卓越性能的同时降低了计算成本，适用于资源受限的场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightGTS%3A+A+Lightweight+General+Time+Series+Forecasting+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06005&send_immediately=true&force_search=false)

**原文摘要:** Existing works on general time series forecasting build foundation models
with heavy model parameters through large-scale multi-source pre-training.
These models achieve superior generalization ability across various datasets at
the cost of significant computational burdens and limitations in
resource-constrained scenarios. This paper introduces LightGTS, a lightweight
general time series forecasting model designed from the perspective of
consistent periodical modeling. To handle diverse scales and intrinsic periods
in multi-source pre-training, we introduce Periodical Tokenization, which
extracts consistent periodic patterns across different datasets with varying
scales. To better utilize the periodicity in the decoding process, we further
introduce Periodical Parallel Decoding, which leverages historical tokens to
improve forecasting. Based on the two techniques above which fully leverage the
inductive bias of periods inherent in time series, LightGTS uses a lightweight
model to achieve outstanding performance on general time series forecasting. It
achieves state-of-the-art forecasting performance on 9 real-world benchmarks in
both zero-shot and full-shot settings with much better efficiency compared with
existing time series foundation models.

</details>


### [107] [Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems](https://arxiv.org/abs/2506.06021)
*Shilong Tao, Zhe Feng, Haonan Sun, Zhanxing Zhu, Yunhuai Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于多固体系统的新颖显式建模方法Unisoma，其基于Transformer结构，在多个数据集和任务上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度学习方法主要依赖隐式建模，随着固体数量的增加，这些方法难以准确捕捉复杂的物理相互作用。

**方法:** 提出了基于Transformer的模型Unisoma，使用接触模块和自适应交互分配机制直接捕捉物理相互作用，并通过三元组关系学习变形。

**结果:** Unisoma在七个成熟数据集和两个复杂的多固体任务中均达到了最先进的性能。

**结论:** Unisoma是一种新颖的显式建模范式，适用于具有多样化耦合模式的多固体系统，能够更详细地处理每个固体，防止信息混合和混淆。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unisoma%3A+A+Unified+Transformer-based+Solver+for+Multi-Solid+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06021，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06021&send_immediately=true&force_search=false)

**原文摘要:** Multi-solid systems are foundational to a wide range of real-world
applications, yet modeling their complex interactions remains challenging.
Existing deep learning methods predominantly rely on implicit modeling, where
the factors influencing solid deformation are not explicitly represented but
are instead indirectly learned. However, as the number of solids increases,
these methods struggle to accurately capture intricate physical interactions.
In this paper, we introduce a novel explicit modeling paradigm that
incorporates factors influencing solid deformation through structured modules.
Specifically, we present Unisoma, a unified and flexible Transformer-based
model capable of handling variable numbers of solids. Unisoma directly captures
physical interactions using contact modules and adaptive interaction allocation
mechanism, and learns the deformation through a triplet relationship. Compared
to implicit modeling techniques, explicit modeling is more well-suited for
multi-solid systems with diverse coupling patterns, as it enables detailed
treatment of each solid while preventing information blending and confusion.
Experimentally, Unisoma achieves consistent state-of-the-art performance across
seven well-established datasets and two complex multi-solid tasks. Code is
avaiable at \href{this link}{https://github.com/therontau0054/Unisoma}.

</details>


### [108] [Do-PFN: In-Context Learning for Causal Effect Estimation](https://arxiv.org/abs/2506.06039)
*Jake Robertson, Arik Reuter, Siyuan Guo, Noah Hollmann, Frank Hutter, Bernhard Schölkopf*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了如何利用Prior-data fitted networks (PFNs) 在未知因果结构的情况下准确估计因果效应，并展示了其在合成数据上的有效性及广泛适用潜力。


<details>
  <summary>更多</summary>
  
**动机:** 论文动因在于现有的因果效应估计方法需要干预数据、对因果图的先验知识或依赖于例如无混杂假设，这限制了它们在现实世界中的适用性。因此，作者探索了是否可以将PFNs应用于这一更具挑战性的领域。

**方法:** 该论文的方法是使用Prior-data fitted networks (PFNs)，通过在各种因果结构（包括干预）的合成数据上进行预训练，以解决因果效应估计问题。

**结果:** 实验结果显示，该方法能够在不需知晓底层因果图的情况下准确预测干预结果，并且通过消融实验验证了Do-PFN在多种因果特性数据集上的性能。

**结论:** 论文结论指出，通过合成数据预训练的PFNs能够在未知真实因果图的情况下准确估计因果效应，并展示了其在不同因果特征数据集上的可扩展性和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do-PFN%3A+In-Context+Learning+for+Causal+Effect+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06039&send_immediately=true&force_search=false)

**原文摘要:** Estimation of causal effects is critical to a range of scientific
disciplines. Existing methods for this task either require interventional data,
knowledge about the ground truth causal graph, or rely on assumptions such as
unconfoundedness, restricting their applicability in real-world settings. In
the domain of tabular machine learning, Prior-data fitted networks (PFNs) have
achieved state-of-the-art predictive performance, having been pre-trained on
synthetic data to solve tabular prediction problems via in-context learning. To
assess whether this can be transferred to the harder problem of causal effect
estimation, we pre-train PFNs on synthetic data drawn from a wide variety of
causal structures, including interventions, to predict interventional outcomes
given observational data. Through extensive experiments on synthetic case
studies, we show that our approach allows for the accurate estimation of causal
effects without knowledge of the underlying causal graph. We also perform
ablation studies that elucidate Do-PFN's scalability and robustness across
datasets with a variety of causal characteristics.

</details>


### [109] [Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics](https://arxiv.org/abs/2506.06045)
*Tobias Würth, Niklas Freymuth, Gerhard Neumann, Luise Kärger*

**主要类别:** cs.LG

**AI概要:** 本文提出ROBIN，一种结合滚动扩散和分层图神经网络的新一代物理模拟器，显著提升了模拟精度与效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统的基于图的学习型模拟器在处理非结构化网格上的物理系统时面临捕捉全局现象（如弯曲或长距离相关性）的困难，并且由于依赖局部消息传递和直接下一步预测，导致长时间展开中误差累积严重。

**方法:** ROBIN引入了两种创新：1) 滚动扩散，通过跨时间窗口重叠去噪步骤来分摊基于扩散的优化成本；2) 基于代数多重网格粗化的分层图神经网络，实现不同网格分辨率上的多尺度消息传递。

**结果:** ROBIN在涉及几何、材料和接触非线性的2D和3D固体力学基准测试中表现出色，实现了最先进的准确性，显著优于现有的学习模拟器，并将推理时间减少了多达一个数量级。

**结论:** ROBIN通过滚动扩散和分层图神经网络实现了高效且准确的物理模拟，克服了传统基于局部消息传递方法在全局现象建模和长期误差积累方面的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion-Based+Hierarchical+Graph+Neural+Networks+for+Simulating+Nonlinear+Solid+Mechanics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06045，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06045&send_immediately=true&force_search=false)

**原文摘要:** Graph-based learned simulators have emerged as a promising approach for
simulating physical systems on unstructured meshes, offering speed and
generalization across diverse geometries. However, they often struggle with
capturing global phenomena, such as bending or long-range correlations, and
suffer from error accumulation over long rollouts due to their reliance on
local message passing and direct next-step prediction. We address these
limitations by introducing the Rolling Diffusion-Batched Inference Network
(ROBIN), a novel learned simulator that integrates two key innovations: (i)
Rolling Diffusion, a parallelized inference scheme that amortizes the cost of
diffusion-based refinement across physical time steps by overlapping denoising
steps across a temporal window. (ii) A Hierarchical Graph Neural Network built
on algebraic multigrid coarsening, enabling multiscale message passing across
different mesh resolutions. This architecture, implemented via
Algebraic-hierarchical Message Passing Networks, captures both fine-scale local
dynamics and global structural effects critical for phenomena like beam bending
or multi-body contact. We validate ROBIN on challenging 2D and 3D solid
mechanics benchmarks involving geometric, material, and contact nonlinearities.
ROBIN achieves state-of-the-art accuracy on all tasks, substantially
outperforming existing next-step learned simulators while reducing inference
time by up to an order of magnitude compared to standard diffusion simulators.

</details>


### [110] [System-Aware Unlearning Algorithms: Use Lesser, Forget Faster](https://arxiv.org/abs/2506.06073)
*Linda Lu, Ayush Sekhari, Karthik Sridharan*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的机器遗忘定义和算法，在保证安全性的前提下提高遗忘效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统遗忘定义过于严格且不现实，需要更高效的遗忘方法。

**方法:** 提出系统感知遗忘定义，设计基于选择性采样的遗忘算法，并进行理论分析。

**结果:** 提出了适用于线性分类的精确遗忘算法，并推广至通用函数类。

**结论:** 系统感知遗忘算法在保证安全性的同时提高了遗忘效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是System-Aware+Unlearning+Algorithms%3A+Use+Lesser%2C+Forget+Faster，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06073，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06073&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning addresses the problem of updating a machine learning
model/system trained on a dataset $S$ so that the influence of a set of
deletion requests $U \subseteq S$ on the unlearned model is minimized. The gold
standard definition of unlearning demands that the updated model, after
deletion, be nearly identical to the model obtained by retraining. This
definition is designed for a worst-case attacker (one who can recover not only
the unlearned model but also the remaining data samples, i.e., $S \setminus
U$). Such a stringent definition has made developing efficient unlearning
algorithms challenging. However, such strong attackers are also unrealistic. In
this work, we propose a new definition, system-aware unlearning, which aims to
provide unlearning guarantees against an attacker that can at best only gain
access to the data stored in the system for learning/unlearning requests and
not all of $S\setminus U$. With this new definition, we use the simple
intuition that if a system can store less to make its learning/unlearning
updates, it can be more secure and update more efficiently against a
system-aware attacker. Towards that end, we present an exact system-aware
unlearning algorithm for linear classification using a selective sampling-based
approach, and we generalize the method for classification with general function
classes. We theoretically analyze the tradeoffs between deletion capacity,
accuracy, memory, and computation time.

</details>


### [111] [Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU](https://arxiv.org/abs/2506.06095)
*Wenhao Dai, Haodong Deng, Mengfei Rong, Xinyu Yang, Hongyu Liu, Fangxin Liu, Hailong Yang, Weifeng Liu, Qingxiao Sun*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种针对稀疏Transformer的优化框架STOF，通过灵活的掩码和操作融合技术，在GPU上实现了性能加速。


<details>
  <summary>更多</summary>
  
**动机:** 现有的稀疏Transformer性能优化研究较少，且基于规则的机制无法适应不同的序列长度并忽略了混合类型操作符的融合机会。

**方法:** STOF统一了多头注意力的存储格式和内核实现，并将融合方案映射到编译模板，通过两阶段搜索引擎确定最优参数设置。

**结果:** 实验结果表明，与最先进的工作相比，STOF在MHA计算中实现了最高1.7倍的加速，在端到端推理中实现了最高1.5倍的加速。

**结论:** STOF为稀疏Transformer提供了一个有效的优化框架，显著提高了GPU上的运行效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flexible+Operator+Fusion+for+Fast+Sparse+Transformer+with+Diverse+Masking+on+GPU，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06095&send_immediately=true&force_search=false)

**原文摘要:** Large language models are popular around the world due to their powerful
understanding capabilities. As the core component of LLMs, accelerating
Transformer through parallelization has gradually become a hot research topic.
Mask layers introduce sparsity into Transformer to reduce calculations.
However, previous works rarely focus on the performance optimization of sparse
Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of
mixed-type operators and fail to adapt to various sequence lengths. To address
the above problems, we propose STOF, a framework that incorporates
optimizations for Sparse Transformer via flexible masking and operator fusion
on GPU. We firstly unify the storage format and kernel implementation for the
multi-head attention. Then, we map fusion schemes to compilation templates and
determine the optimal parameter setting through a two-stage search engine. The
experimental results show that compared to the state-of-the-art work, STOF
achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end
inference.

</details>


### [112] [Synthetic Tabular Data: Methods, Attacks and Defenses](https://arxiv.org/abs/2506.06108)
*Graham Cormode, Samuel Maddock, Enayat Ullah, Shripad Gade*

**主要类别:** cs.LG

**AI概要:** 这篇论文综述了表格合成数据生成的发展，分析了其方法、应用与挑战，探讨了隐私保护和未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器学习和数据分析的发展，合成数据被视为一种替代敏感固定大小数据集的解决方案，可以避免隐私问题。然而，仍需要对合成数据生成的方法、其优势和潜在限制进行系统性的总结和分析。

**方法:** 论文采用了综述的方式，系统地回顾了过去十年中在合成数据生成方面的进展，并深入探讨了相关方法和技术。

**结果:** 论文全面梳理了表格合成数据生成的主要方法，包括概率图模型和深度学习的应用，同时评估了攻击手段如何试图从合成数据中恢复原始敏感信息，并提出了该领域中的扩展研究方向和未解决问题。

**结论:** 论文总结了表格合成数据生成的关键发展和主要概念，涵盖了基于概率图模型和深度学习的方法，并讨论了合成数据的局限性以及未来的研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synthetic+Tabular+Data%3A+Methods%2C+Attacks+and+Defenses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06108，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06108&send_immediately=true&force_search=false)

**原文摘要:** Synthetic data is often positioned as a solution to replace sensitive
fixed-size datasets with a source of unlimited matching data, freed from
privacy concerns. There has been much progress in synthetic data generation
over the last decade, leveraging corresponding advances in machine learning and
data analytics. In this survey, we cover the key developments and the main
concepts in tabular synthetic data generation, including paradigms based on
probabilistic graphical models and on deep learning. We provide background and
motivation, before giving a technical deep-dive into the methodologies. We also
address the limitations of synthetic data, by studying attacks that seek to
retrieve information about the original sensitive data. Finally, we present
extensions and open problems in this area.

</details>


### [113] [Scalable unsupervised feature selection via weight stability](https://arxiv.org/abs/2506.06114)
*Xudong Zhang, Renato Cordeiro de Amorim*

**主要类别:** cs.LG

**AI概要:** 作者开发了一种新的无监督特征选择方法，通过改进的Minkowski加权k均值算法提升了高维数据中的聚类性能。


<details>
  <summary>更多</summary>
  
**动机:** 高维数据中不相关的特征会掩盖有意义的结构，因此需要改进聚类性能的无监督特征选择方法。

**方法:** 文章引入了Minkowski加权k均值++初始化策略，并基于此提出了两种特征选择算法FS-MWK++和SFS-MWK++。

**结果:** 该方法通过聚合多个Minkowski指数上的特征权重来识别稳定且信息丰富的特征，并展示了其优于现有方法的表现。

**结论:** 本文提出了一种新的无监督特征选择方法，通过Minkowski加权k均值++初始化策略以及两个新的特征选择算法FS-MWK++和SFS-MWK++，在理论保证和支持下，这些方法优于现有替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+unsupervised+feature+selection+via+weight+stability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06114，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06114&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised feature selection is critical for improving clustering
performance in high-dimensional data, where irrelevant features can obscure
meaningful structure. In this work, we introduce the Minkowski weighted
$k$-means++, a novel initialisation strategy for the Minkowski Weighted
$k$-means. Our initialisation selects centroids probabilistically using feature
relevance estimates derived from the data itself. Building on this, we propose
two new feature selection algorithms, FS-MWK++, which aggregates feature
weights across a range of Minkowski exponents to identify stable and
informative features, and SFS-MWK++, a scalable variant based on subsampling.
We support our approach with a theoretical guarantee under mild assumptions and
extensive experiments showing that our methods consistently outperform existing
alternatives.

</details>


### [114] [Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library](https://arxiv.org/abs/2506.06122)
*Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, Zichen Liu, Haizhou Zhao, Dakai An, Lunxi Cao, Qiyang Cao, Wanxi Deng, Feilei Du, Yiliang Gu, Jiahe Li, Xiang Li, Mingjie Liu, Yijia Luo, Zihe Liu, Yadao Wang, Pei Wang, Tianyuan Wu, Yanan Wu, Yuheng Zhao, Shuaibing Zhao, Jin Yang, Siran Yang, Yingshui Tan, Huimin Yi, Yuchi Xu, Yujin Yuan, Xingyao Zhang, Lin Qu, Wenbo Su, Wei Wang, Jiamang Wang, Bo Zheng*

**主要类别:** cs.LG

**AI概要:** ROLL is a scalable and user-friendly library designed to optimize large-scale reinforcement learning by providing efficient training, flexible control, and agile experimentation.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenges faced by tech pioneers, developers, and researchers in achieving cost-effective, scalable, fault-tolerant training with flexible control and agile experimentation in reinforcement learning.

**方法:** The paper presents ROLL's modular design, including its single-controller architecture, parallel strategy, data transfer modules, rollout scheduler, environment worker, reward worker, and AutoDeviceMapping functionality.

**结果:** An efficient, scalable, and user-friendly library called ROLL was developed that simplifies the development of training pipelines and enables fine-grained management and flexible resource allocation.

**结论:** ROLL is a highly effective library for reinforcement learning optimization, offering scalability, flexibility, and user-friendliness tailored to large-scale training needs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+Optimization+for+Large-Scale+Learning%3A+An+Efficient+and+User-Friendly+Scaling+Library，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06122，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06122&send_immediately=true&force_search=false)

**原文摘要:** We introduce ROLL, an efficient, scalable, and user-friendly library designed
for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters
to three primary user groups: tech pioneers aiming for cost-effective,
fault-tolerant large-scale training, developers requiring flexible control over
training workflows, and researchers seeking agile experimentation. ROLL is
built upon several key modules to serve these user groups effectively. First, a
single-controller architecture combined with an abstraction of the parallel
worker simplifies the development of the training pipeline. Second, the
parallel strategy and data transfer modules enable efficient and scalable
training. Third, the rollout scheduler offers fine-grained management of each
sample's lifecycle during the rollout stage. Fourth, the environment worker and
reward worker support rapid and flexible experimentation with agentic RL
algorithms and reward designs. Finally, AutoDeviceMapping allows users to
assign resources to different models flexibly across various stages.

</details>


### [115] [Flow-Attentional Graph Neural Networks](https://arxiv.org/abs/2506.06127)
*Pascal Plettenberg, Dominik Köhler, Bernhard Sick, Josephine M. Thomas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 flow attention 的新方法，用于改进图神经网络，使其更好地处理符合物理守恒定律的图结构数据，从而提升模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有GNN未考虑物理资源流动中固有的守恒定律（如电力网络中的电流或交通网络中的车流），这可能导致模型性能下降。

**方法:** 通过引入 flow attention 方法，调整现有的图注意力机制以满足基尔霍夫第一定律，并分析其对模型表达能力的影响。

**结果:** 在两个流量图数据集（电子电路和电网）上进行的广泛实验表明，flow attention 在图级别分类和回归任务中均提升了模型性能。

**结论:** flow attention 提高了基于注意力的GNN在流量图数据集上的性能，同时增强了模型对符合基尔霍夫第一定律的数据的理解能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flow-Attentional+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06127&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have become essential for learning from
graph-structured data. However, existing GNNs do not consider the conservation
law inherent in graphs associated with a flow of physical resources, such as
electrical current in power grids or traffic in transportation networks, which
can lead to reduced model performance. To address this, we propose flow
attention, which adapts existing graph attention mechanisms to satisfy
Kirchhoff\'s first law. Furthermore, we discuss how this modification
influences the expressivity and identify sets of non-isomorphic graphs that can
be discriminated by flow attention but not by standard attention. Through
extensive experiments on two flow graph datasets (electronic circuits and power
grids), we demonstrate that flow attention enhances the performance of
attention-based GNNs on both graph-level classification and regression tasks.

</details>


### [116] [Gradient Similarity Surgery in Multi-Task Deep Learning](https://arxiv.org/abs/2506.06130)
*Thomas Borsani, Andrea Rosani, Giuseppe Nicosia, Giuseppe Di Fatta*

**主要类别:** cs.LG

**AI概要:** 本研究提出了SAM-GS方法，利用梯度幅度相似性解决多任务深度学习中的冲突梯度问题，提高了训练效率和稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 在多任务深度学习中，多个任务可能会产生潜在的冲突梯度，阻碍不同损失函数的同时收敛，因此需要一种有效的梯度手术方法。

**方法:** SAM-GS手术采用梯度幅度相似性度量来指导优化过程，并通过调整整体梯度轨迹进行梯度均衡和一阶动量调节。

**结果:** 一系列实验测试显示了SAM-GS在合成问题和多任务学习基准上的有效性，表明梯度幅度相似性在多任务深度学习优化过程中起到了关键作用。

**结论:** 本文提出了一种新的梯度手术方法，称为相似性感知动量梯度手术(SAM-GS)，用于解决多任务深度学习中的冲突梯度问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gradient+Similarity+Surgery+in+Multi-Task+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06130，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06130&send_immediately=true&force_search=false)

**原文摘要:** The multi-task learning ($MTL$) paradigm aims to simultaneously learn
multiple tasks within a single model capturing higher-level, more general
hidden patterns that are shared by the tasks. In deep learning, a significant
challenge in the backpropagation training process is the design of advanced
optimisers to improve the convergence speed and stability of the gradient
descent learning rule. In particular, in multi-task deep learning ($MTDL$) the
multitude of tasks may generate potentially conflicting gradients that would
hinder the concurrent convergence of the diverse loss functions. This challenge
arises when the gradients of the task objectives have either different
magnitudes or opposite directions, causing one or a few to dominate or to
interfere with each other, thus degrading the training process. Gradient
surgery methods address the problem explicitly dealing with conflicting
gradients by adjusting the overall gradient trajectory. This work introduces a
novel gradient surgery method, the Similarity-Aware Momentum Gradient Surgery
(SAM-GS), which provides an effective and scalable approach based on a gradient
magnitude similarity measure to guide the optimisation process. The SAM-GS
surgery adopts gradient equalisation and modulation of the first-order
momentum. A series of experimental tests have shown the effectiveness of SAM-GS
on synthetic problems and $MTL$ benchmarks. Gradient magnitude similarity plays
a crucial role in regularising gradient aggregation in $MTDL$ for the
optimisation of the learning process.

</details>


### [117] [Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models](https://arxiv.org/abs/2506.06137)
*Rihui Jin, Zheyu Xin, Xing Xie, Zuoyi Li, Guilin Qi, Yongrui Chen, Xinbang Dai, Tongtong Wu, Gholamreza Haffari*

**主要类别:** cs.LG

**AI概要:** 本文提出Table-r1，一种针对小语言模型的两阶段程序化表格推理方法，通过提升表格布局的泛化能力和推理一致性，其性能超越了现有的小语言模型方法，并与大语言模型相媲美。


<details>
  <summary>更多</summary>
  
**动机:** 表格推理对于小语言模型来说是一项挑战性的任务，尤其是因为它们在数值推理方面的能力有限，因此研究者探索了基于程序的表格推理方法来缩小这一差距。

**方法:** Table-r1包括两个阶段：第一阶段引入了一种创新的自监督学习任务，即布局转换推断，以改进从程序角度对表格布局的泛化能力；第二阶段采用了一种混合范式的组相对策略优化变体，在需要时增强程序化表格推理的一致性并允许动态回退到文本化表格推理。

**结果:** 实验结果显示，Table-r1在所有数据集中至少比基础模型（LLaMA-8B）提高了15%的准确性，并且达到了与大语言模型相当的性能。

**结论:** Table-r1是一个专为小语言模型设计的两阶段程序化表格推理方法，在四个基准测试中表现优于其他基于小语言模型的方法，并达到了与大语言模型相竞争的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Table-r1%3A+Self-supervised+and+Reinforcement+Learning+for+Program-based+Table+Reasoning+in+Small+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06137&send_immediately=true&force_search=false)

**原文摘要:** Table reasoning (TR) requires structured reasoning over semi-structured
tabular data and remains challenging, particularly for small language models
(SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs
(LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR),
which circumvents key limitations of text-based TR (T-TR), notably in numerical
reasoning, by generating executable programs. However, applying P-TR to SLMs
introduces two challenges: (i) vulnerability to heterogeneity in table layouts,
and (ii) inconsistency in reasoning due to limited code generation capability.
We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1
introduces an innovative self-supervised learning task, Layout Transformation
Inference, to improve tabular layout generalization from a programmatic view.
Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization,
enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed.
Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all
SLM-based methods, achieving at least a 15% accuracy improvement over the base
model (LLaMA-8B) across all datasets and reaching performance competitive with
LLMs.

</details>


### [118] [carps: A Framework for Comparing N Hyperparameter Optimizers on M Benchmarks](https://arxiv.org/abs/2506.06143)
*Carolin Benjamins, Helena Graf, Sarah Segel, Difan Deng, Tim Ruhkopf, Leona Hennig, Soham Basu, Neeratyoy Mallik, Edward Bergman, Deyao Chen, François Clément, Matthias Feurer, Katharina Eggensperger, Frank Hutter, Carola Doerr, Marius Lindauer*

**主要类别:** cs.LG

**AI概要:** 论文介绍了carps框架，用于评估超参数优化（HPO）方法，其包含了大量任务和优化器，并提出了一个高效的子集选择方法用于评估。


<details>
  <summary>更多</summary>
  
**动机:** 为了简化HPO方法的原型设计和基准测试，需要一个综合性的基准框架。

**方法:** 提出一个名为carps的框架，支持多种HPO任务类型的评估，并使用最小化星差异的方法选择代表性任务子集。

**结果:** 提供了目前最大的基准库，包含3,336个任务和28种优化器变体，并提出了10到30个代表性任务的子集用于高效评估。

**结论:** carps框架为HPO方法的评估和比较提供了一个标准化的基准库，并通过选择代表性的任务子集来提高评估效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是carps%3A+A+Framework+for+Comparing+N+Hyperparameter+Optimizers+on+M+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06143&send_immediately=true&force_search=false)

**原文摘要:** Hyperparameter Optimization (HPO) is crucial to develop well-performing
machine learning models. In order to ease prototyping and benchmarking of HPO
methods, we propose carps, a benchmark framework for Comprehensive Automated
Research Performance Studies allowing to evaluate N optimizers on M benchmark
tasks. In this first release of carps, we focus on the four most important
types of HPO task types: blackbox, multi-fidelity, multi-objective and
multi-fidelity-multi-objective. With 3 336 tasks from 5 community benchmark
collections and 28 variants of 9 optimizer families, we offer the biggest go-to
library to date to evaluate and compare HPO methods. The carps framework relies
on a purpose-built, lightweight interface, gluing together optimizers and
benchmark tasks. It also features an analysis pipeline, facilitating the
evaluation of optimizers on benchmarks. However, navigating a huge number of
tasks while developing and comparing methods can be computationally infeasible.
To address this, we obtain a subset of representative tasks by minimizing the
star discrepancy of the subset, in the space spanned by the full set. As a
result, we propose an initial subset of 10 to 30 diverse tasks for each task
type, and include functionality to re-compute subsets as more benchmarks become
available, enabling efficient evaluations. We also establish a first set of
baseline results on these tasks as a measure for future comparisons. With carps
(https://www.github.com/automl/CARP-S), we make an important step in the
standardization of HPO evaluation.

</details>


### [119] [ENMA: Tokenwise Autoregression for Generative Neural PDE Operators](https://arxiv.org/abs/2506.06158)
*Armand Kassaï Koupaï, Lise Le Boudec, Louis Serrano, Patrick Gallinari*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为 ENMA 的生成神经操作符，用于建模物理现象产生的时空动态，能在数据不确定或不完整时有效预测未来动态。


<details>
  <summary>更多</summary>
  
**动机:** 解决时间相关的参数偏微分方程 (PDEs) 仍然是神经求解器面临的基本挑战，尤其是在需要在广泛的物理参数和动态中进行泛化时。当数据不确定或不完整时，采用生成模型是一种自然的选择。

**方法:** ENMA 使用生成掩码自回归变压器和流匹配损失，在压缩的潜在空间中预测未来的动态。不规则采样的空间观测通过注意力机制编码，并通过时空卷积编码器进一步压缩。

**结果:** ENMA 能够在推理时通过条件依赖目标轨迹的过去状态或具有相似动态的辅助上下文轨迹来执行上下文中的学习。

**结论:** ENMA 是一种强大的、适应性强的框架，可以推广到新的 PDE 状态，并支持时间相关参数 PDE 的一次性代理建模。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ENMA%3A+Tokenwise+Autoregression+for+Generative+Neural+PDE+Operators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06158&send_immediately=true&force_search=false)

**原文摘要:** Solving time-dependent parametric partial differential equations (PDEs)
remains a fundamental challenge for neural solvers, particularly when
generalizing across a wide range of physical parameters and dynamics. When data
is uncertain or incomplete-as is often the case-a natural approach is to turn
to generative models. We introduce ENMA, a generative neural operator designed
to model spatio-temporal dynamics arising from physical phenomena. ENMA
predicts future dynamics in a compressed latent space using a generative masked
autoregressive transformer trained with flow matching loss, enabling tokenwise
generation. Irregularly sampled spatial observations are encoded into uniform
latent representations via attention mechanisms and further compressed through
a spatio-temporal convolutional encoder. This allows ENMA to perform in-context
learning at inference time by conditioning on either past states of the target
trajectory or auxiliary context trajectories with similar dynamics. The result
is a robust and adaptable framework that generalizes to new PDE regimes and
supports one-shot surrogate modeling of time-dependent parametric PDEs.

</details>


### [120] [Reusing Trajectories in Policy Gradients Enables Fast Convergence](https://arxiv.org/abs/2506.06178)
*Alessandro Montenegro, Federico Mansutti, Marco Mussi, Matteo Papini, Alberto Maria Metelli*

**主要类别:** cs.LG

**AI概要:** 本文提出了RPG算法，通过有效重用过去轨迹显著提升了策略梯度方法的收敛速度，达到了最优的样本复杂度。


<details>
  <summary>更多</summary>
  
**动机:** 传统策略梯度方法因依赖新鲜数据而效率低下，需要大量轨迹才能达到近似稳定点；虽然已有方法改进了梯度重用理论，但对过去轨迹的重用缺乏理论探索。

**方法:** 引入了一种幂平均修正的多重要性权重估计器，并提出RPG算法，结合旧轨迹和新轨迹进行策略更新。

**结果:** 理论分析表明，RPG在标准假设下达到了O(ε^-1)的样本复杂度，且实验验证了其优于现有策略梯度方法的有效性。

**结论:** RPG方法通过重用过去轨迹，在策略梯度方法中实现了更快的收敛速度，达到当前文献中最好的样本复杂度O(ε^-1)。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reusing+Trajectories+in+Policy+Gradients+Enables+Fast+Convergence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06178，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06178&send_immediately=true&force_search=false)

**原文摘要:** Policy gradient (PG) methods are a class of effective reinforcement learning
algorithms, particularly when dealing with continuous control problems. These
methods learn the parameters of parametric policies via stochastic gradient
ascent, typically using on-policy trajectory data to estimate the policy
gradient. However, such reliance on fresh data makes them sample-inefficient.
Indeed, vanilla PG methods require $O(\epsilon^{-2})$ trajectories to reach an
$\epsilon$-approximate stationary point. A common strategy to improve
efficiency is to reuse off-policy information from past iterations, such as
previous gradients or trajectories. While gradient reuse has received
substantial theoretical attention, leading to improved rates of
$O(\epsilon^{-3/2})$, the reuse of past trajectories remains largely unexplored
from a theoretical perspective. In this work, we provide the first rigorous
theoretical evidence that extensive reuse of past off-policy trajectories can
significantly accelerate convergence in PG methods. We introduce a power mean
correction to the multiple importance weighting estimator and propose RPG
(Retrospective Policy Gradient), a PG algorithm that combines old and new
trajectories for policy updates. Through a novel analysis, we show that, under
established assumptions, RPG achieves a sample complexity of
$\widetilde{O}(\epsilon^{-1})$, the best known rate in the literature. We
further validate empirically our approach against PG methods with
state-of-the-art rates.

</details>


### [121] [Physics-Informed Neural Networks for Control of Single-Phase Flow Systems Governed by Partial Differential Equations](https://arxiv.org/abs/2506.06188)
*Luis Kin Miyatake, Eduardo Camponogara, Eric Aislan Antonelo, Alexey Pavlov*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于物理信息神经网络的PINC框架，用于单相流动系统的建模与控制，其优势在于无需标记数据和迭代求解器，同时实现了高精度的动态表示和实时控制能力。


<details>
  <summary>更多</summary>
  
**动机:** 单相流动系统在瞬态条件下建模和控制具有挑战性，因此需要一种新的方法来处理受偏微分方程（PDEs）支配的系统的建模与控制问题。

**方法:** 将用于控制物理信息神经网络（PINC）扩展到PDE情况，分为两个阶段：稳态网络学习各种控制输入的平衡解，瞬态网络捕捉时间变化边界条件下的动态响应，并通过简化假设降低初始条件的空间坐标维度以进行高效训练。

**结果:** 数值实验验证了PINC模型能够准确表示流动动力学并实现实时控制应用，且仅使用物理定律进行训练，不需要标记数据。

**结论:** PINC模型无需迭代求解器即可有效近似PDE解，使其成为工程应用中流体流动监测和优化的有前景的替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Informed+Neural+Networks+for+Control+of+Single-Phase+Flow+Systems+Governed+by+Partial+Differential+Equations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06188，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06188&send_immediately=true&force_search=false)

**原文摘要:** The modeling and control of single-phase flow systems governed by Partial
Differential Equations (PDEs) present challenges, especially under transient
conditions. In this work, we extend the Physics-Informed Neural Nets for
Control (PINC) framework, originally proposed to modeling and control of
Ordinary Differential Equations (ODE) without the need of any labeled data, to
the PDE case, particularly to single-phase incompressible and compressible
flows, integrating neural networks with physical conservation laws. The PINC
model for PDEs is structured into two stages: a steady-state network, which
learns equilibrium solutions for a wide range of control inputs, and a
transient network, which captures dynamic responses under time-varying boundary
conditions. We propose a simplifying assumption that reduces the dimensionality
of the spatial coordinate regarding the initial condition, allowing the
efficient training of the PINC network. This simplification enables the
derivation of optimal control policies using Model Predictive Control (MPC). We
validate our approach through numerical experiments, demonstrating that the
PINC model, which is trained exclusively using physical laws, i.e., without
labeled data, accurately represents flow dynamics and enables real-time control
applications. The results highlight the PINC's capability to efficiently
approximate PDE solutions without requiring iterative solvers, making it a
promising alternative for fluid flow monitoring and optimization in engineering
applications.

</details>


### [122] [ICU-TSB: A Benchmark for Temporal Patient Representation Learning for Unsupervised Stratification into Patient Cohorts](https://arxiv.org/abs/2506.06192)
*Dimitrios Proios, Alban Bornet, Anthony Yazdani, Jose F Rodrigues Jr, Douglas Teodoro*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一个用于评估基于时间患者表征学习的患者分层的新基准ICU-TSB，并展示了其在三个ICU电子健康记录数据集上的应用效果。


<details>
  <summary>更多</summary>
  
**动机:** 通过改进诊断和治疗策略来推进个性化医疗，需要确定具有临床意义的亚组。

**方法:** 引入了ICU-TSB（时间分层基准），利用三个公开可用的ICU EHR数据集进行患者分层评估，并采用基于疾病分类的新型层次评估框架。

**结果:** 实验结果显示，v-measure在分类法的最顶层达到最高0.46，在最低层达到0.40，表明尽管有潜力，但该任务仍然具有挑战性。

**结论:** 本文得出时间表示学习可以重新发现具有临床意义的患者群体，但任务仍具挑战性，并且作者评估了多种为识别出的聚类分配可解释标签的策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ICU-TSB%3A+A+Benchmark+for+Temporal+Patient+Representation+Learning+for+Unsupervised+Stratification+into+Patient+Cohorts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06192&send_immediately=true&force_search=false)

**原文摘要:** Patient stratification identifying clinically meaningful subgroups is
essential for advancing personalized medicine through improved diagnostics and
treatment strategies. Electronic health records (EHRs), particularly those from
intensive care units (ICUs), contain rich temporal clinical data that can be
leveraged for this purpose. In this work, we introduce ICU-TSB (Temporal
Stratification Benchmark), the first comprehensive benchmark for evaluating
patient stratification based on temporal patient representation learning using
three publicly available ICU EHR datasets. A key contribution of our benchmark
is a novel hierarchical evaluation framework utilizing disease taxonomies to
measure the alignment of discovered clusters with clinically validated disease
groupings. In our experiments with ICU-TSB, we compared statistical methods and
several recurrent neural networks, including LSTM and GRU, for their ability to
generate effective patient representations for subsequent clustering of patient
trajectories. Our results demonstrate that temporal representation learning can
rediscover clinically meaningful patient cohorts; nevertheless, it remains a
challenging task, with v-measuring varying from up to 0.46 at the top level of
the taxonomy to up to 0.40 at the lowest level. To further enhance the
practical utility of our findings, we also evaluate multiple strategies for
assigning interpretable labels to the identified clusters. The experiments and
benchmark are fully reproducible and available at
https://github.com/ds4dh/CBMS2025stratification.

</details>


### [123] [Transformative or Conservative? Conservation laws for ResNets and Transformers](https://arxiv.org/abs/2506.06194)
*Sibylle Marcotte, Rémi Gribonval, Gabriel Peyré*

**主要类别:** cs.LG

**AI概要:** 该论文探讨了现代神经网络架构（如卷积ResNets和Transformer）在梯度流训练动态中的守恒定律，填补了浅层ReLU和线性网络以外的理论空白。


<details>
  <summary>更多</summary>
  
**动机:** 虽然浅层ReLU和线性网络的梯度流训练动态中守恒定律已经被充分研究，但更实际的网络架构（如ResNets和Transformers）的相关研究仍不完善。

**方法:** 作者首先分析了基本模块（如浅层ReLU或线性网络、注意力层）中的守恒定律，接着提出了仅依赖于部分参数的守恒定律，并展示了如何将这些定律扩展到整个网络结构。此外还讨论了连续梯度流与离散优化动力学（如SGD）之间的关系。

**结果:** 论文揭示了残差块与无跳跃连接块具有相同的守恒定律，完全描述了单个注意力层的守恒定律，并且证明了子结构的守恒定律可以独立分析，从而推广到整体网络。同时验证了这些守恒定律在SGD等离散优化方法下仍然成立。

**结论:** 该研究成功地将梯度流训练动态中的守恒定律从简单网络扩展到了现代深度学习架构，为理解复杂模型的优化过程提供了新的理论工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformative+or+Conservative%3F+Conservation+laws+for+ResNets+and+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06194，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06194&send_immediately=true&force_search=false)

**原文摘要:** While conservation laws in gradient flow training dynamics are well
understood for (mostly shallow) ReLU and linear networks, their study remains
largely unexplored for more practical architectures. This paper bridges this
gap by deriving and analyzing conservation laws for modern architectures, with
a focus on convolutional ResNets and Transformer networks. For this, we first
show that basic building blocks such as ReLU (or linear) shallow networks, with
or without convolution, have easily expressed conservation laws, and no more
than the known ones. In the case of a single attention layer, we also
completely describe all conservation laws, and we show that residual blocks
have the same conservation laws as the same block without a skip connection. We
then introduce the notion of conservation laws that depend only on a subset of
parameters (corresponding e.g. to a pair of consecutive layers, to a residual
block, or to an attention layer). We demonstrate that the characterization of
such laws can be reduced to the analysis of the corresponding building block in
isolation. Finally, we examine how these newly discovered conservation
principles, initially established in the continuous gradient flow regime,
persist under discrete optimization dynamics, particularly in the context of
Stochastic Gradient Descent (SGD).

</details>


### [124] [How to craft a deep reinforcement learning policy for wind farm flow control](https://arxiv.org/abs/2506.06204)
*Elie Kadoche, Pascal Bianchi, Florence Carton, Philippe Ciblat, Damien Ernst*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于深度强化学习的尾流转向控制策略，以优化风力发电场的能量生产，并克服现有方法的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 风电场中涡轮机之间的尾流效应显著降低了整体能源生产，而现有的机器学习方法在动态风况或大型风电场中表现有限。

**方法:** 结合图注意力网络和多头自注意力模块，并设计新的奖励函数和训练策略，计算每台风力发电机的偏航角度。

**结果:** 与全连接神经网络相比，该模型所需的训练步骤减少了约10倍，并比强优化基线提高了最多14%的能源生产能力。

**结论:** 这是首个在低保真、稳态数值模拟环境下有效适应各种时变风况的深度强化学习尾流导向控制器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+to+craft+a+deep+reinforcement+learning+policy+for+wind+farm+flow+control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06204&send_immediately=true&force_search=false)

**原文摘要:** Within wind farms, wake effects between turbines can significantly reduce
overall energy production. Wind farm flow control encompasses methods designed
to mitigate these effects through coordinated turbine control. Wake steering,
for example, consists in intentionally misaligning certain turbines with the
wind to optimize airflow and increase power output. However, designing a robust
wake steering controller remains challenging, and existing machine learning
approaches are limited to quasi-static wind conditions or small wind farms.
This work presents a new deep reinforcement learning methodology to develop a
wake steering policy that overcomes these limitations. Our approach introduces
a novel architecture that combines graph attention networks and multi-head
self-attention blocks, alongside a novel reward function and training strategy.
The resulting model computes the yaw angles of each turbine, optimizing energy
production in time-varying wind conditions. An empirical study conducted on
steady-state, low-fidelity simulation, shows that our model requires
approximately 10 times fewer training steps than a fully connected neural
network and achieves more robust performance compared to a strong optimization
baseline, increasing energy production by up to 14 %. To the best of our
knowledge, this is the first deep reinforcement learning-based wake steering
controller to generalize effectively across any time-varying wind conditions in
a low-fidelity, steady-state numerical simulation setting.

</details>


### [125] [Model-Driven Graph Contrastive Learning](https://arxiv.org/abs/2506.06212)
*Ali Azizpour, Nicolas Zilberstein, Santiago Segarra*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的图对比学习框架MGCL，利用graphon生成模型进行数据驱动的增强，提升了无监督图表示学习的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有GCL方法依赖人工或启发式增强策略，未考虑数据底层分布及不同图之间的相似性。

**方法:** 使用graphon估计数据分布，并据此设计了数据自适应的增强策略；对于图级任务，还按组聚类并估计每组的graphon以反映共享语义和结构。

**结果:** 在基准数据集上进行了大量实验，结果表明MGCL具有优越性能。

**结论:** MGCL通过结合图生成模型(graphon)来指导对比学习，实现了在节点和图分类任务中的先进性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-Driven+Graph+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06212，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06212&send_immediately=true&force_search=false)

**原文摘要:** We propose $\textbf{MGCL}$, a model-driven graph contrastive learning (GCL)
framework that leverages graphons (probabilistic generative models for graphs)
to guide contrastive learning by accounting for the data's underlying
generative process. GCL has emerged as a powerful self-supervised framework for
learning expressive node or graph representations without relying on annotated
labels, which are often scarce in real-world data. By contrasting augmented
views of graph data, GCL has demonstrated strong performance across various
downstream tasks, such as node and graph classification. However, existing
methods typically rely on manually designed or heuristic augmentation
strategies that are not tailored to the underlying data distribution and
operate at the individual graph level, ignoring similarities among graphs
generated from the same model. Conversely, in our proposed approach, MGCL first
estimates the graphon associated with the observed data and then defines a
graphon-informed augmentation process, enabling data-adaptive and principled
augmentations. Additionally, for graph-level tasks, MGCL clusters the dataset
and estimates a graphon per group, enabling contrastive pairs to reflect shared
semantics and structure. Extensive experiments on benchmark datasets
demonstrate that MGCL achieves state-of-the-art performance, highlighting the
advantages of incorporating generative models into GCL.

</details>


### [126] [Corrector Sampling in Language Models](https://arxiv.org/abs/2506.06215)
*Itai Gat, Neta Shaul, Uriel Singer, Yaron Lipman*

**主要类别:** cs.LG

**AI概要:** 为了解决自回归语言模型因固定和不可逆的令牌生成导致的错误累积问题，研究者提出了名为Resample-Previous-Tokens (RPT)的新采样方法。这种方法通过在已生成的文本窗口中迭代地回顾和可能替换令牌来减轻错误累积，同时保留了现有自回归模型的下一个令牌预测质量和速度。经过使用RPT对预训练的80亿参数模型进行100B的微调后，在推理和编码基准测试中相比标准采样方法有了大约10%的相对提升。


<details>
  <summary>更多</summary>
  
**动机:** 由于自回归语言模型的固定、不可逆的从左到右的令牌生成方式会导致错误累积，因此需要一种新方法来解决这个问题。

**方法:** 提出了一种新的采样方法Resample-Previous-Tokens（RPT），通过迭代回顾和可能替换先前生成文本窗口中的令牌来减轻错误累积。

**结果:** 对一个预训练的80亿参数模型进行仅100B的RPT微调后，在推理和编码基准上相较标准采样方法取得了约10%的相对改进。

**结论:** RPT方法可以集成到现有的自回归模型中，保持其下一个令牌预测质量和速度的同时，减轻错误累积。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Corrector+Sampling+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06215&send_immediately=true&force_search=false)

**原文摘要:** Autoregressive language models accumulate errors due to their fixed,
irrevocable left-to-right token generation. To address this, we propose a new
sampling method called Resample-Previous-Tokens (RPT). RPT mitigates error
accumulation by iteratively revisiting and potentially replacing tokens in a
window of previously generated text. This method can be integrated into
existing autoregressive models, preserving their next-token-prediction quality
and speed. Fine-tuning a pretrained 8B parameter model with RPT for only 100B
resulted in ~10% relative improvements on reasoning and coding benchmarks
compared to the standard sampling.

</details>


### [127] [Neural Responses to Affective Sentences Reveal Signatures of Depression](https://arxiv.org/abs/2506.06244)
*Aditya Kommineni, Woojae Jeong, Kleanthis Avramidis, Colin McDaniel, Myzelle Hughes, Thomas McGee, Elsi Kaiser, Kristina Lerman, Idan A. Blank, Dani Byrd, Assal Habibi, B. Rael Cahn, Sudarsana Kadiri, Takfarinas Medani, Richard M. Leahy, Shrikanth Narayanan*

**主要类别:** cs.LG

**AI概要:** 该研究探讨了抑郁症如何改变情绪处理的时间动态，发现抑郁个体在情感和自我参照信息整合上存在神经活动差异，并利用深度学习模型基于脑电图响应区分健康人与抑郁症患者及有/无自杀意念的抑郁亚组。


<details>
  <summary>更多</summary>
  
**动机:** 深入了解重度抑郁症（MDD）的神经认知基础对于识别诸如情绪和自我参照处理等核心功能的影响至关重要。

**方法:** 通过表面脑电图（EEG）测量健康个体和抑郁个体在阅读自我参照情感句子时的神经反应，使用深度学习模型进行分类分析，并进行空间消融实验以识别关键电极区域。

**结果:** 研究发现抑郁个体在句子观看期间表现出显著的神经活动差异，深度学习模型达到0.707的AUC用于区分健康与抑郁个体，0.624的AUC用于区分有和无自杀意念的抑郁亚组，空间消融实验表明前部电极区域对分类贡献最大。

**结论:** 研究结果表明抑郁症具有稳定的、由刺激驱动的神经特征，可能为未来的诊断工具提供依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Responses+to+Affective+Sentences+Reveal+Signatures+of+Depression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06244&send_immediately=true&force_search=false)

**原文摘要:** Major Depressive Disorder (MDD) is a highly prevalent mental health
condition, and a deeper understanding of its neurocognitive foundations is
essential for identifying how core functions such as emotional and
self-referential processing are affected. We investigate how depression alters
the temporal dynamics of emotional processing by measuring neural responses to
self-referential affective sentences using surface electroencephalography (EEG)
in healthy and depressed individuals. Our results reveal significant
group-level differences in neural activity during sentence viewing, suggesting
disrupted integration of emotional and self-referential information in
depression. Deep learning model trained on these responses achieves an area
under the receiver operating curve (AUC) of 0.707 in distinguishing healthy
from depressed participants, and 0.624 in differentiating depressed subgroups
with and without suicidal ideation. Spatial ablations highlight anterior
electrodes associated with semantic and affective processing as key
contributors. These findings suggest stable, stimulus-driven neural signatures
of depression that may inform future diagnostic tools.

</details>


### [128] [Lagrangian-based Equilibrium Propagation: generalisation to arbitrary boundary conditions & equivalence with Hamiltonian Echo Learning](https://arxiv.org/abs/2506.06248)
*Guillaume Pourcel, Debabrota Basu, Maxence Ernoult, Aditya Gilra*

**主要类别:** cs.LG

**AI概要:** 本研究提出GLEP方法以解决EP在时变输入上的局限性，并表明HEL继承了EP的高效特性。


<details>
  <summary>更多</summary>
  
**动机:** 将EP应用于时间变化输入的挑战在于轨迹描述和边界条件处理。

**方法:** 提出了GLEP框架，并分析不同边界条件下的学习算法及HEL的推导。

**结果:** 证明了HEL是从GLEP推导出的特殊情况，并具有适合硬件实现的独特属性。

**结论:** GLEP框架能够扩展EP到时变输入，而HEL是其特例且具备EP的硬件实现优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lagrangian-based+Equilibrium+Propagation%3A+generalisation+to+arbitrary+boundary+conditions+%26+equivalence+with+Hamiltonian+Echo+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06248，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06248&send_immediately=true&force_search=false)

**原文摘要:** Equilibrium Propagation (EP) is a learning algorithm for training
Energy-based Models (EBMs) on static inputs which leverages the variational
description of their fixed points. Extending EP to time-varying inputs is a
challenging problem, as the variational description must apply to the entire
system trajectory rather than just fixed points, and careful consideration of
boundary conditions becomes essential. In this work, we present Generalized
Lagrangian Equilibrium Propagation (GLEP), which extends the variational
formulation of EP to time-varying inputs. We demonstrate that GLEP yields
different learning algorithms depending on the boundary conditions of the
system, many of which are impractical for implementation. We then show that
Hamiltonian Echo Learning (HEL) -- which includes the recently proposed
Recurrent HEL (RHEL) and the earlier known Hamiltonian Echo Backpropagation
(HEB) algorithms -- can be derived as a special case of GLEP. Notably, HEL is
the only instance of GLEP we found that inherits the properties that make EP a
desirable alternative to backpropagation for hardware implementations: it
operates in a "forward-only" manner (i.e. using the same system for both
inference and learning), it scales efficiently (requiring only two or more
passes through the system regardless of model size), and enables local
learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [129] [A Path to Loving](https://arxiv.org/abs/2506.05352)
*John Beverley, Regina Hurley*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了爱的本体论描述，结合情感和评价判断，使用本体论工具为爱在人工智能和跨学科研究中的应用打下基础。


<details>
  <summary>更多</summary>
  
**动机:** 本文旨在对爱进行严谨的本体论描述，解决其哲学复杂性和科学相关性，并强调心理学和社会学方面的内容，以及这种描述如何增强基于人工智能的应用。

**方法:** 本文利用基本形式本体论（BFO）和其他应用本体方法来区分爱的各种含义，并捍卫情感和认知成分之间的因果关联模型。

**结果:** 文章提出了一种将被动感觉（如情绪兴奋）和主动评价判断（如将爱人视为有价值）结合在一起的观点，以平衡爱的非自愿方面与其理性责任。此外，还回应了关于感觉与判断之间关系的反对意见。

**结论:** 通过提供一个精确且可扩展的本体论描述，这项工作为未来跨学科应用奠定了基础，使爱成为本体工程、人工智能和科学领域中正式研究的主题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Path+to+Loving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05352，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05352&send_immediately=true&force_search=false)

**原文摘要:** This work lays the foundations for a rigorous ontological characterization of
love, addressing its philosophical complexity and scientific relevance, with
particular emphasis on psychology and sociology, as well as highlighting ways
in which such characterization enhances relevant AI based applications. The
position defended here is that love is best understood as a concatenation of
passive sensations (e.g., emotional arousal) and active evaluative judgments
(e.g., perceiving the beloved as valuable), in the interest of balancing the
involuntary aspects of love with its rational accountability. To provide a
structured foundation, the paper draws on Basic Formal Ontology (BFO) and other
applied ontological methods to differentiate various senses of love. This work
engages with objections to the understanding of love as concatenation,
particularly concerning the relationship between sensation and judgment. A
causal correlation model is defended, ensuring that the affective and cognitive
components are linked. By offering a precise and scalable ontological account,
this work lays the foundation for future interdisciplinary applications, making
love a subject of formal inquiry in ontology engineering, artificial
intelligence, and the sciences.

</details>


### [130] [Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems](https://arxiv.org/abs/2506.05370)
*Kristy Wedel*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一种新的范式 - Contextual Memory Intelligence (CMI)，旨在解决生成式AI在决策背景存储和反思方面的不足。


<details>
  <summary>更多</summary>
  
**动机:** 生成式AI工作流很少存储或反思决策的完整背景，这导致重复错误和普遍缺乏清晰度。

**方法:** 借鉴认知科学、组织理论、人机交互和AI治理等多个领域，提出了一个模块化架构——Insight Layer，通过人在环中的反思、漂移检测和理由保留来将上下文记忆纳入系统。

**结果:** 介绍了Contextual Memory Intelligence（CMI）作为一种新的构建智能系统的基础范式，并提出了Insight Layer以实现该愿景。

**结论:** CMI提供了一个创建智能系统的框架，使系统能够更好地推理数据、历史、判断和变化的背景，从而解决了当前AI架构和治理工作的基础盲点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Contextual+Memory+Intelligence+--+A+Foundational+Paradigm+for+Human-AI+Collaboration+and+Reflective+Generative+AI+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05370，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05370&send_immediately=true&force_search=false)

**原文摘要:** A critical challenge remains unresolved as generative AI systems are quickly
implemented in various organizational settings. Despite significant advances in
memory components such as RAG, vector stores, and LLM agents, these systems
still have substantial memory limitations. Gen AI workflows rarely store or
reflect on the full context in which decisions are made. This leads to repeated
errors and a general lack of clarity. This paper introduces Contextual Memory
Intelligence (CMI) as a new foundational paradigm for building intelligent
systems. It repositions memory as an adaptive infrastructure necessary for
longitudinal coherence, explainability, and responsible decision-making rather
than passive data. Drawing on cognitive science, organizational theory,
human-computer interaction, and AI governance, CMI formalizes the structured
capture, inference, and regeneration of context as a fundamental system
capability. The Insight Layer is presented in this paper to operationalize this
vision. This modular architecture uses human-in-the-loop reflection, drift
detection, and rationale preservation to incorporate contextual memory into
systems. The paper argues that CMI allows systems to reason with data, history,
judgment, and changing context, thereby addressing a foundational blind spot in
current AI architectures and governance efforts. A framework for creating
intelligent systems that are effective, reflective, auditable, and socially
responsible is presented through CMI. This enhances human-AI collaboration,
generative AI design, and the resilience of the institutions.

</details>


### [131] [Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference](https://arxiv.org/abs/2506.05422)
*Andrei T. Patrascu*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种基于构造逻辑推理的学习和规划框架，取代了传统的基于奖励的优化方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统强化学习需要大量探索，并且可能会出现不安全或无效的状态转换，因此需要一种确保决策正确性和安全性的新方法。

**方法:** 将动作、状态转移和目标表示为逻辑命题，并在直觉主义逻辑下通过构造性证明进行决策。

**结果:** 该方法实现了完全的安全性、可解释的行为以及高效的收敛性，且没有无效动作。

**结论:** 这项工作提供了一个新的强化学习方向，其基础不是数值优化，而是构造逻辑和证明理论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constructive+Symbolic+Reinforcement+Learning+via+Intuitionistic+Logic+and+Goal-Chaining+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05422，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05422&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel learning and planning framework that replaces
traditional reward-based optimisation with constructive logical inference. In
our model, actions, transitions, and goals are represented as logical
propositions, and decision-making proceeds by building constructive proofs
under intuitionistic logic. This method ensures that state transitions and
policies are accepted only when supported by verifiable preconditions --
eschewing probabilistic trial-and-error in favour of guaranteed logical
validity. We implement a symbolic agent operating in a structured gridworld,
where reaching a goal requires satisfying a chain of intermediate subgoals
(e.g., collecting keys to open doors), each governed by logical constraints.
Unlike conventional reinforcement learning agents, which require extensive
exploration and suffer from unsafe or invalid transitions, our constructive
agent builds a provably correct plan through goal chaining, condition tracking,
and knowledge accumulation. Empirical comparison with Q-learning demonstrates
that our method achieves perfect safety, interpretable behaviour, and efficient
convergence with no invalid actions, highlighting its potential for safe
planning, symbolic cognition, and trustworthy AI. This work presents a new
direction for reinforcement learning grounded not in numeric optimisation, but
in constructive logic and proof theory.

</details>


### [132] [Towards Data Systems That Are Business Semantic-Centric and AI Agents-Assisted](https://arxiv.org/abs/2506.05520)
*Cecil Pang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种以业务语义为中心、AI代理辅助的数据系统 BSDS，旨在解决现有数据平台与业务需求不匹配的问题，通过模块化架构和AI代理提高效率和可扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据平台往往强调工具而忽视与业务需求的契合，导致效率低下和延误，因此需要一种新的系统来弥合这一差距。

**方法:** 提出了一个整合架构、工作流程和团队组织的整体系统 BSDS，通过模块化设计和 AI 代理的引入，实现对数据系统的优化。

**结果:** BSDS 被验证能够加速数据驱动计划的上市时间，增强跨职能协作，并提供可扩展的蓝图。

**结论:** BSDS 提供了一个以业务语义为中心、AI代理辅助的数据系统，加速了数据驱动计划的上市时间，增强了跨职能协作，并为各类企业提供了可扩展的蓝图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Data+Systems+That+Are+Business+Semantic-Centric+and+AI+Agents-Assisted，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05520，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05520&send_immediately=true&force_search=false)

**原文摘要:** Contemporary businesses operate in dynamic environments requiring rapid
adaptation to achieve goals and maintain competitiveness. Existing data
platforms often fall short by emphasizing tools over alignment with business
needs, resulting in inefficiencies and delays. To address this gap, I propose
the Business Semantics Centric, AI Agents Assisted Data System (BSDS), a
holistic system that integrates architecture, workflows, and team organization
to ensure data systems are tailored to business priorities rather than dictated
by technical constraints. BSDS redefines data systems as dynamic enablers of
business success, transforming them from passive tools into active drivers of
organizational growth. BSDS has a modular architecture that comprises curated
data linked to business entities, a knowledge base for context-aware AI agents,
and efficient data pipelines. AI agents play a pivotal role in assisting with
data access and system management, reducing human effort, and improving
scalability. Complementing this architecture, BSDS incorporates workflows
optimized for both exploratory data analysis and production requirements,
balancing speed of delivery with quality assurance. A key innovation of BSDS is
its incorporation of the human factor. By aligning data team expertise with
business semantics, BSDS bridges the gap between technical capabilities and
business needs. Validated through real-world implementation, BSDS accelerates
time-to-market for data-driven initiatives, enhances cross-functional
collaboration, and provides a scalable blueprint for businesses of all sizes.
Future research can build on BSDS to explore optimization strategies using
complex systems and adaptive network theories, as well as developing autonomous
data systems leveraging AI agents.

</details>


### [133] [Avoiding Death through Fear Intrinsic Conditioning](https://arxiv.org/abs/2506.05529)
*Rodney Sanchez, Ferat Sahin, Alexander Ororbia, Jamison Heard*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种基于生物启发的内在奖励机制与神经架构，用于模拟恐惧条件反射行为，从而解决缺乏明确反馈的强化学习环境问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在评估时需要人工设计的外在奖励，而真实环境中存在缺乏反馈的高负奖励状态（如死亡），这促使研究者寻找更有效的内在激励机制。

**方法:** 引入了受早期杏仁核发育启发的内在奖励函数，并通过记忆增强神经网络（MANN）架构实现。

**结果:** 该方法能够有效避免探索终止状态，并表现出类似动物恐惧条件反射的行为；修改恐惧反应阈值还可以产生类似于广泛性焦虑障碍（GAD）的行为模式。

**结论:** 研究表明，生物学启发的神经架构和内在奖励机制可以有效地模拟恐惧条件反射，并帮助智能体在没有明确反馈的环境下进行决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Avoiding+Death+through+Fear+Intrinsic+Conditioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05529&send_immediately=true&force_search=false)

**原文摘要:** Biological and psychological concepts have inspired reinforcement learning
algorithms to create new complex behaviors that expand agents' capacity. These
behaviors can be seen in the rise of techniques like goal decomposition,
curriculum, and intrinsic rewards, which have paved the way for these complex
behaviors. One limitation in evaluating these methods is the requirement for
engineered extrinsic for realistic environments. A central challenge in
engineering the necessary reward function(s) comes from these environments
containing states that carry high negative rewards, but provide no feedback to
the agent. Death is one such stimuli that fails to provide direct feedback to
the agent. In this work, we introduce an intrinsic reward function inspired by
early amygdala development and produce this intrinsic reward through a novel
memory-augmented neural network (MANN) architecture. We show how this intrinsic
motivation serves to deter exploration of terminal states and results in
avoidance behavior similar to fear conditioning observed in animals.
Furthermore, we demonstrate how modifying a threshold where the fear response
is active produces a range of behaviors that are described under the paradigm
of general anxiety disorders (GADs). We demonstrate this behavior in the
Miniworld Sidewalk environment, which provides a partially observable Markov
decision process (POMDP) and a sparse reward with a non-descriptive terminal
condition, i.e., death. In effect, this study results in a
biologically-inspired neural architecture and framework for fear conditioning
paradigms; we empirically demonstrate avoidance behavior in a constructed agent
that is able to solve environments with non-descriptive terminal conditions.

</details>


### [134] [When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration](https://arxiv.org/abs/2506.05579)
*Quan Shi, Carlos E. Jimenez, Shunyu Yao, Nick Haber, Diyi Yang, Karthik Narasimhan*

**主要类别:** cs.AI

**AI概要:** 本文提出了KITE框架，用于评估人类-AI之间的知识迁移能力，并发现模型的基准性能并不总是反映其知识迁移能力。


<details>
  <summary>更多</summary>
  
**动机:** AI推理的最新进展是否能提高知识迁移能力，即模型能否以人类可理解、应用和学习的方式传递推理过程。

**方法:** 引入了KITE框架，并进行了大规模人类研究（N=118）来衡量人类-AI的知识迁移能力。

**结果:** 虽然模型基准性能与协作结果相关，但这种关系明显不一致，并且存在显著异常值。

**结论:** 知识迁移需要专门优化，而不仅仅是模型基准性能的体现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Models+Know+More+Than+They+Can+Explain%3A+Quantifying+Knowledge+Transfer+in+Human-AI+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05579&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in AI reasoning have driven substantial improvements
across diverse tasks. A critical open question is whether these improvements
also yields better knowledge transfer: the ability of models to communicate
reasoning in ways humans can understand, apply, and learn from. To investigate
this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a
conceptual and experimental framework for Human-AI knowledge transfer
capabilities and conduct the first large-scale human study (N=118) explicitly
designed to measure it. In our two-phase setup, humans first ideate with an AI
on problem-solving strategies, then independently implement solutions,
isolating model explanations' influence on human understanding. Our findings
reveal that although model benchmark performance correlates with collaborative
outcomes, this relationship is notably inconsistent, featuring significant
outliers, indicating that knowledge transfer requires dedicated optimization.
Our analysis identifies behavioral and strategic factors mediating successful
knowledge transfer. We release our code, dataset, and evaluation framework to
support future work on communicatively aligned models.

</details>


### [135] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing, Yeye He, Mengyu Zhou, Haoyu Dong, Shi Han, Lingjiao Chen, Dongmei Zhang, Surajit Chaudhuri, H. V. Jagadish*

**主要类别:** cs.AI

**AI概要:** 本文介绍了MMTU，一个用于评估模型对真实表格理解和推理能力的大规模基准测试，并指出即使是最前沿的模型也存在显著的改进空间。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs在处理表格方面取得了显著进展，但对此类能力的综合基准测试仍然有限。现有的评估过于狭隘地关注于特定任务，忽视了专业用户面临的更广泛的实际任务。

**方法:** 介绍了一个名为MMTU的大型基准测试，包含超过30K个问题，涵盖25种现实世界的表格任务。

**结果:** 结果显示，即使是当前前沿的推理模型，在MMTU上的得分也只有大约60%，表明在这方面还有很大的提升空间。

**结论:** MMTU是一个大规模基准测试，旨在全面评估模型对真实表格的理解、推理和操作能力。论文希望该基准测试能够推动在结构化数据处理和分析领域基础模型的发展和理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMTU%3A+A+Massive+Multi-Task+Table+Understanding+and+Reasoning+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05587，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05587&send_immediately=true&force_search=false)

**原文摘要:** Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [136] [Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists](https://arxiv.org/abs/2506.05616)
*Lianhao Zhou, Hongyi Ling, Keqiang Yan, Kaiji Zhao, Xiaoning Qian, Raymundo Arróyave, Xiaofeng Qian, Shuiwang Ji*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为 MAPPS 的自主材料发现框架，结合了人工智能、物理学和科学家的直觉，实现了更高的自主性和更优的性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究通常将代理限制在预定义的工作流程中执行特定任务，而本文旨在实现自动化工作流程规划，以实现更高程度的自主晶体材料发现。

**方法:** 提出了名为 MAPPS 的材料代理，该代理包含工作流规划器、工具代码生成器和科学协调器三个部分，分别用于生成结构化多步骤工作流、合成可执行 Python 代码以及协调通信和科学家反馈。

**结果:** MAPPS 在多个任务上进行了广泛的实验，证明其在自主材料发现方面具有灵活性和可靠性，同时在 MP-20 数据集上的稳定性、独特性和新颖性率显著提高了五倍。

**结论:** MAPPS 是一种有前景的自主材料发现框架，通过整合规划、物理和科学家实现了更大的自主性，并在 MP-20 数据集上评估时与之前的生成模型相比实现了稳定性、独特性和新颖性率的五倍提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Greater+Autonomy+in+Materials+Discovery+Agents%3A+Unifying+Planning%2C+Physics%2C+and+Scientists，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05616&send_immediately=true&force_search=false)

**原文摘要:** We aim at designing language agents with greater autonomy for crystal
materials discovery. While most of existing studies restrict the agents to
perform specific tasks within predefined workflows, we aim to automate workflow
planning given high-level goals and scientist intuition. To this end, we
propose Materials Agent unifying Planning, Physics, and Scientists, known as
MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a
Scientific Mediator. The Workflow Planner uses large language models (LLMs) to
generate structured and multi-step workflows. The Tool Code Generator
synthesizes executable Python code for various tasks, including invoking a
force field foundation model that encodes physics. The Scientific Mediator
coordinates communications, facilitates scientist feedback, and ensures
robustness through error reflection and recovery. By unifying planning,
physics, and scientists, MAPPS enables flexible and reliable materials
discovery with greater autonomy, achieving a five-fold improvement in
stability, uniqueness, and novelty rates compared with prior generative models
when evaluated on the MP-20 data. We provide extensive experiments across
diverse tasks to show that MAPPS is a promising framework for autonomous
materials discovery.

</details>


### [137] [Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach](https://arxiv.org/abs/2506.05619)
*Kihyun Kim, Jiawei Zhang, Asuman Ozdaglar, Pablo A. Parrilo*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种能够公平反映群体偏好分布的新颖偏好学习框架，并通过实验验证了其有效性和扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 传统偏好学习方法在聚合多个评估者的意见时倾向于更广泛持有的观点，可能导致政策偏向某些类型的观点或群体。

**方法:** 通过成对比较数据推断可行的评估者群体分布集，并利用这些估计构建满足特定公理的策略；采用软最大值放松方法平衡群体比例代表性和选择康多塞特胜者。

**结果:** 验证了所提方法在表格推荐任务和大规模语言模型对齐上的有效性与可扩展性。

**结论:** 该论文提出了一种新的偏好学习框架，能够在满足社会选择理论基本公理的同时，实现对评估者偏好的群体比例代表性与有界鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Population-Proportional+Preference+Learning+from+Human+Feedback%3A+An+Axiomatic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05619&send_immediately=true&force_search=false)

**原文摘要:** Conventional preference learning methods often prioritize opinions held more
widely when aggregating preferences from multiple evaluators. This may result
in policies that are biased in favor of some types of opinions or groups. The
objective of this paper is to develop a novel preference learning framework
capable of aligning aggregate opinions and policies proportionally with the
true population distribution of evaluator preferences. Our approach infers the
feasible set of evaluator population distributions directly from pairwise
comparison data. Using these estimates, the algorithm constructs a policy that
satisfies foundational axioms from social choice theory, namely monotonicity
and Pareto efficiency, as well as our newly-introduced axioms of
population-proportional representation and population-bounded robustness. We
propose a soft-max relaxation method that smoothly trade-offs
population-proportional representation with the selection of the Condorcet
winner (which beats all other options in pairwise comparisons). Finally, we
validate the effectiveness and scalability of our approach through experiments
on both tabular recommendation tasks and large-scale language model alignment.

</details>


### [138] [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/abs/2506.05744)
*Gouki Minegishi, Hiroki Furuta, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一个基于隐藏状态表示的推理图概念，并利用图论特性来解释和提升大规模推理模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望理解大规模推理模型在数学任务中表现出色的内部机制。

**方法:** 论文提出了一种从隐藏状态表示中提取推理图的方法，并分析了其图论特性，包括循环性、直径和小世界指数。

**结果:** 研究发现蒸馏后的推理模型相比基础模型展现出更强的循环性、更大的图直径以及显著的小世界特性，这些特性随任务难度和模型规模增加而增强。

**结论:** 论文得出结论，通过分析推理图的结构特性，可以提高对大规模推理模型的理解和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Topology+of+Reasoning%3A+Understanding+Large+Reasoning+Models+through+Reasoning+Graph+Properties，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05744，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05744&send_immediately=true&force_search=false)

**原文摘要:** Recent large-scale reasoning models have achieved state-of-the-art
performance on challenging mathematical benchmarks, yet the internal mechanisms
underlying their success remain poorly understood. In this work, we introduce
the notion of a reasoning graph, extracted by clustering hidden-state
representations at each reasoning step, and systematically analyze three key
graph-theoretic properties: cyclicity, diameter, and small-world index, across
multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled
reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly
more recurrent cycles (about 5 per sample), substantially larger graph
diameters, and pronounced small-world characteristics (about 6x) compared to
their base counterparts. Notably, these structural advantages grow with task
difficulty and model capacity, with cycle detection peaking at the 14B scale
and exploration diameter maximized in the 32B variant, correlating positively
with accuracy. Furthermore, we show that supervised fine-tuning on an improved
dataset systematically expands reasoning graph diameters in tandem with
performance gains, offering concrete guidelines for dataset design aimed at
boosting reasoning capabilities. By bridging theoretical insights into
reasoning graph structures with practical recommendations for data
construction, our work advances both the interpretability and the efficacy of
large reasoning models.

</details>


### [139] [Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/abs/2506.05967)
*Katarzyna Kobalczyk, Mihaela van der Schaar*

**主要类别:** cs.AI

**AI概要:** 本论文提出使用因果推理框架改进大型语言模型的奖励建模，解决泛化和鲁棒性问题。


<details>
  <summary>更多</summary>
  
**动机:** 奖励建模需要在新型提示-响应对上具有良好的泛化能力，但存在因果误识别、偏好异质性和用户特定因素引起的混杂等挑战。

**方法:** 将奖励建模问题置于因果范式中，利用因果识别的关键假设来分析当前方法的问题，并通过因果启发的方法提升模型鲁棒性。

**结果:** 展示了朴素奖励模型的失败模式，并证明了因果推理方法在解决这些问题上的有效性。

**结论:** 因果推理的方法能提高奖励模型的鲁棒性，并指出未来研究应关注目标干预以克服观察数据的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preference+Learning+for+AI+Alignment%3A+a+Causal+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05967&send_immediately=true&force_search=false)

**原文摘要:** Reward modelling from preference data is a crucial step in aligning large
language models (LLMs) with human values, requiring robust generalisation to
novel prompt-response pairs. In this work, we propose to frame this problem in
a causal paradigm, providing the rich toolbox of causality to identify the
persistent challenges, such as causal misidentification, preference
heterogeneity, and confounding due to user-specific factors. Inheriting from
the literature of causal inference, we identify key assumptions necessary for
reliable generalisation and contrast them with common data collection
practices. We illustrate failure modes of naive reward models and demonstrate
how causally-inspired approaches can improve model robustness. Finally, we
outline desiderata for future research and practices, advocating targeted
interventions to address inherent limitations of observational data.

</details>


### [140] [SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models](https://arxiv.org/abs/2506.05745)
*Emil Biju, Shayan Talaei, Zhemin Huang, Mohammadreza Pourreza, Azalia Mirhoseini, Amin Saberi*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种名为SPRINT的新框架，旨在提高大型推理模型在处理复杂任务时的效率，该框架通过在推理过程中动态识别并利用并行化机会，从而减少顺序思维链的长度。


<details>
  <summary>更多</summary>
  
**动机:** 大型推理模型在进行复杂推理任务时通常会产生很长的顺序思维链，导致最终答案出现前的推理时间过长，因此需要一种能够在推理过程中动态识别并利用并行化机会的方法。

**方法:** 通过引入一种新的数据整理流程，将自然语言推理路径重新组织为结构化的长视野规划和平行执行，并在少量整理后的数据上微调LRMs。

**结果:** 使用SPRINT框架微调的模型在数学等复杂领域与推理模型的表现相匹配，同时在需要超过8000个输出令牌的问题上生成的顺序令牌减少了约39%；对于GPQA和Countdown这两个分布外任务，平均顺序令牌分别减少了45%和65%，且达到了微调后推理模型的性能。

**结论:** SPRINT框架能够有效提高大型推理模型在复杂任务上的推理效率，同时保持了其性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SPRINT%3A+Enabling+Interleaved+Planning+and+Parallelized+Execution+in+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05745&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs) excel at complex reasoning tasks but typically
generate lengthy sequential chains-of-thought, resulting in long inference
times before arriving at the final answer. To address this challenge, we
introduce SPRINT, a novel post-training and inference-time framework designed
to enable LRMs to dynamically identify and exploit opportunities for
parallelization during their reasoning process. SPRINT incorporates an
innovative data curation pipeline that reorganizes natural language reasoning
trajectories into structured rounds of long-horizon planning and parallel
execution. By fine-tuning LRMs on a small amount of such curated data, the
models learn to dynamically identify independent subtasks within extended
reasoning processes and effectively execute them in parallel. Through extensive
evaluations, we show that the models fine-tuned with the SPRINT framework match
the performance of reasoning models on complex domains such as mathematics
while generating up to ~39% fewer sequential tokens on problems requiring more
than 8000 output tokens. Finally, we observe consistent results transferred to
two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%
reduction in average sequential tokens for longer reasoning trajectories, while
achieving the performance of the fine-tuned reasoning model.

</details>


### [141] [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
*Emmanuel Anaya Gonzalez, Sairam Vaidya, Kanghee Park, Ruyi Ji, Taylor Berg-Kirkpatrick, Loris D'Antoni*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一种新的约束采样框架，利用MCMC方法保证生成的样本既满足约束，又保持了模型的原始分布特性，并在实验中展示了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有约束解码方法通常会扭曲基础模型分布，这在需要生成多样化和有效的程序输入进行测试的应用中尤其成问题。

**方法:** 构建一个在有效输出上的提议分布，并应用基于LM似然的Metropolis-Hastings接受准则，以确保对约束空间进行有原则且高效的探索。

**结果:** 所提出的采样器在合成基准测试和现实世界的程序模糊测试任务中均优于现有方法。

**结论:** 论文提出了一种基于马尔可夫链蒙特卡洛（MCMC）的新约束采样框架，该框架能够同时满足三个核心需求：满足约束、单调收敛和高效生成高质量样本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constrained+Sampling+for+Language+Models+Should+Be+Easy%3A+An+MCMC+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05754，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05754&send_immediately=true&force_search=false)

**原文摘要:** Constrained decoding enables Language Models (LMs) to produce samples that
provably satisfy hard constraints. However, existing constrained-decoding
approaches often distort the underlying model distribution, a limitation that
is especially problematic in applications like program fuzzing, where one wants
to generate diverse and valid program inputs for testing purposes. We propose a
new constrained sampling framework based on Markov Chain Monte Carlo (MCMC)
that simultaneously satisfies three core desiderata: constraint satisfying
(every sample satisfies the constraint), monotonically converging (the sampling
process converges to the true conditional distribution), and efficient
(high-quality samples emerge in few steps). Our method constructs a proposal
distribution over valid outputs and applies a Metropolis-Hastings acceptance
criterion based on the LM's likelihood, ensuring principled and efficient
exploration of the constrained space. Empirically, our sampler outperforms
existing methods on both synthetic benchmarks and real-world program fuzzing
tasks.

</details>


### [142] [Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction](https://arxiv.org/abs/2506.05810)
*Yesheng Zhang, Wenjian Sun, Yuheng Chen, Qingwei Liu, Qi Lin, Rui Zhang, Xu Zhao*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种名为Trajectory Entropy的新方法，用于改进自动驾驶中的level-k博弈框架，以提高准确性和降低计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 传统的level-k博弈框架忽略了智能体之间不同的驾驶复杂性和跨博弈级别的智能体状态的动态变化，而是统一处理它们，导致引入冗余和容易出错的计算。因此，需要一种新的方法来解决这个问题。

**方法:** Trajectory Entropy方法从博弈中智能体的多模态轨迹预测结果中提取表示不确定性的统计信号，并利用该信号的信噪比来量化智能体的游戏状态。基于提出的Trajectory Entropy，通过一个简单的门控机制改进当前的level-k博弈框架。

**结果:** 所提出的方法在Waymo和nuPlan数据集上进行了评估，在轨迹预测、开环和闭环规划任务方面展示了最先进的性能，预测精度提高了最多19.89%，规划精度提高了最多16.48%。

**结论:** 该论文提出了一种称为Trajectory Entropy的方法，用于揭示在level-k博弈框架中智能体的游戏状态。通过这一方法，可以有效地减少冗余和容易出错的计算，并提高整体的准确性同时降低计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Trajectory+Entropy%3A+Modeling+Game+State+Stability+from+Multimodality+Trajectory+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05810&send_immediately=true&force_search=false)

**原文摘要:** Complex interactions among agents present a significant challenge for
autonomous driving in real-world scenarios. Recently, a promising approach has
emerged, which formulates the interactions of agents as a level-k game
framework. It effectively decouples agent policies by hierarchical game levels.
However, this framework ignores both the varying driving complexities among
agents and the dynamic changes in agent states across game levels, instead
treating them uniformly. Consequently, redundant and error-prone computations
are introduced into this framework. To tackle the issue, this paper proposes a
metric, termed as Trajectory Entropy, to reveal the game status of agents
within the level-k game framework. The key insight stems from recognizing the
inherit relationship between agent policy uncertainty and the associated
driving complexity. Specifically, Trajectory Entropy extracts statistical
signals representing uncertainty from the multimodality trajectory prediction
results of agents in the game. Then, the signal-to-noise ratio of this signal
is utilized to quantify the game status of agents. Based on the proposed
Trajectory Entropy, we refine the current level-k game framework through a
simple gating mechanism, significantly improving overall accuracy while
reducing computational costs. Our method is evaluated on the Waymo and nuPlan
datasets, in terms of trajectory prediction, open-loop and closed-loop planning
tasks. The results demonstrate the state-of-the-art performance of our method,
with precision improved by up to 19.89% for prediction and up to 16.48% for
planning.

</details>


### [143] [Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs](https://arxiv.org/abs/2506.05887)
*Marilyn Bello, Rafael Bello, Maria-Matilde García, Ann Nowé, Iván Sevillano-García, Francisco Herrera*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了如何通过多层次的解释框架来提升对AI的信任，并强调了大语言模型在社会层面解释中的作用。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能在敏感领域的广泛应用增加了对系统不仅要准确还要可解释和可信的需求，而现有的许多可解释AI（XAI）方法未考虑与AI系统交互的不同受众。

**方法:** 论文提出了一种多层次框架，包括算法和领域层、人性化层和社会可解释层，并通过案例研究展示了该方法在技术保真度、用户参与和社会责任方面的效果。

**结果:** 论文提出了一个包含三个层次的框架，并强调了大语言模型（LLMs）在生成易于理解的自然语言解释方面的作用，通过案例研究验证了该框架的有效性。

**结论:** 论文得出结论，通过设计和传递符合不同利益相关者期望的解释，可以建立对AI的信任，并将XAI重新定义为一个动态的、建立信任的过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainability+in+Context%3A+A+Multilevel+Framework+Aligning+AI+Explanations+with+Stakeholder+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05887&send_immediately=true&force_search=false)

**原文摘要:** The growing application of artificial intelligence in sensitive domains has
intensified the demand for systems that are not only accurate but also
explainable and trustworthy. Although explainable AI (XAI) methods have
proliferated, many do not consider the diverse audiences that interact with AI
systems: from developers and domain experts to end-users and society. This
paper addresses how trust in AI is influenced by the design and delivery of
explanations and proposes a multilevel framework that aligns explanations with
the epistemic, contextual, and ethical expectations of different stakeholders.
The framework consists of three layers: algorithmic and domain-based,
human-centered, and social explainability. We highlight the emerging role of
Large Language Models (LLMs) in enhancing the social layer by generating
accessible, natural language explanations. Through illustrative case studies,
we demonstrate how this approach facilitates technical fidelity, user
engagement, and societal accountability, reframing XAI as a dynamic,
trust-building process.

</details>


### [144] [Proactive Assistant Dialogue Generation from Streaming Egocentric Videos](https://arxiv.org/abs/2506.05904)
*Yichi Zhang, Xin Luna Dong, Zhaojiang Lin, Andrea Madotto, Anuj Kumar, Babak Damavandi, Joyce Chai, Seungwhan Moon*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的框架，解决了实时任务导向型AI系统中的数据与评估瓶颈问题。


<details>
  <summary>更多</summary>
  
**动机:** 当前在感知任务指导方面的实时系统开发受限于昂贵且耗时的数据收集和系统评估过程。

**方法:** 提出了一个包含新颖数据整理流程、自动评估指标和端到端模型的综合框架。

**结果:** 开发了一个大规模的合成对话数据集，验证了自动评估指标，并提出了可处理数据不平衡和长视频的新技术。

**结论:** 该论文为开发能够通过多样化任务提供实时、主动帮助的AI助手奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Proactive+Assistant+Dialogue+Generation+from+Streaming+Egocentric+Videos，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05904，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05904&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in conversational AI have been substantial, but developing
real-time systems for perceptual task guidance remains challenging. These
systems must provide interactive, proactive assistance based on streaming
visual inputs, yet their development is constrained by the costly and
labor-intensive process of data collection and system evaluation. To address
these limitations, we present a comprehensive framework with three key
contributions. First, we introduce a novel data curation pipeline that
synthesizes dialogues from annotated egocentric videos, resulting in \dataset,
a large-scale synthetic dialogue dataset spanning multiple domains. Second, we
develop a suite of automatic evaluation metrics, validated through extensive
human studies. Third, we propose an end-to-end model that processes streaming
video inputs to generate contextually appropriate responses, incorporating
novel techniques for handling data imbalance and long-duration videos. This
work lays the foundation for developing real-time, proactive AI assistants
capable of guiding users through diverse tasks. Project page:
https://pro-assist.github.io/

</details>


### [145] [CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents](https://arxiv.org/abs/2506.05981)
*Qingbin Zeng, Ruotong Zhao, Jinzhu Mao, Haoyang Li, Fengli Xu, Yong Li*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种名为CrimeMind的新框架，结合了大型语言模型和犯罪理论，能更准确地预测城市犯罪并评估政策干预效果。


<details>
  <summary>更多</summary>
  
**动机:** 传统的基于规则的代理模型（ABM）和深度学习方法在预测城市犯罪方面各有局限性，包括预测准确性有限、解释性差以及缺乏适应环境变化的认知灵活性。

**方法:** 提出了一种新的基于大型语言模型（LLM）的ABM框架，即CrimeMind，并将常规活动理论（RAT）整合到其代理工作流程中，通过一个无需训练的文本梯度方法使CrimeMind的感知与人类判断保持一致。

**结果:** 实验显示，CrimeMind在犯罪热点预测和空间分布准确性方面优于传统ABM和深度学习基线模型，最高提升了24%。此外，它还能成功模拟外部事件和政策干预对犯罪模式的影响。

**结论:** CrimeMind能够实现对个体行为的细粒度建模，并促进现实世界干预措施的评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CrimeMind%3A+Simulating+Urban+Crime+with+Multi-Modal+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05981，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05981&send_immediately=true&force_search=false)

**原文摘要:** Modeling urban crime is an important yet challenging task that requires
understanding the subtle visual, social, and cultural cues embedded in urban
environments. Previous work has predominantly focused on rule-based agent-based
modeling (ABM) and deep learning methods. ABMs offer interpretability of
internal mechanisms but exhibit limited predictive accuracy.In contrast, deep
learning methods are often effective in prediction but are less interpretable
and require extensive training data. Moreover, both lines of work lack the
cognitive flexibility to adapt to changing environments. Leveraging the
capabilities of large language models (LLMs), we propose CrimeMind, a novel
LLM-driven ABM framework for simulating urban crime within a multi-modal urban
context.A key innovation of our design is the integration of the Routine
Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to
process rich multi-modal urban features and reason about criminal
behavior.However, RAT requires LLM agents to infer subtle cues in evaluating
environmental safety as part of assessing guardianship, which can be
challenging for LLMs. To address this, we collect a small-scale human-annotated
dataset and align CrimeMind's perception with human judgment via a
training-free textual gradient method.Experiments across four major U.S. cities
demonstrate that CrimeMind outperforms both traditional ABMs and deep learning
baselines in crime hotspot prediction and spatial distribution accuracy,
achieving up to a 24% improvement over the strongest baseline.Furthermore, we
conduct counterfactual simulations of external incidents and policy
interventions and it successfully captures the expected changes in crime
patterns, demonstrating its ability to reflect counterfactual
scenarios.Overall, CrimeMind enables fine-grained modeling of individual
behaviors and facilitates evaluation of real-world interventions.

</details>


### [146] [CP-Bench: Evaluating Large Language Models for Constraint Modelling](https://arxiv.org/abs/2506.06052)
*Kostis Michailidis, Dimos Tsouros, Tias Guns*

**主要类别:** cs.AI

**AI概要:** 本研究提出CP-Bench数据集用于评估大型语言模型在约束编程中的建模能力，并展示了提升建模准确性的有效方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的约束建模评估数据集通常局限于小规模、同质或特定领域的实例，难以反映现实世界场景的多样性。为了填补这一空白，本文提出了一个多样化的基准数据集。

**方法:** 本文引入了一个新的基准数据集CP-Bench，并评估了三种不同的约束建模系统（MiniZinc语言、CPMpy库和OR-Tools CP-SAT求解器）的LLM建模能力。同时应用了基于提示和推理时间计算的方法来增强LLM生成模型的有效性。

**结果:** 研究结果显示，通过使用文档丰富的系统提示以及改进的采样和验证方法，LLM在新提出的CP-Bench基准数据集上的建模准确性最高可达70%。此外，Python-based框架在建模便利性方面表现出色。

**结论:** 本论文强调了Python框架在约束建模中的优势，并展示了文档丰富的系统提示结合重复采样和自我验证方法的有效性，显著提高了LLM生成有效约束模型的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CP-Bench%3A+Evaluating+Large+Language+Models+for+Constraint+Modelling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06052&send_immediately=true&force_search=false)

**原文摘要:** Combinatorial problems are present in a wide range of industries. Constraint
Programming (CP) is a well-suited problem-solving paradigm, but its core
process, namely constraint modelling, is a bottleneck for wider adoption.
Aiming to alleviate this bottleneck, recent studies have explored using Large
Language Models (LLMs) as modelling assistants, transforming combinatorial
problem descriptions to executable constraint models, similar to coding
assistants. However, the existing evaluation datasets for constraint modelling
are often limited to small, homogeneous, or domain-specific instances, which do
not capture the diversity of real-world scenarios. This work addresses this gap
by introducing CP-Bench, a novel benchmark dataset that includes a diverse set
of well-known combinatorial problem classes sourced from the CP community,
structured explicitly for evaluating LLM-driven CP modelling. With this
dataset, and given the variety of constraint modelling frameworks, we compare
and evaluate the modelling capabilities of LLMs for three distinct constraint
modelling systems, which vary in abstraction level and underlying syntax: the
high-level MiniZinc language and Python-based CPMpy library, and the
lower-level Python interface of the OR-Tools CP-SAT solver. In order to enhance
the ability of LLMs to produce valid constraint models, we systematically
evaluate the use of prompt-based and inference-time compute methods adapted
from existing LLM-based code generation research. Our results underscore the
modelling convenience provided by Python-based frameworks, as well as the
effectiveness of documentation-rich system prompts, which, augmented with
repeated sampling and self-verification, achieve further improvements, reaching
up to 70\% accuracy on this new, highly challenging benchmark.

</details>


### [147] [Decomposability-Guaranteed Cooperative Coevolution for Large-Scale Itinerary Planning](https://arxiv.org/abs/2506.06121)
*Ziyu Zhang, Peilan Xu, Yuetong Sun, Yuhui Shi, Wenjian Luo*

**主要类别:** cs.AI

**AI概要:** 本文研究大规模行程规划问题，提出一种新型多目标协同进化算法，通过动态分解策略和优化潜力定义，解决了组件不平衡和交互问题，在真实数据上验证了其性能优势。


<details>
  <summary>更多</summary>
  
**动机:** 大规模行程规划问题具有挑战性，传统的严格可分解性难以满足，因此需要引入弱可分解性定义和相应的图结构。

**方法:** 设计了一种基于组件归一化适应度的动态分解策略，并定义了优化潜力和计算资源分配策略。

**结果:** 实验表明，所提出的算法在与最先进的多目标行程规划算法比较中表现出优越性，且问题规模越大，性能优势越明显。

**结论:** 本文提出了一种用于大规模行程规划的多目标协同进化算法，并证明了其在真实世界数据集上的性能优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decomposability-Guaranteed+Cooperative+Coevolution+for+Large-Scale+Itinerary+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06121，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06121&send_immediately=true&force_search=false)

**原文摘要:** Large-scale itinerary planning is a variant of the traveling salesman
problem, aiming to determine an optimal path that maximizes the collected
points of interest (POIs) scores while minimizing travel time and cost, subject
to travel duration constraints. This paper analyzes the decomposability of
large-scale itinerary planning, proving that strict decomposability is
difficult to satisfy, and introduces a weak decomposability definition based on
a necessary condition, deriving the corresponding graph structures that fulfill
this property. With decomposability guaranteed, we propose a novel
multi-objective cooperative coevolutionary algorithm for large-scale itinerary
planning, addressing the challenges of component imbalance and interactions.
Specifically, we design a dynamic decomposition strategy based on the
normalized fitness within each component, define optimization potential
considering component scale and contribution, and develop a computational
resource allocation strategy. Finally, we evaluate the proposed algorithm on a
set of real-world datasets. Comparative experiments with state-of-the-art
multi-objective itinerary planning algorithms demonstrate the superiority of
our approach, with performance advantages increasing as the problem scale
grows.

</details>


### [148] [Integer Linear Programming Preprocessing for Maximum Satisfiability](https://arxiv.org/abs/2506.06216)
*Jialu Zhang, Chu-Min Li, Sami Cherif, Shuolin Li, Zhifei Zheng*

**主要类别:** cs.AI

**AI概要:** 该论文探讨了ILP预处理技术如何提升MaxSAT求解性能，并减少对ILP求解器的依赖。


<details>
  <summary>更多</summary>
  
**动机:** 由于MaxSAT问题在实际应用中的重要性以及大多数MaxSAT求解器已采用ILP求解器作为其投资组合的一部分，本文旨在研究ILP预处理技术在这一领域的作用。

**方法:** 本文通过实验评估了ILP预处理技术对MaxSAT求解的影响，并基于MaxSAT Evaluation 2024中的优胜求解器WMaxCDCL-OpenWbo1200进行了研究。

**结果:** 实验结果显示，ILP预处理技术帮助WMaxCDCL-OpenWbo1200解决了15个额外的实例，并减少了当前最先进的MaxSAT求解器对ILP求解器的依赖。

**结论:** 本文的结论是，ILP预处理技术对MaxSAT求解有积极影响，并减少了在包含WMaxCDCL或MaxCDCL的投资组合中调用ILP求解器的需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integer+Linear+Programming+Preprocessing+for+Maximum+Satisfiability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06216&send_immediately=true&force_search=false)

**原文摘要:** The Maximum Satisfiability problem (MaxSAT) is a major optimization challenge
with numerous practical applications. In recent MaxSAT evaluations, most MaxSAT
solvers have adopted an ILP solver as part of their portfolios. This paper
investigates the impact of Integer Linear Programming (ILP) preprocessing
techniques on MaxSAT solving. Experimental results show that ILP preprocessing
techniques help WMaxCDCL-OpenWbo1200, the winner of the MaxSAT evaluation 2024
in the unweighted track, solve 15 additional instances. Moreover, current
state-of-the-art MaxSAT solvers heavily use an ILP solver in their portfolios,
while our proposed approach reduces the need to call an ILP solver in a
portfolio including WMaxCDCL or MaxCDCL.

</details>


### [149] [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)
*Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, Xian Li*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种个性化大型语言模型代理框架PersonaAgent，通过个性化记忆和操作模块以及用户偏好对齐策略，实现定制化的用户体验。


<details>
  <summary>更多</summary>
  
**动机:** 当前的LLM代理通常采用一刀切的方法，缺乏灵活应对用户不同需求和偏好的能力。

**方法:** 开发了PersonaAgent框架，包括个性化记忆模块和个性化操作模块，并提出了一种测试时用户偏好对齐策略。

**结果:** 实验评估表明，PersonaAgent不仅有效个性化操作空间，还在测试时实际应用中显著优于其他基线方法。

**结论:** PersonaAgent能够有效地个性化操作空间，并在测试时实际应用中扩展，提供定制化的动态用户体验。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PersonaAgent%3A+When+Large+Language+Model+Agents+Meet+Personalization+at+Test+Time，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06254&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM) empowered agents have recently emerged as advanced
paradigms that exhibit impressive capabilities in a wide range of domains and
tasks. Despite their potential, current LLM agents often adopt a
one-size-fits-all approach, lacking the flexibility to respond to users'
varying needs and preferences. This limitation motivates us to develop
PersonaAgent, the first personalized LLM agent framework designed to address
versatile personalization tasks. Specifically, PersonaAgent integrates two
complementary components - a personalized memory module that includes episodic
and semantic memory mechanisms; a personalized action module that enables the
agent to perform tool actions tailored to the user. At the core, the persona
(defined as unique system prompt for each user) functions as an intermediary:
it leverages insights from personalized memory to control agent actions, while
the outcomes of these actions in turn refine the memory. Based on the
framework, we propose a test-time user-preference alignment strategy that
simulate the latest n interactions to optimize the persona prompt, ensuring
real-time user preference alignment through textual loss feedback between
simulated and ground-truth responses. Experimental evaluations demonstrate that
PersonaAgent significantly outperforms other baseline methods by not only
personalizing the action space effectively but also scaling during test-time
real-world applications. These results underscore the feasibility and potential
of our approach in delivering tailored, dynamic user experiences.

</details>


### [150] [Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens](https://arxiv.org/abs/2506.06261)
*Jihwan Jeong, Xiaoyu Wang, Jingmin Wang, Scott Sanner, Pascal Poupart*

**主要类别:** cs.AI

**AI概要:** 本文提出 RefPlan 方法，通过双贝叶斯离线模型解决强化学习中的不确定性问题，提高了策略的灵活性和鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的离线强化学习方法依赖于固定的保守策略，限制了适应性和泛化能力，而在线探索又往往成本高昂或不安全。

**方法:** 提出了一种新的双贝叶斯离线模型规划方法 RefPlan，通过将规划重新表述为贝叶斯后验估计来统一不确定性建模和模型规划。

**结果:** 实验结果表明 RefPlan 在标准基准测试中显著提升了性能，具有更强的鲁棒性和适应性。

**结论:** RefPlan 提高了保守离线RL策略的性能，尤其是在高度认知不确定性和有限数据的情况下，并展示了对环境动态变化的适应能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reflect-then-Plan%3A+Offline+Model-Based+Planning+through+a+Doubly+Bayesian+Lens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06261&send_immediately=true&force_search=false)

**原文摘要:** Offline reinforcement learning (RL) is crucial when online exploration is
costly or unsafe but often struggles with high epistemic uncertainty due to
limited data. Existing methods rely on fixed conservative policies, restricting
adaptivity and generalization. To address this, we propose Reflect-then-Plan
(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.
RefPlan unifies uncertainty modeling and MB planning by recasting planning as
Bayesian posterior estimation. At deployment, it updates a belief over
environment dynamics using real-time observations, incorporating uncertainty
into MB planning via marginalization. Empirical results on standard benchmarks
show that RefPlan significantly improves the performance of conservative
offline RL policies. In particular, RefPlan maintains robust performance under
high epistemic uncertainty and limited data, while demonstrating resilience to
changing environment dynamics, improving the flexibility, generalizability, and
robustness of offline-learned policies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [151] [Online Conformal Model Selection for Nonstationary Time Series](https://arxiv.org/abs/2506.05544)
*Shibo Li, Yao Zheng*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种用于非平稳时间序列在线模型选择的新框架MPS，该框架能够自适应地选择最适合当前动态变化的模型，并具有广泛的适用性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的模型选择方法依赖于平稳性假设，在动态环境中表现不佳，而现实世界的数据往往具有非平稳性，因此需要一种新的方法来解决这一问题。

**方法:** 结合共形推理和模型置信集，开发了一种能够实时更新候选模型置信集的方法，以适应未知形式的非平稳性。

**结果:** 通过仿真和实际数据分析，证明了MPS在非平稳条件下能够可靠且高效地识别最优模型，并能生成高质量的小规模模型集合。

**结论:** MPS作为一种通用框架，可以适应任何数据生成过程、数据结构、模型类别、训练方法和评估指标，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Conformal+Model+Selection+for+Nonstationary+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05544&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces the MPS (Model Prediction Set), a novel framework for
online model selection for nonstationary time series. Classical model selection
methods, such as information criteria and cross-validation, rely heavily on the
stationarity assumption and often fail in dynamic environments which undergo
gradual or abrupt changes over time. Yet real-world data are rarely stationary,
and model selection under nonstationarity remains a largely open problem. To
tackle this challenge, we combine conformal inference with model confidence
sets to develop a procedure that adaptively selects models best suited to the
evolving dynamics at any given time. Concretely, the MPS updates in real time a
confidence set of candidate models that covers the best model for the next time
period with a specified long-run probability, while adapting to nonstationarity
of unknown forms. Through simulations and real-world data analysis, we
demonstrate that MPS reliably and efficiently identifies optimal models under
nonstationarity, an essential capability lacking in offline methods. Moreover,
MPS frequently produces high-quality sets with small cardinality, whose
evolution offers deeper insights into changing dynamics. As a generic
framework, MPS accommodates any data-generating process, data structure, model
class, training method, and evaluation metric, making it broadly applicable
across diverse problem settings.

</details>


### [152] [Nonlinear Causal Discovery through a Sequential Edge Orientation Approach](https://arxiv.org/abs/2506.05590)
*Stella Huang, Qing Zhou*

**主要类别:** stat.ML

**AI概要:** 本文介绍了一种高效且稳健的非线性加性噪声模型下的因果发现方法，能够有效克服现有方法的多种局限性。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现有方法存在限制性的模型假设、严重依赖一般的独立性检验或需要大量计算时间的问题。本文旨在解决这些局限性。

**方法:** 通过利用成对加性噪声模型（PANM）来识别因果方向，并开发了一种统计检验方法以比较竞争方向下的子图的对数似然值。此外，还设计了一个排序过程，用于按照未定向边符合PANM的程度进行排序，从而定义边的评估顺序。

**结果:** 实验表明，该方法在合成数据和现实世界数据集上都具有较高的计算效率，对模型误设定具有较强的鲁棒性，并且持续优于许多现有的非线性DAG学习方法。

**结论:** 本文提出了一种新的基于约束的因果发现算法，该算法能够在非线性加性噪声模型下有效地学习因果DAG。这种方法在计算效率、对模型误设定的鲁棒性以及与现有方法的比较表现上均具有优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nonlinear+Causal+Discovery+through+a+Sequential+Edge+Orientation+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.05590，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.05590&send_immediately=true&force_search=false)

**原文摘要:** Recent advances have established the identifiability of a directed acyclic
graph (DAG) under additive noise models (ANMs), spurring the development of
various causal discovery methods. However, most existing methods make
restrictive model assumptions, rely heavily on general independence tests, or
require substantial computational time. To address these limitations, we
propose a sequential procedure to orient undirected edges in a completed
partial DAG (CPDAG), representing an equivalence class of DAGs, by leveraging
the pairwise additive noise model (PANM) to identify their causal directions.
We prove that this procedure can recover the true causal DAG assuming a
restricted ANM. Building on this result, we develop a novel constraint-based
algorithm for learning causal DAGs under nonlinear ANMs. Given an estimated
CPDAG, we develop a ranking procedure that sorts undirected edges by their
adherence to the PANM, which defines an evaluation order of the edges. To
determine the edge direction, we devise a statistical test that compares the
log-likelihood values, evaluated with respect to the competing directions, of a
sub-graph comprising just the candidate nodes and their identified parents in
the partial DAG. We further establish the structural learning consistency of
our algorithm in the large-sample limit. Extensive experiments on synthetic and
real-world datasets demonstrate that our method is computationally efficient,
robust to model misspecification, and consistently outperforms many existing
nonlinear DAG learning methods.

</details>


### [153] [Multilevel neural simulation-based inference](https://arxiv.org/abs/2506.06087)
*Yuga Hikida, Ayush Bharti, Niall Jeffrey, François-Xavier Briol*

**主要类别:** stat.ML

**AI概要:** 本论文提出了一种基于多层次蒙特卡洛技术的新型神经模拟推断方法，旨在解决因模拟器计算昂贵而导致的推理性能问题，在固定计算预算下提高了推断准确性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 当模型仅以模拟器形式存在时，贝叶斯推断面临挑战，而编写似然函数可能非常困难。虽然神经SBI是一种流行的解决方案，但其性能受限于计算昂贵的模拟器，这限制了可执行的模拟次数。

**方法:** 该研究通过结合多级模拟器（具有不同的成本和保真度）和多级蒙特卡洛技术，改进了现有的神经模拟推断方法。作者还进行了理论分析和广泛的实验验证。

**结果:** 所提出的多级神经SBI方法在各种实验中表现出了更高的准确性，并且能够在固定计算资源下优化模拟器使用效率。

**结论:** 本文提出了一种利用多层次蒙特卡洛技术的新方法，以在固定计算预算下显著提高神经模拟推理的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multilevel+neural+simulation-based+inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06087&send_immediately=true&force_search=false)

**原文摘要:** Neural simulation-based inference (SBI) is a popular set of methods for
Bayesian inference when models are only available in the form of a simulator.
These methods are widely used in the sciences and engineering, where writing
down a likelihood can be significantly more challenging than constructing a
simulator. However, the performance of neural SBI can suffer when simulators
are computationally expensive, thereby limiting the number of simulations that
can be performed. In this paper, we propose a novel approach to neural SBI
which leverages multilevel Monte Carlo techniques for settings where several
simulators of varying cost and fidelity are available. We demonstrate through
both theoretical analysis and extensive experiments that our method can
significantly enhance the accuracy of SBI methods given a fixed computational
budget.

</details>
