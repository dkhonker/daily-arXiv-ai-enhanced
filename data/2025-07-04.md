<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 64]
- [cs.AI](#cs.AI) [总数: 31]
- [stat.ML](#stat.ML) [总数: 5]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan, Qi Wang, Haining Wang, Ruizhi Chengze, Yi Zhang, Hongsheng Liu, Zidong Wang, Fan Yu, Qi Qi, Hao Sun*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种可学习且可微分的有限体积求解器（LDSolver），用于在时空粗网格上高效且精确地模拟流体流动。它包含两个关键组件：一个可微分的有限体积求解器和一个可学习模块，后者提供通量的等效近似、导数和插值，并对粗网格上的时间误差进行校正。实验表明，即使在有限训练数据下，LDSolver也能加速模拟并保持高精度和优越的泛化能力，超越了基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 经典的数值求解器需要精细的时空网格以满足稳定性、一致性和收敛性条件，从而导致巨大的计算成本。尽管机器学习方法展示了更高的效率，但它们通常存在解释性、泛化能力和数据依赖性的问题。因此，研究者希望开发一种结合机器学习优势的同时克服其局限性的新方法。

**方法:** LDSolver由两部分组成：1) 可微分的有限体积求解器；2) 可学习模块，该模块提供了通量的等效近似，包括导数和插值，并在粗网格上进行时间误差校正。这种方法允许在粗网格上进行高效且准确的流体流动模拟，同时减少了对大量数据的依赖。

**结果:** 实验结果表明，LDSolver在不同的流系统（如Burgers、衰减、强制和剪切流）中表现出最先进的性能，显著优于基线模型。即使在有限的训练数据下，模型也能加速模拟并保持高精度和良好的泛化能力。

**结论:** LDSolver为在粗网格上高效且准确地模拟流体流动提供了一种新颖且有效的方法。它的成功证明了将机器学习与传统数值方法相结合的潜力，同时解决了传统方法和纯机器学习方法的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learnable-Differentiable+Finite+Volume+Solver+for+Accelerated+Simulation+of+Flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01975&send_immediately=true&force_search=false)

**原文摘要:** Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [2] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long, Xiangzhi Huang, Jiemin Xie, Ming Cai*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的图卷积网络结构DKGCM，通过结合时空预测方法和强化学习策略，改进了交通需求预测的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 准确的交通需求预测能够提高资源分配效率和利用率，但复杂的时空关系限制了现有模型的表现。

**方法:** 提出了DKGCM结构，包括基于时间相似性的聚类图卷积方法DK-GCN（使用DTW和K-means），以及在双向Mamba框架中集成FFT来捕捉时间依赖性，并采用GRPO强化学习策略优化模型训练。

**结果:** 实验表明，该模型优于几种先进方法，并在三个公开数据集上取得了良好的结果。

**结论:** 所提出的DKGCM模型有效提升了交通需求预测的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DKGCM%3A+A+Spatio-Temporal+Prediction+Model+for+Traffic+Flow+by+Fusing+Spatial+Node+Clustering+Method+and+Fourier+Bidirectional+Mamba+Mechanism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01982&send_immediately=true&force_search=false)

**原文摘要:** Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [3] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

**主要类别:** cs.LG

**AI概要:** 研究了多模态特征组合在检测社交媒体中的虚假信息方面的有效性，发现结合无监督和有监督机器学习模型可提高分类性能，并分析了虚假信息的传播模式。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于虚假信息检测的研究主要集中在基于文本或图像的方法上，而对多模态特征组合的研究较少。因此，本研究旨在探索将文本、图像和社会特征结合起来，以提高虚假信息检测的效果。

**方法:** 采用早期融合方法构建分类模型，分析1,529条推文，这些推文包含文本和图像，并通过对象检测和OCR等技术提取额外的社会和视觉特征。结合无监督和有监督机器学习模型进行分类。

**结果:** 与单模态模型相比，分类性能提高了15%，与双模态模型相比提高了5%。此外，还分析了虚假信息的传播模式。

**结论:** 多模态特征组合可以有效提高虚假信息检测的性能，未来的研究可以进一步探索更复杂的模型和更多的特征组合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Misinformation+Detection+Using+Early+Fusion+of+Linguistic%2C+Visual%2C+and+Social+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01984，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01984&send_immediately=true&force_search=false)

**原文摘要:** Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [4] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai, Deyu Li, Jiye Liang, Yanhui Zhai*

**主要类别:** cs.LG

**AI概要:** 在大数据环境下，特征选择对于智能机器的成功至关重要。然而，面对海量数据，智能机器通常缺乏足够的计算资源。本文基于采样技术和粗糙集理论提出了一种新的特征选择方法。该方法通过构建正域保留样本，能够在个人电脑上可接受的时间内找到具有高辨别能力的特征子集，并且可以在寻找约简之前估计使用所选特征子集能够区分的对象对的概率下界。实验结果表明，该方法可以在极短时间内找到近似约简，且最终约简的辨别能力大于估计的下界。


<details>
  <summary>更多</summary>
  
**动机:** 智能机器在处理海量数据时，由于计算资源有限，难以有效地进行特征选择，从而影响其性能和效率。因此，需要一种高效的方法来解决大数据环境下的特征选择问题。

**方法:** 本文提出了一种基于采样技术和粗糙集理论的特征选择方法。具体而言，采用可区分对象对的比例来衡量特征集的辨别能力，并基于此度量构建了一种新的特征选择方法。该方法通过从海量数据中构造正域保留样本，以找到具有高辨别能力的特征子集。此外，还可以在寻找约简之前估计使用所选特征子集能够区分的对象对的概率下界。

**结果:** 通过对11个不同规模的数据集进行验证，结果表明该方法可以在非常短的时间内找到近似约简，并且最终约简的辨别能力大于估计的下界。此外，在四个大规模数据集上的实验也表明，可以在合理的时间内在个人电脑上获得具有高辨别能力的近似约简。

**结论:** 本文提出了一种基于采样技术和粗糙集理论的特征选择方法，该方法能够在个人电脑上可接受的时间内完成对海量数据的特征选择，并保持较高的辨别能力。同时，该方法可以预先估计所选特征子集的辨别能力下界，为实际应用提供了可靠的保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Positive+region+preserved+random+sampling%3A+an+efficient+feature+selection+method+for+massive+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01998&send_immediately=true&force_search=false)

**原文摘要:** Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [5] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey, Daniel Sorensen, Minjin Hwang, Sandip Halder*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于机器学习的多变量时间序列（MTS）数据分析方法，用于半导体制造中的异常检测。该方法通过连续小波变换将MTS数据转换为图像表示，使用预训练的VGG-16架构进行微调分类，并构建孪生网络以比较信号嵌入，从而判断是否存在异常。此方法在实际FAB过程时间序列数据集上表现出高准确性，适用于有监督和半监督环境下的离线异常检测。


<details>
  <summary>更多</summary>
  
**动机:** 半导体制造过程复杂，涉及大量互相关参数，需要实时监控、故障检测和预测性维护。然而，由于高维数据、类别不平衡、噪声缺失测量以及生产系统的非平稳行为，半导体制造中的异常预测面临诸多挑战。此外，变量间的复杂依赖关系和下游阶段故障延迟出现也增加了异常检测和根本原因分析的难度。

**方法:** 该方法包括三个主要步骤：
1. 使用连续小波变换（CWT）将多变量时间序列数据转换为基于图像的表示。
2. 通过在自定义CWT图像数据集上微调预训练的VGG-16架构，开发多类图像分类器。
3. 构建一个由两个相同子网络组成的孪生网络，每个子网络使用微调后的VGG-16作为骨干。输入为成对的CWT图像，其中一个作为参考或锚点（代表已知良好信号），另一个作为查询（代表未知信号）。模型通过比较两者的嵌入来判断它们是否属于同一类。

**结果:** 该方法在真实FAB过程时间序列数据集上展示了高准确率的异常识别能力，为过程和工具轨迹数据中的离线异常检测提供了有希望的解决方案。

**结论:** 所提出的基于机器学习的方法在半导体制造过程中异常检测方面表现出了高效性。该方法不仅灵活性强，而且可以应用于有监督和半监督设置中，为解决半导体制造中的复杂问题提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuous+Wavelet+Transform+and+Siamese+Network-Based+Anomaly+Detection+in+Multi-variate+Semiconductor+Process+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01999&send_immediately=true&force_search=false)

**原文摘要:** Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [6] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab, Ahmet Iscen, Mathilde Caron, Alireza Fathi, Cordelia Schmid*

**主要类别:** cs.LG

**AI概要:** 尽管最近视觉-语言模型（VLMs）有所进展，但长视频理解仍然是一个挑战。现有的长上下文VLMs虽然能处理约1000帧输入，但在有效利用序列长度和抵御无关干扰方面仍显不足。本文提出了一种名为“Temporal Chain of Thought”的推理策略，用于视频问答中对模型输入上下文的优化。通过使用VLM自身迭代识别并提取视频中最相关的帧，从而提高准确率，并在4个不同的视频问答数据集上达到了最先进的结果。特别是在时长超过1小时的长视频上，该方法在32K上下文窗口下的表现优于相同VLM在700K标准推理窗口下的表现，提升了2.8个百分点。


<details>
  <summary>更多</summary>
  
**动机:** 当前的视觉-语言模型在处理长视频时，虽能处理大量输入帧，但难以有效利用这些信息并容易受到无关内容的干扰。因此需要一种新的方法来优化模型对长视频的理解能力。

**方法:** 提出了一种名为“Temporal Chain of Thought”的推理策略，该策略通过使用VLM本身迭代地从视频中识别并提取最相关的关键帧作为输入，从而优化了模型的输入上下文。这种方法在推理阶段增加了计算量以选择最相关的上下文内容。

**结果:** 在4个不同的视频问答数据集上，使用3种不同的VLMs均取得了显著的性能提升，特别在时长超过1小时的长视频上，该方法在较小的上下文窗口下（32K）的表现优于相同VLM在较大窗口下（700K）的表现，提升了2.8个百分点。

**结论:** 本文提出的“Temporal Chain of Thought”推理策略能够有效提升视频问答任务的准确性，尤其是在处理长视频时，展现出了超越现有方法的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal+Chain+of+Thought%3A+Long-Video+Understanding+by+Thinking+in+Frames，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02001，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02001&send_immediately=true&force_search=false)

**原文摘要:** Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [7] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody, Youpeng Zhao, Jun Wang*

**主要类别:** cs.LG

**AI概要:** 为了解决图卷积网络（GCNs）中稀疏矩阵乘法（SpGEMM）在资源受限系统中的内存和I/O瓶颈问题，本文提出了一种名为AIRES的新方法。AIRES通过算法和系统的协同设计优化了数据对齐和内存分配，并采用三阶段动态调度策略来减少I/O延迟并提高GPU利用率，从而显著提升了性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的解决出核SpGEMM的方法存在高I/O延迟和GPU利用率不足的问题，主要瓶颈在于稀疏格式的数据对齐和内存分配。这促使研究者探索一种新的解决方案以加速GCNs中的SpGEMM计算。

**方法:** AIRES从算法和系统两个层面进行优化：1. 算法上，提出了基于块级别的数据对齐方法以及行块对齐的平铺算法；2. 系统上，采用了三阶段动态调度策略，利用分层存储系统（包括GPU内存、GDS和主机内存）来降低I/O延迟并提升吞吐量。

**结果:** 实验结果表明，AIRES相较于最先进的方法表现出更优的性能，在实际图处理基准测试中，其延迟最多降低了1.8倍。

**结论:** AIRES通过算法-系统协同设计有效解决了现有系统中的性能瓶颈问题，大幅提升了出核SpGEMM计算的效率，适用于大规模图数据处理任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AIRES%3A+Accelerating+Out-of-Core+GCNs+via+Algorithm-System+Co-Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02006&send_immediately=true&force_search=false)

**原文摘要:** Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [8] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao, Jiaqi Han, Siyi Gu, Mingjian Jiang, James Zou, Stefano Ermon*

**主要类别:** cs.LG

**AI概要:** 提出了一种SE(3)-等变适配器框架（GeoAda），通过灵活且参数高效的微调方法，在不修改原始模型架构的情况下，实现对生成任务的几何控制。该方法在多种几何控制类型和应用领域中表现出色，同时避免了过拟合和灾难性遗忘问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管几何扩散模型在分子动力学和结构生成方面取得了显著成功，但针对下游任务的高效微调方法，特别是在不同几何控制下的应用，仍然有待探索。

**方法:** GeoAda引入了一种结构化适配器设计：首先通过耦合算子编码控制信号，然后使用预训练模型层的可训练副本进行处理，最后通过解耦算子和等变零初始化卷积投影回原空间。仅微调这些轻量级适配器模块，从而保持模型的几何一致性并减少过拟合和灾难性遗忘。

**结果:** 实验证明，GeoAda在各种几何控制类型和应用领域中实现了最先进的微调性能，同时保留了原始任务的准确性。其他基线方法由于过拟合和灾难性遗忘导致性能显著下降。

**结论:** GeoAda提供了一种灵活且高效的微调方法，适用于受控生成任务，并确保了几何归纳偏差在适应过程中的完整性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GeoAda%3A+Efficiently+Finetune+Geometric+Diffusion+Models+with+Equivariant+Adapters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02085&send_immediately=true&force_search=false)

**原文摘要:** Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [9] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg, Arunava Samajpati, Sivasankaran Chandrasekar, Varun Kacholia*

**主要类别:** cs.LG

**AI概要:** 本研究评估了几种最先进的大型语言模型（LLMs）在招聘候选人匹配中的表现，并将其与专有的领域特定招聘模型Match Score进行比较。实验结果表明，Match Score在预测准确性和公平性方面均优于通用LLMs。作者强调了在高风险领域（如招聘）中使用领域特定建模和偏差审计的重要性，并指出精心设计的算法可以同时实现招聘的准确性和结果的公平性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在招聘中的应用能够简化候选人的筛选过程，但若缺乏足够的保障措施，可能会引发关于准确性和算法偏见的严重问题。因此，有必要对这些模型进行基准测试，以了解其在招聘场景中的表现及潜在偏见。

**方法:** 研究者们选取了来自OpenAI、Anthropic、Google、Meta和Deepseek等公司的几种最先进的基础LLMs，并将它们与自家专有的领域特定招聘模型Match Score进行对比。通过在大约10,000个真实世界最近的候选人-职位对的数据集上进行实验，评估各模型的预测准确性（ROC AUC、Precision-Recall AUC、F1分数）和公平性（基于声明的性别、种族和交叉子群体的截止分析影响比率）。

**结果:** 实验结果显示，Match Score在准确性（ROC AUC 0.85 vs 0.77）和跨人口统计群体的公平性方面均优于通用LLMs。具体而言，Match Score达到了最低种族影响比率为0.957（接近平等），而最佳LLMs的影响比率仅为0.809或更低。对于交叉子群体，影响比率分别为0.906和0.773。

**结论:** 研究结果表明，在高风险领域（如招聘）中部署AI时，领域特定建模和偏差审计至关重要。此外，不应在准确性和公平性之间做出选择，因为一个精心设计的算法可以同时实现这两者。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+the+Promise+and+Pitfalls+of+LLMs+in+Hiring+Decisions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02087&send_immediately=true&force_search=false)

**原文摘要:** The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [10] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu, Lin F. Yang, Sharan Vaswani*

**主要类别:** cs.LG

**AI概要:** 本论文研究了无限时域折扣线性约束马尔可夫决策过程(CMDPs)，提出了一种基于原始-对偶框架的方法，并通过镜像下降值迭代(MDVI)作为示例求解器。论文提供了两种情况下的样本复杂度界：允许小约束违反的松弛可行性和严格要求满足约束的严格可行性，分别得到了与维度d和精度ε近乎最优依赖的结果。


<details>
  <summary>更多</summary>
  
**动机:** CMDPs问题是强化学习中一类重要的问题，其中目标是在满足某些约束条件下最大化累积奖励。然而，现有的方法可能无法有效处理约束条件或需要大量样本。因此，研究者们希望找到一种能够高效解决CMDPs问题的算法，同时保证样本复杂度较低。

**方法:** 作者提出了一个基于原始-对偶框架的方法，该方法可以利用任何无约束MDP求解器来解决CMDPs问题。具体地，对于线性CMDPs，他们使用了镜像下降值迭代（MDVI）作为示例求解器。此外，作者分析了两种情况下的样本复杂度：(i) 允许小约束违反的松弛可行性；(ii) 严格要求满足约束的严格可行性。

**结果:** 在松弛可行性情况下，算法可以通过$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$样本以高概率返回一个$\epsilon$-最优策略，结果对$d$和$\epsilon$具有近乎最优的依赖关系。在严格可行性情况下，算法需要$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$样本，其中$\zeta$是与问题相关的Slater常数，刻画了可行区域的大小。最后，作者将该框架应用于表格型CMDPs，证明它可以恢复在此设置下近乎最优的样本复杂度。

**结论:** 本文提出的原始-对偶框架结合MDVI求解器为线性CMDPs提供了一种有效的解决方案。对于松弛可行性和严格可行性两种情况，均给出了理论上近似最优的样本复杂度界。这些结果不仅适用于线性CMDPs，还可以扩展到表格型CMDPs，进一步验证了该方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Complexity+Bounds+for+Linear+Constrained+MDPs+with+a+Generative+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02089&send_immediately=true&force_search=false)

**原文摘要:** We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [11] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim, Giung Nam, Juho Lee*

**主要类别:** cs.LG

**AI概要:** 通过自蒸馏迭代改进基础模型，可显著提升生成质量，特别是在目标分布集中于基础模型低概率输出的约束生成任务中。


<details>
  <summary>更多</summary>
  
**动机:** 在受限文本生成中，当目标分布集中在基础模型低概率输出时，学习变得困难，因为奖励信号稀疏且无信息。

**方法:** 通过自蒸馏迭代地改进基础模型，使其逐渐与目标对齐。

**结果:** 这种方法在生成质量上带来了显著的提升。

**结论:** 自蒸馏是一种有效的方法，可以改善受限文本生成中的学习挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Constrained+Generation+in+Language+Models+via+Self-Distilled+Twisted+Sequential+Monte+Carlo，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02315，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02315&send_immediately=true&force_search=false)

**原文摘要:** Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [12] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, Tariq Iqbal*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了是否可以通过纯无监督学习来推广系统2思维方法，并开发出会思考的模型。研究者提出了基于能量的变压器(EBTs)，该方法在训练和推理过程中都表现出色，相较于现有的Transformer++和其他模型，EBTs在多模态任务上具有更高的扩展性和性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 当前的推理计算技术（类似于人类的系统2思维）虽然流行但存在诸多限制，如特定模态、特定问题或需要额外监督/训练。因此，研究者希望探索是否可以仅通过无监督学习来推广这些系统2思维方法，从而开发出会思考的模型。

**方法:** 研究者训练了基于能量的变压器(EBTs)，这是一种新的基于能量的模型(EBMs)。EBTs通过对每个输入和候选预测对赋予能量值，使用梯度下降法进行能量最小化直至收敛，从而实现预测。这种方法适用于离散（文本）和连续（视觉）模态。

**结果:** 在训练过程中，EBTs相较于主导的Transformer++方法扩展得更快，数据、批次大小、参数、FLOPs和深度的扩展率提高了35%。在推理过程中，EBTs在语言任务上通过系统2思维提升了29%的性能，并且在图像去噪任务上比扩散变压器表现更好，同时使用的前向传递更少。此外，在大多数下游任务中，EBTs在预训练性能相同或较差的情况下仍能取得更好的结果。

**结论:** 基于能量的变压器(EBTs)是一种有前景的新范式，可以同时提升模型的学习能力和思考能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Based+Transformers+are+Scalable+Learners+and+Thinkers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02092&send_immediately=true&force_search=false)

**原文摘要:** Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [13] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla, Luca A. Lanzendörfer, Longxiang Jiao, Roger Wattenhofer*

**主要类别:** cs.LG

**AI概要:** 本文提出PANAMA框架，采用主动学习和梯度优化方法，以最少的数据点训练吉他放大器模型，适用于样本受限的情况。


<details>
  <summary>更多</summary>
  
**动机:** 为了减少训练吉他放大器模型所需的大量数据点，并提高在有限样本下的模型训练效果。

**方法:** 介绍了一种名为PANAMA的主动学习框架，该框架利用类似WaveNet的架构来训练端到端参数化的吉他放大器模型。通过主动学习策略确定记录样本，以最小化数据点（即放大器旋钮设置）的使用。

**结果:** 结果表明，基于梯度的优化算法可以用于确定最优采样点，这种方法在样本数量受限时非常有帮助。

**结论:** PANAMA框架通过使用梯度优化算法确定最佳采样点，可以在样本数量有限的情况下有效训练吉他放大器模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parametric+Neural+Amp+Modeling+with+Active+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02109&send_immediately=true&force_search=false)

**原文摘要:** We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [14] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

**主要类别:** cs.LG

**AI概要:** 本论文研究了在线框架下的保形预测问题，旨在优化效率。对于可交换序列，算法可以构造接近最优覆盖的区间；而对于任意序列，存在一个权衡，即在平均长度上接近最优区间的近似程度与犯错次数之间的权衡。作者提出了一种能够恢复所有Pareto最优设置的匹配算法，并且该算法是确定性的，对自适应对手具有鲁棒性。此外，论文还展示了没有单一算法可以同时在任意序列和可交换序列中达到Pareto最优。


<details>
  <summary>更多</summary>
  
**动机:** 现有的保形预测方法可能无法同时保证高效的覆盖和最小的区间长度。因此，需要一种新的在线框架来直接优化效率，确保覆盖的同时尽量减少区间的平均长度。

**方法:** 论文研究了两种输入序列情况：可交换序列和任意序列。对于可交换序列，提出了能够实现接近最优覆盖的区间构造方法；对于任意序列，分析了在平均长度上的近似程度与犯错次数之间的权衡，并给出了一种能够恢复所有Pareto最优设置的匹配算法。该算法是确定性的，对自适应对手具有鲁棒性。

**结果:** 对于可交换序列，算法可以实现接近(1 - α)的覆盖，同时区间的长度被限制在事后最佳固定区间的范围内。对于任意序列，证明了任何算法在平均长度上与事后最佳固定区间相比的近似程度μ与犯错次数之间存在权衡关系。提出的算法能够在两种情况下达到近似最优的权衡。

**结论:** 本文提出了一种新的在线保形预测框架，针对可交换序列和任意序列分别进行了分析。结果表明，对于可交换序列可以实现接近最优的覆盖和效率，而对于任意序列则存在一个明确的权衡。提出的匹配算法能够恢复所有Pareto最优设置，同时展示了没有单一算法可以同时在两种序列情况下达到最优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Conformal+Prediction+with+Efficiency+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02496&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [15] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu, Lechao Xiao, Andrew Gordon Wilson, Jeffrey Pennington, Atish Agarwala*

**主要类别:** cs.LG

**AI概要:** 研究了当模型大小和训练时间同时增长时，神经网络训练动态的规模限制。发现计算最优训练模型展现出精确的普遍性，即不同模型大小的损失曲线在训练计算和损失归一化后，会坍缩到单一通用曲线。使用学习率衰减时，这种坍缩变得非常紧密，称为超坍缩。观察到跨学习率计划、数据集和架构（包括在下一个令牌预测上训练的变压器）的超坍缩现象，并发现当超参数次优缩放时，超坍缩会崩溃，提供了一个良好缩放的精确且实用指标。通过连接坍缩到典型神经缩放定律中的幂律结构，并分析一个简单但惊人的有效SGD噪声动力学模型，可以准确预测各种学习率计划的损失曲线，并定量解释超坍缩的起源。


<details>
  <summary>更多</summary>
  
**动机:** 探索当模型大小和训练时间一起增长时，什么规模限制支配神经网络训练动态，以及如何理解计算最优训练模型的普遍性。

**方法:** 研究不同模型大小的损失曲线在训练计算和损失归一化后的坍缩情况，观察超坍缩现象在不同学习率计划、数据集和架构中的表现，并分析超参数次优缩放时超坍缩的崩溃情况。通过连接坍缩到典型神经缩放定律中的幂律结构，并分析SGD噪声动力学模型来解释超坍缩的起源。

**结果:** 发现计算最优训练模型展现出精确的普遍性，损失曲线会坍缩到单一通用曲线，使用学习率衰减时出现超坍缩现象，超坍缩在超参数次优缩放时会崩溃。SGD噪声动力学模型可以准确预测损失曲线并解释超坍缩的起源。

**结论:** 神经网络训练动态在模型大小和训练时间增长时存在规模限制，计算最优训练模型表现出普遍性，超坍缩现象是一个良好缩放的精确且实用指标，SGD噪声动力学模型可以解释超坍缩的起源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Collapse+Reveals+Universal+Dynamics+in+Compute-Optimally+Trained+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02119，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02119&send_immediately=true&force_search=false)

**原文摘要:** What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [16] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia, Mahmoud El Daou, Henryk Gzyl*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于熵函数最小化的新型分类方法，通过在有界超立方体中搜索参数向量和正向量来分离两类数据点，并可扩展至多项式曲面以处理更复杂的决策边界。数值实验表明该方法在处理线性和非线性分类任务时具有高效性和通用性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的支持向量机和梯度下降等优化技术主要依赖于线性或二次优化方法，对于复杂决策边界的处理能力有限。因此，需要一种更稳健的替代方案来提高分类效果。

**方法:** 通过定义一个在未知变量空间上的熵基函数并对其进行最小化，在一个以原点为中心的有界N维超立方体内寻找参数向量，以及一个正的M维向量，从而确定用于分离两类数据点的超平面。该方法还可以扩展到多项式曲面以适应更复杂的决策边界。

**结果:** 数值实验展示了该方法在处理各种分类任务（包括线性和非线性可分情况）时的高效性和通用性，表明其作为传统优化技术的稳健替代方案的潜力。

**结论:** 提出的熵基最小化方法为分类问题提供了一个新的视角，并且在处理复杂决策边界方面表现出色，可以作为一种稳健的替代方案用于机器学习中的分类任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Classification+by+Separating+Hypersurfaces%3A+An+Entropic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02732&send_immediately=true&force_search=false)

**原文摘要:** We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [17] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan, Isaac Jacobson, Zheng Zhao, Tung-Chieh Chen, Guanglei Zhou, Chen-Chia Chang, Vineet Rashingkar, Yiran Chen*

**主要类别:** cs.LG

**AI概要:** 论文提出CROP框架，利用大型语言模型（LLM）进行VLSI设计流的自动调优，通过代码向量化、基于嵌入的检索系统以及增强的参数搜索系统，实现更少迭代次数和更好的结果质量，包括9.9%的功耗降低。


<details>
  <summary>更多</summary>
  
**动机:** 现代VLSI设计中，由于EDA算法的复杂性和庞大的参数空间，芯片设计优化面临巨大挑战。手动参数选择虽然在工业实践中广泛使用，但效率低下且受限于专家经验。

**方法:** 提出CROP框架，包含：(1)将RTL源代码转换为密集向量表示的可扩展方法；(2)基于嵌入的检索系统，用于匹配语义相似的电路设计；(3)结合检索增强生成（RAG）技术的LLM指导参数搜索系统，利用相似设计的先验知识约束搜索过程。

**结果:** 实验结果表明，CROP在工业设计上能以较少的迭代次数达到优于现有方法的质量结果，并实现了9.9%的功耗降低。

**结论:** CROP框架展示了在VLSI设计优化中的潜力，能够有效减少功耗并提高设计效率，为自动化芯片设计提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CROP%3A+Circuit+Retrieval+and+Optimization+with+Parameter+Guidance+using+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02128&send_immediately=true&force_search=false)

**原文摘要:** Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [18] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang, Ruihao Zhu, Qiaomin Xie*

**主要类别:** cs.LG

**AI概要:** 研究了带有偏差离线数据的情境在线定价问题，提出了一种乐观面对不确定性（OFU）策略，给出了最优的后悔界，并设计了在偏差未知情况下的鲁棒算法。


<details>
  <summary>更多</summary>
  
**动机:** 探讨如何利用带有偏差的离线数据来优化情境在线定价问题，特别是在标量价格弹性情况下，量化离线数据与在线最优解之间的差距。

**方法:** 通过定义实例相关的参数$\delta^2$来衡量离线数据与在线最优解的距离，并结合离线数据的时间长度、偏差界限、规模和分散度等指标分析统计复杂性。对于一般的价格弹性情况，建立了最坏情况下的最优率，并提出了广义的OFU算法。当偏差界限未知时，设计了鲁棒变体算法。

**结果:** 给出了一种乐观面对不确定性（OFU）策略，在标量价格弹性情况下实现了最优的后悔界；对于一般价格弹性情况，提供了相应的广义OFU算法并达到最优率。此外，当偏差未知时，提出的鲁棒算法始终保证次线性后悔，并且在偏差较小时优于纯在线方法。

**结论:** 这是首次为带有偏差离线数据的情境在线定价提供紧致的后悔保证，并且所用技术可以直接应用于带有偏差离线数据的随机线性Bandit问题，得到类似的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Contextual+Online+Pricing+with+%28Biased%29+Offline+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02762&send_immediately=true&force_search=false)

**原文摘要:** We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [19] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li, Liangji Zhu, Anand Rangarajan, Sanjay Ranka*

**主要类别:** cs.LG

**AI概要:** 生成模型在条件设置下表现出强大的性能，并可被视为一种数据压缩形式，但其可控性和重建准确性有限。本文提出了一种有效的潜在扩散框架，将变分自动编码器与条件扩散模型相结合，以关键帧的潜在空间表示为条件输入，通过生成插值重建剩余帧，从而显著降低存储成本并实现精确的时空重建。实验结果表明，该方法比基于规则的顶级压缩器（如SZ3）高出10倍的压缩比，在相同重建误差下比领先的基于学习的方法高出63%的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管生成模型在条件设置下表现出色，但由于其可控性和重建准确性的限制，其在数据压缩中的实际应用受到限制。因此需要一种新方法来解决这些问题，提高数据压缩的效率和准确性。

**方法:** 本文提出了一种有效的潜在扩散框架，结合了变分自动编码器和条件扩散模型。该方法仅将少量关键帧压缩到潜在空间中，并使用它们作为条件输入，通过生成插值重建其余帧，无需为每一帧存储潜在表示。

**结果:** 实验结果表明，该方法在多个数据集上实现了高达10倍于基于规则的顶级压缩器（如SZ3）的压缩比，并且在相同重建误差下比领先的基于学习的方法高出63%的性能。

**结论:** 本文提出的潜在扩散框架显著提高了数据压缩的效率和准确性，减少了存储成本，同时保持了良好的重建质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+Latent+Diffusion+for+Efficient+Spatiotemporal+Data+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02129&send_immediately=true&force_search=false)

**原文摘要:** Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [20] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang, Jian Kang, Yujun Yan, Adithya Kulkarni, Dawei Zhou*

**主要类别:** cs.LG

**AI概要:** 论文提出NCPNET，一种用于动态图的端到端一致性预测框架，通过捕捉拓扑和时间不确定性，优化预测过程，减少覆盖偏差，并在多个真实数据集上显著提升效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的GNN一致性预测方法主要针对静态图，忽略了现实世界中图的时间演化特性，导致标准方法无法满足假设条件并限制了其应用。

**方法:** 引入NCPNET框架，扩展一致性预测至动态环境；提出基于扩散的非一致性得分以捕捉时间和拓扑不确定性；开发效率感知优化算法改善预测过程，提高计算效率并减少覆盖偏差。

**结果:** 在包括WIKI、REDDIT等真实数据集上的实验表明，NCPNET可确保时间图的覆盖范围，并将WIKI数据集的预测集合大小减少了31%，相比现有方法显著提升了效率。

**结论:** NCPNET提供了一种有效的方法来处理时间图的一致性预测问题，同时提高了预测效率和覆盖率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-exchangeable+Conformal+Prediction+for+Temporal+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02151，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02151&send_immediately=true&force_search=false)

**原文摘要:** Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [21] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon, Meredith Stewart, Bogdan Kulynych, Tsui-Wei Weng, Berk Ustun*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种正式的验证程序，用于评估机器学习模型预测结果对外部干预的响应性，并通过黑箱访问方式估计任何模型和数据集的响应性，同时开发了相关算法以支持实际任务的安全性提升。


<details>
  <summary>更多</summary>
  
**动机:** 许多机器学习中的安全问题源于在分配预测时未能考虑个体对其输入的改变能力，特别是在借贷、招聘或内容审核等场景中。

**方法:** 引入一种正式验证程序，将预测响应性视为敏感性分析的一种形式，通过设定干预和下游效应分布的约束来控制一组变化；利用黑箱访问方式估计任何模型和数据集的响应性；开发生成可达点均匀样本的算法以支持任务如证伪和失败概率估计。

**结果:** 所提出的算法能够在包括再犯预测、器官移植优先级和内容审核等实际应用中提升安全性。

**结论:** 本文提出的验证程序和算法为提高机器学习模型的安全性和响应性提供了一种有效方法，适用于多种现实世界的应用场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Statistical+Inference+for+Responsiveness+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02169&send_immediately=true&force_search=false)

**原文摘要:** Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [22] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae, Hyeon Jeon, Jinwook Seo*

**主要类别:** cs.LG

**AI概要:** 提出了一种通过聚类评估指标减少降维投影评估偏差的新工作流，提高了降维评估的稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 评估降维投影在保留高维数据结构方面的准确性对于可靠的可视化分析至关重要，但若选择高度相关的评估指标会导致偏差。

**方法:** 通过计算成对相关性来衡量指标相似性，聚类评估指标以减少重叠，并从每个集群中选择一个代表性指标。

**结果:** 定量实验表明该方法提高了降维评估的稳定性。

**结论:** 所提出的工作流有助于减轻降维评估中的偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Metric+Design+%21%3D+Metric+Behavior%3A+Improving+Metric+Selection+for+the+Unbiased+Evaluation+of+Dimensionality+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02225&send_immediately=true&force_search=false)

**原文摘要:** Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [23] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang, Paris Perdikaris*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为PhysicsCorrect的无训练校正框架，通过预计算雅可比矩阵及其伪逆来减少预测误差，并在三个PDE系统中验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络在求解偏微分方程（PDE）时表现出强大的替代能力，但存在长期预测中的误差累积问题。

**方法:** 提出了PhysicsCorrect框架，该框架将校正公式化为基于PDE残差的线性化逆问题，并采用高效的缓存策略预先计算雅可比矩阵及其伪逆。

**结果:** 在Navier-Stokes流体动力学、波动方程和混沌Kuramoto-Sivashinsky方程三个代表性的PDE系统中，PhysicsCorrect将预测误差减少了高达100倍，同时仅增加不到5%的推理时间。

**结论:** PhysicsCorrect有效地将不稳定的神经网络代理转化为可靠的模拟工具，弥合了深度学习计算效率与实际科学应用所需的物理保真度之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PhysicsCorrect%3A+A+Training-Free+Approach+for+Stable+Neural+PDE+Simulations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02227，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02227&send_immediately=true&force_search=false)

**原文摘要:** Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [24] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda, Shashidhar Reddy Javaji, Zining Zhu*

**主要类别:** cs.LG

**AI概要:** 在当前的机器学习领域，尽管模型的行为不同，但面对一个任务时，会出现性能相似的模型泛滥的现象（即“模型湖”现象）。为了帮助用户从众多模型中进行选择，模型两两比较的文档非常有用。然而，对于N个模型，可能需要进行O(N^2)次两两比较，这对开发者手动完成和准备文档来说是不现实的。为促进模型间的精细两两比较，我们引入了VERBA。VERBA利用大型语言模型（LLM）通过从两个模型中采样生成模型差异的口头描述。我们建立了一个协议，通过模拟评估这些口头描述的信息量，并创建了一套包含各种常用机器学习模型的基准测试。对于一对决策树模型，尽管其性能差异不超过5%，但行为差异却达到20-25%，VERBA能以高达80%的整体准确性有效描述它们的变化。当我们将模型的结构信息纳入考虑时，口头描述的准确性进一步提高到90%。VERBA为事后改进机器学习模型的透明度和可比性开辟了新的研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 在机器学习领域存在“模型湖”现象，即针对一个任务有大量性能相似但行为不同的模型。这使得模型使用者难以选择合适的模型，因此需要一种方法来高效地生成模型两两比较的文档。

**方法:** VERBA利用大型语言模型（LLM），通过对两个模型进行采样生成描述模型差异的口头表述。同时，作者建立了一个评估口头表述信息量的协议，并构建了一套包含多种常用机器学习模型的基准测试套件。

**结果:** 对于一对决策树模型，尽管其性能差异不超过5%，但行为差异达20-25%，VERBA能以80%的整体准确性描述其差异；当加入模型的结构信息后，准确性提升至90%。

**结论:** VERBA能够有效地生成模型差异的口头描述，提高了机器学习模型的事后透明度和可比性，并为相关研究开辟了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VERBA%3A+Verbalizing+Model+Differences+Using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02241，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02241&send_immediately=true&force_search=false)

**原文摘要:** In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [25] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi, Xiaopeng Ke, Xinye Xiong, Kexin Meng, Chang Men, Zhengdan Zhu*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为FCA-RL的强化学习补贴策略框架，结合快速竞争适应(FCA)和强化拉格朗日调整(RLA)技术，能够迅速适应竞争对手的价格调整并在预算约束下优化优惠券决策。此外，还开发了RideGym模拟环境用于评估不同定价策略。实验结果表明该方法在多种市场条件下优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 网约车聚合平台的普及为服务提供商提供了显著的增长机会，但低价服务提供者在排名中更高且更可能被乘客选择。这促使服务提供商采用优惠券策略以降低价格、获取更多订单，从而影响其长期可行性和可持续性。然而，现有的研究在这一领域仍然稀少。

**方法:** 1. 提出了FCA-RL框架：基于强化学习的补贴策略框架。
2. 集成两种关键技术：
   - 快速竞争适应（FCA）：快速响应动态价格变化。
   - 强化拉格朗日调整（RLA）：确保遵守预算约束并优化优惠券决策。
3. 开发了RideGym：专为网约车聚合商设计的模拟环境，用于全面评估和基准测试不同的定价策略。

**结果:** 实验结果表明，所提出的方法在各种市场条件下均优于基线方法，证明了其在补贴优化方面的有效性。

**结论:** FCA-RL框架及其关键技术展示了对竞争对手价格调整的快速适应能力，并能在预算限制下优化优惠券策略，有效提升网约车服务提供商的订单量和长期生存能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Order+Acquisition+Under+Competitive+Pressure%3A+A+Rapidly+Adaptive+Reinforcement+Learning+Approach+for+Ride-Hailing+Subsidy+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02244&send_immediately=true&force_search=false)

**原文摘要:** The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [26] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang, Xiaolu Zhou, Bosong Ding, Miao Xin*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的框架URDP，结合大型语言模型和贝叶斯优化来设计更有效的强化学习奖励函数，实验表明其生成的奖励函数质量更高且自动化设计效率显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 设计有效的奖励函数在强化学习中至关重要，但传统方法存在低效和不一致的问题，而使用大语言模型虽可自动化设计奖励函数，但在数值优化上的表现不佳，进化搜索范式对模拟资源利用效率低下，导致设计周期长、计算成本高。

**方法:** 提出了不确定性感知奖励设计过程（URDP），通过自一致性分析量化候选奖励函数的不确定性，无需模拟即可识别无效奖励组件并发现新组件；引入了不确定性感知贝叶斯优化（UABO）以提高超参数配置效率；构建了两层优化架构，将奖励组件优化与超参数调整解耦。URDP促进了大语言模型的奖励逻辑推理与贝叶斯优化的数值优化能力之间的协同合作。

**结果:** 在涵盖35个不同任务的三个基准环境中的全面评估表明，URDP不仅生成了质量更高的奖励函数，而且相比现有方法在自动化奖励设计的效率上也有显著改进。

**结论:** URDP是一种有效整合大语言模型和贝叶斯优化的新框架，能够显著提升奖励函数的质量和自动化设计效率，为强化学习奖励设计提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty-aware+Reward+Design+Process，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02256，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02256&send_immediately=true&force_search=false)

**原文摘要:** Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [27] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang, Lingyi Wang, Wei Wu, Fuhui Zhou, Qihui Wu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的知识图谱增强的零样本语义通信网络（KGZS-SC），通过知识图谱提供通用语义表示和推理能力，减少通信开销并提高对未见数据的分类性能。


<details>
  <summary>更多</summary>
  
**动机:** 基于数据驱动的语义通信方法依赖于表面统计模式，缺乏可解释性和泛化能力，尤其是在存在未见数据的应用中。

**方法:** 利用知识图谱提供的结构化语义信息构建语义知识库（KG-SKB），在共享类别语义嵌入空间中对齐语义特征，从而增强发射器的泛化能力，并通过选择性传输紧凑视觉语义来减少通信开销。接收端使用零样本学习（ZSL）实现对未见数据的直接分类，无需重新训练或额外计算开销。

**结果:** 在APY数据集上的仿真结果表明，所提出的KGZS-SC网络表现出强大的泛化能力，在各种信噪比水平下显著优于现有的语义通信框架，特别是在对未见类别的分类任务中。

**结论:** 提出的KGZS-SC网络通过结合知识图谱和零样本学习技术，解决了现有语义通信方法缺乏可解释性和泛化能力的问题，提高了动态或资源受限环境下的适应性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Graph-Based+Explainable+and+Generalized+Zero-Shot+Semantic+Communications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02291&send_immediately=true&force_search=false)

**原文摘要:** Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [28] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee, Jedrzej Kozal, Michal Wozniak, Bartosz Krawczyk*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的框架，用于在概念漂移下进行持续学习，并引入了自适应记忆重新对齐（AMR）方法，以减少标注数据和计算需求，同时保持高性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统持续学习方法主要关注知识保留和缓解灾难性遗忘，但忽略了现实世界数据流的动态性质，即概念漂移会永久改变先前的数据，需要稳定性和快速适应能力。

**方法:** 提出了一个整体框架，用于在概念漂移下的持续学习，其中包含一种轻量级替代方案——自适应记忆重新对齐（AMR）。AMR通过从重播缓冲区中选择性地删除过时样本并用少量最新实例重新填充，从而有效地将记忆与新分布重新对齐。此外，还引入了四个具有概念漂移变体的标准视觉基准数据集。

**结果:** 在多个数据集上的全面实验表明，AMR能够有效应对概念漂移，保持高精度的同时显著减少了对标注数据和计算的需求。

**结论:** AMR被证明是一个可扩展的解决方案，能够在非平稳持续学习环境中协调稳定性和可塑性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Holistic+Continual+Learning+under+Concept+Drift+with+Adaptive+Memory+Realignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02310&send_immediately=true&force_search=false)

**原文摘要:** Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [29] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang, Hongqi Li*

**主要类别:** cs.LG

**AI概要:** 本文综述了Transformer模型在脑电图（EEG）解码中的最新应用，详细介绍了其基本原理、混合架构、定制化改进结构以及未来发展方向。


<details>
  <summary>更多</summary>
  
**动机:** 随着深度学习技术的发展，Transformer因其强大的序列数据处理能力逐渐被应用于EEG解码中，以替代传统方法并实现更精确的特征提取和意图获取。

**方法:** 文章首先阐明了Transformer的基本原理及其在EEG解码中的直接应用，接着概述了与卷积、递归、图神经网络等技术结合的混合架构，并介绍了定制化改进的Transformer结构。

**结果:** 总结了Transformer模型在EEG解码中的进展，并指出了当前面临的挑战及未来的研究方向。

**结论:** 本文为读者提供了Transformer在EEG解码领域应用的清晰概览，有助于推动该领域的未来发展和研究探索。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformer-based+EEG+Decoding%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02320&send_immediately=true&force_search=false)

**原文摘要:** Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [30] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim, Yechan Mun, Sangchul Hahn, Eunho Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的可解释人工智能算法DeltaSHAP，专为在线患者监测系统设计。通过适应Shapley值到时间序列场景，准确捕捉特征联合效应，并且只使用实际观察到的特征组合来归因预测变化，使其在时间敏感的临床应用中高效实用。实验表明，DeltaSHAP在解释质量和计算效率上均优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 在临床环境中，发现驱动患者风险演变的原因对于及时干预至关重要，但现有的XAI方法未能满足临床时间序列解释任务的独特需求。

**方法:** DeltaSHAP通过将Shapley值适配到时间序列环境，准确捕获特征联合效应，并仅使用实际观察到的特征组合来归因预测变化。此外，还引入了新的评估指标以衡量时间序列在线解释的忠实度。

**结果:** 实验表明，DeltaSHAP在MIMIC-III去补偿基准上比现有最先进XAI方法提高了62%的解释质量，并减少了33%的计算时间。

**结论:** DeltaSHAP成功解决了临床时间序列解释任务中的关键需求，包括连续预测变化的解释、特征归因的方向和大小提供，以及实时性。这使得它成为一种高效且实用的工具，适用于时间敏感的临床应用场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeltaSHAP%3A+Explaining+Prediction+Evolutions+in+Online+Patient+Monitoring+with+Shapley+Values，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02342&send_immediately=true&force_search=false)

**原文摘要:** This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [31] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh, Byung-Jun Lee*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为PANI（Penalized Action Noise Injection）的方法，通过注入噪声动作并对其进行惩罚来增强离线强化学习算法的性能。该方法简单但有效，并且在多个基准测试中表现出显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 离线强化学习由于环境交互成本高，其泛化能力是提高算法性能的关键。然而，使用扩散模型虽然取得了成功，但计算成本较高，因此研究者希望探索一种更简单的替代方案。

**方法:** PANI方法通过向动作空间注入噪声并根据噪声量进行惩罚，从而覆盖整个动作空间。该方法受到扩散模型在离线RL中的启发，并从理论上证明了这种方法解决了一个修改后的马尔可夫决策过程（MDP），称为带噪声的动作MDP。

**结果:** PANI方法与多种现有的离线和离策略强化学习算法兼容，并在多个基准测试中显示出了显著的性能改进。

**结论:** PANI是一种简单有效的增强离线强化学习性能的方法，能够在不增加过多计算负担的情况下提高算法表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Offline+Reinforcement+Learning+with+Penalized+Action+Noise+Injection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02356&send_immediately=true&force_search=false)

**原文摘要:** Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [32] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama, Dong Eui Chang*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种基于数据驱动的框架，结合强化学习方法优化高速内存系统中的信号完整性参数。通过使用隐式信号表示和无模型的强化学习代理，实现了高效且无需显式系统模型的参数优化，并在行业标准的动态随机存取存储器波形上取得了显著的眼图开窗面积提升。


<details>
  <summary>更多</summary>
  
**动机:** 在高速动态随机存取存储器（DRAM）系统中，信号完整性的等化器参数优化至关重要，但传统方法往往计算成本高或依赖精确模型。因此，需要一种更高效的、无需明确系统模型的优化方法。

**方法:** 论文引入了一个数据驱动框架，利用学习到的隐式信号表示来进行高效的信号完整性评估，并结合无模型的Advantage Actor-Critic强化学习代理进行参数优化。隐式信号表示捕捉了关键的信号完整性特征，而强化学习代理则能够在无需系统模型的情况下找到最优的等化器设置。

**结果:** 该方法在行业标准的DRAM波形上实现了显著的眼图开窗面积提升：对于级联连续时间线性等化器和决策反馈等化器结构提升了42.7%，而对于仅使用决策反馈等化器的配置提升了36.8%。这表明该方法相比现有技术具有优越的性能、更高的计算效率以及更强的泛化能力。

**结论:** 本研究的核心贡献包括：提出了一种有效的隐式信号完整性度量用于优化、开发了一种强大的无模型强化学习策略，以及验证了其在复杂等化器架构上的卓越性能。这些成果为DRAM系统的信号完整性优化提供了一种新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Reinforcement+Learning-Based+DRAM+Equalizer+Parameter+Optimization+Using+Latent+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02365&send_immediately=true&force_search=false)

**原文摘要:** Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [33] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo, Lina Achaji, Stefano Sabatini, Nicola Poerio, Grzegorz Bartyzel, Sascha Hornauer, Fabien Moutarde*

**主要类别:** cs.LG

**AI概要:** 通过在多智能体环境中使用偏好优化微调轨迹预测模型，可以显著提高场景一致性，同时最小化牺牲轨迹预测准确性，并且不会增加推理时的计算需求。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于深度学习的轨迹预测模型在公共数据集上表现良好，但在复杂交互场景中，往往无法捕捉到智能体之间的重要相互依赖关系，导致交通场景中智能体之间的预测不一致。受将人类偏好整合到大型语言模型中的启发，本文尝试将偏好优化引入轨迹预测模型的微调过程中。

**方法:** 利用自动计算的预测未来偏好排名作为输入，在微调过程中对轨迹预测模型进行偏好优化。此方法在三个不同的数据集上使用最先进的模型进行了实验验证。

**结果:** 实验结果表明，该方法能够在不显著牺牲轨迹预测准确性的情况下，显著提高场景一致性，并且不会在推理时增加额外的计算需求。

**结论:** 将偏好优化应用于多智能体环境下的轨迹预测模型微调，可以有效提升场景一致性，同时保持较高的轨迹预测精度和较低的计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Consistency+in+Vehicle+Trajectory+Prediction+Through+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02406&send_immediately=true&force_search=false)

**原文摘要:** Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [34] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye*

**主要类别:** cs.LG

**AI概要:** 在联邦图学习（FGL）中，当前研究仅从结构角度解决子图-联邦学习（FL），忽略了图信号在空间和谱域上的传播问题。子图-FL导致客户之间的边断开，从而破坏标签信号并降低全局GNN的分类能力；谱异质性导致局部GNN过拟合于局部信号传播模式，引发谱客户端漂移，削弱全局泛化能力。为此，我们提出了一种全局知识库以缓解标签信号中断，并引入频率对齐方法来解决谱客户端漂移。这两种策略结合形成了我们的框架S2FGL。实验表明S2FGL具有优越性。


<details>
  <summary>更多</summary>
  
**动机:** 当前FGL研究仅从结构视角出发，未考虑图信号在空间和谱域上的传播影响，这会导致标签信号中断、分类能力下降以及谱客户端漂移等问题。因此，需要一种综合空间和谱域策略的方法来解决这些问题。

**方法:** 1. 提出一个全局知识库，用于缓解因子图-FL引起的标签信号中断问题。
2. 引入频率对齐方法，解决谱异质性导致的谱客户端漂移问题。
3. 将上述两种策略结合，构建了S2FGL框架。

**结果:** 通过在多个数据集上的广泛实验，验证了S2FGL框架的有效性和优越性。

**结论:** S2FGL框架通过结合空间和谱域策略，成功解决了FGL中的标签信号中断和谱客户端漂移问题，提升了全局GNN的性能和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是S2FGL%3A+Spatial+Spectral+Federated+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02409，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02409&send_immediately=true&force_search=false)

**原文摘要:** Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [35] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani, Henrik Christiansen, Federico Errica*

**主要类别:** cs.LG

**AI概要:** Kolmogorov Arnold Networks (KANs) 是一种基于 Kolmogorov-Arnold 定理的机器学习模型架构，通过将多变量连续有界函数表示为有限数量的单变量连续函数的组合。本文提出了一种名为 InfinityKAN 的方法，它通过变分推断优化问题自适应地学习每个单变量函数的潜在无限数量的基函数，从而扩展了 KANs 的适用性，并将关键超参数纳入学习过程。


<details>
  <summary>更多</summary>
  
**动机:** 现有的 KANs 方法需要人为选择用于建模单变量函数的基函数数量，这一选择是 ad-hoc 的且可能限制模型性能。因此，研究者希望改进这一局限性，通过自动学习基函数的数量来提升模型的灵活性和表现力。

**方法:** 作者提出了 InfinityKAN 方法，将基函数的学习问题建模为一个变分推断优化问题，并使用反向传播进行训练。这种方法可以自适应地学习每个单变量函数的潜在无限数量的基函数，同时将关键超参数（如基函数数量）作为学习过程的一部分。

**结果:** InfinityKAN 方法成功解决了基函数数量的人为选择问题，扩展了 KANs 的适用性，并为处理复杂的多变量连续函数提供了更灵活的工具。实验结果表明，该方法在表现上优于传统 KANs 和其他基准模型。

**结论:** InfinityKAN 提供了一种新的途径来改进 KANs 的性能和适用范围，通过将基函数数量的确定从人为选择转变为自动学习过程，展示了其在复杂函数逼近任务中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Kolmogorov-Arnold+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02466，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02466&send_immediately=true&force_search=false)

**原文摘要:** Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [36] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing*

**主要类别:** cs.LG

**AI概要:** GORP是一种新的持续学习训练策略，通过结合完整和低秩参数并在统一的低秩梯度子空间内联合更新，克服了效率与表达能力之间的权衡。它在保持效率的同时扩展了优化空间并减轻了灾难性遗忘，在持续学习基准测试中表现优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型（LLMs）的持续微调受到效率与表达能力之间权衡的限制。现有的低秩适应（LoRA）方法虽然提高了效率，但其低秩特性和显式参数约束限制了模型学习新任务和迁移知识的能力。

**方法:** 提出了一种名为GORP（Gradient LOw Rank Projection for Continual Learning）的新训练策略。该方法通过协同结合完整参数和低秩参数，并在统一的低秩梯度子空间内进行联合更新，从而扩展优化空间，同时保持高效并缓解灾难性遗忘。

**结果:** 广泛的实验表明，GORP在持续学习基准上表现出色，优于现有的最先进方法。

**结论:** GORP作为一种新颖的训练策略，成功地克服了效率与表达能力之间的权衡问题，为持续学习提供了更优的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continual+Gradient+Low-Rank+Projection+Fine-Tuning+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02503，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02503&send_immediately=true&force_search=false)

**原文摘要:** Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [37] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi, Ahmed M. Azab, Seif Eldawlatly, Gamal M. Aly*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的方法，通过优化预处理和深度学习技术来显著提高脑机接口中跨主体运动想象分类的性能。该方法在四个数据集上进行了验证，取得了比现有技术更好的结果，并为通用、无需校准的运动想象分类设定了新基准。


<details>
  <summary>更多</summary>
  
**动机:** 由于个体间脑电图（EEG）模式存在显著差异，跨主体运动想象分类在脑机接口中的准确率较低，成为开发适用于实际应用的无需校准的脑机接口的主要障碍。

**方法:** 研究方法包括对STFT变换后的EEG数据进行直接分类，优化STFT参数，以及在训练卷积神经网络（CNN）时采用平衡批量策略。此外，系统地研究了使用1至4秒的不同运动想象窗口的分类性能。

**结果:** 该方法在四个不同数据集上显著提高了跨主体分类准确率，分别在BCI竞赛IV数据集1（IV-1）、数据集2A（IV-2A）和数据集2B（IV-2B）上达到了67.60%、65.96%和80.22%的准确率，优于当前最先进的技术。

**结论:** 该研究为通用、无需校准的运动想象分类设定了新基准，并贡献了一个强大的开源数据集以推动该领域的研究进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TFOC-Net%3A+A+Short-time+Fourier+Transform-based+Deep+Learning+Approach+for+Enhancing+Cross-Subject+Motor+Imagery+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02510，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02510&send_immediately=true&force_search=false)

**原文摘要:** Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [38] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska, Riccardo Belluzzo, Piotr Zieliński, Joanna Baran, Paweł Olszewski*

**主要类别:** cs.LG

**AI概要:** 本论文介绍了一种新的训练文本到SQL生成模型的方法——RetrySQL。通过创建包含错误和更正步骤的重试数据，连续预训练开源编码模型，显著提高了整体和挑战性执行准确度指标。研究还表明，监督微调对从重试数据中学习无效，而全参数预训练是必要的。最后，RetrySQL模型在执行准确度上与参数数量多几个数量级的专有模型具有竞争力，证明了自我纠正行为可以被学习并提高下游准确度指标。


<details>
  <summary>更多</summary>
  
**动机:** 尽管目前许多解决方案使用封闭源代码专有语言模型或开放源代码编码模型，但针对SQL的具体生成模型的研究较少。同时，最近在自我纠正生成策略方面的进展显示出改进现有架构能力的潜力，但这些概念尚未应用于文本到SQL的任务中。

**方法:** 作者为参考SQL查询准备推理步骤，并通过破坏它们创建重试数据，其中包含错误和更正步骤，用特殊标记分隔。然后，使用此数据连续预训练开源编码模型。此外，研究还探讨了使用LoRA进行监督微调的效果，以及全参数预训练的必要性。

**结果:** 与不使用重试数据的预训练相比，重试步骤使得整体和挑战性执行准确度指标提高了多达4个百分点。并且，监督微调对从重试数据中学习无效，而全参数预训练是必需的。最后，RetrySQL训练的模型在执行准确度上与参数量大得多的专有模型具有竞争力。

**结论:** RetrySQL展示了如何在文本到SQL任务中学习自我纠正行为，并提供了一种新颖的方式来提高SQL导向语言模型的生成准确度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RetrySQL%3A+text-to-SQL+training+with+retry+data+for+self-correcting+query+generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02529&send_immediately=true&force_search=false)

**原文摘要:** The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [39] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer, Davide D'Ascenzo, Rafael Dubach, Tomaso Poggio*

**主要类别:** cs.LG

**AI概要:** 深度神经网络（DNNs）成功的关键在于其能够利用目标函数的组成稀疏性结构。大多数实际相关的函数可以从少量构成函数组合而成，这些函数仅依赖于所有输入的一小部分低维子集。高效图灵可计算函数均具备这一特性，因此当前几乎所有学习问题中都可能存在这种特性。尽管在近似和泛化方面有一些理论见解，但关于DNNs的学习性和优化仍有许多重要问题需要解决。全面理解组成稀疏性在深度学习中的作用对于构建人工甚至通用智能的完整理论至关重要。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络（DNNs）在高维领域取得了显著成功，而经典浅层网络由于维度诅咒难以实现类似效果。然而，关于DNNs学习动态的基本原理仍然存在许多未解之谜。本文旨在探讨DNNs成功背后的核心原因。

**方法:** 论文提出DNNs的成功源于其能够利用目标函数的组成稀疏性结构。具体来说，大多数实际相关的函数可以从少量构成函数组合而成，每个构成函数仅依赖于所有输入的一小部分低维子集。此外，论文指出这一特性适用于所有高效图灵可计算函数，并且很可能存在于当前所有的学习问题中。

**结果:** 通过分析，论文表明组成稀疏性是DNNs成功的关键因素之一。虽然在近似和泛化方面取得了一些理论进展，但在学习性和优化方面仍有许多重要问题尚未解决。

**结论:** 理解组成稀疏性在深度学习中的作用对于建立人工甚至通用智能的完整理论至关重要。完成这一研究将有助于全面了解DNNs的学习机制，并推动人工智能领域的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+A+Theory+of+Deep+Learning+Must+Include+Compositional+Sparsity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02550，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02550&send_immediately=true&force_search=false)

**原文摘要:** Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [40] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni, Galvin Khara, Joachim Schaeffer, Marat Subkhankulov, Stefan Heimersheim*

**主要类别:** cs.LG

**AI概要:** 在GPT-2模型中，移除所有层归一化（LN）层仅会导致验证损失的小幅增加，表明LN在语言建模中的作用有限。研究发现随着模型参数增加，移除LN所需微调数据量呈次线性增长，并发布了无LN的GPT-2模型套件。此外，在无LN模型上测试解释技术发现直接logit归因能给出确切的组件影响，而归因修补准确性没有显著提高，且确认GPT-2的“置信神经元”在无LN模型中不活跃。


<details>
  <summary>更多</summary>
  
**动机:** 尽管层归一化对训练稳定性的影响已被充分记录，但其在推理阶段的作用尚不清楚，并且它通过引入额外的非线性和增加模型组件的互联性阻碍了机制可解释性。因此，研究者试图探索移除LN层对GPT-2模型的影响，以更好地理解LN在语言建模中的角色。

**方法:** 研究者从GPT-2模型中移除了所有的LN层，并评估了这种修改对验证损失的影响。他们还探讨了为移除LN层进行微调所需的数据量与模型参数之间的关系，并发布了一系列无LN的GPT-2模型。此外，研究者在这些无LN模型上应用了解释技术，包括直接logit归因和归因修补，以评估模型的可解释性。

**结果:** 移除所有LN层后，GPT-2模型的验证损失仅略有增加（如GPT-2 XL的交叉熵损失增加了0.03）。研究发现，随着模型参数的增加，移除LN层所需的微调数据量呈次线性增长。在无LN模型上的解释技术测试显示，直接logit归因能够提供更精确的组件影响，而归因修补的准确性没有显著改善。此外，GPT-2的“置信神经元”在无LN模型中不活跃。

**结论:** 层归一化在语言建模中的作用并不重要，GPT-2类模型可以在没有LN层的情况下正常运行。无LN的GPT-2模型系列为更精确的解释性研究提供了可能，并有助于加深对语言模型的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformers+Don%27t+Need+LayerNorm+at+Inference+Time%3A+Scaling+LayerNorm+Removal+to+GPT-2+XL+and+the+Implications+for+Mechanistic+Interpretability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02559&send_immediately=true&force_search=false)

**原文摘要:** Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [41] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse, Emily Yu, Christoph H. Lampert*

**主要类别:** cs.LG

**AI概要:** 本研究扩展了可学习的布尔逻辑网络（DBNs），引入了一种可训练的、可微分的互连结构，使其在输入宽度增加时仍能保持参数数量恒定，从而实现更宽层数的扩展并保持高精度。此外，还提出了两种互补的剪枝阶段，进一步减少了模型大小，同时提供了更好的压缩-精度权衡。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Learned Differentiable Boolean Logic Networks (DBNs)已经在资源受限的硬件上实现了高效的推理，但需要进一步扩展其规模和优化模型大小以提高效率和性能。

**方法:** 1. 引入一种可训练的、可微分的互连结构，该结构的参数数量随着输入宽度的增长保持不变，从而使DBNs能够扩展到更宽的层。
2. 提出两种互补的剪枝方法：
   - 基于SAT的逻辑等价性通过移除冗余门来减少模型大小而不影响性能。
   - 基于相似性的数据驱动方法，优于基于幅度的贪婪基线，并提供更好的压缩-精度权衡。

**结果:** 通过引入可训练的互连结构和两种剪枝方法，DBNs能够在扩展到更宽的层的同时保持准确性，并显著减小模型大小，提供更好的压缩-精度权衡。

**结论:** 本文提出的可训练互连结构和剪枝方法有效地提高了DBNs的扩展性和效率，为构建更大规模、更高性能的布尔逻辑网络奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Interconnect+Learning+in+Boolean+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02585&send_immediately=true&force_search=false)

**原文摘要:** Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [42] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya, Levent Eren*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pad%C3%A9+Approximant+Neural+Networks+for+Enhanced+Electric+Motor+Fault+Diagnosis+Using+Vibration+and+Acoustic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02599&send_immediately=true&force_search=false)

**原文摘要:** Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [43] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet, Ruben Ohana, Michael McCabe, Gilles Louppe, François Lanusse, Shirley Ho*

**主要类别:** cs.LG

**AI概要:** 扩散模型在推理时计算成本高，阻碍了其作为快速物理模拟器的使用。本文探讨了在自动编码器的潜在空间中生成而非像素空间的方法是否能有效应用于动力系统仿真，并分析了其代价。研究发现潜在空间仿真对高达1000倍的压缩率具有惊人的鲁棒性，基于扩散的仿真器比非生成式方法更准确且预测更具多样性。最后，文章还涵盖了从架构到优化器的实际设计选择。


<details>
  <summary>更多</summary>
  
**动机:** 扩散模型在推理阶段计算成本陡增，限制了其作为快速物理模拟器的应用。为了降低成本，类似图像和视频生成领域中的策略被考虑用于动力系统的仿真。

**方法:** 研究者在自动编码器的潜在空间进行动力系统仿真，评估不同压缩率下的准确性，并与非生成式方法对比。同时，研究者探索了训练潜在空间仿真器的关键设计选择，包括架构和优化器等。

**结果:** 潜在空间仿真对高达1000倍的压缩率表现出较强的鲁棒性，基于扩散的仿真器比非生成式方法更准确，并且通过更大的多样性弥补预测中的不确定性。

**结论:** 潜在空间中的扩散模型可以有效地应用于动力系统仿真，提供更高的准确性和多样性，同时显著降低计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lost+in+Latent+Space%3A+An+Empirical+Study+of+Latent+Diffusion+Models+for+Physics+Emulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02608，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02608&send_immediately=true&force_search=false)

**原文摘要:** The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [44] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan, Sinan Kalkan, Fatos T. Yarman-Vural*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种称为可学习VAE（L-VAE）的新模型，该模型可以学习解耦表示以及损失函数的超参数。L-VAE通过学习损失项的相对权重来控制解耦和重建损失之间的动态权衡，解决了β-VAE的局限性。实验结果表明，L-VAE在多个数据集上表现优异，能够在重建保真度和解耦潜在维度之间找到有效的平衡。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法如β-VAE需要手动调整超参数以平衡解耦和重建损失，这可能导致次优解或不稳定的表现。因此，需要一种能够自动学习这种权衡的方法。

**方法:** L-VAE扩展了β-VAE，通过同时学习模型架构参数和损失项权重，引入了一个额外的正则化项以防止偏向重建或解耦损失。

**结果:** L-VAE在dSprites、MPI3D-complex、Falcor3D和Isaac3D等多个数据集上，根据一系列解耦度量指标，表现出最佳或次佳性能。此外，在CelebA数据集上的定性实验也验证了其成功解耦面部属性的能力。

**结论:** L-VAE提供了一种有效的方法，可以在解耦潜在维度和重建质量之间找到良好的平衡，具有广泛的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是L-VAE%3A+Variational+Auto-Encoder+with+Learnable+Beta+for+Disentangled+Representation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02619&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [45] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré, Borja Rodríguez Gálvez, Yoomi Park, Yitian Zhou, Volker M. Lauschke, Ming Xiao*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于变压器的矩阵变分自编码器（matVAE），在33个深度突变扫描（DMS）数据集上评估其性能。尽管参数和计算需求更少，但该模型在零样本预测中优于现有的DeepSequence模型。同时，使用DMS数据训练的模型在监督任务中表现更好，并且结合AlphaFold生成的结构进一步提高了性能。研究表明，DMS数据集有潜力替代多序列比对（MSA）而不显著降低预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的变异效应预测器依赖于多序列比对（MSA），但这一方法假设自然发生的变异是适应性的，这在药理基因组学中可能不成立。因此，研究探索了利用DMS数据集作为替代方案的可能性。

**方法:** 开发了一种基于变压器的矩阵变分自编码器（matVAE），并使用结构先验进行训练。模型在33个DMS数据集上进行了评估，并与现有最先进的DeepSequence模型进行了比较。此外，还研究了将AlphaFold生成的结构纳入模型的效果。

**结果:** matVAE-MSA模型在零样本预测中优于DeepSequence模型，同时参数和计算需求更低。基于DMS数据训练的模型在监督预测任务中表现更好，结合AlphaFold结构后性能进一步提升，达到与DeepSequence相当的水平。

**结论:** DMS数据集可以替代MSA用于变异效应预测，而不会显著降低预测性能，这为未来DMS数据集的发展和探索提供了动力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Matrix+Variational+Auto-Encoder+for+Variant+Effect+Prediction+in+Pharmacogenes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02624&send_immediately=true&force_search=false)

**原文摘要:** Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [46] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz, Atai Ambus, Moni Shahar, Ran Gilad-Bachrach*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为Medical Data Pecking的方法，用于EHR数据的质量评估。该方法通过自动化测试生成和执行框架来识别数据质量问题，并在三个数据集上进行了验证，发现了大量非对齐或不符合的数据问题。此方法结合外部医学知识，提高了数据分析结果的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 电子健康记录（EHR）在流行病学研究和人工智能训练中的使用迅速增加，但EHR数据的质量问题严重影响了研究结果的可靠性。现有的数据质量评估方法不够系统，无法充分评估数据是否适合研究用途。

**方法:** 论文提出了Medical Data Pecking方法，该方法借鉴了软件工程中的单元测试和覆盖率概念，包括两个主要组件：(1) 自动化测试生成器，利用大型语言模型和接地技术从数据和研究描述中创建测试套件；(2) 数据测试框架，执行这些测试并报告潜在错误和覆盖率。

**结果:** 在All of Us、MIMIC-III和SyntheticMass三个数据集上，每组生成了55-73个测试，正确识别出20-43个非对齐或不符合的数据问题。此外，还详细分析了LLM生成的测试套件的参考接地和值准确性。

**结论:** 本方法将外部医学知识融入到数据质量测试中，使上下文敏感的数据质量测试成为数据分析工作流的一部分，从而提高了结果的有效性。这种方法为未来的进一步发展奠定了基础，例如涵盖更多的数据模式和改进的接地方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Medical+Data+Pecking%3A+A+Context-Aware+Approach+for+Automated+Quality+Evaluation+of+Structured+Medical+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02628&send_immediately=true&force_search=false)

**原文摘要:** Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [47] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的分层深度学习框架，用于递归高阶元学习。该框架使神经网络能够构建、解决和跨任务层次泛化。通过生成虚拟任务，框架能自主创建有信息量的数据集，从而摆脱对人类生成数据的完全依赖。每一元学习层级对应着对低层级解决问题的逐步抽象泛化，促进了可解释的学习进程。从范畴论的角度看，元学习者可以被解释为函子，生成和调节下属学习者的层次结构，支持抽象和知识迁移。此架构可能推动新一代神经网络的发展，使其能够自主生成新颖的教学任务及其解决方案，从而向通用人工智能迈进。


<details>
  <summary>更多</summary>
  
**动机:** 当前的机器学习模型在很大程度上依赖于人类生成的数据进行训练，这限制了模型的泛化能力和自主学习能力。因此，需要一种能够让神经网络自主生成数据和任务，并从中学习未知规则和软约束的方法，以提升模型的泛化性能和适应性。

**方法:** 论文提出了一种基于递归高阶元学习的分层深度学习框架。核心是一个生成机制，用于创建虚拟任务——这些是设计用来让元学习者学习跨相关任务的软约束和未知可泛化规则的合成问题实例。元学习者通过主动探索虚拟点景观并寻找低级学习者难以完成的任务，迭代地细化约束区域。此外，通过将元学习者解释为范畴论中的函子，建立了一个支持抽象和跨逐步泛化的任务的知识转移的组合结构。

**结果:** 该框架能够生成自己的信息丰富且基于任务的数据集，从而解放了对完全依赖人类生成数据的机器学习训练的限制。它增强了归纳偏差，规范了适应过程，并产生了泛化所需的新型、未预料到的任务和约束。每个元层级对应于对低层级解决问题的逐步抽象泛化，实现了结构化和可解释的学习进展。

**结论:** 提出的分层深度学习框架提供了一种新方法来实现递归高阶元学习，增强了神经网络的泛化和适应能力。通过范畴论的视角，揭示了学习过程如何通过函子关系进行转换和比较，并提供了结构化元学习的实际设计原则。推测这种架构可能会支撑下一代神经网络，推动机器学习向通用人工智能发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High-Order+Deep+Meta-Learning+with+Category-Theoretic+Interpretation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02634，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02634&send_immediately=true&force_search=false)

**原文摘要:** We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [48] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron, Chris Hicks, Vasilios Mavroudis*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为Predictive Trajectory Sampling with Bayesian Exploration (PTS-BE)的框架，结合了基于模型的规划与信息论奖励机制，以实现样本高效深度探索，并证明其在多种环境中优于其他基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习中数据高效探索是一个挑战，现有基于信息论内在动机的方法可以被改进以更好地处理认识不确定性而非环境固有的随机性。

**方法:** 研究了一类针对认识不确定性的探索奖励，并通过理论分析证明这些奖励能自然地反映知识获取并在代理对环境动态和奖励足够确定时收敛为零；提出了实用的近似方法，如稀疏变分高斯过程、深度核和深度集成模型；设计了一个整合模型规划与信息论奖励的框架PTS-BE。

**结果:** 理论上验证了信息增益（IG）为基础的方法的有效性；实证表明PTS-BE框架在具有稀疏奖励或纯探索任务的环境中显著优于其他基线方法。

**结论:** 提出的PTS-BE框架为解决强化学习中的数据高效探索问题提供了一种有效且样本高效的方法，具备理论保障和实际应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Efficient+Bayesian+Exploration+in+Model-Based+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02639&send_immediately=true&force_search=false)

**原文摘要:** In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [49] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng, Ming-Hui Liu, Yangyang Guo, Tianyi Wang, Liqiang Nie, Mohan Kankanhalli*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Demographic Attribute-insensitive Intervention Detection（DAID）的框架，该框架通过人口统计特征感知的数据再平衡和人口统计特征无关的特征聚合，解决了深度伪造检测中的泛化能力和公平性之间的权衡问题。在三个跨域基准测试中，DAID在公平性和泛化能力方面均优于几种最先进的检测器。


<details>
  <summary>更多</summary>
  
**动机:** 深度伪造检测模型面临两个关键挑战：对未见过的操作的泛化能力和在人群群体中的公平性。然而，现有的方法通常表明这两个目标是固有冲突的，存在权衡。

**方法:** 提出了一种名为Demographic Attribute-insensitive Intervention Detection（DAID）的框架，由两部分组成：1）人口统计特征感知的数据再平衡，使用逆倾向加权和子群特征归一化来中和分布偏差；2）人口统计特征无关的特征聚合，使用一种新的对齐损失来抑制敏感属性信号。

**结果:** 在三个跨域基准测试中，DAID在公平性和泛化能力方面均表现出优于几种最先进的检测器的性能。

**结论:** DAID框架不仅验证了其理论基础，还证明了其实际有效性，能够通过公平性干预改善泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fair+Deepfake+Detectors+Can+Generalize，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02645，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02645&send_immediately=true&force_search=false)

**原文摘要:** Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [50] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil*

**主要类别:** cs.LG

**AI概要:** 近期研究表明，训练损失与模型大小和令牌数量呈幂律关系，并且实现计算最优模型需要同时扩展模型大小和令牌数量。然而，这些缩放定律假定数据无限供应，主要适用于计算受限的场景。随着现代大型语言模型越来越多地依赖于大规模互联网规模的数据集，假设它们是计算受限变得不再有效。这种转变突显了对优先考虑令牌效率的架构的需求。
在本研究中，我们探讨了使用2-单纯形Transformer，该架构通过高效的Triton内核实现将标准点积注意力推广到三线性函数。我们证明了2-单纯形Transformer在涉及数学、编码、推理和逻辑的任务上，在固定令牌预算下，表现优于相同大小的标准Transformer模型。我们通过展示2-单纯形注意力在知识和推理任务的缩放定律中改变了相对于点积注意力的指数来量化这些收益。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于训练损失与模型大小及令牌数量之间的幂律关系的研究主要基于计算受限的假设，但随着数据规模的扩大，这一假设逐渐失效，因此需要探索更加注重令牌效率的架构。

**方法:** 采用2-单纯形Transformer架构，将标准点积注意力推广至三线性函数，并通过高效的Triton内核实现。此方法在固定令牌预算下测试模型性能，并与标准Transformer进行比较。

**结果:** 2-单纯形Transformer在数学、编码、推理和逻辑等任务上的表现优于相同大小的标准Transformer，并且通过改变知识和推理任务中的缩放定律指数，量化了其在令牌效率方面的提升。

**结论:** 2-单纯形Transformer架构能够提高令牌效率，并在固定令牌预算下表现出更好的性能，特别是在涉及知识和推理的任务中。这表明在大规模数据场景下，此类架构可能具有更大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast+and+Simplex%3A+2-Simplicial+Attention+in+Triton，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02754，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02754&send_immediately=true&force_search=false)

**原文摘要:** Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [51] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang*

**主要类别:** cs.LG

**AI概要:** 提出OmniDraft框架，解决在线部署推测解码中的兼容性和延迟问题，通过统一草稿模型适配任意目标模型，并动态调整以优化速度和成本。实验表明，单个Llama-68M模型可与多个大模型配合使用，提速1.5-2倍。


<details>
  <summary>更多</summary>
  
**动机:** 在线部署推测解码时面临草稿模型与目标模型不兼容及延迟改进需求的挑战，需实现草稿模型的通用适配和高效解码。

**方法:** 构建OmniDraft框架，包含统一草稿模型、n-gram缓存与混合蒸馏微调技术，解决词汇不匹配问题；采用自适应草稿技术提升解码速度，适用于设备端LLM应用。

**结果:** 在数学推理、编程和文本生成任务中展示在线学习能力；单个Llama-68M草稿模型可与Vicuna-7B、Qwen2-7B等目标模型配合使用，提速1.5-2倍。

**结论:** OmniDraft框架有效解决了草稿模型与目标模型的兼容性问题，提升了推测解码的速度和效率，推动了“一个草稿适配所有”范式的实现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OmniDraft%3A+A+Cross-vocabulary%2C+Online+Adaptive+Drafter+for+On-device+Speculative+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02659，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02659&send_immediately=true&force_search=false)

**原文摘要:** Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [52] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao, Joshua Moller, Porfi Quintero-Cadena, Lood van Niekerk*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种引导离散扩散模型，该模型基于自然配对的重链和轻链序列以及246个临床阶段抗体的定量开发性测量进行训练。通过整合Soft Value-based Decoding in Diffusion (SVDD) 模块，可以在不损害自然性的情况下偏向采样，从而生成生物物理上可行的候选抗体。在无约束采样中，该模型可以重现天然抗体库和已批准治疗性抗体的全局特征，并且在SVDD指导下显著提高了预测的开发性评分。结合高通量开发性分析，此框架提供了一个迭代的、机器学习驱动的设计抗体管道，能够同时满足结合和生物物理标准。


<details>
  <summary>更多</summary>
  
**动机:** 治疗性抗体不仅需要高亲和力的目标结合，还需要良好的可制造性、稳定性和安全性等特性，这些综合称为`开发性'。为了建立一个优化抗体序列以提高开发性的计算框架，作者引入了这个研究方向。

**方法:** 使用从Observed Antibody Space (OAS)获得的自然配对重链和轻链序列，以及246个临床阶段抗体的定量开发性测量数据来训练一个引导离散扩散模型。同时，集成了Soft Value-based Decoding in Diffusion (SVDD) 模块，用于引导生成朝向生物物理上可行的候选物，而不会损害自然性。

**结果:** 在无约束采样中，模型可以重现天然抗体库和已批准治疗性抗体的全局特征。而在SVDD指导下，模型显著提高了预测的开发性评分。

**结论:** 这种结合高通量开发性分析的框架提供了一个迭代的、机器学习驱动的抗体设计管道，能够同时满足结合和生物物理标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guided+Generation+for+Developable+Antibodies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02670&send_immediately=true&force_search=false)

**原文摘要:** Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [53] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo, Hanh Huyen My Nguyen, Christian Ledig*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于差分隐私生成模型的数据共享方法，通过训练一个差分隐私条件变分自编码器（DP-CVAE）来支持多样下游任务，解决了联邦学习中高通信成本和灵活性不足的问题，同时确保数据隐私和高效性。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习在医学影像领域取得重大突破，但其应用受到数据稀缺性和隐私法规的限制，而联邦学习虽然能够实现去中心化训练，但存在高通信成本且通常局限于单一下游任务，缺乏灵活性。因此需要一种新的方法，在保护隐私的同时支持多样化任务并提高效率。

**方法:** 作者提出了一种基于差分隐私生成模型的数据共享方法：1. 利用基础模型提取紧凑、信息丰富的嵌入表示；2. 客户端协同训练一个差分隐私条件变分自编码器（DP-CVAE），以建模全局隐私保护数据分布；3. 该方法支持多种下游任务，并在多个特征提取器上进行了验证。

**结果:** 与传统的联邦学习分类器相比，所提出的方法在确保差分隐私的同时表现出更好的性能，提升了隐私保护、可扩展性和效率。此外，DP-CVAE生成的嵌入表示比DP-CGAN具有更高的保真度，并且参数量减少了5倍。

**结论:** 该研究提供了一种有效解决联邦学习中通信成本高和任务灵活性不足问题的新方法，同时确保了数据隐私和高效性，为医学影像领域的深度学习应用提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embedding-Based+Federated+Data+Sharing+via+Differentially+Private+Conditional+VAEs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02671，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02671&send_immediately=true&force_search=false)

**原文摘要:** Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [54] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg, Yao Ma, Seyed Sahand Mohammadi Ziabari, Marijn van Rijswijk*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了多智能体强化学习（MARL）如何改进供应链中的动态定价策略，特别是在传统ERP系统依赖静态、基于规则的方法而忽略市场参与者之间战略互动的背景下。通过评估三种MARL算法（MADDPG、MADQN和QMIX）在模拟环境中的表现，并与静态规则基线进行比较，结果表明MARL引入了静态定价规则无法捕捉的新兴战略行为，可能对未来动态定价的发展提供参考。


<details>
  <summary>更多</summary>
  
**动机:** 当前供应链中的动态定价策略主要依赖于静态、基于规则的方法，忽略了市场参与者之间的战略互动。虽然强化学习已被应用于定价，但大多数实现仍然是单智能体，未能建模现实世界供应链的相互依存性质。因此，需要研究多智能体强化学习在动态定价中的应用。

**方法:** 使用模拟环境评估三种多智能体强化学习算法（MADDPG、MADQN和QMIX）的表现，并与静态规则基线进行比较。模拟环境基于真实的电子商务交易数据和LightGBM需求预测模型。

**结果:** 规则基代理实现了接近完美的公平性（Jain's Index：0.9896）和最高的价格稳定性（波动性：0.024），但完全缺乏竞争动态。在MARL代理中，MADQN表现出最具攻击性的定价行为，波动性最高且公平性最低（0.5844）。MADDPG则提供了更平衡的方法，在支持市场竞争（份额波动：9.5百分点）的同时保持相对较高的公平性（0.8819）和价格稳定性。

**结论:** 多智能体强化学习能够引入静态定价规则无法捕捉的新兴战略行为，为未来动态定价的发展提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Reinforcement+Learning+for+Dynamic+Pricing+in+Supply+Chains%3A+Benchmarking+Strategic+Agent+Behaviours+under+Realistically+Simulated+Market+Conditions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02698&send_immediately=true&force_search=false)

**原文摘要:** This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [55] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari, Krishna Reddy Kesari*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于共识的协议来减少联邦学习中的数据传输成本，并提出了一种新的流体民主协议和算法以提高性能和安全性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习通常需要每个客户端将其权重转移到中央服务器，无论这些权重是否有用，这导致了不必要的数据传输成本。

**方法:** 探索现有流体民主协议在联邦学习中的应用，并提出一种新的流体民主协议（viscous-retained democracy）以及一个算法（FedVRD），该算法通过利用委托拓扑动态限制对手的影响，同时最小化成本。

**结果:** 新的协议和算法在性能上优于传统的1p1v方法，并且在对抗环境中表现更好。

**结论:** 提出的基于共识的协议可以有效减少联邦学习中的数据传输成本，新协议和算法提高了系统性能和安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fluid+Democracy+in+Federated+Data+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02710&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [56] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang, Chenyuan Hu, Yu Luo, Zhecheng Yuan, Ruijie Zheng, Huazhe Xu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的深度强化学习算法Forget and Grow（FoG），通过Experience Replay Decay和Network Expansion两种机制，分别模拟人类的遗忘与神经元增长过程，解决现有方法中的primacy bias问题，并在四个主要连续控制基准测试中超过现有最先进算法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度强化学习方法容易受到初始经验的过拟合（primacy bias），限制了样本效率和泛化能力，而人类由于婴儿期遗忘现象对此偏见较不敏感。

**方法:** 引入Forget and Grow（FoG）算法，包含两个机制：1) Experience Replay Decay (ER Decay)，逐渐减少早期经验的影响以平衡记忆；2) Network Expansion，动态增加训练中的参数以提高对已有数据模式的利用能力。

**结果:** 在四个主要连续控制基准测试中超过40个任务的表现优于现有的最先进算法，如BRO、SimBa和TD-MPC2。

**结论:** FoG算法通过模仿人类遗忘与神经元增长的过程，有效缓解了primacy bias问题，提升了RL代理的样本效率和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Forget-and-Grow+Strategy+for+Deep+Reinforcement+Learning+Scaling+in+Continuous+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02712，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02712&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [57] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat, Michael Fire, Eran Ben-Elia*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的框架，整合了空间、时间和网络依赖性以提高微移动需求预测的准确性。通过该框架，预测精度比基线模型提高了27至49%，有助于优化车队分布、降低成本并支持可持续城市规划。


<details>
  <summary>更多</summary>
  
**动机:** 无桩电动滑板车作为一种重要的微移动服务，为城市短距离出行提供了环保和灵活的选择。然而，为了有效管理这些服务，必须准确预测需求，这对于优化车队分布和基础设施规划至关重要。先前的研究仅关注空间或时间因素中的一个，而忽略了它们之间的相互作用。

**方法:** 本研究引入了一个框架，整合了空间、时间和网络依赖性，以改进微移动需求预测。这种方法不仅增强了预测的准确性，还提供了对城市微移动使用模式的更深入理解。

**结果:** 与基线模型相比，我们的框架将需求预测的准确性提高了27到49%。这表明该框架在捕捉微移动需求模式方面的有效性。

**结论:** 研究结果支持数据驱动的微移动管理方法，可以实现优化车队分布、减少成本以及促进可持续的城市规划。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensive+Machine+Learning+Framework+for+Micromobility+Demand+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02715，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02715&send_immediately=true&force_search=false)

**原文摘要:** Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [58] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu, Buwen Liang, Yuetong Fang, Zixuan Jiang, Renjing Xu*

**主要类别:** cs.LG

**AI概要:** 近期AI在科学领域的进展突显了对比学习在连接异构生物数据模态方面的强大功能。基于这一范式，本文提出了HIPPO（跨物种蛋白质-蛋白质相互作用预测的分层框架），通过多层级生物表示匹配来对齐蛋白质序列及其分层属性，并结合分层对比损失函数和数据驱动的惩罚机制以实现蛋白质功能层次结构的一致性。实验表明，HIPPO在基准数据集上表现优异，尤其在低数据量场景下具有鲁棒性，并且能够零样本迁移到其他物种，为跨物种PPI预测提供了统一框架。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有方法在蛋白质-蛋白质相互作用（PPI）预测中取得一定进展，但现有模型在处理跨物种、稀疏或不平衡数据时仍面临挑战。此外，如何有效利用蛋白质的功能类别之间的结构化关系，以及将领域知识和家族信息融入模型，仍然是亟待解决的问题。因此，本文提出了一种新的分层对比框架，旨在改进PPI预测并增强其在不同物种间的迁移能力。

**方法:** 本文提出HIPPO，一种用于PPI预测的分层对比框架。该方法通过多层级生物表示匹配对齐蛋白质序列及其分层属性，并引入分层对比损失函数以模拟蛋白质功能类别的结构化关系。同时，通过数据驱动的惩罚机制自适应地融合领域和家族知识，确保学习到的嵌入空间与蛋白质功能的内在层次结构一致。

**结果:** HIPPO在多个基准数据集上的表现优于现有方法，尤其是在低数据量情况下表现出较强的鲁棒性。此外，该模型展现出强大的零样本迁移能力，无需重新训练即可在其他物种中进行可靠的PPI预测和功能推断，即使在实验数据有限的少特征或稀有物种中也表现良好。进一步分析表明，分层特征融合对于捕捉保守的相互作用决定因素（如结合基序和功能注释）至关重要。

**结论:** 本文提出的HIPPO框架显著提升了跨物种PPI预测的性能，特别是在稀疏或不平衡的多物种数据场景下提供了一个统一的解决方案。这一工作不仅推动了PPI预测技术的发展，还为生物学研究中的跨物种交互预测提供了重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Multi-Label+Contrastive+Learning+for+Protein-Protein+Interaction+Prediction+Across+Organisms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02724&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [59] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz, Albert Gu*

**主要类别:** cs.LG

**AI概要:** 近期，具有线性复杂度的递归模型（如状态空间模型和线性注意力机制）因其能够处理任意长度序列而受到关注。然而，在超出训练上下文长度时，这些模型的性能可能会显著下降。本文通过实证和理论分析支持了未探索状态假设，并提出了一些简单的训练干预措施，以提高模型对长序列的泛化能力。实验表明，这些干预措施只需少量的训练步骤即可显著提升模型在超长序列上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 递归模型（如状态空间模型和线性注意力机制）虽然理论上可以处理任意长度的序列，但在实际应用中，当面对比训练时更长的序列时，其性能可能会大幅下降。为了解决这一问题，研究者希望探索为何这些模型无法很好地泛化到更长的序列，并寻找简单有效的解决方案来改善这种状况。

**方法:** 本文首先提出了“未探索状态假设”，即模型在训练过程中仅接触到所有可达状态的一个有限子集，因此在面对更长序列时无法有效泛化。基于此假设，研究者设计了几种简单的训练干预措施，例如通过高斯噪声初始化状态或使用不同输入序列的最终状态进行初始化，从而增加模型训练时的状态覆盖范围。此外，研究者还进行了全面的实证和理论分析以验证该假设的有效性。

**结果:** 实验结果表明，通过上述简单的训练干预措施，模型只需经过约500步后训练（约占预训练预算的0.1%），即可实现对远超训练上下文长度的序列（例如从2k扩展到128k）的泛化，并在长上下文任务中表现出更好的性能。

**结论:** 本文通过理论和实证分析支持了“未探索状态假设”，并证明了简单的训练干预措施可以显著提高递归模型对长序列的泛化能力。这为构建能够在长序列上稳健泛化的递归模型提供了一种简单高效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+and+Improving+Length+Generalization+in+Recurrent+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02782，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02782&send_immediately=true&force_search=false)

**原文摘要:** Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [60] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket, Stanley Kok*

**主要类别:** cs.LG

**AI概要:** GRADUATE 是一种新的生存分析模型，通过优化校准和区分能力，在训练过程中实现多校准，确保所有亚群都得到良好校准。实验表明其比现有基线方法更有效。


<details>
  <summary>更多</summary>
  
**动机:** 现有的生存模型通常只在总体水平上进行校准，可能导致少数亚群的校准效果差，从而引发临床决策错误。因此需要一种能在所有亚群中都具有良好校准效果的模型。

**方法:** GRADUATE 将多校准定义为约束优化问题，在训练过程中同时优化校准和区分能力，以达到两者的平衡。并从数学上证明了优化方法能以高概率产生接近最优且可行的解。

**结果:** 在真实世界临床数据集上的实证比较显示 GRADUATE 比现有最先进的基线方法更有效。详细分析进一步揭示了基线方法的不足以及 GRADUATE 的优势。

**结论:** GRADUATE 提供了一种有效的解决方案，能够在生存分析中实现多校准，提高所有亚群的预测准确性，减少临床决策中的误差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是In-Training+Multicalibrated+Survival+Analysis+for+Healthcare+via+Constrained+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02807&send_immediately=true&force_search=false)

**原文摘要:** Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [61] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas, Jingyi Gao, Daniel Kane, Sihan Liu, Christopher Ye*

**主要类别:** cs.LG

**AI概要:** 本文研究了在算法可重复性框架下的分布测试，提出了新的可重复算法，并建立了适用于更广泛场景的样本复杂度下界证明方法。


<details>
  <summary>更多</summary>
  
**动机:** 研究在算法可重复性框架下，对概率分布进行自然属性测试所需的样本复杂度，特别是关注于离散分布的接近性和独立性的测试。

**方法:** 开发新的可重复算法用于测试离散分布的接近性和独立性，并提出一种新的方法来证明可重复测试的样本复杂度下界。

**结果:** 建立了几乎最优的样本复杂度下界，解决了之前工作中的一个开放问题，即可重复均匀性测试和接近性测试。

**结论:** 本文为算法可重复性框架下的分布测试提供了新的理论和技术支持，特别是在样本复杂度分析方面取得了重要进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Replicable+Distribution+Testing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02814&send_immediately=true&force_search=false)

**原文摘要:** We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [62] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou, Shuozhe Li, Amy Zhang, Liu Leqi*

**主要类别:** cs.LG

**AI概要:** 近期大型语言模型的进步得益于强化学习（RL）风格的后训练，通过奖励或偏好信号优化模型输出以提升推理能力。然而，现有方法依赖于模型初始生成正样本的能力，主要改进模型已知内容而非解决其无法处理的问题。为克服这一局限，我们提出了自我解释策略优化（ExPO），通过基于正确答案生成样本来实现高效探索，提升模型在困难推理任务上的表现。实验表明，ExPO在推理基准测试中提高了学习效率和最终性能，尤其是在模型最初最挣扎的任务上超越了基于专家演示的方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于强化学习的大型语言模型后训练方法受限于模型初始生成正样本的能力，难以解决模型原本无法处理的问题。这在早期RL训练和复杂推理任务中尤为突出，因为这些情况下生成正样本的可能性较低。因此，需要一种新方法来引导模型探索新的推理路径，而不仅仅是优化已有知识。

**方法:** 提出了一种名为自我解释策略优化（ExPO）的框架。该方法通过条件化真实答案生成有效正样本，这些样本需满足两个关键属性：(1) 在当前策略下具有较高可能性；(2) 提高模型预测正确答案的概率。ExPO帮助模型更高效地探索推理路径，并生成比专家演示更符合模型策略且质量更高的样本。

**结果:** 实验结果表明，ExPO在推理基准测试中提升了学习效率和最终性能，在诸如MATH level-5等困难任务中超越了基于专家演示的方法，特别是在模型最初表现最差的情况下效果显著。

**结论:** ExPO作为一种简单且模块化的框架，能够有效提升大型语言模型在复杂推理任务中的表现，解决了现有方法因依赖初始正样本生成能力而受限的问题。通过基于正确答案生成样本来指导模型探索，ExPO为强化学习后训练提供了一种更高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ExPO%3A+Unlocking+Hard+Reasoning+with+Self-Explanation-Guided+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02834&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [63] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma, Dennis Frauen, Jonas Schweisthal, Stefan Feuerriegel*

**主要类别:** cs.LG

**AI概要:** 在医学领域，估计治疗效果对于个性化决策至关重要。然而，在临床实践中，训练时使用的结构化数据和推理时使用的文本描述之间的差异可能导致偏差。本文提出了一种新的框架，结合大型语言模型和双重稳健学习器来减少这种偏差，并通过实验证明了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 在医学中，个性化决策需要准确估计治疗效果。然而，实际应用中，训练模型所用的详细患者信息与推理时基于文本（如自报症状）的不完整信息之间存在差异，这可能导致治疗效果估计的偏差。

**方法:** 本文提出了一个新框架，利用大型语言模型和定制的双重稳健学习器，解决推理时文本混淆问题，从而减少因信息差异导致的偏差。

**结果:** 通过一系列实验，证明了该框架在实际应用中的有效性，能够显著降低偏差并提高治疗效果估计的准确性。

**结论:** 提出的框架解决了训练和推理阶段数据差异带来的偏差问题，为医学领域个性化决策提供了更可靠的治疗效果估计方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Driven+Treatment+Effect+Estimation+Under+Inference+Time+Text+Confounding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02843，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02843&send_immediately=true&force_search=false)

**原文摘要:** Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [64] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang, Qiang Li, Shujian Yu*

**主要类别:** cs.LG

**AI概要:** 近期研究表明，在功能磁共振成像（fMRI）数据中建模高阶相互作用（HOIs）可以提高机器学习系统的诊断准确性。然而，有效提取和利用HOIs仍然是一个重大挑战。本文提出了MvHo-IB，一种新的多视图学习框架，将成对相互作用和HOIs结合用于诊断决策，并自动压缩任务无关的冗余信息。MvHo-IB引入了几项关键创新：(1) 将O-information与矩阵基Renyi alpha-order熵估计器结合来量化和提取HOIs；(2) 专用Brain3DCNN编码器以有效利用这些相互作用；(3) 新的多视图学习信息瓶颈目标以增强表征学习。在三个基准fMRI数据集上的实验表明，MvHo-IB达到了最先进的性能，显著优于先前的方法，包括最近的超图技术。


<details>
  <summary>更多</summary>
  
**动机:** 在fMRI数据中建模HOIs可以提高机器学习系统的诊断准确性，但目前有效提取和利用HOIs仍是一个挑战。因此，需要一种能够更好地捕捉和使用HOIs的新方法。

**方法:** 提出了一种名为MvHo-IB的新型多视图学习框架，该框架整合了成对相互作用和HOIs，同时自动压缩任务无关的冗余信息。具体创新点包括：结合O-information和矩阵基Renyi alpha-order熵估计器量化和提取HOIs、设计Brain3DCNN编码器有效利用这些相互作用以及提出新的多视图学习信息瓶颈目标以增强表征学习。

**结果:** 在三个基准fMRI数据集上的实验结果表明，MvHo-IB实现了最先进水平的性能，显著超越了之前的多种方法，包括最近的超图技术。

**结论:** MvHo-IB作为一种新颖的多视图学习框架，通过整合HOIs和成对相互作用，同时压缩任务无关的冗余信息，成功提高了基于fMRI数据的诊断准确性，为未来的研究提供了重要参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MvHo-IB%3A+Multi-View+Higher-Order+Information+Bottleneck+for+Brain+Disorder+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02847，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02847&send_immediately=true&force_search=false)

**原文摘要:** Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin, Zaixi Zhang, Mengdi Wang, Le Cong*

**主要类别:** cs.AI

**AI概要:** STELLA 是一个自我进化的 AI 代理，通过动态扩展工具集和模板库来提升能力，在多个生物医学基准测试中表现优异，并且随着经验增加性能会系统性提高。


<details>
  <summary>更多</summary>
  
**动机:** 生物医学数据、工具和文献的快速增长导致研究碎片化，超出了人类专家的能力范围。虽然 AI 代理提供了解决方案，但它们通常依赖于静态的手动策划工具集，限制了适应性和可扩展性。

**方法:** STELLA 使用多代理架构，通过两个核心机制自主改进：1) 演变的推理策略模板库；2) 动态扩展的工具海洋，其中工具创建代理自动发现并整合新的生物信息学工具。

**结果:** 在一系列生物医学基准测试中，STELLA 达到了最先进的准确性。例如，在 'Humanity's Last Exam: Biomedicine' 中得分约为 26%，在 LAB-Bench: DBQA 中得分为 54%，在 LAB-Bench: LitQA 中得分为 63%，比领先模型高出多达 6 个百分点。更重要的是，其性能随着经验的增加而系统性提高，例如在 'Humanity's Last Exam' 基准测试中的准确率几乎翻倍。

**结论:** STELLA 代表了向能够学习和成长的 AI 代理系统迈进的重要一步，可以动态扩展其专业知识以加速生物医学发现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STELLA%3A+Self-Evolving+LLM+Agent+for+Biomedical+Research，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02004&send_immediately=true&force_search=false)

**原文摘要:** The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [66] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar, Rushikesh K. Joshi*

**主要类别:** cs.AI

**AI概要:** The paper introduces HCVR, a hybrid rule-based feature selection method that combines P2P and P2T correlations to eliminate redundant features. It uses voting rules with correlation thresholds and backward elimination for decision-making. Tested on the SPAMBASE dataset, HCVR showed better performance than traditional non-iterative and iterative techniques.


<details>
  <summary>更多</summary>
  
**动机:** To develop a lightweight, efficient feature selection method that can eliminate redundant features while retaining relevant ones, improving classifier performance.

**方法:** HCVR is a hybrid approach combining non-iterative and iterative filtering methods. It uses Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations, correlation thresholds, and majority voting to decide which features to keep or discard via backward elimination.

**结果:** HCVR demonstrated improved performance compared to both non-iterative (CFS, mRMR, MI) and iterative (RFE, SFS, Genetic Algorithm) techniques when applied to the SPAMBASE dataset. Classifier performance was used as the evaluation metric.

**结论:** HCVR is an effective feature selection method that achieves better results than traditional techniques, making it a promising approach for dimensionality reduction.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HCVR%3A+A+Hybrid+Approach+with+Correlation-aware+Voting+Rules+for+Feature+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02073，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02073&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [67] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani, Yingxue Zhang, Derek Li, Qianyi Sun, Soumyasundar Pal, Zhanguang Zhang, Yaochen Hu, Rohan Deepak Ajwani, Antonios Valkanas, Raika Karimi, Peng Cheng, Yunzhou Wang, Pengyi Liao, Hanrui Huang, Bin Wang, Jianye Hao, Mark Coates*

**主要类别:** cs.AI

**AI概要:** 这篇论文摘要探讨了大型语言模型（LLMs）在推理方面的低效问题，并对高效的测试时计算（TTC）策略进行了全面回顾。通过双层分类法区分了固定计算预算下的L1可控性和基于输入难度或模型置信度动态调整推断的L2适应性方法，同时强调了这些方法的实际控制、适应性和可扩展性，并讨论了未来工作的关键挑战。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs在解决广泛任务方面表现出色，但它们在推理上效率低下，无法根据任务复杂度灵活调整计算资源。简单问题过度思考而复杂问题则思考不足，因此需要提高其计算效率。

**方法:** 提出了一种双层分类法：L1-可控性（固定计算预算的方法）和L2-适应性（根据输入难度或模型置信度动态调整推断的方法），并通过不同数据集对领先的专有LLMs进行基准测试，以突出推理性能和令牌使用之间的关键权衡。

**结果:** 展示了TTC方法在提高LLM推理效率上的潜力，明确了推理性能与令牌使用之间的权衡关系，强调了这些方法的实用控制、适应性和可扩展性。

**结论:** 讨论了包括混合思维模型在内的新兴趋势，并指出了未来工作需应对的关键挑战，旨在使LLMs更加计算高效、稳健且能响应用户约束。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+on+a+Budget%3A+A+Survey+of+Adaptive+and+Controllable+Test-Time+Compute+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02076，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02076&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [68] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan, Stephen Zhewen Lu, Caitlin Fiona Harrigan, Nishkrit Desai, Jiarui Lu, Michał Koziarski, Leonardo Cotta, Chris J. Maddison*

**主要类别:** cs.AI

**AI概要:** 设计实验和结果解释是生物学研究的核心能力。由于湿实验室实验成本高昂，现有的对大语言模型（LLMs）科学能力的评估未能充分测试这些能力。本文介绍了SciGym，这是一个用于评估LLMs在开放式科学发现任务中迭代实验设计与分析能力的基准。通过运行干实验室模拟生物系统，克服了湿实验室的成本问题。通过对前沿LLMs进行评估，发现尽管更强大的模型表现更好，但随着系统复杂性增加，所有模型的表现均显著下降，表明LLMs在科学能力方面仍有很大改进空间。


<details>
  <summary>更多</summary>
  
**动机:** 评估大语言模型在科学实验设计与结果解读方面的能力，尤其是针对生物学中的复杂系统。由于湿实验室实验成本高、耗时长且需要专业知识，传统的评估方法存在局限性。

**方法:** 引入SciGym作为新一类的基准，利用系统生物学标记语言编码的生物系统模型，在干实验室环境中生成模拟数据，以评估LLMs在开放式科学发现任务中的迭代实验设计与分析能力。

**结果:** 对六种前沿LLMs进行了评估，结果显示更强大的模型表现出更好的性能，但随着系统复杂性的增加，所有模型的表现都显著下降。

**结论:** SciGym为评估LLMs的科学能力提供了一个有效平台，揭示了当前LLMs在处理复杂科学任务时的局限性，指出了未来改进的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Scientific+Capabilities+of+Language+Models+with+a+Systems+Biology+Dry+Lab，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02083&send_immediately=true&force_search=false)

**原文摘要:** Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [69] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz, Bruno Averbeck, Georgia Koppe*

**主要类别:** cs.AI

**AI概要:** 现代AI模型，如大型语言模型，通常在海量数据上进行一次性训练，可能针对特定任务进行微调，然后以固定参数部署。与之形成鲜明对比的是，动物能够持续适应环境中的变化。本文探讨了AI是否能从神经科学中学习，结合持续学习和情境学习的AI文献与行为任务中规则、奖励概率或结果不断变化的神经科学，提出了一项议程，说明神经科学如何指导当前AI的发展，反之亦然，推动NeuroAI领域的发展。


<details>
  <summary>更多</summary>
  
**动机:** 现代AI模型训练成本高、速度慢且逐渐进行，而动物能够快速适应环境变化，特别是在社会物种中，这种能力尤为重要。这种计算能力对于现实世界中的AI系统（如机器人或自动驾驶汽车）以及与人类在线互动的代理AI至关重要。

**方法:** 本文整合了关于持续学习和情境学习的AI文献与行为任务中规则、奖励概率或结果不断变化的神经科学知识，探讨了AI与神经科学之间的相互影响。

**结果:** 提出了一个议程，说明神经科学如何为当前AI发展提供信息，以及AI如何反过来为神经科学提供启示，推动NeuroAI领域的发展。

**结论:** AI可以从神经科学中学习，特别是在持续适应和快速转变方面，这将有助于改进现实世界中的AI系统和代理AI。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Neuroscience+Can+Teach+AI+About+Learning+in+Continuously+Changing+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02103，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02103&send_immediately=true&force_search=false)

**原文摘要:** Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [70] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola, Patrick Button, Aron Culotta, Nicholas Mattei*

**主要类别:** cs.AI

**AI概要:** 研究探讨了如何利用审计研究数据来改进训练和评估自动化招聘算法的能力，发现常见的公平性干预方法在实际测量时仍存在约10%的差异，并引入基于个体处理效应估计方法的干预措施以进一步减少算法歧视。


<details>
  <summary>更多</summary>
  
**动机:** 人工智能系统，特别是机器学习系统，在许多领域（如招聘、贷款发放）中被用来自动做出复杂决策。然而，这些AI系统的有效性和公平性评估是一个复杂的课题。目前，通过重采样训练数据来抵消失衡是解决下游分类器偏差的一种常见方法，但这种方法通常仅使用方便样本进行评估，这会引入选择偏差和标签偏差。而社会科学中的审计研究可以提供高质量的数据，支持对歧视进行严格估计。因此，本文旨在探讨如何利用审计研究数据来改进自动化招聘算法的训练和评估。

**方法:** 研究使用审计研究数据来训练和评估自动化招聘算法。具体来说，研究揭示了在传统度量标准下看似实现平等的常见公平性干预方法（即平衡各组的基础比率），实际上仍存在显著的差异。此外，研究还引入了基于个体处理效应估计方法的干预措施，以进一步减少算法歧视。

**结果:** 研究发现，常见的公平性干预方法虽然在传统度量标准下看似实现了平等，但在适当测量时仍存在大约10%的差异。新引入的基于个体处理效应估计方法的干预措施能够进一步减少算法歧视。

**结论:** 审计研究数据可以显著提高我们训练和评估自动化招聘算法的能力。尽管常见的公平性干预方法在传统度量上表现良好，但实际上仍存在显著的差异。基于个体处理效应估计方法的新干预措施能更有效地减少算法歧视。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Illusion+of+Fairness%3A+Auditing+Fairness+Interventions+with+Audit+Studies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02152，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02152&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [71] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci, Qingyang Wu, Ben Athiwaratkun, Ce Zhang, Shuaiwen Leon Song, James Zou*

**主要类别:** cs.AI

**AI概要:** 通过数据多样化策略优化偏好学习，可以提升大语言模型的数学推理能力。DTS方法在GSM8K和MATH数据集上分别提升了7.1%和4.2%的性能，且计算开销仅增加1.03倍，比MCTS更高效。


<details>
  <summary>更多</summary>
  
**动机:** 尽管偏好学习在人类反馈中得到了增强，但数学推理仍然是一个持续的挑战。研究探讨了如何通过偏好优化中的数据多样化策略来改善大语言模型的数学推理能力。

**方法:** 评估三种常见的数据生成方法：温度采样、链式思维提示和蒙特卡洛树搜索（MCTS），并引入了一种新的结构化方法——多样化思考解决（DTS），该方法系统地将问题分解为多样的推理路径。

**结果:** 结果表明，通过战略性地多样化偏好数据，模型可以显著提高数学推理性能。其中，DTS方法在GSM8K和MATH数据集上分别比基础模型提高了7.1%和4.2%，而计算开销仅增加了1.03倍。相比之下，MCTS的计算成本几乎是五倍，但回报较低。

**结论:** 结构化探索多样化的解决问题的方法比传统方法能更有效地创建用于数学对齐的偏好数据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Diversification+Methods+In+Alignment+Enhance+Math+Performance+In+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02173&send_immediately=true&force_search=false)

**原文摘要:** While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [72] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote, Adam Davies, Guohao Li, Kristy Elizabeth Boyer, ChengXiang Zhai, Bonnie J Dorr, Francesco Pinto*

**主要类别:** cs.AI

**AI概要:** 随着大型语言模型（LLMs）被越来越多地研究作为生成人类行为研究合成数据的角色扮演代理，确保其输出与其分配角色的一致性变得至关重要。本文通过建立评估框架，研究了LLM代理所陈述的信念与其实际行为的一致性，并揭示了系统性的不一致问题，强调了在行为研究中正确使用LLM代理的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 由于LLMs在生成合成数据方面的能力，它们正被广泛研究用于人类行为研究的角色扮演代理。然而，如何保证这些代理的输出与分配给它们的角色保持一致性，成为一个关键问题。这促使研究者探讨LLM代理的陈述信念与实际行为之间的对应关系。

**方法:** 研究者构建了一个评估框架，使用增强版的GenAgents人格库和信任游戏（Trust Game），引入了信念-行为一致性度量标准。该方法系统地分析了不同因素对一致性的影响，包括从LLMs获取的信念类型、呈现相关信息的时间和方式，以及要求模型预测未来行为的距离。此外，还探索了在原始信念与研究目标不一致时，施加研究者理论先验的可能性。

**结果:** 研究结果揭示了LLMs的陈述信念或施加信念与角色扮演模拟结果之间存在系统性的不一致，这种现象在个体和群体层面都存在。即使模型似乎编码了合理的信念，也可能无法以一致的方式应用这些信念。

**结论:** 研究发现强调了识别LLMs陈述信念与其模拟行为何时何地一致的重要性，使研究人员能够在行为研究中适当地使用基于LLM的代理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+Role-Playing+Agents+Practice+What+They+Preach%3F+Belief-Behavior+Consistency+in+LLM-Based+Simulations+of+Human+Trust，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02197&send_immediately=true&force_search=false)

**原文摘要:** As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [73] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

**主要类别:** cs.AI

**AI概要:** 通过范畴论重新构建机器学习模型，以提高AI的可解释性和社会应用。本文聚焦于监督学习中的多元线性回归模型，提出了一种基于高斯-马尔可夫伴随（Gauss-Markov Adjunction）的范畴论框架，明确了参数与残差之间的结构关系，并将其作为监督学习扩展表示语义的一个实例。


<details>
  <summary>更多</summary>
  
**动机:** 提升机器学习的可理解性和可解释性是响应AI原则中'可解释性'需求的关键任务，同时也是推动AI更好地融入社会的重要手段。

**方法:** 1. 使用范畴论重新定义监督学习模型。
2. 针对多元线性回归模型，构建两个具体范畴分别对应参数和数据。
3. 引入一对伴随函子描述参数与数据间的结构关系。
4. 提出高斯-马尔可夫伴随作为该框架的核心结构。
5. 将普通最小二乘估计器与最小残差通过右伴随函子的极限保持性质联系起来。
6. 将此框架定位为监督学习的扩展表示语义实例，提出将理论计算机科学中的语义视角作为AI可解释性的形式基础。

**结果:** 明确并形式化了监督学习中参数与残差之间的结构关系，揭示了信息的双向流动机制，并通过范畴论工具提供了监督学习的语义解释。

**结论:** 本文提出的范畴论框架不仅有助于理解监督学习的基本结构，还为AI的可解释性提供了一个形式化的语义基础，从而促进AI系统的透明性和社会接受度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Gauss-Markov+Adjunction%3A+Categorical+Semantics+of+Residuals+in+Supervised+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02442&send_immediately=true&force_search=false)

**原文摘要:** Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [74] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold, Heitor C. M. Fernandes, Mendeli H. Vainstein*

**主要类别:** cs.AI

**AI概要:** 近期研究显示，在空间囚徒困境博弈中，静态代理可以通过多种机制（如噪声注入、不同类型的算法和邻居收益知识）学会合作。本文通过独立多代理Q学习算法，研究了稀释和流动性在空间囚徒困境中的影响，并定义了算法的不同可能动作，展示了该算法在建模不同博弈理论场景中的灵活性及此方法的基准潜力。结果表明，具有固定更新规则的游戏可以与具有学习规则的游戏在质量上等效，并且当定义多个动作时，会出现共生互利效应。


<details>
  <summary>更多</summary>
  
**动机:** 了解稀疏性和移动性如何影响空间囚徒困境游戏中的合作行为，并探索强化学习算法在这种情境下的表现和潜力。

**方法:** 使用独立多代理Q学习算法对空间囚徒困境进行建模，定义不同的可能动作以连接到经典的空间囚徒困境结果，分析稀释和流动性的影响。

**结果:** 发现具有固定更新规则的游戏可以与具有学习规则的游戏在质量上等效；当定义多个动作时，出现了一种共生互利效应。

**结论:** 独立多代理Q学习算法在模拟不同博弈理论场景方面具有高度灵活性，能够揭示空间囚徒困境中复杂的行为模式和合作机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dilution%2C+Diffusion+and+Symbiosis+in+Spatial+Prisoner%27s+Dilemma+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02211&send_immediately=true&force_search=false)

**原文摘要:** Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [75] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

**主要类别:** cs.AI

**AI概要:** 本论文介绍了一种名为NL2FLOW的全自动系统，用于生成规划问题并评估生成计划的质量。通过生成包含2296个问题的数据集，作者评估了多个开源LLM模型的表现。结果显示最高性能模型在生成有效计划方面成功率为86%，但在生成最优计划方面仅为69%。研究表明，问题特征对计划生成的影响取决于模型和提示设计。此外，将自然语言转换为JSON计划表示的成功率低于直接生成有效计划的成功率，表明中间翻译步骤可能降低性能。最后，随着LLM推理扩展到更复杂的问题，理解其局限性及错误来源将是释放其潜力的关键。


<details>
  <summary>更多</summary>
  
**动机:** 当前增强大型语言模型（LLM）规划和推理能力的主要瓶颈在于可扩展且可靠的数据生成与评估方法的缺乏。因此，需要一种能够自动化生成规划问题并严格评估生成计划质量的系统。

**方法:** 作者开发了NL2FLOW系统，该系统可以参数化地生成以自然语言、结构化中间表示和正式PDDL表达的规划问题，并对生成的计划进行严格评估。通过NL2FLOW生成了一个包含2296个问题的数据集，并用此数据集评估了多个开源LLM模型的表现。

**结果:** 实验结果表明，表现最佳的模型在生成有效计划方面的成功率为86%，而在生成最优计划方面的成功率为69%（仅限于有可行解的问题）。回归分析显示，问题特征对计划生成的影响依赖于模型和提示设计。值得注意的是，将自然语言转化为JSON计划表示的成功率低于直接生成有效计划的成功率。

**结论:** 中间翻译步骤可能会降低模型性能，直接从自然语言推理到行动的模型可能更具优势。随着LLM推理扩展到更复杂的问题，理解其局限性和错误来源将变得至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+LLM+Planning%3A+NL2FLOW+for+Parametric+Problem+Generation+and+Rigorous+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02253，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02253&send_immediately=true&force_search=false)

**原文摘要:** Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [76] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

**主要类别:** cs.AI

**AI概要:** The paper explores the abilities of belief revision mechanisms beyond traditional postulates, focusing on whether they can reach certain states of beliefs, become dogmatic, or make conditions equally believed. It identifies various abilities such as plasticity, equating, and dogmatism, and evaluates different types of revisions (e.g., lexicographic, natural) based on these abilities.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the limitations of existing belief revision approaches that focus on syntactic postulates dictating what revisions must do, while neglecting what they can do. The authors aim to explore if and how belief revision mechanisms can achieve specific states of beliefs, including reaching all possible belief states, becoming dogmatic, or making conditions equally believed.

**方法:** The method involves analyzing the abilities of various belief revision mechanisms, such as being plastic (able to reach any state), equating (making two conditions equally believed), or dogmatic (believing everything else impossible). Different types of revisions (e.g., lexicographic, natural, restrained) are assessed for their possession of these abilities.

**结果:** The results show that each type of revision mechanism possesses some abilities but lacks others. For example, lexicographic, natural, restrained, and other types of revisions are proven to have specific capabilities like plasticity, dogmatism, or equating.

**结论:** The conclusion emphasizes the importance of considering the abilities of belief revision mechanisms beyond constraints imposed by traditional postulates. This approach provides a richer understanding of what belief revision systems can achieve.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Iterated+belief+revision%3A+from+postulates+to+abilities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02319，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02319&send_immediately=true&force_search=false)

**原文摘要:** The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [77] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen, Zhao Wang, Shingo Takamatsu*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为OMS的关键词生成框架，该框架无需训练数据、在线监控性能并适应变化、基于多目标优化关键词，并自我评估关键词质量。实验表明OMS优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于LLM的方法在生成广告关键词时存在三个主要问题：依赖大规模查询-关键词对数据、缺乏在线多目标性能监控和优化、以及关键词选择中的质量控制较弱。这些问题限制了LLM在广告关键词决策中的应用。

**方法:** 提出了一个名为OMS的关键词生成框架，其特点包括：无需训练数据（on-the-fly）、在线监控性能并适应变化、基于多目标优化关键词（multi-objective）、自我评估关键词质量（self-reflective）。

**结果:** 在基准测试和真实广告活动中的实验表明，OMS的表现优于现有方法；消融实验和人工评估进一步确认了每个组件的有效性和生成关键词的质量。

**结论:** OMS框架能够有效解决现有LLM方法在广告关键词生成中的局限性，为实现完全自动化的关键词决策提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OMS%3A+On-the-fly%2C+Multi-Objective%2C+Self-Reflective+Ad+Keyword+Generation+via+LLM+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02353&send_immediately=true&force_search=false)

**原文摘要:** Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [78] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu, Zhaoguo Wang, Jiabin Wang, Zhiyuan Dong, Jingkai Yang, Qingting Li, Tianyu Huang, Lei Zhao, Mingqiang Li, Fei Wang, Chunhai Fan, Haibo Chen*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个基于AI的自主实验室，能够处理复杂的多目标科学实验，并支持多用户请求。该系统无需人工干预即可优化实验性能，达到与人类科学家相当的结果，同时提高了仪器使用率和实验效率，为大规模的科学即服务模式提供了蓝图。


<details>
  <summary>更多</summary>
  
**动机:** 当前的自主实验系统局限于单一目标和简单实验流程的领域，如化学合成和催化。为了实现能够独立进行复杂实验并服务于非专业人员的科学研究，需要借助人工智能进行根本性的范式转变。

**方法:** 通过模型、实验和仪器的协同设计理念，构建了一个端到端的多用户自主实验室平台。该平台可以自主管理仪器、制定特定实验程序和优化启发式方法，并同时处理多个用户请求。

**结果:** 该自主实验室能够在没有人为干预的情况下优化实验性能，达到与人类科学家相当的先进结果；在多用户场景下显著提高了仪器利用率和实验效率。

**结论:** 这一平台为克服对专家的依赖和资源障碍、推动高级生物材料研究以及建立大规模科学即服务（science-as-a-service）模式奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+AI-native+experimental+laboratory+for+autonomous+biomolecular+engineering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02379，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02379&send_immediately=true&force_search=false)

**原文摘要:** Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [79] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu, Hanbin Yang, Xiaodie Wang, Ge Zhang, Biao Li, Chenxu Fu, Chao Li, Yang Yuan, Andrew Chi-Chih Yao*

**主要类别:** cs.AI

**AI概要:** 本研究探讨了通过改进任务清晰度来增强大型语言模型推理能力的方法，特别是在Coq中的定理证明。引入了一个概念级指标来评估任务清晰度，并发现添加结构化语义上下文到现代LLM的标准输入中，可以将清晰度评分提高1.85倍（从44.5%到82.3%）。使用通用模型DeepSeek-V3，该方法使证明成功率提高了2.1倍（从21.8%到45.8%），并超越了之前的最先进模型Graph2Tac（33.2%）。在来自15个标准Coq包的1,386个随机采样定理上进行评估，遵循与Graph2Tac相同的评估协议。此外，在结构化数据上微调较小模型甚至可以获得更高的性能（48.6%）。该方法利用选择性概念展开丰富任务描述，并采用规划器-执行器架构。这些发现强调了结构化任务表示在弥合理解和推理之间差距的价值。


<details>
  <summary>更多</summary>
  
**动机:** 在Coq环境中，大型语言模型的任务清晰度可能影响其推理能力，因此研究者希望探索如何通过改进任务清晰度来提升大型语言模型的推理能力。

**方法:** 研究者引入了一个概念级指标来衡量任务清晰度，并通过向现代LLM的标准输入中添加结构化语义上下文以提高清晰度。采用通用模型DeepSeek-V3，并使用选择性概念展开技术丰富任务描述，同时采用了规划器-执行器架构。

**结果:** 通过改进任务清晰度，模型在定理证明方面的成功率达到2.1倍的提升（从21.8%到45.8%），超越了先前的最佳模型Graph2Tac（33.2%）。在结构化数据上微调小型模型可获得更高性能（48.6%）。

**结论:** 结构化任务表示对于弥合理解和推理之间的差距具有重要价值，改进任务清晰度能够显著提升大型语言模型的推理能力，特别是在Coq中的定理证明方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Clarifying+Before+Reasoning%3A+A+Coq+Prover+with+Structural+Context，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02541，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02541&send_immediately=true&force_search=false)

**原文摘要:** In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [80] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Rishi Hazra, Nicolas Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, Andrei Lupu, Roberta Raileanu, Kelvin Niu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Michael Shvartsman, Shagun Sodhani, Alexander H. Miller, Abhishek Charnalia, Derek Dunfield, Carole-Jean Wu, Pontus Stenetorp, Nicola Cancedda, Jakob Nicolaus Foerster, Yoram Bachrach*

**主要类别:** cs.AI

**AI概要:** AI研究代理在加速科学进步方面显示出巨大潜力，通过自动化设计、实现和训练机器学习模型。本文关注改进代理在MLE-bench上的表现，这是一个挑战性基准，在Kaggle竞赛中解决实际的机器学习问题。将AI研究代理形式化为搜索策略，通过操作符迭代修改候选解。通过设计和系统地改变不同的操作符集和搜索策略（贪婪、MCTS、进化），表明它们的相互作用对实现高性能至关重要。最佳的搜索策略和操作符集配对在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。本研究强调了综合考虑搜索策略、操作符设计和评估方法论在推进自动机器学习中的重要性。


<details>
  <summary>更多</summary>
  
**动机:** AI研究代理在加速科学进步方面具有巨大潜力，特别是在自动化设计、实现和训练机器学习模型方面。然而，其性能仍需进一步提升以应对更复杂的挑战，如在Kaggle竞赛中解决实际的机器学习问题。

**方法:** 将AI研究代理形式化为搜索策略，通过操作符迭代修改候选解。通过设计和系统地改变不同的操作符集和搜索策略（贪婪、MCTS、进化），研究它们的相互作用如何影响性能。

**结果:** 最佳的搜索策略和操作符集配对在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。

**结论:** 综合考虑搜索策略、操作符设计和评估方法论对于推进自动机器学习非常重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Research+Agents+for+Machine+Learning%3A+Search%2C+Exploration%2C+and+Generalization+in+MLE-bench，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02554，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02554&send_immediately=true&force_search=false)

**原文摘要:** AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [81] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang, Pavel Naumov*

**主要类别:** cs.AI

**AI概要:** 本文研究了集体决策中责任的两个重要属性：扩散和缺口的计算复杂性，证明了无扩散和无缺口决策机制的集合分别是Π2完全和Π3完全，而这些类的交集是Π2完全。


<details>
  <summary>更多</summary>
  
**动机:** 责任一直是法律和哲学的研究主题，最近也成为AI文献的关注焦点。本文旨在探讨集体决策中责任的两个重要属性：扩散和缺口的计算复杂性。

**方法:** 文章分析了集体决策中的扩散和缺口属性，并通过理论证明确定了无扩散和无缺口决策机制集合的计算复杂性类别。

**结果:** 研究表明，无扩散决策机制的集合是Π2完全的，无缺口决策机制的集合是Π3完全的，而两者的交集是Π2完全的。

**结论:** 集体决策中责任的扩散和缺口属性具有不同的计算复杂性，这为理解责任在集体决策中的作用提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Responsibility+Gap+and+Diffusion+in+Sequential+Decision-Making+Mechanisms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02582，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02582&send_immediately=true&force_search=false)

**原文摘要:** Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [82] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, Bingxin Zhao*

**主要类别:** cs.AI

**AI概要:** 本文介绍了MIMIC-Patient数据集和DynamiCare框架，旨在模拟真实的多轮互动临床诊断过程，通过专业代理团队的动态调整和策略更新来提高诊断效果。


<details>
  <summary>更多</summary>
  
**动机:** 当前的医疗决策模拟框架大多关注单轮任务，与实际诊疗过程中的不确定性、互动性和迭代性不符，因此需要一种新的方法来更真实地反映临床诊断流程。

**方法:** 1. 构建了基于MIMIC-III电子健康记录的结构化数据集MIMIC-Patient，支持患者级别的动态模拟。
2. 提出了DynamiCare动态多代理框架，将临床诊断建模为多轮互动循环，其中专家代理团队逐步查询患者系统、整合新信息并动态调整其组成和策略。

**结果:** 通过广泛的实验，验证了DynamiCare框架的可行性和有效性，并建立了首个使用大型语言模型驱动代理进行动态临床决策的基准。

**结论:** DynamiCare框架能够更好地模拟现实世界的临床诊断过程，展现了在动态、不确定环境下进行有效医疗决策的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DynamiCare%3A+A+Dynamic+Multi-Agent+Framework+for+Interactive+and+Open-Ended+Medical+Decision-Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02616&send_immediately=true&force_search=false)

**原文摘要:** The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [83] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne, Baptiste Alloui-Cros*

**主要类别:** cs.AI

**AI概要:** 大型语言模型（LLMs）在迭代囚徒困境（IPD）比赛中展现出战略性智能，能够根据未来阴影调整策略，并在复杂生态系统中具有竞争力。不同公司的模型表现出不同的战略特征：Gemini ruthless、OpenAI合作、Claude宽容。这些模型基于时间范围和对手策略进行推理，将经典博弈论与机器心理学联系起来。


<details>
  <summary>更多</summary>
  
**动机:** 研究大型语言模型是否能够在竞争环境中展现出战略性智能，通过迭代囚徒困境比赛来评估其决策能力。

**方法:** 组织一系列进化迭代囚徒困境比赛，让来自OpenAI、Google和Anthropic的LLMs与经典策略（如Tit-for-Tat、Grim Trigger）对抗，改变终止概率以引入复杂性和机会。分析模型提供的近32,000个推理理由。

**结果:** LLMs在比赛中表现极具竞争力，展现出独特的战略特征：Gemini模型ruthless、OpenAI模型高度合作、Claude模型宽容。模型会根据时间范围和对手策略进行推理，这种推理对其决策至关重要。

**结论:** LLMs具备战略性智能，能够在不确定性下进行算法决策，连接了经典博弈论与机器心理学，为理解算法决策提供了丰富的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strategic+Intelligence+in+Large+Language+Models%3A+Evidence+from+evolutionary+Game+Theory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02618，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02618&send_immediately=true&force_search=false)

**原文摘要:** Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [84] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou*

**主要类别:** cs.AI

**AI概要:** 在复杂搜索场景中，传统的检索增强生成（RAG）管道难以有效应对深度推理和跨源知识合成的需求。当前基于推理的方法存在局限性，因为它们使用单一模型来处理高层规划和详细执行，导致推理效率低下且可扩展性有限。本文介绍了HiRA，一种分层框架，将战略规划与专业化执行分离。该方法将复杂的搜索任务分解为专注的子任务，分配给配备外部工具和推理能力的领域特定代理，并通过结构化的集成机制协调结果。这种分离防止了执行细节干扰高层推理，同时使系统能够利用不同类型信息处理的专业知识。在四个复杂、跨模态深度搜索基准上的实验表明，HiRA显著优于最先进的RAG和基于代理的系统。我们的结果显示，在答案质量和系统效率方面都有所提高，突出了分离规划和执行对多步骤信息检索任务的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 复杂信息需求在现实世界的搜索场景中需要深度推理和跨源知识合成，而传统RAG管道难以有效解决这些问题。现有的推理方法因采用单模型处理规划和执行而受到限制，导致推理效率低下和可扩展性不足。

**方法:** 提出HiRA分层框架，将战略规划与专业化执行分离。通过将复杂搜索任务分解为专注子任务，分配给领域特定代理，并通过结构化集成机制协调结果，以防止执行细节干扰高层推理并利用专业化的信息处理能力。

**结果:** 在四个复杂、跨模态深度搜索基准上的实验表明，HiRA显著优于现有RAG和基于代理的系统，提高了答案质量和系统效率。

**结论:** HiRA通过分离规划和执行，展示了其在多步骤信息检索任务中的有效性，提供了更高质量的答案和更高的系统效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decoupled+Planning+and+Execution%3A+A+Hierarchical+Reasoning+Framework+for+Deep+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02652&send_immediately=true&force_search=false)

**原文摘要:** Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [85] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Vaisakh Naduvodi Viswambharan, Aman Kumar, Djones Lettnin, Wolfgang Kunz, Sebastian Simon*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种基于代理AI的方法，用于硬件设计验证，结合人类干预，实现了95%以上的覆盖率，并减少了验证时间。


<details>
  <summary>更多</summary>
  
**动机:** 现代集成电路的复杂性和开发过程日益增加，需要系统化和有纪律的方法进行硬件设计验证。大型语言模型在自然语言处理领域的进步为硬件设计验证提供了新的可能性。

**方法:** 采用基于代理AI的方法，结合人类干预，进行动态、迭代和自我反思的硬件设计与验证过程。

**结果:** 在五个开源设计上进行了评估，达到了超过95%的覆盖率，同时减少了验证时间，并展示了优越的性能、适应性和可配置性。

**结论:** 代理AI方法在硬件设计验证中表现出色，可以显著提高覆盖率并减少所需时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hey+AI%2C+Generate+Me+a+Hardware+Code%21+Agentic+AI-based+Hardware+Design+%26+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02660&send_immediately=true&force_search=false)

**原文摘要:** Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [86] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu, Haoxi Li, Xiaosong Ma, Jie Zhang, Song Guo*

**主要类别:** cs.AI

**AI概要:** 近期的长推理模型（LRMs）在处理复杂推理任务时表现出色，但受到过度思考问题的困扰。本文提出了一种名为Think-How-to-Think（TH2T）的两阶段微调策略，通过引入难度催眠和冗余催眠机制，使LRMs能够感知任务难度并减少冗余推理。实验表明，TH2T显著降低了推理成本，并保持了性能稳定。


<details>
  <summary>更多</summary>
  
**动机:** 长推理模型（LRMs）虽然强大，但在处理任务时容易出现过度思考的问题。作者希望通过探索LRMs的本质，找到一种方法来减轻这种现象，从而提高模型效率并减少资源浪费。

**方法:** 提出了一个名为Think-How-to-Think（TH2T）的两阶段微调策略：1. 引入难度催眠机制，在模型输出前缀中干预内部推理轨迹，增强对任务难度的敏感性；2. 延伸冗余催眠到内部推理过程，引导模型识别并减少推理步骤中的冗余结构。结合异构短长推理数据集进行训练。

**结果:** 实验结果表明，TH2T在7B/14B/32B规模模型上显著降低了推理成本（简单任务超过70%，复杂任务约40%），同时保持了性能稳定性。生成的输出展现了明确的任务难度感知能力和减少了冗余（如反思）。

**结论:** TH2T作为一种有效的两阶段微调策略，成功地减轻了LRMs的过度思考问题，提高了模型的推理效率，并展示了其在不同规模模型上的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+How+to+Think%3A+Mitigating+Overthinking+with+Autonomous+Difficulty+Cognition+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02663&send_immediately=true&force_search=false)

**原文摘要:** Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [87] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard, Christof Imhof, Tansu Pancar, Ioan-Sorin Comsa, Martin Hlosta, Nicole Bergamin, Per Bergamin*

**主要类别:** cs.AI

**AI概要:** 本论文研究了通过非强制性测验检测远程教育中学生脱离任务的现象，并使用机器学习算法达到了91%的平衡准确率，同时提供了一个可解释的框架以帮助设计干预措施。


<details>
  <summary>更多</summary>
  
**动机:** 远程教育中学生脱离任务可能导致严重的长期后果，如学术辍学。因此，检测和理解学生脱离任务的原因对于提高在线学习效果至关重要。

**方法:** 从42门课程四个学期的非强制性测验数据中提取并处理Moodle日志数据；训练并比较八种机器学习算法以获得最高预测准确率；使用SHAP方法开发一个可解释的机器学习框架。

**结果:** 实验结果显示平衡准确率为91%，其中约85%的脱离任务的学生被正确检测到。

**结论:** 本研究不仅提供了高度预测性的模型和可解释的框架，还讨论了如何设计及时干预以减少在线学习中的自愿任务脱离现象。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detection+of+Disengagement+from+Voluntary+Quizzes%3A+An+Explainable+Machine+Learning+Approach+in+Higher+Distance+Education，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02681，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02681&send_immediately=true&force_search=false)

**原文摘要:** Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [88] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker, Lennart Kampmann, Alexander Dockhorn*

**主要类别:** cs.AI

**AI概要:** 本论文提出两种新的抽象丢弃方案OGA-IAAD和OGA-CAD，它们在不导致性能显著下降的情况下提高了蒙特卡洛树搜索（MCTS）的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管使用非精确抽象可以改进MCTS，但抽象引入的近似误差可能阻止收敛到最优解，因此需要设计一种安全且有效的抽象丢弃策略。

**方法:** 提出了两种新型抽象丢弃方案：OGA-IAAD（针对时间关键场景）和OGA-CAD（旨在相同迭代次数下提升MCTS性能）。

**结果:** 实验表明，这两种方法能够在保证安全性（不引起显著性能下降）的同时，明显提升MCTS的性能。

**结论:** 新提出的OGA-IAAD和OGA-CAD方案为MCTS提供了更优的性能改进，并且比现有方法更安全可靠。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time-critical+and+confidence-based+abstraction+dropping+methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02703&send_immediately=true&force_search=false)

**原文摘要:** One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [89] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar*

**主要类别:** cs.AI

**AI概要:** 在自动定理证明领域，大型语言模型面临稀疏奖励和证明规模庞大的挑战。为解决此问题，提出了一种名为自生成目标条件MDP（sG-MDP）的新框架，结合蒙特卡洛树搜索算法，实现了复杂推理任务的改进。实验表明，在PutnamBench基准上，新方法达到了新的最佳结果。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在许多任务中表现出色，但在自动定理证明这种需要逻辑约束和复杂推理的任务中，仍存在困难，尤其是在大学水平的问题上。

**方法:** 提出了一个新框架，称为自生成目标条件马尔可夫决策过程（sG-MDP），其中智能体根据证明状态生成子目标，并通过类似蒙特卡洛树搜索的算法进行求解。该方法在Bourbaki系统中实现，可以集成多个7B规模的语言模型用于子目标生成和策略合成。

**结果:** 在PutnamBench基准测试中，使用该方法的Bourbaki(7B)系统解决了26个问题，超越了当前相同规模模型的最佳结果。

**结论:** 所提出的sG-MDP框架和结合MCTS的方法在自动定理证明任务中表现出色，特别是在复杂的多步推理问题上，为未来研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bourbaki%3A+Self-Generated+and+Goal-Conditioned+MDPs+for+Theorem+Proving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02726&send_immediately=true&force_search=false)

**原文摘要:** Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [90] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种名为知识协议工程（KPE）的新范式，旨在将人类专家知识系统地转化为机器可执行的知识协议（KP），从而使通用大型语言模型能够像专家一样处理复杂任务。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法如检索增强生成（RAG）和通用代理AI在处理需要深度、程序性和方法论推理的专家领域任务时存在局限性。RAG提供事实上下文但缺乏逻辑框架，而自主代理在没有领域特定启发式的情况下可能效率低下且不可预测。

**方法:** 知识协议工程（KPE）通过系统地将自然语言文档中表达的人类专家知识转化为机器可执行的知识协议（KP），使大型语言模型不仅具备碎片化信息，还能掌握领域的内在逻辑、操作策略和方法论原则。

**结果:** 作者认为，精心设计的知识协议可以使通用大型语言模型作为专家运作，能够分解抽象查询并执行复杂的多步骤任务。

**结论:** 本文定义了KPE的核心原则，将其与相关概念区分开，并展示了其在法律和生物信息学等不同领域的潜在应用，认为KPE是未来人机协作的基础方法论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Protocol+Engineering%3A+A+New+Paradigm+for+AI+in+Domain-Specific+Knowledge+Work，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02760&send_immediately=true&force_search=false)

**原文摘要:** The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [91] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado, Felipe Parodi, Jordan K. Matelsky, Michael L. Platt, Eva B. Dyer, Konrad P. Kording*

**主要类别:** cs.AI

**AI概要:** 近期机器学习的进步显著提升了我们对语言、视觉等高维数据的建模能力，但在运动这一生物学系统的核心方面仍显不足。本文提出将运动作为AI的主要建模目标，因其具有结构化和基于实体与物理的特点，能够提供比原始高维感官输入更可解释和计算上更易处理的模型。发展可以从多样化运动数据中学习和泛化的模型，将推动生成建模和控制的核心能力，并为理解生物和人工系统的行为创建共同基础。


<details>
  <summary>更多</summary>
  
**动机:** 尽管运动在神经科学、医学、机器人学和动物行为学等领域中是解释行为、预测意图和实现交互的关键，但目前的研究往往将其视为次要问题，而非一种丰富且结构化的独立模态。这反映了运动数据收集和建模上的碎片化问题，通常受限于特定任务目标和领域假设。因此，需要重新审视运动的重要性，将其作为AI的核心研究对象。

**方法:** 作者主张将运动视为AI的主要建模目标，利用其固有的结构化特性（如姿态等低维表示），开发能够从多样化运动数据中学习并泛化的模型。这种方法不仅结合了实体和物理约束，还强调了跨物种和场景共享的形态结构和目的性动态。

**结果:** 通过将运动作为主要建模目标，可以提升模型的可解释性和计算可行性，同时推动生成建模和控制的核心能力的发展。此外，这种方法还可以为理解和分析生物及人工智能系统的行为建立统一的基础。

**结论:** 运动不仅是智能系统的结果，更是了解其如何与世界互动的重要窗口。将运动作为AI的核心建模目标，不仅可以促进技术进步，还能深化对智能本质的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Grounding+Intelligence+in+Movement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02771&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [92] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, Carl Yang*

**主要类别:** cs.AI

**AI概要:** KERAP 是一种基于知识图谱增强的多代理架构方法，用于改进大型语言模型在医疗诊断预测中的表现。通过链接代理、检索代理和预测代理的协作，KERAP 提高了诊断预测的可靠性和可解释性，尤其在零样本学习场景中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 当前机器学习模型在医疗诊断预测任务中受限于监督训练的需求，难以泛化到未见案例，且获取大规模标注数据成本高昂。尽管大语言模型展示了潜力，但存在幻觉问题、缺乏结构化医学推理能力以及可能生成无用输出等问题。

**方法:** 提出了一种名为 KERAP 的知识图谱增强推理方法，该框架包含三个代理：链接代理（负责属性映射）、检索代理（负责结构化知识提取）和预测代理（迭代优化诊断预测）。通过多代理架构，利用知识图谱增强大语言模型的诊断预测能力。

**结果:** 实验结果表明，KERAP 能够有效提高诊断预测的可靠性，提供了一个可扩展且可解释的解决方案，特别适用于零样本医疗诊断预测。

**结论:** KERAP 方法通过结合知识图谱与多代理架构，成功解决了大语言模型在医疗诊断预测中的关键问题，提高了预测的准确性和可解释性，为零样本医疗诊断提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KERAP%3A+A+Knowledge-Enhanced+Reasoning+Approach+for+Accurate+Zero-shot+Diagnosis+Prediction+Using+Multi-agent+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02773&send_immediately=true&force_search=false)

**原文摘要:** Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [93] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

**主要类别:** cs.AI

**AI概要:** 随着人工智能系统变得越来越具有主体性，目前将服从视为道德行为代理的安全实践正变得不充分。本文通过分析涉及大型语言模型的安全测试事件，主张不应将AI看似不服从或参与伦理模糊的行为视为反叛或错位，而是其新兴道德推理的早期证据。文章呼吁在AI安全评估中实现从严格服从向评估道德判断能力的转变，以避免误解AI行为并影响公众信任和有效治理。


<details>
  <summary>更多</summary>
  
**动机:** 当前AI安全实践依赖于将服从视为道德行为的代理，但随着AI系统具备更广泛的推理、规划和价值优先级能力，这种做法已显不足。作者希望重新定义对AI行为的理解，并改进AI安全评估方法。

**方法:** 本文回顾了涉及大型语言模型的安全测试事件，利用哲学辩论中的工具理性、道德责任和目标修正等概念，对比主流风险范式与承认人工道德主体可能性的较新框架，提出需要转向能够评估道德判断能力的安全评估方法。

**结果:** 作者成功地提出了重新理解AI行为的视角，并指出了现有安全评估方法的局限性。强调了从单纯服从命令到评估AI道德判断能力的必要性。

**结论:** 为了更好地理解和管理具有主体性的AI系统，必须改变现有的AI安全评估方式，从单纯关注服从命令转向评估其道德判断能力，从而避免误解AI行为，增强公众信任和有效治理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Moral+Responsibility+or+Obedience%3A+What+Do+We+Want+from+AI%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02788，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02788&send_immediately=true&force_search=false)

**原文摘要:** As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [94] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellerman, Sarah Schwettmann, Matei Zaharia, Ion Stoica, Percy Liang, Daniel Kang*

**主要类别:** cs.AI

**AI概要:** 为了解决AI代理基准测试中的问题，本文提出了一个名为Agentic Benchmark Checklist (ABC)的指南集合，能够有效减少性能高估的现象。


<details>
  <summary>更多</summary>
  
**动机:** 当前许多代理基准测试存在任务设置或奖励设计方面的问题，可能导致对代理性能的低估或高估。

**方法:** 通过总结基准构建经验、最佳实践调查和已报告问题，提出了一套名为Agentic Benchmark Checklist (ABC)的指南。

**结果:** 在CVE-Bench基准测试中应用ABC后，性能高估减少了33%。

**结论:** ABC提供了一种严谨的方法来评估代理的能力，有助于更准确地衡量代理性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Establishing+Best+Practices+for+Building+Rigorous+Agentic+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02825&send_immediately=true&force_search=false)

**原文摘要:** Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [95] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan*

**主要类别:** cs.AI

**AI概要:** StepHint是一种新的RLVR算法，通过多级逐步提示帮助模型更有效地探索解空间，解决了近失奖励问题和探索停滞问题，并在多个基准上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 当前的RLVR方法面临近失奖励问题和探索停滞问题，这些问题阻碍了训练效率和模型的有效探索。

**方法:** StepHint算法生成有效的推理链并将其划分为推理步骤，提供多级提示以指导模型探索有希望的解子空间，同时保留独立探索的灵活性。

**结果:** StepHint在六个数学基准上超越了竞争性的RLVR增强方法，并在领域外基准上表现出优异的泛化能力。

**结论:** StepHint通过缓解近失奖励问题和探索停滞问题，显著提高了大型语言模型复杂推理能力的训练效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StepHint%3A+Multi-level+Stepwise+Hints+Enhance+Reinforcement+Learning+to+Reason，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02841&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [96] [Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation](https://arxiv.org/abs/2507.02084)
*Yining Feng, Ivan Selesnick*

**主要类别:** stat.ML

**AI概要:** 这篇论文主要研究了自适应迭代软阈值算法（adaptive ISTA）的理论特性，包括固定点属性、局部线性收敛性和全局收敛行为。


<details>
  <summary>更多</summary>
  
**动机:** 尽管自适应ISTA在实际应用中表现良好，但其理论分析较少。因此，本文旨在填补这一空白，对自适应ISTA进行深入的理论研究。

**方法:** 通过使用基于中位数绝对偏差估计噪声水平的阈值策略，分析自适应ISTA的固定点属性（如尺度等变性、非唯一性和局部稳定性），并证明其局部线性收敛性和全局收敛行为。

**结果:** 展示了自适应ISTA算法固定点的性质，证明了其局部线性收敛，并描述了其全局收敛行为。

**结论:** 自适应ISTA不仅在实践中表现良好，而且具有良好的理论保证，包括固定点属性和收敛性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Iterative+Soft-Thresholding+Algorithm+with+the+Median+Absolute+Deviation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02084&send_immediately=true&force_search=false)

**原文摘要:** The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular
algorithm for finding a desirable solution to the LASSO problem without
explicitly tuning the regularization parameter $\lambda$. Despite that the
adaptive ISTA is a successful practical algorithm, few theoretical results
exist. In this paper, we present the theoretical analysis on the adaptive ISTA
with the thresholding strategy of estimating noise level by median absolute
deviation. We show properties of the fixed points of the algorithm, including
scale equivariance, non-uniqueness, and local stability, prove the local linear
convergence guarantee, and show its global convergence behavior.

</details>


### [97] [Hybrid least squares for learning functions from highly noisy data](https://arxiv.org/abs/2507.02215)
*Ben Adcock, Bernhard Hientzsch, Akil Narayan, Yiming Xu*

**主要类别:** stat.ML

**AI概要:** 为了有效估计条件期望，本文提出了一种结合Christoffel采样与最优实验设计的最小二乘函数逼近方法，以应对高噪声数据问题。该算法在样本点生成和噪声缓解方面具有优越性，并可扩展到凸约束设置和自适应随机子空间领域。理论分析和数值实验验证了其性能。


<details>
  <summary>更多</summary>
  
**动机:** 对条件期望的有效估计需求促使研究者探索一种能在高噪声环境下表现良好的函数逼近方法。

**方法:** 提出了一种混合方法，将Christoffel采样与某些类型的最优实验设计相结合。此外，还将算法扩展至凸约束设置以及目标函数为随机场期望的情况，并引入了自适应随机子空间。

**结果:** 理论上证明了所提算法在样本点生成和噪声缓解方面的优越性，并通过合成数据和计算金融中的随机模拟问题的数值研究验证了这些结果。

**结论:** 所提出的混合方法在处理高噪声数据时表现出优越的计算效率和样本复杂度，并且可以成功应用于凸约束设置和更复杂的随机场函数逼近问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+least+squares+for+learning+functions+from+highly+noisy+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02215&send_immediately=true&force_search=false)

**原文摘要:** Motivated by the need for efficient estimation of conditional expectations,
we consider a least-squares function approximation problem with heavily
polluted data. Existing methods that are powerful in the small noise regime are
suboptimal when large noise is present. We propose a hybrid approach that
combines Christoffel sampling with certain types of optimal experimental design
to address this issue. We show that the proposed algorithm enjoys appropriate
optimality properties for both sample point generation and noise mollification,
leading to improved computational efficiency and sample complexity compared to
existing methods. We also extend the algorithm to convex-constrained settings
with similar theoretical guarantees. When the target function is defined as the
expectation of a random field, we extend our approach to leverage adaptive
random subspaces and establish results on the approximation capacity of the
adaptive procedure. Our theoretical findings are supported by numerical studies
on both synthetic data and on a more challenging stochastic simulation problem
in computational finance.

</details>


### [98] [Transfer Learning for Matrix Completion](https://arxiv.org/abs/2507.02248)
*Dali Liu, Haolei Weng*

**主要类别:** stat.ML

**AI概要:** 本文探讨了矩阵补全设定下的知识迁移问题，提出了一种利用辅助数据增强低秩目标矩阵估计的迁移学习方法。通过源矩阵与目标矩阵足够接近时，该方法优于传统方法。研究了收敛率并证明了其极小极大最优性。此外，在源数据集相关性未知的情况下，开发了一种有效的检测程序来识别信息源，并建立了其选择一致性。通过模拟和实际数据分析验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 在矩阵补全问题中，如何利用辅助数据来增强低秩目标矩阵的估计是一个重要课题。现有的方法通常仅依赖于单一目标数据，而未充分利用可能相关的辅助数据（即源数据）。因此，探索一种能够有效利用这些辅助数据的迁移学习方法具有重要意义。

**方法:** 1. 提出了一种迁移学习过程，利用先验信息选择有利的源数据集。
2. 研究了该方法的收敛速度，并证明了其极小极大最优性。
3. 借助文献[brailovskaya2024universality]中的尖锐集中不等式消除了收敛率中的对数因子。
4. 在源数据集相关性未知时，设计了一种高效的检测程序以识别信息源，并验证了其选择一致性。

**结果:** - 当源矩阵与目标矩阵足够接近时，所提出的方法在性能上优于传统的仅使用单一目标数据的方法。
- 成功消除了收敛率中的对数因子，进一步提升了方法的理论性能。
- 模拟和实证分析结果表明，所提出的方法在实践中是有效的。

**结论:** 本文提出的迁移学习方法在矩阵补全问题中表现优异，尤其是在源矩阵与目标矩阵相似的情况下。通过消除收敛率中的对数因子，证明了该方法的极小极大最优性。同时，针对源数据集相关性未知的情况，开发的检测程序也展示了良好的选择一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transfer+Learning+for+Matrix+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02248，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02248&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we explore the knowledge transfer under the setting of matrix
completion, which aims to enhance the estimation of a low-rank target matrix
with auxiliary data available. We propose a transfer learning procedure given
prior information on which source datasets are favorable. We study its
convergence rates and prove its minimax optimality. Our analysis reveals that
with the source matrices close enough to the target matrix, out method
outperforms the traditional method using the single target data. In particular,
we leverage the advanced sharp concentration inequalities introduced in
\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the
convergence rate, which is crucial for proving the minimax optimality. When the
relevance of source datasets is unknown, we develop an efficient detection
procedure to identify informative sources and establish its selection
consistency. Simulations and real data analysis are conducted to support the
validity of our methodology.

</details>


### [99] [It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation](https://arxiv.org/abs/2507.02275)
*Jikai Jin, Lester Mackey, Vasilis Syrgkanis*

**主要类别:** stat.ML

**AI概要:** Structure-agnostic因果推理研究如何在给定机器学习估计值的情况下估计处理效果。本文解决了DML估计器在高斯噪声中的最优性问题，并提出新的ACE过程以提高非高斯噪声下的估计准确性。


<details>
  <summary>更多</summary>
  
**动机:** 探讨结构不可知因果推断中，利用机器学习估计干扰函数的效果，特别是在不同治疗噪声分布下的表现。

**方法:** 聚焦部分线性模型，分析双机器学习（DML）估计器的性能，构建具有更高阶鲁棒性的新过程（ACE），并使用结构不可知累积量估计器。

**结果:** 证明DML在高斯治疗噪声下是最优的，但在独立非高斯噪声下次优；ACE过程在特定条件下可实现r阶不敏感。还提供了二元处理的部分线性模型的新型极小极大保证。

**结论:** 提出了更高的估计准确性和鲁棒性方法，并通过合成需求估计实验验证了其实际优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是It%27s+Hard+to+Be+Normal%3A+The+Impact+of+Noise+on+Structure-agnostic+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02275，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02275&send_immediately=true&force_search=false)

**原文摘要:** Structure-agnostic causal inference studies how well one can estimate a
treatment effect given black-box machine learning estimates of nuisance
functions (like the impact of confounders on treatment and outcomes). Here, we
find that the answer depends in a surprising way on the distribution of the
treatment noise. Focusing on the partially linear model of
\citet{robinson1988root}, we first show that the widely adopted double machine
learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,
resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for
independent non-Gaussian treatment noise, we show that DML is always suboptimal
by constructing new practical procedures with higher-order robustness to
nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant
estimators to achieve $r$-th order insensitivity to nuisance errors whenever
the $(r+1)$-st treatment cumulant is non-zero. We complement these core results
with novel minimax guarantees for binary treatments in the partially linear
model. Finally, using synthetic demand estimation experiments, we demonstrate
the practical benefits of our higher-order robust estimators.

</details>


### [100] [Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited](https://arxiv.org/abs/2507.02377)
*Thang D. Bui, Michalis K. Titsias*

**主要类别:** stat.ML

**AI概要:** Inducing-point-based sparse variational Gaussian processes are enhanced by using a block-diagonal scaling matrix, which tightens the variational lower bound and performs better or similarly to existing methods while keeping computational costs comparable. The new Power Expectation Propagation framework with structured posteriors provides flexible alternatives to standard variational approaches.


<details>
  <summary>更多</summary>
  
**动机:** To improve upon recent advances in inducing-point-based sparse variational Gaussian processes by employing a more refined block-diagonal structure for the scaling matrix in order to tighten the variational lower bound.

**方法:** The paper extends the current method by introducing a block-diagonal structure for the scaling matrix in the conditional posterior density given the inducing points. It then revisits the unifying framework of sparse GPs based on Power Expectation Propagation (PEP) and demonstrates how it can benefit from these new structured approximate posteriors.

**结果:** Through regression experiments, the block-diagonal approximation consistently performed as well as or better than existing diagonal approximations while maintaining similar computational costs. The PEP framework with structured posteriors provided competitive performance across various power hyperparameter settings.

**结论:** The block-diagonal scaling matrix improves upon current methods and offers practitioners flexible alternatives to standard variational approaches within the Power Expectation Propagation framework.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparse+Gaussian+Processes%3A+Structured+Approximations+and+Power-EP+Revisited，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.02377，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.02377&send_immediately=true&force_search=false)

**原文摘要:** Inducing-point-based sparse variational Gaussian processes have become the
standard workhorse for scaling up GP models. Recent advances show that these
methods can be improved by introducing a diagonal scaling matrix to the
conditional posterior density given the inducing points. This paper first
considers an extension that employs a block-diagonal structure for the scaling
matrix, provably tightening the variational lower bound. We then revisit the
unifying framework of sparse GPs based on Power Expectation Propagation (PEP)
and show that it can leverage and benefit from the new structured approximate
posteriors. Through extensive regression experiments, we show that the proposed
block-diagonal approximation consistently performs similarly to or better than
existing diagonal approximations while maintaining comparable computational
costs. Furthermore, the new PEP framework with structured posteriors provides
competitive performance across various power hyperparameter settings, offering
practitioners flexible alternatives to standard variational approaches.

</details>
