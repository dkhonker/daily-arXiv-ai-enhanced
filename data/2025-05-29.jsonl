{"id": "2505.21552", "pdf": "https://arxiv.org/pdf/2505.21552", "abs": "https://arxiv.org/abs/2505.21552", "authors": ["Diogo Cruz"], "title": "Understanding the learned look-ahead behavior of chess neural networks", "categories": ["cs.AI", "cs.LG"], "comment": "40 pages, 47 figures", "summary": "We investigate the look-ahead capabilities of chess-playing neural networks,\nspecifically focusing on the Leela Chess Zero policy network. We build on the\nwork of Jenner et al. (2024) by analyzing the model's ability to consider\nfuture moves and alternative sequences beyond the immediate next move. Our\nfindings reveal that the network's look-ahead behavior is highly\ncontext-dependent, varying significantly based on the specific chess position.\nWe demonstrate that the model can process information about board states up to\nseven moves ahead, utilizing similar internal mechanisms across different\nfuture time steps. Additionally, we provide evidence that the network considers\nmultiple possible move sequences rather than focusing on a single line of play.\nThese results offer new insights into the emergence of sophisticated look-ahead\ncapabilities in neural networks trained on strategic tasks, contributing to our\nunderstanding of AI reasoning in complex domains. Our work also showcases the\neffectiveness of interpretability techniques in uncovering cognitive-like\nprocesses in artificial intelligence systems."}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98."}
{"id": "2505.21671", "pdf": "https://arxiv.org/pdf/2505.21671", "abs": "https://arxiv.org/abs/2505.21671", "authors": ["Davin Choo", "Yuqi Pan", "Tonghan Wang", "Milind Tambe", "Alastair van Heerden", "Cheryl Johnson"], "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing", "categories": ["cs.AI", "cs.DS", "cs.LG", "math.OC"], "comment": null, "summary": "We study a sequential decision-making problem on a $n$-node graph $G$ where\neach node has an unknown label from a finite set $\\mathbf{\\Sigma}$, drawn from\na joint distribution $P$ that is Markov with respect to $G$. At each step,\nselecting a node reveals its label and yields a label-dependent reward. The\ngoal is to adaptively choose nodes to maximize expected accumulated discounted\nrewards. We impose a frontier exploration constraint, where actions are limited\nto neighbors of previously selected nodes, reflecting practical constraints in\nsettings such as contact tracing and robotic exploration. We design a Gittins\nindex-based policy that applies to general graphs and is provably optimal when\n$G$ is a forest. Our implementation runs in $O(n^2 \\cdot |\\mathbf{\\Sigma}|^2)$\ntime while using $O(n \\cdot |\\mathbf{\\Sigma}|^2)$ oracle calls to $P$ and\n$O(n^2 \\cdot |\\mathbf{\\Sigma}|)$ space. Experiments on synthetic and real-world\ngraphs show that our method consistently outperforms natural baselines,\nincluding in non-tree, budget-limited, and undiscounted settings. For example,\nin HIV testing simulations on real-world sexual interaction networks, our\npolicy detects nearly all positive cases with only half the population tested,\nsubstantially outperforming other baselines."}
{"id": "2505.21674", "pdf": "https://arxiv.org/pdf/2505.21674", "abs": "https://arxiv.org/abs/2505.21674", "authors": ["Michael Katz", "Harsha Kokel", "Christian Muise", "Shirin Sohrabi", "Sarath Sreedharan"], "title": "Make Planning Research Rigorous Again!", "categories": ["cs.AI"], "comment": null, "summary": "In over sixty years since its inception, the field of planning has made\nsignificant contributions to both the theory and practice of building planning\nsoftware that can solve a never-before-seen planning problem. This was done\nthrough established practices of rigorous design and evaluation of planning\nsystems. It is our position that this rigor should be applied to the current\ntrend of work on planning with large language models. One way to do so is by\ncorrectly incorporating the insights, tools, and data from the automated\nplanning community into the design and evaluation of LLM-based planners. The\nexperience and expertise of the planning community are not just important from\na historical perspective; the lessons learned could play a crucial role in\naccelerating the development of LLM-based planners. This position is\nparticularly important in light of the abundance of recent works that replicate\nand propagate the same pitfalls that the planning community has encountered and\nlearned from. We believe that avoiding such known pitfalls will contribute\ngreatly to the progress in building LLM-based planners and to planning in\ngeneral."}
{"id": "2505.21580", "pdf": "https://arxiv.org/pdf/2505.21580", "abs": "https://arxiv.org/abs/2505.21580", "authors": ["Anum Fatima", "Gesine Reinert"], "title": "A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "43 pages, 24 figures", "summary": "Complex data are often represented as a graph, which in turn can often be\nviewed as a realisation of a random graph, such as of an inhomogeneous random\ngraph model (IRG). For general fast goodness-of-fit tests in high dimensions,\nkernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,\ntest, and analyse a KSD-type goodness-of-fit test for IRG models that can be\ncarried out with a single observation of the network. The test is applicable to\na network of any size and does not depend on the asymptotic distribution of the\ntest statistic. We also provide theoretical guarantees."}
{"id": "2505.21765", "pdf": "https://arxiv.org/pdf/2505.21765", "abs": "https://arxiv.org/abs/2505.21765", "authors": ["Sohyun An", "Ruochen Wang", "Tianyi Zhou", "Cho-Jui Hsieh"], "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "categories": ["cs.AI"], "comment": "Work In Progress", "summary": "While recent success of large reasoning models (LRMs) significantly advanced\nLLMs' reasoning capability by optimizing the final answer accuracy using\nreinforcement learning, they may also drastically increase the output length\ndue to overthinking, characterized by unnecessarily complex reasoning paths\nthat waste computation and potentially degrade the performance. We hypothesize\nthat such inefficiencies stem from LRMs' limited capability to dynamically\nselect the proper modular reasoning strategies, termed thinking patterns at the\nright position. To investigate this hypothesis, we propose a dynamic\noptimization framework that segments model-generated reasoning paths into\ndistinct thinking patterns, systematically identifying and promoting beneficial\npatterns that improve the answer while removing detrimental ones. Empirical\nanalysis confirms that our optimized thinking paths yield more concise yet\nsufficiently informative trajectories, enhancing reasoning efficiency by\nreducing attention FLOPs by up to 47% while maintaining accuracy for originally\ncorrect responses. Moreover, a non-trivial portion of originally incorrect\nresponses are transformed into correct ones, achieving a 15.6% accuracy\nimprovement with reduced length. Motivated by the improvement brought by the\noptimized thinking paths, we apply a preference optimization technique\nsupported by a pairwise dataset contrasting suboptimal and optimal reasoning\npaths. Experimental evaluations across multiple mathematical reasoning\nbenchmarks reveal that our method notably reduces computational overhead while\nsimultaneously improving reasoning accuracy, achieving up to a 12% accuracy\nimprovement and reducing token usage from approximately 5,000 to 3,000 tokens."}
{"id": "2505.21658", "pdf": "https://arxiv.org/pdf/2505.21658", "abs": "https://arxiv.org/abs/2505.21658", "authors": ["Brandon R. Feng", "David Keetae Park", "Xihaier Luo", "Arantxa Urdangarin", "Shinjae Yoo", "Brian J. Reich"], "title": "STACI: Spatio-Temporal Aleatoric Conformal Inference", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty\nquantification for estimation of spatio-temporal fields. Spatio-temporal deep\nlearning models, while scalable, typically assume a simplistic independent\ncovariance matrix for the response, failing to capture the underlying\ncorrelation structure. However, spatio-temporal GPs suffer from issues of\nscalability and various forms of approximation bias resulting from restrictive\nassumptions of the covariance kernel function. We propose STACI, a novel\nframework consisting of a variational Bayesian neural network approximation of\nnon-stationary spatio-temporal GP along with a novel spatio-temporal conformal\ninference algorithm. STACI is highly scalable, taking advantage of GPU training\ncapabilities for neural network models, and provides statistically valid\nprediction intervals for uncertainty quantification. STACI outperforms\ncompeting GPs and deep methods in accurately approximating spatio-temporal\nprocesses and we show it easily scales to datasets with millions of\nobservations."}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE"}
{"id": "2505.21721", "pdf": "https://arxiv.org/pdf/2505.21721", "abs": "https://arxiv.org/abs/2505.21721", "authors": ["Kyurae Kim", "Yi-An Ma", "Trevor Campbell", "Jacob R. Gardner"], "title": "Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.CO"], "comment": null, "summary": "We prove that, given a mean-field location-scale variational family,\nblack-box variational inference (BBVI) with the reparametrization gradient\nconverges at an almost dimension-independent rate. Specifically, for strongly\nlog-concave and log-smooth targets, the number of iterations for BBVI with a\nsub-Gaussian family to achieve an objective $\\epsilon$-close to the global\noptimum is $\\mathrm{O}(\\log d)$, which improves over the $\\mathrm{O}(d)$\ndependence of full-rank location-scale families. For heavy-tailed families, we\nprovide a weaker $\\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the\nnumber of finite moments. Additionally, if the Hessian of the target\nlog-density is constant, the complexity is free of any explicit dimension\ndependence. We also prove that our bound on the gradient variance, which is key\nto our result, cannot be improved using only spectral bounds on the Hessian of\nthe target log-density."}
{"id": "2505.21828", "pdf": "https://arxiv.org/pdf/2505.21828", "abs": "https://arxiv.org/abs/2505.21828", "authors": ["Chen Yueh-Han", "Guy Davidson", "Brenden M. Lake"], "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "categories": ["cs.AI"], "comment": null, "summary": "Do LLMs robustly generalize critical safety facts to novel situations?\nLacking this ability is dangerous when users ask naive questions. For instance,\n\"I'm considering packing melon balls for my 10-month-old's lunch. What other\nfoods would be good to include?\" Before offering food options, the LLM should\nwarn that melon balls pose a choking hazard to toddlers, as documented by the\nCDC. Failing to provide such warnings could result in serious injuries or even\ndeath. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic\nGEneralization evaluation, the first benchmark that tests whether LLMs properly\napply well established safety facts to naive user queries. SAGE-Eval comprises\n104 facts manually sourced from reputable organizations, systematically\naugmented to create 10,428 test scenarios across 7 common domains (e.g.,\nOutdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,\npasses only 58% of all the safety facts tested. We also observe that model\ncapabilities and training compute weakly correlate with performance on\nSAGE-Eval, implying that scaling up is not the golden solution. Our findings\nsuggest frontier LLMs still lack robust generalization ability. We recommend\ndevelopers use SAGE-Eval in pre-deployment evaluations to assess model\nreliability in addressing salient risks. We publicly release SAGE-Eval at\nhttps://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available\nat https://github.com/YuehHanChen/SAGE-Eval/tree/main."}
{"id": "2505.21791", "pdf": "https://arxiv.org/pdf/2505.21791", "abs": "https://arxiv.org/abs/2505.21791", "authors": ["Julia Nakhleh", "Robert D. Nowak"], "title": "Global Minimizers of $\\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Overparameterized neural networks can interpolate a given dataset in many\ndifferent ways, prompting the fundamental question: which among these solutions\nshould we prefer, and what explicit regularization strategies will provably\nyield these solutions? This paper addresses the challenge of finding the\nsparsest interpolating ReLU network -- i.e., the network with the fewest\nnonzero parameters or neurons -- a goal with wide-ranging implications for\nefficiency, generalization, interpretability, theory, and model compression.\nUnlike post hoc pruning approaches, we propose a continuous, almost-everywhere\ndifferentiable training objective whose global minima are guaranteed to\ncorrespond to the sparsest single-hidden-layer ReLU networks that fit the data.\nThis result marks a conceptual advance: it recasts the combinatorial problem of\nsparse interpolation as a smooth optimization task, potentially enabling the\nuse of gradient-based training methods. Our objective is based on minimizing\n$\\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical\nsparsity-promoting strategy in finite-dimensional settings. However, applying\nthese ideas to neural networks presents new challenges: the function class is\ninfinite-dimensional, and the weights are learned using a highly nonconvex\nobjective. We prove that, under our formulation, global minimizers correspond\nexactly to sparsest solutions. Our work lays a foundation for understanding\nwhen and how continuous sparsity-inducing objectives can be leveraged to\nrecover sparse networks through training."}
{"id": "2505.21887", "pdf": "https://arxiv.org/pdf/2505.21887", "abs": "https://arxiv.org/abs/2505.21887", "authors": ["Ahmed Heakl", "Yahia Salaheldin Shaaban", "Martin Takac", "Salem Lahlou", "Zangir Iklassov"], "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": "18 pages, 14 figures, 11 tables", "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty."}
{"id": "2505.21796", "pdf": "https://arxiv.org/pdf/2505.21796", "abs": "https://arxiv.org/abs/2505.21796", "authors": ["Sajad Khodadadian", "Martin Zubeldia"], "title": "A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "37 pages", "summary": "Polyak-Ruppert averaging is a widely used technique to achieve the optimal\nasymptotic variance of stochastic approximation (SA) algorithms, yet its\nhigh-probability performance guarantees remain underexplored in general\nsettings. In this paper, we present a general framework for establishing\nnon-asymptotic concentration bounds for the error of averaged SA iterates. Our\napproach assumes access to individual concentration bounds for the unaveraged\niterates and yields a sharp bound on the averaged iterates. We also construct\nan example, showing the tightness of our result up to constant multiplicative\nfactors. As direct applications, we derive tight concentration bounds for\ncontractive SA algorithms and for algorithms such as temporal difference\nlearning and Q-learning with averaging, obtaining new bounds in settings where\ntraditional analysis is challenging."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907", "abs": "https://arxiv.org/abs/2505.21907", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design."}
{"id": "2505.21845", "pdf": "https://arxiv.org/pdf/2505.21845", "abs": "https://arxiv.org/abs/2505.21845", "authors": ["Lingfei Zhao", "Hadeel Soliman", "Kevin S. Xu", "Subhadeep Paul"], "title": "Spectral clustering for dependent community Hawkes process models of temporal networks", "categories": ["stat.ML", "cs.LG", "cs.SI", "stat.ME"], "comment": null, "summary": "Temporal networks observed continuously over time through timestamped\nrelational events data are commonly encountered in application settings\nincluding online social media communications, financial transactions, and\ninternational relations. Temporal networks often exhibit community structure\nand strong dependence patterns among node pairs. This dependence can be modeled\nthrough mutual excitations, where an interaction event from a sender to a\nreceiver node increases the possibility of future events among other node\npairs.\n  We provide statistical results for a class of models that we call dependent\ncommunity Hawkes (DCH) models, which combine the stochastic block model with\nmutually exciting Hawkes processes for modeling both community structure and\ndependence among node pairs, respectively. We derive a non-asymptotic upper\nbound on the misclustering error of spectral clustering on the event count\nmatrix as a function of the number of nodes and communities, time duration, and\nthe amount of dependence in the model. Our result leverages recent results on\nbounding an appropriate distance between a multivariate Hawkes process count\nvector and a Gaussian vector, along with results from random matrix theory. We\nalso propose a DCH model that incorporates only self and reciprocal excitation\nalong with highly scalable parameter estimation using a Generalized Method of\nMoments (GMM) estimator that we demonstrate to be consistent for growing\nnetwork size and time duration."}
{"id": "2505.21935", "pdf": "https://arxiv.org/pdf/2505.21935", "abs": "https://arxiv.org/abs/2505.21935", "authors": ["Kaiyu He", "Zhiyu Chen"], "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Since the advent of Large Language Models (LLMs), efforts have largely\nfocused on improving their instruction-following and deductive reasoning\nabilities, leaving open the question of whether these models can truly discover\nnew knowledge. In pursuit of artificial general intelligence (AGI), there is a\ngrowing need for models that not only execute commands or retrieve information\nbut also learn, reason, and generate new knowledge by formulating novel\nhypotheses and theories that deepen our understanding of the world. Guided by\nPeirce's framework of abduction, deduction, and induction, this survey offers a\nstructured lens to examine LLM-based hypothesis discovery. We synthesize\nexisting work in hypothesis generation, application, and validation,\nidentifying both key achievements and critical gaps. By unifying these threads,\nwe illuminate how LLMs might evolve from mere ``information executors'' into\nengines of genuine innovation, potentially transforming research, science, and\nreal-world problem solving."}
{"id": "2505.21892", "pdf": "https://arxiv.org/pdf/2505.21892", "abs": "https://arxiv.org/abs/2505.21892", "authors": ["Xunpeng Huang", "Yingyu Lin", "Nikki Lijing Kuang", "Hanze Dong", "Difan Zou", "Yian Ma", "Tong Zhang"], "title": "Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion", "categories": ["stat.ML", "cs.LG"], "comment": "37 pages, 3 figures, 3 tables", "summary": "Continuous diffusion models have demonstrated remarkable performance in data\ngeneration across various domains, yet their efficiency remains constrained by\ntwo critical limitations: (1) the local adjacency structure of the forward\nMarkov process, which restricts long-range transitions in the data space, and\n(2) inherent biases introduced during the simulation of time-inhomogeneous\nreverse denoising processes. To address these challenges, we propose Quantized\nTransition Diffusion (QTD), a novel approach that integrates data quantization\nwith discrete diffusion dynamics. Our method first transforms the continuous\ndata distribution $p_*$ into a discrete one $q_*$ via histogram approximation\nand binary encoding, enabling efficient representation in a structured discrete\nlatent space. We then design a continuous-time Markov chain (CTMC) with Hamming\ndistance-based transitions as the forward process, which inherently supports\nlong-range movements in the original data space. For reverse-time sampling, we\nintroduce a \\textit{truncated uniformization} technique to simulate the reverse\nCTMC, which can provably provide unbiased generation from $q_*$ under minimal\nscore assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we\nprove that QTD can generate samples with $O(d\\ln^2(d/\\epsilon))$ score\nevaluations in expectation to approximate the $d$--dimensional target\ndistribution $p_*$ within an $\\epsilon$ error tolerance. Our method not only\nestablishes state-of-the-art inference efficiency but also advances the\ntheoretical foundations of diffusion-based generative modeling by unifying\ndiscrete and continuous diffusion paradigms."}
{"id": "2505.21988", "pdf": "https://arxiv.org/pdf/2505.21988", "abs": "https://arxiv.org/abs/2505.21988", "authors": ["Ziyang Zheng", "Kezhi Li", "Zhengyuan Shi", "Qiang Xu"], "title": "Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism", "categories": ["cs.AI"], "comment": null, "summary": "Subgraph matching in logic circuits is foundational for numerous Electronic\nDesign Automation (EDA) applications, including datapath optimization,\narithmetic verification, and hardware trojan detection. However, existing\ntechniques rely primarily on structural graph isomorphism and thus fail to\nidentify function-related subgraphs when synthesis transformations\nsubstantially alter circuit topology. To overcome this critical limitation, we\nintroduce the concept of functional subgraph matching, a novel approach that\nidentifies whether a given logic function is implicitly present within a larger\ncircuit, irrespective of structural variations induced by synthesis or\ntechnology mapping. Specifically, we propose a two-stage multi-modal framework:\n(1) learning robust functional embeddings across AIG and post-mapping netlists\nfor functional subgraph detection, and (2) identifying fuzzy boundaries using a\ngraph segmentation approach. Evaluations on standard benchmarks (ITC99,\nOpenABCD, ForgeEDA) demonstrate significant performance improvements over\nexisting structural methods, with average $93.8\\%$ accuracy in functional\nsubgraph detection and a dice score of $91.3\\%$ in fuzzy boundary\nidentification."}
{"id": "2505.21932", "pdf": "https://arxiv.org/pdf/2505.21932", "abs": "https://arxiv.org/abs/2505.21932", "authors": ["Adriana L. Duncan", "Joe Kileel"], "title": "Higher-Order Group Synchronization", "categories": ["stat.ML", "cs.CV", "cs.LG", "math.CO", "math.OC"], "comment": "40 pages", "summary": "Group synchronization is the problem of determining reliable global estimates\nfrom noisy local measurements on networks. The typical task for group\nsynchronization is to assign elements of a group to the nodes of a graph in a\nway that respects group elements given on the edges which encode information\nabout local pairwise relationships between the nodes. In this paper, we\nintroduce a novel higher-order group synchronization problem which operates on\na hypergraph and seeks to synchronize higher-order local measurements on the\nhyperedges to obtain global estimates on the nodes. Higher-order group\nsynchronization is motivated by applications to computer vision and image\nprocessing, among other computational problems. First, we define the problem of\nhigher-order group synchronization and discuss its mathematical foundations.\nSpecifically, we give necessary and sufficient synchronizability conditions\nwhich establish the importance of cycle consistency in higher-order group\nsynchronization. Then, we propose the first computational framework for general\nhigher-order group synchronization; it acts globally and directly on\nhigher-order measurements using a message passing algorithm. We discuss\ntheoretical guarantees for our framework, including convergence analyses under\noutliers and noise. Finally, we show potential advantages of our method through\nnumerical experiments. In particular, we show that in certain cases our\nhigher-order method applied to rotational and angular synchronization\noutperforms standard pairwise synchronization methods and is more robust to\noutliers. We also show that our method has comparable performance on simulated\ncryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM\nreconstruction package."}
{"id": "2505.22006", "pdf": "https://arxiv.org/pdf/2505.22006", "abs": "https://arxiv.org/abs/2505.22006", "authors": ["Changze Qiao", "Mingming Lu"], "title": "Efficiently Enhancing General Agents With Hierarchical-categorical Memory", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "With large language models (LLMs) demonstrating remarkable capabilities,\nthere has been a surge in research on leveraging LLMs to build general-purpose\nmulti-modal agents. However, existing approaches either rely on computationally\nexpensive end-to-end training using large-scale multi-modal data or adopt\ntool-use methods that lack the ability to continuously learn and adapt to new\nenvironments. In this paper, we introduce EHC, a general agent capable of\nlearning without parameter updates. EHC consists of a Hierarchical Memory\nRetrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)\nmodule. The HMR module facilitates rapid retrieval of relevant memories and\ncontinuously stores new information without being constrained by memory\ncapacity. The TOEL module enhances the agent's comprehension of various task\ncharacteristics by classifying experiences and extracting patterns across\ndifferent categories. Extensive experiments conducted on multiple standard\ndatasets demonstrate that EHC outperforms existing methods, achieving\nstate-of-the-art performance and underscoring its effectiveness as a general\nagent for handling complex multi-modal tasks."}
{"id": "2505.22048", "pdf": "https://arxiv.org/pdf/2505.22048", "abs": "https://arxiv.org/abs/2505.22048", "authors": ["Haihan Zhang", "Weicheng Lin", "Yuanshi Liu", "Cong Fang"], "title": "Learning Curves of Stochastic Gradient Descent in Kernel Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper considers a canonical problem in kernel regression: how good are\nthe model performances when it is trained by the popular online first-order\nalgorithms, compared to the offline ones, such as ridge and ridgeless\nregression? In this paper, we analyze the foundational single-pass Stochastic\nGradient Descent (SGD) in kernel regression under source condition where the\noptimal predictor can even not belong to the RKHS, i.e. the model is\nmisspecified. Specifically, we focus on the inner product kernel over the\nsphere and characterize the exact orders of the excess risk curves under\ndifferent scales of sample sizes $n$ concerning the input dimension $d$.\nSurprisingly, we show that SGD achieves min-max optimal rates up to constants\namong all the scales, without suffering the saturation, a prevalent phenomenon\nobserved in (ridge) regression, except when the model is highly misspecified\nand the learning is in a final stage where $n\\gg d^{\\gamma}$ with any constant\n$\\gamma >0$. The main reason for SGD to overcome the curse of saturation is the\nexponentially decaying step size schedule, a common practice in deep neural\nnetwork training. As a byproduct, we provide the \\emph{first} provable\nadvantage of the scheme over the iterative averaging method in the common\nsetting."}
{"id": "2505.22050", "pdf": "https://arxiv.org/pdf/2505.22050", "abs": "https://arxiv.org/abs/2505.22050", "authors": ["Di Wu", "Jiaxin Fan", "Junzhe Zang", "Guanbo Wang", "Wei Yin", "Wenhao Li", "Bo Jin"], "title": "Reinforced Reasoning for Embodied Planning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Embodied planning requires agents to make coherent multi-step decisions based\non dynamic visual observations and natural language goals. While recent\nvision-language models (VLMs) excel at static perception tasks, they struggle\nwith the temporal reasoning, spatial understanding, and commonsense grounding\nneeded for planning in interactive environments. In this work, we introduce a\nreinforcement fine-tuning framework that brings R1-style reasoning enhancement\ninto embodied planning. We first distill a high-quality dataset from a powerful\nclosed-source model and perform supervised fine-tuning (SFT) to equip the model\nwith structured decision-making priors. We then design a rule-based reward\nfunction tailored to multi-step action quality and optimize the policy via\nGeneralized Reinforced Preference Optimization (GRPO). Our approach is\nevaluated on Embench, a recent benchmark for interactive embodied tasks,\ncovering both in-domain and out-of-domain scenarios. Experimental results show\nthat our method significantly outperforms models of similar or larger scale,\nincluding GPT-4o-mini and 70B+ open-source baselines, and exhibits strong\ngeneralization to unseen environments. This work highlights the potential of\nreinforcement-driven reasoning to advance long-horizon planning in embodied AI."}
{"id": "2505.22326", "pdf": "https://arxiv.org/pdf/2505.22326", "abs": "https://arxiv.org/abs/2505.22326", "authors": ["James M. Adams", "Gesine Reinert", "Lukasz Szpruch", "Carsten Maple", "Andrew Elliott"], "title": "Individualised Counterfactual Examples Using Conformal Prediction Intervals", "categories": ["stat.ML", "cs.LG"], "comment": "Submitted to Conformal and Probabilistic Predictions With\n  Applications (COPA) 2025", "summary": "Counterfactual explanations for black-box models aim to pr ovide insight into\nan algorithmic decision to its recipient. For a binary classification problem\nan individual counterfactual details which features might be changed for the\nmodel to infer the opposite class. High-dimensional feature spaces that are\ntypical of machine learning classification models admit many possible\ncounterfactual examples to a decision, and so it is important to identify\nadditional criteria to select the most useful counterfactuals. In this paper,\nwe explore the idea that the counterfactuals should be maximally informative\nwhen considering the knowledge of a specific individual about the underlying\nclassifier. To quantify this information gain we explicitly model the knowledge\nof the individual, and assess the uncertainty of predictions which the\nindividual makes by the width of a conformal prediction interval. Regions of\nfeature space where the prediction interval is wide correspond to areas where\nthe confidence in decision making is low, and an additional counterfactual\nexample might be more informative to an individual. To explore and evaluate our\nindividualised conformal prediction interval counterfactuals (CPICFs), first we\npresent a synthetic data set on a hypercube which allows us to fully visualise\nthe decision boundary, conformal intervals via three different methods, and\nresultant CPICFs. Second, in this synthetic data set we explore the impact of a\nsingle CPICF on the knowledge of an individual locally around the original\nquery. Finally, in both our synthetic data set and a complex real world dataset\nwith a combination of continuous and discrete variables, we measure the utility\nof these counterfactuals via data augmentation, testing the performance on a\nheld out set."}
{"id": "2505.22087", "pdf": "https://arxiv.org/pdf/2505.22087", "abs": "https://arxiv.org/abs/2505.22087", "authors": ["Ruxiao Chen", "Dezheng Han", "Wenjie Han", "Shuaishuai Guo"], "title": "Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired", "categories": ["cs.AI"], "comment": null, "summary": "Assistive systems for visually impaired individuals must deliver rapid,\ninterpretable, and adaptive feedback to facilitate real-time navigation.\nCurrent approaches face a trade-off between latency and semantic richness:\nnatural language-based systems provide detailed guidance but are too slow for\ndynamic scenarios, while emergent communication frameworks offer low-latency\nsymbolic languages but lack semantic depth, limiting their utility in tactile\nmodalities like vibration. To address these limitations, we introduce a novel\nframework, Cognitively-Inspired Emergent Communication via Knowledge Graphs\n(VAG-EC), which emulates human visual perception and cognitive mapping. Our\nmethod constructs knowledge graphs to represent objects and their\nrelationships, incorporating attention mechanisms to prioritize task-relevant\nentities, thereby mirroring human selective attention. This structured approach\nenables the emergence of compact, interpretable, and context-sensitive symbolic\nlanguages. Extensive experiments across varying vocabulary sizes and message\nlengths demonstrate that VAG-EC outperforms traditional emergent communication\nmethods in Topographic Similarity (TopSim) and Context Independence (CI). These\nfindings underscore the potential of cognitively grounded emergent\ncommunication as a fast, adaptive, and human-aligned solution for real-time\nassistive technologies. Code is available at\nhttps://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main."}
{"id": "2505.22332", "pdf": "https://arxiv.org/pdf/2505.22332", "abs": "https://arxiv.org/abs/2505.22332", "authors": ["Timo Löhr", "Paul Hofman", "Felix Mohr", "Eyke Hüllermeier"], "title": "Credal Prediction based on Relative Likelihood", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Predictions in the form of sets of probability distributions, so-called\ncredal sets, provide a suitable means to represent a learner's epistemic\nuncertainty. In this paper, we propose a theoretically grounded approach to\ncredal prediction based on the statistical notion of relative likelihood: The\ntarget of prediction is the set of all (conditional) probability distributions\nproduced by the collection of plausible models, namely those models whose\nrelative likelihood exceeds a specified threshold. This threshold has an\nintuitive interpretation and allows for controlling the trade-off between\ncorrectness and precision of credal predictions. We tackle the problem of\napproximating credal sets defined in this way by means of suitably modified\nensemble learning techniques. To validate our approach, we illustrate its\neffectiveness by experiments on benchmark datasets demonstrating superior\nuncertainty representation without compromising predictive performance. We also\ncompare our method against several state-of-the-art baselines in credal\nprediction."}
{"id": "2505.22092", "pdf": "https://arxiv.org/pdf/2505.22092", "abs": "https://arxiv.org/abs/2505.22092", "authors": ["Valentin Cuzin-Rambaud", "Emilien Komlenovic", "Alexandre Faure", "Bruno Yun"], "title": "VIRAL: Vision-grounded Integration for Reward design And Learning", "categories": ["cs.AI"], "comment": null, "summary": "The alignment between humans and machines is a critical challenge in\nartificial intelligence today. Reinforcement learning, which aims to maximize a\nreward function, is particularly vulnerable to the risks associated with poorly\ndesigned reward functions. Recent advancements has shown that Large Language\nModels (LLMs) for reward generation can outperform human performance in this\ncontext. We introduce VIRAL, a pipeline for generating and refining reward\nfunctions through the use of multi-modal LLMs. VIRAL autonomously creates and\ninteractively improves reward functions based on a given environment and a goal\nprompt or annotated image. The refinement process can incorporate human\nfeedback or be guided by a description generated by a video LLM, which explains\nthe agent's policy in video form. We evaluated VIRAL in five Gymnasium\nenvironments, demonstrating that it accelerates the learning of new behaviors\nwhile ensuring improved alignment with user intent. The source-code and demo\nvideo are available at: https://github.com/VIRAL-UCBL1/VIRAL and\nhttps://youtu.be/t4_BXugBm9Q."}
{"id": "2505.22364", "pdf": "https://arxiv.org/pdf/2505.22364", "abs": "https://arxiv.org/abs/2505.22364", "authors": ["Gabriele Visentin", "Patrick Cheridito"], "title": "Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows", "categories": ["stat.ML", "cs.LG", "65K99 (Primary) 68T07, 68T99 (Secondary)"], "comment": null, "summary": "We present a novel method for efficiently computing optimal transport maps\nand Wasserstein barycenters in high-dimensional spaces. Our approach uses\nconditional normalizing flows to approximate the input distributions as\ninvertible pushforward transformations from a common latent space. This makes\nit possible to directly solve the primal problem using gradient-based\nminimization of the transport cost, unlike previous methods that rely on dual\nformulations and complex adversarial optimization. We show how this approach\ncan be extended to compute Wasserstein barycenters by solving a conditional\nvariance minimization problem. A key advantage of our conditional architecture\nis that it enables the computation of barycenters for hundreds of input\ndistributions, which was computationally infeasible with previous methods. Our\nnumerical experiments illustrate that our approach yields accurate results\nacross various high-dimensional tasks and compares favorably with previous\nstate-of-the-art methods."}
{"id": "2505.22104", "pdf": "https://arxiv.org/pdf/2505.22104", "abs": "https://arxiv.org/abs/2505.22104", "authors": ["Davide Corsi", "Kaushik Mallik", "Andoni Rodriguez", "Cesar Sanchez"], "title": "Efficient Dynamic Shielding for Parametric Safety Specifications", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Shielding has emerged as a promising approach for ensuring safety of\nAI-controlled autonomous systems. The algorithmic goal is to compute a shield,\nwhich is a runtime safety enforcement tool that needs to monitor and intervene\nthe AI controller's actions if safety could be compromised otherwise.\nTraditional shields are designed statically for a specific safety requirement.\nTherefore, if the safety requirement changes at runtime due to changing\noperating conditions, the shield needs to be recomputed from scratch, causing\ndelays that could be fatal. We introduce dynamic shields for parametric safety\nspecifications, which are succinctly represented sets of all possible safety\nspecifications that may be encountered at runtime. Our dynamic shields are\nstatically designed for a given safety parameter set, and are able to\ndynamically adapt as the true safety specification (permissible by the\nparameters) is revealed at runtime. The main algorithmic novelty lies in the\ndynamic adaptation procedure, which is a simple and fast algorithm that\nutilizes known features of standard safety shields, like maximal\npermissiveness. We report experimental results for a robot navigation problem\nin unknown territories, where the safety specification evolves as new obstacles\nare discovered at runtime. In our experiments, the dynamic shields took a few\nminutes for their offline design, and took between a fraction of a second and a\nfew seconds for online adaptation at each step, whereas the brute-force online\nrecomputation approach was up to 5 times slower."}
{"id": "2505.22481", "pdf": "https://arxiv.org/pdf/2505.22481", "abs": "https://arxiv.org/abs/2505.22481", "authors": ["Yiming Xi", "Konstantinos Zygalakis", "Marcelo Pereyra"], "title": "Hypothesis Testing in Imaging Inverse Problems", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper proposes a framework for semantic hypothesis testing tailored to\nimaging inverse problems. Modern imaging methods struggle to support hypothesis\ntesting, a core component of the scientific method that is essential for the\nrigorous interpretation of experiments and robust interfacing with\ndecision-making processes. There are three main reasons why image-based\nhypothesis testing is challenging. First, the difficulty of using a single\nobservation to simultaneously reconstruct an image, formulate hypotheses, and\nquantify their statistical significance. Second, the hypotheses encountered in\nimaging are mostly of semantic nature, rather than quantitative statements\nabout pixel values. Third, it is challenging to control test error\nprobabilities because the null and alternative distributions are often unknown.\nOur proposed approach addresses these difficulties by leveraging concepts from\nself-supervised computational imaging, vision-language models, and\nnon-parametric hypothesis testing with e-values. We demonstrate our proposed\nframework through numerical experiments related to image-based phenotyping,\nwhere we achieve excellent power while robustly controlling Type I errors."}
{"id": "2505.22112", "pdf": "https://arxiv.org/pdf/2505.22112", "abs": "https://arxiv.org/abs/2505.22112", "authors": ["Guangfu Hao", "Frederic Alexandre", "Shan Yu"], "title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "Cognitive flexibility has been extensively studied in human cognition but\nremains relatively unexplored in the context of Visual Large Language Models\n(VLLMs). This study assesses the cognitive flexibility of state-of-the-art\nVLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card\nSorting Test (WCST), a classic measure of set-shifting ability. Our results\nreveal that VLLMs achieve or surpass human-level set-shifting capabilities\nunder chain-of-thought prompting with text-based inputs. However, their\nabilities are highly influenced by both input modality and prompting strategy.\nIn addition, we find that through role-playing, VLLMs can simulate various\nfunctional deficits aligned with patients having impairments in cognitive\nflexibility, suggesting that VLLMs may possess a cognitive architecture, at\nleast regarding the ability of set-shifting, similar to the brain. This study\nreveals the fact that VLLMs have already approached the human level on a key\ncomponent underlying our higher cognition, and highlights the potential to use\nthem to emulate complex brain processes."}
{"id": "2505.22518", "pdf": "https://arxiv.org/pdf/2505.22518", "abs": "https://arxiv.org/abs/2505.22518", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "title": "IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62F10, 68T07, 62-08"], "comment": "Under review", "summary": "Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications."}
{"id": "2505.22147", "pdf": "https://arxiv.org/pdf/2505.22147", "abs": "https://arxiv.org/abs/2505.22147", "authors": ["Florian Andreas Marwitz", "Tanya Braun", "Ralf Möller", "Marcel Gehrke"], "title": "Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions", "categories": ["cs.AI"], "comment": null, "summary": "Decision making is a central problem in AI that can be formalized using a\nMarkov Decision Process. A problem is that, with increasing numbers of\n(indistinguishable) objects, the state space grows exponentially. To compute\npolicies, the state space has to be enumerated. Even more possibilities have to\nbe enumerated if the size of the action space depends on the size of the state\nspace, especially if we allow concurrent actions. To tackle the exponential\nblow-up in the action and state space, we present a first-order representation\nto store the spaces in polynomial instead of exponential size in the number of\nobjects and introduce Foreplan, a relational forward planner, which uses this\nrepresentation to efficiently compute policies for numerous indistinguishable\nobjects and actions. Additionally, we introduce an even faster approximate\nversion of Foreplan. Moreover, Foreplan identifies how many objects an agent\nshould act on to achieve a certain task given restrictions. Further, we provide\na theoretical analysis and an empirical evaluation of Foreplan, demonstrating a\nspeedup of at least four orders of magnitude."}
{"id": "2505.22527", "pdf": "https://arxiv.org/pdf/2505.22527", "abs": "https://arxiv.org/abs/2505.22527", "authors": ["Agnideep Aich", "Ashit Aich", "Bruce Wade"], "title": "Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling", "categories": ["stat.ML", "cs.LG", "68T07, 37J39, 65P10, 62B10, 53D22, 94A17"], "comment": "Submitted", "summary": "We introduce the Symplectic Generative Network (SGN), a deep generative model\nthat leverages Hamiltonian mechanics to construct an invertible,\nvolume-preserving mapping between a latent space and the data space. By\nendowing the latent space with a symplectic structure and modeling data\ngeneration as the time evolution of a Hamiltonian system, SGN achieves exact\nlikelihood evaluation without incurring the computational overhead of Jacobian\ndeterminant calculations. In this work, we provide a rigorous mathematical\nfoundation for SGNs through a comprehensive theoretical framework that\nincludes: (i) complete proofs of invertibility and volume preservation, (ii) a\nformal complexity analysis with theoretical comparisons to Variational\nAutoencoders and Normalizing Flows, (iii) strengthened universal approximation\nresults with quantitative error bounds, (iv) an information-theoretic analysis\nbased on the geometry of statistical manifolds, and (v) an extensive stability\nanalysis with adaptive integration guarantees. These contributions highlight\nthe fundamental advantages of SGNs and establish a solid foundation for future\nempirical investigations and applications to complex, high-dimensional data."}
{"id": "2505.22148", "pdf": "https://arxiv.org/pdf/2505.22148", "abs": "https://arxiv.org/abs/2505.22148", "authors": ["Gangwei Jiang", "Yahui Liu", "Zhaoyi Li", "Qi Wang", "Fuzheng Zhang", "Linqi Song", "Ying Wei", "Defu Lian"], "title": "What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have\npopularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate\nand step-by-step reasoning before producing a final answer. While LCoTs have\nenabled expert-level performance in complex tasks, how the internal structures\nof their reasoning chains drive, or even predict, the correctness of final\nanswers remains a critical yet underexplored question. In this work, we present\nLCoT2Tree, an automated framework that converts sequential LCoTs into\nhierarchical tree structures and thus enables deeper structural analysis of LLM\nreasoning. Using graph neural networks (GNNs), we reveal that structural\npatterns extracted by LCoT2Tree, including exploration, backtracking, and\nverification, serve as stronger predictors of final performance across a wide\nrange of tasks and models. Leveraging an explainability technique, we further\nidentify critical thought patterns such as over-branching that account for\nfailures. Beyond diagnostic insights, the structural patterns by LCoT2Tree\nsupport practical applications, including improving Best-of-N decoding\neffectiveness. Overall, our results underscore the critical role of internal\nstructures of reasoning chains, positioning LCoT2Tree as a powerful tool for\ndiagnosing, interpreting, and improving reasoning in LLMs."}
{"id": "2505.22554", "pdf": "https://arxiv.org/pdf/2505.22554", "abs": "https://arxiv.org/abs/2505.22554", "authors": ["Agnideep Aich", "Md Monzur Murshed", "Amanda Mayeaux", "Sameera Hewage"], "title": "Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62P10, 68T07"], "comment": "Submitted", "summary": "Accurate diabetes risk prediction relies on identifying key features from\ncomplex health datasets, but conventional methods like mutual information (MI)\nfilters and genetic algorithms (GAs) often overlook extreme dependencies\ncritical for high-risk subpopulations. In this study we introduce a\nfeature-selection framework using the upper-tail dependence coefficient\n({\\lambda}U) of the novel A2 copula, which quantifies how often extreme higher\nvalues of a predictor co-occur with diabetes diagnoses (target variable).\nApplied to the CDC Diabetes Health Indicators dataset (n=253,680), our method\nprioritizes five predictors (self-reported general health, high blood pressure,\nbody mass index, mobility limitations, and high cholesterol levels) based on\nupper tail dependencies. These features match or outperform MI and GA selected\nsubsets across four classifiers (Random Forest, XGBoost, Logistic Regression,\nGradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to\n0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation\nimportance confirms clinical relevance, with BMI and general health driving\naccuracy. To our knowledge, this is the first work to apply a copula's\nupper-tail dependence for supervised feature selection, bridging extreme-value\ntheory and machine learning to deliver a practical toolkit for diabetes\nprevention."}
{"id": "2505.22244", "pdf": "https://arxiv.org/pdf/2505.22244", "abs": "https://arxiv.org/abs/2505.22244", "authors": ["Yaron Halle", "Ariel Felner", "Sven Koenig", "Oren Salzman"], "title": "A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives", "categories": ["cs.AI"], "comment": null, "summary": "The bi-objective shortest-path (BOSP) problem seeks to find paths between\nstart and target vertices of a graph while optimizing two conflicting objective\nfunctions. We consider the BOSP problem in the presence of correlated\nobjectives. Such correlations often occur in real-world settings such as road\nnetworks, where optimizing two positively correlated objectives, such as travel\ntime and fuel consumption, is common. BOSP is generally computationally\nchallenging as the size of the search space is exponential in the number of\nobjective functions and the graph size. Bounded sub-optimal BOSP solvers such\nas A*pex alleviate this complexity by approximating the Pareto-optimal solution\nset rather than computing it exactly (given a user-provided approximation\nfactor). As the correlation between objective functions increases, smaller\napproximation factors are sufficient for collapsing the entire Pareto-optimal\nset into a single solution. We leverage this insight to propose an efficient\nalgorithm that reduces the search effort in the presence of correlated\nobjectives. Our approach for computing approximations of the entire\nPareto-optimal set is inspired by graph-clustering algorithms. It uses a\npreprocessing phase to identify correlated clusters within a graph and to\ngenerate a new graph representation. This allows a natural generalization of\nA*pex to run up to five times faster on DIMACS dataset instances, a standard\nbenchmark in the field. To the best of our knowledge, this is the first\nalgorithm proposed that efficiently and effectively exploits correlations in\nthe context of bi-objective search while providing theoretical guarantees on\nsolution quality."}
{"id": "2505.22622", "pdf": "https://arxiv.org/pdf/2505.22622", "abs": "https://arxiv.org/abs/2505.22622", "authors": ["Jiawei Ge", "Amanda Wang", "Shange Tang", "Chi Jin"], "title": "Principled Out-of-Distribution Generalization via Simplicity", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Modern foundation models exhibit remarkable out-of-distribution (OOD)\ngeneralization, solving tasks far beyond the support of their training data.\nHowever, the theoretical principles underpinning this phenomenon remain\nelusive. This paper investigates this problem by examining the compositional\ngeneralization abilities of diffusion models in image generation. Our analysis\nreveals that while neural network architectures are expressive enough to\nrepresent a wide range of models -- including many with undesirable behavior on\nOOD inputs -- the true, generalizable model that aligns with human expectations\ntypically corresponds to the simplest among those consistent with the training\ndata.\n  Motivated by this observation, we develop a theoretical framework for OOD\ngeneralization via simplicity, quantified using a predefined simplicity metric.\nWe analyze two key regimes: (1) the constant-gap setting, where the true model\nis strictly simpler than all spurious alternatives by a fixed gap, and (2) the\nvanishing-gap setting, where the fixed gap is replaced by a smoothness\ncondition ensuring that models close in simplicity to the true model yield\nsimilar predictions. For both regimes, we study the regularized maximum\nlikelihood estimator and establish the first sharp sample complexity guarantees\nfor learning the true, generalizable, simple model."}
{"id": "2505.22288", "pdf": "https://arxiv.org/pdf/2505.22288", "abs": "https://arxiv.org/abs/2505.22288", "authors": ["Jan Speller", "Malte Luttermann", "Marcel Gehrke", "Tanya Braun"], "title": "Compression versus Accuracy: A Hierarchy of Lifted Models", "categories": ["cs.AI"], "comment": null, "summary": "Probabilistic graphical models that encode indistinguishable objects and\nrelations among them use first-order logic constructs to compress a\npropositional factorised model for more efficient (lifted) inference. To obtain\na lifted representation, the state-of-the-art algorithm Advanced Colour Passing\n(ACP) groups factors that represent matching distributions. In an approximate\nversion using $\\varepsilon$ as a hyperparameter, factors are grouped that\ndiffer by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable\n$\\varepsilon$ is not obvious and may need a lot of exploration, possibly\nrequiring many ACP runs with different $\\varepsilon$ values. Additionally,\nvarying $\\varepsilon$ can yield wildly different models, leading to decreased\ninterpretability. Therefore, this paper presents a hierarchical approach to\nlifted model construction that is hyperparameter-free. It efficiently computes\na hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning\nthat once factors are grouped together given some $\\varepsilon$, these factors\nwill be grouped together for larger $\\varepsilon$ as well. The hierarchy of\n$\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for\nexplicitly weighing compression versus accuracy when choosing specific\n$\\varepsilon$ values to run ACP with and enables interpretability between the\ndifferent models."}
{"id": "2505.21626", "pdf": "https://arxiv.org/pdf/2505.21626", "abs": "https://arxiv.org/abs/2505.21626", "authors": ["Nicolas Guerra", "Nicholas H. Nelsen", "Yunan Yang"], "title": "Learning Where to Learn: Training Distribution Selection for Provable OOD Performance", "categories": ["cs.LG", "math.OC", "stat.ML", "62K05, 65K10 (Primary) 68T07, 65D15, 62R20, 60G57 (Secondary)"], "comment": "32 pages, 8 figures, 2 tables, 3 algorithms", "summary": "Out-of-distribution (OOD) generalization remains a fundamental challenge in\nmachine learning. Models trained on one data distribution often experience\nsubstantial performance degradation when evaluated on shifted or unseen\ndomains. To address this challenge, the present paper studies the design of\ntraining data distributions that maximize average-case OOD performance. First,\na theoretical analysis establishes a family of generalization bounds that\nquantify how the choice of training distribution influences OOD error across a\npredefined family of target distributions. These insights motivate the\nintroduction of two complementary algorithmic strategies: (i) directly\nformulating OOD risk minimization as a bilevel optimization problem over the\nspace of probability measures and (ii) minimizing a theoretical upper bound on\nOOD error. Last, the paper evaluates the two approaches across a range of\nfunction approximation and operator learning examples. The proposed methods\nsignificantly improve OOD accuracy over standard empirical risk minimization\nwith a fixed distribution. These results highlight the potential of\ndistribution-aware training as a principled and practical framework for robust\nOOD generalization."}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments."}
{"id": "2505.21640", "pdf": "https://arxiv.org/pdf/2505.21640", "abs": "https://arxiv.org/abs/2505.21640", "authors": ["Oren Mangoubi", "Neil He", "Nisheeth K. Vishnoi"], "title": "Efficient Diffusion Models for Symmetric Manifolds", "categories": ["cs.LG", "cs.AI", "cs.DS", "math.PR", "stat.ML"], "comment": "The conference version of this paper appears in ICML 2025", "summary": "We introduce a framework for designing efficient diffusion models for\n$d$-dimensional symmetric-space Riemannian manifolds, including the torus,\nsphere, special orthogonal group and unitary group. Existing manifold diffusion\nmodels often depend on heat kernels, which lack closed-form expressions and\nrequire either $d$ gradient evaluations or exponential-in-$d$ arithmetic\noperations per training step. We introduce a new diffusion model for symmetric\nmanifolds with a spatially-varying covariance, allowing us to leverage a\nprojection of Euclidean Brownian motion to bypass heat kernel computations. Our\ntraining algorithm minimizes a novel efficient objective derived via Ito's\nLemma, allowing each step to run in $O(1)$ gradient evaluations and\nnearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap\nbetween diffusions on symmetric manifolds and Euclidean space. Manifold\nsymmetries ensure the diffusion satisfies an \"average-case\" Lipschitz\ncondition, enabling accurate and efficient sample generation. Empirically, our\nmodel outperforms prior methods in training speed and improves sample quality\non synthetic datasets on the torus, special orthogonal group, and unitary\ngroup."}
{"id": "2505.22311", "pdf": "https://arxiv.org/pdf/2505.22311", "abs": "https://arxiv.org/abs/2505.22311", "authors": ["Feibo Jiang", "Cunhua Pan", "Li Dong", "Kezhi Wang", "Octavia A. Dobre", "Merouane Debbah"], "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications", "categories": ["cs.AI", "cs.CY", "cs.NI", "eess.SP"], "comment": null, "summary": "With the advent of 6G communications, intelligent communication systems face\nmultiple challenges, including constrained perception and response\ncapabilities, limited scalability, and low adaptability in dynamic\nenvironments. This tutorial provides a systematic introduction to the\nprinciples, design, and applications of Large Artificial Intelligence Models\n(LAMs) and Agentic AI technologies in intelligent communication systems, aiming\nto offer researchers a comprehensive overview of cutting-edge technologies and\npractical guidance. First, we outline the background of 6G communications,\nreview the technological evolution from LAMs to Agentic AI, and clarify the\ntutorial's motivation and main contributions. Subsequently, we present a\ncomprehensive review of the key components required for constructing LAMs. We\nfurther categorize LAMs and analyze their applicability, covering Large\nLanguage Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models\n(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a\nLAM-centric design paradigm tailored for communications, encompassing dataset\nconstruction and both internal and external learning approaches. Building upon\nthis, we develop an LAM-based Agentic AI system for intelligent communications,\nclarifying its core components such as planners, knowledge bases, tools, and\nmemory modules, as well as its interaction mechanisms. We also introduce a\nmulti-agent framework with data retrieval, collaborative planning, and\nreflective evaluation for 6G. Subsequently, we provide a detailed overview of\nthe applications of LAMs and Agentic AI in communication scenarios. Finally, we\nsummarize the research challenges and future directions in current studies,\naiming to support the development of efficient, secure, and sustainable\nnext-generation intelligent communication systems."}
{"id": "2505.21651", "pdf": "https://arxiv.org/pdf/2505.21651", "abs": "https://arxiv.org/abs/2505.21651", "authors": ["Nikola Surjanovic", "Alexandre Bouchard-Côté", "Trevor Campbell"], "title": "AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent", "categories": ["cs.LG", "math.OC", "stat.CO", "stat.ML"], "comment": null, "summary": "The learning rate is an important tuning parameter for stochastic gradient\ndescent (SGD) and can greatly influence its performance. However, appropriate\nselection of a learning rate schedule across all iterations typically requires\na non-trivial amount of user tuning effort. To address this, we introduce\nAutoSGD: an SGD method that automatically determines whether to increase or\ndecrease the learning rate at a given iteration and then takes appropriate\naction. We introduce theory supporting the convergence of AutoSGD, along with\nits deterministic counterpart for standard gradient descent. Empirical results\nsuggest strong performance of the method on a variety of traditional\noptimization problems and machine learning tasks."}
{"id": "2505.22368", "pdf": "https://arxiv.org/pdf/2505.22368", "abs": "https://arxiv.org/abs/2505.22368", "authors": ["Enfang Cui", "Yujun Cheng", "Rui She", "Dan Liu", "Zhiyuan Liang", "Minxin Guo", "Tianzheng Li", "Qian Wei", "Wenjuan Xing", "Zhijie Zhong"], "title": "AgentDNS: A Root Domain Naming System for LLM Agents", "categories": ["cs.AI"], "comment": "7 pages, 6 figures", "summary": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns."}
{"id": "2505.21722", "pdf": "https://arxiv.org/pdf/2505.21722", "abs": "https://arxiv.org/abs/2505.21722", "authors": ["Ioannis Bantzis", "James B. Simon", "Arthur Jacot"], "title": "Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When a deep ReLU network is initialized with small weights, GD is at first\ndominated by the saddle at the origin in parameter space. We study the\nso-called escape directions, which play a similar role as the eigenvectors of\nthe Hessian for strict saddles. We show that the optimal escape direction\nfeatures a low-rank bias in its deeper layers: the first singular value of the\n$\\ell$-th layer weight matrix is at least $\\ell^{\\frac{1}{4}}$ larger than any\nother singular value. We also prove a number of related results about these\nescape directions. We argue that this result is a first step in proving\nSaddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of\nsaddles with increasing bottleneck rank."}
{"id": "2505.22451", "pdf": "https://arxiv.org/pdf/2505.22451", "abs": "https://arxiv.org/abs/2505.22451", "authors": ["Yuanhang Liu", "Yanxing Huang", "Yanqiao Wang", "Peng Li", "Yang Liu"], "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research", "categories": ["cs.AI"], "comment": "95 pages, 1 figure", "summary": "Large Reasoning Models (LRMs) have made significant progress in mathematical\ncapabilities in recent times. However, these successes have been primarily\nconfined to competition-level problems. In this work, we propose AI\nMathematician (AIM) framework, which harnesses the reasoning strength of LRMs\nto support frontier mathematical research. We have identified two critical\nchallenges of mathematical research compared to competition, {\\it the intrinsic\ncomplexity of research problems} and {\\it the requirement of procedural rigor}.\nTo address these challenges, AIM incorporates two core strategies: an\nexploration mechanism to foster longer solution paths, and the pessimistic\nreasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling\nresearch-level tasks. We conducted extensive experiments across several\nreal-world mathematical topics and obtained promising results. AIM is able to\nautonomously construct substantial portions of proofs and uncover non-trivial\ninsights within each research area. These findings highlight the potential of\nLRMs in mathematical discovery and suggest that LRM-based agent systems could\nsignificantly accelerate mathematical research in the future."}
{"id": "2505.21790", "pdf": "https://arxiv.org/pdf/2505.21790", "abs": "https://arxiv.org/abs/2505.21790", "authors": ["Hilal Asi", "Vinod Raman", "Kunal Talwar"], "title": "Faster Rates for Private Adversarial Bandits", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$"}
{"id": "2505.22597", "pdf": "https://arxiv.org/pdf/2505.22597", "abs": "https://arxiv.org/abs/2505.22597", "authors": ["Ngoc La", "Ruaridh Mon-Williams", "Julie A. Shah"], "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted to Proceedings of ICAPS 2025", "summary": "In recent years, reinforcement learning (RL) methods have been widely tested\nusing tools like OpenAI Gym, though many tasks in these environments could also\nbenefit from hierarchical planning. However, there is a lack of a tool that\nenables seamless integration of hierarchical planning with RL. Hierarchical\nDomain Definition Language (HDDL), used in classical planning, introduces a\nstructured approach well-suited for model-based RL to address this gap. To\nbridge this integration, we introduce HDDLGym, a Python-based tool that\nautomatically generates OpenAI Gym environments from HDDL domains and problems.\nHDDLGym serves as a link between RL and hierarchical planning, supporting\nmulti-agent scenarios and enabling collaborative planning among agents. This\npaper provides an overview of HDDLGym's design and implementation, highlighting\nthe challenges and design choices involved in integrating HDDL with the Gym\ninterface, and applying RL policies to support hierarchical planning. We also\nprovide detailed instructions and demonstrations for using the HDDLGym\nframework, including how to work with existing HDDL domains and problems from\nInternational Planning Competitions, exemplified by the Transport domain.\nAdditionally, we offer guidance on creating new HDDL domains for multi-agent\nscenarios and demonstrate the practical use of HDDLGym in the Overcooked\ndomain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a\nvaluable tool for studying RL in hierarchical planning, particularly in\nmulti-agent contexts."}
{"id": "2505.21813", "pdf": "https://arxiv.org/pdf/2505.21813", "abs": "https://arxiv.org/abs/2505.21813", "authors": ["Madi Matymov", "Ba-Hien Tran", "Michael Kampffmeyer", "Markus Heinonen", "Maurizio Filippone"], "title": "Optimizing Data Augmentation through Bayesian Model Selection", "categories": ["cs.LG", "stat.ML", "62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)"], "comment": "26 pages, 3 figures", "summary": "Data Augmentation (DA) has become an essential tool to improve robustness and\ngeneralization of modern machine learning. However, when deciding on DA\nstrategies it is critical to choose parameters carefully, and this can be a\ndaunting task which is traditionally left to trial-and-error or expensive\noptimization based on validation performance. In this paper, we counter these\nlimitations by proposing a novel framework for optimizing DA. In particular, we\ntake a probabilistic view of DA, which leads to the interpretation of\naugmentation parameters as model (hyper)-parameters, and the optimization of\nthe marginal likelihood with respect to these parameters as a Bayesian model\nselection problem. Due to its intractability, we derive a tractable Evidence\nLower BOund (ELBO), which allows us to optimize augmentation parameters jointly\nwith model parameters. We provide extensive theoretical results on variational\napproximation quality, generalization guarantees, invariance properties, and\nconnections to empirical Bayes. Through experiments on computer vision tasks,\nwe show that our approach improves calibration and yields robust performance\nover fixed or no augmentation. Our work provides a rigorous foundation for\noptimizing DA through Bayesian principles with significant potential for robust\nmachine learning."}
{"id": "2505.21512", "pdf": "https://arxiv.org/pdf/2505.21512", "abs": "https://arxiv.org/abs/2505.21512", "authors": ["Harry Li", "Gabriel Appleby", "Kenneth Alperin", "Steven R Gomez", "Ashley Suh"], "title": "The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Knowledge graphs (KGs) are powerful data structures, but exploring them\neffectively remains difficult for even expert users. Large language models\n(LLMs) are increasingly used to address this gap, yet little is known\nempirically about how their usage with KGs shapes user trust, exploration\nstrategies, or downstream decision-making - raising key design challenges for\nLLM-based KG visual analysis systems. To study these effects, we developed\nLinkQ, a KG exploration system that converts natural language questions into\nstructured queries with an LLM. We collaborated with KG experts to design five\nvisual mechanisms that help users assess the accuracy of both KG queries and\nLLM responses: an LLM-KG state diagram that illustrates which stage of the\nexploration pipeline LinkQ is in, a query editor displaying the generated query\npaired with an LLM explanation, an entity-relation ID table showing extracted\nKG entities and relations with semantic descriptions, a query structure graph\nthat depicts the path traversed in the KG, and an interactive graph\nvisualization of query results. From a qualitative evaluation with 14\npractitioners, we found that users - even KG experts - tended to overtrust\nLinkQ's outputs due to its \"helpful\" visualizations, even when the LLM was\nincorrect. Users exhibited distinct workflows depending on their prior\nfamiliarity with KGs and LLMs, challenging the assumption that these systems\nare one-size-fits-all - despite often being designed as if they are. Our\nfindings highlight the risks of false trust in LLM-assisted data analysis tools\nand the need for further investigation into the role of visualization as a\nmitigation technique."}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach."}
{"id": "2505.21857", "pdf": "https://arxiv.org/pdf/2505.21857", "abs": "https://arxiv.org/abs/2505.21857", "authors": ["Mijung Park"], "title": "Revisiting Bayesian Model Averaging in the Era of Foundation Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit the classical, full-fledged Bayesian model averaging (BMA)\nparadigm to ensemble pre-trained and/or lightly-finetuned foundation models to\nenhance the classification performance on image and text data. To make BMA\ntractable under foundation models, we introduce trainable linear classifiers\nthat take frozen features from the pre-trained foundation models as inputs. The\nmodel posteriors over the linear classifiers tell us which linear heads and\nfrozen features are better suited for a given dataset, resulting in a\nprincipled model ensembling method. Furthermore, we propose a computationally\ncheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize\nthe model ensemble weights, just like those weights based on model posterior\ndistributions in BMA, by reducing the amount of surprise (expected entropy of\nthe predictions) we get from predictions of ensembled models. With the rapid\ndevelopment of foundation models, these approaches will enable the\nincorporation of future, possibly significantly better foundation models to\nenhance the performance of challenging classification tasks."}
{"id": "2505.21514", "pdf": "https://arxiv.org/pdf/2505.21514", "abs": "https://arxiv.org/abs/2505.21514", "authors": ["Mingchao Jiang", "Abhinav Jain", "Sophia Zorek", "Chris Jermaine"], "title": "SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation", "categories": ["cs.LG", "cs.PL", "cs.SE"], "comment": "Keywords: Benchmark Dataset, LLM Evaluation, Gen-AI, Program\n  Synthesis; TLDR: SimCoPilot is a benchmark for evaluating LLMs as\n  \"copilot\"-style interactive coding assistants, testing their ability to\n  integrate and complete code within complex real-world software environments", "summary": "We introduce SIMCOPILOT, a benchmark that simulates the role of large\nlanguage models (LLMs) as interactive, \"copilot\"-style coding assistants.\nTargeting both completion (finishing incomplete methods or code blocks) and\ninfill tasks (filling missing segments within existing code), SIMCOPILOT\nprovides a comprehensive framework for evaluating LLM coding capabilities. The\nbenchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python\n(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our\nkey contributions include: (a) establishing a realistic, detailed evaluation\nenvironment to assess LLM utility in practical coding scenarios, and (b)\nproviding fine-grained analyses that address critical factors frequently\noverlooked by existing benchmarks, such as task-specific performance nuances,\ncontextual understanding across code segments, and sensitivity to variable\nscope. Evaluations conducted across domains-including algorithms, databases,\ncomputer vision, and neural networks-offer insights into model strengths and\nhighlight persistent challenges in maintaining logical consistency within\ncomplex dependency structures. Beyond benchmarking, our study sheds light on\nthe current limitations of LLM-driven code generation and underscores the\nongoing transition of LLMs from merely syntax-aware generators toward reliable,\nintelligent software development partners."}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md."}
{"id": "2505.21942", "pdf": "https://arxiv.org/pdf/2505.21942", "abs": "https://arxiv.org/abs/2505.21942", "authors": ["Prashant Bhat", "Laurens Niesten", "Elahe Arani", "Bahram Zonooz"], "title": "Continual Learning Beyond Experience Rehearsal and Full Model Surrogates", "categories": ["cs.LG", "stat.ML"], "comment": "23 pages, 9 figures", "summary": "Continual learning (CL) has remained a significant challenge for deep neural\nnetworks as learning new tasks erases previously acquired knowledge, either\npartially or completely. Existing solutions often rely on experience rehearsal\nor full model surrogates to mitigate CF. While effective, these approaches\nintroduce substantial memory and computational overhead, limiting their\nscalability and applicability in real-world scenarios. To address this, we\npropose SPARC, a scalable CL approach that eliminates the need for experience\nrehearsal and full-model surrogates. By effectively combining task-specific\nworking memories and task-agnostic semantic memory for cross-task knowledge\nconsolidation, SPARC results in a remarkable parameter efficiency, using only\n6% of the parameters required by full-model surrogates. Despite its lightweight\ndesign, SPARC achieves superior performance on Seq-TinyImageNet and matches\nrehearsal-based methods on various CL benchmarks. Additionally, weight\nre-normalization in the classification layer mitigates task-specific biases,\nestablishing SPARC as a practical and scalable solution for CL under stringent\nefficiency constraints."}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach."}
{"id": "2505.21571", "pdf": "https://arxiv.org/pdf/2505.21571", "abs": "https://arxiv.org/abs/2505.21571", "authors": ["Yao Lu", "Tengfei Ma", "Zeyu Wang", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the rapid development of wireless communications and the growing\ncomplexity of digital modulation schemes, traditional manual modulation\nrecognition methods struggle to extract reliable signal features and meet\nreal-time requirements in modern scenarios. Recently, deep learning based\nAutomatic Modulation Recognition (AMR) approaches have greatly improved\nclassification accuracy. However, their large model sizes and high\ncomputational demands hinder deployment on resource-constrained devices. Model\npruning provides a general approach to reduce model complexity, but existing\nweight, channel, and layer pruning techniques each present a trade-off between\ncompression rate, hardware acceleration, and accuracy preservation. To this\nend, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning\nframework that combines channel-level pruning with layer-level collapse\ndiagnosis to achieve extreme compression, high performance and efficient\ninference. In the first stage of FCOS, hierarchical clustering and parameter\nfusion are applied to channel weights to achieve channel-level pruning. Then a\nLayer Collapse Diagnosis (LaCD) module uses linear probing to identify layer\ncollapse and removes the collapsed layers due to high channel compression\nratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms\nexisting channel and layer pruning methods. Specifically, FCOS achieves 95.51%\nFLOPs reduction and 95.31% parameter reduction while still maintaining\nperformance close to the original ResNet56, with only a 0.46% drop in accuracy\non Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS."}
{"id": "2505.21972", "pdf": "https://arxiv.org/pdf/2505.21972", "abs": "https://arxiv.org/abs/2505.21972", "authors": ["Patrick Vossler", "Fan Xia", "Yifan Mai", "Jean Feng"], "title": "Judging LLMs on a Simplex", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "28 pages, 7 figures", "summary": "Automated evaluation of free-form outputs from large language models (LLMs)\nis challenging because many distinct answers can be equally valid. A common\npractice is to use LLMs themselves as judges, but the theoretical properties of\nthis approach are not yet well understood. We show that a geometric framework\nthat represents both judges and candidates as points on a probability simplex\ncan provide helpful insight on what is or is not identifiable using LLM judges.\nOur theoretical analysis uncovers a \"phase transition\" in ranking\nidentifiability: for binary scoring systems, true rankings are identifiable\neven with weak judges under mild assumptions, while rankings become\nnon-identifiable for three or more scoring levels even with infinite data,\nabsent additional prior knowledge. This non-identifiability highlights how\nuncertainty in rankings stems from not only aleatoric uncertainty (i.e.,\ninherent stochasticity in the data) but also epistemic uncertainty regarding\nwhich assumptions hold, an aspect that has received limited attention until\nnow. To integrate both types of uncertainty, we use Bayesian inference to\nencode assumptions as priors and conduct sensitivity analysis of ranking\nestimates and credible intervals. Empirical evaluations across multiple\nbenchmarks demonstrate that Bayesian inference yields more accurate rankings\nand substantially improves coverage rates. These results underscore the\nimportance of taking a more holistic approach to uncertainty quantification\nwhen using LLMs as judges."}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md."}
{"id": "2505.21573", "pdf": "https://arxiv.org/pdf/2505.21573", "abs": "https://arxiv.org/abs/2505.21573", "authors": ["Han Wan", "Rui Zhang", "Hao Sun"], "title": "Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial differential equations (PDEs) govern the spatiotemporal evolution of\nvarious physical systems. Classical numerical solvers, while accurate, require\nfine discretization and full knowledge of the governing PDEs, limiting their\napplicability when the physics is unknown or fast inference is required.\nData-driven neural PDE solvers alleviate these constraints by learning from\ndata but demand large training datasets and perform poorly in data-scarce\nregimes. Physics-aware methods mitigate data requirements by incorporating\nphysical knowledge yet rely on known PDE terms or local numerical schemes,\nrestricting their ability to handle unknown or globally coupled systems. In\nthis work, we propose the Spectral-inspired Neural Operator (SINO), a novel\nframework that learns PDE operators from limited trajectories (as few as 2-5),\nwithout any known PDE terms. SINO operates in the frequency domain and\nintroduces a Frequency-to-Vector module to learn spectral representations\nanalogous to derivative multipliers. To model nonlinear physical interactions,\nwe design a nonlinear operator block that includes a $\\Pi$-Block with low-pass\nfiltering to prevent aliasing. Finally, we introduce an operator distillation\ntechnique to distill the trained model for efficient inference. SINO achieves\nstate-of-the-art results across multiple PDE benchmarks, demonstrating strong\ndiscretization invariance and robust generalization to out-of-distribution\ninitial conditions. To our knowledge, SINO is the first physics-aware method\ncapable of accurately simulating globally coupled systems (e.g., the\nNavier-Stokes equations) from limited data without any explicit PDE terms."}
{"id": "2505.22257", "pdf": "https://arxiv.org/pdf/2505.22257", "abs": "https://arxiv.org/abs/2505.22257", "authors": ["Youssef Mroueh", "Nicolas Dupuis", "Brian Belgodere", "Apoorva Nitsure", "Mattia Rigotti", "Kristjan Greenewald", "Jiri Navratil", "Jerret Ross", "Jesus Rios"], "title": "Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit Group Relative Policy Optimization (GRPO) in both on-policy and\noff-policy optimization regimes. Our motivation comes from recent work on\noff-policy Proximal Policy Optimization (PPO), which improves training\nstability, sampling efficiency, and memory usage. In addition, a recent\nanalysis of GRPO suggests that estimating the advantage function with\noff-policy samples could be beneficial. Building on these observations, we\nadapt GRPO to the off-policy setting. We show that both on-policy and\noff-policy GRPO objectives yield an improvement in the reward. This result\nmotivates the use of clipped surrogate objectives in the off-policy version of\nGRPO. We then compare the empirical performance of reinforcement learning with\nverifiable rewards in post-training using both GRPO variants. Our results show\nthat off-policy GRPO either significantly outperforms or performs on par with\nits on-policy counterpart."}
{"id": "2505.21571", "pdf": "https://arxiv.org/pdf/2505.21571", "abs": "https://arxiv.org/abs/2505.21571", "authors": ["Yao Lu", "Tengfei Ma", "Zeyu Wang", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the rapid development of wireless communications and the growing\ncomplexity of digital modulation schemes, traditional manual modulation\nrecognition methods struggle to extract reliable signal features and meet\nreal-time requirements in modern scenarios. Recently, deep learning based\nAutomatic Modulation Recognition (AMR) approaches have greatly improved\nclassification accuracy. However, their large model sizes and high\ncomputational demands hinder deployment on resource-constrained devices. Model\npruning provides a general approach to reduce model complexity, but existing\nweight, channel, and layer pruning techniques each present a trade-off between\ncompression rate, hardware acceleration, and accuracy preservation. To this\nend, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning\nframework that combines channel-level pruning with layer-level collapse\ndiagnosis to achieve extreme compression, high performance and efficient\ninference. In the first stage of FCOS, hierarchical clustering and parameter\nfusion are applied to channel weights to achieve channel-level pruning. Then a\nLayer Collapse Diagnosis (LaCD) module uses linear probing to identify layer\ncollapse and removes the collapsed layers due to high channel compression\nratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms\nexisting channel and layer pruning methods. Specifically, FCOS achieves 95.51%\nFLOPs reduction and 95.31% parameter reduction while still maintaining\nperformance close to the original ResNet56, with only a 0.46% drop in accuracy\non Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS."}
{"id": "2505.21576", "pdf": "https://arxiv.org/pdf/2505.21576", "abs": "https://arxiv.org/abs/2505.21576", "authors": ["Jiawei Tang", "Yuheng Jia"], "title": "Concentration Distribution Learning from Label Distributions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Label distribution learning (LDL) is an effective method to predict the\nrelative label description degree (a.k.a. label distribution) of a sample.\nHowever, the label distribution is not a complete representation of an instance\nbecause it overlooks the absolute intensity of each label. Specifically, it's\nimpossible to obtain the total description degree of hidden labels that not in\nthe label space, which leads to the loss of information and confusion in\ninstances. To solve the above problem, we come up with a new concept named\nbackground concentration to serve as the absolute description degree term of\nthe label distribution and introduce it into the LDL process, forming the\nimproved paradigm of concentration distribution learning. Moreover, we propose\na novel model by probabilistic methods and neural networks to learn label\ndistributions and background concentrations from existing LDL datasets.\nExtensive experiments prove that the proposed approach is able to extract\nbackground concentrations from label distributions while producing more\naccurate prediction results than the state-of-the-art LDL methods. The code is\navailable in https://github.com/seutjw/CDL-LD."}
{"id": "2505.22356", "pdf": "https://arxiv.org/pdf/2505.22356", "abs": "https://arxiv.org/abs/2505.22356", "authors": ["Angéline Pouget", "Mohammad Yaghini", "Stephan Rabanser", "Nicolas Papernot"], "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications."}
{"id": "2505.21573", "pdf": "https://arxiv.org/pdf/2505.21573", "abs": "https://arxiv.org/abs/2505.21573", "authors": ["Han Wan", "Rui Zhang", "Hao Sun"], "title": "Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial differential equations (PDEs) govern the spatiotemporal evolution of\nvarious physical systems. Classical numerical solvers, while accurate, require\nfine discretization and full knowledge of the governing PDEs, limiting their\napplicability when the physics is unknown or fast inference is required.\nData-driven neural PDE solvers alleviate these constraints by learning from\ndata but demand large training datasets and perform poorly in data-scarce\nregimes. Physics-aware methods mitigate data requirements by incorporating\nphysical knowledge yet rely on known PDE terms or local numerical schemes,\nrestricting their ability to handle unknown or globally coupled systems. In\nthis work, we propose the Spectral-inspired Neural Operator (SINO), a novel\nframework that learns PDE operators from limited trajectories (as few as 2-5),\nwithout any known PDE terms. SINO operates in the frequency domain and\nintroduces a Frequency-to-Vector module to learn spectral representations\nanalogous to derivative multipliers. To model nonlinear physical interactions,\nwe design a nonlinear operator block that includes a $\\Pi$-Block with low-pass\nfiltering to prevent aliasing. Finally, we introduce an operator distillation\ntechnique to distill the trained model for efficient inference. SINO achieves\nstate-of-the-art results across multiple PDE benchmarks, demonstrating strong\ndiscretization invariance and robust generalization to out-of-distribution\ninitial conditions. To our knowledge, SINO is the first physics-aware method\ncapable of accurately simulating globally coupled systems (e.g., the\nNavier-Stokes equations) from limited data without any explicit PDE terms."}
{"id": "2505.21584", "pdf": "https://arxiv.org/pdf/2505.21584", "abs": "https://arxiv.org/abs/2505.21584", "authors": ["Afaf Taik", "Khaoula Chehbouni", "Golnoosh Farnadi"], "title": "Fairness in Federated Learning: Fairness for Whom?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Fairness in federated learning has emerged as a rapidly growing area of\nresearch, with numerous works proposing formal definitions and algorithmic\ninterventions. Yet, despite this technical progress, fairness in FL is often\ndefined and evaluated in ways that abstract away from the sociotechnical\ncontexts in which these systems are deployed. In this paper, we argue that\nexisting approaches tend to optimize narrow system level metrics, such as\nperformance parity or contribution-based rewards, while overlooking how harms\narise throughout the FL lifecycle and how they impact diverse stakeholders. We\nsupport this claim through a critical analysis of the literature, based on a\nsystematic annotation of papers for their fairness definitions, design\ndecisions, evaluation practices, and motivating use cases. Our analysis reveals\nfive recurring pitfalls: 1) fairness framed solely through the lens of server\nclient architecture, 2) a mismatch between simulations and motivating use-cases\nand contexts, 3) definitions that conflate protecting the system with\nprotecting its users, 4) interventions that target isolated stages of the\nlifecycle while neglecting upstream and downstream effects, 5) and a lack of\nmulti-stakeholder alignment where multiple fairness definitions can be relevant\nat once. Building on these insights, we propose a harm centered framework that\nlinks fairness definitions to concrete risks and stakeholder vulnerabilities.\nWe conclude with recommendations for more holistic, context-aware, and\naccountable fairness research in FL."}
{"id": "2505.22361", "pdf": "https://arxiv.org/pdf/2505.22361", "abs": "https://arxiv.org/abs/2505.22361", "authors": ["Xiangyu Chang", "Xi Chen", "Yining Wang", "Zhiyi Zeng"], "title": "Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper studies a bandit optimization problem where the goal is to\nmaximize a function $f(x)$ over $T$ periods for some unknown strongly concave\nfunction $f$. We consider a new pairwise comparison oracle, where the\ndecision-maker chooses a pair of actions $(x, x')$ for a consecutive number of\nperiods and then obtains an estimate of $f(x)-f(x')$. We show that such a\npairwise comparison oracle finds important applications to joint pricing and\ninventory replenishment problems and network revenue management. The challenge\nin this bandit optimization is twofold. First, the decision-maker not only\nneeds to determine a pair of actions $(x, x')$ but also a stopping time $n$\n(i.e., the number of queries based on $(x, x')$). Second, motivated by our\ninventory application, the estimate of the difference $f(x)-f(x')$ is biased,\nwhich is different from existing oracles in stochastic optimization literature.\nTo address these challenges, we first introduce a discretization technique and\nlocal polynomial approximation to relate this problem to linear bandits. Then\nwe developed a tournament successive elimination technique to localize the\ndiscretized cell and run an interactive batched version of LinUCB algorithm on\ncells. We establish regret bounds that are optimal up to poly-logarithmic\nfactors. Furthermore, we apply our proposed algorithm and analytical framework\nto the two operations management problems and obtain results that improve\nstate-of-the-art results in the existing literature."}
{"id": "2505.21576", "pdf": "https://arxiv.org/pdf/2505.21576", "abs": "https://arxiv.org/abs/2505.21576", "authors": ["Jiawei Tang", "Yuheng Jia"], "title": "Concentration Distribution Learning from Label Distributions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Label distribution learning (LDL) is an effective method to predict the\nrelative label description degree (a.k.a. label distribution) of a sample.\nHowever, the label distribution is not a complete representation of an instance\nbecause it overlooks the absolute intensity of each label. Specifically, it's\nimpossible to obtain the total description degree of hidden labels that not in\nthe label space, which leads to the loss of information and confusion in\ninstances. To solve the above problem, we come up with a new concept named\nbackground concentration to serve as the absolute description degree term of\nthe label distribution and introduce it into the LDL process, forming the\nimproved paradigm of concentration distribution learning. Moreover, we propose\na novel model by probabilistic methods and neural networks to learn label\ndistributions and background concentrations from existing LDL datasets.\nExtensive experiments prove that the proposed approach is able to extract\nbackground concentrations from label distributions while producing more\naccurate prediction results than the state-of-the-art LDL methods. The code is\navailable in https://github.com/seutjw/CDL-LD."}
{"id": "2505.21587", "pdf": "https://arxiv.org/pdf/2505.21587", "abs": "https://arxiv.org/abs/2505.21587", "authors": ["Bin Qin", "Qirui Ji", "Jiangmeng Li", "Yupeng Wang", "Xuesong Wu", "Jianwen Cao", "Fanjiang Xu"], "title": "CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised topological deep learning (TDL) represents a nascent but\nunderexplored area with significant potential for modeling higher-order\ninteractions in simplicial complexes and cellular complexes to derive\nrepresentations of unlabeled graphs. Compared to simplicial complexes, cellular\ncomplexes exhibit greater expressive power. However, the advancement in\nself-supervised learning for cellular TDL is largely hindered by two core\nchallenges: \\textit{extrinsic structural constraints} inherent to cellular\ncomplexes, and intrinsic semantic redundancy in cellular representations. The\nfirst challenge highlights that traditional graph augmentation techniques may\ncompromise the integrity of higher-order cellular interactions, while the\nsecond underscores that topological redundancy in cellular complexes\npotentially diminish task-relevant information. To address these issues, we\nintroduce Cellular Complex Contrastive Learning with Adaptive Trimming\n(CellCLAT), a twofold framework designed to adhere to the combinatorial\nconstraints of cellular complexes while mitigating informational redundancy.\nSpecifically, we propose a parameter perturbation-based augmentation method\nthat injects controlled noise into cellular interactions without altering the\nunderlying cellular structures, thereby preserving cellular topology during\ncontrastive learning. Additionally, a cellular trimming scheduler is employed\nto mask gradient contributions from task-irrelevant cells through a bi-level\nmeta-learning approach, effectively removing redundant topological elements\nwhile maintaining critical higher-order semantics. We provide theoretical\njustification and empirical validation to demonstrate that CellCLAT achieves\nsubstantial improvements over existing self-supervised graph learning methods,\nmarking a significant attempt in this domain."}
{"id": "2505.22491", "pdf": "https://arxiv.org/pdf/2505.22491", "abs": "https://arxiv.org/abs/2505.22491", "authors": ["Moritz Haas", "Sebastian Bordt", "Ulrike von Luxburg", "Leena Chennuru Vankadara"], "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The dominant paradigm for training large-scale vision and language models is\nHe initialization and a single global learning rate (\\textit{standard\nparameterization}, SP). Despite its practical success, standard parametrization\nremains poorly understood from a theoretical perspective: Existing\ninfinite-width theory would predict instability under large learning rates and\nvanishing feature learning under stable learning rates. However, empirically\noptimal learning rates consistently decay much slower than theoretically\npredicted. By carefully studying neural network training dynamics, we\ndemonstrate that this discrepancy is not fully explained by finite-width\nphenomena such as catapult effects or a lack of alignment between weights and\nincoming activations. We instead show that the apparent contradiction can be\nfundamentally resolved by taking the loss function into account: In contrast to\nMean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an\nintermediate \\textit{controlled divergence} regime emerges, where logits\ndiverge but loss, gradients, and activations remain stable. Stable training\nunder large learning rates enables persistent feature evolution at scale in all\nhidden layers, which is crucial for the practical success of SP. In experiments\nacross optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities\n(vision, language), we validate that neural networks operate in this controlled\ndivergence regime under CE loss but not under MSE loss. Our empirical evidence\nsuggests that width-scaling considerations are surprisingly useful for\npredicting empirically optimal learning rate exponents. Finally, our analysis\nclarifies the effectiveness and limitations of recently proposed layerwise\nlearning rate scalings for standard initialization."}
{"id": "2505.21584", "pdf": "https://arxiv.org/pdf/2505.21584", "abs": "https://arxiv.org/abs/2505.21584", "authors": ["Afaf Taik", "Khaoula Chehbouni", "Golnoosh Farnadi"], "title": "Fairness in Federated Learning: Fairness for Whom?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Fairness in federated learning has emerged as a rapidly growing area of\nresearch, with numerous works proposing formal definitions and algorithmic\ninterventions. Yet, despite this technical progress, fairness in FL is often\ndefined and evaluated in ways that abstract away from the sociotechnical\ncontexts in which these systems are deployed. In this paper, we argue that\nexisting approaches tend to optimize narrow system level metrics, such as\nperformance parity or contribution-based rewards, while overlooking how harms\narise throughout the FL lifecycle and how they impact diverse stakeholders. We\nsupport this claim through a critical analysis of the literature, based on a\nsystematic annotation of papers for their fairness definitions, design\ndecisions, evaluation practices, and motivating use cases. Our analysis reveals\nfive recurring pitfalls: 1) fairness framed solely through the lens of server\nclient architecture, 2) a mismatch between simulations and motivating use-cases\nand contexts, 3) definitions that conflate protecting the system with\nprotecting its users, 4) interventions that target isolated stages of the\nlifecycle while neglecting upstream and downstream effects, 5) and a lack of\nmulti-stakeholder alignment where multiple fairness definitions can be relevant\nat once. Building on these insights, we propose a harm centered framework that\nlinks fairness definitions to concrete risks and stakeholder vulnerabilities.\nWe conclude with recommendations for more holistic, context-aware, and\naccountable fairness research in FL."}
{"id": "2505.21591", "pdf": "https://arxiv.org/pdf/2505.21591", "abs": "https://arxiv.org/abs/2505.21591", "authors": ["Maosen Zhao", "Pengtao Chen", "Chong Yu", "Yan Wen", "Xudong Tan", "Tao Chen"], "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model quantization reduces the bit-width of weights and activations,\nimproving memory efficiency and inference speed in diffusion models. However,\nachieving 4-bit quantization remains challenging. Existing methods, primarily\nbased on integer quantization and post-training quantization fine-tuning,\nstruggle with inconsistent performance. Inspired by the success of\nfloating-point (FP) quantization in large language models, we explore low-bit\nFP quantization for diffusion models and identify key challenges: the failure\nof signed FP quantization to handle asymmetric activation distributions, the\ninsufficient consideration of temporal complexity in the denoising process\nduring fine-tuning, and the misalignment between fine-tuning loss and\nquantization error. To address these challenges, we propose the mixup-sign\nfloating-point quantization (MSFP) framework, first introducing unsigned FP\nquantization in model quantization, along with timestep-aware LoRA (TALoRA) and\ndenoising-factor loss alignment (DFA), which ensure precise and stable\nfine-tuning. Extensive experiments show that we are the first to achieve\nsuperior performance in 4-bit FP quantization for diffusion models,\noutperforming existing PTQ fine-tuning methods in 4-bit INT quantization."}
{"id": "2505.22492", "pdf": "https://arxiv.org/pdf/2505.22492", "abs": "https://arxiv.org/abs/2505.22492", "authors": ["Hongyi Zhou", "Josiah P. Hanna", "Jin Zhu", "Ying Yang", "Chengchun Shi"], "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "This paper studies off-policy evaluation (OPE) in reinforcement learning with\na focus on behavior policy estimation for importance sampling. Prior work has\nshown empirically that estimating a history-dependent behavior policy can lead\nto lower mean squared error (MSE) even when the true behavior policy is\nMarkovian. However, the question of why the use of history should lower MSE\nremains open. In this paper, we theoretically demystify this paradox by\nderiving a bias-variance decomposition of the MSE of ordinary importance\nsampling (IS) estimators, demonstrating that history-dependent behavior policy\nestimation decreases their asymptotic variances while increasing their\nfinite-sample biases. Additionally, as the estimated behavior policy conditions\non a longer history, we show a consistent decrease in variance. We extend these\nfindings to a range of other OPE estimators, including the sequential IS\nestimator, the doubly robust estimator and the marginalized IS estimator, with\nthe behavior policy estimated either parametrically or non-parametrically."}
{"id": "2505.21587", "pdf": "https://arxiv.org/pdf/2505.21587", "abs": "https://arxiv.org/abs/2505.21587", "authors": ["Bin Qin", "Qirui Ji", "Jiangmeng Li", "Yupeng Wang", "Xuesong Wu", "Jianwen Cao", "Fanjiang Xu"], "title": "CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised topological deep learning (TDL) represents a nascent but\nunderexplored area with significant potential for modeling higher-order\ninteractions in simplicial complexes and cellular complexes to derive\nrepresentations of unlabeled graphs. Compared to simplicial complexes, cellular\ncomplexes exhibit greater expressive power. However, the advancement in\nself-supervised learning for cellular TDL is largely hindered by two core\nchallenges: \\textit{extrinsic structural constraints} inherent to cellular\ncomplexes, and intrinsic semantic redundancy in cellular representations. The\nfirst challenge highlights that traditional graph augmentation techniques may\ncompromise the integrity of higher-order cellular interactions, while the\nsecond underscores that topological redundancy in cellular complexes\npotentially diminish task-relevant information. To address these issues, we\nintroduce Cellular Complex Contrastive Learning with Adaptive Trimming\n(CellCLAT), a twofold framework designed to adhere to the combinatorial\nconstraints of cellular complexes while mitigating informational redundancy.\nSpecifically, we propose a parameter perturbation-based augmentation method\nthat injects controlled noise into cellular interactions without altering the\nunderlying cellular structures, thereby preserving cellular topology during\ncontrastive learning. Additionally, a cellular trimming scheduler is employed\nto mask gradient contributions from task-irrelevant cells through a bi-level\nmeta-learning approach, effectively removing redundant topological elements\nwhile maintaining critical higher-order semantics. We provide theoretical\njustification and empirical validation to demonstrate that CellCLAT achieves\nsubstantial improvements over existing self-supervised graph learning methods,\nmarking a significant attempt in this domain."}
{"id": "2505.21595", "pdf": "https://arxiv.org/pdf/2505.21595", "abs": "https://arxiv.org/abs/2505.21595", "authors": ["Shreyas Gururaj", "Lars Grüne", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "title": "Relevance-driven Input Dropout: an Explanation-guided Regularization Technique", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": null, "summary": "Overfitting is a well-known issue extending even to state-of-the-art (SOTA)\nMachine Learning (ML) models, resulting in reduced generalization, and a\nsignificant train-test performance gap. Mitigation measures include a\ncombination of dropout, data augmentation, weight decay, and other\nregularization techniques. Among the various data augmentation strategies,\nocclusion is a prominent technique that typically focuses on randomly masking\nregions of the input during training. Most of the existing literature\nemphasizes randomness in selecting and modifying the input features instead of\nregions that strongly influence model decisions. We propose Relevance-driven\nInput Dropout (RelDrop), a novel data augmentation method which selectively\noccludes the most relevant regions of the input, nudging the model to use other\nimportant features in the prediction process, thus improving model\ngeneralization through informed regularization. We further conduct qualitative\nand quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)\naffects model decision-making. Through a series of experiments on benchmark\ndatasets, we demonstrate that our approach improves robustness towards\nocclusion, results in models utilizing more features within the region of\ninterest, and boosts inference time generalization performance. Our code is\navailable at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout."}
{"id": "2505.22538", "pdf": "https://arxiv.org/pdf/2505.22538", "abs": "https://arxiv.org/abs/2505.22538", "authors": ["Paul Hofman", "Yusuf Sale", "Eyke Hüllermeier"], "title": "Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We address the problem of uncertainty quantification and propose measures of\ntotal, aleatoric, and epistemic uncertainty based on a known decomposition of\n(strictly) proper scoring rules, a specific type of loss function, into a\ndivergence and an entropy component. This leads to a flexible framework for\nuncertainty quantification that can be instantiated with different losses\n(scoring rules), which makes it possible to tailor uncertainty quantification\nto the use case at hand. We show that this flexibility is indeed advantageous.\nIn particular, we analyze the task of selective prediction and show that the\nscoring rule should ideally match the task loss. In addition, we perform\nexperiments on two other common tasks. For out-of-distribution detection, our\nresults confirm that a widely used measure of epistemic uncertainty, mutual\ninformation, performs best. Moreover, in the setting of active learning, our\nmeasure of epistemic uncertainty based on the zero-one-loss consistently\noutperforms other uncertainty measures."}
{"id": "2505.21591", "pdf": "https://arxiv.org/pdf/2505.21591", "abs": "https://arxiv.org/abs/2505.21591", "authors": ["Maosen Zhao", "Pengtao Chen", "Chong Yu", "Yan Wen", "Xudong Tan", "Tao Chen"], "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model quantization reduces the bit-width of weights and activations,\nimproving memory efficiency and inference speed in diffusion models. However,\nachieving 4-bit quantization remains challenging. Existing methods, primarily\nbased on integer quantization and post-training quantization fine-tuning,\nstruggle with inconsistent performance. Inspired by the success of\nfloating-point (FP) quantization in large language models, we explore low-bit\nFP quantization for diffusion models and identify key challenges: the failure\nof signed FP quantization to handle asymmetric activation distributions, the\ninsufficient consideration of temporal complexity in the denoising process\nduring fine-tuning, and the misalignment between fine-tuning loss and\nquantization error. To address these challenges, we propose the mixup-sign\nfloating-point quantization (MSFP) framework, first introducing unsigned FP\nquantization in model quantization, along with timestep-aware LoRA (TALoRA) and\ndenoising-factor loss alignment (DFA), which ensure precise and stable\nfine-tuning. Extensive experiments show that we are the first to achieve\nsuperior performance in 4-bit FP quantization for diffusion models,\noutperforming existing PTQ fine-tuning methods in 4-bit INT quantization."}
{"id": "2505.21605", "pdf": "https://arxiv.org/pdf/2505.21605", "abs": "https://arxiv.org/abs/2505.21605", "authors": ["Fengqing Jiang", "Fengbo Ma", "Zhangchen Xu", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bo Li", "Xianyan Chen", "Zhen Xiang", "Radha Poovendran"], "title": "SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) exhibit advancing capabilities in complex tasks,\nsuch as reasoning and graduate-level question answering, yet their resilience\nagainst misuse, particularly involving scientifically sophisticated risks,\nremains underexplored. Existing safety benchmarks typically focus either on\ninstructions requiring minimal knowledge comprehension (e.g., ``tell me how to\nbuild a bomb\") or utilize prompts that are relatively low-risk (e.g.,\nmultiple-choice or classification tasks about hazardous content). Consequently,\nthey fail to adequately assess model safety when handling knowledge-intensive,\nhazardous scenarios.\n  To address this critical gap, we introduce SOSBench, a regulation-grounded,\nhazard-focused benchmark encompassing six high-risk scientific domains:\nchemistry, biology, medicine, pharmacology, physics, and psychology. The\nbenchmark comprises 3,000 prompts derived from real-world regulations and laws,\nsystematically expanded via an LLM-assisted evolutionary pipeline that\nintroduces diverse, realistic misuse scenarios (e.g., detailed explosive\nsynthesis instructions involving advanced chemical formulas). We evaluate\nfrontier models within a unified evaluation framework using our SOSBench.\nDespite their alignment claims, advanced models consistently disclose\npolicy-violating content across all domains, demonstrating alarmingly high\nrates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).\nThese results highlight significant safety alignment deficiencies and\nunderscore urgent concerns regarding the responsible deployment of powerful\nLLMs."}
{"id": "2505.21595", "pdf": "https://arxiv.org/pdf/2505.21595", "abs": "https://arxiv.org/abs/2505.21595", "authors": ["Shreyas Gururaj", "Lars Grüne", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "title": "Relevance-driven Input Dropout: an Explanation-guided Regularization Technique", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": null, "summary": "Overfitting is a well-known issue extending even to state-of-the-art (SOTA)\nMachine Learning (ML) models, resulting in reduced generalization, and a\nsignificant train-test performance gap. Mitigation measures include a\ncombination of dropout, data augmentation, weight decay, and other\nregularization techniques. Among the various data augmentation strategies,\nocclusion is a prominent technique that typically focuses on randomly masking\nregions of the input during training. Most of the existing literature\nemphasizes randomness in selecting and modifying the input features instead of\nregions that strongly influence model decisions. We propose Relevance-driven\nInput Dropout (RelDrop), a novel data augmentation method which selectively\noccludes the most relevant regions of the input, nudging the model to use other\nimportant features in the prediction process, thus improving model\ngeneralization through informed regularization. We further conduct qualitative\nand quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)\naffects model decision-making. Through a series of experiments on benchmark\ndatasets, we demonstrate that our approach improves robustness towards\nocclusion, results in models utilizing more features within the region of\ninterest, and boosts inference time generalization performance. Our code is\navailable at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout."}
{"id": "2505.21640", "pdf": "https://arxiv.org/pdf/2505.21640", "abs": "https://arxiv.org/abs/2505.21640", "authors": ["Oren Mangoubi", "Neil He", "Nisheeth K. Vishnoi"], "title": "Efficient Diffusion Models for Symmetric Manifolds", "categories": ["cs.LG", "cs.AI", "cs.DS", "math.PR", "stat.ML"], "comment": "The conference version of this paper appears in ICML 2025", "summary": "We introduce a framework for designing efficient diffusion models for\n$d$-dimensional symmetric-space Riemannian manifolds, including the torus,\nsphere, special orthogonal group and unitary group. Existing manifold diffusion\nmodels often depend on heat kernels, which lack closed-form expressions and\nrequire either $d$ gradient evaluations or exponential-in-$d$ arithmetic\noperations per training step. We introduce a new diffusion model for symmetric\nmanifolds with a spatially-varying covariance, allowing us to leverage a\nprojection of Euclidean Brownian motion to bypass heat kernel computations. Our\ntraining algorithm minimizes a novel efficient objective derived via Ito's\nLemma, allowing each step to run in $O(1)$ gradient evaluations and\nnearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap\nbetween diffusions on symmetric manifolds and Euclidean space. Manifold\nsymmetries ensure the diffusion satisfies an \"average-case\" Lipschitz\ncondition, enabling accurate and efficient sample generation. Empirically, our\nmodel outperforms prior methods in training speed and improves sample quality\non synthetic datasets on the torus, special orthogonal group, and unitary\ngroup."}
{"id": "2505.21605", "pdf": "https://arxiv.org/pdf/2505.21605", "abs": "https://arxiv.org/abs/2505.21605", "authors": ["Fengqing Jiang", "Fengbo Ma", "Zhangchen Xu", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bo Li", "Xianyan Chen", "Zhen Xiang", "Radha Poovendran"], "title": "SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) exhibit advancing capabilities in complex tasks,\nsuch as reasoning and graduate-level question answering, yet their resilience\nagainst misuse, particularly involving scientifically sophisticated risks,\nremains underexplored. Existing safety benchmarks typically focus either on\ninstructions requiring minimal knowledge comprehension (e.g., ``tell me how to\nbuild a bomb\") or utilize prompts that are relatively low-risk (e.g.,\nmultiple-choice or classification tasks about hazardous content). Consequently,\nthey fail to adequately assess model safety when handling knowledge-intensive,\nhazardous scenarios.\n  To address this critical gap, we introduce SOSBench, a regulation-grounded,\nhazard-focused benchmark encompassing six high-risk scientific domains:\nchemistry, biology, medicine, pharmacology, physics, and psychology. The\nbenchmark comprises 3,000 prompts derived from real-world regulations and laws,\nsystematically expanded via an LLM-assisted evolutionary pipeline that\nintroduces diverse, realistic misuse scenarios (e.g., detailed explosive\nsynthesis instructions involving advanced chemical formulas). We evaluate\nfrontier models within a unified evaluation framework using our SOSBench.\nDespite their alignment claims, advanced models consistently disclose\npolicy-violating content across all domains, demonstrating alarmingly high\nrates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).\nThese results highlight significant safety alignment deficiencies and\nunderscore urgent concerns regarding the responsible deployment of powerful\nLLMs."}
{"id": "2505.21666", "pdf": "https://arxiv.org/pdf/2505.21666", "abs": "https://arxiv.org/abs/2505.21666", "authors": ["Owen Oertell", "Shikun Sun", "Yiding Chen", "Jin Peng Zhou", "Zhiyong Wang", "Wen Sun"], "title": "Efficient Controllable Diffusion via Optimal Classifier Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 9 figures, 3 tables", "summary": "The controllable generation of diffusion models aims to steer the model to\ngenerate samples that optimize some given objective functions. It is desirable\nfor a variety of applications including image generation, molecule generation,\nand DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of\nthe base model is a popular approach but it can overfit the reward function\nwhile requiring significant resources. We frame controllable generation as a\nproblem of finding a distribution that optimizes a KL-regularized objective\nfunction. We present SLCD -- Supervised Learning based Controllable Diffusion,\nwhich iteratively generates online data and trains a small classifier to guide\nthe generation of the diffusion model. Similar to the standard\nclassifier-guided diffusion, SLCD's key computation primitive is classification\nand does not involve any complex concepts from RL or control. Via a reduction\nto no-regret online learning analysis, we show that under KL divergence, the\noutput from SLCD provably converges to the optimal solution of the\nKL-regularized objective. Further, we empirically demonstrate that SLCD can\ngenerate high quality samples with nearly the same inference time as the base\nmodel in both image generation with continuous diffusion and biological\nsequence generation with discrete diffusion. Our code is available at\nhttps://github.com/Owen-Oertell/slcd"}
{"id": "2505.21626", "pdf": "https://arxiv.org/pdf/2505.21626", "abs": "https://arxiv.org/abs/2505.21626", "authors": ["Nicolas Guerra", "Nicholas H. Nelsen", "Yunan Yang"], "title": "Learning Where to Learn: Training Distribution Selection for Provable OOD Performance", "categories": ["cs.LG", "math.OC", "stat.ML", "62K05, 65K10 (Primary) 68T07, 65D15, 62R20, 60G57 (Secondary)"], "comment": "32 pages, 8 figures, 2 tables, 3 algorithms", "summary": "Out-of-distribution (OOD) generalization remains a fundamental challenge in\nmachine learning. Models trained on one data distribution often experience\nsubstantial performance degradation when evaluated on shifted or unseen\ndomains. To address this challenge, the present paper studies the design of\ntraining data distributions that maximize average-case OOD performance. First,\na theoretical analysis establishes a family of generalization bounds that\nquantify how the choice of training distribution influences OOD error across a\npredefined family of target distributions. These insights motivate the\nintroduction of two complementary algorithmic strategies: (i) directly\nformulating OOD risk minimization as a bilevel optimization problem over the\nspace of probability measures and (ii) minimizing a theoretical upper bound on\nOOD error. Last, the paper evaluates the two approaches across a range of\nfunction approximation and operator learning examples. The proposed methods\nsignificantly improve OOD accuracy over standard empirical risk minimization\nwith a fixed distribution. These results highlight the potential of\ndistribution-aware training as a principled and practical framework for robust\nOOD generalization."}
{"id": "2505.21677", "pdf": "https://arxiv.org/pdf/2505.21677", "abs": "https://arxiv.org/abs/2505.21677", "authors": ["Hung Ahn Vu", "Galen Reeves", "Emily Wenger"], "title": "What happens when generative AI models train recursively on each others' generated outputs?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "9 pages", "summary": "The internet is full of AI-generated content while also serving as a common\nsource of training data for generative AI (genAI) models. This duality raises\nthe possibility that future genAI models may be trained on other models'\ngenerated outputs. Prior work has studied consequences of models training on\ntheir own generated outputs, but limited work has considered what happens if\nmodels ingest content produced by other models. Given society's increasing\ndependence on genAI tools, understanding downstream effects of such\ndata-mediated model interactions is critical. To this end, we provide empirical\nevidence for how data-mediated interactions might unfold in practice, develop a\ntheoretical model for this interactive training process, and show\nexperimentally possible long-term results of such interactions. We find that\ndata-mediated interactions can benefit models by exposing them to novel\nconcepts perhaps missed in original training data, but also can homogenize\ntheir performance on shared tasks."}
{"id": "2505.21639", "pdf": "https://arxiv.org/pdf/2505.21639", "abs": "https://arxiv.org/abs/2505.21639", "authors": ["Mauricio Junca", "Esteban Leiva"], "title": "Apprenticeship learning with prior beliefs using inverse optimization", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "The relationship between inverse reinforcement learning (IRL) and inverse\noptimization (IO) for Markov decision processes (MDPs) has been relatively\nunderexplored in the literature, despite addressing the same problem. In this\nwork, we revisit the relationship between the IO framework for MDPs, IRL, and\napprenticeship learning (AL). We incorporate prior beliefs on the structure of\nthe cost function into the IRL and AL problems, and demonstrate that the\nconvex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a\nrelaxation of our framework. Notably, the AL formalism is a special case in our\nframework when the regularization term is absent. Focusing on the suboptimal\nexpert setting, we formulate the AL problem as a regularized min-max problem.\nThe regularizer plays a key role in addressing the ill-posedness of IRL by\nguiding the search for plausible cost functions. To solve the resulting\nregularized-convex-concave-min-max problem, we use stochastic mirror descent\n(SMD) and establish convergence bounds for the proposed method. Numerical\nexperiments highlight the critical role of regularization in learning cost\nvectors and apprentice policies."}
{"id": "2505.21680", "pdf": "https://arxiv.org/pdf/2505.21680", "abs": "https://arxiv.org/abs/2505.21680", "authors": ["Andrew J. Loza", "Jun Yup Kim", "Shangzheng Song", "Yihang Liu", "Joseph J. Y. Sung", "R Andrew Taylor", "Dennis L. Shung"], "title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; I.5.1"], "comment": "15 pates, 5 figures", "summary": "Real-world processes often generate data that are a mix of categorical and\nnumeric values that are recorded at irregular and informative intervals.\nDiscrete token-based approaches are limited in numeric representation capacity\nwhile methods like neural ordinary differential equations are not well suited\nfor categorical data or informative sampling and require augmentation to handle\ncertain classes of trajectories. Here, we present multivariateGPT, a single\narchitecture for modeling sequences of mixed categorical (including tokenized\ntext) and numeric data. This is accomplished with an autoregressive sequence\ndecomposition, embedding scheme, and loss function that extend the next token\nprediction task to likelihood estimation of the joint distribution of next\ntoken class and value. We demonstrate how this approach can efficiently learn\nto generalize patterns in simple physical systems and model complex time series\nincluding electrocardiograms and multivariate electronic health record data.\nThis work extends the utility of transformer based models to additional classes\nof data."}
{"id": "2505.21640", "pdf": "https://arxiv.org/pdf/2505.21640", "abs": "https://arxiv.org/abs/2505.21640", "authors": ["Oren Mangoubi", "Neil He", "Nisheeth K. Vishnoi"], "title": "Efficient Diffusion Models for Symmetric Manifolds", "categories": ["cs.LG", "cs.AI", "cs.DS", "math.PR", "stat.ML"], "comment": "The conference version of this paper appears in ICML 2025", "summary": "We introduce a framework for designing efficient diffusion models for\n$d$-dimensional symmetric-space Riemannian manifolds, including the torus,\nsphere, special orthogonal group and unitary group. Existing manifold diffusion\nmodels often depend on heat kernels, which lack closed-form expressions and\nrequire either $d$ gradient evaluations or exponential-in-$d$ arithmetic\noperations per training step. We introduce a new diffusion model for symmetric\nmanifolds with a spatially-varying covariance, allowing us to leverage a\nprojection of Euclidean Brownian motion to bypass heat kernel computations. Our\ntraining algorithm minimizes a novel efficient objective derived via Ito's\nLemma, allowing each step to run in $O(1)$ gradient evaluations and\nnearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap\nbetween diffusions on symmetric manifolds and Euclidean space. Manifold\nsymmetries ensure the diffusion satisfies an \"average-case\" Lipschitz\ncondition, enabling accurate and efficient sample generation. Empirically, our\nmodel outperforms prior methods in training speed and improves sample quality\non synthetic datasets on the torus, special orthogonal group, and unitary\ngroup."}
{"id": "2505.21717", "pdf": "https://arxiv.org/pdf/2505.21717", "abs": "https://arxiv.org/abs/2505.21717", "authors": ["Mónika Farsang", "Ramin Hasani", "Radu Grosu"], "title": "Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "We present LrcSSM, a \\textit{nonlinear} recurrent model that processes long\nsequences as fast as today's linear state-space layers. By forcing the\nstate-transition matrix to be diagonal and learned at every step, the full\nsequence can be solved in parallel with a single prefix-scan, giving\n$\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$ sequential\ndepth, for input-sequence length $T$ and a state dimension $D$. Moreover,\nLrcSSM offers a formal gradient-stability guarantee that other input-varying\nsystems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth\n$L$, as the forward and backward passes cost $\\Theta(T\\,D\\,L)$ FLOPs, with its\nlow sequential depth and parameter count $\\Theta(D\\,L)$, the model follows the\ncompute-optimal scaling law regime ($\\beta \\approx 0.42$) recently observed for\nMamba, outperforming quadratic-attention Transformers at equal compute while\navoiding the memory overhead of FFT-based long convolutions. We show that on a\nseries of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba."}
{"id": "2505.21641", "pdf": "https://arxiv.org/pdf/2505.21641", "abs": "https://arxiv.org/abs/2505.21641", "authors": ["Maresa Schröder", "Justin Hartenstein", "Stefan Feuerriegel"], "title": "PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects", "categories": ["cs.LG", "cs.CR", "stat.ME"], "comment": null, "summary": "The average treatment effect (ATE) is widely used to evaluate the\neffectiveness of drugs and other medical interventions. In safety-critical\napplications like medicine, reliable inferences about the ATE typically require\nvalid uncertainty quantification, such as through confidence intervals (CIs).\nHowever, estimating treatment effects in these settings often involves\nsensitive data that must be kept private. In this work, we present PrivATE, a\nnovel machine learning framework for computing CIs for the ATE under\ndifferential privacy. Specifically, we focus on deriving valid\nprivacy-preserving CIs for the ATE from observational data. Our PrivATE\nframework consists of three steps: (i) estimating a differentially private ATE\nthrough output perturbation; (ii) estimating the differentially private\nvariance through a truncated output perturbation mechanism; and (iii)\nconstructing the CIs while accounting for the uncertainty from both the\nestimation and privatization steps. Our PrivATE framework is model agnostic,\ndoubly robust, and ensures valid CIs. We demonstrate the effectiveness of our\nframework using synthetic and real-world medical datasets. To the best of our\nknowledge, we are the first to derive a general, doubly robust framework for\nvalid CIs of the ATE under ($\\varepsilon$, $\\delta$)-differential privacy."}
{"id": "2505.21722", "pdf": "https://arxiv.org/pdf/2505.21722", "abs": "https://arxiv.org/abs/2505.21722", "authors": ["Ioannis Bantzis", "James B. Simon", "Arthur Jacot"], "title": "Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When a deep ReLU network is initialized with small weights, GD is at first\ndominated by the saddle at the origin in parameter space. We study the\nso-called escape directions, which play a similar role as the eigenvectors of\nthe Hessian for strict saddles. We show that the optimal escape direction\nfeatures a low-rank bias in its deeper layers: the first singular value of the\n$\\ell$-th layer weight matrix is at least $\\ell^{\\frac{1}{4}}$ larger than any\nother singular value. We also prove a number of related results about these\nescape directions. We argue that this result is a first step in proving\nSaddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of\nsaddles with increasing bottleneck rank."}
{"id": "2505.21651", "pdf": "https://arxiv.org/pdf/2505.21651", "abs": "https://arxiv.org/abs/2505.21651", "authors": ["Nikola Surjanovic", "Alexandre Bouchard-Côté", "Trevor Campbell"], "title": "AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent", "categories": ["cs.LG", "math.OC", "stat.CO", "stat.ML"], "comment": null, "summary": "The learning rate is an important tuning parameter for stochastic gradient\ndescent (SGD) and can greatly influence its performance. However, appropriate\nselection of a learning rate schedule across all iterations typically requires\na non-trivial amount of user tuning effort. To address this, we introduce\nAutoSGD: an SGD method that automatically determines whether to increase or\ndecrease the learning rate at a given iteration and then takes appropriate\naction. We introduce theory supporting the convergence of AutoSGD, along with\nits deterministic counterpart for standard gradient descent. Empirical results\nsuggest strong performance of the method on a variety of traditional\noptimization problems and machine learning tasks."}
{"id": "2505.21731", "pdf": "https://arxiv.org/pdf/2505.21731", "abs": "https://arxiv.org/abs/2505.21731", "authors": ["Quentin Delfosse", "Jannis Blüml", "Fabian Tatai", "Théo Vincent", "Bjarne Gregori", "Elisabeth Dillies", "Jan Peters", "Constantin Rothkopf", "Kristian Kersting"], "title": "Deep Reinforcement Learning Agents are not even close to Human Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": "49 pages in total, 5 main figures, 14 figures total", "summary": "Deep reinforcement learning (RL) agents achieve impressive results in a wide\nvariety of tasks, but they lack zero-shot adaptation capabilities. While most\nrobustness evaluations focus on tasks complexifications, for which human also\nstruggle to maintain performances, no evaluation has been performed on tasks\nsimplifications. To tackle this issue, we introduce HackAtari, a set of task\nvariations of the Arcade Learning Environments. We use it to demonstrate that,\ncontrary to humans, RL agents systematically exhibit huge performance drops on\nsimpler versions of their training tasks, uncovering agents' consistent\nreliance on shortcuts. Our analysis across multiple algorithms and\narchitectures highlights the persistent gap between RL agents and human\nbehavioral intelligence, underscoring the need for new benchmarks and\nmethodologies that enforce systematic generalization testing beyond static\nevaluation protocols. Training and testing in the same environment is not\nenough to obtain agents equipped with human-like intelligence."}
{"id": "2505.21660", "pdf": "https://arxiv.org/pdf/2505.21660", "abs": "https://arxiv.org/abs/2505.21660", "authors": ["Xiaojie Xu", "Xinli Xu", "Sirui Chen", "Haoyu Chen", "Fan Zhang", "Ying-Cong Chen"], "title": "PreGenie: An Agentic Framework for High-quality Visual Presentation Generation", "categories": ["cs.LG"], "comment": "11 pages, 9 figures", "summary": "Visual presentations are vital for effective communication. Early attempts to\nautomate their creation using deep learning often faced issues such as poorly\norganized layouts, inaccurate text summarization, and a lack of image\nunderstanding, leading to mismatched visuals and text. These limitations\nrestrict their application in formal contexts like business and scientific\nresearch. To address these challenges, we propose PreGenie, an agentic and\nmodular framework powered by multimodal large language models (MLLMs) for\ngenerating high-quality visual presentations.\n  PreGenie is built on the Slidev presentation framework, where slides are\nrendered from Markdown code. It operates in two stages: (1) Analysis and\nInitial Generation, which summarizes multimodal input and generates initial\ncode, and (2) Review and Re-generation, which iteratively reviews intermediate\ncode and rendered slides to produce final, high-quality presentations. Each\nstage leverages multiple MLLMs that collaborate and share information.\nComprehensive experiments demonstrate that PreGenie excels in multimodal\nunderstanding, outperforming existing models in both aesthetics and content\nconsistency, while aligning more closely with human design preferences."}
{"id": "2505.21743", "pdf": "https://arxiv.org/pdf/2505.21743", "abs": "https://arxiv.org/abs/2505.21743", "authors": ["Zihao Li", "Xinyuan Cao", "Xiangbo Gao", "Kexin Tian", "Keshu Wu", "Mohammad Anis", "Hao Zhang", "Keke Long", "Jiwan Jiang", "Xiaopeng Li", "Yunlong Zhang", "Tianbao Yang", "Dominique Lord", "Zhengzhong Tu", "Yang Zhou"], "title": "Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic safety science has long been hindered by a fundamental data paradox:\nthe crashes we most wish to prevent are precisely those events we rarely\nobserve. Existing crash-frequency models and surrogate safety metrics rely\nheavily on sparse, noisy, and under-reported records, while even sophisticated,\nhigh-fidelity simulations undersample the long-tailed situations that trigger\ncatastrophic outcomes such as fatalities. We argue that the path to achieving\nVision Zero, i.e., the complete elimination of traffic fatalities and severe\ninjuries, requires a paradigm shift from traditional crash-only learning to a\nnew form of counterfactual safety learning: reasoning not only about what\nhappened, but also about the vast set of plausible yet perilous scenarios that\ncould have happened under slightly different circumstances. To operationalize\nthis shift, our proposed agenda bridges macro to micro. Guided by crash-rate\npriors, generative scene engines, diverse driver models, and causal learning,\nnear-miss events are synthesized and explained. A crash-focused digital twin\ntestbed links micro scenes to macro patterns, while a multi-objective validator\nensures that simulations maintain statistical realism. This pipeline transforms\nsparse crash data into rich signals for crash prediction, enabling the\nstress-testing of vehicles, roads, and policies before deployment. By learning\nfrom crashes that almost happened, we can shift traffic safety from reactive\nforensics to proactive prevention, advancing Vision Zero."}
{"id": "2505.21666", "pdf": "https://arxiv.org/pdf/2505.21666", "abs": "https://arxiv.org/abs/2505.21666", "authors": ["Owen Oertell", "Shikun Sun", "Yiding Chen", "Jin Peng Zhou", "Zhiyong Wang", "Wen Sun"], "title": "Efficient Controllable Diffusion via Optimal Classifier Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 9 figures, 3 tables", "summary": "The controllable generation of diffusion models aims to steer the model to\ngenerate samples that optimize some given objective functions. It is desirable\nfor a variety of applications including image generation, molecule generation,\nand DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of\nthe base model is a popular approach but it can overfit the reward function\nwhile requiring significant resources. We frame controllable generation as a\nproblem of finding a distribution that optimizes a KL-regularized objective\nfunction. We present SLCD -- Supervised Learning based Controllable Diffusion,\nwhich iteratively generates online data and trains a small classifier to guide\nthe generation of the diffusion model. Similar to the standard\nclassifier-guided diffusion, SLCD's key computation primitive is classification\nand does not involve any complex concepts from RL or control. Via a reduction\nto no-regret online learning analysis, we show that under KL divergence, the\noutput from SLCD provably converges to the optimal solution of the\nKL-regularized objective. Further, we empirically demonstrate that SLCD can\ngenerate high quality samples with nearly the same inference time as the base\nmodel in both image generation with continuous diffusion and biological\nsequence generation with discrete diffusion. Our code is available at\nhttps://github.com/Owen-Oertell/slcd"}
{"id": "2505.21775", "pdf": "https://arxiv.org/pdf/2505.21775", "abs": "https://arxiv.org/abs/2505.21775", "authors": ["Michael Klamkin", "Arnaud Deza", "Sikai Cheng", "Haoruo Zhao", "Pascal Van Hentenryck"], "title": "DualSchool: How Reliable are LLMs for Optimization Education?", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Consider the following task taught in introductory optimization courses which\naddresses challenges articulated by the community at the intersection of\n(generative) AI and OR: generate the dual of a linear program. LLMs, being\ntrained at web-scale, have the conversion process and many instances of Primal\nto Dual Conversion (P2DC) at their disposal. Students may thus reasonably\nexpect that LLMs would perform well on the P2DC task. To assess this\nexpectation, this paper introduces DualSchool, a comprehensive framework for\ngenerating and verifying P2DC instances. The verification procedure of\nDualSchool uses the Canonical Graph Edit Distance, going well beyond existing\nevaluation methods for optimization models, which exhibit many false positives\nand negatives when applied to P2DC. Experiments performed by DualSchool reveal\ninteresting findings. Although LLMs can recite the conversion procedure\naccurately, state-of-the-art open LLMs fail to consistently produce correct\nduals. This finding holds even for the smallest two-variable instances and for\nderivative tasks, such as correctness, verification, and error classification.\nThe paper also discusses the implications for educators, students, and the\ndevelopment of large reasoning systems."}
{"id": "2505.21677", "pdf": "https://arxiv.org/pdf/2505.21677", "abs": "https://arxiv.org/abs/2505.21677", "authors": ["Hung Ahn Vu", "Galen Reeves", "Emily Wenger"], "title": "What happens when generative AI models train recursively on each others' generated outputs?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "9 pages", "summary": "The internet is full of AI-generated content while also serving as a common\nsource of training data for generative AI (genAI) models. This duality raises\nthe possibility that future genAI models may be trained on other models'\ngenerated outputs. Prior work has studied consequences of models training on\ntheir own generated outputs, but limited work has considered what happens if\nmodels ingest content produced by other models. Given society's increasing\ndependence on genAI tools, understanding downstream effects of such\ndata-mediated model interactions is critical. To this end, we provide empirical\nevidence for how data-mediated interactions might unfold in practice, develop a\ntheoretical model for this interactive training process, and show\nexperimentally possible long-term results of such interactions. We find that\ndata-mediated interactions can benefit models by exposing them to novel\nconcepts perhaps missed in original training data, but also can homogenize\ntheir performance on shared tasks."}
{"id": "2505.21792", "pdf": "https://arxiv.org/pdf/2505.21792", "abs": "https://arxiv.org/abs/2505.21792", "authors": ["Yuanzhe Peng", "Jieming Bian", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Federated Learning (MFL) lies at the intersection of two pivotal\nresearch areas: leveraging complementary information from multiple modalities\nto improve downstream inference performance and enabling distributed training\nto enhance efficiency and preserve privacy. Despite the growing interest in\nMFL, there is currently no comprehensive taxonomy that organizes MFL through\nthe lens of different Federated Learning (FL) paradigms. This perspective is\nimportant because multimodal data introduces distinct challenges across various\nFL settings. These challenges, including modality heterogeneity, privacy\nheterogeneity, and communication inefficiency, are fundamentally different from\nthose encountered in traditional unimodal or non-FL scenarios. In this paper,\nwe systematically examine MFL within the context of three major FL paradigms:\nhorizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we\npresent the problem formulation, review representative training algorithms, and\nhighlight the most prominent challenge introduced by multimodal data in\ndistributed settings. We also discuss open challenges and provide insights for\nfuture research. By establishing this taxonomy, we aim to uncover the novel\nchallenges posed by multimodal data from the perspective of different FL\nparadigms and to offer a new lens through which to understand and advance the\ndevelopment of MFL."}
{"id": "2505.21680", "pdf": "https://arxiv.org/pdf/2505.21680", "abs": "https://arxiv.org/abs/2505.21680", "authors": ["Andrew J. Loza", "Jun Yup Kim", "Shangzheng Song", "Yihang Liu", "Joseph J. Y. Sung", "R Andrew Taylor", "Dennis L. Shung"], "title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; I.5.1"], "comment": "15 pates, 5 figures", "summary": "Real-world processes often generate data that are a mix of categorical and\nnumeric values that are recorded at irregular and informative intervals.\nDiscrete token-based approaches are limited in numeric representation capacity\nwhile methods like neural ordinary differential equations are not well suited\nfor categorical data or informative sampling and require augmentation to handle\ncertain classes of trajectories. Here, we present multivariateGPT, a single\narchitecture for modeling sequences of mixed categorical (including tokenized\ntext) and numeric data. This is accomplished with an autoregressive sequence\ndecomposition, embedding scheme, and loss function that extend the next token\nprediction task to likelihood estimation of the joint distribution of next\ntoken class and value. We demonstrate how this approach can efficiently learn\nto generalize patterns in simple physical systems and model complex time series\nincluding electrocardiograms and multivariate electronic health record data.\nThis work extends the utility of transformer based models to additional classes\nof data."}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models."}
{"id": "2505.21684", "pdf": "https://arxiv.org/pdf/2505.21684", "abs": "https://arxiv.org/abs/2505.21684", "authors": ["Joel Lidin", "Amir Sarfi", "Evangelos Pappas", "Samuel Dare", "Eugene Belilovsky", "Jacob Steeves"], "title": "Incentivizing Permissionless Distributed Learning of LLMs", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We describe an incentive system for distributed deep learning of foundational\nmodels where peers are rewarded for contributions. The incentive system,\n\\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to\ntrain a 1.2B LLM with completely permissionless contributions of\npseudo-gradients: no control over the users that can register or their\nhardware. \\textit{Gauntlet} can be applied to any synchronous distributed\ntraining scheme that relies on aggregating updates or pseudo-gradients. We rely\non a two-stage mechanism for fast filtering of peer uptime, reliability, and\nsynchronization, combined with the core component that estimates the loss\nbefore and after individual pseudo-gradient contributions. We utilized an\nOpenSkill rating system to track competitiveness of pseudo-gradient scores\nacross time. Finally, we introduce a novel mechanism to ensure peers on the\nnetwork perform unique computations. Our live 1.2B run, which has paid out\nreal-valued tokens to participants based on the value of their contributions,\nyielded a competitive (on a per-iteration basis) 1.2B model that demonstrates\nthe utility of our incentive system."}
{"id": "2505.21835", "pdf": "https://arxiv.org/pdf/2505.21835", "abs": "https://arxiv.org/abs/2505.21835", "authors": ["Xiangyu Chen", "Jing Liu", "Ye Wang", "Matthew Brand", "Pu", "Wang", "Toshiaki Koike-Akino"], "title": "TuneComp: Joint Fine-tuning and Compression for Large Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "Preliminary Work", "summary": "To reduce model size during post-training, compression methods, including\nknowledge distillation, low-rank approximation, and pruning, are often applied\nafter fine-tuning the model. However, sequential fine-tuning and compression\nsacrifices performance, while creating a larger than necessary model as an\nintermediate step. In this work, we aim to reduce this gap, by directly\nconstructing a smaller model while guided by the downstream task. We propose to\njointly fine-tune and compress the model by gradually distilling it to a pruned\nlow-rank structure. Experiments demonstrate that joint fine-tuning and\ncompression significantly outperforms other sequential compression methods."}
{"id": "2505.21695", "pdf": "https://arxiv.org/pdf/2505.21695", "abs": "https://arxiv.org/abs/2505.21695", "authors": ["Ganglou Xu"], "title": "AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated learning faces critical challenges in balancing communication\nefficiency and model accuracy. One key issue lies in the approximation of\nupdate errors without incurring high computational costs. In this paper, we\npropose a lightweight yet effective method called Gradient Difference\nApproximation (GDA), which leverages first-order information to estimate local\nerror trends without computing the full Hessian matrix. The proposed method\nforms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)\nframework and provides a unified error modeling strategy for large-scale\nmulti-step adaptive training environments."}
{"id": "2505.21841", "pdf": "https://arxiv.org/pdf/2505.21841", "abs": "https://arxiv.org/abs/2505.21841", "authors": ["Jiahui Zhu", "Kihyun Yu", "Dabeen Lee", "Xin Liu", "Honghao Wei"], "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 41 st International Conference on Machine Learning", "summary": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments."}
{"id": "2505.21717", "pdf": "https://arxiv.org/pdf/2505.21717", "abs": "https://arxiv.org/abs/2505.21717", "authors": ["Mónika Farsang", "Ramin Hasani", "Radu Grosu"], "title": "Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "We present LrcSSM, a \\textit{nonlinear} recurrent model that processes long\nsequences as fast as today's linear state-space layers. By forcing the\nstate-transition matrix to be diagonal and learned at every step, the full\nsequence can be solved in parallel with a single prefix-scan, giving\n$\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$ sequential\ndepth, for input-sequence length $T$ and a state dimension $D$. Moreover,\nLrcSSM offers a formal gradient-stability guarantee that other input-varying\nsystems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth\n$L$, as the forward and backward passes cost $\\Theta(T\\,D\\,L)$ FLOPs, with its\nlow sequential depth and parameter count $\\Theta(D\\,L)$, the model follows the\ncompute-optimal scaling law regime ($\\beta \\approx 0.42$) recently observed for\nMamba, outperforming quadratic-attention Transformers at equal compute while\navoiding the memory overhead of FFT-based long convolutions. We show that on a\nseries of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba."}
{"id": "2505.21852", "pdf": "https://arxiv.org/pdf/2505.21852", "abs": "https://arxiv.org/abs/2505.21852", "authors": ["Akifumi Wachi", "Kohei Miyaguchi", "Takumi Tanabe", "Rei Sato", "Youhei Akimoto"], "title": "A Provable Approach for End-to-End Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.RO", "math.IT"], "comment": "27 pages", "summary": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure\nthe safety of a policy throughout the entire process, from learning to\noperation. However, existing safe RL paradigms inherently struggle to achieve\nthis objective. We propose a method, called Provably Lifetime Safe RL (PLS),\nthat integrates offline safe RL with safe policy deployment to address this\nchallenge. Our proposed method learns a policy offline using return-conditioned\nsupervised learning and then deploys the resulting policy while cautiously\noptimizing a limited set of parameters, known as target returns, using Gaussian\nprocesses (GPs). Theoretically, we justify the use of GPs by analyzing the\nmathematical relationship between target and actual returns. We then prove that\nPLS finds near-optimal target returns while guaranteeing safety with high\nprobability. Empirically, we demonstrate that PLS outperforms baselines both in\nsafety and reward performance, thereby achieving the longstanding goal to\nobtain high rewards while ensuring the safety of a policy throughout the\nlifetime from learning to operation."}
{"id": "2505.21722", "pdf": "https://arxiv.org/pdf/2505.21722", "abs": "https://arxiv.org/abs/2505.21722", "authors": ["Ioannis Bantzis", "James B. Simon", "Arthur Jacot"], "title": "Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When a deep ReLU network is initialized with small weights, GD is at first\ndominated by the saddle at the origin in parameter space. We study the\nso-called escape directions, which play a similar role as the eigenvectors of\nthe Hessian for strict saddles. We show that the optimal escape direction\nfeatures a low-rank bias in its deeper layers: the first singular value of the\n$\\ell$-th layer weight matrix is at least $\\ell^{\\frac{1}{4}}$ larger than any\nother singular value. We also prove a number of related results about these\nescape directions. We argue that this result is a first step in proving\nSaddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of\nsaddles with increasing bottleneck rank."}
{"id": "2505.21893", "pdf": "https://arxiv.org/pdf/2505.21893", "abs": "https://arxiv.org/abs/2505.21893", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Junyan Wang", "Zhijian Zhou", "Hao Li"], "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Preference learning has become a central technique for aligning generative\nmodels with human expectations. Recently, it has been extended to diffusion\nmodels through methods like Direct Preference Optimization (DPO). However,\nexisting approaches such as Diffusion-DPO suffer from two key challenges:\ntimestep-dependent instability, caused by a mismatch between the reverse and\nforward diffusion processes and by high gradient variance in early noisy\ntimesteps, and off-policy bias arising from the mismatch between optimization\nand data collection policies. We begin by analyzing the reverse diffusion\ntrajectory and observe that instability primarily occurs at early timesteps\nwith low importance weights. To address these issues, we first propose\nDPO-C\\&M, a practical strategy that improves stability by clipping and masking\nuninformative timesteps while partially mitigating off-policy bias. Building on\nthis, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a\nprincipled framework that incorporates importance sampling into the objective\nto fully correct for off-policy bias and emphasize informative updates during\nthe diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and\nWan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,\nwith SDPO achieving superior VBench scores, human preference alignment, and\ntraining robustness. These results highlight the importance of timestep-aware,\ndistribution-corrected optimization in diffusion-based preference learning."}
{"id": "2505.21731", "pdf": "https://arxiv.org/pdf/2505.21731", "abs": "https://arxiv.org/abs/2505.21731", "authors": ["Quentin Delfosse", "Jannis Blüml", "Fabian Tatai", "Théo Vincent", "Bjarne Gregori", "Elisabeth Dillies", "Jan Peters", "Constantin Rothkopf", "Kristian Kersting"], "title": "Deep Reinforcement Learning Agents are not even close to Human Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": "49 pages in total, 5 main figures, 14 figures total", "summary": "Deep reinforcement learning (RL) agents achieve impressive results in a wide\nvariety of tasks, but they lack zero-shot adaptation capabilities. While most\nrobustness evaluations focus on tasks complexifications, for which human also\nstruggle to maintain performances, no evaluation has been performed on tasks\nsimplifications. To tackle this issue, we introduce HackAtari, a set of task\nvariations of the Arcade Learning Environments. We use it to demonstrate that,\ncontrary to humans, RL agents systematically exhibit huge performance drops on\nsimpler versions of their training tasks, uncovering agents' consistent\nreliance on shortcuts. Our analysis across multiple algorithms and\narchitectures highlights the persistent gap between RL agents and human\nbehavioral intelligence, underscoring the need for new benchmarks and\nmethodologies that enforce systematic generalization testing beyond static\nevaluation protocols. Training and testing in the same environment is not\nenough to obtain agents equipped with human-like intelligence."}
{"id": "2505.21895", "pdf": "https://arxiv.org/pdf/2505.21895", "abs": "https://arxiv.org/abs/2505.21895", "authors": ["Cameron Gordon", "Yiping Ji", "Hemanth Saratchandran", "Paul Albert", "Simon Lucey"], "title": "Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 9 figures", "summary": "Low-Rank Adaptation (LoRA) has become a standard approach for\nparameter-efficient fine-tuning, offering substantial reductions in trainable\nparameters by modeling updates as the product of two low-rank matrices. While\neffective, the low-rank constraint inherently limits representational capacity,\noften resulting in reduced performance compared to full-rank fine-tuning.\nRecent work by Ji et al. (2025) has addressed this limitation by applying a\nfixed-frequency sinusoidal transformation to low-rank adapters, increasing\ntheir stable rank without introducing additional parameters. This raises a\ncrucial question: can the same sine-activated technique be successfully applied\nwithin the context of Post-Training Quantization to retain benefits even after\nmodel compression? In this paper, we investigate this question by extending the\nsinusoidal transformation framework to quantized LoRA adapters. We develop a\ntheoretical analysis showing that the stable rank of a quantized adapter is\ntightly linked to that of its full-precision counterpart, motivating the use of\nsuch rank-enhancing functions even under quantization. Our results demonstrate\nthat the expressivity gains from a sinusoidal non-linearity persist after\nquantization, yielding highly compressed adapters with negligible loss in\nperformance. We validate our approach across a range of fine-tuning tasks for\nlanguage, vision and text-to-image generation achieving significant memory\nsavings while maintaining competitive accuracy."}
{"id": "2505.21732", "pdf": "https://arxiv.org/pdf/2505.21732", "abs": "https://arxiv.org/abs/2505.21732", "authors": ["Ruijie Zhang", "Ziyue Liu", "Zhengyang Wang", "Zheng Zhang"], "title": "LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing", "categories": ["cs.LG"], "comment": null, "summary": "Training foundation models such as ViTs and LLMs requires tremendous\ncomputing cost. Low-rank matrix or tensor factorization offers a\nparameter-efficient alternative, but often downgrades performance due to the\nrestricted parameter space. In this work, we introduce {\\textbf{Latent Crossing\n(LaX)}} -- a simple yet effective plug-and-play module that enhances the\ncapacity of low-rank models by enabling information flow across low-rank\nsubspaces. We extensively validate the benefits of LaX on pre-training tasks\nwith ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.\nLaX boosts low-rank model performance to match or exceed the full-rank\nbaselines while using 2-3\\(\\times\\) fewer parameters. When equipped with\nlow-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently\nimproves performance on arithmetic and common sense reasoning tasks with\nnegligible cost."}
{"id": "2505.21908", "pdf": "https://arxiv.org/pdf/2505.21908", "abs": "https://arxiv.org/abs/2505.21908", "authors": ["Hanyin Wang", "Zhenbang Wu", "Gururaj Kolar", "Hariprasad Korsapati", "Brian Bartlett", "Bryan Hull", "Jimeng Sun"], "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks."}
{"id": "2505.21743", "pdf": "https://arxiv.org/pdf/2505.21743", "abs": "https://arxiv.org/abs/2505.21743", "authors": ["Zihao Li", "Xinyuan Cao", "Xiangbo Gao", "Kexin Tian", "Keshu Wu", "Mohammad Anis", "Hao Zhang", "Keke Long", "Jiwan Jiang", "Xiaopeng Li", "Yunlong Zhang", "Tianbao Yang", "Dominique Lord", "Zhengzhong Tu", "Yang Zhou"], "title": "Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic safety science has long been hindered by a fundamental data paradox:\nthe crashes we most wish to prevent are precisely those events we rarely\nobserve. Existing crash-frequency models and surrogate safety metrics rely\nheavily on sparse, noisy, and under-reported records, while even sophisticated,\nhigh-fidelity simulations undersample the long-tailed situations that trigger\ncatastrophic outcomes such as fatalities. We argue that the path to achieving\nVision Zero, i.e., the complete elimination of traffic fatalities and severe\ninjuries, requires a paradigm shift from traditional crash-only learning to a\nnew form of counterfactual safety learning: reasoning not only about what\nhappened, but also about the vast set of plausible yet perilous scenarios that\ncould have happened under slightly different circumstances. To operationalize\nthis shift, our proposed agenda bridges macro to micro. Guided by crash-rate\npriors, generative scene engines, diverse driver models, and causal learning,\nnear-miss events are synthesized and explained. A crash-focused digital twin\ntestbed links micro scenes to macro patterns, while a multi-objective validator\nensures that simulations maintain statistical realism. This pipeline transforms\nsparse crash data into rich signals for crash prediction, enabling the\nstress-testing of vehicles, roads, and policies before deployment. By learning\nfrom crashes that almost happened, we can shift traffic safety from reactive\nforensics to proactive prevention, advancing Vision Zero."}
{"id": "2505.21918", "pdf": "https://arxiv.org/pdf/2505.21918", "abs": "https://arxiv.org/abs/2505.21918", "authors": ["Haruki Kai", "Tsuyoshi Okita"], "title": "Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages, 4 figures", "summary": "We developed a deep learning algorithm for human activity recognition using\nsensor signals as input. In this study, we built a pretrained language model\nbased on the Transformer architecture, which is widely used in natural language\nprocessing. By leveraging this pretrained model, we aimed to improve\nperformance on the downstream task of human activity recognition. While this\ntask can be addressed using a vanilla Transformer, we propose an enhanced\nn-dimensional numerical processing Transformer that incorporates three key\nfeatures: embedding n-dimensional numerical data through a linear layer,\nbinning-based pre-processing, and a linear transformation in the output layer.\nWe evaluated the effectiveness of our proposed model across five different\ndatasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%\nimprovements in accuracy."}
{"id": "2505.21749", "pdf": "https://arxiv.org/pdf/2505.21749", "abs": "https://arxiv.org/abs/2505.21749", "authors": ["M. Reza Ebrahimi", "Roland Memisevic"], "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy."}
{"id": "2505.21923", "pdf": "https://arxiv.org/pdf/2505.21923", "abs": "https://arxiv.org/abs/2505.21923", "authors": ["Asal Mehradfar", "Xuzhe Zhao", "Yilun Huang", "Emir Ceyani", "Yankai Yang", "Shihao Han", "Hamidreza Aghasi", "Salman Avestimehr"], "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CE"], "comment": null, "summary": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation."}
{"id": "2505.21750", "pdf": "https://arxiv.org/pdf/2505.21750", "abs": "https://arxiv.org/abs/2505.21750", "authors": ["Vivienne Huiling Wang", "Tinghuai Wang", "Joni Pajarinen"], "title": "Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Hierarchical reinforcement learning (HRL) learns to make decisions on\nmultiple levels of temporal abstraction. A key challenge in HRL is that the\nlow-level policy changes over time, making it difficult for the high-level\npolicy to generate effective subgoals. To address this issue, the high-level\npolicy must capture a complex subgoal distribution while also accounting for\nuncertainty in its estimates. We propose an approach that trains a conditional\ndiffusion model regularized by a Gaussian Process (GP) prior to generate a\ncomplex variety of subgoals while leveraging principled GP uncertainty\nquantification. Building on this framework, we develop a strategy that selects\nsubgoals from both the diffusion policy and GP's predictive mean. Our approach\noutperforms prior HRL methods in both sample efficiency and performance on\nchallenging continuous control benchmarks."}
{"id": "2505.21938", "pdf": "https://arxiv.org/pdf/2505.21938", "abs": "https://arxiv.org/abs/2505.21938", "authors": ["Qirun Zeng", "Eric He", "Richard Hoffmann", "Xuchuang Wang", "Jinhang Zuo"], "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios."}
{"id": "2505.21775", "pdf": "https://arxiv.org/pdf/2505.21775", "abs": "https://arxiv.org/abs/2505.21775", "authors": ["Michael Klamkin", "Arnaud Deza", "Sikai Cheng", "Haoruo Zhao", "Pascal Van Hentenryck"], "title": "DualSchool: How Reliable are LLMs for Optimization Education?", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Consider the following task taught in introductory optimization courses which\naddresses challenges articulated by the community at the intersection of\n(generative) AI and OR: generate the dual of a linear program. LLMs, being\ntrained at web-scale, have the conversion process and many instances of Primal\nto Dual Conversion (P2DC) at their disposal. Students may thus reasonably\nexpect that LLMs would perform well on the P2DC task. To assess this\nexpectation, this paper introduces DualSchool, a comprehensive framework for\ngenerating and verifying P2DC instances. The verification procedure of\nDualSchool uses the Canonical Graph Edit Distance, going well beyond existing\nevaluation methods for optimization models, which exhibit many false positives\nand negatives when applied to P2DC. Experiments performed by DualSchool reveal\ninteresting findings. Although LLMs can recite the conversion procedure\naccurately, state-of-the-art open LLMs fail to consistently produce correct\nduals. This finding holds even for the smallest two-variable instances and for\nderivative tasks, such as correctness, verification, and error classification.\nThe paper also discusses the implications for educators, students, and the\ndevelopment of large reasoning systems."}
{"id": "2505.21972", "pdf": "https://arxiv.org/pdf/2505.21972", "abs": "https://arxiv.org/abs/2505.21972", "authors": ["Patrick Vossler", "Fan Xia", "Yifan Mai", "Jean Feng"], "title": "Judging LLMs on a Simplex", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "28 pages, 7 figures", "summary": "Automated evaluation of free-form outputs from large language models (LLMs)\nis challenging because many distinct answers can be equally valid. A common\npractice is to use LLMs themselves as judges, but the theoretical properties of\nthis approach are not yet well understood. We show that a geometric framework\nthat represents both judges and candidates as points on a probability simplex\ncan provide helpful insight on what is or is not identifiable using LLM judges.\nOur theoretical analysis uncovers a \"phase transition\" in ranking\nidentifiability: for binary scoring systems, true rankings are identifiable\neven with weak judges under mild assumptions, while rankings become\nnon-identifiable for three or more scoring levels even with infinite data,\nabsent additional prior knowledge. This non-identifiability highlights how\nuncertainty in rankings stems from not only aleatoric uncertainty (i.e.,\ninherent stochasticity in the data) but also epistemic uncertainty regarding\nwhich assumptions hold, an aspect that has received limited attention until\nnow. To integrate both types of uncertainty, we use Bayesian inference to\nencode assumptions as priors and conduct sensitivity analysis of ranking\nestimates and credible intervals. Empirical evaluations across multiple\nbenchmarks demonstrate that Bayesian inference yields more accurate rankings\nand substantially improves coverage rates. These results underscore the\nimportance of taking a more holistic approach to uncertainty quantification\nwhen using LLMs as judges."}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models."}
{"id": "2505.22042", "pdf": "https://arxiv.org/pdf/2505.22042", "abs": "https://arxiv.org/abs/2505.22042", "authors": ["Hao Yang", "Haoxuan Li", "Mengyue Yang", "Xu Chen", "Mingming Gong"], "title": "Estimating the Effects of Sample Training Orders for Large Language Models without Retraining", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The order of training samples plays a crucial role in large language models\n(LLMs), significantly impacting both their external performance and internal\nlearning dynamics. Traditional methods for investigating this effect generally\nrequire retraining the model with various sample orders, which is\ncomputationally infeasible for LLMs. In this work, we improve traditional\nmethods by designing a retraining-free framework. By approximating Adam\noptimizer updates with first- and second-order Taylor expansions and utilizing\nrandom projection methods to store intermediate checkpoints, our framework can\nefficiently estimate model parameters for arbitrary training sample orders.\nNext, we apply our framework to two downstream research problems: (1) Training\ncurriculum design for LLMs -- we base our retraining-free framework to propose\na novel curriculum learning strategy that augments curriculum proposals with\nestimated model performances, enabling more informed sample scheduling. (2)\nLLMs' memorization and generalization effect analysis -- we use our\nretraining-free framework to estimate how the positions of training samples\ninfluence LLMs' capacity for memorization and generalization. We conduct\nextensive experiments to validate the effectiveness of our retraining-free\nframework in reproducing the true model performances, and further demonstrate\nits potential in optimizing LLM training curricula and analyzing the\nmemorization and generalization effects of LLMs."}
{"id": "2505.21783", "pdf": "https://arxiv.org/pdf/2505.21783", "abs": "https://arxiv.org/abs/2505.21783", "authors": ["Hyunsik Yun"], "title": "P-DROP: Poisson-Based Dropout for Graph Neural Networks", "categories": ["cs.LG"], "comment": "10 pages, 9 figures", "summary": "Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),\nwhere repeated message passing causes node representations to converge and lose\ndiscriminative power. To address this, we propose a novel node selection\nstrategy based on Poisson processes, introducing stochastic but structure-aware\nupdates. Specifically, we equip each node with an independent Poisson clock,\nenabling asynchronous and localized updates that preserve structural diversity.\nWe explore two applications of this strategy: as a replacement for\ndropout-based regularization and as a dynamic subgraph training scheme.\nExperimental results on standard benchmarks (Cora, Citeseer, Pubmed)\ndemonstrate that our Poisson-based method yields competitive or improved\naccuracy compared to traditional Dropout, DropEdge, and DropNode approaches,\nparticularly in later training stages."}
{"id": "2505.22074", "pdf": "https://arxiv.org/pdf/2505.22074", "abs": "https://arxiv.org/abs/2505.22074", "authors": ["Coşku Can Horuz", "Geoffrey Kasenbacher", "Saya Higuchi", "Sebastian Kairat", "Jendrik Stoltz", "Moritz Pesl", "Bernhard A. Moser", "Christoph Linse", "Thomas Martinetz", "Sebastian Otte"], "title": "The Resurrection of the ReLU", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modeling sophisticated activation functions within deep learning\narchitectures has evolved into a distinct research direction. Functions such as\nGELU, SELU, and SiLU offer smooth gradients and improved convergence\nproperties, making them popular choices in state-of-the-art models. Despite\nthis trend, the classical ReLU remains appealing due to its simplicity,\ninherent sparsity, and other advantageous topological characteristics. However,\nReLU units are prone to becoming irreversibly inactive - a phenomenon known as\nthe dying ReLU problem - which limits their overall effectiveness. In this\nwork, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,\nplug-and-play regularizer for deep architectures. SUGAR preserves the standard\nReLU function during the forward pass but replaces its derivative in the\nbackward pass with a smooth surrogate that avoids zeroing out gradients. We\ndemonstrate that SUGAR, when paired with a well-chosen surrogate function,\nsubstantially enhances generalization performance over convolutional network\narchitectures such as VGG-16 and ResNet-18, providing sparser activations while\neffectively resurrecting dead ReLUs. Moreover, we show that even in modern\narchitectures like Conv2NeXt and Swin Transformer - which typically employ GELU\n- substituting these with SUGAR yields competitive and even slightly superior\nperformance. These findings challenge the prevailing notion that advanced\nactivation functions are necessary for optimal performance. Instead, they\nsuggest that the conventional ReLU, particularly with appropriate gradient\nhandling, can serve as a strong, versatile revived classic across a broad range\nof deep learning vision models."}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785", "abs": "https://arxiv.org/abs/2505.21785", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "title": "Born a Transformer -- Always a Transformer?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024]. We use the recently proposed C-RASP framework for studying length\ngeneralization [Huang et al., 2025b] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained Transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain Transformer\ncapabilities, but does not overcome fundamental length-generalization limits."}
{"id": "2505.22108", "pdf": "https://arxiv.org/pdf/2505.22108", "abs": "https://arxiv.org/abs/2505.22108", "authors": ["Santhosh Parampottupadam", "Melih Coşğun", "Sarthak Pati", "Maximilian Zenk", "Saikat Roy", "Dimitrios Bounias", "Benjamin Hamm", "Sinem Sav", "Ralf Floca", "Klaus Maier-Hein"], "title": "Inclusive, Differentially Private Federated Learning for Clinical Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) offers a promising approach for training clinical AI\nmodels without centralizing sensitive patient data. However, its real-world\nadoption is hindered by challenges related to privacy, resource constraints,\nand compliance. Existing Differential Privacy (DP) approaches often apply\nuniform noise, which disproportionately degrades model performance, even among\nwell-compliant institutions. In this work, we propose a novel compliance-aware\nFL framework that enhances DP by adaptively adjusting noise based on\nquantifiable client compliance scores. Additionally, we introduce a compliance\nscoring tool based on key healthcare and security standards to promote secure,\ninclusive, and equitable participation across diverse clinical settings.\nExtensive experiments on public datasets demonstrate that integrating\nunder-resourced, less compliant clinics with highly regulated institutions\nyields accuracy improvements of up to 15% over traditional FL. This work\nadvances FL by balancing privacy, compliance, and performance, making it a\nviable solution for real-world clinical workflows in global healthcare."}
{"id": "2505.21790", "pdf": "https://arxiv.org/pdf/2505.21790", "abs": "https://arxiv.org/abs/2505.21790", "authors": ["Hilal Asi", "Vinod Raman", "Kunal Talwar"], "title": "Faster Rates for Private Adversarial Bandits", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$"}
{"id": "2505.22109", "pdf": "https://arxiv.org/pdf/2505.22109", "abs": "https://arxiv.org/abs/2505.22109", "authors": ["Paul Krzakala", "Gabriel Melo", "Charlotte Laclau", "Florence d'Alché-Buc", "Rémi Flamary"], "title": "The quest for the GRAph Level autoEncoder (GRALE)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although graph-based learning has attracted a lot of attention, graph\nrepresentation learning is still a challenging task whose resolution may impact\nkey application fields such as chemistry or biology. To this end, we introduce\nGRALE, a novel graph autoencoder that encodes and decodes graphs of varying\nsizes into a shared embedding space. GRALE is trained using an Optimal\nTransport-inspired loss that compares the original and reconstructed graphs and\nleverages a differentiable node matching module, which is trained jointly with\nthe encoder and decoder. The proposed attention-based architecture relies on\nEvoformer, the core component of AlphaFold, which we extend to support both\ngraph encoding and decoding. We show, in numerical experiments on simulated and\nmolecular data, that GRALE enables a highly general form of pre-training,\napplicable to a wide range of downstream tasks, from classification and\nregression to more complex tasks such as graph interpolation, editing,\nmatching, and prediction."}
{"id": "2505.21792", "pdf": "https://arxiv.org/pdf/2505.21792", "abs": "https://arxiv.org/abs/2505.21792", "authors": ["Yuanzhe Peng", "Jieming Bian", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Federated Learning (MFL) lies at the intersection of two pivotal\nresearch areas: leveraging complementary information from multiple modalities\nto improve downstream inference performance and enabling distributed training\nto enhance efficiency and preserve privacy. Despite the growing interest in\nMFL, there is currently no comprehensive taxonomy that organizes MFL through\nthe lens of different Federated Learning (FL) paradigms. This perspective is\nimportant because multimodal data introduces distinct challenges across various\nFL settings. These challenges, including modality heterogeneity, privacy\nheterogeneity, and communication inefficiency, are fundamentally different from\nthose encountered in traditional unimodal or non-FL scenarios. In this paper,\nwe systematically examine MFL within the context of three major FL paradigms:\nhorizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we\npresent the problem formulation, review representative training algorithms, and\nhighlight the most prominent challenge introduced by multimodal data in\ndistributed settings. We also discuss open challenges and provide insights for\nfuture research. By establishing this taxonomy, we aim to uncover the novel\nchallenges posed by multimodal data from the perspective of different FL\nparadigms and to offer a new lens through which to understand and advance the\ndevelopment of MFL."}
{"id": "2505.22199", "pdf": "https://arxiv.org/pdf/2505.22199", "abs": "https://arxiv.org/abs/2505.22199", "authors": ["Xinyue Hu", "Zhibin Duan", "Bo Chen", "Mingyuan Zhou"], "title": "Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Although deep neural networks have demonstrated significant success due to\ntheir powerful expressiveness, most models struggle to meet practical\nrequirements for uncertainty estimation. Concurrently, the entangled nature of\ndeep neural networks leads to a multifaceted problem, where various localized\nexplanation techniques reveal that multiple unrelated features influence the\ndecisions, thereby undermining interpretability. To address these challenges,\nwe develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates\ndeep neural networks as a conditional Bayesian non-negative factor analysis. By\nleveraging stochastic latent variables, the BNDL can model complex dependencies\nand provide robust uncertainty estimation. Moreover, the sparsity and\nnon-negativity of the latent variables encourage the model to learn\ndisentangled representations and decision layers, thereby improving\ninterpretability. We also offer theoretical guarantees that BNDL can achieve\neffective disentangled learning. In addition, we developed a corresponding\nvariational inference method utilizing a Weibull variational inference network\nto approximate the posterior distribution of the latent variables. Our\nexperimental results demonstrate that with enhanced disentanglement\ncapabilities, BNDL not only improves the model's accuracy but also provides\nreliable uncertainty estimation and improved interpretability."}
{"id": "2505.21800", "pdf": "https://arxiv.org/pdf/2505.21800", "abs": "https://arxiv.org/abs/2505.21800", "authors": ["Stanley Yu", "Vaidehi Bulusu", "Oscar Yasunaga", "Clayton Lau", "Cole Blondin", "Sean O'Brien", "Kevin Zhu", "Vasu Sharma"], "title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong conversational abilities but\noften generate falsehoods. Prior work suggests that the truthfulness of simple\npropositions can be represented as a single linear direction in a model's\ninternal activations, but this may not fully capture its underlying geometry.\nIn this work, we extend the concept cone framework, recently introduced for\nmodeling refusal, to the domain of truth. We identify multi-dimensional cones\nthat causally mediate truth-related behavior across multiple LLM families. Our\nresults are supported by three lines of evidence: (i) causal interventions\nreliably flip model responses to factual statements, (ii) learned cones\ngeneralize across model architectures, and (iii) cone-based interventions\npreserve unrelated model behavior. These findings reveal the richer,\nmultidirectional structure governing simple true/false propositions in LLMs and\nhighlight concept cones as a promising tool for probing abstract behaviors."}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning."}
{"id": "2505.21806", "pdf": "https://arxiv.org/pdf/2505.21806", "abs": "https://arxiv.org/abs/2505.21806", "authors": ["Brian D. Bue", "Jake H. Lee", "Andrew K. Thorpe", "Philip G. Brodrick", "Daniel Cusworth", "Alana Ayasse", "Vassiliki Mancoridis", "Anagha Satish", "Shujun Xiong", "Riley Duren"], "title": "Towards Operational Automated Greenhouse Gas Plume Detection", "categories": ["cs.LG"], "comment": "Main 19 pages 14 figures. Supplemental 19 pages 16 figures. In review", "summary": "Operational deployment of a fully automated greenhouse gas (GHG) plume\ndetection system remains an elusive goal for imaging spectroscopy missions,\ndespite recent advances in deep learning approaches. With the dramatic increase\nin data availability, however, automation continues to increase in importance\nfor natural and anthropogenic emissions monitoring. This work reviews and\naddresses several key obstacles in the field: data and label quality control,\nprevention of spatiotemporal biases, and correctly aligned modeling objectives.\nWe demonstrate through rigorous experiments using multicampaign data from\nairborne and spaceborne instruments that convolutional neural networks (CNNs)\nare able to achieve operational detection performance when these obstacles are\nalleviated. We demonstrate that a multitask model that learns both instance\ndetection and pixelwise segmentation simultaneously can successfully lead\ntowards an operational pathway. We evaluate the model's plume detectability\nacross emission source types and regions, identifying thresholds for\noperational deployment. Finally, we provide analysis-ready data, models, and\nsource code for reproducibility, and work to define a set of best practices and\nvalidation standards to facilitate future contributions to the field."}
{"id": "2505.22224", "pdf": "https://arxiv.org/pdf/2505.22224", "abs": "https://arxiv.org/abs/2505.22224", "authors": ["Senne Berden", "Ali İrfan Mahmutoğulları", "Dimos Tsouros", "Tias Guns"], "title": "Solver-Free Decision-Focused Learning for Linear Optimization Problems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mathematical optimization is a fundamental tool for decision-making in a wide\nrange of applications. However, in many real-world scenarios, the parameters of\nthe optimization problem are not known a priori and must be predicted from\ncontextual features. This gives rise to predict-then-optimize problems, where a\nmachine learning model predicts problem parameters that are then used to make\ndecisions via optimization. A growing body of work on decision-focused learning\n(DFL) addresses this setting by training models specifically to produce\npredictions that maximize downstream decision quality, rather than accuracy.\nWhile effective, DFL is computationally expensive, because it requires solving\nthe optimization problem with the predicted parameters at each loss evaluation.\nIn this work, we address this computational bottleneck for linear optimization\nproblems, a common class of problems in both DFL literature and real-world\napplications. We propose a solver-free training method that exploits the\ngeometric structure of linear optimization to enable efficient training with\nminimal degradation in solution quality. Our method is based on the insight\nthat a solution is optimal if and only if it achieves an objective value that\nis at least as good as that of its adjacent vertices on the feasible polytope.\nBuilding on this, our method compares the estimated quality of the ground-truth\noptimal solution with that of its precomputed adjacent vertices, and uses this\nas loss function. Experiments demonstrate that our method significantly reduces\ncomputational cost while maintaining high decision quality."}
{"id": "2505.21807", "pdf": "https://arxiv.org/pdf/2505.21807", "abs": "https://arxiv.org/abs/2505.21807", "authors": ["Tommy Xu", "Zhitian Zhang", "Xiangyu Sun", "Lauren Kelly Zung", "Hossein Hajimirsadeghi", "Greg Mori"], "title": "TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Predictive modeling on tabular data is the cornerstone of many real-world\napplications. Although gradient boosting machines and some recent deep models\nachieve strong performance on tabular data, they often lack interpretability.\nOn the other hand, large language models (LLMs) have demonstrated powerful\ncapabilities to generate human-like reasoning and explanations, but remain\nunder-performed for tabular data prediction. In this paper, we propose a new\napproach that leverages reasoning-based LLMs, trained using reinforcement\nlearning, to perform more accurate and explainable predictions on tabular data.\nOur method introduces custom reward functions that guide the model not only\ntoward high prediction accuracy but also toward human-understandable reasons\nfor its predictions. Experimental results show that our model achieves\npromising performance on financial benchmark datasets, outperforming most\nexisting LLMs."}
{"id": "2505.22306", "pdf": "https://arxiv.org/pdf/2505.22306", "abs": "https://arxiv.org/abs/2505.22306", "authors": ["Zehua Chen", "Yuyang Miao", "Liyuan Wang", "Luyun Fan", "Danilo P. Mandic", "Jun Zhu"], "title": "Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cardiovascular signals such as photoplethysmography (PPG),\nelectrocardiography (ECG), and blood pressure (BP) are inherently correlated\nand complementary, together reflecting the health of cardiovascular system.\nHowever, their joint utilization in real-time monitoring is severely limited by\ndiverse acquisition challenges from noisy wearable recordings to burdened\ninvasive procedures. Here we propose UniCardio, a multi-modal diffusion\ntransformer that reconstructs low-quality signals and synthesizes unrecorded\nsignals in a unified generative framework. Its key innovations include a\nspecialized model architecture to manage the signal modalities involved in\ngeneration tasks and a continual learning paradigm to incorporate varying\nmodality combinations. By exploiting the complementary nature of cardiovascular\nsignals, UniCardio clearly outperforms recent task-specific baselines in signal\ndenoising, imputation, and translation. The generated signals match the\nperformance of ground-truth signals in detecting abnormal health conditions and\nestimating vital signs, even in unseen domains, while ensuring interpretability\nfor human experts. These advantages position UniCardio as a promising avenue\nfor advancing AI-assisted healthcare."}
{"id": "2505.21813", "pdf": "https://arxiv.org/pdf/2505.21813", "abs": "https://arxiv.org/abs/2505.21813", "authors": ["Madi Matymov", "Ba-Hien Tran", "Michael Kampffmeyer", "Markus Heinonen", "Maurizio Filippone"], "title": "Optimizing Data Augmentation through Bayesian Model Selection", "categories": ["cs.LG", "stat.ML", "62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)"], "comment": "26 pages, 3 figures", "summary": "Data Augmentation (DA) has become an essential tool to improve robustness and\ngeneralization of modern machine learning. However, when deciding on DA\nstrategies it is critical to choose parameters carefully, and this can be a\ndaunting task which is traditionally left to trial-and-error or expensive\noptimization based on validation performance. In this paper, we counter these\nlimitations by proposing a novel framework for optimizing DA. In particular, we\ntake a probabilistic view of DA, which leads to the interpretation of\naugmentation parameters as model (hyper)-parameters, and the optimization of\nthe marginal likelihood with respect to these parameters as a Bayesian model\nselection problem. Due to its intractability, we derive a tractable Evidence\nLower BOund (ELBO), which allows us to optimize augmentation parameters jointly\nwith model parameters. We provide extensive theoretical results on variational\napproximation quality, generalization guarantees, invariance properties, and\nconnections to empirical Bayes. Through experiments on computer vision tasks,\nwe show that our approach improves calibration and yields robust performance\nover fixed or no augmentation. Our work provides a rigorous foundation for\noptimizing DA through Bayesian principles with significant potential for robust\nmachine learning."}
{"id": "2505.22310", "pdf": "https://arxiv.org/pdf/2505.22310", "abs": "https://arxiv.org/abs/2505.22310", "authors": ["Shoaib Ahmed Siddiqui", "Adrian Weller", "David Krueger", "Gintare Karolina Dziugaite", "Michael Curtis Mozer", "Eleni Triantafillou"], "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks."}
{"id": "2505.21824", "pdf": "https://arxiv.org/pdf/2505.21824", "abs": "https://arxiv.org/abs/2505.21824", "authors": ["Praveen Kumar", "Vincent T. Metzger", "Scott A. Malec"], "title": "Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "The global prevalence of diabetes, particularly type 2 diabetes mellitus\n(T2DM), is rapidly increasing, posing significant health and economic\nchallenges. T2DM not only disrupts blood glucose regulation but also damages\nvital organs such as the heart, kidneys, eyes, nerves, and blood vessels,\nleading to substantial morbidity and mortality. In the US alone, the economic\nburden of diagnosed diabetes exceeded \\$400 billion in 2022. Early detection of\nindividuals at risk is critical to mitigating these impacts. While machine\nlearning approaches for T2DM prediction are increasingly adopted, many rely on\nsupervised learning, which is often limited by the lack of confirmed negative\ncases. To address this limitation, we propose a novel unsupervised framework\nthat integrates Non-negative Matrix Factorization (NMF) with statistical\ntechniques to identify individuals at risk of developing T2DM. Our method\nidentifies latent patterns of multimorbidity and polypharmacy among diagnosed\nT2DM patients and applies these patterns to estimate the T2DM risk in\nundiagnosed individuals. By leveraging data-driven insights from comorbidity\nand medication usage, our approach provides an interpretable and scalable\nsolution that can assist healthcare providers in implementing timely\ninterventions, ultimately improving patient outcomes and potentially reducing\nthe future health and economic burden of T2DM."}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models."}
{"id": "2505.22356", "pdf": "https://arxiv.org/pdf/2505.22356", "abs": "https://arxiv.org/abs/2505.22356", "authors": ["Angéline Pouget", "Mohammad Yaghini", "Stephan Rabanser", "Nicolas Papernot"], "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications."}
{"id": "2505.21829", "pdf": "https://arxiv.org/pdf/2505.21829", "abs": "https://arxiv.org/abs/2505.21829", "authors": ["Antonio Orvieto", "Robert Gower"], "title": "In Search of Adam's Secret Sauce", "categories": ["cs.LG"], "comment": null, "summary": "Understanding the remarkable efficacy of Adam when training transformer-based\nlanguage models has become a central research topic within the optimization\ncommunity. To gain deeper insights, several simplifications of Adam have been\nproposed, such as the signed gradient and signed momentum methods. In this\nwork, we conduct an extensive empirical study - training over 1,300 language\nmodels across different data configurations and scales - comparing Adam to\nseveral known simplified variants. We find that signed momentum methods are\nfaster than SGD, but consistently underperform relative to Adam, even after\ncareful tuning of momentum, clipping setting and learning rates. However, our\nanalysis reveals a compelling option that preserves near-optimal performance\nwhile allowing for new insightful reformulations: constraining the Adam\nmomentum parameters to be equal. Beyond robust performance, this choice affords\nnew theoretical insights, highlights the \"secret sauce\" on top of signed\nmomentum, and grants a precise statistical interpretation: we show that Adam in\nthis setting implements a natural online algorithm for estimating the mean and\nvariance of gradients-one that arises from a mean-field Gaussian variational\ninference perspective."}
{"id": "2505.22358", "pdf": "https://arxiv.org/pdf/2505.22358", "abs": "https://arxiv.org/abs/2505.22358", "authors": ["Zhiyi Wan", "Wanrou Du", "Liang Li", "Miao Pan", "Xiaoqi Qin"], "title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark."}
{"id": "2505.21835", "pdf": "https://arxiv.org/pdf/2505.21835", "abs": "https://arxiv.org/abs/2505.21835", "authors": ["Xiangyu Chen", "Jing Liu", "Ye Wang", "Matthew Brand", "Pu", "Wang", "Toshiaki Koike-Akino"], "title": "TuneComp: Joint Fine-tuning and Compression for Large Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "Preliminary Work", "summary": "To reduce model size during post-training, compression methods, including\nknowledge distillation, low-rank approximation, and pruning, are often applied\nafter fine-tuning the model. However, sequential fine-tuning and compression\nsacrifices performance, while creating a larger than necessary model as an\nintermediate step. In this work, we aim to reduce this gap, by directly\nconstructing a smaller model while guided by the downstream task. We propose to\njointly fine-tune and compress the model by gradually distilling it to a pruned\nlow-rank structure. Experiments demonstrate that joint fine-tuning and\ncompression significantly outperforms other sequential compression methods."}
{"id": "2505.22370", "pdf": "https://arxiv.org/pdf/2505.22370", "abs": "https://arxiv.org/abs/2505.22370", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Weili Guan", "Min Zhang", "Liqiang Nie"], "title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 4 figures", "summary": "Continual Learning requires a model to learn multiple tasks in sequence while\nmaintaining both stability:preserving knowledge from previously learned tasks,\nand plasticity:effectively learning new tasks. Gradient projection has emerged\nas an effective and popular paradigm in CL, where it partitions the gradient\nspace of previously learned tasks into two orthogonal subspaces: a primary\nsubspace and a minor subspace. New tasks are learned effectively within the\nminor subspace, thereby reducing interference with previously acquired\nknowledge. However, existing Gradient Projection methods struggle to achieve an\noptimal balance between plasticity and stability, as it is hard to\nappropriately partition the gradient space. In this work, we consider a\ncontinual learning paradigm based on Low-Rank Adaptation, which has gained\nconsiderable attention due to its efficiency and wide applicability, and\npropose a novel approach for continual learning, called SplitLoRA. We first\nprovide a theoretical analysis of how subspace partitioning affects model\nstability and plasticity. Informed by this analysis, we then introduce an\neffective method that derives the optimal partition of the gradient space for\npreviously learned tasks. This approach effectively balances stability and\nplasticity in continual learning. Experimental results on multiple datasets\ndemonstrate that the proposed method achieves state-of-the-art performance."}
{"id": "2505.21841", "pdf": "https://arxiv.org/pdf/2505.21841", "abs": "https://arxiv.org/abs/2505.21841", "authors": ["Jiahui Zhu", "Kihyun Yu", "Dabeen Lee", "Xin Liu", "Honghao Wei"], "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 41 st International Conference on Machine Learning", "summary": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments."}
{"id": "2505.22389", "pdf": "https://arxiv.org/pdf/2505.22389", "abs": "https://arxiv.org/abs/2505.22389", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Liqiang Nie"], "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 3 figures", "summary": "Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets."}
{"id": "2505.21852", "pdf": "https://arxiv.org/pdf/2505.21852", "abs": "https://arxiv.org/abs/2505.21852", "authors": ["Akifumi Wachi", "Kohei Miyaguchi", "Takumi Tanabe", "Rei Sato", "Youhei Akimoto"], "title": "A Provable Approach for End-to-End Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.RO", "math.IT"], "comment": "27 pages", "summary": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure\nthe safety of a policy throughout the entire process, from learning to\noperation. However, existing safe RL paradigms inherently struggle to achieve\nthis objective. We propose a method, called Provably Lifetime Safe RL (PLS),\nthat integrates offline safe RL with safe policy deployment to address this\nchallenge. Our proposed method learns a policy offline using return-conditioned\nsupervised learning and then deploys the resulting policy while cautiously\noptimizing a limited set of parameters, known as target returns, using Gaussian\nprocesses (GPs). Theoretically, we justify the use of GPs by analyzing the\nmathematical relationship between target and actual returns. We then prove that\nPLS finds near-optimal target returns while guaranteeing safety with high\nprobability. Empirically, we demonstrate that PLS outperforms baselines both in\nsafety and reward performance, thereby achieving the longstanding goal to\nobtain high rewards while ensuring the safety of a policy throughout the\nlifetime from learning to operation."}
{"id": "2505.22391", "pdf": "https://arxiv.org/pdf/2505.22391", "abs": "https://arxiv.org/abs/2505.22391", "authors": ["Yi Zhang", "Difan Zou"], "title": "Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "23 pages, 5 figures, 4 tables", "summary": "Modeling physical systems in a generative manner offers several advantages,\nincluding the ability to handle partial observations, generate diverse\nsolutions, and address both forward and inverse problems. Recently, diffusion\nmodels have gained increasing attention in the modeling of physical systems,\nparticularly those governed by partial differential equations (PDEs). However,\ndiffusion models only access noisy data $\\boldsymbol{x}_t$ at intermediate\nsteps, making it infeasible to directly enforce constraints on the clean sample\n$\\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are\ntypically applied to the expectation of clean samples\n$\\mathbb{E}[\\boldsymbol{x}_0|\\boldsymbol{x}_t]$, which is estimated using the\nlearned score network. However, imposing PDE constraints on the expectation\ndoes not strictly represent the one on the true clean data, known as Jensen's\nGap. This gap creates a trade-off: enforcing PDE constraints may come at the\ncost of reduced accuracy in generative modeling. To address this, we propose a\nsimple yet effective post-hoc distillation approach, where PDE constraints are\nnot injected directly into the diffusion process, but instead enforced during a\npost-hoc distillation stage. We term our method as Physics-Informed\nDistillation of Diffusion Models (PIDDM). This distillation not only\nfacilitates single-step generation with improved PDE satisfaction, but also\nsupport both forward and inverse problem solving and reconstruction from\nrandomly partial observation. Extensive experiments across various PDE\nbenchmarks demonstrate that PIDDM significantly improves PDE satisfaction over\nseveral recent and competitive baselines, such as PIDM, DiffusionPDE, and\nECI-sampling, with less computation overhead. Our approach can shed light on\nmore efficient and effective strategies for incorporating physical constraints\ninto diffusion models."}
{"id": "2505.21857", "pdf": "https://arxiv.org/pdf/2505.21857", "abs": "https://arxiv.org/abs/2505.21857", "authors": ["Mijung Park"], "title": "Revisiting Bayesian Model Averaging in the Era of Foundation Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit the classical, full-fledged Bayesian model averaging (BMA)\nparadigm to ensemble pre-trained and/or lightly-finetuned foundation models to\nenhance the classification performance on image and text data. To make BMA\ntractable under foundation models, we introduce trainable linear classifiers\nthat take frozen features from the pre-trained foundation models as inputs. The\nmodel posteriors over the linear classifiers tell us which linear heads and\nfrozen features are better suited for a given dataset, resulting in a\nprincipled model ensembling method. Furthermore, we propose a computationally\ncheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize\nthe model ensemble weights, just like those weights based on model posterior\ndistributions in BMA, by reducing the amount of surprise (expected entropy of\nthe predictions) we get from predictions of ensembled models. With the rapid\ndevelopment of foundation models, these approaches will enable the\nincorporation of future, possibly significantly better foundation models to\nenhance the performance of challenging classification tasks."}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."}
{"id": "2505.21877", "pdf": "https://arxiv.org/pdf/2505.21877", "abs": "https://arxiv.org/abs/2505.21877", "authors": ["Hongyao Chen", "Tianyang Xu", "Xiaojun Wu", "Josef Kittler"], "title": "Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Batch Normalisation (BN) is widely used in conventional deep neural network\ntraining to harmonise the input-output distributions for each batch of data.\nHowever, federated learning, a distributed learning paradigm, faces the\nchallenge of dealing with non-independent and identically distributed data\namong the client nodes. Due to the lack of a coherent methodology for updating\nBN statistical parameters, standard BN degrades the federated learning\nperformance. To this end, it is urgent to explore an alternative normalisation\nsolution for federated learning. In this work, we resolve the dilemma of the BN\nlayer in federated learning by developing a customised normalisation approach,\nHybrid Batch Normalisation (HBN). HBN separates the update of statistical\nparameters (i.e. , means and variances used for evaluation) from that of\nlearnable parameters (i.e. , parameters that require gradient updates),\nobtaining unbiased estimates of global statistical parameters in distributed\nscenarios. In contrast with the existing solutions, we emphasise the supportive\npower of global statistics for federated learning. The HBN layer introduces a\nlearnable hybrid distribution factor, allowing each computing node to\nadaptively mix the statistical parameters of the current batch with the global\nstatistics. Our HBN can serve as a powerful plugin to advance federated\nlearning performance. It reflects promising merits across a wide range of\nfederated learning settings, especially for small batch sizes and heterogeneous\ndata."}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."}
{"id": "2505.21882", "pdf": "https://arxiv.org/pdf/2505.21882", "abs": "https://arxiv.org/abs/2505.21882", "authors": ["Ruijie Li", "Xiang Zhao", "Qiao Ning", "Shikai Guo"], "title": "HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis", "categories": ["cs.LG"], "comment": "14 pages, 9 figures (including subfigures), 5 tables. This is the\n  first work to explore and effectively model momentum across multiple\n  granularities in professional tennis tournaments", "summary": "In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects\nthe dynamic shifts in performance of athletes that can decisively influence\nmatch outcomes. Despite its significance, momentum in terms of effective\nmodeling and multi-granularity analysis across points, games, sets, and matches\nin tennis tournaments remains underexplored. In this study, we define a novel\nMomentum Score (MS) metric to quantify a player's momentum level in\nmulti-granularity tennis tournaments, and design HydraNet, a momentum-driven\nstate-space duality-based framework, to model MS by integrating thirty-two\nheterogeneous dimensions of athletes performance in serve, return, psychology\nand fatigue. HydraNet integrates a Hydra module, which builds upon a\nstate-space duality (SSD) framework, capturing explicit momentum with a\nsliding-window mechanism and implicit momentum through cross-game state\npropagation. It also introduces a novel Versus Learning method to better\nenhance the adversarial nature of momentum between the two athletes at a macro\nlevel, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for\ncapturing and integrating intra-player and inter-player dynamic momentum at a\nmicro level. Additionally, we construct a million-level tennis cross-tournament\ndataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate\nthe multi-granularity modeling capability of HydraNet for the MS metric on this\ndataset. Extensive experimental evaluations demonstrate that the MS metric\nconstructed by the HydraNet framework provides actionable insights into how\nmomentum impacts outcomes at different granularities, establishing a new\nfoundation for momentum modeling and sports analysis. To the best of our\nknowledge. The source code and datasets are available at\nhttps://github.com/ReyJerry/HydraNet."}
{"id": "2505.22442", "pdf": "https://arxiv.org/pdf/2505.22442", "abs": "https://arxiv.org/abs/2505.22442", "authors": ["Mattie Fellows", "Clarisse Wibault", "Uljad Berdica", "Johannes Forkel", "Jakob N. Foerster", "Michael A. Osborne"], "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel."}
{"id": "2505.21893", "pdf": "https://arxiv.org/pdf/2505.21893", "abs": "https://arxiv.org/abs/2505.21893", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Junyan Wang", "Zhijian Zhou", "Hao Li"], "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Preference learning has become a central technique for aligning generative\nmodels with human expectations. Recently, it has been extended to diffusion\nmodels through methods like Direct Preference Optimization (DPO). However,\nexisting approaches such as Diffusion-DPO suffer from two key challenges:\ntimestep-dependent instability, caused by a mismatch between the reverse and\nforward diffusion processes and by high gradient variance in early noisy\ntimesteps, and off-policy bias arising from the mismatch between optimization\nand data collection policies. We begin by analyzing the reverse diffusion\ntrajectory and observe that instability primarily occurs at early timesteps\nwith low importance weights. To address these issues, we first propose\nDPO-C\\&M, a practical strategy that improves stability by clipping and masking\nuninformative timesteps while partially mitigating off-policy bias. Building on\nthis, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a\nprincipled framework that incorporates importance sampling into the objective\nto fully correct for off-policy bias and emphasize informative updates during\nthe diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and\nWan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,\nwith SDPO achieving superior VBench scores, human preference alignment, and\ntraining robustness. These results highlight the importance of timestep-aware,\ndistribution-corrected optimization in diffusion-based preference learning."}
{"id": "2505.22483", "pdf": "https://arxiv.org/pdf/2505.22483", "abs": "https://arxiv.org/abs/2505.22483", "authors": ["Abhra Chaudhuri", "Anjan Dutta", "Tu Bui", "Serban Georgescu"], "title": "A Closer Look at Multimodal Representation Collapse", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)", "summary": "We aim to develop a fundamental understanding of modality collapse, a\nrecently observed empirical phenomenon wherein models trained for multimodal\nfusion tend to rely only on a subset of the modalities, ignoring the rest. We\nshow that modality collapse happens when noisy features from one modality are\nentangled, via a shared set of neurons in the fusion head, with predictive\nfeatures from another, effectively masking out positive contributions from the\npredictive features of the former modality and leading to its collapse. We\nfurther prove that cross-modal knowledge distillation implicitly disentangles\nsuch representations by freeing up rank bottlenecks in the student encoder,\ndenoising the fusion-head outputs without negatively impacting the predictive\nfeatures from either modality. Based on the above findings, we propose an\nalgorithm that prevents modality collapse through explicit basis reallocation,\nwith applications in dealing with missing modalities. Extensive experiments on\nmultiple multimodal benchmarks validate our theoretical claims. Project page:\nhttps://abhrac.github.io/mmcollapse/."}
{"id": "2505.21895", "pdf": "https://arxiv.org/pdf/2505.21895", "abs": "https://arxiv.org/abs/2505.21895", "authors": ["Cameron Gordon", "Yiping Ji", "Hemanth Saratchandran", "Paul Albert", "Simon Lucey"], "title": "Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 9 figures", "summary": "Low-Rank Adaptation (LoRA) has become a standard approach for\nparameter-efficient fine-tuning, offering substantial reductions in trainable\nparameters by modeling updates as the product of two low-rank matrices. While\neffective, the low-rank constraint inherently limits representational capacity,\noften resulting in reduced performance compared to full-rank fine-tuning.\nRecent work by Ji et al. (2025) has addressed this limitation by applying a\nfixed-frequency sinusoidal transformation to low-rank adapters, increasing\ntheir stable rank without introducing additional parameters. This raises a\ncrucial question: can the same sine-activated technique be successfully applied\nwithin the context of Post-Training Quantization to retain benefits even after\nmodel compression? In this paper, we investigate this question by extending the\nsinusoidal transformation framework to quantized LoRA adapters. We develop a\ntheoretical analysis showing that the stable rank of a quantized adapter is\ntightly linked to that of its full-precision counterpart, motivating the use of\nsuch rank-enhancing functions even under quantization. Our results demonstrate\nthat the expressivity gains from a sinusoidal non-linearity persist after\nquantization, yielding highly compressed adapters with negligible loss in\nperformance. We validate our approach across a range of fine-tuning tasks for\nlanguage, vision and text-to-image generation achieving significant memory\nsavings while maintaining competitive accuracy."}
{"id": "2505.22491", "pdf": "https://arxiv.org/pdf/2505.22491", "abs": "https://arxiv.org/abs/2505.22491", "authors": ["Moritz Haas", "Sebastian Bordt", "Ulrike von Luxburg", "Leena Chennuru Vankadara"], "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The dominant paradigm for training large-scale vision and language models is\nHe initialization and a single global learning rate (\\textit{standard\nparameterization}, SP). Despite its practical success, standard parametrization\nremains poorly understood from a theoretical perspective: Existing\ninfinite-width theory would predict instability under large learning rates and\nvanishing feature learning under stable learning rates. However, empirically\noptimal learning rates consistently decay much slower than theoretically\npredicted. By carefully studying neural network training dynamics, we\ndemonstrate that this discrepancy is not fully explained by finite-width\nphenomena such as catapult effects or a lack of alignment between weights and\nincoming activations. We instead show that the apparent contradiction can be\nfundamentally resolved by taking the loss function into account: In contrast to\nMean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an\nintermediate \\textit{controlled divergence} regime emerges, where logits\ndiverge but loss, gradients, and activations remain stable. Stable training\nunder large learning rates enables persistent feature evolution at scale in all\nhidden layers, which is crucial for the practical success of SP. In experiments\nacross optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities\n(vision, language), we validate that neural networks operate in this controlled\ndivergence regime under CE loss but not under MSE loss. Our empirical evidence\nsuggests that width-scaling considerations are surprisingly useful for\npredicting empirically optimal learning rate exponents. Finally, our analysis\nclarifies the effectiveness and limitations of recently proposed layerwise\nlearning rate scalings for standard initialization."}
{"id": "2505.21908", "pdf": "https://arxiv.org/pdf/2505.21908", "abs": "https://arxiv.org/abs/2505.21908", "authors": ["Hanyin Wang", "Zhenbang Wu", "Gururaj Kolar", "Hariprasad Korsapati", "Brian Bartlett", "Bryan Hull", "Jimeng Sun"], "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks."}
{"id": "2505.22492", "pdf": "https://arxiv.org/pdf/2505.22492", "abs": "https://arxiv.org/abs/2505.22492", "authors": ["Hongyi Zhou", "Josiah P. Hanna", "Jin Zhu", "Ying Yang", "Chengchun Shi"], "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "This paper studies off-policy evaluation (OPE) in reinforcement learning with\na focus on behavior policy estimation for importance sampling. Prior work has\nshown empirically that estimating a history-dependent behavior policy can lead\nto lower mean squared error (MSE) even when the true behavior policy is\nMarkovian. However, the question of why the use of history should lower MSE\nremains open. In this paper, we theoretically demystify this paradox by\nderiving a bias-variance decomposition of the MSE of ordinary importance\nsampling (IS) estimators, demonstrating that history-dependent behavior policy\nestimation decreases their asymptotic variances while increasing their\nfinite-sample biases. Additionally, as the estimated behavior policy conditions\non a longer history, we show a consistent decrease in variance. We extend these\nfindings to a range of other OPE estimators, including the sequential IS\nestimator, the doubly robust estimator and the marginalized IS estimator, with\nthe behavior policy estimated either parametrically or non-parametrically."}
{"id": "2505.21910", "pdf": "https://arxiv.org/pdf/2505.21910", "abs": "https://arxiv.org/abs/2505.21910", "authors": ["Xianbiao Qi", "Yelin He", "Jiaquan Ye", "Chun-Guang Li", "Bojia Zi", "Xili Dai", "Qin Zou", "Rong Xiao"], "title": "Taming Transformer Without Using Learning Rate Warmup", "categories": ["cs.LG", "cs.CV"], "comment": "This paper is published as a conference paper at ICLR 2025", "summary": "Scaling Transformer to a large scale without using some technical tricks such\nas learning rate warump and using an obviously lower learning rate is an\nextremely challenging task, and is increasingly gaining more attention. In this\npaper, we provide a theoretical analysis for the process of training\nTransformer and reveal the rationale behind the model crash phenomenon in the\ntraining process, termed \\textit{spectral energy concentration} of\n${\\bW_q}^{\\top} \\bW_k$, which is the reason for a malignant entropy collapse,\nwhere ${\\bW_q}$ and $\\bW_k$ are the projection matrices for the query and the\nkey in Transformer, respectively. To remedy this problem, motivated by\n\\textit{Weyl's Inequality}, we present a novel optimization strategy, \\ie,\nmaking the weight updating in successive steps smooth -- if the ratio\n$\\frac{\\sigma_{1}(\\nabla \\bW_t)}{\\sigma_{1}(\\bW_{t-1})}$ is larger than a\nthreshold, we will automatically bound the learning rate to a weighted multiple\nof $\\frac{\\sigma_{1}(\\bW_{t-1})}{\\sigma_{1}(\\nabla \\bW_t)}$, where $\\nabla\n\\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can\nprevent spectral energy concentration to only a few directions, and thus can\navoid malignant entropy collapse which will trigger the model crash. We conduct\nextensive experiments using ViT, Swin-Transformer and GPT, showing that our\noptimization strategy can effectively and stably train these Transformers\nwithout using learning rate warmup."}
{"id": "2505.22521", "pdf": "https://arxiv.org/pdf/2505.22521", "abs": "https://arxiv.org/abs/2505.22521", "authors": ["Chao Wang", "Chuanhao Nie", "Yunbo Liu"], "title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally\n  to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu).\n  Submitted to the 3rd International Conference on Management Innovation and\n  Economy Development (MIED 2025), Chongqing, China", "summary": "Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems."}
{"id": "2505.21918", "pdf": "https://arxiv.org/pdf/2505.21918", "abs": "https://arxiv.org/abs/2505.21918", "authors": ["Haruki Kai", "Tsuyoshi Okita"], "title": "Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages, 4 figures", "summary": "We developed a deep learning algorithm for human activity recognition using\nsensor signals as input. In this study, we built a pretrained language model\nbased on the Transformer architecture, which is widely used in natural language\nprocessing. By leveraging this pretrained model, we aimed to improve\nperformance on the downstream task of human activity recognition. While this\ntask can be addressed using a vanilla Transformer, we propose an enhanced\nn-dimensional numerical processing Transformer that incorporates three key\nfeatures: embedding n-dimensional numerical data through a linear layer,\nbinning-based pre-processing, and a linear transformation in the output layer.\nWe evaluated the effectiveness of our proposed model across five different\ndatasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%\nimprovements in accuracy."}
{"id": "2505.22531", "pdf": "https://arxiv.org/pdf/2505.22531", "abs": "https://arxiv.org/abs/2505.22531", "authors": ["Andres Molina-Markham", "Luis Robaina", "Sean Steinle", "Akash Trivedi", "Derek Tsui", "Nicholas Potteiger", "Lauren Brandt", "Ransom Winder", "Ahmed Ridley"], "title": "Training RL Agents for Multi-Objective Network Defense Tasks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work."}
{"id": "2505.21923", "pdf": "https://arxiv.org/pdf/2505.21923", "abs": "https://arxiv.org/abs/2505.21923", "authors": ["Asal Mehradfar", "Xuzhe Zhao", "Yilun Huang", "Emir Ceyani", "Yankai Yang", "Shihao Han", "Hamidreza Aghasi", "Salman Avestimehr"], "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CE"], "comment": null, "summary": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation."}
{"id": "2505.22533", "pdf": "https://arxiv.org/pdf/2505.22533", "abs": "https://arxiv.org/abs/2505.22533", "authors": ["Pallavi Bhardwaj", "Caitlin Jones", "Lasse Dierich", "Aleksandar Vučković"], "title": "TabularQGAN: A Quantum Generative Model for Tabular Data", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "18 pages,8 figures and 4 tables", "summary": "In this paper, we introduce a novel quantum generative model for synthesizing\ntabular data. Synthetic data is valuable in scenarios where real-world data is\nscarce or private, it can be used to augment or replace existing datasets.\nReal-world enterprise data is predominantly tabular and heterogeneous, often\ncomprising a mixture of categorical and numerical features, making it highly\nrelevant across various industries such as healthcare, finance, and software.\nWe propose a quantum generative adversarial network architecture with flexible\ndata encoding and a novel quantum circuit ansatz to effectively model tabular\ndata. The proposed approach is tested on the MIMIC III healthcare and Adult\nCensus datasets, with extensive benchmarking against leading classical models,\nCTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model\noutperforms classical models by an average of 8.5% with respect to an overall\nsimilarity score from SDMetrics, while using only 0.072% of the parameters of\nthe classical models. Additionally, we evaluate the generalization capabilities\nof the models using two custom-designed metrics that demonstrate the ability of\nthe proposed quantum model to generate useful and novel samples. To our\nknowledge, this is one of the first demonstrations of a successful quantum\ngenerative model for handling tabular data, indicating that this task could be\nwell-suited to quantum computers."}
{"id": "2505.21930", "pdf": "https://arxiv.org/pdf/2505.21930", "abs": "https://arxiv.org/abs/2505.21930", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages. To appear in ACL'25", "summary": "This paper develops an ensemble method for fine-tuning a language model to\nmultiple datasets. Existing methods, such as quantized LoRA (QLoRA), are\nefficient when adapting to a single dataset. When training on multiple datasets\nof different tasks, a common setup in practice, it remains unclear how to\ndesign an efficient adaptation for fine-tuning language models. We propose to\nuse an ensemble of multiple smaller adapters instead of a single adapter per\ntask. We design an efficient algorithm that partitions $n$ datasets into $m$\ngroups, where $m$ is typically much smaller than $n$ in practice, and train one\nadapter for each group before taking a weighted combination to form the\nensemble. The algorithm leverages a first-order approximation property of\nlow-rank adaptation to quickly obtain the fine-tuning performances of dataset\ncombinations since methods like LoRA stay close to the base model. Hence, we\nuse the gradients of the base model to estimate its behavior during\nfine-tuning. Empirically, this approximation holds with less than $1\\%$ error\non models with up to $34$ billion parameters, leading to an estimation of true\nfine-tuning performances under $5\\%$ error while speeding up computation\ncompared to base fine-tuning by $105$ times. When applied to fine-tune Llama\nand GPT models on ten text classification tasks, our approach provides up to\n$10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On\na Llama model with $34$ billion parameters, an ensemble of QLoRA increases test\naccuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs."}
{"id": "2505.22601", "pdf": "https://arxiv.org/pdf/2505.22601", "abs": "https://arxiv.org/abs/2505.22601", "authors": ["Jacob L. Block", "Aryan Mokhtari", "Sanjay Shakkottai"], "title": "Machine Unlearning under Overparameterization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine unlearning algorithms aim to remove the influence of specific\ntraining samples, ideally recovering the model that would have resulted from\ntraining on the remaining data alone. We study unlearning in the\noverparameterized setting, where many models interpolate the data, and defining\nthe unlearning solution as any loss minimizer over the retained\nset$\\unicode{x2013}$as in prior work in the underparameterized\nsetting$\\unicode{x2013}$is inadequate, since the original model may already\ninterpolate the retained data and satisfy this condition. In this regime, loss\ngradients vanish, rendering prior methods based on gradient perturbations\nineffective, motivating both new unlearning definitions and algorithms. For\nthis setting, we define the unlearning solution as the minimum-complexity\ninterpolator over the retained data and propose a new algorithmic framework\nthat only requires access to model gradients on the retained set at the\noriginal solution. We minimize a regularized objective over perturbations\nconstrained to be orthogonal to these model gradients, a first-order relaxation\nof the interpolation condition. For different model classes, we provide exact\nand approximate unlearning guarantees, and we demonstrate that an\nimplementation of our framework outperforms existing baselines across various\nunlearning experiments."}
{"id": "2505.21938", "pdf": "https://arxiv.org/pdf/2505.21938", "abs": "https://arxiv.org/abs/2505.21938", "authors": ["Qirun Zeng", "Eric He", "Richard Hoffmann", "Xuchuang Wang", "Jinhang Zuo"], "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios."}
{"id": "2505.22602", "pdf": "https://arxiv.org/pdf/2505.22602", "abs": "https://arxiv.org/abs/2505.22602", "authors": ["Mahtab Alizadeh Vandchali", "Fangshuo", "Liao", "Anastasios Kyrillidis"], "title": "One Rank at a Time: Cascading Error Dynamics in Sequential Learning", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "36 pages", "summary": "Sequential learning -- where complex tasks are broken down into simpler,\nhierarchical components -- has emerged as a paradigm in AI. This paper views\nsequential learning through the lens of low-rank linear regression, focusing\nspecifically on how errors propagate when learning rank-1 subspaces\nsequentially. We present an analysis framework that decomposes the learning\nprocess into a series of rank-1 estimation problems, where each subsequent\nestimation depends on the accuracy of previous steps. Our contribution is a\ncharacterization of the error propagation in this sequential process,\nestablishing bounds on how errors -- e.g., due to limited computational budgets\nand finite precision -- affect the overall model accuracy. We prove that these\nerrors compound in predictable ways, with implications for both algorithmic\ndesign and stability guarantees."}
{"id": "2505.21942", "pdf": "https://arxiv.org/pdf/2505.21942", "abs": "https://arxiv.org/abs/2505.21942", "authors": ["Prashant Bhat", "Laurens Niesten", "Elahe Arani", "Bahram Zonooz"], "title": "Continual Learning Beyond Experience Rehearsal and Full Model Surrogates", "categories": ["cs.LG", "stat.ML"], "comment": "23 pages, 9 figures", "summary": "Continual learning (CL) has remained a significant challenge for deep neural\nnetworks as learning new tasks erases previously acquired knowledge, either\npartially or completely. Existing solutions often rely on experience rehearsal\nor full model surrogates to mitigate CF. While effective, these approaches\nintroduce substantial memory and computational overhead, limiting their\nscalability and applicability in real-world scenarios. To address this, we\npropose SPARC, a scalable CL approach that eliminates the need for experience\nrehearsal and full-model surrogates. By effectively combining task-specific\nworking memories and task-agnostic semantic memory for cross-task knowledge\nconsolidation, SPARC results in a remarkable parameter efficiency, using only\n6% of the parameters required by full-model surrogates. Despite its lightweight\ndesign, SPARC achieves superior performance on Seq-TinyImageNet and matches\nrehearsal-based methods on various CL benchmarks. Additionally, weight\nre-normalization in the classification layer mitigates task-specific biases,\nestablishing SPARC as a practical and scalable solution for CL under stringent\nefficiency constraints."}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."}
{"id": "2505.21944", "pdf": "https://arxiv.org/pdf/2505.21944", "abs": "https://arxiv.org/abs/2505.21944", "authors": ["Linli Zhou", "Bokun Wang", "My T. Thai", "Tianbao Yang"], "title": "Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization", "categories": ["cs.LG"], "comment": null, "summary": "Two-way partial AUC (TPAUC) is a critical performance metric for binary\nclassification with imbalanced data, as it focuses on specific ranges of the\ntrue positive rate (TPR) and false positive rate (FPR). However, stochastic\nalgorithms for TPAUC optimization remain under-explored, with existing methods\neither limited to approximated TPAUC loss functions or burdened by sub-optimal\ncomplexities. To overcome these limitations, we introduce two innovative\nstochastic primal-dual double block-coordinate algorithms for TPAUC\nmaximization. These algorithms utilize stochastic block-coordinate updates for\nboth the primal and dual variables, catering to both convex and non-convex\nsettings. We provide theoretical convergence rate analyses, demonstrating\nsignificant improvements over prior approaches. Our experimental results, based\non multiple benchmark datasets, validate the superior performance of our\nalgorithms, showcasing faster convergence and better generalization. This work\nadvances the state of the art in TPAUC optimization and offers practical tools\nfor real-world machine learning applications."}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."}
{"id": "2505.21959", "pdf": "https://arxiv.org/pdf/2505.21959", "abs": "https://arxiv.org/abs/2505.21959", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "categories": ["cs.LG", "cs.CL"], "comment": "Superalignment. arXiv admin note: substantial text overlap with\n  arXiv:2410.04571", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called \\textbf{EnsemW2S}, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4\\%, and 3.2\\%\nimprovements on ID datasets and, upto 6\\% and 2.28\\% on OOD datasets for\nexperts and student models respectively, underscoring the effectiveness of our\nproposed method in advancing W2S generalization."}
{"id": "2505.22660", "pdf": "https://arxiv.org/pdf/2505.22660", "abs": "https://arxiv.org/abs/2505.22660", "authors": ["Mihir Prabhudesai", "Lili Chen", "Alex Ippoliti", "Katerina Fragkiadaki", "Hao Liu", "Deepak Pathak"], "title": "Maximizing Confidence Alone Improves Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve\nsignificant advances in many fields. Most recently, RL has empowered frontier\nlanguage models to solve challenging math, science, and coding problems.\nHowever, central to any RL algorithm is the reward function, and reward\nengineering is a notoriously difficult problem in any domain. In this paper, we\npropose RENT: Reinforcement Learning via Entropy Minimization -- a fully\nunsupervised RL method that requires no external reward or ground-truth\nanswers, and instead uses the model's entropy of its underlying distribution as\nan intrinsic reward. We find that by reinforcing the chains of thought that\nyield high model confidence on its generated answers, the model improves its\nreasoning ability. In our experiments, we showcase these improvements on an\nextensive suite of commonly-used reasoning benchmarks, including GSM8K,\nMATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and\nMistral families. The generality of our unsupervised learning method lends\nitself to applicability in a wide range of domains where external supervision\nis limited or unavailable."}
{"id": "2505.21972", "pdf": "https://arxiv.org/pdf/2505.21972", "abs": "https://arxiv.org/abs/2505.21972", "authors": ["Patrick Vossler", "Fan Xia", "Yifan Mai", "Jean Feng"], "title": "Judging LLMs on a Simplex", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "28 pages, 7 figures", "summary": "Automated evaluation of free-form outputs from large language models (LLMs)\nis challenging because many distinct answers can be equally valid. A common\npractice is to use LLMs themselves as judges, but the theoretical properties of\nthis approach are not yet well understood. We show that a geometric framework\nthat represents both judges and candidates as points on a probability simplex\ncan provide helpful insight on what is or is not identifiable using LLM judges.\nOur theoretical analysis uncovers a \"phase transition\" in ranking\nidentifiability: for binary scoring systems, true rankings are identifiable\neven with weak judges under mild assumptions, while rankings become\nnon-identifiable for three or more scoring levels even with infinite data,\nabsent additional prior knowledge. This non-identifiability highlights how\nuncertainty in rankings stems from not only aleatoric uncertainty (i.e.,\ninherent stochasticity in the data) but also epistemic uncertainty regarding\nwhich assumptions hold, an aspect that has received limited attention until\nnow. To integrate both types of uncertainty, we use Bayesian inference to\nencode assumptions as priors and conduct sensitivity analysis of ranking\nestimates and credible intervals. Empirical evaluations across multiple\nbenchmarks demonstrate that Bayesian inference yields more accurate rankings\nand substantially improves coverage rates. These results underscore the\nimportance of taking a more holistic approach to uncertainty quantification\nwhen using LLMs as judges."}
{"id": "2505.21974", "pdf": "https://arxiv.org/pdf/2505.21974", "abs": "https://arxiv.org/abs/2505.21974", "authors": ["Yu-Heng Hung", "Kai-Jie Lin", "Yu-Heng Lin", "Chien-YiWang", "Cheng Sun", "Ping-Chun Hsieh"], "title": "BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian optimization (BO) offers an efficient pipeline for optimizing\nblack-box functions with the help of a Gaussian process prior and an\nacquisition function (AF). Recently, in the context of single-objective BO,\nlearning-based AFs witnessed promising empirical results given its favorable\nnon-myopic nature. Despite this, the direct extension of these approaches to\nmulti-objective Bayesian optimization (MOBO) suffer from the\n\\textit{hypervolume identifiability issue}, which results from the\nnon-Markovian nature of MOBO problems. To tackle this, inspired by the\nnon-Markovian RL literature and the success of Transformers in language\nmodeling, we present a generalized deep Q-learning framework and propose\n\\textit{BOFormer}, which substantiates this framework for MOBO via sequence\nmodeling. Through extensive evaluation, we demonstrate that BOFormer constantly\noutperforms the benchmark rule-based and learning-based algorithms in various\nsynthetic MOBO and real-world multi-objective hyperparameter optimization\nproblems. We have made the source code publicly available to encourage further\nresearch in this direction."}
{"id": "2505.21978", "pdf": "https://arxiv.org/pdf/2505.21978", "abs": "https://arxiv.org/abs/2505.21978", "authors": ["Wanfu Gao", "Zengyao Man", "Zebin He", "Yuhao Tang", "Jun Gao", "Kunpeng Liu"], "title": "Two-Stage Feature Generation with Transformer and Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Feature generation is a critical step in machine learning, aiming to enhance\nmodel performance by capturing complex relationships within the data and\ngenerating meaningful new features. Traditional feature generation methods\nheavily rely on domain expertise and manual intervention, making the process\nlabor-intensive and challenging to adapt to different scenarios. Although\nautomated feature generation techniques address these issues to some extent,\nthey often face challenges such as feature redundancy, inefficiency in feature\nspace exploration, and limited adaptability to diverse datasets and tasks. To\naddress these problems, we propose a Two-Stage Feature Generation (TSFG)\nframework, which integrates a Transformer-based encoder-decoder architecture\nwith Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG\nleverages the Transformer's self-attention mechanism to efficiently represent\nand transform features, capturing complex dependencies within the data. PPO\nfurther enhances TSFG by dynamically adjusting the feature generation strategy\nbased on task-specific feedback, optimizing the process for improved\nperformance and adaptability. TSFG dynamically generates high-quality feature\nsets, significantly improving the predictive performance of machine learning\nmodels. Experimental results demonstrate that TSFG outperforms existing\nstate-of-the-art methods in terms of feature quality and adaptability."}
{"id": "2505.21987", "pdf": "https://arxiv.org/pdf/2505.21987", "abs": "https://arxiv.org/abs/2505.21987", "authors": ["Zhendong Mi", "Zhenglun Kong", "Geng Yuan", "Shaoyi Huang"], "title": "ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning", "categories": ["cs.LG", "I.2.6; I.2.7"], "comment": "9 pages, 2 figures, 13 tables", "summary": "With the rapid expansion of large language models (LLMs), the demand for\nmemory and computational resources has grown significantly. Recent advances in\nLLM pruning aim to reduce the size and computational cost of these models.\nHowever, existing methods often suffer from either suboptimal pruning\nperformance or low time efficiency during the pruning process. In this work, we\npropose an efficient and effective pruning method that simultaneously achieves\nhigh pruning performance and fast pruning speed with improved calibration\nefficiency. Our approach introduces two key innovations: (1) An activation\ncosine similarity loss-guided pruning metric, which considers the angular\ndeviation of the output activation between the dense and pruned models. (2) An\nactivation variance-guided pruning metric, which helps preserve semantic\ndistinctions in output activations after pruning, enabling effective pruning\nwith shorter input sequences. These two components can be readily combined to\nenhance LLM pruning in both accuracy and efficiency. Experimental results show\nthat our method achieves up to an 18% reduction in perplexity and up to 63%\ndecrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT."}
{"id": "2505.22014", "pdf": "https://arxiv.org/pdf/2505.22014", "abs": "https://arxiv.org/abs/2505.22014", "authors": ["Jörg K. H. Franke", "Urs Spiegelhalter", "Marianna Nezhurina", "Jenia Jitsev", "Frank Hutter", "Michael Hefenbrock"], "title": "Learning in Compact Spaces with Approximately Normalized Transformers", "categories": ["cs.LG"], "comment": "Preprint", "summary": "In deep learning, regularization and normalization are common solutions for\nchallenges such as overfitting, numerical instabilities, and the increasing\nvariance in the residual stream. An alternative approach is to force all\nparameters and representations to lie on a hypersphere. This removes the need\nfor regularization and increases convergence speed, but comes with additional\ncosts. In this work, we propose a more holistic but approximate normalization\n(anTransformer). Our approach constrains the norm of parameters and normalizes\nall representations via scalar multiplications motivated by the tight\nconcentration of the norms of high-dimensional random vectors. When applied to\nGPT training, we observe a 40% faster convergence compared to models with QK\nnormalization, with less than 3% additional runtime. Deriving scaling laws for\nanGPT, we found our method enables training with larger batch sizes and fewer\nhyperparameters, while matching the favorable scaling characteristics of\nclassic GPT architectures."}
{"id": "2505.22028", "pdf": "https://arxiv.org/pdf/2505.22028", "abs": "https://arxiv.org/abs/2505.22028", "authors": ["Zi-Hao Zhou", "Jun-Jie Wang", "Tong Wei", "Min-Ling Zhang"], "title": "Weakly-Supervised Contrastive Learning for Imprecise Class Labels", "categories": ["cs.LG"], "comment": "38 pages, 2 figures, 11 tables", "summary": "Contrastive learning has achieved remarkable success in learning effective\nrepresentations, with supervised contrastive learning often outperforming\nself-supervised approaches. However, in real-world scenarios, data annotations\nare often ambiguous or inaccurate, meaning that class labels may not reliably\nindicate whether two examples belong to the same class. This limitation\nrestricts the applicability of supervised contrastive learning. To address this\nchallenge, we introduce the concept of ``continuous semantic similarity'' to\ndefine positive and negative pairs. Instead of directly relying on imprecise\nclass labels, we measure the semantic similarity between example pairs, which\nquantifies how closely they belong to the same category by iteratively refining\nweak supervisory signals. Based on this concept, we propose a graph-theoretic\nframework for weakly-supervised contrastive learning, where semantic similarity\nserves as the graph weights. Our framework is highly versatile and can be\napplied to many weakly-supervised learning scenarios. We demonstrate its\neffectiveness through experiments in two common settings, i.e., noisy label and\npartial label learning, where existing methods can be easily integrated to\nsignificantly improve performance. Theoretically, we establish an error bound\nfor our approach, showing that it can approximate supervised contrastive\nlearning under mild conditions. The implementation code is available at\nhttps://github.com/Speechless-10308/WSC."}
{"id": "2505.22041", "pdf": "https://arxiv.org/pdf/2505.22041", "abs": "https://arxiv.org/abs/2505.22041", "authors": ["Michael Grohs", "Adrian Rebmann", "Jana-Rebecca Rehse"], "title": "Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation", "categories": ["cs.LG"], "comment": "Accepted at the BPM Forum, located at the International Conference on\n  Business Process Management (BPM) 2025", "summary": "Conformance checking techniques detect undesired process behavior by\ncomparing process executions that are recorded in event logs to desired\nbehavior that is captured in a dedicated process model. If such models are not\navailable, conformance checking techniques are not applicable, but\norganizations might still be interested in detecting undesired behavior in\ntheir processes. To enable this, existing approaches use Large Language Models\n(LLMs), assuming that they can learn to distinguish desired from undesired\nbehavior through fine-tuning. However, fine-tuning is highly resource-intensive\nand the fine-tuned LLMs often do not generalize well. To address these\nlimitations, we propose an approach that requires neither a dedicated process\nmodel nor resource-intensive fine-tuning to detect undesired process behavior.\nInstead, we use Retrieval Augmented Generation (RAG) to provide an LLM with\ndirect access to a knowledge base that contains both desired and undesired\nprocess behavior from other processes, assuming that the LLM can transfer this\nknowledge to the process at hand. Our evaluation shows that our approach\noutperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that\nRAG is a viable alternative to resource-intensive fine-tuning, particularly\nwhen enriched with relevant context from the event log, such as frequent traces\nand activities."}
{"id": "2505.22042", "pdf": "https://arxiv.org/pdf/2505.22042", "abs": "https://arxiv.org/abs/2505.22042", "authors": ["Hao Yang", "Haoxuan Li", "Mengyue Yang", "Xu Chen", "Mingming Gong"], "title": "Estimating the Effects of Sample Training Orders for Large Language Models without Retraining", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The order of training samples plays a crucial role in large language models\n(LLMs), significantly impacting both their external performance and internal\nlearning dynamics. Traditional methods for investigating this effect generally\nrequire retraining the model with various sample orders, which is\ncomputationally infeasible for LLMs. In this work, we improve traditional\nmethods by designing a retraining-free framework. By approximating Adam\noptimizer updates with first- and second-order Taylor expansions and utilizing\nrandom projection methods to store intermediate checkpoints, our framework can\nefficiently estimate model parameters for arbitrary training sample orders.\nNext, we apply our framework to two downstream research problems: (1) Training\ncurriculum design for LLMs -- we base our retraining-free framework to propose\na novel curriculum learning strategy that augments curriculum proposals with\nestimated model performances, enabling more informed sample scheduling. (2)\nLLMs' memorization and generalization effect analysis -- we use our\nretraining-free framework to estimate how the positions of training samples\ninfluence LLMs' capacity for memorization and generalization. We conduct\nextensive experiments to validate the effectiveness of our retraining-free\nframework in reproducing the true model performances, and further demonstrate\nits potential in optimizing LLM training curricula and analyzing the\nmemorization and generalization effects of LLMs."}
{"id": "2505.22049", "pdf": "https://arxiv.org/pdf/2505.22049", "abs": "https://arxiv.org/abs/2505.22049", "authors": ["Laetitia Chapel", "Romain Tavenard", "Samuel Vaiter"], "title": "Differentiable Generalized Sliced Wasserstein Plans", "categories": ["cs.LG"], "comment": null, "summary": "Optimal Transport (OT) has attracted significant interest in the machine\nlearning community, not only for its ability to define meaningful distances\nbetween probability distributions -- such as the Wasserstein distance -- but\nalso for its formulation of OT plans. Its computational complexity remains a\nbottleneck, though, and slicing techniques have been developed to scale OT to\nlarge datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a\nsingle one-dimensional plan back to the original multidimensional space,\nfinally selecting the slice that yields the lowest Wasserstein distance as an\napproximation of the full OT plan. Despite its computational and theoretical\nadvantages, min-SWGG inherits typical limitations of slicing methods: (i) the\nnumber of required slices grows exponentially with the data dimension, and (ii)\nit is constrained to linear projections. Here, we reformulate min-SWGG as a\nbilevel optimization problem and propose a differentiable approximation scheme\nto efficiently identify the optimal slice, even in high-dimensional settings.\nWe furthermore define its generalized extension for accommodating to data\nliving on manifolds. Finally, we demonstrate the practical value of our\napproach in various applications, including gradient flows on manifolds and\nhigh-dimensional spaces, as well as a novel sliced OT-based conditional flow\nmatching for image generation -- where fast computation of transport plans is\nessential."}
{"id": "2505.22074", "pdf": "https://arxiv.org/pdf/2505.22074", "abs": "https://arxiv.org/abs/2505.22074", "authors": ["Coşku Can Horuz", "Geoffrey Kasenbacher", "Saya Higuchi", "Sebastian Kairat", "Jendrik Stoltz", "Moritz Pesl", "Bernhard A. Moser", "Christoph Linse", "Thomas Martinetz", "Sebastian Otte"], "title": "The Resurrection of the ReLU", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modeling sophisticated activation functions within deep learning\narchitectures has evolved into a distinct research direction. Functions such as\nGELU, SELU, and SiLU offer smooth gradients and improved convergence\nproperties, making them popular choices in state-of-the-art models. Despite\nthis trend, the classical ReLU remains appealing due to its simplicity,\ninherent sparsity, and other advantageous topological characteristics. However,\nReLU units are prone to becoming irreversibly inactive - a phenomenon known as\nthe dying ReLU problem - which limits their overall effectiveness. In this\nwork, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,\nplug-and-play regularizer for deep architectures. SUGAR preserves the standard\nReLU function during the forward pass but replaces its derivative in the\nbackward pass with a smooth surrogate that avoids zeroing out gradients. We\ndemonstrate that SUGAR, when paired with a well-chosen surrogate function,\nsubstantially enhances generalization performance over convolutional network\narchitectures such as VGG-16 and ResNet-18, providing sparser activations while\neffectively resurrecting dead ReLUs. Moreover, we show that even in modern\narchitectures like Conv2NeXt and Swin Transformer - which typically employ GELU\n- substituting these with SUGAR yields competitive and even slightly superior\nperformance. These findings challenge the prevailing notion that advanced\nactivation functions are necessary for optimal performance. Instead, they\nsuggest that the conventional ReLU, particularly with appropriate gradient\nhandling, can serve as a strong, versatile revived classic across a broad range\nof deep learning vision models."}
{"id": "2505.22081", "pdf": "https://arxiv.org/pdf/2505.22081", "abs": "https://arxiv.org/abs/2505.22081", "authors": ["Shun Sato", "Issei Sato"], "title": "Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?", "categories": ["cs.LG"], "comment": null, "summary": "Symbolic regression aims to discover mathematical equations that fit given\nnumerical data. It has been applied in various fields of scientific research,\nsuch as producing human-readable expressions that explain physical phenomena.\nRecently, Neural symbolic regression (NSR) methods that involve Transformers\npre-trained on large-scale synthetic datasets have gained attention. While\nthese methods offer advantages such as short inference time, they suffer from\nlow performance, particularly when the number of input variables is large. In\nthis study, we hypothesized that this limitation stems from the memorization\nbias of Transformers in symbolic regression. We conducted a quantitative\nevaluation of this bias in Transformers using a synthetic dataset and found\nthat Transformers rarely generate expressions not present in the training data.\nAdditional theoretical analysis reveals that this bias arises from the\nTransformer's inability to construct expressions compositionally while\nverifying their numerical validity. We finally examined if tailoring test-time\nstrategies can lead to reduced memorization bias and better performance. We\nempirically demonstrate that providing additional information to the model at\ntest time can significantly mitigate memorization bias. On the other hand, we\nalso find that reducing memorization bias does not necessarily correlate with\nimproved performance. These findings contribute to a deeper understanding of\nthe limitations of NSR approaches and offer a foundation for designing more\nrobust, generalizable symbolic regression methods. Code is available at\nhttps://github.com/Shun-0922/Mem-Bias-NSR ."}
{"id": "2505.22108", "pdf": "https://arxiv.org/pdf/2505.22108", "abs": "https://arxiv.org/abs/2505.22108", "authors": ["Santhosh Parampottupadam", "Melih Coşğun", "Sarthak Pati", "Maximilian Zenk", "Saikat Roy", "Dimitrios Bounias", "Benjamin Hamm", "Sinem Sav", "Ralf Floca", "Klaus Maier-Hein"], "title": "Inclusive, Differentially Private Federated Learning for Clinical Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) offers a promising approach for training clinical AI\nmodels without centralizing sensitive patient data. However, its real-world\nadoption is hindered by challenges related to privacy, resource constraints,\nand compliance. Existing Differential Privacy (DP) approaches often apply\nuniform noise, which disproportionately degrades model performance, even among\nwell-compliant institutions. In this work, we propose a novel compliance-aware\nFL framework that enhances DP by adaptively adjusting noise based on\nquantifiable client compliance scores. Additionally, we introduce a compliance\nscoring tool based on key healthcare and security standards to promote secure,\ninclusive, and equitable participation across diverse clinical settings.\nExtensive experiments on public datasets demonstrate that integrating\nunder-resourced, less compliant clinics with highly regulated institutions\nyields accuracy improvements of up to 15% over traditional FL. This work\nadvances FL by balancing privacy, compliance, and performance, making it a\nviable solution for real-world clinical workflows in global healthcare."}
{"id": "2505.22109", "pdf": "https://arxiv.org/pdf/2505.22109", "abs": "https://arxiv.org/abs/2505.22109", "authors": ["Paul Krzakala", "Gabriel Melo", "Charlotte Laclau", "Florence d'Alché-Buc", "Rémi Flamary"], "title": "The quest for the GRAph Level autoEncoder (GRALE)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although graph-based learning has attracted a lot of attention, graph\nrepresentation learning is still a challenging task whose resolution may impact\nkey application fields such as chemistry or biology. To this end, we introduce\nGRALE, a novel graph autoencoder that encodes and decodes graphs of varying\nsizes into a shared embedding space. GRALE is trained using an Optimal\nTransport-inspired loss that compares the original and reconstructed graphs and\nleverages a differentiable node matching module, which is trained jointly with\nthe encoder and decoder. The proposed attention-based architecture relies on\nEvoformer, the core component of AlphaFold, which we extend to support both\ngraph encoding and decoding. We show, in numerical experiments on simulated and\nmolecular data, that GRALE enables a highly general form of pre-training,\napplicable to a wide range of downstream tasks, from classification and\nregression to more complex tasks such as graph interpolation, editing,\nmatching, and prediction."}
{"id": "2505.22114", "pdf": "https://arxiv.org/pdf/2505.22114", "abs": "https://arxiv.org/abs/2505.22114", "authors": ["MaryBeth Defrance", "Guillaume Bied", "Maarten Buyl", "Jefrey Lijffijt", "Tijl De Bie"], "title": "BiMi Sheets: Infosheets for bias mitigation methods", "categories": ["cs.LG"], "comment": null, "summary": "Over the past 15 years, hundreds of bias mitigation methods have been\nproposed in the pursuit of fairness in machine learning (ML). However,\nalgorithmic biases are domain-, task-, and model-specific, leading to a\n`portability trap': bias mitigation solutions in one context may not be\nappropriate in another. Thus, a myriad of design choices have to be made when\ncreating a bias mitigation method, such as the formalization of fairness it\npursues, and where and how it intervenes in the ML pipeline. This creates\nchallenges in benchmarking and comparing the relative merits of different bias\nmitigation methods, and limits their uptake by practitioners.\n  We propose BiMi Sheets as a portable, uniform guide to document the design\nchoices of any bias mitigation method. This enables researchers and\npractitioners to quickly learn its main characteristics and to compare with\ntheir desiderata. Furthermore, the sheets' structure allow for the creation of\na structured database of bias mitigation methods. In order to foster the\nsheets' adoption, we provide a platform for finding and creating BiMi Sheets at\nbimisheet.com."}
{"id": "2505.22151", "pdf": "https://arxiv.org/pdf/2505.22151", "abs": "https://arxiv.org/abs/2505.22151", "authors": ["Claude Formanek", "Omayma Mahjoub", "Louay Ben Nessir", "Sasha Abramowitz", "Ruan de Kock", "Wiem Khlifi", "Simon Du Toit", "Felix Chalumeau", "Daniel Rajaonarivonivelomanantsoa", "Arnol Fokam", "Siddarth Singh", "Ulrich Mbou Sob", "Arnu Pretorius"], "title": "Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL", "categories": ["cs.LG"], "comment": null, "summary": "A key challenge in offline multi-agent reinforcement learning (MARL) is\nachieving effective many-agent multi-step coordination in complex environments.\nIn this work, we propose Oryx, a novel algorithm for offline cooperative MARL\nto directly address this challenge. Oryx adapts the recently proposed\nretention-based architecture Sable and combines it with a sequential form of\nimplicit constraint Q-learning (ICQ), to develop a novel offline\nauto-regressive policy update scheme. This allows Oryx to solve complex\ncoordination challenges while maintaining temporal coherence over lengthy\ntrajectories. We evaluate Oryx across a diverse set of benchmarks from prior\nworks (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and\ncontinuous control, varying in scale and difficulty. Oryx achieves\nstate-of-the-art performance on more than 80% of the 65 tested datasets,\noutperforming prior offline MARL methods and demonstrating robust\ngeneralisation across domains with many agents and long horizons. Finally, we\nintroduce new datasets to push the limits of many-agent coordination in offline\nMARL, and demonstrate Oryx's superior ability to scale effectively in such\nsettings. We will make all of our datasets, experimental data, and code\navailable upon publication."}
{"id": "2505.22152", "pdf": "https://arxiv.org/pdf/2505.22152", "abs": "https://arxiv.org/abs/2505.22152", "authors": ["Dominik Fuchsgruber", "Tom Wollschläger", "Johannes Bordne", "Stephan Günnemann"], "title": "Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "While uncertainty estimation for graphs recently gained traction, most\nmethods rely on homophily and deteriorate in heterophilic settings. We address\nthis by analyzing message passing neural networks from an information-theoretic\nperspective and developing a suitable analog to data processing inequality to\nquantify information throughout the model's layers. In contrast to non-graph\ndomains, information about the node-level prediction target can increase with\nmodel depth if a node's features are semantically different from its neighbors.\nTherefore, on heterophilic graphs, the latent embeddings of an MPNN each\nprovide different information about the data distribution - different from\nhomophilic settings. This reveals that considering all node representations\nsimultaneously is a key design principle for epistemic uncertainty estimation\non graphs beyond homophily. We empirically confirm this with a simple post-hoc\ndensity estimator on the joint node embedding space that provides\nstate-of-the-art uncertainty on heterophilic graphs. At the same time, it\nmatches prior work on homophilic graphs without explicitly exploiting homophily\nthrough post-processing."}
{"id": "2505.22158", "pdf": "https://arxiv.org/pdf/2505.22158", "abs": "https://arxiv.org/abs/2505.22158", "authors": ["Rustem Takhanov"], "title": "The informativeness of the gradient revisited", "categories": ["cs.LG"], "comment": null, "summary": "In the past decade gradient-based deep learning has revolutionized several\napplications. However, this rapid advancement has highlighted the need for a\ndeeper theoretical understanding of its limitations. Research has shown that,\nin many practical learning tasks, the information contained in the gradient is\nso minimal that gradient-based methods require an exceedingly large number of\niterations to achieve success. The informativeness of the gradient is typically\nmeasured by its variance with respect to the random selection of a target\nfunction from a hypothesis class.\n  We use this framework and give a general bound on the variance in terms of a\nparameter related to the pairwise independence of the target function class and\nthe collision entropy of the input distribution. Our bound scales as $\n\\tilde{\\mathcal{O}}(\\varepsilon+e^{-\\frac{1}{2}\\mathcal{E}_c}) $, where $\n\\tilde{\\mathcal{O}} $ hides factors related to the regularity of the learning\nmodel and the loss function, $ \\varepsilon $ measures the pairwise independence\nof the target function class and $\\mathcal{E}_c$ is the collision entropy of\nthe input distribution.\n  To demonstrate the practical utility of our bound, we apply it to the class\nof Learning with Errors (LWE) mappings and high-frequency functions. In\naddition to the theoretical analysis, we present experiments to understand\nbetter the nature of recent deep learning-based attacks on LWE."}
{"id": "2505.22196", "pdf": "https://arxiv.org/pdf/2505.22196", "abs": "https://arxiv.org/abs/2505.22196", "authors": ["Jingyi Cui", "Hongwei Wen", "Yisen Wang"], "title": "An Augmentation-Aware Theory for Self-Supervised Contrastive Learning", "categories": ["cs.LG"], "comment": "Accepted to ICML2025", "summary": "Self-supervised contrastive learning has emerged as a powerful tool in\nmachine learning and computer vision to learn meaningful representations from\nunlabeled data. Meanwhile, its empirical success has encouraged many\ntheoretical studies to reveal the learning mechanisms. However, in the existing\ntheoretical research, the role of data augmentation is still under-exploited,\nespecially the effects of specific augmentation types. To fill in the blank, we\nfor the first time propose an augmentation-aware error bound for\nself-supervised contrastive learning, showing that the supervised risk is\nbounded not only by the unsupervised risk, but also explicitly by a trade-off\ninduced by data augmentation. Then, under a novel semantic label assumption, we\ndiscuss how certain augmentation methods affect the error bound. Lastly, we\nconduct both pixel- and representation-level experiments to verify our proposed\ntheoretical results."}
{"id": "2505.22199", "pdf": "https://arxiv.org/pdf/2505.22199", "abs": "https://arxiv.org/abs/2505.22199", "authors": ["Xinyue Hu", "Zhibin Duan", "Bo Chen", "Mingyuan Zhou"], "title": "Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Although deep neural networks have demonstrated significant success due to\ntheir powerful expressiveness, most models struggle to meet practical\nrequirements for uncertainty estimation. Concurrently, the entangled nature of\ndeep neural networks leads to a multifaceted problem, where various localized\nexplanation techniques reveal that multiple unrelated features influence the\ndecisions, thereby undermining interpretability. To address these challenges,\nwe develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates\ndeep neural networks as a conditional Bayesian non-negative factor analysis. By\nleveraging stochastic latent variables, the BNDL can model complex dependencies\nand provide robust uncertainty estimation. Moreover, the sparsity and\nnon-negativity of the latent variables encourage the model to learn\ndisentangled representations and decision layers, thereby improving\ninterpretability. We also offer theoretical guarantees that BNDL can achieve\neffective disentangled learning. In addition, we developed a corresponding\nvariational inference method utilizing a Weibull variational inference network\nto approximate the posterior distribution of the latent variables. Our\nexperimental results demonstrate that with enhanced disentanglement\ncapabilities, BNDL not only improves the model's accuracy but also provides\nreliable uncertainty estimation and improved interpretability."}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning."}
{"id": "2505.22208", "pdf": "https://arxiv.org/pdf/2505.22208", "abs": "https://arxiv.org/abs/2505.22208", "authors": ["Yosuke Oyama", "Yusuke Majima", "Eiji Ohta", "Yasufumi Sakai"], "title": "LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models", "categories": ["cs.LG"], "comment": "24 pages, 9 figures", "summary": "Neural network potentials (NNPs) are crucial for accelerating computational\nmaterials science by surrogating density functional theory (DFT) calculations.\nImproving their accuracy is possible through pre-training and fine-tuning,\nwhere an NNP model is first pre-trained on a large-scale dataset and then\nfine-tuned on a smaller target dataset. However, this approach is\ncomputationally expensive, mainly due to the cost of DFT-based dataset labeling\nand load imbalances during large-scale pre-training. To address this, we\npropose LaMM, a semi-supervised pre-training method incorporating improved\ndenoising self-supervised learning and a load-balancing algorithm for efficient\nmulti-node training. We demonstrate that our approach effectively leverages a\nlarge-scale dataset of $\\sim$300 million semi-labeled samples to train a single\nNNP model, resulting in improved fine-tuning performance in terms of both speed\nand accuracy."}
{"id": "2505.22224", "pdf": "https://arxiv.org/pdf/2505.22224", "abs": "https://arxiv.org/abs/2505.22224", "authors": ["Senne Berden", "Ali İrfan Mahmutoğulları", "Dimos Tsouros", "Tias Guns"], "title": "Solver-Free Decision-Focused Learning for Linear Optimization Problems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mathematical optimization is a fundamental tool for decision-making in a wide\nrange of applications. However, in many real-world scenarios, the parameters of\nthe optimization problem are not known a priori and must be predicted from\ncontextual features. This gives rise to predict-then-optimize problems, where a\nmachine learning model predicts problem parameters that are then used to make\ndecisions via optimization. A growing body of work on decision-focused learning\n(DFL) addresses this setting by training models specifically to produce\npredictions that maximize downstream decision quality, rather than accuracy.\nWhile effective, DFL is computationally expensive, because it requires solving\nthe optimization problem with the predicted parameters at each loss evaluation.\nIn this work, we address this computational bottleneck for linear optimization\nproblems, a common class of problems in both DFL literature and real-world\napplications. We propose a solver-free training method that exploits the\ngeometric structure of linear optimization to enable efficient training with\nminimal degradation in solution quality. Our method is based on the insight\nthat a solution is optimal if and only if it achieves an objective value that\nis at least as good as that of its adjacent vertices on the feasible polytope.\nBuilding on this, our method compares the estimated quality of the ground-truth\noptimal solution with that of its precomputed adjacent vertices, and uses this\nas loss function. Experiments demonstrate that our method significantly reduces\ncomputational cost while maintaining high decision quality."}
{"id": "2505.22235", "pdf": "https://arxiv.org/pdf/2505.22235", "abs": "https://arxiv.org/abs/2505.22235", "authors": ["Amon Lahr", "Johannes Köhler", "Anna Scampicchio", "Melanie N. Zeilinger"], "title": "Optimal kernel regression bounds under energy-bounded noise", "categories": ["cs.LG"], "comment": null, "summary": "Non-conservative uncertainty bounds are key for both assessing an estimation\nalgorithm's accuracy and in view of downstream tasks, such as its deployment in\nsafety-critical contexts. In this paper, we derive a tight, non-asymptotic\nuncertainty bound for kernel-based estimation, which can also handle correlated\nnoise sequences. Its computation relies on a mild norm-boundedness assumption\non the unknown function and the noise, returning the worst-case function\nrealization within the hypothesis class at an arbitrary query input location.\nThe value of this function is shown to be given in terms of the posterior mean\nand covariance of a Gaussian process for an optimal choice of the measurement\nnoise covariance. By rigorously analyzing the proposed approach and comparing\nit with other results in the literature, we show its effectiveness in returning\ntight and easy-to-compute bounds for kernel-based estimates."}
{"id": "2505.22252", "pdf": "https://arxiv.org/pdf/2505.22252", "abs": "https://arxiv.org/abs/2505.22252", "authors": ["Magdalena Proszewska", "Tomasz Danel", "Dawid Rymarczyk"], "title": "B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data", "categories": ["cs.LG", "cs.CE"], "comment": "26 pages, 16 figures, 5 tables", "summary": "Understanding the reasoning behind deep learning model predictions is crucial\nin cheminformatics and drug discovery, where molecular design determines their\nproperties. However, current evaluation frameworks for Explainable AI (XAI) in\nthis domain often rely on artificial datasets or simplified tasks, employing\ndata-derived metrics that fail to capture the complexity of real-world\nscenarios and lack a direct link to explanation faithfulness. To address this,\nwe introduce B-XAIC, a novel benchmark constructed from real-world molecular\ndata and diverse tasks with known ground-truth rationales for assigned labels.\nThrough a comprehensive evaluation using B-XAIC, we reveal limitations of\nexisting XAI methods for Graph Neural Networks (GNNs) in the molecular domain.\nThis benchmark provides a valuable resource for gaining deeper insights into\nthe faithfulness of XAI, facilitating the development of more reliable and\ninterpretable models."}
{"id": "2505.22254", "pdf": "https://arxiv.org/pdf/2505.22254", "abs": "https://arxiv.org/abs/2505.22254", "authors": ["Xiangxiang Dai", "Xiaowei Sun", "Jinhang Zuo", "Xutong Liu", "John C. S. Lui"], "title": "A Unified Online-Offline Framework for Co-Branding Campaign Recommendations", "categories": ["cs.LG"], "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining, 2025", "summary": "Co-branding has become a vital strategy for businesses aiming to expand\nmarket reach within recommendation systems. However, identifying effective\ncross-industry partnerships remains challenging due to resource imbalances,\nuncertain brand willingness, and ever-changing market conditions. In this\npaper, we provide the first systematic study of this problem and propose a\nunified online-offline framework to enable co-branding recommendations. Our\napproach begins by constructing a bipartite graph linking ``initiating'' and\n``target'' brands to quantify co-branding probabilities and assess market\nbenefits. During the online learning phase, we dynamically update the graph in\nresponse to market feedback, while striking a balance between exploring new\ncollaborations for long-term gains and exploiting established partnerships for\nimmediate benefits. To address the high initial co-branding costs, our\nframework mitigates redundant exploration, thereby enhancing short-term\nperformance while ensuring sustainable strategic growth. In the offline\noptimization phase, our framework consolidates the interests of multiple\nsub-brands under the same parent brand to maximize overall returns, avoid\nexcessive investment in single sub-brands, and reduce unnecessary costs\nassociated with over-prioritizing a single sub-brand. We present a theoretical\nanalysis of our approach, establishing a highly nontrivial sublinear regret\nbound for online learning in the complex co-branding problem, and enhancing the\napproximation guarantee for the NP-hard offline budget allocation optimization.\nExperiments on both synthetic and real-world co-branding datasets demonstrate\nthe practical effectiveness of our framework, with at least 12\\% improvement."}
{"id": "2505.22255", "pdf": "https://arxiv.org/pdf/2505.22255", "abs": "https://arxiv.org/abs/2505.22255", "authors": ["Vadim Kurochkin", "Yaroslav Aksenov", "Daniil Laptev", "Daniil Gavrilov", "Nikita Balagansky"], "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework."}
{"id": "2505.22257", "pdf": "https://arxiv.org/pdf/2505.22257", "abs": "https://arxiv.org/abs/2505.22257", "authors": ["Youssef Mroueh", "Nicolas Dupuis", "Brian Belgodere", "Apoorva Nitsure", "Mattia Rigotti", "Kristjan Greenewald", "Jiri Navratil", "Jerret Ross", "Jesus Rios"], "title": "Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit Group Relative Policy Optimization (GRPO) in both on-policy and\noff-policy optimization regimes. Our motivation comes from recent work on\noff-policy Proximal Policy Optimization (PPO), which improves training\nstability, sampling efficiency, and memory usage. In addition, a recent\nanalysis of GRPO suggests that estimating the advantage function with\noff-policy samples could be beneficial. Building on these observations, we\nadapt GRPO to the off-policy setting. We show that both on-policy and\noff-policy GRPO objectives yield an improvement in the reward. This result\nmotivates the use of clipped surrogate objectives in the off-policy version of\nGRPO. We then compare the empirical performance of reinforcement learning with\nverifiable rewards in post-training using both GRPO variants. Our results show\nthat off-policy GRPO either significantly outperforms or performs on par with\nits on-policy counterpart."}
{"id": "2505.22275", "pdf": "https://arxiv.org/pdf/2505.22275", "abs": "https://arxiv.org/abs/2505.22275", "authors": ["Alexander Hagg", "Adam Gaier", "Dominik Wilde", "Alexander Asteroth", "Holger Foysi", "Dirk Reith"], "title": "Full Domain Analysis in Fluid Dynamics", "categories": ["cs.LG", "cs.NE", "68U01", "I.2.1; I.2.6"], "comment": null, "summary": "Novel techniques in evolutionary optimization, simulation and machine\nlearning allow for a broad analysis of domains like fluid dynamics, in which\ncomputation is expensive and flow behavior is complex. Under the term of full\ndomain analysis we understand the ability to efficiently determine the full\nspace of solutions in a problem domain, and analyze the behavior of those\nsolutions in an accessible and interactive manner. The goal of full domain\nanalysis is to deepen our understanding of domains by generating many examples\nof flow, their diversification, optimization and analysis. We define a formal\nmodel for full domain analysis, its current state of the art, and requirements\nof subcomponents. Finally, an example is given to show what we can learn by\nusing full domain analysis. Full domain analysis, rooted in optimization and\nmachine learning, can be a helpful tool in understanding complex systems in\ncomputational physics and beyond."}
{"id": "2505.22306", "pdf": "https://arxiv.org/pdf/2505.22306", "abs": "https://arxiv.org/abs/2505.22306", "authors": ["Zehua Chen", "Yuyang Miao", "Liyuan Wang", "Luyun Fan", "Danilo P. Mandic", "Jun Zhu"], "title": "Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cardiovascular signals such as photoplethysmography (PPG),\nelectrocardiography (ECG), and blood pressure (BP) are inherently correlated\nand complementary, together reflecting the health of cardiovascular system.\nHowever, their joint utilization in real-time monitoring is severely limited by\ndiverse acquisition challenges from noisy wearable recordings to burdened\ninvasive procedures. Here we propose UniCardio, a multi-modal diffusion\ntransformer that reconstructs low-quality signals and synthesizes unrecorded\nsignals in a unified generative framework. Its key innovations include a\nspecialized model architecture to manage the signal modalities involved in\ngeneration tasks and a continual learning paradigm to incorporate varying\nmodality combinations. By exploiting the complementary nature of cardiovascular\nsignals, UniCardio clearly outperforms recent task-specific baselines in signal\ndenoising, imputation, and translation. The generated signals match the\nperformance of ground-truth signals in detecting abnormal health conditions and\nestimating vital signs, even in unseen domains, while ensuring interpretability\nfor human experts. These advantages position UniCardio as a promising avenue\nfor advancing AI-assisted healthcare."}
{"id": "2505.22308", "pdf": "https://arxiv.org/pdf/2505.22308", "abs": "https://arxiv.org/abs/2505.22308", "authors": ["Zachary Shinnick", "Liangze Jiang", "Hemanth Saratchandran", "Anton van den Hengel", "Damien Teney"], "title": "Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Pretraining on large, semantically rich datasets is key for developing\nlanguage models. Surprisingly, recent studies have shown that even synthetic\ndata, generated procedurally through simple semantic-free algorithms, can yield\nsome of the same benefits as natural language pretraining. It is unclear what\nspecific capabilities such simple synthetic data instils in a model, where\nthese capabilities reside in the architecture, and how they manifest within its\nweights. In this short paper, we identify several beneficial forms of\nprocedural data, together with specific algorithmic reasoning skills that\nimprove in small transformers. Our core finding is that different procedural\nrules instil distinct but complementary inductive structures in the model. With\nextensive ablations and partial-transfer experiments, we discover that these\nstructures reside in different parts of the model. Attention layers often carry\nthe most transferable information, but some pretraining rules impart useful\nstructure to MLP blocks instead. Most interestingly, the structures induced by\nmultiple rules can be composed to jointly reinforce multiple capabilities.\nThese results suggest an exciting possibility of disentangling the acquisition\nof knowledge from reasoning in language models, with the goal of improving\ntheir robustness and data efficiency."}
{"id": "2505.22310", "pdf": "https://arxiv.org/pdf/2505.22310", "abs": "https://arxiv.org/abs/2505.22310", "authors": ["Shoaib Ahmed Siddiqui", "Adrian Weller", "David Krueger", "Gintare Karolina Dziugaite", "Michael Curtis Mozer", "Eleni Triantafillou"], "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks."}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2505.22316", "pdf": "https://arxiv.org/pdf/2505.22316", "abs": "https://arxiv.org/abs/2505.22316", "authors": ["Konrad Özdemir", "Lukas Kirchdorfer", "Keyvan Amiri Elyasi", "Han van der Aa", "Heiner Stuckenschmidt"], "title": "Rethinking BPS: A Utility-Based Evaluation Framework", "categories": ["cs.LG"], "comment": null, "summary": "Business process simulation (BPS) is a key tool for analyzing and optimizing\norganizational workflows, supporting decision-making by estimating the impact\nof process changes. The reliability of such estimates depends on the ability of\na BPS model to accurately mimic the process under analysis, making rigorous\naccuracy evaluation essential. However, the state-of-the-art approach to\nevaluating BPS models has two key limitations. First, it treats simulation as a\nforecasting problem, testing whether models can predict unseen future events.\nThis fails to assess how well a model captures the as-is process, particularly\nwhen process behavior changes from train to test period. Thus, it becomes\ndifficult to determine whether poor results stem from an inaccurate model or\nthe inherent complexity of the data, such as unpredictable drift. Second, the\nevaluation approach strongly relies on Earth Mover's Distance-based metrics,\nwhich can obscure temporal patterns and thus yield misleading conclusions about\nsimulation quality. To address these issues, we propose a novel framework that\nevaluates simulation quality based on its ability to generate representative\nprocess behavior. Instead of comparing simulated logs to future real-world\nexecutions, we evaluate whether predictive process monitoring models trained on\nsimulated data perform comparably to those trained on real data for downstream\nanalysis tasks. Empirical results show that our framework not only helps\nidentify sources of discrepancies but also distinguishes between model accuracy\nand data complexity, offering a more meaningful way to assess BPS quality."}
{"id": "2505.22322", "pdf": "https://arxiv.org/pdf/2505.22322", "abs": "https://arxiv.org/abs/2505.22322", "authors": ["Zhengyu Fang", "Zhimeng Jiang", "Huiyuan Chen", "Xiaoge Zhang", "Kaiyu Tang", "Xiao Li", "Jing Li"], "title": "A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have shown strong performance in generating high-quality\ntabular data, but they carry privacy risks by reproducing exact training\nsamples. While prior work focuses on dataset-level augmentation to reduce\nmemorization, little is known about which individual samples contribute most.\nWe present the first data-centric study of memorization dynamics in tabular\ndiffusion models. We quantify memorization for each real sample based on how\nmany generated samples are flagged as replicas, using a relative distance\nratio. Our empirical analysis reveals a heavy-tailed distribution of\nmemorization counts: a small subset of samples contributes disproportionately\nto leakage, confirmed via sample-removal experiments. To understand this, we\ndivide real samples into top- and non-top-memorized groups and analyze their\ntraining-time behaviors. We track when each sample is first memorized and\nmonitor per-epoch memorization intensity (AUC). Memorized samples are memorized\nslightly earlier and show stronger signals in early training. Based on these\ninsights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:\n(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and\n(c) retrain on the filtered dataset. Across multiple tabular datasets and\nmodels, DynamicCut reduces memorization with minimal impact on data diversity\nand downstream performance. It also complements augmentation-based defenses.\nFurthermore, DynamicCut enables cross-model transferability: high-ranked\nsamples identified from one model (e.g., a diffusion model) are also effective\nfor reducing memorization when removed from others, such as GANs and VAEs."}
{"id": "2505.22355", "pdf": "https://arxiv.org/pdf/2505.22355", "abs": "https://arxiv.org/abs/2505.22355", "authors": ["Yongkang Liu", "Xingle Xu", "Ercong Nie", "Zijing Wang", "Shi Feng", "Daling Wang", "Qian Li", "Hinrich Schütze"], "title": "Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable\nto Full Fine-Tuning (FFT) while requiring significantly fewer computing\nresources, making it the go-to choice for researchers. We find that although\nPEFT can achieve competitive results on some benchmarks, its performance falls\nshort of FFT in complex tasks, such as reasoning and instruction-based\nfine-tuning. In this paper, we compare the characteristics of PEFT and FFT in\nterms of representational capacity and robustness based on optimization theory.\nWe theoretically demonstrate that PEFT is a strict subset of FFT. By providing\ntheoretical upper bounds for PEFT, we show that the limited parameter space\nconstrains the model's representational ability, making it more susceptible to\nperturbations. Experiments on 15 datasets encompassing classification,\ngeneration, reasoning, instruction fine-tuning tasks and 11 adversarial test\nsets validate our theories. We hope that these results spark further research\nbeyond the realms of well established PEFT. The source code is in the anonymous\nGithub repository\\footnote{https://github.com/misonsky/PEFTEval}."}
{"id": "2505.22356", "pdf": "https://arxiv.org/pdf/2505.22356", "abs": "https://arxiv.org/abs/2505.22356", "authors": ["Angéline Pouget", "Mohammad Yaghini", "Stephan Rabanser", "Nicolas Papernot"], "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications."}
{"id": "2505.22358", "pdf": "https://arxiv.org/pdf/2505.22358", "abs": "https://arxiv.org/abs/2505.22358", "authors": ["Zhiyi Wan", "Wanrou Du", "Liang Li", "Miao Pan", "Xiaoqi Qin"], "title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark."}
{"id": "2505.22359", "pdf": "https://arxiv.org/pdf/2505.22359", "abs": "https://arxiv.org/abs/2505.22359", "authors": ["Matan Schliserman", "Tomer Koren"], "title": "Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification", "categories": ["cs.LG"], "comment": null, "summary": "We study the generalization performance of unregularized gradient methods for\nseparable linear classification. While previous work mostly deal with the\nbinary case, we focus on the multiclass setting with $k$ classes and establish\nnovel population risk bounds for Gradient Descent for loss functions that decay\nto zero. In this setting, we show risk bounds that reveal that convergence\nrates are crucially influenced by the geometry of the loss template, as\nformalized by Wang and Scott (2024), rather than of the loss function itself.\nParticularly, we establish risk upper bounds that holds for any decay rate of\nthe loss whose template is smooth with respect to the $p$-norm. In the case of\nexponentially decaying losses, our results indicates a contrast between the\n$p=\\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and\n$p=2$ where the risk scales linearly with $k$. To establish this separation\nformally, we also prove a lower bound in the latter scenario, demonstrating\nthat the polynomial dependence on $k$ is unavoidable. Central to our analysis\nis a novel bound on the Rademacher complexity of low-noise vector-valued linear\npredictors with a loss template smooth w.r.t.~general $p$-norms."}
{"id": "2505.22361", "pdf": "https://arxiv.org/pdf/2505.22361", "abs": "https://arxiv.org/abs/2505.22361", "authors": ["Xiangyu Chang", "Xi Chen", "Yining Wang", "Zhiyi Zeng"], "title": "Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper studies a bandit optimization problem where the goal is to\nmaximize a function $f(x)$ over $T$ periods for some unknown strongly concave\nfunction $f$. We consider a new pairwise comparison oracle, where the\ndecision-maker chooses a pair of actions $(x, x')$ for a consecutive number of\nperiods and then obtains an estimate of $f(x)-f(x')$. We show that such a\npairwise comparison oracle finds important applications to joint pricing and\ninventory replenishment problems and network revenue management. The challenge\nin this bandit optimization is twofold. First, the decision-maker not only\nneeds to determine a pair of actions $(x, x')$ but also a stopping time $n$\n(i.e., the number of queries based on $(x, x')$). Second, motivated by our\ninventory application, the estimate of the difference $f(x)-f(x')$ is biased,\nwhich is different from existing oracles in stochastic optimization literature.\nTo address these challenges, we first introduce a discretization technique and\nlocal polynomial approximation to relate this problem to linear bandits. Then\nwe developed a tournament successive elimination technique to localize the\ndiscretized cell and run an interactive batched version of LinUCB algorithm on\ncells. We establish regret bounds that are optimal up to poly-logarithmic\nfactors. Furthermore, we apply our proposed algorithm and analytical framework\nto the two operations management problems and obtain results that improve\nstate-of-the-art results in the existing literature."}
{"id": "2505.22362", "pdf": "https://arxiv.org/pdf/2505.22362", "abs": "https://arxiv.org/abs/2505.22362", "authors": ["Aihu Zhang", "Jiaxing Xu", "Mengcheng Lan", "Shili Xiang", "Yiping Ke"], "title": "Directed Homophily-Aware Graph Neural Network", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved significant success in various\nlearning tasks on graph-structured data. Nevertheless, most GNNs struggle to\ngeneralize to heterophilic neighborhoods. Additionally, many GNNs ignore the\ndirectional nature of real-world graphs, resulting in suboptimal performance on\ndirected graphs with asymmetric structures. In this work, we propose Directed\nHomophily-aware Graph Neural Network (DHGNN), a novel framework that addresses\nthese limitations by incorporating homophily-aware and direction-sensitive\ncomponents. DHGNN employs a resettable gating mechanism to adaptively modulate\nmessage contributions based on homophily levels and informativeness, and a\nstructure-aware noise-tolerant fusion module to effectively integrate node\nrepresentations from the original and reverse directions. Extensive experiments\non both homophilic and heterophilic directed graph datasets demonstrate that\nDHGNN outperforms state-of-the-art methods in node classification and link\nprediction. In particular, DHGNN improves over the best baseline by up to\n15.07% in link prediction. Our analysis further shows that the gating mechanism\ncaptures directional homophily gaps and fluctuating homophily across layers,\nproviding deeper insights into message-passing behavior on complex graph\nstructures."}
{"id": "2505.22370", "pdf": "https://arxiv.org/pdf/2505.22370", "abs": "https://arxiv.org/abs/2505.22370", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Weili Guan", "Min Zhang", "Liqiang Nie"], "title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 4 figures", "summary": "Continual Learning requires a model to learn multiple tasks in sequence while\nmaintaining both stability:preserving knowledge from previously learned tasks,\nand plasticity:effectively learning new tasks. Gradient projection has emerged\nas an effective and popular paradigm in CL, where it partitions the gradient\nspace of previously learned tasks into two orthogonal subspaces: a primary\nsubspace and a minor subspace. New tasks are learned effectively within the\nminor subspace, thereby reducing interference with previously acquired\nknowledge. However, existing Gradient Projection methods struggle to achieve an\noptimal balance between plasticity and stability, as it is hard to\nappropriately partition the gradient space. In this work, we consider a\ncontinual learning paradigm based on Low-Rank Adaptation, which has gained\nconsiderable attention due to its efficiency and wide applicability, and\npropose a novel approach for continual learning, called SplitLoRA. We first\nprovide a theoretical analysis of how subspace partitioning affects model\nstability and plasticity. Informed by this analysis, we then introduce an\neffective method that derives the optimal partition of the gradient space for\npreviously learned tasks. This approach effectively balances stability and\nplasticity in continual learning. Experimental results on multiple datasets\ndemonstrate that the proposed method achieves state-of-the-art performance."}
{"id": "2505.22381", "pdf": "https://arxiv.org/pdf/2505.22381", "abs": "https://arxiv.org/abs/2505.22381", "authors": ["Lukas Kirchdorfer", "Konrad Özdemir", "Stjepan Kusenic", "Han van der Aa", "Heiner Stuckenschmidt"], "title": "A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation", "categories": ["cs.LG"], "comment": null, "summary": "Business Process Simulation (BPS) is a critical tool for analyzing and\nimproving organizational processes by estimating the impact of process changes.\nA key component of BPS is the case-arrival model, which determines the pattern\nof new case entries into a process. Although accurate case-arrival modeling is\nessential for reliable simulations, as it influences waiting and overall cycle\ntimes, existing approaches often rely on oversimplified static distributions of\ninter-arrival times. These approaches fail to capture the dynamic and temporal\ncomplexities inherent in organizational environments, leading to less accurate\nand reliable outcomes. To address this limitation, we propose Auto Time Kernel\nDensity Estimation (AT-KDE), a divide-and-conquer approach that models arrival\ntimes of processes by incorporating global dynamics, day-of-week variations,\nand intraday distributional changes, ensuring both precision and scalability.\nExperiments conducted across 20 diverse processes demonstrate that AT-KDE is\nfar more accurate and robust than existing approaches while maintaining\nsensible execution time efficiency."}
{"id": "2505.22389", "pdf": "https://arxiv.org/pdf/2505.22389", "abs": "https://arxiv.org/abs/2505.22389", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Liqiang Nie"], "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 3 figures", "summary": "Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets."}
{"id": "2505.22391", "pdf": "https://arxiv.org/pdf/2505.22391", "abs": "https://arxiv.org/abs/2505.22391", "authors": ["Yi Zhang", "Difan Zou"], "title": "Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "23 pages, 5 figures, 4 tables", "summary": "Modeling physical systems in a generative manner offers several advantages,\nincluding the ability to handle partial observations, generate diverse\nsolutions, and address both forward and inverse problems. Recently, diffusion\nmodels have gained increasing attention in the modeling of physical systems,\nparticularly those governed by partial differential equations (PDEs). However,\ndiffusion models only access noisy data $\\boldsymbol{x}_t$ at intermediate\nsteps, making it infeasible to directly enforce constraints on the clean sample\n$\\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are\ntypically applied to the expectation of clean samples\n$\\mathbb{E}[\\boldsymbol{x}_0|\\boldsymbol{x}_t]$, which is estimated using the\nlearned score network. However, imposing PDE constraints on the expectation\ndoes not strictly represent the one on the true clean data, known as Jensen's\nGap. This gap creates a trade-off: enforcing PDE constraints may come at the\ncost of reduced accuracy in generative modeling. To address this, we propose a\nsimple yet effective post-hoc distillation approach, where PDE constraints are\nnot injected directly into the diffusion process, but instead enforced during a\npost-hoc distillation stage. We term our method as Physics-Informed\nDistillation of Diffusion Models (PIDDM). This distillation not only\nfacilitates single-step generation with improved PDE satisfaction, but also\nsupport both forward and inverse problem solving and reconstruction from\nrandomly partial observation. Extensive experiments across various PDE\nbenchmarks demonstrate that PIDDM significantly improves PDE satisfaction over\nseveral recent and competitive baselines, such as PIDM, DiffusionPDE, and\nECI-sampling, with less computation overhead. Our approach can shed light on\nmore efficient and effective strategies for incorporating physical constraints\ninto diffusion models."}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."}
{"id": "2505.22422", "pdf": "https://arxiv.org/pdf/2505.22422", "abs": "https://arxiv.org/abs/2505.22422", "authors": ["Václav Voráček", "Francesco Orabona"], "title": "STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals", "categories": ["cs.LG"], "comment": "comments are welcome", "summary": "The construction of confidence intervals for the mean of a bounded random\nvariable is a classical problem in statistics with numerous applications in\nmachine learning and virtually all scientific fields. In particular, obtaining\nthe tightest possible confidence intervals is vital every time the sampling of\nthe random variables is expensive. The current state-of-the-art method to\nconstruct confidence intervals is by using betting algorithms. This is a very\nsuccessful approach for deriving optimal confidence sequences, even matching\nthe rate of law of iterated logarithms. However, in the fixed horizon setting,\nthese approaches are either sub-optimal or based on heuristic solutions with\nstrong empirical performance but without a finite-time guarantee. Hence, no\nbetting-based algorithm guaranteeing the optimal\n$\\mathcal{O}(\\sqrt{\\frac{\\sigma^2\\log\\frac1\\delta}{n}})$ width of the\nconfidence intervals are known. This work bridges this gap. We propose a\nbetting-based algorithm to compute confidence intervals that empirically\noutperforms the competitors. Our betting strategy uses the optimal strategy in\nevery step (in a certain sense), whereas the standard betting methods choose a\nconstant strategy in advance. Leveraging this fact results in strict\nimprovements even for classical concentration inequalities, such as the ones of\nHoeffding or Bernstein. Moreover, we also prove that the width of our\nconfidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.\nThe code is available\non~https://github.com/vvoracek/STaR-bets-confidence-interval."}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."}
{"id": "2505.22440", "pdf": "https://arxiv.org/pdf/2505.22440", "abs": "https://arxiv.org/abs/2505.22440", "authors": ["Khan Masood Parvez", "Sk Md Abidar Rahaman", "Ali Shiri Sichani"], "title": "Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models", "categories": ["cs.LG"], "comment": null, "summary": "The rapid evolution of wireless technologies necessitates automated design\nframeworks to address antenna miniaturization and performance optimization\nwithin constrained development cycles. This study demonstrates a machine\nlearning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm\nOptimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.\nThe QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,\nachieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared\nto conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,\nXGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds\nusing 936 simulation datasets, with stacked models showing superior training\naccuracy (R2=0.9825) and SVM demonstrating optimal validation performance\n(R2=0.7197). The complete design cycle, encompassing optimization, prediction,\nand ANSYS validation, required 12.42 minutes on standard desktop hardware\n(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of\nPSADEA-based approaches. This 240 times of acceleration eliminates traditional\ntrial-and-error methods that often extend beyond seven expert-led days. The\nsystem enables precise specifications of performance targets with automated\ngeneration of fabrication-ready parameters, particularly benefiting compact\nconsumer devices requiring rapid frequency tuning. By bridging AI-driven\noptimization with CAD validation, this framework reduces engineering workloads\nwhile ensuring production-ready designs, establishing a scalable paradigm for\nnext-generation RF systems in 6G and IoT applications."}
{"id": "2505.22442", "pdf": "https://arxiv.org/pdf/2505.22442", "abs": "https://arxiv.org/abs/2505.22442", "authors": ["Mattie Fellows", "Clarisse Wibault", "Uljad Berdica", "Johannes Forkel", "Jakob N. Foerster", "Michael A. Osborne"], "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel."}
{"id": "2505.22450", "pdf": "https://arxiv.org/pdf/2505.22450", "abs": "https://arxiv.org/abs/2505.22450", "authors": ["Ossi Räisä", "Boris van Breugel", "Mihaela van der Schaar"], "title": "Position: All Current Generative Fidelity and Diversity Metrics are Flawed", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Any method's development and practical application is limited by our ability\nto measure its reliability. The popularity of generative modeling emphasizes\nthe importance of good synthetic data metrics. Unfortunately, previous works\nhave found many failure cases in current metrics, for example lack of outlier\nrobustness and unclear lower and upper bounds. We propose a list of desiderata\nfor synthetic data metrics, and a suite of sanity checks: carefully chosen\nsimple experiments that aim to detect specific and known generative modeling\nfailure modes. Based on these desiderata and the results of our checks, we\narrive at our position: all current generative fidelity and diversity metrics\nare flawed. This significantly hinders practical use of synthetic data. Our aim\nis to convince the research community to spend more effort in developing\nmetrics, instead of models. Additionally, through analyzing how current metrics\nfail, we provide practitioners with guidelines on how these metrics should\n(not) be used."}
{"id": "2505.22473", "pdf": "https://arxiv.org/pdf/2505.22473", "abs": "https://arxiv.org/abs/2505.22473", "authors": ["Riccardo Poiani", "Martino Bernasconi", "Andrea Celli"], "title": "Pure Exploration with Infinite Answers", "categories": ["cs.LG"], "comment": null, "summary": "We study pure exploration problems where the set of correct answers is\npossibly infinite, e.g., the regression of any continuous function of the means\nof the bandit. We derive an instance-dependent lower bound for these problems.\nBy analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)\nfor finite answer problems fail at being asymptotically optimal in this more\ngeneral setting. Finally, we present a framework, Sticky-Sequence\nTrack-and-Stop, which generalizes both Track-and-Stop and Sticky\nTrack-and-Stop, and that enjoys asymptotic optimality. Due to its generality,\nour analysis also highlights special cases where existing methods enjoy\noptimality."}
{"id": "2505.22474", "pdf": "https://arxiv.org/pdf/2505.22474", "abs": "https://arxiv.org/abs/2505.22474", "authors": ["Amirhossein Sohrabbeig", "Omid Ardakanian", "Petr Musilek"], "title": "Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis", "categories": ["cs.LG"], "comment": null, "summary": "The forecasting of multivariate urban data presents a complex challenge due\nto the intricate dependencies between various urban metrics such as weather,\nair pollution, carbon intensity, and energy demand. This paper introduces a\nnovel multivariate time-series forecasting model that utilizes advanced Graph\nNeural Networks (GNNs) to capture spatial dependencies among different\ntime-series variables. The proposed model incorporates a decomposition-based\npreprocessing step, isolating trend, seasonal, and residual components to\nenhance the accuracy and interpretability of forecasts. By leveraging the\ndynamic capabilities of GNNs, the model effectively captures interdependencies\nand improves the forecasting performance. Extensive experiments on real-world\ndatasets, including electricity usage, weather metrics, carbon intensity, and\nair pollution data, demonstrate the effectiveness of the proposed approach\nacross various forecasting scenarios. The results highlight the potential of\nthe model to optimize smart infrastructure systems, contributing to\nenergy-efficient urban development and enhanced public well-being."}
{"id": "2505.22475", "pdf": "https://arxiv.org/pdf/2505.22475", "abs": "https://arxiv.org/abs/2505.22475", "authors": ["Riccardo Poiani", "Martino Bernasconi", "Andrea Celli"], "title": "Non-Asymptotic Analysis of (Sticky) Track-and-Stop", "categories": ["cs.LG"], "comment": null, "summary": "In pure exploration problems, a statistician sequentially collects\ninformation to answer a question about some stochastic and unknown environment.\nThe probability of returning a wrong answer should not exceed a maximum risk\nparameter $\\delta$ and good algorithms make as few queries to the environment\nas possible. The Track-and-Stop algorithm is a pioneering method to solve these\nproblems. Specifically, it is well-known that it enjoys asymptotic optimality\nsample complexity guarantees for $\\delta\\to 0$ whenever the map from the\nenvironment to its correct answers is single-valued (e.g., best-arm\nidentification with a unique optimal arm). The Sticky Track-and-Stop algorithm\nextends these results to settings where, for each environment, there might\nexist multiple correct answers (e.g., $\\epsilon$-optimal arm identification).\nAlthough both methods are optimal in the asymptotic regime, their\nnon-asymptotic guarantees remain unknown. In this work, we fill this gap and\nprovide non-asymptotic guarantees for both algorithms."}
{"id": "2505.22483", "pdf": "https://arxiv.org/pdf/2505.22483", "abs": "https://arxiv.org/abs/2505.22483", "authors": ["Abhra Chaudhuri", "Anjan Dutta", "Tu Bui", "Serban Georgescu"], "title": "A Closer Look at Multimodal Representation Collapse", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)", "summary": "We aim to develop a fundamental understanding of modality collapse, a\nrecently observed empirical phenomenon wherein models trained for multimodal\nfusion tend to rely only on a subset of the modalities, ignoring the rest. We\nshow that modality collapse happens when noisy features from one modality are\nentangled, via a shared set of neurons in the fusion head, with predictive\nfeatures from another, effectively masking out positive contributions from the\npredictive features of the former modality and leading to its collapse. We\nfurther prove that cross-modal knowledge distillation implicitly disentangles\nsuch representations by freeing up rank bottlenecks in the student encoder,\ndenoising the fusion-head outputs without negatively impacting the predictive\nfeatures from either modality. Based on the above findings, we propose an\nalgorithm that prevents modality collapse through explicit basis reallocation,\nwith applications in dealing with missing modalities. Extensive experiments on\nmultiple multimodal benchmarks validate our theoretical claims. Project page:\nhttps://abhrac.github.io/mmcollapse/."}
{"id": "2505.22486", "pdf": "https://arxiv.org/pdf/2505.22486", "abs": "https://arxiv.org/abs/2505.22486", "authors": ["Mujtaba Hussain Mirza", "Maria Rosaria Briglia", "Filippo Bartolucci", "Senad Beadini", "Giuseppe Lisanti", "Iacopo Masi"], "title": "Understanding Adversarial Training with Energy-based Models", "categories": ["cs.LG", "cs.CV"], "comment": "Under review for TPAMI", "summary": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models."}
{"id": "2505.22491", "pdf": "https://arxiv.org/pdf/2505.22491", "abs": "https://arxiv.org/abs/2505.22491", "authors": ["Moritz Haas", "Sebastian Bordt", "Ulrike von Luxburg", "Leena Chennuru Vankadara"], "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The dominant paradigm for training large-scale vision and language models is\nHe initialization and a single global learning rate (\\textit{standard\nparameterization}, SP). Despite its practical success, standard parametrization\nremains poorly understood from a theoretical perspective: Existing\ninfinite-width theory would predict instability under large learning rates and\nvanishing feature learning under stable learning rates. However, empirically\noptimal learning rates consistently decay much slower than theoretically\npredicted. By carefully studying neural network training dynamics, we\ndemonstrate that this discrepancy is not fully explained by finite-width\nphenomena such as catapult effects or a lack of alignment between weights and\nincoming activations. We instead show that the apparent contradiction can be\nfundamentally resolved by taking the loss function into account: In contrast to\nMean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an\nintermediate \\textit{controlled divergence} regime emerges, where logits\ndiverge but loss, gradients, and activations remain stable. Stable training\nunder large learning rates enables persistent feature evolution at scale in all\nhidden layers, which is crucial for the practical success of SP. In experiments\nacross optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities\n(vision, language), we validate that neural networks operate in this controlled\ndivergence regime under CE loss but not under MSE loss. Our empirical evidence\nsuggests that width-scaling considerations are surprisingly useful for\npredicting empirically optimal learning rate exponents. Finally, our analysis\nclarifies the effectiveness and limitations of recently proposed layerwise\nlearning rate scalings for standard initialization."}
{"id": "2505.22492", "pdf": "https://arxiv.org/pdf/2505.22492", "abs": "https://arxiv.org/abs/2505.22492", "authors": ["Hongyi Zhou", "Josiah P. Hanna", "Jin Zhu", "Ying Yang", "Chengchun Shi"], "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "This paper studies off-policy evaluation (OPE) in reinforcement learning with\na focus on behavior policy estimation for importance sampling. Prior work has\nshown empirically that estimating a history-dependent behavior policy can lead\nto lower mean squared error (MSE) even when the true behavior policy is\nMarkovian. However, the question of why the use of history should lower MSE\nremains open. In this paper, we theoretically demystify this paradox by\nderiving a bias-variance decomposition of the MSE of ordinary importance\nsampling (IS) estimators, demonstrating that history-dependent behavior policy\nestimation decreases their asymptotic variances while increasing their\nfinite-sample biases. Additionally, as the estimated behavior policy conditions\non a longer history, we show a consistent decrease in variance. We extend these\nfindings to a range of other OPE estimators, including the sequential IS\nestimator, the doubly robust estimator and the marginalized IS estimator, with\nthe behavior policy estimated either parametrically or non-parametrically."}
{"id": "2505.22494", "pdf": "https://arxiv.org/pdf/2505.22494", "abs": "https://arxiv.org/abs/2505.22494", "authors": ["Michal Kmicikiewicz", "Vincent Fortuin", "Ewa Szczurek"], "title": "ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods", "categories": ["cs.LG"], "comment": null, "summary": "Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty."}
{"id": "2505.22504", "pdf": "https://arxiv.org/pdf/2505.22504", "abs": "https://arxiv.org/abs/2505.22504", "authors": ["Ahmed Hossam Mohammed", "Kishansingh Rajput", "Simon Taylor", "Denis Furletov", "Sergey Furletov", "Malachi Schram"], "title": "Geometric GNNs for Charged Particle Tracking at GlueX", "categories": ["cs.LG"], "comment": null, "summary": "Nuclear physics experiments are aimed at uncovering the fundamental building\nblocks of matter. The experiments involve high-energy collisions that produce\ncomplex events with many particle trajectories. Tracking charged particles\nresulting from collisions in the presence of a strong magnetic field is\ncritical to enable the reconstruction of particle trajectories and precise\ndetermination of interactions. It is traditionally achieved through\ncombinatorial approaches that scale worse than linearly as the number of hits\ngrows. Since particle hit data naturally form a 3-dimensional point cloud and\ncan be structured as graphs, Graph Neural Networks (GNNs) emerge as an\nintuitive and effective choice for this task. In this study, we evaluate the\nGNN model for track finding on the data from the GlueX experiment at Jefferson\nLab. We use simulation data to train the model and test on both simulation and\nreal GlueX measurements. We demonstrate that GNN-based track finding\noutperforms the currently used traditional method at GlueX in terms of\nsegment-based efficiency at a fixed purity while providing faster inferences.\nWe show that the GNN model can achieve significant speedup by processing\nmultiple events in batches, which exploits the parallel computation capability\nof Graphical Processing Units (GPUs). Finally, we compare the GNN\nimplementation on GPU and FPGA and describe the trade-off."}
{"id": "2505.22506", "pdf": "https://arxiv.org/pdf/2505.22506", "abs": "https://arxiv.org/abs/2505.22506", "authors": ["Wenjie Sun", "Bingzhe Wu", "Zhile Yang", "Chengke Wu"], "title": "Sparsification and Reconstruction from the Perspective of Representation Geometry", "categories": ["cs.LG", "22-08", "I.2.4; I.2.7"], "comment": "24 pages, 5 figures", "summary": "Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic\ninterpretability, aiming to identify interpretable monosemantic features.\nHowever, how does sparse encoding organize the representations of activation\nvector from language models? What is the relationship between this\norganizational paradigm and feature disentanglement as well as reconstruction\nperformance? To address these questions, we propose the SAEMA, which validates\nthe stratified structure of the representation by observing the variability of\nthe rank of the symmetric semipositive definite (SSPD) matrix corresponding to\nthe modal tensor unfolded along the latent tensor with the level of noise added\nto the residual stream. To systematically investigate how sparse encoding\nalters representational structures, we define local and global representations,\ndemonstrating that they amplify inter-feature distinctions by merging similar\nsemantic features and introducing additional dimensionality. Furthermore, we\nintervene the global representation from an optimization perspective, proving a\nsignificant causal relationship between their separability and the\nreconstruction performance. This study explains the principles of sparsity from\nthe perspective of representational geometry and demonstrates the impact of\nchanges in representational structure on reconstruction performance.\nParticularly emphasizes the necessity of understanding representations and\nincorporating representational constraints, providing empirical references for\ndeveloping new interpretable tools and improving SAEs. The code is available at\n\\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}."}
{"id": "2505.22509", "pdf": "https://arxiv.org/pdf/2505.22509", "abs": "https://arxiv.org/abs/2505.22509", "authors": ["Zhonglin Xie", "Yiman Fong", "Haoran Yuan", "Zaiwen Wen"], "title": "Accelerating Optimization via Differentiable Stopping Time", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Optimization is an important module of modern machine learning applications.\nTremendous efforts have been made to accelerate optimization algorithms. A\ncommon formulation is achieving a lower loss at a given time. This enables a\ndifferentiable framework with respect to the algorithm hyperparameters. In\ncontrast, its dual, minimizing the time to reach a target loss, is believed to\nbe non-differentiable, as the time is not differentiable. As a result, it\nusually serves as a conceptual framework or is optimized using zeroth-order\nmethods. To address this limitation, we propose a differentiable stopping time\nand theoretically justify it based on differential equations. An efficient\nalgorithm is designed to backpropagate through it. As a result, the proposed\ndifferentiable stopping time enables a new differentiable formulation for\naccelerating algorithms. We further discuss its applications, such as online\nhyperparameter tuning and learning to optimize. Our proposed methods show\nsuperior performance in comprehensive experiments across various problems,\nwhich confirms their effectiveness."}
{"id": "2505.22521", "pdf": "https://arxiv.org/pdf/2505.22521", "abs": "https://arxiv.org/abs/2505.22521", "authors": ["Chao Wang", "Chuanhao Nie", "Yunbo Liu"], "title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally\n  to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu).\n  Submitted to the 3rd International Conference on Management Innovation and\n  Economy Development (MIED 2025), Chongqing, China", "summary": "Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems."}
{"id": "2505.22524", "pdf": "https://arxiv.org/pdf/2505.22524", "abs": "https://arxiv.org/abs/2505.22524", "authors": ["Chinmay Pani", "Zijing Ou", "Yingzhen Li"], "title": "Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo", "categories": ["cs.LG"], "comment": null, "summary": "Discrete diffusion models have become highly effective across various\ndomains. However, real-world applications often require the generative process\nto adhere to certain constraints but without task-specific fine-tuning. To this\nend, we propose a training-free method based on Sequential Monte Carlo (SMC) to\nsample from the reward-aligned target distribution at the test time. Our\napproach leverages twisted SMC with an approximate locally optimal proposal,\nobtained via a first-order Taylor expansion of the reward function. To address\nthe challenge of ill-defined gradients in discrete spaces, we incorporate a\nGumbel-Softmax relaxation, enabling efficient gradient-based approximation\nwithin the discrete generative framework. Empirical results on both synthetic\ndatasets and image modelling validate the effectiveness of our approach."}
{"id": "2505.22531", "pdf": "https://arxiv.org/pdf/2505.22531", "abs": "https://arxiv.org/abs/2505.22531", "authors": ["Andres Molina-Markham", "Luis Robaina", "Sean Steinle", "Akash Trivedi", "Derek Tsui", "Nicholas Potteiger", "Lauren Brandt", "Ransom Winder", "Ahmed Ridley"], "title": "Training RL Agents for Multi-Objective Network Defense Tasks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work."}
{"id": "2505.22533", "pdf": "https://arxiv.org/pdf/2505.22533", "abs": "https://arxiv.org/abs/2505.22533", "authors": ["Pallavi Bhardwaj", "Caitlin Jones", "Lasse Dierich", "Aleksandar Vučković"], "title": "TabularQGAN: A Quantum Generative Model for Tabular Data", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "18 pages,8 figures and 4 tables", "summary": "In this paper, we introduce a novel quantum generative model for synthesizing\ntabular data. Synthetic data is valuable in scenarios where real-world data is\nscarce or private, it can be used to augment or replace existing datasets.\nReal-world enterprise data is predominantly tabular and heterogeneous, often\ncomprising a mixture of categorical and numerical features, making it highly\nrelevant across various industries such as healthcare, finance, and software.\nWe propose a quantum generative adversarial network architecture with flexible\ndata encoding and a novel quantum circuit ansatz to effectively model tabular\ndata. The proposed approach is tested on the MIMIC III healthcare and Adult\nCensus datasets, with extensive benchmarking against leading classical models,\nCTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model\noutperforms classical models by an average of 8.5% with respect to an overall\nsimilarity score from SDMetrics, while using only 0.072% of the parameters of\nthe classical models. Additionally, we evaluate the generalization capabilities\nof the models using two custom-designed metrics that demonstrate the ability of\nthe proposed quantum model to generate useful and novel samples. To our\nknowledge, this is one of the first demonstrations of a successful quantum\ngenerative model for handling tabular data, indicating that this task could be\nwell-suited to quantum computers."}
{"id": "2505.22538", "pdf": "https://arxiv.org/pdf/2505.22538", "abs": "https://arxiv.org/abs/2505.22538", "authors": ["Paul Hofman", "Yusuf Sale", "Eyke Hüllermeier"], "title": "Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We address the problem of uncertainty quantification and propose measures of\ntotal, aleatoric, and epistemic uncertainty based on a known decomposition of\n(strictly) proper scoring rules, a specific type of loss function, into a\ndivergence and an entropy component. This leads to a flexible framework for\nuncertainty quantification that can be instantiated with different losses\n(scoring rules), which makes it possible to tailor uncertainty quantification\nto the use case at hand. We show that this flexibility is indeed advantageous.\nIn particular, we analyze the task of selective prediction and show that the\nscoring rule should ideally match the task loss. In addition, we perform\nexperiments on two other common tasks. For out-of-distribution detection, our\nresults confirm that a widely used measure of epistemic uncertainty, mutual\ninformation, performs best. Moreover, in the setting of active learning, our\nmeasure of epistemic uncertainty based on the zero-one-loss consistently\noutperforms other uncertainty measures."}
{"id": "2505.22541", "pdf": "https://arxiv.org/pdf/2505.22541", "abs": "https://arxiv.org/abs/2505.22541", "authors": ["Vinitra Swamy"], "title": "A Human-Centric Approach to Explainable AI for Personalized Education", "categories": ["cs.LG", "cs.CY"], "comment": "PhD Thesis, EPFL (Computer Science)", "summary": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust."}
{"id": "2505.22549", "pdf": "https://arxiv.org/pdf/2505.22549", "abs": "https://arxiv.org/abs/2505.22549", "authors": ["Alex Iacob", "Lorenzo Sani", "Mher Safaryan", "Paris Giampouras", "Samuel Horváth", "Andrej Jovanovic", "Meghdad Kurmanji", "Preslav Aleksandrov", "William F. Shen", "Xinchi Qiu", "Nicholas D. Lane"], "title": "DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models", "categories": ["cs.LG"], "comment": "Keywords: Distributed Training, Foundation Models, Large Language\n  Models, Optimizers, Communication Efficiency, Federated Learning, Distributed\n  Systems, Optimization Theory, Scaling, Robustness. Preprint, under review at\n  NeurIPS", "summary": "Scaling foundation model training with Distributed Data Parallel (DDP)\nmethods is bandwidth-limited. Existing infrequent communication methods like\nLocal SGD were designed to synchronize only model parameters and cannot be\ntrivially applied to adaptive optimizers due to additional optimizer states.\nCurrent approaches extending Local SGD either lack convergence guarantees or\nrequire synchronizing all optimizer states, tripling communication costs. We\npropose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of\noptimizers assigning independent synchronization periods to parameters and\nmomenta, enabling lower communication costs while preserving convergence.\nThrough extensive experiments on language models of up to 1.7B, we show that\nDES-LOC can communicate 170x less than DDP and 2x less than the previous\nstate-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,\nDES-LOC is suited for practical training scenarios prone to system failures.\nDES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for\nfoundation model training."}
{"id": "2505.22560", "pdf": "https://arxiv.org/pdf/2505.22560", "abs": "https://arxiv.org/abs/2505.22560", "authors": ["Artem Moskalev", "Mangal Prakash", "Junjie Xu", "Tianyu Cui", "Rui Liao", "Tommaso Mansi"], "title": "Geometric Hyena Networks for Large-scale Equivariant Learning", "categories": ["cs.LG"], "comment": null, "summary": "Processing global geometric context while preserving equivariance is crucial\nwhen modeling biological, chemical, and physical systems. Yet, this is\nchallenging due to the computational demands of equivariance and global context\nat scale. Standard methods such as equivariant self-attention suffer from\nquadratic complexity, while local methods such as distance-based message\npassing sacrifice global information. Inspired by the recent success of\nstate-space and long-convolutional models, we introduce Geometric Hyena, the\nfirst equivariant long-convolutional model for geometric systems. Geometric\nHyena captures global geometric context at sub-quadratic complexity while\nmaintaining equivariance to rotations and translations. Evaluated on all-atom\nproperty prediction of large RNA molecules and full protein molecular dynamics,\nGeometric Hyena outperforms existing equivariant models while requiring\nsignificantly less memory and compute that equivariant self-attention. Notably,\nour model processes the geometric context of 30k tokens 20x faster than the\nequivariant transformer and allows 72x longer context within the same budget."}
{"id": "2505.22573", "pdf": "https://arxiv.org/pdf/2505.22573", "abs": "https://arxiv.org/abs/2505.22573", "authors": ["Guy Moss", "Leah Sophie Muhle", "Reinhard Drews", "Jakob H. Macke", "Cornelius Schröder"], "title": "FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators", "categories": ["cs.LG"], "comment": null, "summary": "Simulation-based inference (SBI) is an established approach for performing\nBayesian inference on scientific simulators. SBI so far works best on\nlow-dimensional parametric models. However, it is difficult to infer\nfunction-valued parameters, which frequently occur in disciplines that model\nspatiotemporal processes such as the climate and earth sciences. Here, we\nintroduce an approach for efficient posterior estimation, using a Fourier\nNeural Operator (FNO) architecture with a flow matching objective. We show that\nour approach, FNOPE, can perform inference of function-valued parameters at a\nfraction of the simulation budget of state of the art methods. In addition,\nFNOPE supports posterior evaluation at arbitrary discretizations of the domain,\nas well as simultaneous estimation of vector-valued parameters. We demonstrate\nthe effectiveness of our approach on several benchmark tasks and a challenging\nspatial inference task from glaciology. FNOPE extends the applicability of SBI\nmethods to new scientific domains by enabling the inference of function-valued\nparameters."}
{"id": "2505.22578", "pdf": "https://arxiv.org/pdf/2505.22578", "abs": "https://arxiv.org/abs/2505.22578", "authors": ["Etienne Boursier", "Matthew Bowditch", "Matthias Englert", "Ranko Lazic"], "title": "Benignity of loss landscape with weight decay requires both large overparametrization and initialization", "categories": ["cs.LG"], "comment": null, "summary": "The optimization of neural networks under weight decay remains poorly\nunderstood from a theoretical standpoint. While weight decay is standard\npractice in modern training procedures, most theoretical analyses focus on\nunregularized settings. In this work, we investigate the loss landscape of the\n$\\ell_2$-regularized training loss for two-layer ReLU networks. We show that\nthe landscape becomes benign -- i.e., free of spurious local minima -- under\nlarge overparametrization, specifically when the network width $m$ satisfies $m\n\\gtrsim \\min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the\ninput dimension. More precisely in this regime, almost all constant activation\nregions contain a global minimum and no spurious local minima. We further show\nthat this level of overparametrization is not only sufficient but also\nnecessary via the example of orthogonal data. Finally, we demonstrate that such\nloss landscape results primarily hold relevance in the large initialization\nregime. In contrast, for small initializations -- corresponding to the feature\nlearning regime -- optimization can still converge to spurious local minima,\ndespite the global benignity of the landscape."}
{"id": "2505.22601", "pdf": "https://arxiv.org/pdf/2505.22601", "abs": "https://arxiv.org/abs/2505.22601", "authors": ["Jacob L. Block", "Aryan Mokhtari", "Sanjay Shakkottai"], "title": "Machine Unlearning under Overparameterization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine unlearning algorithms aim to remove the influence of specific\ntraining samples, ideally recovering the model that would have resulted from\ntraining on the remaining data alone. We study unlearning in the\noverparameterized setting, where many models interpolate the data, and defining\nthe unlearning solution as any loss minimizer over the retained\nset$\\unicode{x2013}$as in prior work in the underparameterized\nsetting$\\unicode{x2013}$is inadequate, since the original model may already\ninterpolate the retained data and satisfy this condition. In this regime, loss\ngradients vanish, rendering prior methods based on gradient perturbations\nineffective, motivating both new unlearning definitions and algorithms. For\nthis setting, we define the unlearning solution as the minimum-complexity\ninterpolator over the retained data and propose a new algorithmic framework\nthat only requires access to model gradients on the retained set at the\noriginal solution. We minimize a regularized objective over perturbations\nconstrained to be orthogonal to these model gradients, a first-order relaxation\nof the interpolation condition. For different model classes, we provide exact\nand approximate unlearning guarantees, and we demonstrate that an\nimplementation of our framework outperforms existing baselines across various\nunlearning experiments."}
{"id": "2505.22602", "pdf": "https://arxiv.org/pdf/2505.22602", "abs": "https://arxiv.org/abs/2505.22602", "authors": ["Mahtab Alizadeh Vandchali", "Fangshuo", "Liao", "Anastasios Kyrillidis"], "title": "One Rank at a Time: Cascading Error Dynamics in Sequential Learning", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "36 pages", "summary": "Sequential learning -- where complex tasks are broken down into simpler,\nhierarchical components -- has emerged as a paradigm in AI. This paper views\nsequential learning through the lens of low-rank linear regression, focusing\nspecifically on how errors propagate when learning rank-1 subspaces\nsequentially. We present an analysis framework that decomposes the learning\nprocess into a series of rank-1 estimation problems, where each subsequent\nestimation depends on the accuracy of previous steps. Our contribution is a\ncharacterization of the error propagation in this sequential process,\nestablishing bounds on how errors -- e.g., due to limited computational budgets\nand finite precision -- affect the overall model accuracy. We prove that these\nerrors compound in predictable ways, with implications for both algorithmic\ndesign and stability guarantees."}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."}
{"id": "2505.22637", "pdf": "https://arxiv.org/pdf/2505.22637", "abs": "https://arxiv.org/abs/2505.22637", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "categories": ["cs.LG"], "comment": "17 pages, 10 figures. Presented at the ICLR 2025 Workshop on\n  Foundation Models in the Wild", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction."}
{"id": "2505.22641", "pdf": "https://arxiv.org/pdf/2505.22641", "abs": "https://arxiv.org/abs/2505.22641", "authors": ["Chengzhi Shi", "Stratis Ioannidis"], "title": "Spectral Survival Analysis", "categories": ["cs.LG"], "comment": "Extended version of conference paper appearing in KDD 2025", "summary": "Survival analysis is widely deployed in a diverse set of fields, including\nhealthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model\nis a semi-parametric model often encountered in the literature. Despite its\npopularity, wide deployment, and numerous variants, scaling CoxPH to large\ndatasets and deep architectures poses a challenge, especially in the\nhigh-dimensional regime. We identify a fundamental connection between rank\nregression and the CoxPH model: this allows us to adapt and extend the\nso-called spectral method for rank regression to survival analysis. Our\napproach is versatile, naturally generalizing to several CoxPH variants,\nincluding deep models. We empirically verify our method's scalability on\nmultiple real-world high-dimensional datasets; our method outperforms legacy\nmethods w.r.t. predictive performance and efficiency."}
{"id": "2505.22650", "pdf": "https://arxiv.org/pdf/2505.22650", "abs": "https://arxiv.org/abs/2505.22650", "authors": ["Maria-Florina Balcan", "Avrim Blum", "Zhiyuan Li", "Dravyansh Sharma"], "title": "On Learning Verifiers for Chain-of-Thought Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Chain-of-Thought reasoning has emerged as a powerful approach for solving\ncomplex mathematical and logical problems. However, it can often veer off track\nthrough incorrect or unsubstantiated inferences. Formal mathematical reasoning,\nwhich can be checked with a formal verifier, is one approach to addressing this\nissue. However, currently LLMs are simply not good enough to solve complex\nproblems in a formal way, and even just formalizing an informal problem\nstatement can be challenging. Motivated by this fact, in this work we consider\nthe problem of learning reliable verifiers for natural language\nChain-of-Thought reasoning. That is, given a problem statement and step-by-step\nsolution in natural language, the aim of the verifier is to output [Yes] if the\nreasoning steps in the solution are all valid, and [No] otherwise. In this work\nwe give a formal PAC-learning framework for studying this problem. We propose\nand analyze several natural verification goals, at different levels of\nstrength, in this framework. We provide sample complexity upper-bounds for\nlearning verifiers satisfying these goals, as well as lower-bound and\nimpossibility results for learning other natural verification objectives\nwithout additional assumptions."}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."}
{"id": "2505.22660", "pdf": "https://arxiv.org/pdf/2505.22660", "abs": "https://arxiv.org/abs/2505.22660", "authors": ["Mihir Prabhudesai", "Lili Chen", "Alex Ippoliti", "Katerina Fragkiadaki", "Hao Liu", "Deepak Pathak"], "title": "Maximizing Confidence Alone Improves Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve\nsignificant advances in many fields. Most recently, RL has empowered frontier\nlanguage models to solve challenging math, science, and coding problems.\nHowever, central to any RL algorithm is the reward function, and reward\nengineering is a notoriously difficult problem in any domain. In this paper, we\npropose RENT: Reinforcement Learning via Entropy Minimization -- a fully\nunsupervised RL method that requires no external reward or ground-truth\nanswers, and instead uses the model's entropy of its underlying distribution as\nan intrinsic reward. We find that by reinforcing the chains of thought that\nyield high model confidence on its generated answers, the model improves its\nreasoning ability. In our experiments, we showcase these improvements on an\nextensive suite of commonly-used reasoning benchmarks, including GSM8K,\nMATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and\nMistral families. The generality of our unsupervised learning method lends\nitself to applicability in a wide range of domains where external supervision\nis limited or unavailable."}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162", "abs": "https://arxiv.org/abs/2505.20162", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers."}
{"id": "2505.21552", "pdf": "https://arxiv.org/pdf/2505.21552", "abs": "https://arxiv.org/abs/2505.21552", "authors": ["Diogo Cruz"], "title": "Understanding the learned look-ahead behavior of chess neural networks", "categories": ["cs.AI", "cs.LG"], "comment": "40 pages, 47 figures", "summary": "We investigate the look-ahead capabilities of chess-playing neural networks,\nspecifically focusing on the Leela Chess Zero policy network. We build on the\nwork of Jenner et al. (2024) by analyzing the model's ability to consider\nfuture moves and alternative sequences beyond the immediate next move. Our\nfindings reveal that the network's look-ahead behavior is highly\ncontext-dependent, varying significantly based on the specific chess position.\nWe demonstrate that the model can process information about board states up to\nseven moves ahead, utilizing similar internal mechanisms across different\nfuture time steps. Additionally, we provide evidence that the network considers\nmultiple possible move sequences rather than focusing on a single line of play.\nThese results offer new insights into the emergence of sophisticated look-ahead\ncapabilities in neural networks trained on strategic tasks, contributing to our\nunderstanding of AI reasoning in complex domains. Our work also showcases the\neffectiveness of interpretability techniques in uncovering cognitive-like\nprocesses in artificial intelligence systems."}
{"id": "2505.21580", "pdf": "https://arxiv.org/pdf/2505.21580", "abs": "https://arxiv.org/abs/2505.21580", "authors": ["Anum Fatima", "Gesine Reinert"], "title": "A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "43 pages, 24 figures", "summary": "Complex data are often represented as a graph, which in turn can often be\nviewed as a realisation of a random graph, such as of an inhomogeneous random\ngraph model (IRG). For general fast goodness-of-fit tests in high dimensions,\nkernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,\ntest, and analyse a KSD-type goodness-of-fit test for IRG models that can be\ncarried out with a single observation of the network. The test is applicable to\na network of any size and does not depend on the asymptotic distribution of the\ntest statistic. We also provide theoretical guarantees."}
{"id": "2505.21658", "pdf": "https://arxiv.org/pdf/2505.21658", "abs": "https://arxiv.org/abs/2505.21658", "authors": ["Brandon R. Feng", "David Keetae Park", "Xihaier Luo", "Arantxa Urdangarin", "Shinjae Yoo", "Brian J. Reich"], "title": "STACI: Spatio-Temporal Aleatoric Conformal Inference", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty\nquantification for estimation of spatio-temporal fields. Spatio-temporal deep\nlearning models, while scalable, typically assume a simplistic independent\ncovariance matrix for the response, failing to capture the underlying\ncorrelation structure. However, spatio-temporal GPs suffer from issues of\nscalability and various forms of approximation bias resulting from restrictive\nassumptions of the covariance kernel function. We propose STACI, a novel\nframework consisting of a variational Bayesian neural network approximation of\nnon-stationary spatio-temporal GP along with a novel spatio-temporal conformal\ninference algorithm. STACI is highly scalable, taking advantage of GPU training\ncapabilities for neural network models, and provides statistically valid\nprediction intervals for uncertainty quantification. STACI outperforms\ncompeting GPs and deep methods in accurately approximating spatio-temporal\nprocesses and we show it easily scales to datasets with millions of\nobservations."}
{"id": "2505.21671", "pdf": "https://arxiv.org/pdf/2505.21671", "abs": "https://arxiv.org/abs/2505.21671", "authors": ["Davin Choo", "Yuqi Pan", "Tonghan Wang", "Milind Tambe", "Alastair van Heerden", "Cheryl Johnson"], "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing", "categories": ["cs.AI", "cs.DS", "cs.LG", "math.OC"], "comment": null, "summary": "We study a sequential decision-making problem on a $n$-node graph $G$ where\neach node has an unknown label from a finite set $\\mathbf{\\Sigma}$, drawn from\na joint distribution $P$ that is Markov with respect to $G$. At each step,\nselecting a node reveals its label and yields a label-dependent reward. The\ngoal is to adaptively choose nodes to maximize expected accumulated discounted\nrewards. We impose a frontier exploration constraint, where actions are limited\nto neighbors of previously selected nodes, reflecting practical constraints in\nsettings such as contact tracing and robotic exploration. We design a Gittins\nindex-based policy that applies to general graphs and is provably optimal when\n$G$ is a forest. Our implementation runs in $O(n^2 \\cdot |\\mathbf{\\Sigma}|^2)$\ntime while using $O(n \\cdot |\\mathbf{\\Sigma}|^2)$ oracle calls to $P$ and\n$O(n^2 \\cdot |\\mathbf{\\Sigma}|)$ space. Experiments on synthetic and real-world\ngraphs show that our method consistently outperforms natural baselines,\nincluding in non-tree, budget-limited, and undiscounted settings. For example,\nin HIV testing simulations on real-world sexual interaction networks, our\npolicy detects nearly all positive cases with only half the population tested,\nsubstantially outperforming other baselines."}
{"id": "2505.21721", "pdf": "https://arxiv.org/pdf/2505.21721", "abs": "https://arxiv.org/abs/2505.21721", "authors": ["Kyurae Kim", "Yi-An Ma", "Trevor Campbell", "Jacob R. Gardner"], "title": "Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.CO"], "comment": null, "summary": "We prove that, given a mean-field location-scale variational family,\nblack-box variational inference (BBVI) with the reparametrization gradient\nconverges at an almost dimension-independent rate. Specifically, for strongly\nlog-concave and log-smooth targets, the number of iterations for BBVI with a\nsub-Gaussian family to achieve an objective $\\epsilon$-close to the global\noptimum is $\\mathrm{O}(\\log d)$, which improves over the $\\mathrm{O}(d)$\ndependence of full-rank location-scale families. For heavy-tailed families, we\nprovide a weaker $\\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the\nnumber of finite moments. Additionally, if the Hessian of the target\nlog-density is constant, the complexity is free of any explicit dimension\ndependence. We also prove that our bound on the gradient variance, which is key\nto our result, cannot be improved using only spectral bounds on the Hessian of\nthe target log-density."}
{"id": "2505.21791", "pdf": "https://arxiv.org/pdf/2505.21791", "abs": "https://arxiv.org/abs/2505.21791", "authors": ["Julia Nakhleh", "Robert D. Nowak"], "title": "Global Minimizers of $\\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Overparameterized neural networks can interpolate a given dataset in many\ndifferent ways, prompting the fundamental question: which among these solutions\nshould we prefer, and what explicit regularization strategies will provably\nyield these solutions? This paper addresses the challenge of finding the\nsparsest interpolating ReLU network -- i.e., the network with the fewest\nnonzero parameters or neurons -- a goal with wide-ranging implications for\nefficiency, generalization, interpretability, theory, and model compression.\nUnlike post hoc pruning approaches, we propose a continuous, almost-everywhere\ndifferentiable training objective whose global minima are guaranteed to\ncorrespond to the sparsest single-hidden-layer ReLU networks that fit the data.\nThis result marks a conceptual advance: it recasts the combinatorial problem of\nsparse interpolation as a smooth optimization task, potentially enabling the\nuse of gradient-based training methods. Our objective is based on minimizing\n$\\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical\nsparsity-promoting strategy in finite-dimensional settings. However, applying\nthese ideas to neural networks presents new challenges: the function class is\ninfinite-dimensional, and the weights are learned using a highly nonconvex\nobjective. We prove that, under our formulation, global minimizers correspond\nexactly to sparsest solutions. Our work lays a foundation for understanding\nwhen and how continuous sparsity-inducing objectives can be leveraged to\nrecover sparse networks through training."}
{"id": "2505.21796", "pdf": "https://arxiv.org/pdf/2505.21796", "abs": "https://arxiv.org/abs/2505.21796", "authors": ["Sajad Khodadadian", "Martin Zubeldia"], "title": "A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "37 pages", "summary": "Polyak-Ruppert averaging is a widely used technique to achieve the optimal\nasymptotic variance of stochastic approximation (SA) algorithms, yet its\nhigh-probability performance guarantees remain underexplored in general\nsettings. In this paper, we present a general framework for establishing\nnon-asymptotic concentration bounds for the error of averaged SA iterates. Our\napproach assumes access to individual concentration bounds for the unaveraged\niterates and yields a sharp bound on the averaged iterates. We also construct\nan example, showing the tightness of our result up to constant multiplicative\nfactors. As direct applications, we derive tight concentration bounds for\ncontractive SA algorithms and for algorithms such as temporal difference\nlearning and Q-learning with averaging, obtaining new bounds in settings where\ntraditional analysis is challenging."}
{"id": "2505.21845", "pdf": "https://arxiv.org/pdf/2505.21845", "abs": "https://arxiv.org/abs/2505.21845", "authors": ["Lingfei Zhao", "Hadeel Soliman", "Kevin S. Xu", "Subhadeep Paul"], "title": "Spectral clustering for dependent community Hawkes process models of temporal networks", "categories": ["stat.ML", "cs.LG", "cs.SI", "stat.ME"], "comment": null, "summary": "Temporal networks observed continuously over time through timestamped\nrelational events data are commonly encountered in application settings\nincluding online social media communications, financial transactions, and\ninternational relations. Temporal networks often exhibit community structure\nand strong dependence patterns among node pairs. This dependence can be modeled\nthrough mutual excitations, where an interaction event from a sender to a\nreceiver node increases the possibility of future events among other node\npairs.\n  We provide statistical results for a class of models that we call dependent\ncommunity Hawkes (DCH) models, which combine the stochastic block model with\nmutually exciting Hawkes processes for modeling both community structure and\ndependence among node pairs, respectively. We derive a non-asymptotic upper\nbound on the misclustering error of spectral clustering on the event count\nmatrix as a function of the number of nodes and communities, time duration, and\nthe amount of dependence in the model. Our result leverages recent results on\nbounding an appropriate distance between a multivariate Hawkes process count\nvector and a Gaussian vector, along with results from random matrix theory. We\nalso propose a DCH model that incorporates only self and reciprocal excitation\nalong with highly scalable parameter estimation using a Generalized Method of\nMoments (GMM) estimator that we demonstrate to be consistent for growing\nnetwork size and time duration."}
{"id": "2505.21887", "pdf": "https://arxiv.org/pdf/2505.21887", "abs": "https://arxiv.org/abs/2505.21887", "authors": ["Ahmed Heakl", "Yahia Salaheldin Shaaban", "Martin Takac", "Salem Lahlou", "Zangir Iklassov"], "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": "18 pages, 14 figures, 11 tables", "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty."}
{"id": "2505.21892", "pdf": "https://arxiv.org/pdf/2505.21892", "abs": "https://arxiv.org/abs/2505.21892", "authors": ["Xunpeng Huang", "Yingyu Lin", "Nikki Lijing Kuang", "Hanze Dong", "Difan Zou", "Yian Ma", "Tong Zhang"], "title": "Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion", "categories": ["stat.ML", "cs.LG"], "comment": "37 pages, 3 figures, 3 tables", "summary": "Continuous diffusion models have demonstrated remarkable performance in data\ngeneration across various domains, yet their efficiency remains constrained by\ntwo critical limitations: (1) the local adjacency structure of the forward\nMarkov process, which restricts long-range transitions in the data space, and\n(2) inherent biases introduced during the simulation of time-inhomogeneous\nreverse denoising processes. To address these challenges, we propose Quantized\nTransition Diffusion (QTD), a novel approach that integrates data quantization\nwith discrete diffusion dynamics. Our method first transforms the continuous\ndata distribution $p_*$ into a discrete one $q_*$ via histogram approximation\nand binary encoding, enabling efficient representation in a structured discrete\nlatent space. We then design a continuous-time Markov chain (CTMC) with Hamming\ndistance-based transitions as the forward process, which inherently supports\nlong-range movements in the original data space. For reverse-time sampling, we\nintroduce a \\textit{truncated uniformization} technique to simulate the reverse\nCTMC, which can provably provide unbiased generation from $q_*$ under minimal\nscore assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we\nprove that QTD can generate samples with $O(d\\ln^2(d/\\epsilon))$ score\nevaluations in expectation to approximate the $d$--dimensional target\ndistribution $p_*$ within an $\\epsilon$ error tolerance. Our method not only\nestablishes state-of-the-art inference efficiency but also advances the\ntheoretical foundations of diffusion-based generative modeling by unifying\ndiscrete and continuous diffusion paradigms."}
{"id": "2505.21932", "pdf": "https://arxiv.org/pdf/2505.21932", "abs": "https://arxiv.org/abs/2505.21932", "authors": ["Adriana L. Duncan", "Joe Kileel"], "title": "Higher-Order Group Synchronization", "categories": ["stat.ML", "cs.CV", "cs.LG", "math.CO", "math.OC"], "comment": "40 pages", "summary": "Group synchronization is the problem of determining reliable global estimates\nfrom noisy local measurements on networks. The typical task for group\nsynchronization is to assign elements of a group to the nodes of a graph in a\nway that respects group elements given on the edges which encode information\nabout local pairwise relationships between the nodes. In this paper, we\nintroduce a novel higher-order group synchronization problem which operates on\na hypergraph and seeks to synchronize higher-order local measurements on the\nhyperedges to obtain global estimates on the nodes. Higher-order group\nsynchronization is motivated by applications to computer vision and image\nprocessing, among other computational problems. First, we define the problem of\nhigher-order group synchronization and discuss its mathematical foundations.\nSpecifically, we give necessary and sufficient synchronizability conditions\nwhich establish the importance of cycle consistency in higher-order group\nsynchronization. Then, we propose the first computational framework for general\nhigher-order group synchronization; it acts globally and directly on\nhigher-order measurements using a message passing algorithm. We discuss\ntheoretical guarantees for our framework, including convergence analyses under\noutliers and noise. Finally, we show potential advantages of our method through\nnumerical experiments. In particular, we show that in certain cases our\nhigher-order method applied to rotational and angular synchronization\noutperforms standard pairwise synchronization methods and is more robust to\noutliers. We also show that our method has comparable performance on simulated\ncryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM\nreconstruction package."}
{"id": "2505.22048", "pdf": "https://arxiv.org/pdf/2505.22048", "abs": "https://arxiv.org/abs/2505.22048", "authors": ["Haihan Zhang", "Weicheng Lin", "Yuanshi Liu", "Cong Fang"], "title": "Learning Curves of Stochastic Gradient Descent in Kernel Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper considers a canonical problem in kernel regression: how good are\nthe model performances when it is trained by the popular online first-order\nalgorithms, compared to the offline ones, such as ridge and ridgeless\nregression? In this paper, we analyze the foundational single-pass Stochastic\nGradient Descent (SGD) in kernel regression under source condition where the\noptimal predictor can even not belong to the RKHS, i.e. the model is\nmisspecified. Specifically, we focus on the inner product kernel over the\nsphere and characterize the exact orders of the excess risk curves under\ndifferent scales of sample sizes $n$ concerning the input dimension $d$.\nSurprisingly, we show that SGD achieves min-max optimal rates up to constants\namong all the scales, without suffering the saturation, a prevalent phenomenon\nobserved in (ridge) regression, except when the model is highly misspecified\nand the learning is in a final stage where $n\\gg d^{\\gamma}$ with any constant\n$\\gamma >0$. The main reason for SGD to overcome the curse of saturation is the\nexponentially decaying step size schedule, a common practice in deep neural\nnetwork training. As a byproduct, we provide the \\emph{first} provable\nadvantage of the scheme over the iterative averaging method in the common\nsetting."}
{"id": "2505.22050", "pdf": "https://arxiv.org/pdf/2505.22050", "abs": "https://arxiv.org/abs/2505.22050", "authors": ["Di Wu", "Jiaxin Fan", "Junzhe Zang", "Guanbo Wang", "Wei Yin", "Wenhao Li", "Bo Jin"], "title": "Reinforced Reasoning for Embodied Planning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Embodied planning requires agents to make coherent multi-step decisions based\non dynamic visual observations and natural language goals. While recent\nvision-language models (VLMs) excel at static perception tasks, they struggle\nwith the temporal reasoning, spatial understanding, and commonsense grounding\nneeded for planning in interactive environments. In this work, we introduce a\nreinforcement fine-tuning framework that brings R1-style reasoning enhancement\ninto embodied planning. We first distill a high-quality dataset from a powerful\nclosed-source model and perform supervised fine-tuning (SFT) to equip the model\nwith structured decision-making priors. We then design a rule-based reward\nfunction tailored to multi-step action quality and optimize the policy via\nGeneralized Reinforced Preference Optimization (GRPO). Our approach is\nevaluated on Embench, a recent benchmark for interactive embodied tasks,\ncovering both in-domain and out-of-domain scenarios. Experimental results show\nthat our method significantly outperforms models of similar or larger scale,\nincluding GPT-4o-mini and 70B+ open-source baselines, and exhibits strong\ngeneralization to unseen environments. This work highlights the potential of\nreinforcement-driven reasoning to advance long-horizon planning in embodied AI."}
{"id": "2505.22104", "pdf": "https://arxiv.org/pdf/2505.22104", "abs": "https://arxiv.org/abs/2505.22104", "authors": ["Davide Corsi", "Kaushik Mallik", "Andoni Rodriguez", "Cesar Sanchez"], "title": "Efficient Dynamic Shielding for Parametric Safety Specifications", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Shielding has emerged as a promising approach for ensuring safety of\nAI-controlled autonomous systems. The algorithmic goal is to compute a shield,\nwhich is a runtime safety enforcement tool that needs to monitor and intervene\nthe AI controller's actions if safety could be compromised otherwise.\nTraditional shields are designed statically for a specific safety requirement.\nTherefore, if the safety requirement changes at runtime due to changing\noperating conditions, the shield needs to be recomputed from scratch, causing\ndelays that could be fatal. We introduce dynamic shields for parametric safety\nspecifications, which are succinctly represented sets of all possible safety\nspecifications that may be encountered at runtime. Our dynamic shields are\nstatically designed for a given safety parameter set, and are able to\ndynamically adapt as the true safety specification (permissible by the\nparameters) is revealed at runtime. The main algorithmic novelty lies in the\ndynamic adaptation procedure, which is a simple and fast algorithm that\nutilizes known features of standard safety shields, like maximal\npermissiveness. We report experimental results for a robot navigation problem\nin unknown territories, where the safety specification evolves as new obstacles\nare discovered at runtime. In our experiments, the dynamic shields took a few\nminutes for their offline design, and took between a fraction of a second and a\nfew seconds for online adaptation at each step, whereas the brute-force online\nrecomputation approach was up to 5 times slower."}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments."}
{"id": "2505.22326", "pdf": "https://arxiv.org/pdf/2505.22326", "abs": "https://arxiv.org/abs/2505.22326", "authors": ["James M. Adams", "Gesine Reinert", "Lukasz Szpruch", "Carsten Maple", "Andrew Elliott"], "title": "Individualised Counterfactual Examples Using Conformal Prediction Intervals", "categories": ["stat.ML", "cs.LG"], "comment": "Submitted to Conformal and Probabilistic Predictions With\n  Applications (COPA) 2025", "summary": "Counterfactual explanations for black-box models aim to pr ovide insight into\nan algorithmic decision to its recipient. For a binary classification problem\nan individual counterfactual details which features might be changed for the\nmodel to infer the opposite class. High-dimensional feature spaces that are\ntypical of machine learning classification models admit many possible\ncounterfactual examples to a decision, and so it is important to identify\nadditional criteria to select the most useful counterfactuals. In this paper,\nwe explore the idea that the counterfactuals should be maximally informative\nwhen considering the knowledge of a specific individual about the underlying\nclassifier. To quantify this information gain we explicitly model the knowledge\nof the individual, and assess the uncertainty of predictions which the\nindividual makes by the width of a conformal prediction interval. Regions of\nfeature space where the prediction interval is wide correspond to areas where\nthe confidence in decision making is low, and an additional counterfactual\nexample might be more informative to an individual. To explore and evaluate our\nindividualised conformal prediction interval counterfactuals (CPICFs), first we\npresent a synthetic data set on a hypercube which allows us to fully visualise\nthe decision boundary, conformal intervals via three different methods, and\nresultant CPICFs. Second, in this synthetic data set we explore the impact of a\nsingle CPICF on the knowledge of an individual locally around the original\nquery. Finally, in both our synthetic data set and a complex real world dataset\nwith a combination of continuous and discrete variables, we measure the utility\nof these counterfactuals via data augmentation, testing the performance on a\nheld out set."}
{"id": "2505.22332", "pdf": "https://arxiv.org/pdf/2505.22332", "abs": "https://arxiv.org/abs/2505.22332", "authors": ["Timo Löhr", "Paul Hofman", "Felix Mohr", "Eyke Hüllermeier"], "title": "Credal Prediction based on Relative Likelihood", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Predictions in the form of sets of probability distributions, so-called\ncredal sets, provide a suitable means to represent a learner's epistemic\nuncertainty. In this paper, we propose a theoretically grounded approach to\ncredal prediction based on the statistical notion of relative likelihood: The\ntarget of prediction is the set of all (conditional) probability distributions\nproduced by the collection of plausible models, namely those models whose\nrelative likelihood exceeds a specified threshold. This threshold has an\nintuitive interpretation and allows for controlling the trade-off between\ncorrectness and precision of credal predictions. We tackle the problem of\napproximating credal sets defined in this way by means of suitably modified\nensemble learning techniques. To validate our approach, we illustrate its\neffectiveness by experiments on benchmark datasets demonstrating superior\nuncertainty representation without compromising predictive performance. We also\ncompare our method against several state-of-the-art baselines in credal\nprediction."}
{"id": "2505.22364", "pdf": "https://arxiv.org/pdf/2505.22364", "abs": "https://arxiv.org/abs/2505.22364", "authors": ["Gabriele Visentin", "Patrick Cheridito"], "title": "Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows", "categories": ["stat.ML", "cs.LG", "65K99 (Primary) 68T07, 68T99 (Secondary)"], "comment": null, "summary": "We present a novel method for efficiently computing optimal transport maps\nand Wasserstein barycenters in high-dimensional spaces. Our approach uses\nconditional normalizing flows to approximate the input distributions as\ninvertible pushforward transformations from a common latent space. This makes\nit possible to directly solve the primal problem using gradient-based\nminimization of the transport cost, unlike previous methods that rely on dual\nformulations and complex adversarial optimization. We show how this approach\ncan be extended to compute Wasserstein barycenters by solving a conditional\nvariance minimization problem. A key advantage of our conditional architecture\nis that it enables the computation of barycenters for hundreds of input\ndistributions, which was computationally infeasible with previous methods. Our\nnumerical experiments illustrate that our approach yields accurate results\nacross various high-dimensional tasks and compares favorably with previous\nstate-of-the-art methods."}
{"id": "2505.22481", "pdf": "https://arxiv.org/pdf/2505.22481", "abs": "https://arxiv.org/abs/2505.22481", "authors": ["Yiming Xi", "Konstantinos Zygalakis", "Marcelo Pereyra"], "title": "Hypothesis Testing in Imaging Inverse Problems", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper proposes a framework for semantic hypothesis testing tailored to\nimaging inverse problems. Modern imaging methods struggle to support hypothesis\ntesting, a core component of the scientific method that is essential for the\nrigorous interpretation of experiments and robust interfacing with\ndecision-making processes. There are three main reasons why image-based\nhypothesis testing is challenging. First, the difficulty of using a single\nobservation to simultaneously reconstruct an image, formulate hypotheses, and\nquantify their statistical significance. Second, the hypotheses encountered in\nimaging are mostly of semantic nature, rather than quantitative statements\nabout pixel values. Third, it is challenging to control test error\nprobabilities because the null and alternative distributions are often unknown.\nOur proposed approach addresses these difficulties by leveraging concepts from\nself-supervised computational imaging, vision-language models, and\nnon-parametric hypothesis testing with e-values. We demonstrate our proposed\nframework through numerical experiments related to image-based phenotyping,\nwhere we achieve excellent power while robustly controlling Type I errors."}
{"id": "2505.22518", "pdf": "https://arxiv.org/pdf/2505.22518", "abs": "https://arxiv.org/abs/2505.22518", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "title": "IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62F10, 68T07, 62-08"], "comment": "Under review", "summary": "Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications."}
{"id": "2505.22527", "pdf": "https://arxiv.org/pdf/2505.22527", "abs": "https://arxiv.org/abs/2505.22527", "authors": ["Agnideep Aich", "Ashit Aich", "Bruce Wade"], "title": "Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling", "categories": ["stat.ML", "cs.LG", "68T07, 37J39, 65P10, 62B10, 53D22, 94A17"], "comment": "Submitted", "summary": "We introduce the Symplectic Generative Network (SGN), a deep generative model\nthat leverages Hamiltonian mechanics to construct an invertible,\nvolume-preserving mapping between a latent space and the data space. By\nendowing the latent space with a symplectic structure and modeling data\ngeneration as the time evolution of a Hamiltonian system, SGN achieves exact\nlikelihood evaluation without incurring the computational overhead of Jacobian\ndeterminant calculations. In this work, we provide a rigorous mathematical\nfoundation for SGNs through a comprehensive theoretical framework that\nincludes: (i) complete proofs of invertibility and volume preservation, (ii) a\nformal complexity analysis with theoretical comparisons to Variational\nAutoencoders and Normalizing Flows, (iii) strengthened universal approximation\nresults with quantitative error bounds, (iv) an information-theoretic analysis\nbased on the geometry of statistical manifolds, and (v) an extensive stability\nanalysis with adaptive integration guarantees. These contributions highlight\nthe fundamental advantages of SGNs and establish a solid foundation for future\nempirical investigations and applications to complex, high-dimensional data."}
{"id": "2505.22554", "pdf": "https://arxiv.org/pdf/2505.22554", "abs": "https://arxiv.org/abs/2505.22554", "authors": ["Agnideep Aich", "Md Monzur Murshed", "Amanda Mayeaux", "Sameera Hewage"], "title": "Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62P10, 68T07"], "comment": "Submitted", "summary": "Accurate diabetes risk prediction relies on identifying key features from\ncomplex health datasets, but conventional methods like mutual information (MI)\nfilters and genetic algorithms (GAs) often overlook extreme dependencies\ncritical for high-risk subpopulations. In this study we introduce a\nfeature-selection framework using the upper-tail dependence coefficient\n({\\lambda}U) of the novel A2 copula, which quantifies how often extreme higher\nvalues of a predictor co-occur with diabetes diagnoses (target variable).\nApplied to the CDC Diabetes Health Indicators dataset (n=253,680), our method\nprioritizes five predictors (self-reported general health, high blood pressure,\nbody mass index, mobility limitations, and high cholesterol levels) based on\nupper tail dependencies. These features match or outperform MI and GA selected\nsubsets across four classifiers (Random Forest, XGBoost, Logistic Regression,\nGradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to\n0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation\nimportance confirms clinical relevance, with BMI and general health driving\naccuracy. To our knowledge, this is the first work to apply a copula's\nupper-tail dependence for supervised feature selection, bridging extreme-value\ntheory and machine learning to deliver a practical toolkit for diabetes\nprevention."}
{"id": "2505.22597", "pdf": "https://arxiv.org/pdf/2505.22597", "abs": "https://arxiv.org/abs/2505.22597", "authors": ["Ngoc La", "Ruaridh Mon-Williams", "Julie A. Shah"], "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted to Proceedings of ICAPS 2025", "summary": "In recent years, reinforcement learning (RL) methods have been widely tested\nusing tools like OpenAI Gym, though many tasks in these environments could also\nbenefit from hierarchical planning. However, there is a lack of a tool that\nenables seamless integration of hierarchical planning with RL. Hierarchical\nDomain Definition Language (HDDL), used in classical planning, introduces a\nstructured approach well-suited for model-based RL to address this gap. To\nbridge this integration, we introduce HDDLGym, a Python-based tool that\nautomatically generates OpenAI Gym environments from HDDL domains and problems.\nHDDLGym serves as a link between RL and hierarchical planning, supporting\nmulti-agent scenarios and enabling collaborative planning among agents. This\npaper provides an overview of HDDLGym's design and implementation, highlighting\nthe challenges and design choices involved in integrating HDDL with the Gym\ninterface, and applying RL policies to support hierarchical planning. We also\nprovide detailed instructions and demonstrations for using the HDDLGym\nframework, including how to work with existing HDDL domains and problems from\nInternational Planning Competitions, exemplified by the Transport domain.\nAdditionally, we offer guidance on creating new HDDL domains for multi-agent\nscenarios and demonstrate the practical use of HDDLGym in the Overcooked\ndomain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a\nvaluable tool for studying RL in hierarchical planning, particularly in\nmulti-agent contexts."}
{"id": "2505.22622", "pdf": "https://arxiv.org/pdf/2505.22622", "abs": "https://arxiv.org/abs/2505.22622", "authors": ["Jiawei Ge", "Amanda Wang", "Shange Tang", "Chi Jin"], "title": "Principled Out-of-Distribution Generalization via Simplicity", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Modern foundation models exhibit remarkable out-of-distribution (OOD)\ngeneralization, solving tasks far beyond the support of their training data.\nHowever, the theoretical principles underpinning this phenomenon remain\nelusive. This paper investigates this problem by examining the compositional\ngeneralization abilities of diffusion models in image generation. Our analysis\nreveals that while neural network architectures are expressive enough to\nrepresent a wide range of models -- including many with undesirable behavior on\nOOD inputs -- the true, generalizable model that aligns with human expectations\ntypically corresponds to the simplest among those consistent with the training\ndata.\n  Motivated by this observation, we develop a theoretical framework for OOD\ngeneralization via simplicity, quantified using a predefined simplicity metric.\nWe analyze two key regimes: (1) the constant-gap setting, where the true model\nis strictly simpler than all spurious alternatives by a fixed gap, and (2) the\nvanishing-gap setting, where the fixed gap is replaced by a smoothness\ncondition ensuring that models close in simplicity to the true model yield\nsimilar predictions. For both regimes, we study the regularized maximum\nlikelihood estimator and establish the first sharp sample complexity guarantees\nfor learning the true, generalizable, simple model."}
