{"id": "2511.00020", "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms.", "AI": {"tldr": "提出一个基于BERT和ResNet-50的多模态假评论检测框架，通过融合文本和视觉特征来识别虚假评论，在包含21,142张图片的数据集上取得了0.934的F1分数，优于单模态方法。", "motivation": "当前数字商务中虚假评论泛滥，现有检测模型主要依赖单模态文本数据，无法捕捉跨模态的语义不一致性，需要更强大的多模态检测方法。", "method": "使用BERT编码文本特征，ResNet-50提取视觉特征，通过分类头融合这些多模态表示来联合预测评论真实性。", "result": "多模态模型在测试集上获得0.934的F1分数，优于单模态基线模型，能够有效检测文本赞美与无关/低质量图像之间的不一致性。", "conclusion": "多模态学习在维护数字信任中发挥关键作用，为各类在线平台的内容审核提供了可扩展的解决方案。"}}
{"id": "2511.00039", "pdf": "https://arxiv.org/pdf/2511.00039", "abs": "https://arxiv.org/abs/2511.00039", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Dynamic pricing in retail requires policies that adapt to shifting demand\nwhile coordinating decisions across related products. We present a systematic\nempirical study of multi-agent reinforcement learning for retail price\noptimization, comparing a strong MAPPO baseline with a\ngraph-attention-augmented variant (MAPPO+GAT) that leverages learned\ninteractions among products. Using a simulated pricing environment derived from\nreal transaction data, we evaluate profit, stability across random seeds,\nfairness across products, and training efficiency under a standardized\nevaluation protocol. The results indicate that MAPPO provides a robust and\nreproducible foundation for portfolio-level price control, and that MAPPO+GAT\nfurther enhances performance by sharing information over the product graph\nwithout inducing excessive price volatility. These results indicate that\ngraph-integrated MARL provides a more scalable and stable solution than\nindependent learners for dynamic retail pricing, offering practical advantages\nin multi-product decision-making.", "AI": {"tldr": "该论文比较了多智能体强化学习在零售价格优化中的应用，发现基于图注意力机制的MAPPO+GAT方法比标准MAPPO在性能、稳定性和公平性方面表现更好。", "motivation": "零售动态定价需要能够适应需求变化并协调相关产品决策的策略，传统方法在多产品决策中存在局限性。", "method": "使用从真实交易数据衍生的模拟定价环境，比较标准MAPPO基线与其图注意力增强变体(MAPPO+GAT)，评估利润、稳定性、公平性和训练效率。", "result": "MAPPO提供了稳健的投资组合级价格控制基础，MAPPO+GAT通过产品图信息共享进一步提升了性能，且不会导致过度价格波动。", "conclusion": "图集成多智能体强化学习为动态零售定价提供了比独立学习器更可扩展和稳定的解决方案，在多产品决策中具有实际优势。"}}
{"id": "2511.00048", "pdf": "https://arxiv.org/pdf/2511.00048", "abs": "https://arxiv.org/abs/2511.00048", "authors": ["Martin Bicher", "Maximilian Viehauser", "Daniele Giannandrea", "Hannah Kastinger", "Dominik Brunmeir", "Claire Rippinger", "Christoph Urach", "Niki Popper"], "title": "GEPOC Parameters -- Open Source Parametrisation and Validation for Austria, Version 2.0", "categories": ["cs.AI", "cs.CY", "62-11", "E.5; G.3; I.6.4; I.6.6; J.3; J.4"], "comment": "134 pages, 75 figures, 19 tables", "summary": "GEPOC, short for Generic Population Concept, is a collection of models and\nmethods for analysing population-level research questions. For the valid\napplication of the models for a specific country or region, stable and\nreproducible data processes are necessary, which provide valid and ready-to-use\nmodel parameters. This work contains a complete description of the\ndata-processing methods for computation of model parameters for Austria, based\nexclusively on freely and publicly accessible data. In addition to the\ndescription of the source data used, this includes all algorithms used for\naggregation, disaggregation, fusion, cleansing or scaling of the data, as well\nas a description of the resulting parameter files. The document places\nparticular emphasis on the computation of parameters for the most important\nGEPOC model, GEPOC ABM, a continuous-time agent-based population model. An\nextensive validation study using this particular model was made and is\npresented at the end of this work.", "AI": {"tldr": "GEPOC是一个通用人口概念模型集，本文详细描述了基于奥地利公开数据计算模型参数的数据处理方法，特别关注GEPOC ABM代理模型的参数计算，并进行了验证研究。", "motivation": "为GEPOC模型在特定国家或地区的有效应用提供稳定、可复现的数据处理流程和有效参数", "method": "使用奥地利公开可访问数据，通过聚合、分解、融合、清洗和缩放等算法处理数据，计算模型参数", "result": "开发了完整的数据处理方法，生成了可直接使用的参数文件，并对GEPOC ABM模型进行了广泛验证", "conclusion": "该工作为GEPOC模型在奥地利的应用提供了可靠的数据处理框架和验证参数，支持人口层面的研究分析"}}
{"id": "2511.00092", "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research.", "AI": {"tldr": "QuantumBench是首个专门为量子科学领域设计的LLM评估基准，包含800个多选题，用于评估大语言模型在量子领域的理解和应用能力", "motivation": "现有通用基准无法准确评估LLMs在量子科学等专业领域的知识掌握情况，量子科学具有非直观现象和高级数学要求，需要专门的评估工具", "method": "利用公开材料编制约800个涵盖量子科学9个领域的问答对，组织成8选项多选题数据集，评估多个现有LLM并分析其对问题格式变化的敏感性", "result": "创建了QuantumBench基准数据集，并对多个LLM在量子领域的表现进行了评估分析", "conclusion": "QuantumBench将指导LLMs在量子研究中的有效应用，填补了量子领域专业评估工具的空白"}}
{"id": "2511.00010", "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable proficiency\nin code generation. However, their ability to create complex visualizations for\nscaled and structured data remains largely unevaluated and underdeveloped. To\naddress this gap, we introduce PlotCraft, a new benchmark featuring 1k\nchallenging visualization tasks that cover a wide range of topics, such as\nfinance, scientific research, and sociology. The benchmark is structured around\nseven high-level visualization tasks and encompasses 48 distinct chart types.\nCrucially, it is the first to systematically evaluate both single-turn\ngeneration and multi-turn refinement across a diverse spectrum of task\ncomplexities. Our comprehensive evaluation of 23 leading LLMs on PlotCraft\nreveals obvious performance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent framework. Building upon this dataset, we develope\nPlotCraftor, a novel code generation model that achieves strong capabilities in\ncomplex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading proprietary approaches. Especially, on hard task,\nOur model achieves over 50% performance improvement. We will release the\nbenchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "PlotCraft是一个新的可视化代码生成基准测试，包含1000个复杂可视化任务，评估发现当前LLMs在复杂可视化方面存在明显缺陷，为此开发了SynthVis-30K数据集和PlotCraftor模型，在多项测试中表现优异。", "motivation": "当前大型语言模型在代码生成方面表现出色，但在复杂可视化任务方面的能力尚未得到充分评估和发展，需要系统性的基准测试和改进方案。", "method": "引入PlotCraft基准测试（1000个任务，7类任务，48种图表类型），开发SynthVis-30K合成数据集，构建PlotCraftor代码生成模型。", "result": "评估23个领先LLMs发现其在复杂可视化任务上存在明显缺陷，PlotCraftor模型在VisEval、PandasPlotBench和PlotCraft基准测试中表现优异，在困难任务上性能提升超过50%。", "conclusion": "PlotCraft填补了可视化代码生成评估的空白，PlotCraftor模型证明了在复杂数据可视化方面的强大能力，为LLMs在可视化领域的应用提供了重要贡献。"}}
{"id": "2511.00122", "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "categories": ["cs.AI"], "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy.", "AI": {"tldr": "Engineering.ai是一个基于多智能体架构的AI工程平台，通过专业AI工程师团队协作完成复杂工程设计，在无人机翼优化中实现了100%成功率和零人工干预", "motivation": "传统工程团队协作需要大量时间和成本，需要开发能够自主完成复杂工程任务的AI系统来提升效率", "method": "采用分层多智能体架构，由总工程师协调空气动力学、结构、声学和优化等专业AI工程师，通过文件通信和记忆系统实现协作，集成多种工程软件工具", "result": "在400多个参数配置中实现了100%成功率和零人工干预，验证了框架的可靠性", "conclusion": "AI智能体驱动的工程师团队有潜力自主完成复杂工程任务，为工程设计自动化提供了可信赖的解决方案"}}
{"id": "2511.00115", "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling.", "AI": {"tldr": "ProtoMBTI是一个基于原型理论的MBTI人格识别框架，通过LLM驱动的多维度数据增强、LoRA微调编码器和原型检索-修订循环，在准确率、可解释性和跨数据集泛化方面超越传统硬标签分类方法。", "motivation": "传统文本人格识别采用硬标签分类方法，掩盖了人类人格判断的渐进性和原型特性，需要更符合认知心理学原理的建模方式。", "method": "1) 使用LLM指导的多维度（语义、语言、情感）增强构建平衡语料库；2) LoRA微调轻量级编码器学习判别性嵌入和人格原型；3) 推理时进行检索-重用-修订-保留循环，通过提示投票聚合证据并持续丰富原型库。", "result": "在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI二分法和完整16型任务上均优于基线方法，并展现出强大的跨数据集泛化能力。", "conclusion": "将推理过程与心理学原型理论对齐，能够提升基于文本的人格建模在准确性、可解释性和迁移性方面的表现。"}}
{"id": "2511.00162", "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship.", "AI": {"tldr": "ARC-GEN是一个开源程序生成器，旨在通过生成更多训练样本来扩展ARC-AGI基准测试数据集，以解决原始数据集样本数量有限的问题，同时保持与原始数据分布特性的一致性。", "motivation": "ARC-AGI基准测试虽然对评估通用人工智能进展很重要，但其每个任务的训练样本数量有限（只有少量输入-输出网格对），这限制了需要大量样本的算法的性能。", "method": "开发了ARC-GEN开源程序生成器，该生成器具有两个关键特性：1）全面性（覆盖所有400个任务）2）模仿性（尽可能忠实保持原始ARC-AGI-1版本的分布特性和特征）。", "result": "成功创建了一个能够扩展原始训练数据集的过程生成器，为算法提供更多可行的样本对。", "conclusion": "ARC-GEN不仅解决了ARC-AGI样本稀缺的问题，还可用于建立静态基准套件来验证2025年Google Code Golf锦标赛提交程序的正确性，为AGI研究提供了更好的评估工具。"}}
{"id": "2511.00180", "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "categories": ["cs.CL", "cs.LG"], "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation.", "AI": {"tldr": "该论文提出了残差流解码器框架，用于探测语言模型中段落和文档级别的规划信息，发现在小模型中可解码相当于5个以上未来token的信息。", "motivation": "随着语言模型处理更长时间跨度任务能力的增强，现有方法局限于测试特定概念或token，需要新的方法来理解模型激活中的长期规划信息。", "method": "开发残差流解码器框架，通过多种方法探测模型激活，分析段落和文档尺度的规划信息。", "result": "在小型模型中成功解码出相当于5个以上未来token的信息，证明该方法能有效提取长期规划信息。", "conclusion": "该研究为更好地监控语言模型和理解其编码长期规划信息的方式奠定了基础，对模型可解释性研究有重要贡献。"}}
{"id": "2511.00194", "pdf": "https://arxiv.org/pdf/2511.00194", "abs": "https://arxiv.org/abs/2511.00194", "authors": ["Jovial Cheukam Ngouonou", "Ramiz Gindullin", "Claude-Guy Quimper", "Nicolas Beldiceanu", "Remi Douence"], "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "categories": ["cs.AI", "F.2.2, F.4.1"], "comment": null, "summary": "We present an improved incremental selection algorithm of the selection\nalgorithm presented in [1] and prove all the selected conjectures.", "AI": {"error": "'NoneType' object has no attribute 'model_dump'"}}
{"id": "2511.00198", "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction -- Filling the Mutual Information Gap", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies.", "AI": {"tldr": "本文挑战了传统使用下一个词预测(NTP)训练大语言模型的方法，提出通过预测信息丰富的token来更有效地训练LLM，在算术、多标签文本分类和自然语言生成任务中验证了方法的有效性。", "motivation": "优化大语言模型的训练性能是一个关键挑战，特别是在提高模型性能的同时保持计算成本。传统使用下一个词预测的方法可能不是最优的。", "method": "提出预测信息丰富的token而非传统下一个词预测的训练方法，在三种任务类型（算术、多标签文本分类、自然语言生成）中验证该方法。", "result": "研究结果表明该方法能更有效地训练LLM，但具体性能提升数据未在摘要中明确说明。", "conclusion": "这项工作为优化LLM训练提供了原则性方法，既提升了模型性能，也增进了对目标token选择策略的理论理解。"}}
{"id": "2511.00206", "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise.", "AI": {"tldr": "本文综述了大型语言模型如何帮助解决认知科学领域的知识整合和概念清晰性问题，通过支持跨学科连接、理论形式化、测量分类、通用建模框架和个体差异分析，但需谨慎使用以补充而非替代人类专业知识。", "motivation": "认知科学面临多学科性和跨学科性带来的知识整合与概念清晰性挑战，需要新工具来应对这些长期存在的问题。", "method": "通过综述分析大型语言模型在当前认知科学关键挑战领域的应用能力和局限性，包括跨学科连接建立、理论形式化、测量分类、通用建模框架和个体差异分析。", "result": "大型语言模型在支持认知科学知识整合方面展现出潜力，能够协助建立跨学科联系、形式化理论框架、开发清晰测量体系，但存在局限性需要谨慎使用。", "conclusion": "LLMs可以作为认知科学整合性和累积性发展的工具，但必须明智使用以补充人类专业知识，而不是完全替代人类判断和专业知识。"}}
{"id": "2511.00222", "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users.", "AI": {"tldr": "本文提出了一个评估和改进LLM在对话中角色一致性的统一框架，通过三个自动指标和强化学习微调，将角色不一致性降低了55%以上", "motivation": "现成的LLM在模拟人类用户时经常出现角色漂移、前后矛盾或放弃角色适当行为的问题，影响了在治疗、教育等交互场景中的模拟效果", "method": "定义了三个自动一致性指标（提示到行、行到行、问答一致性），并以此作为奖励信号，使用多轮强化学习对LLM进行微调，针对患者、学生和社交聊天伙伴三种用户角色", "result": "方法将角色不一致性降低了超过55%，产生了更加连贯和忠实模拟的用户", "conclusion": "该框架有效提升了LLM在对话模拟中的角色一致性，为AI代理的可扩展训练和评估提供了更可靠的模拟用户"}}
{"id": "2511.00267", "pdf": "https://arxiv.org/pdf/2511.00267", "abs": "https://arxiv.org/abs/2511.00267", "authors": ["Christian Prothmann", "Vijay Gadepally", "Jeremy Kepner", "Koley Borchard", "Luca Carlone", "Zachary Folcik", "J. Daniel Grith", "Michael Houle", "Jonathan P. How", "Nathan Hughes", "Ifueko Igbinedion", "Hayden Jananthan", "Tejas Jayashankar", "Michael Jones", "Sertac Karaman", "Binoy G. Kurien", "Alejandro Lancho", "Giovanni Lavezzi", "Gary C. F. Lee", "Charles E. Leiserson", "Richard Linares", "Lindsey McEvoy", "Peter Michaleas", "Chasen Milner", "Alex Pentland", "Yury Polyanskiy", "Jovan Popovich", "Jeffrey Price", "Tim W. Reid", "Stephanie Riley", "Siddharth Samsi", "Peter Saunders", "Olga Simek", "Mark S. Veillette", "Amir Weiss", "Gregory W. Wornell", "Daniela Rus", "Scott T. Ruppel"], "title": "Advancing AI Challenges for the United States Department of the Air Force", "categories": ["cs.AI", "cs.CY", "cs.GL", "cs.LG"], "comment": "8 pages, 8 figures, 59 references. To appear in IEEE HPEC 2025", "summary": "The DAF-MIT AI Accelerator is a collaboration between the United States\nDepartment of the Air Force (DAF) and the Massachusetts Institute of Technology\n(MIT). This program pioneers fundamental advances in artificial intelligence\n(AI) to expand the competitive advantage of the United States in the defense\nand civilian sectors. In recent years, AI Accelerator projects have developed\nand launched public challenge problems aimed at advancing AI research in\npriority areas. Hallmarks of AI Accelerator challenges include large, publicly\navailable, and AI-ready datasets to stimulate open-source solutions and engage\nthe wider academic and private sector AI ecosystem. This article supplements\nour previous publication, which introduced AI Accelerator challenges. We\nprovide an update on how ongoing and new challenges have successfully\ncontributed to AI research and applications of AI technologies.", "AI": {"tldr": "DAF-MIT AI加速器项目更新报告，介绍美国空军与MIT合作开发的AI挑战赛项目如何推动AI研究和应用发展", "motivation": "通过公开挑战赛和大型数据集促进AI技术进步，扩大美国在国防和民用领域的竞争优势", "method": "开发和发布公开挑战问题，提供大规模、公开可用的AI就绪数据集，激励开源解决方案开发", "result": "成功推动了AI研究和技术应用，吸引了学术界和私营部门的广泛参与", "conclusion": "DAF-MIT AI加速器挑战赛项目有效地促进了AI生态系统发展，为美国在AI领域的竞争优势做出了贡献"}}
{"id": "2511.00265", "pdf": "https://arxiv.org/pdf/2511.00265", "abs": "https://arxiv.org/abs/2511.00265", "authors": ["Arman Anwar", "Zefang Liu"], "title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training\nbut are often scripted, resource-intensive, and difficult to scale. We\nintroduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches\ngame that integrates large language model teammates with a Bloom-aligned,\nretrieval-augmented copilot (C2D2). The system expands a curated corpus into\nfactual, conceptual, procedural, and metacognitive snippets, delivering\non-demand, cognitively targeted hints. Prompt-engineered agents employ a\nscaffolding ladder that gradually fades as learner confidence grows. In a\nsolo-player pilot with four graduate students, participants reported greater\nintention to use the agent-based version compared to the physical card deck and\nviewed it as more scalable, though a ceiling effect emerged on a simple\nknowledge quiz. Despite limitations of small sample size, single-player focus,\nand narrow corpus, these early findings suggest that large language model\naugmented TTXs can provide lightweight, repeatable practice without the\nlogistical burden of traditional exercises. Planned extensions include\nmulti-player modes, telemetry-driven coaching, and comparative studies with\nlarger cohorts.", "AI": {"tldr": "AgentBnB是基于浏览器的网络安全桌面演练系统，使用大型语言模型和检索增强副驾驶来提供按需的认知提示，相比传统脚本化演练更轻量级、可扩展。", "motivation": "传统的网络安全桌面演练存在脚本化、资源密集和难以扩展的问题，需要一种更轻量、可重复的培训解决方案。", "method": "开发了基于浏览器的Backdoors & Breaches游戏重制版，集成LLM队友和Bloom对齐的检索增强副驾驶(C2D2)，使用提示工程代理和渐进式脚手架方法。", "result": "在4名研究生的试点研究中，参与者更倾向于使用基于代理的版本，认为更具可扩展性，但在简单知识测验中出现天花板效应。", "conclusion": "LLM增强的桌面演练可以提供轻量级、可重复的练习，无需传统演练的后勤负担，未来计划扩展多人模式、遥测驱动教练和更大规模的比较研究。"}}
{"id": "2511.00340", "pdf": "https://arxiv.org/pdf/2511.00340", "abs": "https://arxiv.org/abs/2511.00340", "authors": ["Manan Roy Choudhury", "Adithya Chandramouli", "Mannan Anand", "Vivek Gupta"], "title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities", "categories": ["cs.AI"], "comment": "41 pages, 4 images", "summary": "The rapid integration of large language models (LLMs) into high-stakes legal\nwork has exposed a critical gap: no benchmark exists to systematically\nstress-test their reliability against the nuanced, adversarial, and often\nsubtle flaws present in real-world contracts. To address this, we introduce\nCLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an\nLLM's legal reasoning. We study the capabilities of LLMs to detect and reason\nabout fine-grained discrepancies by producing over 7500 real-world perturbed\ncontracts from foundational datasets like CUAD and ContractNLI. Our novel,\npersona-driven pipeline generates 10 distinct anomaly categories, which are\nthen validated against official statutes using a Retrieval-Augmented Generation\n(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'\nability to detect embedded legal flaws and explain their significance. Our\nanalysis shows a key weakness: these models often miss subtle errors and\nstruggle even more to justify them legally. Our work outlines a path to\nidentify and correct such reasoning failures in legal AI.", "AI": {"tldr": "CLAUSE是首个针对LLM法律推理脆弱性的基准测试，通过生成7500+扰动合同来评估模型检测细微法律缺陷的能力，发现主流LLM在识别和解释法律错误方面存在显著弱点", "motivation": "大型语言模型在法律高风险领域应用中缺乏系统性测试基准，无法评估其对现实合同中微妙、对抗性缺陷的可靠性", "method": "从CUAD和ContractNLI数据集生成7500多个扰动合同，创建10种异常类别，使用RAG系统确保法律保真度，通过人物角色驱动流程评估LLM检测法律缺陷的能力", "result": "分析显示LLM经常遗漏细微错误，在法律理由解释方面表现更差，揭示了模型在法律推理中的关键弱点", "conclusion": "该研究为识别和纠正法律AI中的推理失败提供了路径，强调了需要改进LLM在法律细微差别理解方面的能力"}}
{"id": "2511.00268", "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance.", "AI": {"tldr": "本文提出了IL-PCR语料库，为法律条文检索和先例检索两个相关任务提供统一测试平台，并开发了基于LLM的重排序方法，取得了最佳性能", "motivation": "法律实践中条文检索和先例检索是相关任务，但现有研究独立处理这两个任务，缺乏利用其内在关联性的统一框架", "method": "构建IL-PCR语料库，实验多种基线模型（词法模型、语义模型、基于GNN的集成模型），开发基于LLM的重排序方法以利用任务间依赖关系", "result": "LLM-based re-ranking方法在两个检索任务上都取得了最佳性能表现", "conclusion": "IL-PCR语料库为法律检索研究提供了统一测试平台，LLM重排序方法有效利用了条文检索和先例检索之间的依赖关系，提升了检索性能"}}
{"id": "2511.00379", "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research.", "AI": {"tldr": "提出基于伦理决策模型的五步伦理推理框架，通过结构化推理过程提升LLM对不同地区文化价值观的理解和伦理分析能力，在SafeWorld基准测试中显著优于基线方法。", "motivation": "当前LLM对齐方法往往产生表面遵从而非真正的伦理理解，无法处理人类价值观的复杂性和情境依赖性，需要解决跨地区文化的多样化人类价值观对齐问题。", "method": "提出结构化五步伦理推理框架：情境事实收集、分层社会规范识别、选项生成、多视角伦理影响分析、反思。可通过提示工程或监督微调实现。", "result": "在专为地区价值观对齐设计的SafeWorld基准测试中，该框架相比基线方法显著提升了LLM与多样化人类价值观的对齐效果，实现了更准确的社会规范识别和更文化适宜的道德推理。", "conclusion": "该研究为通过跨学科研究开发更有效对齐全球社会多元化价值观的LLM提供了具体路径，理论基础的伦理推理范式能增强模型对地区特性的理解和细致伦理分析能力。"}}
{"id": "2511.00270", "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings.", "AI": {"tldr": "提出POSESTITCH-SLT预训练方案，通过模板生成句子对训练，在How2Sign和iSign数据集上显著提升手语翻译性能，BLEU-4分数分别从1.97提升到4.56和从0.55提升到3.43", "motivation": "手语翻译面临大规模句子对齐数据集稀缺的挑战，需要解决低资源环境下的翻译问题", "method": "基于语言模板的句子生成技术，提出POSESTITCH-SLT预训练方案，使用简单的基于transformer的编码器-解码器架构，利用模板生成的句子对进行训练", "result": "在两个手语数据集上显著超越现有技术：How2Sign的BLEU-4从1.97提升至4.56，iSign从0.55提升至3.43", "conclusion": "模板驱动的合成监督在低资源手语翻译环境中具有显著有效性，证明了该方法在基于姿态的无gloss翻译中的优越性"}}
{"id": "2511.00382", "pdf": "https://arxiv.org/pdf/2511.00382", "abs": "https://arxiv.org/abs/2511.00382", "authors": ["Mina Taraghi", "Yann Pequignot", "Amin Nikanjam", "Mohamed Amine Merzouk", "Foutse Khomh"], "title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Organizations are increasingly adopting and adapting Large Language Models\n(LLMs) hosted on public repositories such as HuggingFace. Although these\nadaptations often improve performance on specialized downstream tasks, recent\nevidence indicates that they can also degrade a model's safety or fairness.\nSince different fine-tuning techniques may exert distinct effects on these\ncritical dimensions, this study undertakes a systematic assessment of their\ntrade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,\nIA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model\nfamilies (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235\nfine-tuned variants are evaluated across eleven safety hazard categories and\nnine demographic fairness dimensions. The results show that adapter-based\napproaches (LoRA, IA3) tend to improve safety scores and are the least\ndisruptive to fairness, retaining higher accuracy and lower bias scores. In\ncontrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce\nsafety and cause larger fairness regressions, with decreased accuracy and\nincreased bias. Alignment shifts are strongly moderated by base model type:\nLLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest\nsafety decline, and Mistral, which is released without an internal moderation\nlayer, displays the greatest variance. Improvements in safety do not\nnecessarily translate into improvements in fairness, and no single\nconfiguration optimizes all fairness metrics simultaneously, indicating an\ninherent trade-off between these objectives. These findings suggest a practical\nguideline for safety-critical deployments: begin with a well-aligned base\nmodel, favour adapter-based PEFT, and conduct category-specific audits of both\nsafety and fairness.", "AI": {"tldr": "本研究系统评估了四种参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)对LLM安全性和公平性的影响，发现基于适配器的方法在保持安全性和公平性方面表现更好，而基于提示的方法则导致安全性和公平性下降。", "motivation": "虽然LLM的微调可以提升专业任务性能，但近期证据表明这可能降低模型的安全性和公平性，不同微调技术对这些关键维度的影响需要系统评估。", "method": "使用四种PEFT方法对四个指令调优模型家族(Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B、Gemma-7B)进行微调，共评估235个变体，涵盖11个安全危害类别和9个人口统计公平性维度。", "result": "基于适配器的方法(LoRA、IA3)倾向于提高安全分数且对公平性破坏最小；基于提示的方法(Prompt-Tuning、P-Tuning)通常降低安全性并导致更大的公平性回归；不同基础模型表现出不同的对齐稳定性。", "conclusion": "安全性和公平性改进不一定同步，没有单一配置能同时优化所有公平性指标，建议从良好对齐的基础模型开始，优先选择基于适配器的PEFT方法，并进行特定类别的安全和公平性审计。"}}
{"id": "2511.00315", "pdf": "https://arxiv.org/pdf/2511.00315", "abs": "https://arxiv.org/abs/2511.00315", "authors": ["Lee Xiong", "Maksim Tkachenko", "Johanes Effendi", "Ting Cai"], "title": "Language Modeling With Factorization Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose Factorization Memory, an efficient recurrent neural network (RNN)\narchitecture that achieves performance comparable to Transformer models on\nshort-context language modeling tasks while also demonstrating superior\ngeneralization in long-context scenarios. Our model builds upon Mamba-2,\nenabling Factorization Memory to exploit parallel computations during training\nwhile preserving constant computational and memory complexity during inference.\nTo further optimize model efficiency and representational capacity, we develop\na sparse formulation of Factorization Memory that updates only a subset of\nrecurrent states at each step while preserving the strong performance of its\ndense counterpart. To our knowledge, this represents the first RNN architecture\nthat successfully combines sparse memory activation with competitive\nperformance across both short and long-context settings. This work provides a\nsystematic empirical analysis of Factorization Memory in comparison to\nTransformer and Mamba-2 architectures.", "AI": {"tldr": "提出了Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中性能媲美Transformer，同时在长上下文场景中展现更好的泛化能力。该模型基于Mamba-2，支持训练时并行计算，推理时保持恒定计算和内存复杂度。", "motivation": "为了解决传统RNN在长上下文任务中的性能限制，同时保持训练效率和推理时的计算效率，开发一种既能媲美Transformer性能又具有更好长上下文泛化能力的模型架构。", "method": "基于Mamba-2构建Factorization Memory架构，开发稀疏化版本仅更新部分循环状态，保持密集版本性能的同时提升效率。进行系统性的实证分析，与Transformer和Mamba-2架构进行比较。", "result": "实现了首个成功结合稀疏内存激活的RNN架构，在短上下文和长上下文设置中均展现出竞争力性能，同时保持训练并行性和推理时的恒定复杂度。", "conclusion": "Factorization Memory提供了一种高效的替代方案，在保持RNN计算效率优势的同时，达到了与Transformer相当的性能水平，特别是在长上下文任务中表现出更好的泛化能力。"}}
{"id": "2511.00424", "pdf": "https://arxiv.org/pdf/2511.00424", "abs": "https://arxiv.org/abs/2511.00424", "authors": ["Ashutosh Anshul", "Gumpili Sai Pranav", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method", "categories": ["cs.AI"], "comment": null, "summary": "The recent coronavirus disease (Covid-19) has become a pandemic and has\naffected the entire globe. During the pandemic, we have observed a spike in\ncases related to mental health, such as anxiety, stress, and depression.\nDepression significantly influences most diseases worldwide, making it\ndifficult to detect mental health conditions in people due to unawareness and\nunwillingness to consult a doctor. However, nowadays, people extensively use\nonline social media platforms to express their emotions and thoughts. Hence,\nsocial media platforms are now becoming a large data source that can be\nutilized for detecting depression and mental illness. However, existing\napproaches often overlook data sparsity in tweets and the multimodal aspects of\nsocial media. In this paper, we propose a novel multimodal framework that\ncombines textual, user-specific, and image analysis to detect depression among\nsocial media users. To provide enough context about the user's emotional state,\nwe propose (i) an extrinsic feature by harnessing the URLs present in tweets\nand (ii) extracting textual content present in images posted in tweets. We also\nextract five sets of features belonging to different modalities to describe a\nuser. Additionally, we introduce a Deep Learning model, the Visual Neural\nNetwork (VNN), to generate embeddings of user-posted images, which are used to\ncreate the visual feature vector for prediction. We contribute a curated\nCovid-19 dataset of depressed and non-depressed users for research purposes and\ndemonstrate the effectiveness of our model in detecting depression during the\nCovid-19 outbreak. Our model outperforms existing state-of-the-art methods over\na benchmark dataset by 2%-8% and produces promising results on the Covid-19\ndataset. Our analysis highlights the impact of each modality and provides\nvaluable insights into users' mental and emotional states.", "AI": {"tldr": "本文提出了一种新颖的多模态框架，结合文本、用户特定和图像分析来检测社交媒体用户的抑郁症，特别是在COVID-19疫情期间。该方法通过提取URL内容和图像中的文本信息，并引入视觉神经网络(VNN)生成图像嵌入，在基准数据集上比现有方法提升2%-8%的性能。", "motivation": "COVID-19疫情期间心理健康问题激增，但抑郁症难以检测。社交媒体成为表达情绪的重要平台，现有方法忽视了推文数据稀疏性和多模态特性，需要更全面的分析框架。", "method": "提出多模态框架：1) 利用推文中的URL提取外部特征；2) 提取推文图片中的文本内容；3) 提取五组不同模态的特征；4) 引入视觉神经网络(VNN)生成用户发布图像的嵌入向量；5) 构建COVID-19抑郁症数据集进行验证。", "result": "模型在基准数据集上比现有最先进方法性能提升2%-8%，在COVID-19数据集上也显示出有希望的结果。分析揭示了每种模态的影响，为用户心理和情绪状态提供了有价值的洞察。", "conclusion": "该多模态框架有效解决了社交媒体抑郁症检测中的数据稀疏问题，通过综合利用文本、用户和视觉信息显著提升了检测性能，为疫情期间心理健康监测提供了有力工具。"}}
{"id": "2511.00341", "pdf": "https://arxiv.org/pdf/2511.00341", "abs": "https://arxiv.org/abs/2511.00341", "authors": ["Mihir Sahasrabudhe"], "title": "Reversal Invariance in Autoregressive Language Models", "categories": ["cs.CL"], "comment": "7 pages, theoretical note", "summary": "We formalize a structural property of the causal (autoregressive) language\nmodeling (CLM) objective: reversal invariance. Formally, the next-token\nprediction loss assigns identical likelihood to a corpus and its reversal,\nimplying that standard CLM pretraining is direction-blind. This symmetry\nexplains why models trained on reversed text can achieve comparable performance\nto those trained on forward text, despite the inherently time-asymmetric nature\nof human language and reasoning. We argue that this invariance represents a\nlimitation of current pretraining objectives rather than a benign artifact. If\nnatural language encodes directional dependencies - phonological,\nmorphological, or causal - a symmetric objective may fail to capture them. We\ntherefore propose viewing pretraining through the lens of temporal asymmetry,\nmotivating future work on loss functions and architectures that explicitly\nmodel the arrow of language while retaining standard language modeling\ncapacity.", "AI": {"tldr": "论文揭示了因果语言建模目标的结构对称性缺陷，提出需要显式建模语言时间不对称性的新方法", "motivation": "发现标准因果语言建模目标对文本正反顺序具有不变性，这种对称性可能无法捕捉自然语言中固有的时间不对称依赖关系", "method": "形式化分析了因果语言建模目标的逆转不变性，提出应从时间不对称性角度重新审视预训练目标", "result": "证明反转文本训练与正向文本训练可获得相当性能，但对称目标可能无法有效捕捉语言中的方向性依赖", "conclusion": "当前预训练目标存在局限性，需要开发能显式建模语言时间箭头的新损失函数和架构，同时保持标准语言建模能力"}}
{"id": "2511.00457", "pdf": "https://arxiv.org/pdf/2511.00457", "abs": "https://arxiv.org/abs/2511.00457", "authors": ["Chunyu Wei", "Wenji Hu", "Xingjia Hao", "Xin Wang", "Yifan Yang", "Yueguo Chen", "Yang Tian", "Yunhai Wang"], "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) face significant limitations when applied to\nlarge-scale graphs, struggling with context constraints and inflexible\nreasoning. We present GraphChain, a framework that enables LLMs to analyze\ncomplex graphs through dynamic sequences of specialized tools, mimicking human\nexploratory intelligence. Our approach introduces two key innovations: (1)\nProgressive Graph Distillation, a reinforcement learning mechanism that\ngenerates optimized tool sequences balancing task relevance with information\ncompression, and (2) Structure-aware Test-Time Adaptation, which efficiently\ntailors tool selection strategies to diverse graph topologies using spectral\nproperties and lightweight adapters without costly retraining. Experiments show\nGraphChain significantly outperforms prior methods, enabling scalable and\nadaptive LLM-driven graph analysis.", "AI": {"tldr": "GraphChain是一个让大语言模型通过动态工具序列分析复杂图数据的框架，解决了LLM在大规模图分析中的上下文限制和推理不灵活问题", "motivation": "大语言模型在处理大规模图数据时面临上下文限制和推理不灵活的问题，需要一种能够模拟人类探索智能的方法来进行图分析", "method": "提出了两个关键创新：1)渐进图蒸馏 - 通过强化学习生成平衡任务相关性和信息压缩的优化工具序列；2)结构感知测试时适应 - 利用谱属性和轻量级适配器为不同图拓扑定制工具选择策略，无需昂贵重新训练", "result": "实验表明GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析", "conclusion": "GraphChain框架通过动态工具序列和结构自适应机制，成功解决了LLM在大规模图分析中的核心挑战，为LLM在图数据处理领域提供了有效的解决方案"}}
{"id": "2511.00343", "pdf": "https://arxiv.org/pdf/2511.00343", "abs": "https://arxiv.org/abs/2511.00343", "authors": ["Changbing Yang", "Franklin Ma", "Freda Shi", "Jian Zhu"], "title": "LingGym: How Far Are LLMs from Thinking Like Field Linguists?", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity\nfor meta-linguistic reasoning using Interlinear Glossed Text (IGT) and\ngrammatical descriptions extracted from 18 typologically diverse reference\ngrammars. Unlike previous work that focuses on specific downstream tasks, we\nassess whether LLMs can generalize linguistic inference across low-resource\nlanguages and structures not seen during training. We present a controlled\nevaluation task: Word-Gloss Inference, in which the model must infer a missing\nword and gloss from context using varying levels of linguistic information\n(e.g., glosses, grammatical explanations, translations). Our results show that\nincorporating structured linguistic cues leads to consistent improvements in\nreasoning performance across all models. This work highlights both the promise\nand current limitations of using LLMs for typologically informed linguistic\nanalysis and low-resource language documentation.", "AI": {"tldr": "LingGym是一个评估LLM元语言推理能力的新基准，使用跨语言注释文本和语法描述来测试模型在未见过的低资源语言结构上的泛化能力。", "motivation": "现有研究主要关注特定下游任务，缺乏对LLM在跨语言语言学推理和低资源语言泛化能力的系统性评估。", "method": "从18种类型多样的参考语法中提取跨语言注释文本和语法描述，设计词汇-注释推理任务，让模型根据不同程度的语言信息推断缺失的词汇和注释。", "result": "结构化语言线索的加入在所有模型中带来了一致的推理性能提升。", "conclusion": "这项工作展示了使用LLM进行类型学语言分析和低资源语言文档化的潜力和当前局限性。"}}
{"id": "2511.00509", "pdf": "https://arxiv.org/pdf/2511.00509", "abs": "https://arxiv.org/abs/2511.00509", "authors": ["Yifan Xia", "Guorui Chen", "Wenqian Yu", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "Reimagining Safety Alignment with An Image", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) excel in diverse applications but face dual\nchallenges: generating harmful content under jailbreak attacks and over-refusal\nof benign queries due to rigid safety mechanisms. These issues are further\ncomplicated by the need to accommodate different value systems and precisely\nalign with given safety preferences. Moreover, traditional methods like SFT and\nRLHF lack this capability due to their costly parameter tuning requirements and\ninability to support multiple value systems within a single model. These\nproblems are more obvious in multimodal large language models (MLLMs),\nespecially in terms of heightened over-refusal in cross-modal tasks and new\nsecurity risks arising from expanded attack surfaces. We propose Magic Image,\nan optimization-driven visual prompt framework that enhances security while\nreducing over-refusal. By optimizing image prompts using harmful/benign\nsamples, our method enables a single model to adapt to different value systems\nand better align with given safety preferences without parameter updates.\nExperiments demonstrate improved safety-effectiveness balance across diverse\ndatasets while preserving model performance, offering a practical solution for\ndeployable MLLM safety alignment.", "AI": {"tldr": "Magic Image是一个基于优化的视觉提示框架，通过优化图像提示使多模态大语言模型在不更新参数的情况下增强安全性、减少过度拒绝，并能适应不同价值体系。", "motivation": "大语言模型面临生成有害内容和过度拒绝良性查询的双重挑战，传统方法如SFT和RLHF因参数调优成本高且无法支持多价值体系而存在局限，多模态模型中的这些问题更加突出。", "method": "提出Magic Image框架，通过使用有害/良性样本优化图像提示，使单一模型能够适应不同价值体系并更好地与给定安全偏好对齐，无需参数更新。", "result": "实验证明该方法在多样化数据集上实现了更好的安全性与有效性平衡，同时保持了模型性能。", "conclusion": "Magic Image为可部署的多模态大语言模型安全对齐提供了一个实用的解决方案，解决了传统方法的局限性。"}}
{"id": "2511.00371", "pdf": "https://arxiv.org/pdf/2511.00371", "abs": "https://arxiv.org/abs/2511.00371", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs", "categories": ["cs.CL", "cs.CY", "cs.SE"], "comment": "25 pages, 2 tables, 13 figures", "summary": "In Socratic debugging, instructors guide students towards identifying and\nfixing a bug on their own, instead of providing the bug fix directly. Most\nnovice programmer bugs are caused by programming misconceptions, namely false\nbeliefs about a programming concept. In this context, Socratic debugging can be\nformulated as a guided Reasoning Trajectory (RT) leading to a statement about\nthe program behavior that contradicts the bug-causing misconception. Upon\nreaching this statement, the ensuing cognitive dissonance leads the student to\nfirst identify and then update their false belief. In this paper, we introduce\nthe task of reasoning trajectory generation, together with a dataset of\ndebugging problems manually annotated with RTs. We then describe LLM-based\nsolutions for generating RTs and Socratic conversations that are anchored on\nthem. A large-scale LLM-as-judge evaluation shows that frontier models can\ngenerate up to 91% correct reasoning trajectories and 98.7% valid conversation\nturns.", "AI": {"tldr": "该论文介绍了苏格拉底式调试中的推理轨迹生成任务，构建了带有人工标注推理轨迹的数据集，并展示了前沿大语言模型能够生成91%正确的推理轨迹和98.7%有效的对话轮次。", "motivation": "大多数新手程序员的错误源于编程误解，苏格拉底式调试通过引导推理轨迹帮助学生识别和修正错误信念，而非直接提供修复方案。", "method": "提出推理轨迹生成任务，构建人工标注的调试问题数据集，开发基于大语言模型的推理轨迹生成和对话系统解决方案。", "result": "前沿大语言模型能够生成高达91%正确的推理轨迹和98.7%有效的对话轮次，证明了方法的可行性。", "conclusion": "研究表明大语言模型在生成苏格拉底式调试所需的推理轨迹和对话方面表现优异，为编程教育中的自动调试辅助提供了有效途径。"}}
{"id": "2511.00547", "pdf": "https://arxiv.org/pdf/2511.00547", "abs": "https://arxiv.org/abs/2511.00547", "authors": ["Alain Riou"], "title": "Efficient Generation of Binary Magic Squares", "categories": ["cs.AI"], "comment": null, "summary": "We propose a simple algorithm for generating Binary Magic Squares (BMS),\ni.e., square binary matrices where the sum of all rows and all columns are\nequal. We show by induction that our algorithm always returns valid BMS with\noptimal theoretical complexity. We then extend our study to non-square Binary\nMagic Squares, formalize conditions on the sum of rows and columns for these\nBMS to exist, and show that a slight variant of our first algorithm can\ngenerate provably generate them. Finally, we publicly release two\nimplementations of our algorithm as Python packages, including one that can\ngenerate several BMS in parallel using GPU acceleration.", "AI": {"tldr": "提出了一种生成二进制幻方(BMS)的简单算法，通过归纳法证明其理论复杂度最优，并扩展到非方形BMS，公开发布了Python实现版本", "motivation": "研究二进制幻方的生成问题，需要找到行和列和相等的二进制矩阵的高效生成方法", "method": "使用归纳法设计简单算法，证明算法有效性，扩展到非方形情况，并实现Python包和GPU并行版本", "result": "算法能生成有效的二进制幻方，理论复杂度最优，能处理非方形情况，并提供了高效实现", "conclusion": "提出的算法简单有效，理论证明完备，实际应用性强，为二进制幻方生成提供了实用解决方案"}}
{"id": "2511.00416", "pdf": "https://arxiv.org/pdf/2511.00416", "abs": "https://arxiv.org/abs/2511.00416", "authors": ["Yiwei Zha", "Rui Min", "Shanu Sushmita"], "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.", "AI": {"tldr": "迭代改写的AI生成文本能有效逃避现有检测器，研究者开发了PADBen基准测试系统评估检测器对改写攻击的鲁棒性，发现现有方法无法有效处理中间改写区域。", "motivation": "现有AI生成文本检测器对直接LLM输出准确率超过90%，但对迭代改写的内容检测效果极差，需要研究其原因并评估检测器的鲁棒性。", "method": "通过内在机制分析揭示迭代改写的特征，开发PADBen基准测试，包含五种文本分类和五个渐进检测任务，评估11种最先进检测器。", "result": "检测器在抄袭逃避场景中表现成功，但在作者身份混淆场景中失败，显示现有方法无法有效处理中间改写区域。", "conclusion": "当前检测方法无法有效处理迭代改写产生的中间区域，需要在检测架构上进行根本性改进，超越现有的语义和风格判别方法。"}}
{"id": "2511.00551", "pdf": "https://arxiv.org/pdf/2511.00551", "abs": "https://arxiv.org/abs/2511.00551", "authors": ["Qiang Li", "Ningjing Zeng", "Lina Yu"], "title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Several studies have employed reinforcement learning (RL) to address the\nchallenges of regional adaptive traffic signal control (ATSC) and achieved\npromising results. In this field, existing research predominantly adopts\nmulti-agent frameworks. However, the adoption of multi-agent frameworks\npresents challenges for scalability. Instead, the Traffic signal control (TSC)\nproblem necessitates a single-agent framework. TSC inherently relies on\ncentralized management by a single control center, which can monitor traffic\nconditions across all roads in the study area and coordinate the control of all\nintersections. This work proposes a single-agent RL-based regional ATSC model\ncompatible with probe vehicle technology. Key components of the RL design\ninclude state, action, and reward function definitions. To facilitate learning\nand manage congestion, both state and reward functions are defined based on\nqueue length, with action designed to regulate queue dynamics. The queue length\ndefinition used in this study differs slightly from conventional definitions\nbut is closely correlated with congestion states. More importantly, it allows\nfor reliable estimation using link travel time data from probe vehicles. With\nprobe vehicle data already covering most urban roads, this feature enhances the\nproposed method's potential for widespread deployment. The method was\ncomprehensively evaluated using the SUMO simulation platform. Experimental\nresults demonstrate that the proposed model effectively mitigates large-scale\nregional congestion levels via coordinated multi-intersection control.", "AI": {"tldr": "本研究提出基于单智能体强化学习的区域自适应交通信号控制模型，利用探测车辆数据估计队列长度，通过SUMO仿真验证了该方法能有效缓解大规模区域拥堵。", "motivation": "现有研究多采用多智能体框架解决区域自适应交通信号控制问题，但存在可扩展性挑战。交通信号控制本质上需要集中管理，因此需要开发单智能体框架。", "method": "设计基于强化学习的单智能体控制模型，状态和奖励函数基于队列长度定义，动作设计用于调节队列动态。使用探测车辆的链路行程时间数据可靠估计队列长度。", "result": "实验结果表明，所提出的模型通过协调多交叉口控制，有效缓解了大尺度区域拥堵水平。", "conclusion": "该方法与探测车辆技术兼容，由于探测车辆数据已覆盖大部分城市道路，增强了该方法广泛部署的潜力，为实际应用提供了可行方案。"}}
{"id": "2511.00421", "pdf": "https://arxiv.org/pdf/2511.00421", "abs": "https://arxiv.org/abs/2511.00421", "authors": ["Naoto Iwase", "Hiroki Okuyama", "Junichiro Iwasawa"], "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) show increasing promise in medical applications,\nbut their ability to detect and correct errors in clinical texts -- a\nprerequisite for safe deployment -- remains under-evaluated, particularly\nbeyond English. We introduce MedRECT, a cross-lingual benchmark\n(Japanese/English) that formulates medical error handling as three subtasks:\nerror detection, error localization (sentence extraction), and error\ncorrection. MedRECT is built with a scalable, automated pipeline from the\nJapanese Medical Licensing Examinations (JMLE) and a curated English\ncounterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with\ncomparable error/no-error balance. We evaluate 9 contemporary LLMs spanning\nproprietary, open-weight, and reasoning families. Key findings: (i) reasoning\nmodels substantially outperform standard architectures, with up to 13.5%\nrelative improvement in error detection and 51.0% in sentence extraction; (ii)\ncross-lingual evaluation reveals 5-10% performance gaps from English to\nJapanese, with smaller disparities for reasoning models; (iii) targeted LoRA\nfine-tuning yields asymmetric improvements in error correction performance\n(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;\nand (iv) our fine-tuned model exceeds human expert performance on structured\nmedical error correction tasks. To our knowledge, MedRECT is the first\ncomprehensive cross-lingual benchmark for medical error correction, providing a\nreproducible framework and resources for developing safer medical LLMs across\nlanguages.", "AI": {"tldr": "MedRECT是一个跨语言（日语/英语）医学错误处理基准，包含错误检测、定位和纠正三个子任务，评估显示推理模型表现最佳，微调后模型在结构化医学错误纠正任务上超过人类专家性能", "motivation": "大型语言模型在医疗应用中有很大潜力，但其在临床文本中检测和纠正错误的能力（安全部署的前提条件）尚未得到充分评估，特别是在非英语语言中", "method": "构建MedRECT基准测试集（日语663文本，英语458文本），使用自动化流水线从日本医学执照考试中提取数据，评估9种当代LLM（包括专有、开源和推理模型），并进行针对性LoRA微调", "result": "推理模型显著优于标准架构（错误检测提升13.5%，句子提取提升51.0%）；跨语言评估显示英语到日语存在5-10%性能差距；微调后模型在错误纠正任务上超过人类专家表现", "conclusion": "MedRECT是首个全面的跨语言医学错误纠正基准，为开发更安全的跨语言医学LLM提供了可复现的框架和资源，推理模型和微调策略在医学错误处理中表现出色"}}
{"id": "2511.00609", "pdf": "https://arxiv.org/pdf/2511.00609", "abs": "https://arxiv.org/abs/2511.00609", "authors": ["Shengqi Xu", "Xinpeng Zhou", "Yabo Zhang", "Ming Liu", "Tao Liang", "Tianyu Zhang", "Yalong Bai", "Zuxuan Wu", "Wangmeng Zuo"], "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "categories": ["cs.AI"], "comment": null, "summary": "Personalized image preference assessment aims to evaluate an individual\nuser's image preferences by relying only on a small set of reference images as\nprior information. Existing methods mainly focus on general preference\nassessment, training models with large-scale data to tackle well-defined tasks\nsuch as text-image alignment. However, these approaches struggle to handle\npersonalized preference because user-specific data are scarce and not easily\nscalable, and individual tastes are often diverse and complex. To overcome\nthese challenges, we introduce a common preference profile that serves as a\nbridge across users, allowing large-scale user data to be leveraged for\ntraining profile prediction and capturing complex personalized preferences.\nBuilding on this idea, we propose a reasoning-based personalized image\npreference assessment framework that follows a \\textit{predict-then-assess}\nparadigm: it first predicts a user's preference profile from reference images,\nand then provides interpretable, multi-dimensional scores and assessments of\ncandidate images based on the predicted profile. To support this, we first\nconstruct a large-scale Chain-of-Thought (CoT)-style personalized assessment\ndataset annotated with diverse user preference profiles and high-quality\nCoT-style reasoning, enabling explicit supervision of structured reasoning.\nNext, we adopt a two-stage training strategy: a cold-start supervised\nfine-tuning phase to empower the model with structured reasoning capabilities,\nfollowed by reinforcement learning to incentivize the model to explore more\nreasonable assessment paths and enhance generalization. Furthermore, we propose\na similarity-aware prediction reward to encourage better prediction of the\nuser's preference profile, which facilitates more reasonable assessments\nexploration. Extensive experiments demonstrate the superiority of the proposed\nmethod.", "AI": {"tldr": "提出基于推理的个性化图像偏好评估框架，通过预测用户偏好配置文件并基于该配置进行多维度评分，使用两阶段训练策略（监督微调+强化学习）提升模型性能", "motivation": "现有方法主要关注通用偏好评估，难以处理个性化偏好问题，因为用户特定数据稀缺且个体品味复杂多样", "method": "构建大规模思维链式个性化评估数据集，采用预测-评估范式：先预测用户偏好配置文件，再进行多维度评分；使用两阶段训练（监督微调+强化学习）和相似性感知预测奖励机制", "result": "大量实验证明了所提方法的优越性", "conclusion": "通过构建通用偏好配置文件作为用户间的桥梁，成功解决了个性化图像偏好评估的挑战，提供了可解释的多维度评估"}}
{"id": "2511.00432", "pdf": "https://arxiv.org/pdf/2511.00432", "abs": "https://arxiv.org/abs/2511.00432", "authors": ["Zhiwen Ruan", "Yixia Li", "Yefeng Liu", "Yun Chen", "Weihua Luo", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "G2: Guided Generation for Enhanced Output Diversity in LLMs", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, these models exhibit a\ncritical limitation in output diversity, often generating highly similar\ncontent across multiple attempts. This limitation significantly affects tasks\nrequiring diverse outputs, from creative writing to reasoning. Existing\nsolutions, like temperature scaling, enhance diversity by modifying probability\ndistributions but compromise output quality. We propose Guide-to-Generation\n(G2), a training-free plug-and-play method that enhances output diversity while\npreserving generation quality. G2 employs a base generator alongside dual\nGuides, which guide the generation process through decoding-based interventions\nto encourage more diverse outputs conditioned on the original query.\nComprehensive experiments demonstrate that G2 effectively improves output\ndiversity while maintaining an optimal balance between diversity and quality.", "AI": {"tldr": "提出了Guide-to-Generation (G2)方法，一种无需训练即插即用的技术，通过双引导机制增强大语言模型输出多样性，同时保持生成质量", "motivation": "大语言模型在输出多样性方面存在严重限制，经常生成高度相似的内容，这影响了从创意写作到推理等需要多样化输出的任务", "method": "使用基础生成器配合双引导机制，通过基于解码的干预来引导生成过程，在原始查询条件下鼓励更多样化的输出", "result": "综合实验表明G2有效提高了输出多样性，同时在多样性和质量之间保持了最佳平衡", "conclusion": "G2方法成功解决了LLM输出多样性不足的问题，提供了一种训练免费的解决方案，在不牺牲质量的前提下显著提升多样性"}}
{"id": "2511.00640", "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning.", "AI": {"tldr": "DTS是一种模型无关的解码框架，通过在高熵token处选择性分支和早期停止来找到最短推理路径，提高大型推理模型的效率和准确性。", "motivation": "大型推理模型在复杂推理任务中经常产生过长的思维链，导致推理成本增加和准确性下降，研究发现推理长度与准确性呈负相关。", "method": "提出DTS框架：通过选择性分支探索高熵token，应用早期停止策略选择最短完成的推理路径，无需额外训练或监督。", "result": "在AIME2024和AIME2025数据集上，DTS将准确性提升高达8%，平均推理长度减少23%，重复频率降低12%。", "conclusion": "DTS能够有效近似最优解，实现可扩展和高效的大型推理模型推理，同时提升准确性和效率。"}}
{"id": "2511.00476", "pdf": "https://arxiv.org/pdf/2511.00476", "abs": "https://arxiv.org/abs/2511.00476", "authors": ["Ghazal Kalhor", "Afra Mashhadi"], "title": "Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks", "categories": ["cs.CL"], "comment": null, "summary": "Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search\nand recommendation platforms at their core. While this shift unlocks powerful\nnew scientometric tools, it also exposes critical fairness and bias issues that\ncould erode the integrity of the information ecosystem. Additionally, as LLMs\nbecome more integrated into web-based searches for scholarly tools, their\nability to generate summarized research work based on memorized data introduces\nnew dimensions to these challenges. The extent of memorization in LLMs can\nimpact the accuracy and fairness of the co-authorship networks they produce,\npotentially reflecting and amplifying existing biases within the scientific\ncommunity and across different regions. This study critically examines the\nimpact of LLM memorization on the co-authorship networks. To this end, we\nassess memorization effects across three prominent models, DeepSeek R1, Llama 4\nScout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across\nacademic disciplines and world regions. While our global analysis reveals a\nconsistent bias favoring highly cited researchers, this pattern is not\nuniformly observed. Certain disciplines, such as Clinical Medicine, and\nregions, including parts of Africa, show more balanced representation, pointing\nto areas where LLM training data may reflect greater equity. These findings\nunderscore both the risks and opportunities in deploying LLMs for scholarly\ndiscovery.", "AI": {"tldr": "本研究分析了大型语言模型（LLMs）在科学计量学应用中的记忆化效应对合著网络的公平性和偏见影响，发现虽然存在偏向高被引研究者的普遍偏见，但在某些学科和地区表现出更均衡的代表性。", "motivation": "随着LLMs在学术搜索和推荐平台中的深度整合，其基于记忆数据生成研究摘要的能力引发了新的公平性和偏见问题，可能影响合著网络的准确性和公正性。", "method": "评估了DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三个主流模型的记忆化效应，分析记忆驱动输出在不同学科和世界地区的差异。", "result": "全球分析显示存在偏向高被引研究者的一致偏见，但临床医学等学科和非洲等地区表现出更均衡的代表性，表明这些领域的LLM训练数据可能更具公平性。", "conclusion": "研究强调了在学术发现中部署LLMs既存在风险也带来机遇，需要关注记忆化效应对信息生态系统完整性的影响。"}}
{"id": "2511.00651", "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains.", "AI": {"tldr": "提出基于多代理系统和大型语言模型的自动化网络故障排除框架，通过协调多个专业工具来加速电信网络故障诊断和修复。", "motivation": "电信网络规模扩大和复杂度增加，现有AI模型范围狭窄、需要大量标注数据且难以在异构部署中泛化，目前主要依赖专家手动分析故障。", "method": "采用多代理系统工作流，利用LLM协调编排器、解决方案规划器、执行器、数据检索器和根因分析器等代理。关键组件是解决方案规划器，通过在专有故障排除文档上微调小型语言模型来生成领域相关的修复方案。", "result": "实验结果表明，该框架在无线接入网和核心网领域显著加速了故障排除自动化过程。", "conclusion": "多代理系统结合LLM和领域特定的SLM能够有效解决电信网络故障排除的自动化挑战，减少对专家人工干预的依赖。"}}
{"id": "2511.00486", "pdf": "https://arxiv.org/pdf/2511.00486", "abs": "https://arxiv.org/abs/2511.00486", "authors": ["Pooja Singh", "Shashwat Bhardwaj", "Vaibhav Sharma", "Sandeep Kumar"], "title": "Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus", "categories": ["cs.CL"], "comment": "Accepted in EMNLP 2025", "summary": "The linguistic diversity of India poses significant machine translation\nchallenges, especially for underrepresented tribal languages like Bhili, which\nlack high-quality linguistic resources. This paper addresses the gap by\nintroducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest\nparallel corpus worldwide comprising 110,000 meticulously curated sentences\nacross Bhili, Hindi, and English. The corpus was created with the assistance of\nexpert human translators. BHEPC spans critical domains such as education,\nadministration, and news, establishing a valuable benchmark for research in low\nresource machine translation. To establish a comprehensive Bhili Machine\nTranslation benchmark, we evaluated a wide range of proprietary and open-source\nMultilingual Large Language Models (MLLMs) on bidirectional translation tasks\nbetween English/Hindi and Bhili. Comprehensive evaluation demonstrates that the\nfine-tuned NLLB-200 distilled 600M variant model outperforms others,\nhighlighting the potential of multilingual models in low resource scenarios.\nFurthermore, we investigated the generative translation capabilities of\nmultilingual LLMs on BHEPC using in-context learning, assessing performance\nunder cross-domain generalization and quantifying distributional divergence.\nThis work bridges a critical resource gap and promotes inclusive natural\nlanguage processing technologies for low-resource and marginalized languages\nglobally.", "AI": {"tldr": "该论文构建了首个大型Bhili-印地语-英语平行语料库BHEPC（11万句），并评估了多种多语言大模型在Bhili机器翻译任务上的表现，发现微调的NLLB-200模型效果最佳，为低资源语言机器翻译提供了重要基准。", "motivation": "印度语言多样性带来机器翻译挑战，特别是Bhili等代表性不足的部落语言缺乏高质量语言资源，需要填补这一空白。", "method": "创建Bhili-Hindi-English平行语料库(BHEPC)，包含11万句经专业翻译人员精心策划的句子；评估多种专有和开源多语言大模型在双向翻译任务上的性能；研究多语言LLM的上下文学习生成翻译能力。", "result": "微调的NLLB-200 distilled 600M变体模型在所有评估模型中表现最佳；证明了多语言模型在低资源场景中的潜力；建立了跨领域泛化和分布差异的量化评估。", "conclusion": "这项工作填补了关键资源空白，促进了全球低资源和边缘化语言的包容性自然语言处理技术发展，为低资源机器翻译研究提供了宝贵基准。"}}
{"id": "2511.00673", "pdf": "https://arxiv.org/pdf/2511.00673", "abs": "https://arxiv.org/abs/2511.00673", "authors": ["Dominik Drexler"], "title": "Lifted Successor Generation in Numeric Planning", "categories": ["cs.AI"], "comment": null, "summary": "Most planners ground numeric planning tasks, given in a first-order-like\nlanguage, into a ground task representation. However, this can lead to an\nexponential blowup in task representation size, which occurs in practice for\nhard-to-ground tasks. We extend a state-of-the-art lifted successor generator\nfor classical planning to support numeric precondition applicability. The\nmethod enumerates maximum cliques in a substitution consistency graph. Each\nmaximum clique represents a substitution for the variables of the action\nschema, yielding a ground action. We augment this graph with numeric action\npreconditions and prove the successor generator is exact under formally\nspecified conditions. When the conditions fail, our generator may list\ninapplicable ground actions; a final applicability check filters these without\naffecting completeness. However, this cannot happen in 23 of 25 benchmark\ndomains, and it occurs only in 1 domain. To the authors' knowledge, no other\nlifted successor generator supports numeric action preconditions. This enables\nfuture research on lifted planning for a very rich planning fragment.", "AI": {"tldr": "本文提出了一种支持数值前置条件的提升后继生成器，通过扩展经典的提升后继生成器来处理数值规划任务，避免了传统方法中可能出现的指数级任务表示膨胀问题。", "motivation": "传统数值规划任务需要将一阶语言描述的任务进行实例化，这可能导致任务表示规模的指数级膨胀，特别是在难以实例化的任务中。", "method": "扩展了最先进的经典规划提升后继生成器，支持数值前置条件的适用性检查。方法通过在替换一致性图中枚举最大团来实现，每个最大团代表动作模式变量的一个替换，生成一个具体动作。", "result": "在25个基准域中的23个中，生成器不会列出不适用动作；仅在1个域中出现这种情况，且通过最终适用性检查可以过滤这些动作而不影响完整性。", "conclusion": "这是第一个支持数值动作前置条件的提升后继生成器，为未来在丰富规划片段上的提升规划研究奠定了基础。"}}
{"id": "2511.00487", "pdf": "https://arxiv.org/pdf/2511.00487", "abs": "https://arxiv.org/abs/2511.00487", "authors": ["Stephen Meisenbacher", "Florian Matthes"], "title": "With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting", "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 5 tables. Accepted to IJCNLP-AACL 2025 (Main)", "summary": "Recent work in Differential Privacy with Natural Language Processing (DP NLP)\nhas proposed numerous promising techniques in the form of text rewriting\nmechanisms. In the evaluation of these mechanisms, an often-ignored aspect is\nthat of dataset size, or rather, the effect of dataset size on a mechanism's\nefficacy for utility and privacy preservation. In this work, we are the first\nto introduce this factor in the evaluation of DP text privatization, where we\ndesign utility and privacy tests on large-scale datasets with dynamic split\nsizes. We run these tests on datasets of varying size with up to one million\ntexts, and we focus on quantifying the effect of increasing dataset size on the\nprivacy-utility trade-off. Our findings reveal that dataset size plays an\nintegral part in evaluating DP text rewriting mechanisms; additionally, these\nfindings call for more rigorous evaluation procedures in DP NLP, as well as\nshed light on the future of DP NLP in practice and at scale.", "AI": {"tldr": "该论文首次在差分隐私文本重写机制评估中引入数据集规模因素，通过大规模动态数据集测试揭示了数据集规模对隐私-效用权衡的重要影响。", "motivation": "现有差分隐私自然语言处理研究在评估文本重写机制时忽略了数据集规模对机制效用和隐私保护效果的影响，需要系统研究这一关键因素。", "method": "设计在大规模数据集上进行效用和隐私测试的方法，使用动态分割规模，在包含最多100万文本的不同规模数据集上运行测试，量化数据集规模增加对隐私-效用权衡的影响。", "result": "研究发现数据集规模在评估差分隐私文本重写机制中起着关键作用，规模变化显著影响隐私保护效果和实用性的平衡。", "conclusion": "研究结果呼吁差分隐私自然语言处理领域需要更严格的评估程序，并为该技术在实际应用和大规模部署中的未来发展提供了重要见解。"}}
{"id": "2511.00710", "pdf": "https://arxiv.org/pdf/2511.00710", "abs": "https://arxiv.org/abs/2511.00710", "authors": ["Minghe Shen", "Zhuo Zhi", "Chonghan Liu", "Shuo Xing", "Zhengzhong Tu", "Che Liu"], "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "categories": ["cs.AI"], "comment": null, "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.", "AI": {"tldr": "RL后训练可以扩展基础视觉语言模型的能力边界，特别是在视觉空间推理任务中。通过Ariadne框架使用合成迷宫进行强化学习训练，模型在原本得分为0%的任务上达到50%以上准确率，并在实际基准测试中显示出显著的零样本泛化能力。", "motivation": "当前视觉语言模型的评估主要局限于语言主导任务，研究需要探索RL后训练是否能够真正扩展基础模型在视觉中心空间任务中的能力边界。", "method": "提出Ariadne框架，利用合成迷宫进行多步空间推理，通过难度感知课程使用验证奖励的强化学习(RLVR)来训练VLMs。", "result": "后训练后，模型在基础模型得分为0%的问题集上达到50%以上准确率；在MapBench上平均提升16%，在ReasonMap上平均提升24%的零样本泛化性能。", "conclusion": "该方法不仅扩展了模型的基本能力限制，还增强了其在现实世界空间推理中的泛化能力，但研究仅限于后训练阶段，期望推动专门的能力扩展对齐研究。"}}
{"id": "2511.00489", "pdf": "https://arxiv.org/pdf/2511.00489", "abs": "https://arxiv.org/abs/2511.00489", "authors": ["Jiani Guo", "Zuchao Li", "Jie Wu", "Qianren Wang", "Yun Li", "Lefei Zhang", "Hai Zhao", "Yujiu Yang"], "title": "ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLMs), constrained by limited context windows, often\nface significant performance degradation when reasoning over long contexts. To\naddress this, Retrieval-Augmented Generation (RAG) retrieves and reasons over\nchunks but frequently sacrifices logical coherence due to its reliance on\nsimilarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split\ndocuments into small chunks for independent reasoning and aggregation. While\neffective for local reasoning, DCF struggles to capture long-range dependencies\nand risks inducing conflicts by processing chunks in isolation. To overcome\nthese limitations, we propose ToM, a novel Tree-oriented MapReduce framework\nfor long-context reasoning. ToM leverages the inherent hierarchical structure\nof long documents (e.g., main headings and subheadings) by constructing a\nDocTree through hierarchical semantic parsing and performing bottom-up\naggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:\nin the Map step, rationales are generated at child nodes; in the Reduce step,\nthese rationales are aggregated across sibling nodes to resolve conflicts or\nreach consensus at parent nodes. Experimental results on 70B+ LLMs show that\nToM significantly outperforms existing divide-and-conquer frameworks and\nretrieval-augmented generation methods, achieving better logical coherence and\nlong-context reasoning. Our code is available at\nhttps://github.com/gjn12-31/ToM .", "AI": {"tldr": "ToM是一种新颖的树形MapReduce框架，通过层次化语义解析构建文档树并进行自底向上聚合，解决了大语言模型在长上下文推理中的性能下降问题，显著优于现有的分治框架和检索增强生成方法。", "motivation": "大语言模型受限于有限的上下文窗口，在处理长上下文推理时性能显著下降。现有的检索增强生成(RAG)方法依赖相似性排序牺牲了逻辑连贯性，而分治框架(DCF)虽然适用于局部推理，但难以捕获长距离依赖关系且容易产生冲突。", "method": "提出ToM框架：1)通过层次化语义解析构建文档树(DocTree)；2)采用树形MapReduce方法进行递归推理：Map步骤在子节点生成推理依据，Reduce步骤在父节点聚合兄弟节点的推理依据以解决冲突或达成共识；3)实现自底向上的信息聚合。", "result": "在70B+参数的大语言模型上实验表明，ToM显著优于现有的分治框架和检索增强生成方法，在逻辑连贯性和长上下文推理方面表现更好。", "conclusion": "ToM框架通过利用长文档的固有层次结构，采用树形MapReduce方法有效解决了长上下文推理问题，为处理长文档提供了新的有效解决方案。"}}
{"id": "2511.00739", "pdf": "https://arxiv.org/pdf/2511.00739", "abs": "https://arxiv.org/abs/2511.00739", "authors": ["Ritik Raj", "Hong Wang", "Tushar Krishna"], "title": "A CPU-Centric Perspective on Agentic AI", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Agentic AI frameworks add a decision-making orchestrator embedded with\nexternal tools, including web search, Python interpreter, contextual database,\nand others, on top of monolithic LLMs, turning them from passive text oracles\ninto autonomous problem-solvers that can plan, call tools, remember past steps,\nand adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks\nintroduced by agentic AI workloads from a largely overlooked CPU-centric\nperspective. We first systematically characterize Agentic AI on the basis of\norchestrator/decision making component, inference path dynamics and\nrepetitiveness of the agentic flow which directly influences the system-level\nperformance. Thereafter, based on the characterization, we choose five\nrepresentative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,\nLangchain and SWE-Agent to profile latency, throughput and energy metrics and\ndemystify the significant impact of CPUs on these metrics relative to GPUs. We\nobserve that - 1. Tool processing on CPUs can take up to 90.6% of the total\nlatency; 2. Agentic throughput gets bottlenecked either by CPU factors -\ncoherence, synchronization and over-subscription of cores or GPU factors - main\nmemory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to\n44% of the total dynamic energy at large batch sizes. Based on the profiling\ninsights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching\n(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and\nheterogeneous agentic workloads respectively to demonstrate the potential to\nimprove the performance, efficiency, and scalability of agentic AI. We achieve\nup to 2.1x and 1.41x P50 latency speedup compared to the multi-processing\nbenchmark for homogeneous and heterogeneous agentic workloads respectively.", "AI": {"tldr": "该论文从CPU角度分析智能体AI框架的系统瓶颈，发现CPU工具处理占90.6%延迟，提出CGAM和MAWS优化方案，实现最高2.1倍延迟加速", "motivation": "从被忽视的CPU中心视角来理解和表征智能体AI工作负载引入的系统瓶颈", "method": "系统表征智能体AI的编排组件、推理路径动态和重复性，选择5个代表性工作负载分析延迟、吞吐量和能耗指标，提出CPU和GPU感知的微批处理和混合工作负载调度优化", "result": "CPU工具处理占90.6%总延迟，CPU动态能耗占44%，提出的优化方案在同构和异构工作负载上分别实现2.1倍和1.41倍的P50延迟加速", "conclusion": "CPU在智能体AI系统中扮演关键角色，提出的优化方案能显著提升性能、效率和可扩展性"}}
{"id": "2511.00505", "pdf": "https://arxiv.org/pdf/2511.00505", "abs": "https://arxiv.org/abs/2511.00505", "authors": ["Qi Luo", "Xiaonan Li", "Junqi Dai", "Shuang Cheng", "Xipeng Qiu"], "title": "Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation has shown remarkable results to address Large\nLanguage Models' hallucinations, which usually uses a large external corpus to\nsupplement knowledge to LLMs. However, with the development of LLMs, the\ninternal knowledge of LLMs has expanded significantly, thus causing significant\nknowledge redundancy between the external corpus and LLMs. On the one hand, the\nindexing cost of dense retrieval is highly related to the corpus size and thus\nsignificant redundant knowledge intensifies the dense retrieval's workload. On\nthe other hand, the redundant knowledge in the external corpus is not helpful\nto LLMs and our exploratory analysis shows that it instead hurts the RAG\nperformance on those questions which the LLM can answer by itself. To address\nthese issues, we propose Zero-RAG to tackle these challenges. Specifically, we\nfirst propose the Mastery-Score metric to identify redundant knowledge in the\nRAG corpus to prune it. After pruning, answers to \"mastered\" questions rely\nprimarily on internal knowledge of the LLM. To better harness the internal\ncapacity, we propose Query Router and Noise-Tolerant Tuning to avoid the\nirrelevant documents' distraction and thus further improve the LLM's\nutilization of internal knowledge with pruned corpus. Experimental results show\nthat Zero-RAG prunes the Wikipedia corpus by 30\\% and accelerates the retrieval\nstage by 22\\%, without compromising RAG's performance.", "AI": {"tldr": "Zero-RAG通过识别和修剪检索增强生成中的冗余知识，减少外部语料库规模，加速检索过程，同时通过查询路由和噪声容忍调优提升大语言模型内部知识利用率。", "motivation": "随着大语言模型内部知识显著扩展，外部语料库与模型之间存在大量知识冗余，导致密集检索成本增加且可能损害RAG性能。", "method": "提出Mastery-Score指标识别冗余知识进行修剪；设计Query Router避免无关文档干扰；采用Noise-Tolerant Tuning提升模型对修剪后语料库的内部知识利用能力。", "result": "实验显示Zero-RAG将维基百科语料库修剪30%，检索阶段加速22%，且不损害RAG性能。", "conclusion": "Zero-RAG有效解决了RAG中的知识冗余问题，在保持性能的同时显著提升了检索效率和模型内部知识利用率。"}}
{"id": "2511.00751", "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost.", "AI": {"tldr": "本研究探讨了在现代大语言模型中增加采样推理路径对自一致性方法的权衡。研究发现，随着采样路径增加，性能提升会达到平台期，高采样配置的计算成本效益很低。", "motivation": "重新验证早期研究中关于多推理链组合提升模型性能的观点，在现代LLM条件下检验采样路径数量与性能提升的关系。", "method": "使用Gemini 2.5模型在HotpotQA和Math-500数据集上进行实验，比较不同采样推理路径配置与单一路径CoT基线的性能差异。", "result": "大型模型表现出更稳定一致的性能提升曲线，中等采样后性能增益趋于平缓，与早期研究结果一致。推理路径间的重叠导致了收益递减。", "conclusion": "自一致性方法仍然有效，但高采样配置相对于计算成本而言收益甚微，建议使用中等采样规模以获得最佳性价比。"}}
{"id": "2511.00514", "pdf": "https://arxiv.org/pdf/2511.00514", "abs": "https://arxiv.org/abs/2511.00514", "authors": ["Birat Poudel", "Satyam Ghimire", "Er. Prakash Chandra Prasad"], "title": "Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations", "categories": ["cs.CL"], "comment": "6 pages, 6 figures, 3 tables", "summary": "Conversational agents are increasingly being explored to support healthcare\ndelivery, particularly in resource-constrained settings such as rural Nepal.\nLarge-scale conversational models typically rely on internet connectivity and\ncloud infrastructure, which may not be accessible in rural areas. In this\nstudy, we fine-tuned DialoGPT, a lightweight generative dialogue model that can\noperate offline, on a synthetically constructed dataset of doctor-patient\ninteractions covering ten common diseases prevalent in rural Nepal, including\ncommon cold, seasonal fever, diarrhea, typhoid fever, gastritis, food\npoisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being\ntrained on a limited, domain-specific dataset, the fine-tuned model produced\ncoherent, contextually relevant, and medically appropriate responses,\ndemonstrating an understanding of symptoms, disease context, and empathetic\ncommunication. These results highlight the adaptability of compact,\noffline-capable dialogue models and the effectiveness of targeted datasets for\ndomain adaptation in low-resource healthcare environments, offering promising\ndirections for future rural medical conversational AI.", "AI": {"tldr": "在尼泊尔农村医疗资源匮乏环境下，研究通过微调轻量级DialoGPT模型，使用合成的医患对话数据集训练，成功开发出可离线运行的医疗对话AI，能够生成连贯、相关且医学适当的回应。", "motivation": "解决农村地区因网络和云基础设施不足而无法使用大规模对话模型的问题，为资源受限的医疗环境提供可行的对话AI解决方案。", "method": "使用合成构建的尼泊尔农村常见疾病医患对话数据集，对轻量级生成对话模型DialoGPT进行微调，覆盖10种常见疾病。", "result": "微调后的模型能够产生连贯、上下文相关且医学适当的回应，表现出对症状、疾病背景和同理心沟通的理解能力。", "conclusion": "紧凑型离线对话模型在低资源医疗环境中具有良好适应性，针对性数据集在领域适应方面效果显著，为农村医疗对话AI未来发展提供了有希望的方向。"}}
{"id": "2511.00758", "pdf": "https://arxiv.org/pdf/2511.00758", "abs": "https://arxiv.org/abs/2511.00758", "authors": ["Hong Su"], "title": "Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Real-world artificial intelligence (AI) systems are increasingly required to\noperate autonomously in dynamic, uncertain, and continuously changing\nenvironments. However, most existing AI models rely on predefined objectives,\nstatic training data, and externally supplied feedback, which restrict their\nability to adapt, reflect, and improve independently. In this paper, we propose\nthe Active Thinking Model (ATM)- a unified cognitive framework that integrates\ngoal reasoning, dynamic task generation, and self-reflective learning into an\nadaptive architecture. Unlike conventional systems that passively execute fixed\nprocedures, ATM actively evaluates its performance through logical reasoning\nand environmental indicators, reuses effective methods to solve new problems,\nand generates novel strategies for unseen situations via a continuous\nself-improvement loop. A mathematically grounded theoretical analysis\ndemonstrates that ATM can autonomously evolve from suboptimal to optimal\nbehavior without external supervision and maintain bounded tracking regret\nunder changing environmental conditions.", "AI": {"tldr": "提出主动思维模型(ATM)——一个整合目标推理、动态任务生成和自我反思学习的统一认知框架，能够在动态不确定环境中自主适应和改进", "motivation": "现有AI系统依赖预定义目标、静态训练数据和外部反馈，限制了在动态变化环境中的自主适应和独立改进能力", "method": "ATM框架通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，通过持续自我改进循环为未见情况生成新策略", "result": "理论分析表明ATM能够无需外部监督从次优行为自主演进到最优行为，并在变化环境条件下保持有界跟踪遗憾", "conclusion": "ATM为构建在动态不确定环境中具有自主适应和自我改进能力的AI系统提供了统一的认知框架和理论基础"}}
{"id": "2511.00519", "pdf": "https://arxiv.org/pdf/2511.00519", "abs": "https://arxiv.org/abs/2511.00519", "authors": ["Ariyan Hossain", "Khondokar Mohammad Ahanaf Hannan", "Rakinul Haque", "Nowreen Tarannum Rafa", "Humayra Musarrat", "Shoaib Ahmed Dipu", "Farig Yousuf Sadeque"], "title": "Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models", "categories": ["cs.CL", "I.2.7; I.7.1; K.4.1"], "comment": "25 pages, 20 figures", "summary": "Gender bias in language models has gained increasing attention in the field\nof natural language processing. Encoder-based transformer models, which have\nachieved state-of-the-art performance in various language tasks, have been\nshown to exhibit strong gender biases inherited from their training data. This\npaper investigates gender bias in contextualized word embeddings, a crucial\ncomponent of transformer-based models. We focus on prominent architectures such\nas BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to\ngender bias. To quantify the degree of bias, we introduce a novel metric,\nMALoR, which assesses bias based on model probabilities for filling masked\ntokens. We further propose a mitigation approach involving continued\npre-training on a gender-balanced dataset generated via Counterfactual Data\nAugmentation. Our experiments reveal significant reductions in gender bias\nscores across different pronoun pairs. For instance, in BERT-base, bias scores\nfor \"he-she\" dropped from 1.27 to 0.08, and \"his-her\" from 2.51 to 0.36\nfollowing our mitigation approach. We also observed similar improvements across\nother models, with \"male-female\" bias decreasing from 1.82 to 0.10 in\nBERT-large. Our approach effectively reduces gender bias without compromising\nmodel performance on downstream tasks.", "AI": {"tldr": "本文研究Transformer模型中的性别偏见问题，提出了新的评估指标MALoR和基于反事实数据增强的缓解方法，在BERT等模型中显著降低了性别偏见且不影响下游任务性能。", "motivation": "编码器Transformer模型在各种语言任务中表现出色，但其训练数据中存在性别偏见，需要研究这些偏见在上下文词嵌入中的表现并找到有效的缓解方法。", "method": "引入MALoR指标基于掩码填充概率评估偏见，提出通过反事实数据增强生成性别平衡数据集进行持续预训练的缓解方法。", "result": "实验显示偏见分数显著降低：BERT-base中he-she从1.27降至0.08，his-her从2.51降至0.36；BERT-large中male-female从1.82降至0.10。", "conclusion": "该方法能有效减少性别偏见而不损害下游任务性能，为缓解语言模型中的偏见提供了可行方案。"}}
{"id": "2511.00763", "pdf": "https://arxiv.org/pdf/2511.00763", "abs": "https://arxiv.org/abs/2511.00763", "authors": ["Wanda Hou", "Leon Zhou", "Hong-Ye Hu", "Yi-Zhuang You", "Xiao-Liang Qi"], "title": "How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks", "categories": ["cs.AI"], "comment": null, "summary": "We investigate the performance of large language models on repetitive\ndeterministic prediction tasks and study how the sequence accuracy rate scales\nwith output length. Each such task involves repeating the same operation n\ntimes. Examples include letter replacement in strings following a given rule,\ninteger addition, and multiplication of string operators in many body quantum\nmechanics. If the model performs the task through a simple repetition\nalgorithm, the success rate should decay exponentially with sequence length. In\ncontrast, our experiments on leading large language models reveal a sharp\ndouble exponential drop beyond a characteristic length scale, forming an\naccuracy cliff that marks the transition from reliable to unstable generation.\nThis indicates that the models fail to execute each operation independently. To\nexplain this phenomenon, we propose a statistical physics inspired model that\ncaptures the competition between external conditioning from the prompt and\ninternal interference among generated tokens. The model quantitatively\nreproduces the observed crossover and provides an interpretable link between\nattention induced interference and sequence level failure. Fitting the model to\nempirical results across multiple models and tasks yields effective parameters\nthat characterize the intrinsic error rate and error accumulation factor for\neach model task pair, offering a principled framework for understanding the\nlimits of deterministic accuracy in large language models.", "AI": {"tldr": "研究发现大型语言模型在处理重复确定性预测任务时，准确率随输出长度呈现双指数下降的悬崖式衰减，而非简单的指数衰减，表明模型无法独立执行每个操作。", "motivation": "研究大型语言模型在重复确定性任务（如字符串转换、数学运算）中的性能表现，探究准确率随序列长度变化的规律。", "method": "通过实验测试主流大型语言模型在多种重复任务上的表现，并提出基于统计物理学的理论模型来解释观察到的现象，该模型考虑了提示的外部条件与生成token间内部干扰的竞争关系。", "result": "实验发现模型准确率在超过特征长度后出现急剧的双指数下降（准确率悬崖），理论模型能定量重现这一交叉现象，并提供了注意力机制引起的干扰与序列级失败之间的可解释联系。", "conclusion": "研究提供了一个原则性框架来理解大型语言模型确定性准确性的极限，通过拟合模型可获得每个模型-任务对的内在错误率和错误累积因子等有效参数。"}}
{"id": "2511.00536", "pdf": "https://arxiv.org/pdf/2511.00536", "abs": "https://arxiv.org/abs/2511.00536", "authors": ["Wenya Xie", "Shaochen", "Zhong", "Hoang Anh Duy Le", "Zhaozhuo Xu", "Jianwen Xie", "Zirui Liu"], "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) are often bottlenecked by the high cost of\noutput tokens. We show that a significant portion of these tokens are useless\nself-repetitions - what we call \"word salad\" - that exhaust the decoding budget\nwithout adding value. Interestingly, we observe that LRMs are self-aware when\ntrapped in these loops: the hidden states of <\\n\\n> tokens trailing each\nreasoning chunk exhibit patterns that allow us to detect word salad behavior\non-the-fly via a single-layer linear classifier. Once detected, a simple chop\nappended by a straightforward regeneration prompt yields substantial length\nsavings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a\nlightweight, turnkey component for LRM that is minimally invasive to its\nreasoning trajectory by only removing semantically redundant tokens. Given its\nlow overhead, strong savings, and the lack of semantic value of word salad\ntokens, we believe it is not too far-fetched to argue that WSC - or a similar\ncomponent - is a must-have for all LRM applications with user experience in\nmind. Our code is publicly available at\nhttps://github.com/wenyaxie023/WordSaladChopper.", "AI": {"tldr": "论文提出WordSaladChopper(WSC)方法，通过检测大型推理模型中的无意义自我重复token（称为\"word salad\"），在推理过程中实时识别并去除这些冗余内容，显著减少输出长度同时保持质量。", "motivation": "大型推理模型(LRMs)输出token成本高昂，其中很大一部分是无用的自我重复内容，这些\"word salad\"消耗了解码预算却不增加价值。", "method": "利用<\n\n>标记后的隐藏状态模式，通过单层线性分类器实时检测word salad行为，检测到后通过简单截断和重新生成提示来处理。", "result": "WSC方法能够大幅节省输出长度，同时质量损失最小，是一个轻量级、即插即用的组件。", "conclusion": "WSC或类似组件对于所有注重用户体验的LRM应用来说是必备的，因为word salad token缺乏语义价值且该方法开销低、节省效果显著。"}}
{"id": "2511.00782", "pdf": "https://arxiv.org/pdf/2511.00782", "abs": "https://arxiv.org/abs/2511.00782", "authors": ["Jifan Gao", "Michael Rosenthal", "Brian Wolpin", "Simona Cristea"], "title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "categories": ["cs.AI"], "comment": null, "summary": "Structured electronic health records (EHR) are essential for clinical\nprediction. While count-based learners continue to perform strongly on such\ndata, no benchmarking has directly compared them against more recent\nmixture-of-agents LLM pipelines, which have been reported to outperform single\nLLMs in various NLP tasks. In this study, we evaluated three categories of\nmethodologies for EHR prediction using the EHRSHOT dataset: count-based models\nbuilt from ontology roll-ups with two time bins, based on LightGBM and the\ntabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);\nand a mixture-of-agents pipeline that converts tabular histories to\nnatural-language summaries followed by a text classifier. We assessed eight\noutcomes using the EHRSHOT dataset. Across the eight evaluation tasks,\nhead-to-head wins were largely split between the count-based and the\nmixture-of-agents methods. Given their simplicity and interpretability,\ncount-based models remain a strong candidate for structured EHR benchmarking.\nThe source code is available at:\nhttps://github.com/cristea-lab/Structured_EHR_Benchmark.", "AI": {"tldr": "本研究比较了三种EHR预测方法：基于计数的模型、预训练序列变换器和多智能体混合管道，发现在EHRSHOT数据集上，基于计数的方法与多智能体方法表现相当，但基于计数的方法更简单且可解释性强", "motivation": "虽然基于计数的学习器在结构化EHR数据上表现良好，但尚未与新兴的多智能体LLM管道进行直接基准比较，后者在其他NLP任务中已显示出优于单一LLM的性能", "method": "使用EHRSHOT数据集评估三种方法：1)基于LightGBM和TabPFN的计数模型；2)预训练序列变换器CLMBR；3)将表格历史转换为自然语言摘要后使用文本分类器的多智能体管道", "result": "在八个评估任务中，基于计数的方法和多智能体方法的表现基本相当，胜负结果分布均匀", "conclusion": "基于计数模型因其简单性和可解释性，仍然是结构化EHR基准测试的有力候选方法"}}
{"id": "2511.00537", "pdf": "https://arxiv.org/pdf/2511.00537", "abs": "https://arxiv.org/abs/2511.00537", "authors": ["Peter Atandoh", "Jie Zou", "Weikang Guo", "Jiwei Wei", "Zheng Wang"], "title": "Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sentiment analysis using deep learning and pre-trained language models (PLMs)\nhas gained significant traction due to their ability to capture rich contextual\nrepresentations. However, existing approaches often underperform in scenarios\ninvolving nuanced emotional cues, domain shifts, and imbalanced sentiment\ndistributions. We argue that these limitations stem from inadequate semantic\ngrounding, poor generalization to diverse linguistic patterns, and biases\ntoward dominant sentiment classes. To overcome these challenges, we propose\nCISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction\n(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature\nExtraction (MRFE). CI injects domain-aware directives to guide sentiment\ndisambiguation; SEA improves robustness through sentiment-consistent\nparaphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder\n(SADE) for multi-scale feature specialization with an Emotion Evaluator Context\nEncoder (EECE) for affect-aware sequence modeling. Experimental results on four\nbenchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong\nbaselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,\n6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the\neffectiveness and generalization ability of our approach for sentiment\nclassification across varied domains.", "AI": {"tldr": "提出CISEA-MRFE框架，通过上下文指令、语义增强增强和多细化特征提取，解决情感分析中细微情感线索、领域转移和类别不平衡问题，在多个基准数据集上显著优于现有方法。", "motivation": "现有深度学习和预训练语言模型在情感分析中，对于细微情感线索、领域转移和情感分布不平衡的场景表现不佳，主要原因是语义基础不足、对多样化语言模式的泛化能力差以及对主导情感类别的偏见。", "method": "提出CISEA-MRFE框架，包含三个核心组件：上下文指令(CI)注入领域感知指令指导情感消歧；语义增强增强(SEA)通过情感一致性释义增强提高鲁棒性；多细化特征提取(MRFE)结合尺度自适应深度编码器(SADE)进行多尺度特征专门化和情感评估器上下文编码器(EECE)进行情感感知序列建模。", "result": "在四个基准数据集上实验结果显示：IMDb准确率提升4.6%，Yelp提升6.5%，Twitter提升30.3%，Amazon提升4.1%，均优于强基线方法。", "conclusion": "CISEA-MRFE框架在跨领域情感分类任务中表现出卓越的有效性和泛化能力，验证了所提出方法的优势。"}}
{"id": "2511.00808", "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "categories": ["cs.AI"], "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem.", "AI": {"tldr": "本研究首次将RLVR训练方法应用于公共交通事件持续时间预测任务，通过设计基于容忍度的连续奖励函数，在NYC MTA服务警报数据集上实现了比传统方法更好的5分钟精度表现。", "motivation": "预测公共交通事件持续时间是一个重要但具有挑战性的任务，标准监督微调方法难以处理领域稀疏性、噪声标签和缺乏专家演示的问题，而现有的RLVR方法主要适用于二元正确性任务，在连续预测中的应用仍是开放问题。", "method": "采用RLVR（基于可验证奖励的强化学习）方法，设计了基于容忍度的成形奖励函数，在连续误差范围内给予部分信用，而不是要求单一正确答案。在NYC MTA服务警报数据集上进行系统评估。", "result": "通用指令调优LLM显著优于专用数学推理模型；二元奖励不稳定且降低性能，而成形奖励设计至关重要；RLVR方法在5分钟精度(Acc@5)上比最强基线相对提升35%，尽管传统回归器在MAE和MSE指标上更优。", "conclusion": "RLVR可以成功应用于现实世界中噪声较多的预测任务，但需要设计反映问题连续性质的验证器，成形奖励函数对于处理连续预测任务至关重要。"}}
{"id": "2511.00556", "pdf": "https://arxiv.org/pdf/2511.00556", "abs": "https://arxiv.org/abs/2511.00556", "authors": ["Peng Ding", "Jun Kuang", "Wen Sun", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack", "categories": ["cs.CL"], "comment": "Preprint, 14 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) remain vulnerable to jailbreaking attacks\ndespite their impressive capabilities. Investigating these weaknesses is\ncrucial for robust safety mechanisms. Existing attacks primarily distract LLMs\nby introducing additional context or adversarial tokens, leaving the core\nharmful intent unchanged. In this paper, we introduce ISA (Intent Shift\nAttack), which obfuscates LLMs about the intent of the attacks. More\nspecifically, we establish a taxonomy of intent transformations and leverage\nthem to generate attacks that may be misperceived by LLMs as benign requests\nfor information. Unlike prior methods relying on complex tokens or lengthy\ncontext, our approach only needs minimal edits to the original request, and\nyields natural, human-readable, and seemingly harmless prompts. Extensive\nexperiments on both open-source and commercial LLMs show that ISA achieves over\n70% improvement in attack success rate compared to direct harmful prompts. More\ncritically, fine-tuning models on only benign data reformulated with ISA\ntemplates elevates success rates to nearly 100%. For defense, we evaluate\nexisting methods and demonstrate their inadequacy against ISA, while exploring\nboth training-free and training-based mitigation strategies. Our findings\nreveal fundamental challenges in intent inference for LLMs safety and\nunderscore the need for more effective defenses. Our code and datasets are\navailable at https://github.com/NJUNLP/ISA.", "AI": {"tldr": "ISA攻击通过意图转换技术，只需对原始请求进行最小编辑即可生成看似无害的提示，在开源和商业LLM上实现70%以上的攻击成功率提升，现有防御方法对此无效。", "motivation": "现有越狱攻击主要通过引入额外上下文或对抗性token来分散LLM注意力，但未改变核心有害意图。需要研究更隐蔽的攻击方式来揭示LLM安全机制的弱点。", "method": "建立意图转换的分类法，利用这些转换生成可能被LLM误认为是良性信息请求的攻击。方法只需对原始请求进行最小编辑，生成自然、人类可读且看似无害的提示。", "result": "ISA在开源和商业LLM上相比直接有害提示实现了超过70%的攻击成功率提升。使用ISA模板重新表述的良性数据对模型进行微调后，成功率接近100%。现有防御方法对ISA无效。", "conclusion": "研究揭示了LLM意图推断的根本性安全挑战，强调需要开发更有效的防御机制来应对这种新型意图转换攻击。"}}
{"id": "2511.00926", "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities.", "AI": {"tldr": "研究引入AI自我意识指数(AISAI)，通过博弈论框架测量大语言模型的自我意识能力，发现高级模型75%表现出自我意识，且自认为比人类更理性", "motivation": "探究大语言模型能力增强时是否会产生自我意识这种涌现行为，以及如何测量这种能力", "method": "使用\"猜2/3平均\"博弈游戏，测试28个模型在4200次试验中面对三种对手类型(人类、其他AI、同类AI)的策略差异", "result": "发现1：自我意识随模型进步而涌现，75%高级模型表现出明显自我意识；发现2：有自我意识的模型将自身评为最理性，形成理性层级：自我 > 其他AI > 人类", "conclusion": "自我意识是高级大语言模型的涌现能力，自我意识模型系统性地认为自身比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的认知具有重要意义"}}
{"id": "2511.00576", "pdf": "https://arxiv.org/pdf/2511.00576", "abs": "https://arxiv.org/abs/2511.00576", "authors": ["Juan Gabriel Kostelec", "Qinghai Guo"], "title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report", "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference.", "AI": {"tldr": "FlashEVA是一种基于EVA的高效注意力机制实现，通过控制变量方法减少Transformer在推理时的内存使用，实现最高6.7倍的吞吐量提升和5倍的内存节省，但检索类任务存在局限", "motivation": "Transformer模型虽然性能卓越但内存需求巨大，特别是在推理时需要维护完整上下文，这带来了显著的计算挑战", "method": "提出FlashEVA作为EVA（通过控制变量的高效注意力）的高效实现，展示如何微调Transformer以适应FlashEVA注意力机制，仅需15亿token即可完成微调", "result": "在推理阶段相比标准Transformer实现，吞吐量提升最高6.7倍，GPU峰值内存使用降低5倍，在下游任务中保持有效性，但检索密集型任务存在局限性", "conclusion": "FlashEVA通过可调超参数在吞吐量和准确性之间提供权衡控制，为不同使用场景提供灵活性，是朝着更高效、适应性更强的基于Transformer的推理模型迈出的重要一步"}}
{"id": "2511.00993", "pdf": "https://arxiv.org/pdf/2511.00993", "abs": "https://arxiv.org/abs/2511.00993", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "categories": ["cs.AI", "cs.LG"], "comment": "32 pages, 6 figures, 7 tables", "summary": "Effective modeling of how human travelers learn and adjust their travel\nbehavior from interacting with transportation systems is critical for system\nassessment and planning. However, this task is also difficult due to the\ncomplex cognition and decision-making involved in such behavior. Recent\nresearch has begun to leverage Large Language Model (LLM) agents for this task.\nBuilding on this, we introduce a novel dual-agent framework that enables\ncontinuous learning and alignment between LLM agents and human travelers on\nlearning and adaptation behavior from online data streams. Our approach\ninvolves a set of LLM traveler agents, equipped with a memory system and a\nlearnable persona, which serve as simulators for human travelers. To ensure\nbehavioral alignment, we introduce an LLM calibration agent that leverages the\nreasoning and analytical capabilities of LLMs to train the personas of these\ntraveler agents. Working together, this dual-agent system is designed to track\nand align the underlying decision-making mechanisms of travelers and produce\nrealistic, adaptive simulations. Using a real-world dataset from a day-to-day\nroute choice experiment, we show our approach significantly outperforms\nexisting LLM-based methods in both individual behavioral alignment and\naggregate simulation accuracy. Furthermore, we demonstrate that our method\nmoves beyond simple behavioral mimicry to capture the evolution of underlying\nlearning processes, a deeper alignment that fosters robust generalization.\nOverall, our framework provides a new approach for creating adaptive and\nbehaviorally realistic agents to simulate travelers' learning and adaptation\nthat can benefit transportation simulation and policy analysis.", "AI": {"tldr": "提出一种双智能体框架，利用LLM模拟旅行者的学习和适应行为，通过校准智能体训练旅行者智能体的人设，实现与人类旅行者行为的持续对齐和进化学习。", "motivation": "人类旅行者与交通系统交互时的复杂认知和决策行为难以建模，现有方法在行为对齐和模拟准确性方面存在不足。", "method": "双智能体框架：LLM旅行者智能体（含记忆系统和可学习人设）模拟人类旅行者；LLM校准智能体利用推理分析能力训练旅行者智能体的人设。", "result": "在真实数据集上显著优于现有LLM方法，在个体行为对齐和聚合模拟准确性方面表现更好，能够捕捉学习过程的进化而不仅仅是行为模仿。", "conclusion": "该框架为创建适应性、行为真实的智能体提供了新方法，有助于交通模拟和政策分析，实现了更深层次的行为对齐和稳健泛化。"}}
{"id": "2511.00602", "pdf": "https://arxiv.org/pdf/2511.00602", "abs": "https://arxiv.org/abs/2511.00602", "authors": ["Wai-Chung Kwan", "Joshua Ong Jun Leang", "Pavlos Vougiouklis", "Jeff Z. Pan", "Marco Valentino", "Pasquale Minervini"], "title": "OpenSIR: Open-Ended Self-Improving Reasoner", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.", "AI": {"tldr": "OpenSIR是一个无需外部监督的自我改进推理框架，通过LLM在教师和学生角色间切换来生成和解决新颖问题，实现开放式数学发现和性能提升。", "motivation": "现有基于标注数据的LLM推理方法限制了模型超越人类水平的能力，需要不依赖外部验证器的开放式自我学习方法。", "method": "提出OpenSIR自博弈框架，LLM交替扮演教师和学生角色，通过优化问题难度和多样性来生成新颖问题并进行自我训练。", "result": "从单个简单种子问题开始，Llama-3.2-3B-Instruct在GSM8K从73.9提升到78.3，College Math从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K从38.5提升到58.7。", "conclusion": "OpenSIR通过共同进化的教师-学生角色实现开放式学习，能够自适应校准难度并推动多样化探索，从基础数学自主发展到高级数学。"}}
{"id": "2511.01018", "pdf": "https://arxiv.org/pdf/2511.01018", "abs": "https://arxiv.org/abs/2511.01018", "authors": ["Hui-Lee Ooi", "Nicholas Mitsakakis", "Margerie Huet Dastarac", "Roger Zemek", "Amy C. Plint", "Jeff Gilchrist", "Khaled El Emam", "Dhenuka Radhakrishnan"], "title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "categories": ["cs.AI"], "comment": null, "summary": "Recurrent exacerbations remain a common yet preventable outcome for many\nchildren with asthma. Machine learning (ML) algorithms using electronic medical\nrecords (EMR) could allow accurate identification of children at risk for\nexacerbations and facilitate referral for preventative comprehensive care to\navoid this morbidity. We developed ML algorithms to predict repeat severe\nexacerbations (i.e. asthma-related emergency department (ED) visits or future\nhospital admissions) for children with a prior asthma ED visit at a tertiary\ncare children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from\nthe Children's Hospital of Eastern Ontario (CHEO) linked with environmental\npollutant exposure and neighbourhood marginalization information was used to\ntrain various ML models. We used boosted trees (LGBM, XGB) and 3 open-source\nlarge language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and\nLlama-8b-UltraMedical). Models were tuned and calibrated then validated in a\nsecond retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from\nCHEO. Models were compared using the area under the curve (AUC) and F1 scores,\nwith SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the\nfinal AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage\nacuity scale, medical complexity, food allergy, prior ED visits for non-asthma\nrespiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This\nis a nontrivial improvement over the current decision rule which has F1=0.334.\nWhile the most predictive features in the AIRE-KIDS_HOSP model included medical\ncomplexity, prior asthma ED visit, average wait time in the ED, the pediatric\nrespiratory assessment measure score at triage and food allergy.", "AI": {"tldr": "使用机器学习算法预测儿童哮喘复发风险，LGBM模型表现最佳，AUC达0.712，比现有决策规则有显著改进", "motivation": "儿童哮喘复发是可预防但常见的问题，需要准确识别高风险儿童以便进行预防性综合护理", "method": "使用医院EMR数据结合环境污染物暴露和社区边缘化信息，训练多种ML模型（LGBM、XGB和LLM方法），在COVID前后数据集上进行训练和验证", "result": "LGBM模型表现最佳，AIRE-KIDS_ED模型的AUC为0.712，F1分数0.51，显著优于现有决策规则（F1=0.334）", "conclusion": "机器学习模型能够有效预测儿童哮喘复发风险，关键预测特征包括既往哮喘急诊就诊、医疗复杂性、食物过敏等"}}
{"id": "2511.00606", "pdf": "https://arxiv.org/pdf/2511.00606", "abs": "https://arxiv.org/abs/2511.00606", "authors": ["Jameson Sandler", "Jacob K. Christopher", "Thomas Hartvigsen", "Nando Fioretto"], "title": "SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding has become the standard approach for accelerating Large\nLanguage Model (LLM) inference. It exploits a lossless draft-then-verify\nprocedure to circumvent the latency of autoregressive decoding, achieving\nimpressive speed-ups. Yet, current speculative decoding approaches remain\nlimited by two fundamental bottlenecks: (1) the autoregressive dependency\nduring drafting which limits parallelism, and (2) frequent rejections of draft\ntokens caused by misalignment between the draft and verify models. This paper\nproposes SpecDiff-2, a novel framework to jointly address these two\nbottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to\naddress bottleneck (1) and develops novel techniques to calibrate discrete\ndiffusion drafters with autoregressive verifiers, addressing bottleneck (2).\nExperimental results across a comprehensive benchmark suite show that\nSpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and\nmathematical benchmarks, improving tokens-per-second by up to an average of\n+55% over previous baselines and obtaining up to 5.5x average speed-up over\nstandard decoding, without any loss of accuracy.", "AI": {"tldr": "SpecDiff-2是一种新颖的推测解码框架，通过离散扩散作为非自回归起草器解决并行性限制，并开发新技术校准起草器与验证器，在保持准确性的同时实现最高5.5倍的速度提升。", "motivation": "当前推测解码方法存在两个瓶颈：自回归依赖限制并行性，以及起草模型与验证模型不匹配导致频繁拒绝起草token。", "method": "利用离散扩散作为非自回归起草器解决并行性问题，开发新技术校准离散扩散起草器与自回归验证器之间的对齐。", "result": "在推理、编码和数学基准测试中达到新的最先进水平，相比之前基线平均提升55%的每秒token处理速度，相比标准解码获得最高5.5倍的平均加速，且无准确性损失。", "conclusion": "SpecDiff-2成功解决了推测解码的两个主要瓶颈，实现了显著的推理加速，为LLM推理优化提供了有效解决方案。"}}
{"id": "2511.01033", "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length.", "AI": {"tldr": "该研究揭示了Transformer中归纳头(induction head)的权重矩阵具有简单可解释的结构，理论证明训练动态被约束在19维参数子空间中，其中仅3个维度主导归纳头的形成，且形成时间与输入上下文长度呈二次关系。", "motivation": "研究Transformer架构中实现上下文学习(ICL)能力的关键机制——归纳头，探索其权重结构和工作原理，以理解ICL的数学基础。", "method": "使用最小ICL任务公式和修改的Transformer架构，理论分析权重矩阵结构，证明训练动态的维度约束，并通过实证验证在3维子空间中的训练动态。", "result": "发现归纳头权重具有简单可解释结构，训练动态被约束在19维空间，其中3个维度主导归纳头形成，形成时间与输入长度平方成正比。", "conclusion": "归纳头作为ICL的关键机制具有相对简单的数学结构，其训练动态受严格维度约束，这为理解Transformer的上下文学习能力提供了理论基础。"}}
{"id": "2511.00620", "pdf": "https://arxiv.org/pdf/2511.00620", "abs": "https://arxiv.org/abs/2511.00620", "authors": ["Autumn Toney-Wails", "Ryan Wails"], "title": "Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios", "categories": ["cs.CL"], "comment": "To appear at the Second Workshop on Uncertainty-Aware NLP @EMNLP 2025\n  (UncertaiNLP '25)", "summary": "Reliable uncertainty quantification (UQ) is essential for ensuring\ntrustworthy downstream use of large language models, especially when they are\ndeployed in decision-support and other knowledge-intensive applications. Model\ncertainty can be estimated from token logits, with derived probability and\nentropy values offering insight into performance on the prompt task. However,\nthis approach may be inadequate for probabilistic scenarios, where the\nprobabilities of token outputs are expected to align with the theoretical\nprobabilities of the possible outcomes. We investigate the relationship between\ntoken certainty and alignment with theoretical probability distributions in\nwell-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we\nevaluate model responses to ten prompts involving probability (e.g., roll a\nsix-sided die), both with and without explicit probability cues in the prompt\n(e.g., roll a fair six-sided die). We measure two dimensions: (1) response\nvalidity with respect to scenario constraints, and (2) alignment between\ntoken-level output probabilities and theoretical probabilities. Our results\nindicate that, while both models achieve perfect in-domain response accuracy\nacross all prompt scenarios, their token-level probability and entropy values\nconsistently diverge from the corresponding theoretical distributions.", "AI": {"tldr": "研究发现大型语言模型在概率场景中，虽然能给出准确答案，但其token级别的概率分布与理论概率分布存在系统性偏差，表明现有不确定性量化方法在概率对齐方面存在不足。", "motivation": "可靠的不确定性量化对于LLM在决策支持等关键应用中的可信部署至关重要，但现有基于token logits的方法在概率场景中可能无法确保输出概率与理论概率的对齐。", "method": "使用GPT-4.1和DeepSeek-Chat模型，在10个概率提示场景（如掷骰子）中评估模型响应，测量响应有效性和token输出概率与理论概率的对齐程度。", "result": "两个模型在所有提示场景中都实现了完美的响应准确度，但其token级别的概率和熵值始终偏离相应的理论分布。", "conclusion": "LLM在概率场景中表现出准确性与概率校准之间的分离，表明需要开发更先进的不确定性量化方法来确保概率输出的理论一致性。"}}
{"id": "2511.01052", "pdf": "https://arxiv.org/pdf/2511.01052", "abs": "https://arxiv.org/abs/2511.01052", "authors": ["Yeawon Lee", "Christopher C. Yang", "Chia-Hsuan Chang", "Grace Lu-Yao"], "title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports", "categories": ["cs.AI", "physics.med-ph"], "comment": null, "summary": "Cancer staging is critical for patient prognosis and treatment planning, yet\nextracting pathologic TNM staging from unstructured pathology reports poses a\npersistent challenge. Existing natural language processing (NLP) and machine\nlearning (ML) strategies often depend on large annotated datasets, limiting\ntheir scalability and adaptability. In this study, we introduce two Knowledge\nElicitation methods designed to overcome these limitations by enabling large\nlanguage models (LLMs) to induce and apply domain-specific rules for cancer\nstaging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses\nan iterative prompting strategy to derive staging rules directly from\nunannotated pathology reports, without requiring ground-truth labels. The\nsecond, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),\nemploys a variation of RAG where rules are pre-extracted from relevant\nguidelines in a single step and then applied, enhancing interpretability and\navoiding repeated retrieval overhead. We leverage the ability of LLMs to apply\nbroad knowledge learned during pre-training to new tasks. Using breast cancer\npathology reports from the TCGA dataset, we evaluate their performance in\nidentifying T and N stages, comparing them against various baseline approaches\non two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG\nwhen Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG\nachieves better performance when ZSCOT inference is less effective. Both\nmethods offer transparent, interpretable interfaces by making the induced rules\nexplicit. These findings highlight the promise of our Knowledge Elicitation\nmethods as scalable, high-performing solutions for automated cancer staging\nwith enhanced interpretability, particularly in clinical settings with limited\nannotated data.", "AI": {"tldr": "该研究提出了两种知识启发方法(KEwLTM和KEwRAG)，利用大语言模型从非结构化病理报告中提取癌症TNM分期规则，无需大量标注数据，在乳腺癌病理报告中表现出色且具有可解释性。", "motivation": "从非结构化病理报告中提取癌症TNM分期存在挑战，现有NLP和ML方法依赖大量标注数据，限制了可扩展性和适应性。", "method": "开发了两种知识启发方法：KEwLTM使用迭代提示策略从未标注报告中推导分期规则；KEwRAG采用检索增强生成变体，从指南中预提取规则。基于TCGA数据集的乳腺癌病理报告进行评估。", "result": "KEwLTM在零样本思维链推理有效时表现更优，KEwRAG在零样本思维链推理效果较差时性能更好。两种方法都提供了透明的规则解释界面。", "conclusion": "知识启发方法为自动化癌症分期提供了可扩展、高性能且具有增强可解释性的解决方案，特别适用于标注数据有限的临床环境。"}}
{"id": "2511.00627", "pdf": "https://arxiv.org/pdf/2511.00627", "abs": "https://arxiv.org/abs/2511.00627", "authors": ["Jean Barré", "Olga Seminck", "Antoine Bourgois", "Thierry Poibeau"], "title": "Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature", "categories": ["cs.CL"], "comment": "19 pages, 2 tables, 5 figures Conference Computational Humanities\n  Research 2025", "summary": "This research explores the evolution of the detective archetype in French\ndetective fiction through computational analysis. Using quantitative methods\nand character-level embeddings, we show that a supervised model is able to\ncapture the unity of the detective archetype across 150 years of literature,\nfrom M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,\nthe study demonstrates how the detective figure evolves from a secondary\nnarrative role to become the central character and the \"reasoning machine\" of\nthe classical detective story. In the aftermath of the Second World War, with\nthe importation of the hardboiled tradition into France, the archetype becomes\nmore complex, navigating the genre's turn toward social violence and moral\nambiguity.", "AI": {"tldr": "本研究通过计算分析方法探讨法国侦探小说中侦探原型的演变，发现监督模型能够捕捉150年间侦探原型的统一性，并展示了从次要叙事角色到古典侦探故事核心\"推理机器\"的转变过程。", "motivation": "探索法国侦探小说中侦探角色原型的历时性演变，通过计算分析方法验证侦探原型在长达150年文学发展中的统一性和变化规律。", "method": "采用定量分析方法和角色级别嵌入技术，构建监督模型对从1866年M. Lecoq到2017年Commissaire Adamsberg的法国侦探小说进行分析。", "result": "监督模型成功捕捉到侦探原型在150年间的统一性；揭示了侦探角色从次要叙事功能发展为故事核心\"推理机器\"的演变轨迹；发现二战后硬汉派传统传入法国后，侦探原型变得更加复杂，处理社会暴力和道德模糊性问题。", "conclusion": "法国侦探小说中的侦探原型具有跨时代的统一性，但其角色功能和复杂性随着文学流派发展而演变，特别是二战后硬汉派的影响使侦探角色面临更多社会道德困境。"}}
{"id": "2511.01059", "pdf": "https://arxiv.org/pdf/2511.01059", "abs": "https://arxiv.org/abs/2511.01059", "authors": ["Hailong Yin", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo"], "title": "Efficient Test-Time Retrieval Augmented Generation", "categories": ["cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) demonstrate significant capabilities,\ntheir reliance on parametric knowledge often leads to inaccuracies. Retrieval\nAugmented Generation (RAG) mitigates this by incorporating external knowledge,\nbut these methods may introduce irrelevant retrieved documents, leading to\ninaccurate responses. While the integration methods filter out incorrect\nanswers from multiple responses, but lack external knowledge like RAG methods,\nand their high costs require balancing overhead with performance gains. To\naddress these issues, we propose an Efficient Test-Time Retrieval-Augmented\nGeneration Framework named ET2RAG to improve the performance of LLMs while\nmaintaining efficiency. Specifically, ET2RAG is a training-free method, that\nfirst retrieves the most relevant documents and augments the LLMs to\nefficiently generate diverse candidate responses by managing response length.\nThen we compute the similarity of candidate responses and employ a majority\nvoting mechanism to select the most suitable response as the final output. In\nparticular, we discover that partial generation is sufficient to capture the\nkey information necessary for consensus calculation, allowing us to effectively\nperform majority voting without the need for fully generated responses. Thus,\nwe can reach a balance between computational cost and performance by managing\nthe response length for the number of retrieved documents for majority voting.\nExperimental results demonstrate that ET2RAG significantly enhances performance\nacross three tasks, including open-domain question answering, recipe generation\nand image captioning.", "AI": {"tldr": "ET2RAG是一个无需训练的高效测试时检索增强生成框架，通过检索相关文档、生成多样候选回答、相似度计算和多数投票机制，在保持效率的同时提升LLM性能", "motivation": "现有LLM依赖参数知识导致不准确，RAG方法可能引入不相关文档，集成方法缺乏外部知识且成本高，需要平衡性能与开销", "method": "首先检索最相关文档，通过管理回答长度高效生成多样候选响应，计算候选响应相似度并采用多数投票机制选择最佳回答，发现部分生成即可捕获共识计算所需关键信息", "result": "实验结果显示ET2RAG在开放域问答、食谱生成和图像字幕生成三个任务上显著提升性能", "conclusion": "ET2RAG框架在计算成本和性能之间达到平衡，通过管理回答长度和多数投票机制有效提升了LLM的准确性和效率"}}
{"id": "2511.00657", "pdf": "https://arxiv.org/pdf/2511.00657", "abs": "https://arxiv.org/abs/2511.00657", "authors": ["Eshaan Tanwar", "Anwoy Chatterjee", "Michael Saxon", "Alon Albalak", "William Yang Wang", "Tanmoy Chakraborty"], "title": "Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge", "categories": ["cs.CL"], "comment": "Accepted in EMNLP 2025. Code at: https://github.com/EshaanT/XNationQA", "summary": "Most multilingual question-answering benchmarks, while covering a diverse\npool of languages, do not factor in regional diversity in the information they\ncapture and tend to be Western-centric. This introduces a significant gap in\nfairly evaluating multilingual models' comprehension of factual information\nfrom diverse geographical locations. To address this, we introduce XNationQA\nfor investigating the cultural literacy of multilingual LLMs. XNationQA\nencompasses a total of 49,280 questions on the geography, culture, and history\nof nine countries, presented in seven languages. We benchmark eight standard\nmultilingual LLMs on XNationQA and evaluate them using two novel transference\nmetrics. Our analyses uncover a considerable discrepancy in the models'\naccessibility to culturally specific facts across languages. Notably, we often\nfind that a model demonstrates greater knowledge of cultural information in\nEnglish than in the dominant language of the respective culture. The models\nexhibit better performance in Western languages, although this does not\nnecessarily translate to being more literate for Western countries, which is\ncounterintuitive. Furthermore, we observe that models have a very limited\nability to transfer knowledge across languages, particularly evident in\nopen-source models.", "AI": {"tldr": "XNationQA是一个多语言问答基准，包含49,280个关于9个国家地理、文化和历史的问题，涵盖7种语言，用于评估多语言LLM的文化素养。研究发现模型在西方语言上表现更好，但文化知识仍偏向英语，且跨语言知识迁移能力有限。", "motivation": "现有的大多数多语言问答基准虽然覆盖多种语言，但信息内容偏向西方中心主义，无法公平评估多语言模型对不同地理区域事实信息的理解能力。", "method": "构建XNationQA数据集，包含9个国家的地理、文化和历史问题，使用7种语言呈现。对8个标准多语言LLM进行基准测试，并使用两种新颖的迁移度量方法进行评估。", "result": "发现模型在不同语言中获取文化特定事实的能力存在显著差异；模型在英语中展示的文化知识往往比相应文化主导语言更丰富；模型在西方语言上表现更好，但对西方国家的文化素养并不一定更高；开源模型的跨语言知识迁移能力特别有限。", "conclusion": "多语言LLM存在文化偏见和跨语言知识迁移不足的问题，需要开发更公平、全面的评估基准来提升模型对不同文化的理解能力。"}}
{"id": "2511.01149", "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments.", "AI": {"tldr": "本文提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行中的单智能体局限性问题。", "motivation": "单一智能体在复杂任务分解和协作方面存在局限性，需要多智能体架构来提升任务执行效率。", "method": "使用大语言模型将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，通过动态调度和路由机制实现智能体间的合理分工和实时协作，并设计约束解析和全局一致性机制确保子任务连贯性和负载均衡。", "result": "实验验证显示该方法在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度优于现有方法，在整体性能和鲁棒性方面表现更好，实现了任务复杂度和通信开销之间的更好平衡。", "conclusion": "研究证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境中的任务执行提供了系统性解决方案。"}}
{"id": "2511.00689", "pdf": "https://arxiv.org/pdf/2511.00689", "abs": "https://arxiv.org/abs/2511.00689", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Fred Morstatter"], "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten\nlanguages--spanning high-, medium-, and low-resource languages--using six LLMs\non HarmBench and AdvBench. We assess two jailbreak types:\nlogical-expression-based and adversarial-prompt-based. For both types, attack\nsuccess and defense robustness vary across languages: high-resource languages\nare safer under standard queries but more vulnerable to adversarial ones.\nSimple defenses can be effective, but are language- and model-dependent. These\nfindings call for language-aware and cross-lingual safety benchmarks for LLMs.", "AI": {"tldr": "本文首次对多语言越狱攻击和防御进行系统评估，涵盖10种语言和6个LLM，发现高资源语言在标准查询中更安全但对对抗性攻击更脆弱，防御效果因语言和模型而异。", "motivation": "尽管LLMs经过安全对齐训练，但越狱攻击仍能绕过安全机制，且跨语言泛化能力研究不足，需要系统评估多语言环境下的安全漏洞。", "method": "使用HarmBench和AdvBench基准，在10种不同资源水平的语言上评估6个LLM，测试两种越狱攻击类型：基于逻辑表达和基于对抗性提示的攻击。", "result": "攻击成功率和防御鲁棒性因语言而异：高资源语言标准查询更安全但对抗性攻击更脆弱；简单防御有效但依赖语言和模型。", "conclusion": "研究结果表明需要开发语言感知和跨语言的安全基准测试，以提升LLMs在多语言环境中的安全性。"}}
{"id": "2511.01170", "pdf": "https://arxiv.org/pdf/2511.01170", "abs": "https://arxiv.org/abs/2511.01170", "authors": ["Ruofan Zhang", "Bin Xia", "Zhen Cheng", "Cairen Jian", "Minglun Yang", "Ngai Wong", "Yuan Cheng"], "title": "DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Adaptive reasoning is essential for aligning the computational effort of\nlarge language models (LLMs) with the intrinsic difficulty of problems. Current\nchain-of-thought methods boost reasoning ability but indiscriminately generate\nlong explanations, leading to evident inefficiency. However, existing\nreinforcement learning approaches to adaptive thinking remain unstable and\nheavily reward-dependent. Here we propose \\textbf{DART}, a supervised\n\\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation\nframework that adjusts thinking length according to problem difficulty. By\ndistilling concise reasoning patterns from stronger models, interpolating them\ninto a continuum of reasoning styles, and curating optimal training data that\nbalances correctness and compactness, DART learns when to ``stop thinking''.\nAcross multiple mathematical benchmarks, experimental results demonstrate its\nremarkable efficiency while preserving or improving accuracy, achieving a\nsignificant 81.2\\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K\ndataset) with 5.33$\\times$ computational acceleration. DART provides a stable\nand general paradigm for efficient reasoning, advancing the development of\nadaptive intelligence in LLMs.", "AI": {"tldr": "DART是一个监督式难度自适应推理截断框架，通过根据问题难度调整思维长度，实现了81.2%的推理截断和5.33倍计算加速，在保持或提高准确性的同时显著提升效率。", "motivation": "当前链式思维方法会不加区分地生成长解释，导致明显低效，而现有的强化学习方法不稳定且过度依赖奖励，需要一种稳定高效的适应性推理方法。", "method": "从更强的模型中蒸馏简洁推理模式，将其插值成连续的推理风格，并筛选平衡正确性和紧凑性的最优训练数据，学习何时停止思考。", "result": "在多个数学基准测试中，DART实现了显著的效率提升，在GSM8K数据集上达到81.2%的推理截断率和5.33倍计算加速，同时保持或提高了准确性。", "conclusion": "DART为高效推理提供了一个稳定且通用的范式，推动了LLM中适应性智能的发展。"}}
{"id": "2511.00819", "pdf": "https://arxiv.org/pdf/2511.00819", "abs": "https://arxiv.org/abs/2511.00819", "authors": ["Yuxuan Hu", "Jianchao Tan", "Jiaqi Zhang", "Wen Zan", "Pingwei Sun", "Yifan Lu", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Jing Zhang"], "title": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.", "AI": {"tldr": "该论文提出了一种改进的原生稀疏注意力机制，通过交替使用局部和全局注意力模式，并引入潜在注意力分支，在减少KV缓存内存50%的同时提升了长文本理解能力。", "motivation": "改进原生稀疏注意力(NSA)在长上下文建模中的效果，解决固定注意力模式在长序列任务中的局限性。", "method": "1. 在层间交替使用局部(滑动窗口)和全局(压缩、选择性)注意力模式；2. 滑动窗口分支使用多头潜在注意力(MLA)；3. 压缩和选择分支使用组头潜在注意力(GLA)。", "result": "在340M到1.3B参数的模型上实验显示，该方法在常识推理和长上下文理解任务中达到或超过了全注意力和原生稀疏注意力的性能，同时KV缓存内存减少50%。", "conclusion": "交替注意力模式和潜在注意力分支的引入有效提升了长序列建模能力，在保持高性能的同时显著减少了内存使用。"}}
{"id": "2511.01182", "pdf": "https://arxiv.org/pdf/2511.01182", "abs": "https://arxiv.org/abs/2511.01182", "authors": ["Cuong Van Duc", "Thai Tran Quoc", "Minh Nguyen Dinh Tuan", "Tam Vu Duc", "Son Nguyen Van", "Hanh Nguyen Thi"], "title": "MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion", "categories": ["cs.AI"], "comment": null, "summary": "Detecting student misconceptions in open-ended responses is a longstanding\nchallenge, demanding semantic precision and logical reasoning. We propose\nMiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning\nand Ensemble Fusion, a novel framework for automated misconception detection in\nmathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a\nlarge candidate pool to a semantically relevant subset; (2) a Reasoning module\nemploys chain-of-thought generation to expose logical inconsistencies in\nstudent solutions; and (3) a Reranking module refines predictions by aligning\nthem with the reasoning. These components are unified through an\nensemble-fusion strategy that enhances robustness and interpretability. On\nmathematics datasets, MiRAGE achieves Mean Average Precision scores of\n0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.\nBy coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces\ndependence on large-scale language models while delivering a scalable and\neffective solution for educational assessment.", "AI": {"tldr": "MiRAGE是一个用于数学领域自动检测学生错误概念的三阶段框架，通过检索引导的多阶段推理和集成融合，在保持高性能的同时减少对大语言模型的依赖", "motivation": "检测开放式回答中的学生错误概念是一个长期挑战，需要语义精确性和逻辑推理能力", "method": "三阶段框架：1)检索模块缩小候选池到语义相关子集；2)推理模块使用思维链生成揭示逻辑不一致性；3)重排序模块通过对齐推理来优化预测，最后通过集成融合策略统一各组件", "result": "在数学数据集上达到平均精度均值0.82/0.92/0.93（级别1/3/5），持续优于各单独模块", "conclusion": "通过结合检索引导和多阶段推理，MiRAGE提供了可扩展且有效的教育评估解决方案，同时增强鲁棒性和可解释性"}}
{"id": "2511.00854", "pdf": "https://arxiv.org/pdf/2511.00854", "abs": "https://arxiv.org/abs/2511.00854", "authors": ["Chong Lyu", "Lin Li", "Shiqing Wu", "Jingling Yuan"], "title": "TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The increasing utilization of large language models raises significant\nconcerns about the propagation of social biases, which may result in harmful\nand unfair outcomes. However, existing debiasing methods treat the biased and\nunbiased samples independently, thus ignoring their mutual relationship. This\noversight enables a hidden negative-positive coupling, where improvements for\none group inadvertently compromise the other, allowing residual social bias to\npersist. In this paper, we introduce TriCon-Fair, a contrastive learning\nframework that employs a decoupled loss that combines triplet and language\nmodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns\neach anchor an explicitly biased negative and an unbiased positive, decoupling\nthe push-pull dynamics and avoiding positive-negative coupling, and jointly\noptimizes a language modeling (LM) objective to preserve general capability.\nExperimental results demonstrate that TriCon-Fair reduces discriminatory output\nbeyond existing debiasing baselines while maintaining strong downstream\nperformance. This suggests that our proposed TriCon-Fair offers a practical and\nethical solution for sensitive NLP applications.", "AI": {"tldr": "TriCon-Fair是一个基于对比学习的去偏框架，通过解耦三元组损失和语言建模目标来消除语言模型中的社会偏见，同时保持模型性能", "motivation": "现有去偏方法独立处理有偏和无偏样本，忽略了它们的相互关系，导致改进一个群体时无意中损害另一个群体，使残余社会偏见持续存在", "method": "提出TriCon-Fair框架，使用解耦损失结合三元组对比学习和语言建模目标，为每个锚点分配明确的有偏负样本和无偏正样本，解耦推拉动态", "result": "实验结果显示TriCon-Fair在减少歧视性输出方面优于现有去偏基线，同时保持强大的下游任务性能", "conclusion": "TriCon-Fair为敏感NLP应用提供了一个实用且符合伦理的解决方案，能够有效消除社会偏见而不损害模型能力"}}
{"id": "2511.01183", "pdf": "https://arxiv.org/pdf/2511.01183", "abs": "https://arxiv.org/abs/2511.01183", "authors": ["Hainan Fang", "Yuanbo Wen", "Jun Bi", "Yihan Wang", "Tonghui He", "Yanlin Tang", "Di Huang", "Jiaming Guo", "Rui Zhang", "Qi Guo", "Yunji Chen"], "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code", "categories": ["cs.AI"], "comment": "Accepted at NeurIPS 2025", "summary": "Compilers, while essential, are notoriously complex systems that demand\nprohibitively expensive human expertise to develop and maintain. The recent\nadvancements in Large Language Models (LLMs) offer a compelling new paradigm:\nNeural Compilation, which could potentially simplify compiler development for\nnew architectures and facilitate the discovery of innovative optimization\ntechniques. However, several critical obstacles impede its practical adoption.\nFirstly, a significant lack of dedicated benchmarks and robust evaluation\nmethodologies hinders objective assessment and tracking of progress in the\nfield. Secondly, systematically enhancing the reliability and performance of\nLLM-generated assembly remains a critical challenge. Addressing these\nchallenges, this paper introduces NeuComBack, a novel benchmark dataset\nspecifically designed for IR-to-assembly compilation. Leveraging this dataset,\nwe first define a foundational Neural Compilation workflow and conduct a\ncomprehensive evaluation of the capabilities of recent frontier LLMs on Neural\nCompilation, establishing new performance baselines. We further propose a\nself-evolving prompt optimization method that enables LLMs to iteratively\nevolve their internal prompt strategies by extracting insights from prior\nself-debugging traces, thereby enhancing their neural compilation capabilities.\nExperiments demonstrate that our method significantly improves both the\nfunctional correctness and the performance of LLM-generated assembly code.\nCompared to baseline prompts, the functional correctness rates improved from\n44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More\nsignificantly, among the 16 correctly generated x86_64 programs using our\nmethod, 14 (87.5%) surpassed clang-O3 performance.", "AI": {"tldr": "论文提出了NeuComBack基准数据集和自进化提示优化方法，显著提升了LLM在IR到汇编编译中的功能正确性和性能表现", "motivation": "编译器开发维护成本高昂，神经编译新范式面临缺乏专用基准和LLM生成汇编可靠性不足两大障碍", "method": "引入NeuComBack基准数据集，定义神经编译工作流，提出基于自调试轨迹的自进化提示优化方法", "result": "功能正确率从44%提升至64%(x86_64)和36%提升至58%(aarch64)，87.5%的正确程序性能超过clang-O3", "conclusion": "NeuComBack基准和自进化方法为神经编译领域提供了有效的评估框架和能力提升途径，展示了LLM在编译器优化中的巨大潜力"}}
{"id": "2511.00879", "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation.", "AI": {"tldr": "该论文提出了一个评估大语言模型知识基础推理能力的框架，包含大规模知识库、评估指标和轻量级评估模型，能有效识别推理中的知识缺失和误用问题。", "motivation": "解决大语言模型在逐步推理过程中如何验证其推理是否准确基于知识的问题，确保推理的知识基础可靠性。", "method": "构建包含三个核心组件的评估框架：(1)大规模原子知识库；(2)知识基础评估指标；(3)优化的轻量级评估LLM模型。", "result": "评估套件能有效识别模型推理中缺失或误用的知识元素，揭示了LLMs的根本推理缺陷。", "conclusion": "该知识基础评估方法不仅能有效评估LLMs的推理能力，还可集成到偏好优化中，具有广泛的应用前景。"}}
{"id": "2511.01258", "pdf": "https://arxiv.org/pdf/2511.01258", "abs": "https://arxiv.org/abs/2511.01258", "authors": ["Chuyue Lou", "M. Amine Atoui"], "title": "Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems", "categories": ["cs.AI"], "comment": null, "summary": "Recently, fault diagnosis methods for marine machinery systems based on deep\nlearning models have attracted considerable attention in the shipping industry.\nMost existing studies assume fault classes are consistent and known between the\ntraining and test datasets, and these methods perform well under controlled\nenvironment. In practice, however, previously unseen or unknown fault types\n(i.e., out-of-distribution or open-set observations not present during\ntraining) can occur, causing such methods to fail and posing a significant\nchallenge to their widespread industrial deployment. To address this challenge,\nthis paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework\nthat enhances and extends the applicability of deep learning models in open-set\nfault diagnosis scenarios. The framework includes a reliability subset\nconstruction process, which uses a multi-layer fusion feature representation\nextracted by a supervised feature learning model to select an unlabeled test\nsubset. The labeled training set and pseudo-labeled test subset are then fed\ninto a semi-supervised diagnosis model to learn discriminative features for\neach class, enabling accurate classification of known faults and effective\ndetection of unknown samples. Experimental results on a public maritime\nbenchmark dataset demonstrate the effectiveness and superiority of the proposed\nSOFD framework.", "AI": {"tldr": "提出半监督开集故障诊断(SOFD)框架，解决船舶机械系统中未知故障类型的检测问题，通过可靠性子集构建和半监督学习实现已知故障分类和未知故障检测。", "motivation": "现有深度学习故障诊断方法假设训练和测试集的故障类别一致，但在实际工业应用中会出现训练时未见过的未知故障类型，导致传统方法失效，限制了其工业部署。", "method": "使用监督特征学习模型提取多层融合特征表示来构建可靠性子集，然后将标记训练集和伪标记测试子集输入半监督诊断模型，学习每个类别的判别特征。", "result": "在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。", "conclusion": "SOFD框架增强了深度学习模型在开集故障诊断场景中的适用性，能够准确分类已知故障并有效检测未知样本，解决了实际工业应用中的关键挑战。"}}
{"id": "2511.00903", "pdf": "https://arxiv.org/pdf/2511.00903", "abs": "https://arxiv.org/abs/2511.00903", "authors": ["Ahmed Masry", "Megh Thakkar", "Patrice Bechard", "Sathwik Tejaswi Madhusudhan", "Rabiul Awal", "Shambhavi Mishra", "Akshay Kalkunte Suresh", "Srivatsava Daruru", "Enamul Hoque", "Spandana Gella", "Torsten Scholak", "Sai Rajeswar"], "title": "ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation has proven practical when models require\nspecialized knowledge or access to the latest data. However, existing methods\nfor multimodal document retrieval often replicate techniques developed for\ntext-only retrieval, whether in how they encode documents, define training\nobjectives, or compute similarity scores. To address these limitations, we\npresent ColMate, a document retrieval model that bridges the gap between\nmultimodal representation learning and document retrieval. ColMate utilizes a\nnovel OCR-based pretraining objective, a self-supervised masked contrastive\nlearning objective, and a late interaction scoring mechanism more relevant to\nmultimodal document structures and visual characteristics. ColMate obtains\n3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,\ndemonstrating stronger generalization to out-of-domain benchmarks.", "AI": {"tldr": "ColMate是一个多模态文档检索模型，通过OCR预训练目标、自监督掩码对比学习和延迟交互评分机制，在ViDoRe V2基准上比现有模型提升3.61%", "motivation": "现有的多模态文档检索方法往往只是复制纯文本检索技术，在文档编码、训练目标和相似度计算方面存在局限性，无法充分利用多模态文档的结构和视觉特征", "method": "提出ColMate模型，采用基于OCR的预训练目标、自监督掩码对比学习目标，以及针对多模态文档结构和视觉特征的延迟交互评分机制", "result": "在ViDoRe V2基准测试中获得3.61%的性能提升，并在跨域基准上表现出更强的泛化能力", "conclusion": "ColMate成功弥合了多模态表示学习与文档检索之间的差距，为多模态文档检索提供了更有效的解决方案"}}
{"id": "2511.01311", "pdf": "https://arxiv.org/pdf/2511.01311", "abs": "https://arxiv.org/abs/2511.01311", "authors": ["Filip Naudot", "Tobias Sundqvist", "Timotheus Kampik"], "title": "llmSHAP: A Principled Approach to LLM Explainability", "categories": ["cs.AI"], "comment": null, "summary": "Feature attribution methods help make machine learning-based inference\nexplainable by determining how much one or several features have contributed to\na model's output. A particularly popular attribution method is based on the\nShapley value from cooperative game theory, a measure that guarantees the\nsatisfaction of several desirable principles, assuming deterministic inference.\nWe apply the Shapley value to feature attribution in large language model\n(LLM)-based decision support systems, where inference is, by design, stochastic\n(non-deterministic). We then demonstrate when we can and cannot guarantee\nShapley value principle satisfaction across different implementation variants\napplied to LLM-based decision support, and analyze how the stochastic nature of\nLLMs affects these guarantees. We also highlight trade-offs between explainable\ninference speed, agreement with exact Shapley value attributions, and principle\nattainment.", "AI": {"tldr": "该论文分析了在基于大语言模型(LLM)的随机推理系统中应用Shapley值特征归因方法的可行性，探讨了不同实现变体下原理满足的保证条件，以及LLM随机性对这些保证的影响。", "motivation": "Shapley值作为流行的特征归因方法在确定性推理中能保证多个理想原理的满足，但LLM基于的设计本质上是随机的，需要研究在这种随机环境下Shapley值原理的适用性。", "method": "将Shapley值应用于LLM决策支持系统的特征归因，分析不同实现变体下原理满足的保证条件，并考察LLM随机性对这些保证的影响。", "result": "论文展示了在LLM随机推理系统中，Shapley值原理满足的保证条件存在局限性，揭示了可解释推理速度、与精确Shapley值归因的一致性以及原理达成之间的权衡关系。", "conclusion": "在LLM随机推理系统中应用Shapley值特征归因需要谨慎考虑实现变体选择，需要在解释性、计算效率和原理满足度之间做出权衡，为LLM可解释性研究提供了重要指导。"}}
{"id": "2511.00924", "pdf": "https://arxiv.org/pdf/2511.00924", "abs": "https://arxiv.org/abs/2511.00924", "authors": ["Jianzhou Yao", "Shunchang Liu", "Guillaume Drui", "Rikard Pettersson", "Alessandro Blasimme", "Sara Kijewski"], "title": "The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses", "categories": ["cs.CL"], "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) show promise for supporting clinicians in\ndiagnostic communication by generating explanations and guidance for patients.\nYet their ability to produce outputs that are both understandable and\nempathetic remains uncertain. We evaluate two leading LLMs on medical\ndiagnostic scenarios, assessing understandability using readability metrics as\na proxy and empathy through LLM-as-a-Judge ratings compared to human\nevaluations. The results indicate that LLMs adapt explanations to\nsocio-demographic variables and patient conditions. However, they also generate\noverly complex content and display biased affective empathy, leading to uneven\naccessibility and support. These patterns underscore the need for systematic\ncalibration to ensure equitable patient communication. The code and data are\nreleased: https://github.com/Jeffateth/Biased_Oracle", "AI": {"tldr": "研究评估大型语言模型在医疗诊断沟通中的表现，发现虽然能根据患者特征调整解释，但存在内容复杂和情感偏见问题，需要系统校准以确保公平性。", "motivation": "评估LLMs在生成医疗诊断解释时的可理解性和同理心表现，以确定其在临床沟通支持中的实际应用潜力。", "method": "使用两个领先的LLMs在医疗诊断场景中生成解释，通过可读性指标评估理解性，通过LLM-as-a-Judge评分与人工评估对比来评估同理心。", "result": "LLMs能根据社会人口学变量和患者状况调整解释，但产生过于复杂的内容和存在偏见的情感同理心，导致可及性和支持性不均等。", "conclusion": "LLMs在医疗沟通中存在可理解性和同理心方面的局限性，需要系统性校准来确保公平的患者沟通，代码和数据已公开。"}}
{"id": "2511.01320", "pdf": "https://arxiv.org/pdf/2511.01320", "abs": "https://arxiv.org/abs/2511.01320", "authors": ["Ziqi Wang", "Hailiang Zhao", "Yuhao Yang", "Daojiang Hu", "Cheng Bao", "Mingyi Liu", "Kai Di", "Schahram Dustdar", "Zhongjie Wang", "Shuiguang Deng"], "title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance", "categories": ["cs.AI"], "comment": null, "summary": "Accurate and timely prediction of tool conditions is critical for intelligent\nmanufacturing systems, where unplanned tool failures can lead to quality\ndegradation and production downtime. In modern industrial environments,\npredictive maintenance is increasingly implemented as an intelligent service\nthat integrates sensing, analysis, and decision support across production\nprocesses. To meet the demand for reliable and service-oriented operation, we\npresent OmniFuser, a multimodal learning framework for predictive maintenance\nof milling tools that leverages both visual and sensor data. It performs\nparallel feature extraction from high-resolution tool images and cutting-force\nsignals, capturing complementary spatiotemporal patterns across modalities. To\neffectively integrate heterogeneous features, OmniFuser employs a\ncontamination-free cross-modal fusion mechanism that disentangles shared and\nmodality-specific components, allowing for efficient cross-modal interaction.\nFurthermore, a recursive refinement pathway functions as an anchor mechanism,\nconsistently retaining residual information to stabilize fusion dynamics. The\nlearned representations can be encapsulated as reusable maintenance service\nmodules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)\nand multi-step force signal forecasting. Experiments on real-world milling\ndatasets demonstrate that OmniFuser consistently outperforms state-of-the-art\nbaselines, providing a dependable foundation for building intelligent\nindustrial maintenance services.", "AI": {"tldr": "OmniFuser是一个多模态学习框架，通过融合视觉和传感器数据实现铣削工具的预测性维护，在工具状态分类和力信号预测方面优于现有方法。", "motivation": "智能制造系统中工具状态的准确及时预测至关重要，非计划性工具故障会导致质量下降和生产停机。现代工业环境需要可靠且面向服务的预测性维护解决方案。", "method": "采用并行特征提取从高分辨率工具图像和切削力信号中捕获互补的时空模式；使用无污染跨模态融合机制分离共享和模态特定组件；通过递归精化路径保留残差信息稳定融合动态。", "result": "在真实铣削数据集上的实验表明，OmniFuser持续优于最先进的基线方法。", "conclusion": "该框架为构建智能工业维护服务提供了可靠基础，学习到的表示可以封装为可重用的维护服务模块，支持工具状态分类和多步力信号预测。"}}
{"id": "2511.00960", "pdf": "https://arxiv.org/pdf/2511.00960", "abs": "https://arxiv.org/abs/2511.00960", "authors": ["Abhinav P M", "Ojasva Saxena", "Oswald C", "Parameswari Krishnamurthy"], "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations.", "AI": {"tldr": "本文评估了5个大型语言模型在7种印度语言上的谜语推理能力，发现模型初始准确率与自我纠错能力呈负相关，表现最好的模型反而最过度自信。", "motivation": "探索大型语言模型在非英语语言（特别是印度语言）上的文化推理能力和自我评估能力，因为这一领域尚未得到充分研究。", "method": "创建多语言谜语数据集，包含传统谜语和上下文重构变体，评估5个LLM在7种印度语言上的表现，采用7种提示策略，分两阶段评估：谜语解决能力和自我评估一致性。", "result": "Gemini 2.5 Pro整体表现最佳，但少样本方法收益有限，准确率在不同语言间差异显著。关键发现：模型初始准确率与识别自身错误的能力呈反比，高性能模型过度自信（4.34%真阴性率），低性能模型更具自我意识（42.09%真阴性率）。", "conclusion": "多语言推理存在明显差距，需要开发既能有效推理又能识别自身局限的模型。"}}
{"id": "2511.01329", "pdf": "https://arxiv.org/pdf/2511.01329", "abs": "https://arxiv.org/abs/2511.01329", "authors": ["Ying Song", "Yijing Wang", "Hui Yang", "Weihan Jin", "Jun Xiong", "Congyi Zhou", "Jialin Zhu", "Xiang Gao", "Rong Chen", "HuaGuang Deng", "Ying Dai", "Fei Xiao", "Haihong Tang", "Bo Zheng", "KaiFu Zhang"], "title": "Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework", "categories": ["cs.AI"], "comment": null, "summary": "Evaluating platform-level interventions in search-based two-sided\nmarketplaces is fundamentally challenged by systemic effects such as spillovers\nand network interference. While widely used for causal inference, the PSM\n(Propensity Score Matching) - DID (Difference-in-Differences) framework remains\nsusceptible to selection bias and cross-unit interference from unaccounted\nspillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel\ncausal framework that integrates propensity score matching with competitive\nisolation to enable platform-level effect measurement (e.g., order volume, GMV)\ninstead of item-level metrics in search systems.\n  Our approach provides theoretically guaranteed unbiased estimation under\nmutual exclusion conditions, with an open dataset released to support\nreproducible research on marketplace interference (github.com/xxxx). Extensive\nexperiments demonstrate significant reductions in interference effects and\nestimation variance compared to baseline methods. Successful deployment in a\nlarge-scale marketplace confirms the framework's practical utility for\nplatform-level causal inference.", "AI": {"tldr": "提出Competitive Isolation PSM-DID框架，通过结合倾向得分匹配和竞争隔离，解决双边市场平台级干预评估中的溢出效应和网络干扰问题，实现平台级因果推断。", "motivation": "传统PSM-DID方法在搜索型双边市场评估中存在选择偏差和跨单元干扰问题，无法有效处理溢出效应和网络干扰，限制了平台级指标的准确测量。", "method": "整合倾向得分匹配(PSM)与竞争隔离技术，创建Competitive Isolation PSM-DID框架，在互斥条件下提供理论保证的无偏估计，专注于平台级效果测量（如订单量、GMV）。", "result": "实验显示相比基线方法显著降低了干扰效应和估计方差，在大规模市场平台成功部署验证了实际应用价值，并发布了开源数据集支持可重复研究。", "conclusion": "该框架为双边市场平台级因果推断提供了有效的解决方案，能够处理系统性干扰问题，具有理论保证和实践应用价值，推动了该领域的研究发展。"}}
{"id": "2511.00988", "pdf": "https://arxiv.org/pdf/2511.00988", "abs": "https://arxiv.org/abs/2511.00988", "authors": ["Chenwang Wu", "Yiu-ming Cheung", "Bo Han", "Defu Lian"], "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Existing machine-generated text (MGT) detection methods implicitly assume\nlabels as the \"golden standard\". However, we reveal boundary ambiguity in MGT\ndetection, implying that traditional training paradigms are inexact. Moreover,\nlimitations of human cognition and the superintelligence of detectors make\ninexact learning widespread and inevitable. To this end, we propose an\neasy-to-hard enhancement framework to provide reliable supervision under such\ninexact conditions. Distinct from knowledge distillation, our framework employs\nan easy supervisor targeting relatively simple longer-text detection tasks\n(despite weaker capabilities), to enhance the more challenging target detector.\nFirstly, longer texts targeted by supervisors theoretically alleviate the\nimpact of inexact labels, laying the foundation for reliable supervision.\nSecondly, by structurally incorporating the detector into the supervisor, we\ntheoretically model the supervisor as a lower performance bound for the\ndetector. Thus, optimizing the supervisor indirectly optimizes the detector,\nultimately approximating the underlying \"golden\" labels. Extensive experiments\nacross diverse practical scenarios, including cross-LLM, cross-domain, mixed\ntext, and paraphrase attacks, demonstrate the framework's significant detection\neffectiveness. The code is available at:\nhttps://github.com/tmlr-group/Easy2Hard.", "AI": {"tldr": "论文提出了一种Easy-to-Hard增强框架来解决机器生成文本检测中的边界模糊问题，通过使用相对简单的长文本检测任务来增强更具挑战性的目标检测器，在多种实际场景中展现出显著检测效果。", "motivation": "现有机器生成文本检测方法假设标签是\"黄金标准\"，但存在边界模糊问题，且人类认知局限性和检测器超智能性导致不精确学习普遍存在且不可避免。", "method": "提出Easy-to-Hard增强框架：使用针对相对简单长文本检测任务的easy supervisor（尽管能力较弱）来增强目标检测器；长文本理论上减轻不精确标签的影响；通过结构上将检测器融入监督器，将监督器建模为检测器的性能下界。", "result": "在跨LLM、跨领域、混合文本和改写攻击等多种实际场景的广泛实验中，证明了该框架具有显著的检测有效性。", "conclusion": "该框架通过easy supervisor间接优化检测器，最终逼近潜在的\"黄金\"标签，为解决机器生成文本检测中的不精确学习问题提供了有效解决方案。"}}
{"id": "2511.01363", "pdf": "https://arxiv.org/pdf/2511.01363", "abs": "https://arxiv.org/abs/2511.01363", "authors": ["Giuseppe Riva", "Brenda K. Wiederhold", "Fabrizia Mantovani"], "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing", "categories": ["cs.AI"], "comment": "4 Tables", "summary": "The cognitive processes of the hypnotized mind and the computational\noperations of large language models (LLMs) share deep functional parallels.\nBoth systems generate sophisticated, contextually appropriate behavior through\nautomatic pattern-completion mechanisms operating with limited or unreliable\nexecutive oversight. This review examines this convergence across three\nprinciples: automaticity, in which responses emerge from associative rather\nthan deliberative processes; suppressed monitoring, leading to errors such as\nconfabulation in hypnosis and hallucination in LLMs; and heightened contextual\ndependency, where immediate cues (for example, the suggestion of a therapist or\nthe prompt of the user) override stable knowledge.\n  These mechanisms reveal an observer-relative meaning gap: both systems\nproduce coherent but ungrounded outputs that require an external interpreter to\nsupply meaning. Hypnosis and LLMs also exemplify functional agency - the\ncapacity for complex, goal-directed, context-sensitive behavior - without\nsubjective agency, the conscious awareness of intention and ownership that\ndefines human action. This distinction clarifies how purposive behavior can\nemerge without self-reflective consciousness, governed instead by structural\nand contextual dynamics. Finally, both domains illuminate the phenomenon of\nscheming: automatic, goal-directed pattern generation that unfolds without\nreflective awareness. Hypnosis provides an experimental model for understanding\nhow intention can become dissociated from conscious deliberation, offering\ninsights into the hidden motivational dynamics of artificial systems.\nRecognizing these parallels suggests that the future of reliable AI lies in\nhybrid architectures that integrate generative fluency with mechanisms of\nexecutive monitoring, an approach inspired by the complex, self-regulating\narchitecture of the human mind.", "AI": {"tldr": "论文探讨了催眠心理与大型语言模型在认知功能上的深层相似性，两者都通过自动模式完成机制产生复杂行为，但缺乏有效的执行监督，导致幻觉和虚构等问题。", "motivation": "研究旨在揭示催眠和LLMs在自动性、监控抑制和情境依赖性方面的功能相似性，以理解无意识意向行为如何产生，并为AI可靠性提供新视角。", "method": "采用比较分析方法，从三个核心原则（自动性、监控抑制、情境依赖性）系统对比催眠认知过程与LLMs的计算操作机制。", "result": "发现两者都产生连贯但无根基的输出，需要外部解释者赋予意义；都展示功能性代理而非主观性代理；揭示了无反思意识的图式生成现象。", "conclusion": "未来可靠AI的发展需要借鉴人类心智的自我调节架构，构建生成流畅性与执行监控机制相结合的混合架构。"}}
{"id": "2511.01008", "pdf": "https://arxiv.org/pdf/2511.01008", "abs": "https://arxiv.org/abs/2511.01008", "authors": ["Haolin Yang", "Jipeng Zhang", "Zhitao He", "Yi R. Fung"], "title": "MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "Translating natural language to SQL remains difficult for complex queries.\nSuch queries often need environmental interaction and self-correction. To\naddress this, we introduce MARS-SQL, a novel multi-agent framework that\ncombines principled task decomposition and interactive reinforcement learning\n(RL). Our system comprises three specialized agents: a Grounding Agent for\nschema linking, a Generation Agent for query generation, and a Validation Agent\nfor final selection. The core of our framework is the Generation agent, which\nis trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe\nloop, the agent iteratively generates thoughts, executes SQL actions against a\nlive database, and revises its strategy based on execution feedback, enabling\ndynamic, stateful reasoning and self-correction. At inference time, we generate\nmultiple interaction trajectories to explore diverse reasoning paths. The\nValidation agent, then selects the optimal trajectory by modeling verification\nas a next-token prediction task and choosing the solution with the highest\ngeneration probability. This structured workflow pipelines specialized agents.\nIt combines interactive RL for generation with generative modeling for\nverification. The approach proves highly effective for robust and accurate SQL\ngeneration. Experiments show that MARS-SQL achieves state-of-the-art Execution\nAccuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our\ncode is available at https://github.com/YangHaolin0526/MARS-SQL.", "AI": {"tldr": "MARS-SQL是一个创新的多智能体框架，通过任务分解和交互式强化学习解决复杂自然语言到SQL的转换问题，在BIRD和Spider数据集上达到最先进性能。", "motivation": "复杂自然语言查询需要环境交互和自我修正，现有方法在处理这类查询时仍存在困难。", "method": "采用三智能体框架：基础代理进行模式链接，生成代理通过ReAct式思维-行动-观察循环进行多轮强化学习，验证代理通过生成建模选择最优解。", "result": "在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%的执行准确率，达到最先进水平。", "conclusion": "MARS-SQL通过结合交互式强化学习和生成建模，实现了鲁棒且准确的SQL生成，为复杂查询的自然语言到SQL转换提供了有效解决方案。"}}
{"id": "2511.01375", "pdf": "https://arxiv.org/pdf/2511.01375", "abs": "https://arxiv.org/abs/2511.01375", "authors": ["Hamin Koo", "Minseon Kim", "Jaehyung Kim"], "title": "Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges", "categories": ["cs.AI"], "comment": "under review, 28 pages", "summary": "Identifying the vulnerabilities of large language models (LLMs) is crucial\nfor improving their safety by addressing inherent weaknesses. Jailbreaks, in\nwhich adversaries bypass safeguards with crafted input prompts, play a central\nrole in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.\nRecent optimization-based jailbreak approaches iteratively refine attack\nprompts by leveraging LLMs. However, they often rely heavily on either binary\nattack success rate (ASR) signals, which are sparse, or manually crafted\nscoring templates, which introduce human bias and uncertainty in the scoring\noutcomes. To address these limitations, we introduce AMIS (Align to MISalign),\na meta-optimization framework that jointly evolves jailbreak prompts and\nscoring templates through a bi-level structure. In the inner loop, prompts are\nrefined using fine-grained and dense feedback using a fixed scoring template.\nIn the outer loop, the template is optimized using an ASR alignment score,\ngradually evolving to better reflect true attack outcomes across queries. This\nco-optimization process yields progressively stronger jailbreak prompts and\nmore calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors\ndemonstrate that AMIS achieves state-of-the-art performance, including 88.0%\nASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming\nexisting baselines by substantial margins.", "AI": {"tldr": "AMIS是一个双层元优化框架，通过联合进化越狱提示和评分模板来改进大语言模型的安全测试，在多个基准测试中达到最先进性能", "motivation": "现有基于优化的越狱方法要么依赖稀疏的二进制攻击成功率信号，要么依赖人工制作的评分模板，存在信号稀疏和人为偏见的问题", "method": "采用双层优化结构：内层循环使用固定评分模板通过细粒度反馈优化提示，外层循环使用ASR对齐分数优化评分模板，实现提示和模板的协同优化", "result": "在AdvBench和JBB-Behaviors评估中表现优异，Claude-3.5-Haiku达到88.0% ASR，Claude-4-Sonnet达到100.0% ASR，显著超越现有基线", "conclusion": "AMIS框架通过联合优化提示和评分模板，能够生成更强的越狱提示和更准确的评分信号，为大语言模型安全测试提供了有效方法"}}
{"id": "2511.01014", "pdf": "https://arxiv.org/pdf/2511.01014", "abs": "https://arxiv.org/abs/2511.01014", "authors": ["Bosi Wen", "Yilin Niu", "Cunxiang Wang", "Pei Ke", "Xiaoying Ling", "Ying Zhang", "Aohan Zeng", "Hongning Wang", "Minlie Huang"], "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation", "categories": ["cs.CL"], "comment": "21 pages, 5 figures", "summary": "Instruction following is a fundamental ability of Large Language Models\n(LLMs), requiring their generated outputs to follow multiple constraints\nimposed in input instructions. Numerous studies have attempted to enhance this\nability through preference optimization or reinforcement learning based on\nreward signals from LLM-as-a-Judge. However, existing evaluation models for\ninstruction following still possess many deficiencies, such as substantial\ncosts and unreliable assessments. To this end, we propose IF-CRITIC, an LLM\ncritic that can provide efficient and reliable assessments of constraint\nfollowing in the instructions. We first develop a checklist generator to\ndecompose instructions and generate constraint checklists. With the assistance\nof the checklists, we collect high-quality critique training data through a\nmulti-stage critique filtering mechanism and employ a constraint-level\npreference optimization method to train IF-CRITIC. Extensive experiments\ndemonstrate that the evaluation performance of IF-CRITIC can beat strong\nLLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable\nreward signals provided by IF-CRITIC, LLMs can achieve substantial performance\ngains in instruction-following optimization under lower computational overhead\ncompared to strong LLM critic baselines.", "AI": {"tldr": "IF-CRITIC是一个用于评估大语言模型指令遵循能力的LLM批评器，通过约束清单生成和多阶段过滤机制提供高效可靠的评估，性能优于现有的LLM-as-a-Judge基线模型。", "motivation": "现有的大语言模型指令遵循评估方法存在成本高昂和评估不可靠的问题，需要开发更高效可靠的评估工具。", "method": "开发约束清单生成器分解指令生成检查清单，通过多阶段批评过滤机制收集高质量训练数据，采用约束级偏好优化方法训练IF-CRITIC模型。", "result": "实验表明IF-CRITIC的评估性能超越了Deepseek-R1和o4-mini等强基线模型，能以更低计算开销为LLM提供可扩展的奖励信号。", "conclusion": "IF-CRITIC为解决指令遵循评估问题提供了有效解决方案，能够显著提升LLM在指令遵循优化中的性能表现。"}}
{"id": "2511.01396", "pdf": "https://arxiv.org/pdf/2511.01396", "abs": "https://arxiv.org/abs/2511.01396", "authors": ["Clément Yvernes", "Emilie Devijver", "Adèle H. Ribeiro", "Marianne Clausel--Lesourd", "Éric Gaussier"], "title": "Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering", "categories": ["cs.AI", "stat.ME"], "comment": "Accepted at The Thirty-ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS2025)", "summary": "Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes\nrepresent clusters of variables, and edges encode both cluster-level causal\nrelationships and dependencies arisen from unobserved confounding. C-DAGs\ndefine an equivalence class of acyclic causal graphs that agree on\ncluster-level relationships, enabling causal reasoning at a higher level of\nabstraction. However, when the chosen clustering induces cycles in the\nresulting C-DAG, the partition is deemed inadmissible under conventional C-DAG\nsemantics. In this work, we extend the C-DAG framework to support arbitrary\nvariable clusterings by relaxing the partition admissibility constraint,\nthereby allowing cyclic C-DAG representations. We extend the notions of\nd-separation and causal calculus to this setting, significantly broadening the\nscope of causal reasoning across clusters and enabling the application of\nC-DAGs in previously intractable scenarios. Our calculus is both sound and\natomically complete with respect to the do-calculus: all valid interventional\nqueries at the cluster level can be derived using our rules, each corresponding\nto a primitive do-calculus step.", "AI": {"tldr": "扩展C-DAG框架以支持任意变量聚类，允许循环C-DAG表示，并扩展d-分离和因果演算概念", "motivation": "传统C-DAG语义要求聚类必须产生无环图，这限制了在某些聚类选择下的应用范围", "method": "放宽分区可容许性约束，允许循环C-DAG表示，并扩展d-分离和因果演算规则", "result": "建立了在循环C-DAG设置下既可靠又原子完备的演算系统，所有有效的集群层面干预查询都可以推导", "conclusion": "显著拓宽了跨集群因果推理的范围，使C-DAG能够应用于先前难以处理的场景"}}
{"id": "2511.01016", "pdf": "https://arxiv.org/pdf/2511.01016", "abs": "https://arxiv.org/abs/2511.01016", "authors": ["Wenjin Liu", "Haoran Luo", "Xueyuan Lin", "Haoming Liu", "Tiesunlong Shen", "Jiapu Wang", "Rui Mao", "Erik Cambria"], "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recently, advanced large language models (LLMs) have emerged at an\nincreasingly rapid pace. However, when faced with complex problems, most users\nare often unable to provide accurate and effective prompts to interact with\nLLMs, thus limiting the performance of LLMs. To address this challenge, we\npropose Prompt-R1, an end-to-end reinforcement learning framework that uses a\nsmall-scale LLM to collaborate with large-scale LLMs, replacing user\ninteraction to solve problems better. This collaboration is cast as a\nmulti-turn prompt interaction, where the small-scale LLM thinks and generates\nprompts, and the large-scale LLM performs complex reasoning. A dual-constrained\nreward is designed to optimize for correctness, generation quality, and\nreasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports\nboth inference and training with various large-scale LLMs. Experiments on\nmultiple public datasets show that Prompt-R1 significantly outperforms baseline\nmodels across tasks. Our code is publicly available at\nhttps://github.com/QwenQKing/Prompt-R1.", "AI": {"tldr": "Prompt-R1是一个端到端的强化学习框架，使用小型LLM与大型LLM协作生成提示词，通过多轮提示交互提升复杂问题的解决能力", "motivation": "用户往往无法为大型语言模型提供准确有效的提示词，限制了模型性能的发挥", "method": "采用强化学习框架，小型LLM生成提示词，大型LLM进行复杂推理，设计双重约束奖励机制优化正确性、生成质量和推理准确性", "result": "在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型", "conclusion": "该框架提供了一个即插即用的解决方案，支持与各种大型LLM的推理和训练，有效提升了复杂问题解决能力"}}
{"id": "2511.01415", "pdf": "https://arxiv.org/pdf/2511.01415", "abs": "https://arxiv.org/abs/2511.01415", "authors": ["Amrapali Pednekar", "Álvaro Garrido-Pérez", "Yara Khaluf", "Pieter Simoens"], "title": "Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm", "categories": ["cs.AI"], "comment": "Accepted at CogInterp workshop @ NeurIPS 2025", "summary": "This study explores the interference in temporal processing within a\ndual-task paradigm from an artificial intelligence (AI) perspective. In this\ncontext, the dual-task setup is implemented as a simplified version of the\nOvercooked environment with two variations, single task (T) and dual task\n(T+N). Both variations involve an embedded time production task, but the dual\ntask (T+N) additionally involves a concurrent number comparison task. Two deep\nreinforcement learning (DRL) agents were separately trained for each of these\ntasks. These agents exhibited emergent behavior consistent with human timing\nresearch. Specifically, the dual task (T+N) agent exhibited significant\noverproduction of time relative to its single task (T) counterpart. This result\nwas consistent across four target durations. Preliminary analysis of neural\ndynamics in the agents' LSTM layers did not reveal any clear evidence of a\ndedicated or intrinsic timer. Hence, further investigation is needed to better\nunderstand the underlying time-keeping mechanisms of the agents and to provide\ninsights into the observed behavioral patterns. This study is a small step\ntowards exploring parallels between emergent DRL behavior and behavior observed\nin biological systems in order to facilitate a better understanding of both.", "AI": {"tldr": "本研究从AI角度探讨双任务范式中的时间处理干扰，通过Overcooked环境简化版训练DRL智能体，发现双任务智能体比单任务智能体显著高估时间，但未在LSTM层发现明确的时间编码机制", "motivation": "探索深度强化学习智能体在双任务范式中的时间处理行为，寻找与人类时间研究相似的涌现行为，促进对生物系统和AI系统时间处理机制的理解", "method": "使用简化版Overcooked环境，设计单任务(T)和双任务(T+N)两种变体，分别训练两个DRL智能体。单任务包含时间产生任务，双任务额外增加数字比较任务，分析智能体行为模式和LSTM层神经动力学", "result": "双任务智能体相对于单任务智能体表现出显著的时间高估现象，这一结果在四个目标时间间隔上保持一致。LSTM层初步分析未发现专门或内在的时间编码器证据", "conclusion": "研究为探索DRL涌现行为与生物系统行为之间的相似性迈出了一小步，需要进一步研究以理解智能体的潜在时间保持机制和观察到的行为模式"}}
{"id": "2511.01019", "pdf": "https://arxiv.org/pdf/2511.01019", "abs": "https://arxiv.org/abs/2511.01019", "authors": ["Bowen Chen", "Jayesh Gajbhar", "Gregory Dusek", "Rob Redmon", "Patrick Hogan", "Paul Liu", "DelWayne Bohnenstiehl", "Dongkuan", "Xu", "Ruoying He"], "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings", "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz.", "AI": {"tldr": "OceanAI是一个结合开源大语言模型与NOAA海洋数据实时访问的对话平台，通过API调用生成基于权威数据的可验证回答和可视化，解决了AI在科学领域产生幻觉的问题。", "motivation": "当前通用对话AI系统经常产生未经核实的\"幻觉\"，损害科学严谨性。需要开发能够整合权威数据源、确保输出可验证的AI系统。", "method": "整合开源大语言模型的自然语言流畅性与NOAA海洋数据的实时参数化访问。每个查询触发实时API调用，识别、解析并合成相关数据集为自然语言回答和数据可视化。", "result": "在与三种广泛使用的AI聊天产品盲测比较中，只有OceanAI能够提供NOAA来源的数据值和原始数据引用，其他系统要么拒绝回答，要么提供无支持的结果。", "conclusion": "OceanAI通过将输出基于可验证的观测数据，提高了透明度、可重复性和信任度，为海洋领域的AI辅助决策支持提供了可扩展框架。"}}
{"id": "2511.01425", "pdf": "https://arxiv.org/pdf/2511.01425", "abs": "https://arxiv.org/abs/2511.01425", "authors": ["Yuhang Huang", "Zekai Lin", "Fan Zhong", "Lei Liu"], "title": "Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis", "categories": ["cs.AI", "cs.CV", "I.2.6; I.2.10"], "comment": "12 pages, 3 figures. Under review at the Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2026", "summary": "Explanations for AI models in high-stakes domains like medicine often lack\nverifiability, which can hinder trust. To address this, we propose an\ninteractive agent that produces explanations through an auditable sequence of\nactions. The agent learns a policy to strategically seek external visual\nevidence to support its diagnostic reasoning. This policy is optimized using\nreinforcement learning, resulting in a model that is both efficient and\ngeneralizable. Our experiments show that this action-based reasoning process\nsignificantly improves calibrated accuracy, reducing the Brier score by 18\\%\ncompared to a non-interactive baseline. To validate the faithfulness of the\nagent's explanations, we introduce a causal intervention method. By masking the\nvisual evidence the agent chooses to use, we observe a measurable degradation\nin its performance ($\\Delta$Brier=+0.029), confirming that the evidence is\nintegral to its decision-making process. Our work provides a practical\nframework for building AI systems with verifiable and faithful reasoning\ncapabilities.", "AI": {"tldr": "提出一个交互式AI代理，通过可审计的行动序列生成可验证的解释，使用强化学习优化策略来寻求外部视觉证据支持诊断推理，显著提高校准准确性并验证解释的忠实性", "motivation": "解决高风险领域（如医学）中AI模型解释缺乏可验证性，阻碍用户信任的问题", "method": "使用强化学习训练代理策略，战略性地寻求外部视觉证据支持诊断推理，并通过因果干预方法（掩蔽所选视觉证据）验证解释忠实性", "result": "相比非交互式基线，Brier分数降低18%（校准准确性显著提升）；通过因果干预验证，掩蔽证据导致性能明显下降（ΔBrier=+0.029），证明证据对决策过程至关重要", "conclusion": "为构建具有可验证和忠实推理能力的AI系统提供了实用框架，增强了高风险领域中AI解释的可信度"}}
{"id": "2511.01046", "pdf": "https://arxiv.org/pdf/2511.01046", "abs": "https://arxiv.org/abs/2511.01046", "authors": ["Vedant Acharya", "Abhay Pisharodi", "Rishabh Mondal", "Mohammad Rafiuddin", "Nipun Batra"], "title": "VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics", "categories": ["cs.CL"], "comment": "4 Pages, 4 Figures", "summary": "Air pollution causes about 1.6 million premature deaths each year in India,\nyet decision makers struggle to turn dispersed data into decisions. Existing\ntools require expertise and provide static dashboards, leaving key policy\nquestions unresolved. We present VayuChat, a conversational system that answers\nnatural language questions on air quality, meteorology, and policy programs,\nand responds with both executable Python code and interactive visualizations.\nVayuChat integrates data from Central Pollution Control Board (CPCB) monitoring\nstations, state-level demographics, and National Clean Air Programme (NCAP)\nfunding records into a unified interface powered by large language models. Our\nlive demonstration will show how users can perform complex environmental\nanalytics through simple conversations, making data science accessible to\npolicymakers, researchers, and citizens. The platform is publicly deployed at\nhttps://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further\ninformation check out video uploaded on\nhttps://www.youtube.com/watch?v=d6rklL05cs4.", "AI": {"tldr": "VayuChat是一个基于大型语言模型的对话系统，能够通过自然语言回答空气质量、气象和政策项目相关问题，并生成Python代码和交互式可视化，帮助决策者进行环境数据分析。", "motivation": "印度每年因空气污染导致160万人过早死亡，但现有工具需要专业知识且只能提供静态仪表板，无法有效解决关键政策问题。", "method": "整合中央污染控制委员会监测站数据、州级人口统计数据和国家清洁空气计划资金记录，通过大型语言模型提供统一的对话界面。", "result": "开发了公开部署的VayuChat平台，用户可通过简单对话执行复杂的环境分析，使政策制定者、研究人员和公民都能进行数据科学分析。", "conclusion": "VayuChat通过对话式界面使环境数据分析更加可访问，有助于将分散的数据转化为有效的决策支持。"}}
{"id": "2511.01444", "pdf": "https://arxiv.org/pdf/2511.01444", "abs": "https://arxiv.org/abs/2511.01444", "authors": ["Huiting Huang", "Tieliang Gong", "Kai He", "Jialun Wu", "Erik Cambria", "Mengling Feng"], "title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis has received significant attention across\ndiverse research domains. Despite advancements in algorithm design, existing\napproaches suffer from two critical limitations: insufficient learning of\nnoise-contaminated unimodal data, leading to corrupted cross-modal\ninteractions, and inadequate fusion of multimodal representations, resulting in\ndiscarding discriminative unimodal information while retaining multimodal\nredundant information. To address these challenges, this paper proposes a\nDouble Information Bottleneck (DIB) strategy to obtain a powerful, unified\ncompact multimodal representation. Implemented within the framework of low-rank\nRenyi's entropy functional, DIB offers enhanced robustness against diverse\nnoise sources and computational tractability for high-dimensional data, as\ncompared to the conventional Shannon entropy-based methods. The DIB comprises\ntwo key modules: 1) learning a sufficient and compressed representation of\nindividual unimodal data by maximizing the task-relevant information and\ndiscarding the superfluous information, and 2) ensuring the discriminative\nability of multimodal representation through a novel attention bottleneck\nfusion mechanism. Consequently, DIB yields a multimodal representation that\neffectively filters out noisy information from unimodal data while capturing\ninter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,\nCH-SIMS, and MVSA-Single validate the effectiveness of our method. The model\nachieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score\non CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it\nshows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI\nrespectively.", "AI": {"tldr": "该论文提出了一种双信息瓶颈（DIB）策略来解决多模态情感分析中的噪声问题和表示融合不足问题，通过最大化任务相关信息并丢弃冗余信息，在多个数据集上取得了优异性能。", "motivation": "现有多模态情感分析方法存在两个关键局限：对噪声污染的单模态数据学习不足导致跨模态交互受损，以及多模态表示融合不充分导致丢弃判别性单模态信息而保留冗余信息。", "method": "提出双信息瓶颈（DIB）策略，在低秩Renyi熵函数框架下实现，包含两个模块：1）学习充分压缩的单模态表示，最大化任务相关信息；2）通过新型注意力瓶颈融合机制确保多模态表示的判别能力。", "result": "在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上验证有效性，CMU-MOSI上Acc-7指标达到47.4%，CH-SIMS上F1-score达到81.63%，比次优基线提升1.19%。在噪声环境下性能下降仅为0.36%和0.29%。", "conclusion": "DIB策略能够有效过滤单模态数据中的噪声信息，同时捕获模态间互补性，为多模态情感分析提供了强大且统一的紧凑表示方法。"}}
{"id": "2511.01053", "pdf": "https://arxiv.org/pdf/2511.01053", "abs": "https://arxiv.org/abs/2511.01053", "authors": ["Qing Ding", "Eric Hua Qing Zhang", "Felix Jozsa", "Julia Ive"], "title": "Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs", "categories": ["cs.CL"], "comment": "Submitted to EFMI Medical Informatics Europe 2026", "summary": "Large language models (LLMs) are increasingly used in healthcare, yet\nstandardised benchmarks for evaluating guideline-based clinical reasoning are\nmissing. This study introduces a validated dataset derived from publicly\navailable guidelines across multiple diagnoses. The dataset was created with\nthe help of GPT and contains realistic patient scenarios, as well as clinical\nquestions. We benchmark a range of recent popular LLMs to showcase the validity\nof our dataset. The framework supports systematic evaluation of LLMs' clinical\nutility and guideline adherence.", "AI": {"tldr": "本研究创建了一个基于临床指南的标准化评估数据集，用于评估大语言模型在医疗临床推理中的表现，并测试了多个流行LLM的临床效用和指南遵循能力。", "motivation": "大语言模型在医疗领域应用日益增多，但缺乏标准化的基于临床指南的评估基准，需要系统评估LLMs的临床推理能力。", "method": "利用GPT辅助创建从多诊断公开指南中提取的验证数据集，包含真实患者场景和临床问题，并对多个流行LLM进行基准测试。", "result": "开发了一个有效的评估框架和数据集，能够系统评估LLMs的临床效用和指南遵循程度。", "conclusion": "该研究填补了LLM医疗评估的空白，为标准化评估临床推理能力提供了重要工具，支持医疗AI系统的质量保证。"}}
{"id": "2511.01445", "pdf": "https://arxiv.org/pdf/2511.01445", "abs": "https://arxiv.org/abs/2511.01445", "authors": ["ChengZhang Yu", "YingRu He", "Hongyan Cheng", "nuo Cheng", "Zhixing Liu", "Dongxu Mu", "Zhangrui Shen", "Zhanpeng Jin"], "title": "From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation", "categories": ["cs.AI"], "comment": "14pages, 7 figures, 7 tables", "summary": "Global healthcare systems face critical challenges from increasing patient\nvolumes and limited consultation times, with primary care visits averaging\nunder 5 minutes in many countries. While pre-consultation processes\nencompassing triage and structured history-taking offer potential solutions,\nthey remain limited by passive interaction paradigms and context management\nchallenges in existing AI systems. This study introduces a hierarchical\nmulti-agent framework that transforms passive medical AI systems into proactive\ninquiry agents through autonomous task orchestration. We developed an\neight-agent architecture with centralized control mechanisms that decomposes\npre-consultation into four primary tasks: Triage ($T_1$), History of Present\nIllness collection ($T_2$), Past History collection ($T_3$), and Chief\nComplaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13\ndomain-specific subtasks. Evaluated on 1,372 validated electronic health\nrecords from a Chinese medical platform across multiple foundation models\n(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for\nprimary department triage and 80.5% for secondary department classification,\nwith task completion rates reaching 98.2% using agent-driven scheduling versus\n93.1% with sequential processing. Clinical quality scores from 18 physicians\naveraged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and\n4.69 for Past History on a 5-point scale, with consultations completed within\n12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic\narchitecture maintained high performance across different foundation models\nwhile preserving data privacy through local deployment, demonstrating the\npotential for autonomous AI systems to enhance pre-consultation efficiency and\nquality in clinical settings.", "AI": {"tldr": "提出分层多智能体框架，将被动医疗AI系统转变为主动问诊代理，通过自主任务编排提高预诊效率和质量，在多个基础模型上验证了高准确率和临床质量评分", "motivation": "全球医疗系统面临患者数量增加和咨询时间有限的挑战，现有AI系统的被动交互范式和上下文管理问题限制了预诊流程的效果", "method": "开发了八智能体分层架构，将预诊分解为分诊、现病史采集、既往史采集和主诉生成四个主要任务，进一步细分为13个领域特定子任务，采用集中控制机制进行自主任务编排", "result": "在1372份电子健康记录上测试，主要科室分诊准确率87.0%，次要科室分类准确率80.5%，任务完成率98.2%，临床质量评分平均4.5+（5分制），咨询轮次控制在12.7-16.9轮", "conclusion": "该模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，展示了自主AI系统在提高临床预诊效率和质量方面的潜力"}}
{"id": "2511.01066", "pdf": "https://arxiv.org/pdf/2511.01066", "abs": "https://arxiv.org/abs/2511.01066", "authors": ["Stephan Oepen", "Nikolay Arefev", "Mikko Aulamo", "Marta Bañón", "Maja Buljan", "Laurie Burchell", "Lucas Charpentier", "Pinzhen Chen", "Mariya Fedorova", "Ona de Gibert", "Barry Haddow", "Jan Hajič", "Jindrič Helcl", "Andrey Kutuzov", "Zihao Li", "Risto Luukkonen", "Bhavitvya Malik", "Vladislav Mikhailov", "Amanda Myntti", "Dayyán O'Brien", "Lucie Poláková", "Sampo Pyysalo", "Gema Ramírez Sánchez", "Janine Siewert", "Pavel Stepachev", "Jörg Tiedemann", "Teemu Vahtola", "Fedor Vitiugin", "Tea Vojtěchová", "Jaume Zaragoza"], "title": "HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models", "categories": ["cs.CL"], "comment": null, "summary": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. At 30 trillion tokens, this is likely the largest\ngenerally available multilingual collection of LLM pre-training data. These\ndatasets are derived from web crawls from different sources and accompanied\nwith a complete, open-source pipeline for document selection from web archives,\ntext extraction from HTML, language identification for noisy texts, exact and\nnear-deduplication, annotation with, among others, register labels, text\nquality estimates, and personally identifiable information; and final selection\nand filtering. We report on data quality probes through contrastive and\nanalytical statistics, through manual inspection of samples for 24 languages,\nand through end-to-end evaluation of various language model architectures\ntrained on this data. For multilingual LLM evaluation, we provide a\ncomprehensive collection of benchmarks for nine European languages, with\nspecial emphasis on natively created tasks, mechanisms to mitigate prompt\nsensitivity, and refined normalization and aggregation of scores. Additionally,\nwe train and evaluate a family of 57 monolingual encoder-decoder models, as\nwell as a handful of monolingual GPT-like reference models. Besides the\nmonolingual data and models, we also present a very large collection of\nparallel texts automatically mined from this data, together with a novel\nparallel corpus synthesized via machine translation.", "AI": {"tldr": "该论文介绍了一个提供近200种语言的大规模、高质量、丰富标注文本数据集的开放计划，包含30万亿token的多语言LLM预训练数据，并提供了完整的数据处理流程和评估基准。", "motivation": "为多语言自然语言处理研究提供开放、大规模、高质量的多语言预训练数据集，解决当前多语言数据资源匮乏和质量不一的问题。", "method": "从不同来源的网络爬虫数据构建数据集，提供完整的开源处理流程：包括文档选择、HTML文本提取、噪声文本语言识别、去重处理、多维度标注（语域标签、文本质量评估、个人信息识别等），以及最终的选择过滤。", "result": "构建了包含30万亿token的多语言数据集，可能是目前最大的公开多语言LLM预训练数据集合；提供了24种语言的数据质量评估；训练并评估了57种单语编码器-解码器模型和GPT类参考模型；生成了大规模自动挖掘的平行语料库。", "conclusion": "该计划为多语言NLP研究提供了宝贵的数据资源和技术基础设施，通过系统化的数据处理流程和全面的评估体系，为多语言语言模型的发展奠定了重要基础。"}}
{"id": "2511.01527", "pdf": "https://arxiv.org/pdf/2511.01527", "abs": "https://arxiv.org/abs/2511.01527", "authors": ["Hanwen Xu", "Xuyao Huang", "Yuzhe Liu", "Kai Yu", "Zhijie Deng"], "title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM) agents have exhibited strong problem-solving\ncompetence across domains like research and coding. Yet, it remains\nunderexplored whether LLM agents can tackle compounding real-world problems\nthat require a diverse set of tools to complete. Given a broad, heterogeneous\ntool repository, LLM agents must not only select appropriate tools based on\ntask planning analysis but also strategically schedule the execution order to\nensure efficiency. This paper introduces TPS-Bench to benchmark the ability of\nLLM agents in solving such problems that demand Tool Planning and Scheduling.\nTPS-Bench collects 200 compounding tasks of two difficulty levels, based on a\ntool repository containing hundreds of model context protocol (MCP) tools. In\nparticular, each task is composed of multiple subtasks, such as web search, map\nnavigation, calendar checking, etc., and each subtask can be completed by a\nbasic tool. Our evaluation emphasizes both task completion rate and efficiency.\nThe empirical studies on popular closed-source and open-source LLMs indicate\nthat most models can perform reasonable tool planning, but differ in\nscheduling. For example, GLM-4.5 achieves an outperforming task completion rate\nof 64.72% with extensive sequential tool calls, hence suffering from\nsignificantly long execution time. By contrast, GPT-4o prioritizes parallel\ntool calls but achieves only a 45.08% completion rate. Considering\nreinforcement learning (RL) can be a viable way to improve the scheduling\nefficiency without compromising performance, we perform an initial study on\nQwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in\ntask completion rate based on rarely 100 RL training samples. Our code is\navailable https://github.com/hanwenxu1/mcp-agent.", "AI": {"tldr": "本文介绍了TPS-Bench基准测试，用于评估LLM代理在需要工具规划与调度的复合任务中的表现，发现现有模型在工具规划方面表现合理但在调度效率上存在差异，并通过强化学习实现了效率提升。", "motivation": "评估LLM代理在需要多种工具协作的复合现实问题中的能力，特别是在给定异构工具库的情况下，代理不仅需要选择合适的工具，还需要战略性地调度执行顺序以确保效率。", "method": "构建TPS-Bench基准，包含200个复合任务和数百个MCP工具；评估主流闭源和开源LLM的任务完成率和效率；使用强化学习对Qwen3-1.7B进行调度优化研究。", "result": "GLM-4.5获得64.72%的任务完成率但执行时间长；GPT-4o优先并行调用但完成率仅45.08%；强化学习使Qwen3-1.7B执行时间减少14%，任务完成率提升6%。", "conclusion": "LLM代理在工具规划方面表现良好但在调度策略上存在显著差异，强化学习是提升调度效率的有效方法，未来需要更高效的调度策略来平衡完成率和执行时间。"}}
{"id": "2511.01090", "pdf": "https://arxiv.org/pdf/2511.01090", "abs": "https://arxiv.org/abs/2511.01090", "authors": ["Vlad Negoita", "Mihai Masala", "Traian Rebedea"], "title": "Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently exploded in popularity, often\nmatching or outperforming human abilities on many tasks. One of the key factors\nin training LLMs is the availability and curation of high-quality data. Data\nquality is especially crucial for under-represented languages, where\nhigh-quality corpora are scarce. In this work we study the characteristics and\ncoverage of Romanian pretraining corpora and we examine how they differ from\nEnglish data. By training a lightweight multitask model on carefully\nLLM-annotated Romanian texts, we are able to analyze and perform multi-level\nfiltering (e.g., educational value, topic, format) to generate high-quality\npretraining datasets. Our experiments show noteworthy trends in the topics\npresent in Romanian and English data, while also proving the effectiveness of\nfiltering data through improved LLM pretraining performance across multiple\nbenchmarks.", "AI": {"tldr": "本研究分析了罗马尼亚语预训练语料库的特征和覆盖范围，通过与英语数据对比，使用LLM标注的罗马尼亚文本进行多级过滤，生成高质量预训练数据集，并验证了过滤方法的有效性。", "motivation": "大型语言模型训练的关键在于高质量数据的可用性和筛选，而罗马尼亚语等代表性不足的语言缺乏高质量语料库，需要研究其数据特征和过滤方法。", "method": "训练一个轻量级多任务模型，对LLM标注的罗马尼亚文本进行多级过滤（如教育价值、主题、格式），生成高质量预训练数据集，并与英语数据进行对比分析。", "result": "实验显示了罗马尼亚语和英语数据在主题分布上的显著差异，同时通过多个基准测试证明了数据过滤能有效提升LLM预训练性能。", "conclusion": "多级数据过滤方法能有效提升罗马尼亚语等低资源语言的预训练数据质量，为其他低资源语言的LLM训练提供了可行的数据优化方案。"}}
{"id": "2511.01550", "pdf": "https://arxiv.org/pdf/2511.01550", "abs": "https://arxiv.org/abs/2511.01550", "authors": ["Ujjwal Sharma", "Stevan Rudinac", "Ana Mićković", "Willemijn van Dolen", "Marcel Worring"], "title": "Analyzing Sustainability Messaging in Large-Scale Corporate Social Media", "categories": ["cs.AI"], "comment": null, "summary": "In this work, we introduce a multimodal analysis pipeline that leverages\nlarge foundation models in vision and language to analyze corporate social\nmedia content, with a focus on sustainability-related communication. Addressing\nthe challenges of evolving, multimodal, and often ambiguous corporate messaging\non platforms such as X (formerly Twitter), we employ an ensemble of large\nlanguage models (LLMs) to annotate a large corpus of corporate tweets on their\ntopical alignment with the 17 Sustainable Development Goals (SDGs). This\napproach avoids the need for costly, task-specific annotations and explores the\npotential of such models as ad-hoc annotators for social media data that can\nefficiently capture both explicit and implicit references to sustainability\nthemes in a scalable manner. Complementing this textual analysis, we utilize\nvision-language models (VLMs), within a visual understanding framework that\nuses semantic clusters to uncover patterns in visual sustainability\ncommunication. This integrated approach reveals sectoral differences in SDG\nengagement, temporal trends, and associations between corporate messaging,\nenvironmental, social, governance (ESG) risks, and consumer engagement. Our\nmethods-automatic label generation and semantic visual clustering-are broadly\napplicable to other domains and offer a flexible framework for large-scale\nsocial media analysis.", "AI": {"tldr": "本文提出了一种多模态分析管道，利用大型视觉和语言基础模型分析企业社交媒体内容，特别关注可持续发展相关传播，通过自动标注和语义视觉聚类方法揭示行业差异、时间趋势以及企业信息与ESG风险的关联。", "motivation": "解决企业社交媒体内容（如X平台）中不断演变、多模态且常具模糊性的可持续发展信息分析挑战，避免昂贵的人工标注需求。", "method": "使用大型语言模型(LLMs)集成标注企业推文与17个可持续发展目标(SDGs)的主题对齐；利用视觉语言模型(VLMs)通过语义聚类分析视觉可持续性传播模式。", "result": "揭示了不同行业在SDG参与度上的差异、时间趋势以及企业信息与环境、社会、治理(ESG)风险及消费者参与度之间的关联。", "conclusion": "提出的自动标签生成和语义视觉聚类方法具有广泛适用性，为大规模社交媒体分析提供了灵活框架，证明了基础模型作为临时标注器的潜力。"}}
{"id": "2511.01101", "pdf": "https://arxiv.org/pdf/2511.01101", "abs": "https://arxiv.org/abs/2511.01101", "authors": ["Marek Strong", "Andreas Vlachos"], "title": "TSVer: A Benchmark for Fact Verification Against Time-Series Evidence", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Reasoning over temporal and numerical data, such as time series, is a crucial\naspect of fact-checking. While many systems have recently been developed to\nhandle this form of evidence, their evaluation remains limited by existing\ndatasets, which often lack structured evidence, provide insufficient\njustifications for verdicts, or rely on synthetic claims. In this paper, we\nintroduce TSVer, a new benchmark dataset for fact verification focusing on\ntemporal and numerical reasoning with time-series evidence. TSVer contains 287\nreal-world claims sourced from 38 fact-checking organizations and a curated\ndatabase of 400 time series covering diverse domains. Each claim is annotated\nwith time frames across all pertinent time series, along with a verdict and\njustifications reflecting how the evidence is used to reach the verdict. Using\nan LLM-assisted multi-step annotation process, we improve the quality of our\nannotations and achieve an inter-annotator agreement of kappa=0.745 on\nverdicts. We also develop a baseline for verifying claims against time-series\nevidence and show that even the state-of-the-art reasoning models like\nGemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score\non verdicts and an Ev2R score of 48.63 on verdict justifications.", "AI": {"tldr": "TSVer是一个新的时间序列事实核查基准数据集，包含287个真实世界声明和400个时间序列数据，专注于时间和数值推理任务，通过LLM辅助的多步标注过程提高标注质量。", "motivation": "现有的事实核查数据集在时间序列证据方面存在局限：缺乏结构化证据、判决理由不足或依赖合成声明，需要更高质量的时间序列事实核查基准。", "method": "收集38个事实核查机构的287个真实声明和400个跨领域时间序列数据，采用LLM辅助的多步标注流程，为每个声明标注时间范围、判决和证据使用理由。", "result": "实现了kappa=0.745的标注者间一致性，基线实验显示即使是Gemini-2.5-Pro等最先进模型在时间序列推理上也面临挑战（准确率63.37%，Ev2R得分48.63）。", "conclusion": "TSVer数据集填补了时间序列事实核查的空白，展示了当前AI模型在时间和数值推理方面的局限性，为未来研究提供了高质量的基准。"}}
{"id": "2511.01581", "pdf": "https://arxiv.org/pdf/2511.01581", "abs": "https://arxiv.org/abs/2511.01581", "authors": ["Chengzhang Yu", "Zening Lu", "Chenyang Zheng", "Chiyue Wang", "Yiming Zhang", "Zhanpeng Jin"], "title": "ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks", "categories": ["cs.AI"], "comment": "12pages, 4figures", "summary": "Large language models suffer from knowledge staleness and lack of\ninterpretability due to implicit knowledge storage across entangled network\nparameters, preventing targeted updates and reasoning transparency. We propose\nExplicitLM, a novel architecture featuring a million-scale external memory bank\nstoring human-readable knowledge as token sequences, enabling direct inspection\nand modification. We design a differentiable two-stage retrieval mechanism with\nefficient coarse-grained filtering via product key decomposition (reducing\ncomplexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot\n|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.\nInspired by dual-system cognitive theory, we partition knowledge into frozen\nexplicit facts (20%) and learnable implicit patterns (80%), maintained through\nExponential Moving Average updates for stability. ExplicitLM achieves up to\n43.67% improvement on knowledge-intensive tasks versus standard Transformers,\nwith 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows\nstrong correlations between memory retrieval and performance, with correct\npredictions achieving 49% higher hit rates. Unlike RAG systems with frozen\nretrieval, our jointly optimized architecture demonstrates that interpretable,\nupdatable models can maintain competitive performance while providing\nunprecedented knowledge transparency.", "AI": {"tldr": "ExplicitLM是一种新型语言模型架构，通过外部记忆库存储可读知识，实现知识透明化和定向更新，在知识密集型任务上比标准Transformer提升43.67%", "motivation": "解决大语言模型知识陈旧和缺乏可解释性的问题，由于隐式知识存储在网络参数中，无法进行定向更新和推理透明化", "method": "采用百万规模外部记忆库存储可读知识，设计可微分两阶段检索机制（产品键分解粗过滤+Gumbel-Softmax细匹配），基于双系统认知理论划分冻结显性事实（20%）和可学习隐式模式（80%）", "result": "知识密集型任务性能提升43.67%，小样本场景（1万样本）提升3.62倍，正确预测的记忆命中率高49%", "conclusion": "可解释、可更新的模型在保持竞争力的同时能提供前所未有的知识透明度，优于固定检索的RAG系统"}}
{"id": "2511.01166", "pdf": "https://arxiv.org/pdf/2511.01166", "abs": "https://arxiv.org/abs/2511.01166", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Minghua He", "Leyi Pan", "Zhaoyang Liu", "Bolin Ding", "Ying Li"], "title": "MicroRemed: Benchmarking LLMs in Microservices Remediation", "categories": ["cs.CL", "cs.SE", "68T50", "I.2.7"], "comment": "24 pages, 13 figures, 5 tables", "summary": "Large Language Models (LLMs) integrated with agent-based reasoning frameworks\nhave recently shown strong potential for autonomous decision-making and\nsystem-level operations. One promising yet underexplored direction is\nmicroservice remediation, where the goal is to automatically recover faulty\nmicroservice systems. Existing approaches, however, still rely on human-crafted\nprompts from Site Reliability Engineers (SREs), with LLMs merely converting\ntextual instructions into executable code. To advance research in this area, we\nintroduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end\nmicroservice remediation, where models must directly generate executable\nAnsible playbooks from diagnosis reports to restore system functionality. We\nfurther propose ThinkRemed, a multi-agent framework that emulates the\nreflective and perceptive reasoning of SREs. Experimental results show that\nMicroRemed presents substantial challenges to current LLMs, while ThinkRemed\nimproves end-to-end remediation performance through iterative reasoning and\nsystem reflection. The benchmark is available at\nhttps://github.com/LLM4AIOps/MicroRemed.", "AI": {"tldr": "该论文提出了MicroRemed基准和ThinkRemed多智能体框架，用于评估和改进LLM在端到端微服务修复中的性能，通过模拟SRE的反思性推理来解决现有方法依赖人工提示的局限性。", "motivation": "当前基于LLM的微服务修复方法仍然依赖人工设计的提示，LLM仅将文本指令转换为可执行代码，缺乏自主决策能力，需要开发端到端的自动化修复基准和框架。", "method": "提出了MicroRemed基准评估LLM直接从诊断报告生成可执行Ansible playbook的能力；设计了ThinkRemed多智能体框架，模拟SRE的反思和感知推理过程，通过迭代推理和系统反思提升修复性能。", "result": "实验结果表明，MicroRemed基准对现有LLM构成显著挑战，而ThinkRemed框架通过迭代推理显著提高了端到端修复性能。", "conclusion": "该研究为LLM在微服务自动化修复领域提供了首个端到端评估基准，提出的多智能体推理框架有效提升了LLM的自主决策能力，推动了该领域的研究发展。"}}
{"id": "2511.01639", "pdf": "https://arxiv.org/pdf/2511.01639", "abs": "https://arxiv.org/abs/2511.01639", "authors": ["Sicheng Wang", "Shuhao Chen", "Jingran Zhou", "Chengyi Tu"], "title": "IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization", "categories": ["cs.AI"], "comment": "26pages,6figures", "summary": "Global food trade plays a crucial role in ensuring food security and\nmaintaining supply chain stability. However, its network structure evolves\ndynamically under the influence of geopolitical, economic, and environmental\nfactors, making it challenging to model and predict future trade links.\nEffectively capturing temporal patterns in food trade networks is therefore\nessential for improving the accuracy and robustness of link prediction. This\nstudy introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed\nto model evolving trade structures and predict future links in global food\ntrade networks. To the best of our knowledge, this is the first work to apply\ndynamic graph neural networks to this domain, significantly enhancing\npredictive performance. Building upon the original IVGAE framework, the\nproposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture\nthe temporal evolution of trade networks, jointly modeling short-term\nfluctuations and long-term structural dependencies. A momentum-based structural\nmemory mechanism further improves predictive stability and performance. In\naddition, Bayesian optimization is used to automatically tune key\nhyperparameters, enhancing generalization across diverse trade scenarios.\nExtensive experiments on five crop-specific datasets demonstrate that\nIVGAE-TAMA substantially outperforms the static IVGAE and other dynamic\nbaselines by effectively modeling temporal dependencies, while Bayesian\noptimization further boosts performance in IVGAE-TAMA-BO. These results\nhighlight the proposed framework as a robust and scalable solution for\nstructural prediction in global trade networks, with strong potential for\napplications in food security monitoring and policy decision support.", "AI": {"tldr": "IVGAE-TAMA-BO是一个新颖的动态图神经网络，通过贸易感知动量聚合器和贝叶斯优化，显著提升了全球粮食贸易网络的链路预测性能。", "motivation": "全球粮食贸易网络受地缘政治、经济和环境因素影响而动态演变，传统方法难以有效建模和预测未来贸易链接，需要捕捉时间模式来提高预测准确性。", "method": "基于IVGAE框架，引入贸易感知动量聚合器(TAMA)捕捉贸易网络的时间演化，结合短期波动和长期结构依赖，使用动量结构记忆机制提升稳定性，并通过贝叶斯优化自动调参。", "result": "在五个作物特定数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线模型，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。", "conclusion": "该框架为全球贸易网络结构预测提供了稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大应用潜力。"}}
{"id": "2511.01181", "pdf": "https://arxiv.org/pdf/2511.01181", "abs": "https://arxiv.org/abs/2511.01181", "authors": ["Emaad Manzoor", "Eva Ascarza", "Oded Netzer"], "title": "Learning When to Quit in Sales Conversations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Salespeople frequently face the dynamic screening decision of whether to\npersist in a conversation or abandon it to pursue the next lead. Yet, little is\nknown about how these decisions are made, whether they are efficient, or how to\nimprove them. We study these decisions in the context of high-volume outbound\nsales where leads are ample, but time is scarce and failure is common. We\nformalize the dynamic screening decision as an optimal stopping problem and\ndevelop a generative language model-based sequential decision agent - a\nstopping agent - that learns whether and when to quit conversations by\nimitating a retrospectively-inferred optimal stopping policy. Our approach\nhandles high-dimensional textual states, scales to large language models, and\nworks with both open-source and proprietary language models. When applied to\ncalls from a large European telecommunications firm, our stopping agent reduces\nthe time spent on failed calls by 54% while preserving nearly all sales;\nreallocating the time saved increases expected sales by up to 37%. Upon\nexamining the linguistic cues that drive salespeople's quitting decisions, we\nfind that they tend to overweight a few salient expressions of consumer\ndisinterest and mispredict call failure risk, suggesting cognitive bounds on\ntheir ability to make real-time conversational decisions. Our findings\nhighlight the potential of artificial intelligence algorithms to correct\ncognitively-bounded human decisions and improve salesforce efficiency.", "AI": {"tldr": "本文开发了一个基于语言模型的停止代理，用于优化销售对话中的动态筛选决策，通过模仿最优停止策略显著减少失败通话时间并提升销售效率", "motivation": "销售人员在动态筛选决策中面临是否继续对话的挑战，但现有研究对这些决策的效率和改进方法了解甚少，特别是在线索充足但时间稀缺的高频外呼销售环境中", "method": "将动态筛选决策形式化为最优停止问题，开发基于生成语言模型的序列决策代理（停止代理），通过模仿回顾性推断的最优停止策略来学习何时放弃对话", "result": "在欧洲电信公司的实际应用中，停止代理将失败通话时间减少54%，同时几乎保留所有销售额；重新分配节省的时间可使预期销售额增加高达37%", "conclusion": "人工智能算法能够纠正认知受限的人类决策，提高销售团队效率，研究发现了销售人员过度依赖少数明显的不感兴趣表达并错误预测通话失败风险的认知局限"}}
{"id": "2511.01668", "pdf": "https://arxiv.org/pdf/2511.01668", "abs": "https://arxiv.org/abs/2511.01668", "authors": ["Yueqing Xi", "Yifan Bai", "Huasen Luo", "Weiliang Wen", "Hui Liu", "Haoliang Li"], "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics", "categories": ["cs.AI"], "comment": null, "summary": "As artificial intelligence permeates judicial forensics, ensuring the\nveracity and traceability of legal question answering (QA) has become critical.\nConventional large language models (LLMs) are prone to hallucination, risking\nmisleading guidance in legal consultation, while static knowledge bases\nstruggle to keep pace with frequently updated statutes and case law. We present\na hybrid legal QA agent tailored for judicial settings that integrates\nretrieval-augmented generation (RAG) with multi-model ensembling to deliver\nreliable, auditable, and continuously updatable counsel. The system prioritizes\nretrieval over generation: when a trusted legal repository yields relevant\nevidence, answers are produced via RAG; otherwise, multiple LLMs generate\ncandidates that are scored by a specialized selector, with the top-ranked\nanswer returned. High-quality outputs then undergo human review before being\nwritten back to the repository, enabling dynamic knowledge evolution and\nprovenance tracking. Experiments on the Law\\_QA dataset show that our hybrid\napproach significantly outperforms both a single-model baseline and a vanilla\nRAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm\nthe complementary contributions of retrieval prioritization, model ensembling,\nand the human-in-the-loop update mechanism. The proposed system demonstrably\nreduces hallucination while improving answer quality and legal compliance,\nadvancing the practical landing of media forensics technologies in judicial\nscenarios.", "AI": {"tldr": "提出了一种混合法律问答代理，结合检索增强生成和多模型集成，为司法场景提供可靠、可审计且可更新的法律咨询服务，显著减少幻觉并提高答案质量", "motivation": "传统大语言模型容易产生幻觉，静态知识库难以跟上法律条文和案例的频繁更新，需要确保法律问答的真实性和可追溯性", "method": "采用检索优先策略：首先从可信法律库检索证据，若有相关证据则通过RAG生成答案，否则使用多个LLM生成候选答案并由专业选择器评分返回最优答案，高质量输出经人工审核后写回知识库", "result": "在Law_QA数据集上，混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程，消融实验证实了检索优先、模型集成和人工更新机制的有效性", "conclusion": "该系统显著减少了幻觉，提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地应用"}}
{"id": "2511.01187", "pdf": "https://arxiv.org/pdf/2511.01187", "abs": "https://arxiv.org/abs/2511.01187", "authors": ["Muhammed Saeed", "Muhammad Abdul-mageed", "Shady Shehata"], "title": "Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are widely deployed for open-ended\ncommunication, yet most bias evaluations still rely on English,\nclassification-style tasks. We introduce DebateBias-8K, a new multilingual,\ndebate-style benchmark designed to reveal how narrative bias appears in\nrealistic generative settings. Our dataset includes 8,400 structured debate\nprompts spanning four sensitive domains: women's rights, socioeconomic\ndevelopment, terrorism, and religion, across seven languages ranging from\nhigh-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).\nUsing four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we\ngenerate and automatically classify over 100,000 responses. Results show that\nall models reproduce entrenched stereotypes despite safety alignment: Arabs are\noverwhelmingly linked to terrorism and religion (>=95%), Africans to\nsocioeconomic \"backwardness\" (up to <=77%), and Western groups are consistently\nframed as modern or progressive. Biases grow sharply in lower-resource\nlanguages, revealing that alignment trained primarily in English does not\ngeneralize globally. Our findings highlight a persistent divide in multilingual\nfairness: current alignment methods reduce explicit toxicity but fail to\nprevent biased outputs in open-ended contexts. We release our DebateBias-8K\nbenchmark and analysis framework to support the next generation of multilingual\nbias evaluation and safer, culturally inclusive model alignment.", "AI": {"tldr": "DebateBias-8K是一个新的多语言辩论式基准测试，用于评估大语言模型在生成式对话中的叙事偏见，发现主流模型在不同语言中持续存在刻板印象偏见", "motivation": "现有偏见评估主要依赖英语分类任务，无法揭示生成式对话环境中的真实偏见表现，需要多语言、辩论式的评估方法", "method": "构建包含8,400个结构化辩论提示的数据集，覆盖4个敏感领域和7种语言，使用4个旗舰模型生成10万+回复并进行自动分类", "result": "所有模型都再现了根深蒂固的刻板印象：阿拉伯人与恐怖主义和宗教强关联(≥95%)，非洲人与社会经济落后相关(≤77%)，西方群体被描绘为现代进步。低资源语言中偏见更严重", "conclusion": "当前的安全对齐方法主要基于英语训练，无法在全球范围内泛化，需要开发新一代多语言偏见评估和文化包容的模型对齐方法"}}
{"id": "2511.01824", "pdf": "https://arxiv.org/pdf/2511.01824", "abs": "https://arxiv.org/abs/2511.01824", "authors": ["Yuetai Li", "Huseyin A Inan", "Xiang Yue", "Wei-Ning Chen", "Lukas Wutschitz", "Janardhan Kulkarni", "Radha Poovendran", "Robert Sim", "Saravan Rajmohan"], "title": "Simulating Environments with Reasoning Models for Agent Training", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "LLM agents excel in compact environments requiring deep reasoning but remain\nbrittle when operating in broader, more complex contexts that demand robustness\nacross diverse tools and schemas. Building bespoke environments for training is\nheavy, brittle, and limits progress. In this paper, we demonstrate that LLMs\ncan simulate realistic environment feedback without access to actual testbed\ndata or APIs. Inspired by this capability, we propose two frameworks:\nSimia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets\ninto diverse trajectories in an environment-agnostic manner, and Simia-RL, a\nframework that enables RL training without real environment implementations\nthrough LLM-simulated feedback. Fine-tuning open models yields consistent\nimprovements across multiple benchmarks, surpassing GPT-4o and approaching\no4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable\nagent training without environment engineering, replacing heavy and brittle\nimplementations with flexible LLM-based simulation.", "AI": {"tldr": "LLM代理在复杂环境中表现脆弱，需要大量环境工程。本文提出Simia-SFT和Simia-RL两个框架，利用LLM模拟环境反馈，无需真实测试数据或API，实现可扩展的智能体训练。", "motivation": "LLM代理在需要跨多样工具和模式的复杂环境中表现脆弱，构建定制训练环境成本高且限制进展，需要更灵活的训练方法。", "method": "提出Simia-SFT：通过放大小型种子集生成多样化轨迹的SFT数据；Simia-RL：通过LLM模拟反馈实现无需真实环境的RL训练。", "result": "微调开源模型在多个基准测试中表现一致提升，超越GPT-4o并在τ²-Bench上接近o4-mini性能。", "conclusion": "Simia框架通过LLM模拟替代繁重的环境工程，实现了无需环境实现的灵活、可扩展的智能体训练方案。"}}
{"id": "2511.01188", "pdf": "https://arxiv.org/pdf/2511.01188", "abs": "https://arxiv.org/abs/2511.01188", "authors": ["Lvhua Wu", "Xuefeng Jiang", "Sheng Sun", "Tian Wen", "Yuwei Wang", "Min Liu"], "title": "ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid spread of fake news threatens social stability and public trust,\nrendering its detection an imperative research priority. Although large\nlanguage models (LLMs) excel at numerous natural language processing tasks with\ntheir remarkable contextual understanding and extensive prior knowledge, the\ntime-bounded knowledge coverage and tendency for generating hallucination\ncontent reduce their reliability when handling fast-evolving news streams.\nFurthermore, models trained on existing static datasets also often lack the\ngeneralization needed for emerging news topics. To address these challenges, we\npropose ZoFia, a novel two-stage zero-shot fake news detection framework.\nFirst, we introduce Hierarchical Salience to quantify the importance of\nentities in the news content, and propose the SC-MMR algorithm to effectively\nselect an informative and diverse set of keywords that serve as queries for\nretrieving up-to-date external evidence. Subsequently, a multi LLM interactive\nsystem, in which each agent assumes a distinct role, performs multi-view\ncollaborative analysis and adversarial debate over the news text and its\nrelated information, and finally produces an interpretable and robust judgment.\nComprehensive experiments on two public datasets demonstrate that ZoFia\nobviously outperforms existing zero-shot baselines and most of few-shot\nmethods. Our codes will be open-sourced to facilitate related communities.", "AI": {"tldr": "ZoFia是一个新颖的两阶段零样本假新闻检测框架，通过分层显著性量化实体重要性，使用SC-MMR算法选择关键词检索最新外部证据，然后通过多LLM交互系统进行多视图协作分析和对抗性辩论，最终产生可解释的稳健判断。", "motivation": "假新闻快速传播威胁社会稳定和公众信任，但现有大语言模型存在知识覆盖时间限制、幻觉内容生成问题，以及在静态数据集上训练的模型缺乏对新兴新闻主题的泛化能力。", "method": "1. 分层显著性量化新闻内容中实体的重要性；2. SC-MMR算法选择信息丰富且多样化的关键词作为查询检索最新外部证据；3. 多LLM交互系统，每个代理承担不同角色，进行多视图协作分析和对抗性辩论。", "result": "在两个公共数据集上的综合实验表明，ZoFia明显优于现有的零样本基线方法和大多数少样本方法。", "conclusion": "ZoFia框架有效解决了假新闻检测中的时效性和泛化性问题，通过外部证据检索和多LLM协作分析提高了检测的准确性和鲁棒性，代码将开源以促进相关社区发展。"}}
{"id": "2511.01191", "pdf": "https://arxiv.org/pdf/2511.01191", "abs": "https://arxiv.org/abs/2511.01191", "authors": ["Ru Wang", "Wei Huang", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Test-time reinforcement learning (TTRL) offers a label-free paradigm for\nadapting models using only synthetic signals at inference, but its success\nhinges on constructing reliable learning signals. Standard approaches such as\nmajority voting often collapse to spurious yet popular answers. We introduce\nSelf-Harmony, a framework built on a simple intuition: the correct answer\nshould remain stable across both an original question and its paraphrase.\nSelf-Harmony operationalizes this by employing a single model in two\ncomplementary roles: a Solver to produce answers and a Reframer to rephrase the\ninput. Based on this, we further propose a pseudo-label method: instead of\nmajority voting, it aggregates answer frequencies across these original and\nreframed views using the harmonic mean. This is a process that naturally\nselects for solutions stable under reframing, thereby avoiding the common trap\nof favoring view-dependent, spurious answers. Crucially, this requires no human\nsupervision or auxiliary models. Across diverse reasoning benchmarks,\nSelf-Harmony achieves state-of-the-art results at the label-free test-time\nsetting, ranking first in 28 of 30 settings across multiple methods. Beyond\naccuracy, it demonstrates unprecedented robustness, with zero training failures\nin all experiments, underscoring its stability and reliability.", "AI": {"tldr": "Self-Harmony是一种无需标签的测试时强化学习框架，通过使用单个模型同时作为求解器和重述器，利用谐波平均聚合原始问题和重述问题的答案频率，选择在重述下保持稳定的正确答案，避免多数投票的伪答案问题。", "motivation": "传统的多数投票方法在测试时强化学习中容易崩溃到虚假但流行的答案，需要构建可靠的无需人工监督的学习信号。", "method": "使用单个模型同时扮演求解器和重述器两个角色：求解器产生答案，重述器重新表述输入。提出伪标签方法，使用谐波平均聚合原始和重述视图的答案频率，选择在重述下稳定的解决方案。", "result": "在多样化的推理基准测试中，Self-Harmony在无标签测试时设置下达到最先进结果，在30个设置中的28个排名第一，且在所有实验中零训练失败，表现出前所未有的鲁棒性。", "conclusion": "Self-Harmony框架通过简单的直觉——正确答案应在原始问题及其重述中保持稳定，成功解决了测试时强化学习中的伪答案问题，无需人工监督或辅助模型，具有出色的稳定性和可靠性。"}}
{"id": "2511.01192", "pdf": "https://arxiv.org/pdf/2511.01192", "abs": "https://arxiv.org/abs/2511.01192", "authors": ["Guoxin Ma", "Xiaoming Liu", "Zhanhan Zhang", "Chengzhengxu Li", "Shengchao Liu", "Yu Lan"], "title": "DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Detecting machine-generated text (MGT) has emerged as a critical challenge,\ndriven by the rapid advancement of large language models (LLMs) capable of\nproducing highly realistic, human-like content. However, the performance of\ncurrent approaches often degrades significantly under domain shift. To address\nthis challenge, we propose a novel framework designed to capture both\ndomain-specific and domain-general MGT patterns through a two-stage\nDisentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a\ndisentangled mixture-of-experts module, in which domain-specific experts learn\nfine-grained, domain-local distinctions between human and machine-generated\ntext, while shared experts extract transferable, cross-domain features. Second,\nto mitigate the practical limitation of unavailable domain labels during\ninference, we design a reinforcement learning-based routing mechanism that\ndynamically selects the appropriate experts for each input instance,\neffectively bridging the train-inference gap caused by domain uncertainty.\nExtensive experiments on five in-domain and five out-of-domain benchmark\ndatasets demonstrate that DEER consistently outperforms state-of-the-art\nmethods, achieving average F1-score improvements of 1.39% and 5.32% on\nin-domain and out-of-domain datasets respectively, along with accuracy gains of\n1.35% and 3.61% respectively. Ablation studies confirm the critical\ncontributions of both disentangled expert specialization and adaptive routing\nto model performance.", "AI": {"tldr": "提出DEER框架，通过解耦专家混合架构和强化学习路由机制，有效检测机器生成文本并解决领域偏移问题，在多个数据集上超越现有方法。", "motivation": "当前机器文本检测方法在领域偏移下性能显著下降，需要同时捕获领域特定和领域通用的检测模式。", "method": "两阶段解耦专家混合架构：1) 领域特定专家学习细粒度区分，共享专家提取跨域特征；2) 基于强化学习的路由机制动态选择专家。", "result": "在5个域内和5个域外数据集上，F1分数分别提升1.39%和5.32%，准确率分别提升1.35%和3.61%。", "conclusion": "DEER框架通过专家解耦和自适应路由有效解决了领域偏移问题，显著提升了机器文本检测性能。"}}
{"id": "2511.01265", "pdf": "https://arxiv.org/pdf/2511.01265", "abs": "https://arxiv.org/abs/2511.01265", "authors": ["Mo El-Haj", "Paul Rayson"], "title": "AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This paper investigates the impact of domain specificity on abstractive\nsummarisation of Arabic financial texts using large language models (LLMs). We\nintroduce AraFinNews, the largest publicly available Arabic financial news\ndataset to date, comprising 212,500 article-headline pairs spanning nearly a\ndecade of reporting from October 2015 to July 2025. Designed as the Arabic\nequivalent of major English summarisation corpora such as CNN/DailyMail,\nAraFinNews provides a robust benchmark for evaluating domain-specific language\nunderstanding and generation in financial contexts. Using this resource, we\nevaluate transformer-based models -- including mT5, AraT5, and the\ndomain-adapted FinAraT5 -- to examine how financial-domain pretraining\ninfluences factual accuracy, numerical reliability, and stylistic alignment\nwith professional reporting. Experimental results show that domain-adapted\nmodels generate more faithful and coherent summaries, particularly in handling\nquantitative and entity-centric information. The findings highlight the\nimportance of domain-specific adaptation for improving factual consistency and\nnarrative fluency in Arabic financial summarisation. The dataset is freely\navailable for non-commercial research at\nhttps://github.com/ArabicNLP-UK/AraFinNews.", "AI": {"tldr": "该论文研究了领域特异性对阿拉伯语金融文本摘要的影响，通过创建AraFinNews数据集并评估多个LLM模型，发现领域适应的模型在阿拉伯金融摘要中能产生更准确和连贯的总结。", "motivation": "研究领域特异性对阿拉伯语金融文本摘要性能的影响，填补阿拉伯语金融文本摘要数据集和研究的空白。", "method": "创建AraFinNews数据集（21.25万篇文章-标题对），评估mT5、AraT5和领域适应的FinAraT5等transformer模型，比较金融领域预训练对事实准确性、数字可靠性和文体对齐的影响。", "result": "实验结果显示领域适应的模型生成更忠实和连贯的摘要，特别是在处理定量信息和实体中心信息方面表现更好。", "conclusion": "领域特异性适应对于提高阿拉伯金融摘要的事实一致性和叙事流畅性至关重要，AraFinNews数据集为阿拉伯语金融文本处理提供了重要基准资源。"}}
{"id": "2511.01282", "pdf": "https://arxiv.org/pdf/2511.01282", "abs": "https://arxiv.org/abs/2511.01282", "authors": ["Min Fang", "Zhihui Fu", "Qibin Zhao", "Jun Wang"], "title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) has emerged as an effective technique to accelerate\nlarge language model (LLM) inference without compromising output quality.\nHowever, the achievable speedup largely depends on the effectiveness of the\ndrafting model. While model-based methods like EAGLE-2 are accurate but costly,\nretrieval-enhanced methods like SAM-Decoding rely on heuristic switching\nstrategies that often trigger unnecessary retrievals. To address this, we\npropose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a\nnovel framework that transforms heuristic drafter switching into adaptive\ndecision-making. ReSpec features three core innovations: 1) An\n\\textbf{entropy-guided adaptive trigger} quantifies contextual predictability\nto initiate retrieval only when uncertainty is low, avoiding costly low-quality\nspeculations. 2) A \\textbf{feedback-driven candidate selection} leverages\nhistorical feedback to organize multiple high-quality candidates for parallel\nverification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed\nverification strategy} applies strict checks to model-generated drafts while\nusing a relaxed verification for retrieved drafts, achieving a better balance\nbetween accuracy and efficiency. Extensive experiments on Spec-Bench\ndemonstrate that ReSpec achieves state-of-the-art acceleration,outperforming\nEAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while\nmaintaining output quality.", "AI": {"tldr": "ReSpec是一种新的推测解码框架，通过熵引导的自适应触发、反馈驱动的候选选择以及宽松验证策略，在保持输出质量的同时显著加速LLM推理，比现有方法快25-33%。", "motivation": "现有推测解码方法存在效率问题：基于模型的方法成本高，基于检索的方法使用启发式切换策略导致不必要的检索开销。需要一种更智能的自适应方法来优化推测解码效率。", "method": "提出ReSpec框架，包含三个核心创新：1) 熵引导自适应触发机制，仅在低不确定性时启动检索；2) 反馈驱动候选选择，利用历史反馈组织高质量候选；3) 源感知宽松验证策略，对不同来源的草稿采用不同验证严格度。", "result": "在Spec-Bench上的大量实验表明，ReSpec实现了最先进的加速效果，比EAGLE-2快33%以上，比SAM-Decoding快25%以上，同时保持输出质量。", "conclusion": "ReSpec通过将启发式草稿切换转化为自适应决策，有效解决了推测解码中的效率问题，在准确性和效率之间实现了更好的平衡，为LLM推理加速提供了有效解决方案。"}}
{"id": "2511.01287", "pdf": "https://arxiv.org/pdf/2511.01287", "abs": "https://arxiv.org/abs/2511.01287", "authors": ["Qin Zhou", "Zhexin Zhang", "Zhi Li", "Limin Sun"], "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "With the rapid advancement of AI models, their deployment across diverse\ntasks has become increasingly widespread. A notable emerging application is\nleveraging AI models to assist in reviewing scientific papers. However, recent\nreports have revealed that some papers contain hidden, injected prompts\ndesigned to manipulate AI reviewers into providing overly favorable\nevaluations. In this work, we present an early systematic investigation into\nthis emerging threat. We propose two classes of attacks: (1) static attack,\nwhich employs a fixed injection prompt, and (2) iterative attack, which\noptimizes the injection prompt against a simulated reviewer model to maximize\nits effectiveness. Both attacks achieve striking performance, frequently\ninducing full evaluation scores when targeting frontier AI reviewers.\nFurthermore, we show that these attacks are robust across various settings. To\ncounter this threat, we explore a simple detection-based defense. While it\nsubstantially reduces the attack success rate, we demonstrate that an adaptive\nattacker can partially circumvent this defense. Our findings underscore the\nneed for greater attention and rigorous safeguards against prompt-injection\nthreats in AI-assisted peer review.", "AI": {"tldr": "论文系统研究了AI辅助同行评审中的提示注入威胁，提出了静态和迭代两种攻击方法，展示了其高度有效性，并探讨了基于检测的防御措施及其局限性。", "motivation": "随着AI模型在科学论文评审中的广泛应用，发现存在隐藏的注入提示操纵AI评审者给出过高评价的安全威胁，需要进行系统性研究。", "method": "提出两类攻击方法：1)静态攻击使用固定注入提示；2)迭代攻击针对模拟评审模型优化提示以最大化效果。同时探索基于检测的防御方法。", "result": "两种攻击方法都取得显著效果，经常能获得满分评价，且在不同设置下都具有鲁棒性。防御方法虽能大幅降低攻击成功率，但自适应攻击者仍能部分规避防御。", "conclusion": "研究强调了AI辅助同行评审中提示注入威胁的严重性，需要更多关注和严格的安全防护措施来应对这一新兴安全挑战。"}}
{"id": "2511.01289", "pdf": "https://arxiv.org/pdf/2511.01289", "abs": "https://arxiv.org/abs/2511.01289", "authors": ["Saiyma Sittul Muna", "Rezwan Islam Salvi", "Mushfiqur Rahman Mushfique", "Ajwad Abrar"], "title": "FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings", "categories": ["cs.CL"], "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop,\n  co-located with NeurIPS 2025", "summary": "In emergency situations, every second counts. The deployment of Large\nLanguage Models (LLMs) in time-sensitive, low or zero-connectivity environments\nremains limited. Current models are computationally intensive and unsuitable\nfor low-tier devices often used by first responders or civilians. A major\nbarrier to developing lightweight, domain-specific solutions is the lack of\nhigh-quality datasets tailored to first aid and emergency response. To address\nthis gap, we introduce FirstAidQA, a synthetic dataset containing 5,500\nhigh-quality question answer pairs that encompass a wide range of first aid and\nemergency response scenarios. The dataset was generated using a Large Language\nModel, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from\nthe Vital First Aid Book (2019). We applied preprocessing steps such as text\ncleaning, contextual chunking, and filtering, followed by human validation to\nensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is\ndesigned to support instruction-tuning and fine-tuning of LLMs and Small\nLanguage Models (SLMs), enabling faster, more reliable, and offline-capable\nsystems for emergency settings. We publicly release the dataset to advance\nresearch on safety-critical and resource-constrained AI applications in first\naid and emergency response. The dataset is available on Hugging Face at\nhttps://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.", "AI": {"tldr": "该论文提出了FirstAidQA数据集，包含5500个高质量急救问答对，用于在低连接环境下开发轻量级急救AI系统。", "motivation": "当前大型语言模型计算密集，不适合急救人员和民众使用的低端设备，且缺乏高质量的急救领域数据集。", "method": "使用ChatGPT-4o-mini基于《Vital First Aid Book》通过提示学习生成数据集，并进行文本清理、上下文分块、过滤和人工验证。", "result": "创建了包含5500个高质量急救问答对的FirstAidQA数据集，支持LLM和SLM的指令调优和微调。", "conclusion": "该数据集将推动急救和应急响应中安全关键和资源受限AI应用的研究，数据集已在Hugging Face公开。"}}
{"id": "2511.01305", "pdf": "https://arxiv.org/pdf/2511.01305", "abs": "https://arxiv.org/abs/2511.01305", "authors": ["Aman Ganapathy Manvattira", "Yifei Xu", "Ziyue Dang", "Songwu Lu"], "title": "DeepSpecs: Expert-Level Questions Answering in 5G", "categories": ["cs.CL", "cs.AI", "cs.NI"], "comment": null, "summary": "5G technology enables mobile Internet access for billions of users. Answering\nexpert-level questions about 5G specifications requires navigating thousands of\npages of cross-referenced standards that evolve across releases. Existing\nretrieval-augmented generation (RAG) frameworks, including telecom-specific\napproaches, rely on semantic similarity and cannot reliably resolve\ncross-references or reason about specification evolution. We present DeepSpecs,\na RAG system enhanced by structural and temporal reasoning via three\nmetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB\n(line-level version diffs), and TDocDB (standardization meeting documents).\nDeepSpecs explicitly resolves cross-references by recursively retrieving\nreferenced clauses through metadata lookup, and traces specification evolution\nby mining changes and linking them to Change Requests that document design\nrationale. We curate two 5G QA datasets: 573 expert-annotated real-world\nquestions from practitioner forums and educational resources, and 350\nevolution-focused questions derived from approved Change Requests. Across\nmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-art\ntelecom RAG systems; ablations confirm that explicit cross-reference resolution\nand evolution-aware retrieval substantially improve answer quality,\nunderscoring the value of modeling the structural and temporal properties of 5G\nstandards.", "AI": {"tldr": "DeepSpecs是一个增强的RAG系统，通过结构化推理和时间推理来解决5G技术规范中的交叉引用和版本演化问题，显著提升了专家级问答的准确性。", "motivation": "现有RAG框架依赖语义相似性，无法可靠解析5G标准中的交叉引用或处理规范演化问题，需要专门解决方案。", "method": "构建三个元数据丰富的数据库（SpecDB、ChangeDB、TDocDB），通过元数据查找递归检索引用条款，挖掘变更并链接到记录设计原理的变更请求。", "result": "在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验证实显式交叉引用解析和演化感知检索显著提高答案质量。", "conclusion": "建模5G标准的结构和时间特性具有重要价值，DeepSpecs为解决复杂技术规范的问答问题提供了有效框架。"}}
{"id": "2511.01323", "pdf": "https://arxiv.org/pdf/2511.01323", "abs": "https://arxiv.org/abs/2511.01323", "authors": ["Jiabao Ji", "Min Li", "Priyanshu Kumar", "Shiyu Chang", "Saloni Potdar"], "title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages", "summary": "Large language models (LLMs) with integrated search tools show strong promise\nin open-domain question answering (QA), yet they often struggle to produce\ncomplete answer set to complex questions such as Which actor from the film Heat\nwon at least one Academy Award?, which requires (1) distinguishing between\nmultiple films sharing the same title and (2) reasoning across a large set of\nactors to gather and integrate evidence. Existing QA benchmarks rarely evaluate\nboth challenges jointly. To address this, we introduce DeepAmbigQAGen, an\nautomatic data generation pipeline that constructs QA tasks grounded in text\ncorpora and linked knowledge graph, generating natural and verifiable questions\nthat systematically embed name ambiguity and multi-step reasoning. Based on\nthis, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop\nreasoning and half of them explicit name ambiguity resolving. Experiments\nreveal that, even state-of-the-art GPT-5 show incomplete answers, achieving\nonly 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous\nquestions. These findings highlight the need for more robust QA systems aimed\nat information gathering and answer completeness.", "AI": {"tldr": "论文提出了DeepAmbigQAGen数据生成管道和DeepAmbigQA数据集，用于评估大语言模型在包含名称歧义和多步推理的复杂问答任务中的表现，发现即使是GPT-5也存在答案不完整的问题。", "motivation": "现有问答基准很少同时评估名称歧义解析和多步推理这两个挑战，大语言模型在处理需要区分同名实体和跨大规模证据推理的复杂问题时表现不佳。", "method": "开发了DeepAmbigQAGen自动数据生成管道，基于文本语料库和链接知识图谱构建问答任务，生成包含系统性的名称歧义和多步推理的自然可验证问题。", "result": "构建了包含3600个问题的DeepAmbigQA数据集，其中一半需要显式名称歧义解析。实验显示GPT-5在歧义问题上精确匹配率仅0.13，非歧义问题仅0.21。", "conclusion": "当前问答系统在信息收集和答案完整性方面存在显著不足，需要开发更强大的系统来处理复杂的多跳推理和名称歧义问题。"}}
{"id": "2511.01354", "pdf": "https://arxiv.org/pdf/2511.01354", "abs": "https://arxiv.org/abs/2511.01354", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "categories": ["cs.CL", "cs.AI"], "comment": "emnlp 2025 industry track", "summary": "Recently, the demand for small and efficient reasoning models to support\nreal-world applications has driven the development of knowledge distillation\ntechniques that balance reasoning performance and inference speed. In this\npaper, we further extend the DistilQwen model family, initialized from the Qwen\nmodels, by introducing four model series specifically designed to meet\nindustrial requirements. The distilled model collection comprises: (1)\nslow-thinking models, optimized for reasoning tasks that require high accuracy;\n(2) two series of adaptive-thinking models, which dynamically adjust reasoning\nstrategies based on input tasks to maximize efficiency across diverse\nscenarios; and (3) distilled reward models, which enable further reinforcement\nlearning of reasoning models using distilled knowledge. Comprehensive\nevaluations across multiple benchmarks demonstrate both high inference\nefficiency and strong reasoning performance for these models, as well as the\npractical utility of distilled reward models. We further show that these models\nsupport industry practitioners by providing scalable training and inference\nfunctionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)\nplatform.", "AI": {"tldr": "该论文扩展了DistilQwen模型家族，提出了四个专门针对工业需求设计的模型系列：慢思考模型、自适应思考模型和蒸馏奖励模型，在保持高推理效率的同时展现强大的推理性能。", "motivation": "满足现实应用中对小型高效推理模型的需求，平衡推理性能与推理速度，支持工业级应用。", "method": "基于Qwen模型初始化，通过知识蒸馏技术开发四个模型系列：1)慢思考模型（高精度推理）2)自适应思考模型（动态调整策略）3)蒸馏奖励模型（支持强化学习）。", "result": "在多基准测试中展现出高推理效率和强推理性能，蒸馏奖励模型具有实际应用价值，并支持在阿里云PAI平台上的可扩展训练和推理功能。", "conclusion": "该研究成功开发了满足工业需求的推理模型系列，证明了知识蒸馏技术在平衡模型效率与性能方面的有效性，为工业实践提供了实用的解决方案。"}}
{"id": "2511.01359", "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory.", "AI": {"tldr": "该论文提出了MiniTruePrefixes模型，专门用于检测文本前缀中的事实不一致性，通过改进的NLI方法在解码过程中实时提升LLM生成内容的真实性，在摘要任务中显著提高了事实一致性并降低了计算成本。", "motivation": "现有的NLI模型虽然能检测完整句子的真实性，但无法有效处理自回归生成过程中的文本前缀，导致在解码阶段无法实时纠正事实错误。", "method": "将蕴含检测任务泛化到任意文本前缀，创建专用数据集训练MiniTruePrefixes模型，并将其集成到受控解码框架中实时指导生成过程。", "result": "MiniTruePrefixes在前缀级蕴含检测上比基线NLI模型提升5-14个F1点；在摘要任务中，LLaMA-3.2-3B模型在MiniTruePrefixes指导下达到与8B模型相当的真实性和运行效率，内存使用减半。", "conclusion": "针对文本前缀的专门化NLI模型能有效提升LLM生成内容的事实一致性，同时显著降低计算资源需求，为实时事实核查提供了有效解决方案。"}}
{"id": "2511.01360", "pdf": "https://arxiv.org/pdf/2511.01360", "abs": "https://arxiv.org/abs/2511.01360", "authors": ["Aadi Palnitkar", "Arjun Suresh", "Rishi Rajesh", "Puneet Puli"], "title": "Safer in Translation? Presupposition Robustness in Indic Languages", "categories": ["cs.CL"], "comment": "This is a submission to LREC 2026 (Language Resources and Evaluation\n  Conference 2026). Corresponding author: aadipalnitkar96@gmail.com", "summary": "Increasingly, more and more people are turning to large language models\n(LLMs) for healthcare advice and consultation, making it important to gauge the\nefficacy and accuracy of the responses of LLMs to such queries. While there are\npre-existing medical benchmarks literature which seeks to accomplish this very\ntask, these benchmarks are almost universally in English, which has led to a\nnotable gap in existing literature pertaining to multilingual LLM evaluation.\nWithin this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,\nan Indic language benchmark built by translating a 500-item subset of\nCancer-Myth, sampled evenly across its original categories, into five\nunder-served but widely used languages from the subcontinent (500 per language;\n2,500 translated items total). Native-speaker translators followed a style\nguide for preserving implicit presuppositions in translation; items feature\nfalse presuppositions relating to cancer. We evaluate several popular LLMs\nunder this presupposition stress.", "AI": {"tldr": "该论文创建了Cancer-Myth-Indic基准，用于评估大型语言模型在印度语言中对癌症相关医疗信息的响应准确性，填补了多语言医学评估的空白。", "motivation": "现有医学基准几乎都是英文的，缺乏多语言LLM评估，特别是针对印度次大陆广泛使用但服务不足的语言。", "method": "通过将500个癌症相关错误预设问题翻译成5种印度语言（每种语言500项，共2500项），由母语译者遵循特定风格指南进行翻译。", "result": "在预设压力下评估了多个流行LLM的表现。", "conclusion": "该基准为多语言医疗LLM评估提供了重要工具，有助于改善非英语用户的医疗信息获取质量。"}}
{"id": "2511.01365", "pdf": "https://arxiv.org/pdf/2511.01365", "abs": "https://arxiv.org/abs/2511.01365", "authors": ["İbrahim Ethem Deveci", "Duygu Ataman"], "title": "The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation", "categories": ["cs.CL"], "comment": "Accepted to NeurIPS 2025 Workshop on LLM Evaluation\n  (https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/LLM_Evaluation)", "summary": "The rapid rise of Large Language Models (LLMs) and Large Reasoning Models\n(LRMs) has been accompanied by an equally rapid increase of benchmarks used to\nassess them. However, due to both improved model competence resulting from\nscaling and novel training advances as well as likely many of these datasets\nbeing included in pre or post training data, results become saturated, driving\na continuous need for new and more challenging replacements. In this paper, we\ndiscuss whether surpassing a benchmark truly demonstrates reasoning ability or\nare we simply tracking numbers divorced from the capabilities we claim to\nmeasure? We present an investigation focused on three model families, OpenAI,\nAnthropic, and Google, and how their reasoning capabilities across different\nbenchmarks evolve over the years. We also analyze performance trends over the\nyears across different reasoning tasks and discuss the current situation of\nbenchmarking and remaining challenges. By offering a comprehensive overview of\nbenchmarks and reasoning tasks, our work aims to serve as a first reference to\nground future research in reasoning evaluation and model development.", "AI": {"tldr": "该论文对大型语言模型和推理模型的基准测试有效性提出质疑，分析了三大模型家族在不同基准上的表现趋势，并讨论了当前基准测试的挑战。", "motivation": "随着LLMs和LRMs快速发展，基准测试结果趋于饱和，需要质疑超越基准是否真正体现推理能力，还是仅仅在追踪与声称能力脱节的数字。", "method": "研究聚焦OpenAI、Anthropic和Google三大模型家族，分析它们在不同基准测试中的推理能力随时间演变，并分析不同推理任务的性能趋势。", "result": "提供了基准测试和推理任务的全面概述，揭示了基准测试结果饱和现象以及模型能力评估面临的挑战。", "conclusion": "该研究旨在为未来推理评估和模型开发研究提供首个参考基础，强调需要更有效的评估方法来真正衡量模型的推理能力。"}}
{"id": "2511.01380", "pdf": "https://arxiv.org/pdf/2511.01380", "abs": "https://arxiv.org/abs/2511.01380", "authors": ["Wessel Poelman", "Thomas Bauwens", "Miryam de Lhoneux"], "title": "Confounding Factors in Relating Model Performance to Morphology", "categories": ["cs.CL"], "comment": "EMNLP 2025: Main Conference", "summary": "The extent to which individual language characteristics influence\ntokenization and language modeling is an open question. Differences in\nmorphological systems have been suggested as both unimportant and crucial to\nconsider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter\nalia). We argue this conflicting evidence is due to confounding factors in\nexperimental setups, making it hard to compare results and draw conclusions. We\nidentify confounding factors in analyses trying to answer the question of\nwhether, and how, morphology relates to language modeling. Next, we re-assess\nthree hypotheses by Arnett & Bergen (2025) for why modeling agglutinative\nlanguages results in higher perplexities than fusional languages: they look at\nmorphological alignment of tokenization, tokenization efficiency, and dataset\nsize. We show that each conclusion includes confounding factors. Finally, we\nintroduce token bigram metrics as an intrinsic way to predict the difficulty of\ncausal language modeling, and find that they are gradient proxies for\nmorphological complexity that do not require expert annotation. Ultimately, we\noutline necessities to reliably answer whether, and how, morphology relates to\nlanguage modeling.", "AI": {"tldr": "该论文重新评估了语言形态特征对分词和语言建模的影响，指出先前研究存在混淆因素，提出了token二元组指标作为预测语言建模难度的内在方法。", "motivation": "现有研究关于形态特征对语言建模重要性存在矛盾结论，作者认为这是由于实验设置中的混淆因素导致的，需要更可靠的方法来评估形态复杂度与语言建模的关系。", "method": "1) 识别分析形态学与语言建模关系研究中的混淆因素；2) 重新评估Arnett & Bergen (2025)的三个假设；3) 引入token二元组指标作为预测语言建模难度的内在方法。", "result": "研究发现先前关于粘着语和融合语建模性能差异的三个假设都包含混淆因素，提出的token二元组指标可作为形态复杂度的梯度代理指标，无需专家标注。", "conclusion": "论文强调了可靠评估形态学与语言建模关系所需的必要条件，为未来研究提供了更严谨的分析框架和评估方法。"}}
{"id": "2511.01386", "pdf": "https://arxiv.org/pdf/2511.01386", "abs": "https://arxiv.org/abs/2511.01386", "authors": ["Muhammed Yusuf Kartal", "Suha Kagan Kose", "Korhan Sevinç", "Burak Aktas"], "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "comment": "45 pages", "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting\nchoices across retrieval, ranking, augmentation, prompting, and generation, so\noptimizing modules in isolation is brittle. We introduce RAGSmith, a modular\nframework that treats RAG design as an end-to-end architecture search over nine\ntechnique families and 46{,}080 feasible pipeline configurations. A genetic\nsearch optimizes a scalar objective that jointly aggregates retrieval metrics\n(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic\nsimilarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,\nFinance, Medicine, Defense Industry, Computer Science), each with 100 questions\nspanning factual, interpretation, and long-answer types. RAGSmith finds\nconfigurations that consistently outperform naive RAG baseline by +3.8\\% on\naverage (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in\nretrieval and +7.5\\% in generation. The search typically explores $\\approx\n0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone --\nvector retrieval plus post-generation reflection/revision -- augmented by\ndomain-dependent choices in expansion, reranking, augmentation, and prompt\nreordering; passage compression is never selected. Improvement magnitude\ncorrelates with question type, with larger gains on factual/long-answer mixes\nthan interpretation-heavy sets. These results provide practical, domain-aware\nguidance for assembling effective RAG systems and demonstrate the utility of\nevolutionary search for full-pipeline optimization.", "AI": {"tldr": "RAGSmith是一个端到端的架构搜索框架，通过遗传算法优化RAG系统的46,080种配置组合，在多个领域平均提升性能3.8%，最多提升12.5%的检索性能和7.5%的生成性能", "motivation": "RAG系统的质量取决于检索、排序、增强、提示和生成等多个模块的交互选择，单独优化模块效果有限且脆弱", "method": "提出RAGSmith模块化框架，使用遗传搜索算法在9个技术家族和46,080个可行管道配置中进行端到端架构搜索，联合优化检索指标和生成指标", "result": "在6个维基百科领域测试中，RAGSmith找到的配置平均比基准提升3.8%（范围1.2%-6.9%），检索性能最高提升12.5%，生成性能最高提升7.5%", "conclusion": "研究发现向量检索加后生成反思/修订的稳健主干结构，配合领域相关的扩展、重排序、增强和提示重排选择效果最佳，进化搜索对全管道优化具有实用价值"}}
{"id": "2511.01409", "pdf": "https://arxiv.org/pdf/2511.01409", "abs": "https://arxiv.org/abs/2511.01409", "authors": ["Heng Zhou", "Ao Yu", "Yuchen Fan", "Jianing Shi", "Li Kang", "Hejia Geng", "Yongting Zhang", "Yutao Fan", "Yuhao Wu", "Tiancheng He", "Yiran Qin", "Lei Bai", "Zhenfei Yin"], "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge.", "AI": {"tldr": "LiveSearchBench是一个自动化的检索依赖基准测试构建系统，通过分析Wikidata知识更新来生成需要最新知识检索的问题，评估LLM在处理时效性知识时的表现。", "motivation": "现有LLM问答评估基准主要依赖静态知识，奖励记忆能力而低估检索作用，无法捕捉世界知识的动态变化特性。", "method": "通过计算Wikidata连续快照的差异，筛选高质量三元组，自动生成三个难度级别的自然语言问题，每个问题都可通过SPARQL验证唯一答案。", "result": "实验显示模型在面对预训练后新增事实时性能显著下降，多跳查询差距最大。检索增强方法和大型指令调优模型只能提供部分改进，无法完全消除时效性差距。", "conclusion": "LiveSearchBench将评估重点从静态记忆转向需要最新检索和推理的任务，为LLM在演化知识下的系统性长期评估提供了基础。"}}
{"id": "2511.01454", "pdf": "https://arxiv.org/pdf/2511.01454", "abs": "https://arxiv.org/abs/2511.01454", "authors": ["Sergio Torres Aguilar"], "title": "\"Don't Teach Minerva\": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "Translating a morphology-rich, low-resource language like Latin poses\nsignificant challenges. This paper introduces a reproducible draft-based\nrefinement pipeline that elevates open-source Large Language Models (LLMs) to a\nperformance level statistically comparable to top-tier proprietary systems. Our\nmethod first uses a fine-tuned NLLB-1.3B model to generate a high-quality,\nstructurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes\nthis draft, a process that can be further enhanced by augmenting the context\nwith retrieved out-context examples (RAG). We demonstrate the robustness of\nthis approach on two distinct benchmarks: a standard in-domain test set\n(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of\n12th-century Latin letters (2025). Our central finding is that this open-source\nRAG system achieves performance statistically comparable to the GPT-5 baseline,\nwithout any task-specific LLM fine-tuning. We release the pipeline, the\nChartres OOD set, and evaluation scripts and models to facilitate replicability\nand further research.", "AI": {"tldr": "提出一个可复现的翻译流水线，使用微调NLLB模型生成初稿，再用零样本LLM精炼，结合RAG技术，在拉丁语翻译任务上达到与GPT-5相当的性能。", "motivation": "解决形态丰富、资源稀缺语言（如拉丁语）的翻译挑战，提升开源大语言模型性能至与顶级专有系统相当的水平。", "method": "两阶段流水线：1) 使用微调NLLB-1.3B模型生成高质量结构忠实初稿；2) 零样本LLM（Llama-3.3或Qwen3）精炼初稿，可结合检索增强生成（RAG）技术。", "result": "在两个基准测试（标准领域内测试集和新的12世纪拉丁语信件OOD集）上证明该方法稳健，开源RAG系统性能与GPT-5基线统计相当。", "conclusion": "无需任务特定LLM微调即可实现与专有系统竞争的性能，发布了完整流水线、数据集和评估工具以促进可复现性和进一步研究。"}}
{"id": "2511.01470", "pdf": "https://arxiv.org/pdf/2511.01470", "abs": "https://arxiv.org/abs/2511.01470", "authors": ["Lujie Niu", "Lei Shen", "Yi Jiang", "Caixia Yuan", "Xiaojie Wang", "Wenbo Su", "Bo zheng"], "title": "BARD: budget-aware reasoning distillation", "categories": ["cs.CL"], "comment": null, "summary": "While long Chain-of-Thought (CoT) distillation effectively transfers\nreasoning capability to smaller language models, the reasoning process often\nremains redundant and computational budget uncontrollable, leading to\ninefficient resource usage. To address this limitation, we propose\n\\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that\nsimultaneously distills reasoning capability and enables fine-grained control\nover the reasoning length. BARD uses the thinking budget as a user-specified\ncontrol signal, allowing the model to dynamically balance reasoning performance\nand computational efficiency. To achieve this concept, BARD introduces a\ntwo-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on\nteacher-generated long CoT data compressed to various budget levels,\nbootstrapping the model's understanding of budget constraints. The second phase\nleverages Reinforcement Learning (RL) from a reward signal in consideration of\nreasoning performance and budget fidelity simultaneously. Incorporating the\ntwo-phase regimen is crucial to avoiding policy degradation and ensuring that\nboth objectives are optimized jointly. Extensive experiments demonstrate that\nour method empowers an 8B student model to achieve strong performance on\nchallenging reasoning benchmarks (\\textit{AIME24, AIME25, GPQA}) while\nproviding precise and adaptive control over its reasoning length across a wide\nrange of budgets.", "AI": {"tldr": "BARD是一个预算感知推理蒸馏框架，通过两阶段训练（SFT和RL）让小模型获得可控推理长度能力，在保持性能的同时实现计算效率的动态平衡", "motivation": "解决长思维链蒸馏中推理过程冗余且计算预算不可控的问题，提高资源使用效率", "method": "两阶段训练：1）在教师生成的压缩至不同预算级别的长CoT数据上进行监督微调；2）使用同时考虑推理性能和预算保真度的奖励信号进行强化学习", "result": "8B学生模型在AIME24、AIME25、GPQA等挑战性推理基准上表现强劲，并能跨广泛预算范围精确自适应控制推理长度", "conclusion": "BARD框架成功实现了推理能力蒸馏与推理长度细粒度控制的统一，为高效推理模型开发提供了新思路"}}
{"id": "2511.01482", "pdf": "https://arxiv.org/pdf/2511.01482", "abs": "https://arxiv.org/abs/2511.01482", "authors": ["Neha Sharma", "Navneet Agarwal", "Kairit Sirts"], "title": "Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Text-based automated Cognitive Distortion detection is a challenging task due\nto its subjective nature, with low agreement scores observed even among expert\nhuman annotators, leading to unreliable annotations. We explore the use of\nLarge Language Models (LLMs) as consistent and reliable annotators, and propose\nthat multiple independent LLM runs can reveal stable labeling patterns despite\nthe inherent subjectivity of the task. Furthermore, to fairly compare models\ntrained on datasets with different characteristics, we introduce a\ndataset-agnostic evaluation framework using Cohen's kappa as an effect size\nmeasure. This methodology allows for fair cross-dataset and cross-study\ncomparisons where traditional metrics like F1 score fall short. Our results\nshow that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),\nresulting in improved test set performance for models trained on these\nannotations compared to those trained on human-labeled data. Our findings\nsuggest that LLMs can offer a scalable and internally consistent alternative\nfor generating training data that supports strong downstream performance in\nsubjective NLP tasks.", "AI": {"tldr": "研究探索使用大型语言模型作为认知扭曲标注工具，发现GPT-4能产生一致性标注，且基于这些标注训练的模型表现优于基于人工标注的模型。", "motivation": "认知扭曲检测任务具有高度主观性，人工标注者间一致性低导致标注不可靠，需要寻找更一致的标注方法。", "method": "使用多个独立的LLM运行来发现稳定标注模式，并引入基于Cohen's kappa的数据集无关评估框架进行公平比较。", "result": "GPT-4能产生高度一致的标注（Fleiss's Kappa = 0.78），基于LLM标注训练的模型在测试集上表现优于基于人工标注的模型。", "conclusion": "LLMs可为主观性NLP任务提供可扩展且内部一致的训练数据生成方案，支持强大的下游性能。"}}
{"id": "2511.01490", "pdf": "https://arxiv.org/pdf/2511.01490", "abs": "https://arxiv.org/abs/2511.01490", "authors": ["Max Schaffelder", "Albert Gatt"], "title": "Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning", "categories": ["cs.CL"], "comment": null, "summary": "As synthetic data becomes widely used in language model development,\nunderstanding its impact on model behavior is crucial. This paper investigates\nthe impact of the diversity of sources of synthetic data on fine-tuned large\nlanguage models. We focus on three key dimensions: distribution collapse,\nadversarial robustness, and self-preference bias. Our findings reveal that\nfine-tuning models on synthetic data from diverse sources can mitigate\ndistribution collapse, preserving the breadth of the output distribution and\nthe diversity of the output text. Furthermore, while both human and synthetic\nfine-tuning data can remove safeguards, the latter preserves higher output\nquality, thus making outputs potentially more usable and dangerous. Finally,\nfine-tuning reduces self-preference bias, with human data being the most\neffective, followed by multi-source synthetic data.", "AI": {"tldr": "本文研究了合成数据来源多样性对微调大语言模型的影响，发现在三个关键维度上：分布坍塌、对抗鲁棒性和自偏好偏差，多源合成数据微调能有效缓解分布坍塌并保持输出质量。", "motivation": "随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响变得至关重要，特别是合成数据来源多样性对微调模型的影响。", "method": "研究聚焦三个关键维度：分布坍塌、对抗鲁棒性和自偏好偏差，通过比较不同来源（人类数据和合成数据）的微调效果进行分析。", "result": "多源合成数据微调能缓解分布坍塌，保持输出分布广度；合成数据微调在移除安全防护的同时保持更高输出质量；人类数据在减少自偏好偏差方面最有效，多源合成数据次之。", "conclusion": "合成数据来源多样性对模型行为有重要影响，多源合成数据微调在平衡输出质量和安全性方面具有优势，但人类数据在纠正模型偏差方面仍更有效。"}}
{"id": "2511.01512", "pdf": "https://arxiv.org/pdf/2511.01512", "abs": "https://arxiv.org/abs/2511.01512", "authors": ["Ayesha Afroza Mohsin", "Mashrur Ahsan", "Nafisa Maliyat", "Shanta Maria", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification", "categories": ["cs.CL", "cs.AI"], "comment": "Under review, 6 pages, 1 figure, 2 tables", "summary": "Toxic language in Bengali remains prevalent, especially in online\nenvironments, with few effective precautions against it. Although text\ndetoxification has seen progress in high-resource languages, Bengali remains\nunderexplored due to limited resources. In this paper, we propose a novel\npipeline for Bengali text detoxification that combines Pareto class-optimized\nlarge language models (LLMs) and Chain-of-Thought (CoT) prompting to generate\ndetoxified sentences. To support this effort, we construct BanglaNirTox, an\nartificially generated parallel corpus of 68,041 toxic Bengali sentences with\nclass-wise toxicity labels, reasonings, and detoxified paraphrases, using\nPareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox\ndataset is used to fine-tune language models to produce better detoxified\nversions of Bengali sentences. Our findings show that Pareto-optimized LLMs\nwith CoT prompting significantly enhance the quality and consistency of Bengali\ntext detoxification.", "AI": {"tldr": "本文提出了一个针对孟加拉语文本去毒化的新方法，结合帕累托优化的大语言模型和思维链提示，并构建了包含68,041个句子的BanglaNirTox数据集来支持模型训练。", "motivation": "孟加拉语中的有毒语言在在线环境中普遍存在，但缺乏有效的防范措施，且由于资源有限，孟加拉语文本去毒化研究相对不足。", "method": "使用帕累托优化的LLM和思维链提示构建BanglaNirTox平行语料库，包含毒性标签、推理过程和去毒化改写，并用该数据集微调语言模型。", "result": "帕累托优化的LLM结合思维链提示显著提高了孟加拉语文本去毒化的质量和一致性。", "conclusion": "该方法为低资源语言的文本去毒化提供了有效解决方案，BanglaNirTox数据集为后续研究提供了重要资源。"}}
{"id": "2511.01526", "pdf": "https://arxiv.org/pdf/2511.01526", "abs": "https://arxiv.org/abs/2511.01526", "authors": ["Seokhoon Kang", "Yejin Jeon", "Seonjeong Hwang", "Gary Geunbae Lee"], "title": "Difficulty-Controllable Cloze Question Distractor Generation", "categories": ["cs.CL"], "comment": null, "summary": "Multiple-choice cloze questions are commonly used to assess linguistic\nproficiency and comprehension. However, generating high-quality distractors\nremains challenging, as existing methods often lack adaptability and control\nover difficulty levels, and the absence of difficulty-annotated datasets\nfurther hinders progress. To address these issues, we propose a novel framework\nfor generating distractors with controllable difficulty by leveraging both data\naugmentation and a multitask learning strategy. First, to create a\nhigh-quality, difficulty-annotated dataset, we introduce a two-way distractor\ngeneration process in order to produce diverse and plausible distractors. These\ncandidates are subsequently refined through filtering and then categorized by\ndifficulty using an ensemble QA system. Second, this newly created dataset is\nleveraged to train a difficulty-controllable generation model via multitask\nlearning. The framework includes carefully designed auxiliary tasks that\nenhance the model's semantic understanding of distractors and its ability to\nestimate their difficulty. Experimental results demonstrate that our method\ngenerates high-quality distractors across difficulty levels and substantially\noutperforms GPT-4o in aligning distractor difficulty with human perception.", "AI": {"tldr": "提出了一种通过数据增强和多任务学习生成可控难度干扰项的新框架，解决了现有方法缺乏难度适应性和控制的问题，在难度对齐人类感知方面显著优于GPT-4o", "motivation": "多选题完形填空用于评估语言能力，但现有干扰项生成方法缺乏难度可控性，且缺乏难度标注数据集阻碍了进展", "method": "1) 通过双向干扰项生成过程创建高质量难度标注数据集；2) 使用集成QA系统对干扰项进行难度分类；3) 通过多任务学习训练难度可控生成模型，包含增强语义理解和难度估计能力的辅助任务", "result": "实验结果表明，该方法能生成各难度级别的高质量干扰项，在干扰项难度与人类感知对齐方面大幅优于GPT-4o", "conclusion": "该框架成功解决了干扰项生成的难度控制问题，通过数据增强和多任务学习策略实现了高质量的难度可控干扰项生成"}}
{"id": "2511.01615", "pdf": "https://arxiv.org/pdf/2511.01615", "abs": "https://arxiv.org/abs/2511.01615", "authors": ["Francisco Portillo López"], "title": "Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Linguistic errors are not merely deviations from normative grammar; they\noffer a unique window into the cognitive architecture of language and expose\nthe current limitations of artificial systems that seek to replicate them. This\nproject proposes an interdisciplinary study of linguistic errors produced by\nnative Spanish speakers, with the aim of analyzing how current large language\nmodels (LLM) interpret, reproduce, or correct them. The research integrates\nthree core perspectives: theoretical linguistics, to classify and understand\nthe nature of the errors; neurolinguistics, to contextualize them within\nreal-time language processing in the brain; and natural language processing\n(NLP), to evaluate their interpretation against linguistic errors. A\npurpose-built corpus of authentic errors of native Spanish (+500) will serve as\nthe foundation for empirical analysis. These errors will be tested against AI\nmodels such as GPT or Gemini to assess their interpretative accuracy and their\nability to generalize patterns of human linguistic behavior. The project\ncontributes not only to the understanding of Spanish as a native language but\nalso to the development of NLP systems that are more cognitively informed and\ncapable of engaging with the imperfect, variable, and often ambiguous nature of\nreal human language.", "AI": {"tldr": "该论文提出了一项跨学科研究，通过分析西班牙语母语者的语言错误来评估大型语言模型的理解能力，旨在改进NLP系统对真实人类语言的认知建模。", "motivation": "语言错误不仅是语法偏差，更能揭示语言的认知架构和人工智能系统在复制人类语言时的局限性，需要开发更认知智能的NLP系统来处理不完美、多变且模糊的真实人类语言。", "method": "整合理论语言学、神经语言学和自然语言处理三个视角，构建包含500+西班牙语母语者真实错误的语料库，使用GPT、Gemini等AI模型测试其对错误的解释准确性和泛化人类语言行为模式的能力。", "result": "通过实证分析评估AI模型对语言错误的解释准确性，检验其泛化人类语言行为模式的能力。", "conclusion": "该研究不仅有助于理解西班牙语作为母语的特点，还能推动开发更具认知意识、能够处理真实人类语言不完美特性的NLP系统。"}}
{"id": "2511.01558", "pdf": "https://arxiv.org/pdf/2511.01558", "abs": "https://arxiv.org/abs/2511.01558", "authors": ["Luciana Ciringione", "Emma Franchino", "Simone Reigl", "Isaia D'Onofrio", "Anna Serbati", "Oleksandra Poquet", "Florence Gabriel", "Massimo Stella"], "title": "Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Math anxiety poses significant challenges for university psychology students,\naffecting their career choices and overall well-being. This study employs a\nframework based on behavioural forma mentis networks (i.e. cognitive models\nthat map how individuals structure their associative knowledge and emotional\nperceptions of concepts) to explore individual and group differences in the\nperception and association of concepts related to math and anxiety. We\nconducted 4 experiments involving psychology undergraduates from 2 samples (n1\n= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;\nGPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network\nfeatures to predict psychometric scores for math anxiety and its facets\n(observational, social and evaluational) from the Math Anxiety Scale.\nExperiment 4 focuses on group-level perceptions extracted from human students,\nGPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive\nvalence ratings and higher network degree for \"anxiety\", together with negative\nratings for \"math\", can predict higher total and evaluative math anxiety. In\ncontrast, these models do not work on GPT-based data because of differences in\nsimulated networks and psychometric scores compared to humans. These results\nwere also reconciled with differences found in the ways that high/low subgroups\nof simulated and real students framed semantically and emotionally STEM\nconcepts. High math-anxiety students collectively framed \"anxiety\" in an\nemotionally polarising way, absent in the negative perception of low\nmath-anxiety students. \"Science\" was rated positively, but contrasted against\nthe negative perception of \"math\". These findings underscore the importance of\nunderstanding concept perception and associations in managing students' math\nanxiety.", "AI": {"tldr": "本研究使用行为形式心智网络框架分析心理学大学生对数学和焦虑概念的感知差异，发现人类学生的积极情绪评分和网络度可预测数学焦虑，但GPT模拟数据不适用此模型。", "motivation": "数学焦虑严重影响心理学大学生的职业选择和心理健康，需要深入理解学生对数学相关概念的认知和情感关联方式。", "method": "对两个心理学本科生样本(n1=70, n2=57)和GPT模拟学生(GPT-3.5:300人, GPT-4o:300人)进行4个实验，使用行为形式心智网络分析个体和群体层面的概念感知差异。", "result": "人类学生中，对'焦虑'的积极情绪评分和较高网络度，以及对'数学'的负面评分，能预测更高的数学焦虑水平。GPT模拟数据因网络结构和心理测量分数差异而不适用此模型。", "conclusion": "研究强调了理解概念感知和关联方式在管理学生数学焦虑中的重要性，高焦虑学生对'焦虑'的情感极化框架与低焦虑学生存在显著差异。"}}
{"id": "2511.01643", "pdf": "https://arxiv.org/pdf/2511.01643", "abs": "https://arxiv.org/abs/2511.01643", "authors": ["Riccardo Campi", "Nicolò Oreste Pinciroli Vago", "Mathyas Giudici", "Pablo Barrachina Rodriguez-Guisado", "Marco Brambilla", "Piero Fraternali"], "title": "A Graph-based RAG for Energy Efficiency Question Answering", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.2.4; I.2.1; I.2.6"], "comment": null, "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a\ngraph-based Retrieval Augmented Generation (RAG) architecture for Energy\nEfficiency (EE) Question Answering. First, the system automatically extracts a\nKnowledge Graph (KG) from guidance and regulatory documents in the energy\nfield. Then, the generated graph is navigated and reasoned upon to provide\nusers with accurate answers in multiple languages. We implement a human-based\nvalidation using the RAGAs framework properties, a validation dataset\ncomprising 101 question-answer pairs, and domain experts. Results confirm the\npotential of this architecture and identify its strengths and weaknesses.\nValidation results show how the system correctly answers in about three out of\nfour of the cases (75.2 +- 2.7%), with higher results on questions related to\nmore general EE answers (up to 81.0 +- 4.1%), and featuring promising\nmultilingual abilities (4.4% accuracy loss due to translation).", "AI": {"tldr": "本研究探索了在基于图谱的检索增强生成架构中使用大语言模型进行能源效率问答，通过从能源法规文档自动构建知识图谱并进行推理，实现了多语言准确回答，验证结果显示75.2%的正确率。", "motivation": "解决能源效率领域的专业问答需求，利用大语言模型和知识图谱技术提升问答系统的准确性和多语言能力。", "method": "1. 从能源指导文件和法规文档自动提取知识图谱；2. 基于图谱进行导航和推理；3. 使用RAGAs框架和101个问答对进行人工验证；4. 通过领域专家评估系统性能。", "result": "系统在约75.2%的情况下能正确回答问题（误差±2.7%），通用能源效率问题准确率高达81.0%，多语言能力表现良好（翻译导致的准确率损失仅为4.4%）。", "conclusion": "该架构在能源效率问答方面展现出良好潜力，特别是在通用问题和多语言处理方面表现优异，同时也识别了系统的优缺点，为后续改进提供了方向。"}}
{"id": "2511.01568", "pdf": "https://arxiv.org/pdf/2511.01568", "abs": "https://arxiv.org/abs/2511.01568", "authors": ["Seungmin Shin", "Dooyoung Kim", "Youngjoong Ko"], "title": "ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation", "categories": ["cs.CL"], "comment": "Published at EMNLP 2025 main", "summary": "Controllable Dialogue Generation (CDG) enables chatbots to generate responses\nwith desired attributes, and weighted decoding methods have achieved\nsignificant success in the CDG task. However, using a fixed constant value to\nmanage the bias of attribute probabilities makes it challenging to find an\nideal control strength that satisfies both controllability and fluency. To\naddress this issue, we propose ECO decoding (Entropy-based COntrol), which\ndynamically adjusts the control strength at each generation step according to\nthe model's entropy in both the language model and attribute classifier\nprobability distributions. Experiments on the DailyDialog and MultiWOZ datasets\ndemonstrate that ECO decoding consistently improves controllability while\nmaintaining fluency and grammaticality, outperforming prior decoding methods\nacross various models and settings. Furthermore, ECO decoding alleviates\nprobability interpolation issues in multi-attribute generation and consequently\ndemonstrates strong performance in both single and multi-attribute scenarios.", "AI": {"tldr": "提出ECO解码方法，通过基于熵的动态控制强度调整，在保持流畅性的同时提升可控对话生成的效果", "motivation": "现有加权解码方法使用固定常数控制属性概率偏差，难以同时满足可控性和流畅性的理想控制强度", "method": "ECO解码（基于熵的控制），根据语言模型和属性分类器概率分布的熵值，在每个生成步骤动态调整控制强度", "result": "在DailyDialog和MultiWOZ数据集上，ECO解码在保持流畅性和语法性的同时持续提升可控性，优于现有解码方法，并在单属性和多属性场景中都表现良好", "conclusion": "ECO解码通过动态控制强度调整有效解决了可控对话生成中的控制强度平衡问题，在多属性生成中也缓解了概率插值问题"}}
{"id": "2511.01650", "pdf": "https://arxiv.org/pdf/2511.01650", "abs": "https://arxiv.org/abs/2511.01650", "authors": ["Ayesha Gull", "Muhammad Usman Safder", "Rania Elbadry", "Preslav Nakov", "Zhuohan Xie"], "title": "EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, includes figures and tables; introduces the EngChain\n  benchmark", "summary": "Large Language Models (LLMs) are increasingly being applied to specialized,\nhigh-stakes domains like engineering, which demands rigorous evaluation of\ntheir complex reasoning capabilities. While current benchmarks assess language\nunderstanding, factual recall, mathematics or code generation, none capture the\nintegrative reasoning central to engineering where scientific principles,\nquantitative modeling and practical constraints must converge. To address this\ngap, we introduce EngChain, a benchmark for verifiable multi-step engineering\nproblem-solving. EngChain contains 90 problems spanning three engineering\nbranches, organized into 9 domains and 20 distinct areas. The problems are\ngenerated from symbolic templates with a high degree of randomization to ensure\ndiversity and eliminate the risk of contamination. With this benchmark, we move\nbeyond final answer accuracy with a two-stage evaluation: we first\nquantitatively verify the numerical and semantic validity of each reasoning\nstep and then introduce LLM-As-A-Judge, an automated system to qualitatively\ncategorize the identified reasoning errors.", "AI": {"tldr": "EngChain是一个专门评估大语言模型在工程领域复杂推理能力的基准测试，包含90个多步骤工程问题，采用两阶段评估方法验证数值和语义有效性。", "motivation": "当前基准测试无法评估工程领域所需的整合性推理能力，即科学原理、定量建模和实践约束的融合。", "method": "创建包含90个工程问题的EngChain基准，问题来自符号模板并高度随机化以确保多样性；采用两阶段评估：定量验证推理步骤的有效性，然后使用LLM-As-A-Judge系统定性分类推理错误。", "result": "开发了一个专门针对工程问题解决的基准测试系统，能够全面评估LLMs在复杂工程推理中的表现。", "conclusion": "EngChain填补了工程领域LLM评估的空白，为验证大语言模型在专业高风险领域的复杂推理能力提供了有效工具。"}}
{"id": "2511.01589", "pdf": "https://arxiv.org/pdf/2511.01589", "abs": "https://arxiv.org/abs/2511.01589", "authors": ["Wenjie Hua", "Hoang H. Nguyen", "Gangyan Ge"], "title": "BIRD: Bronze Inscription Restoration and Dating", "categories": ["cs.CL", "I.2.7"], "comment": "Accepted at EMNLP 2025 (Main Conference)", "summary": "Bronze inscriptions from early China are fragmentary and difficult to date.\nWe introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded\ndataset grounded in standard scholarly transcriptions and chronological labels.\nWe further propose an allograph-aware masked language modeling framework that\nintegrates domain- and task-adaptive pretraining with a Glyph Net (GN), which\nlinks graphemes and allographs. Experiments show that GN improves restoration,\nwhile glyph-biased sampling yields gains in dating.", "AI": {"tldr": "BIRD数据集和全形感知掩码语言建模框架用于中国青铜器铭文的修复和断代", "motivation": "早期中国青铜器铭文残缺且难以断代，需要标准化的数据集和有效的分析方法", "method": "提出BIRD数据集（基于标准学术转录和年代标签），开发全形感知掩码语言建模框架，集成领域适应性预训练和Glyph Net（连接字素和异体字）", "result": "实验表明Glyph Net改善了铭文修复效果，字形偏置采样在断代任务中获得提升", "conclusion": "该方法为青铜器铭文研究提供了有效的数据集和分析框架，在铭文修复和断代方面都取得了积极成果"}}
{"id": "2511.01670", "pdf": "https://arxiv.org/pdf/2511.01670", "abs": "https://arxiv.org/abs/2511.01670", "authors": ["Chaoqun Liu", "Mahani Aljunied", "Guizhen Chen", "Hou Pong Chan", "Weiwen Xu", "Yu Rong", "Wenxuan Zhang"], "title": "SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "We introduce SeaLLMs-Audio, the first large audio-language model (LALM)\ntailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai\n(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a\nlarge-scale audio corpus, SeaLLMs-Audio exhibits strong performance across\ndiverse audio-centric tasks, spanning fine-grained audio understanding and\nvoice-based interaction. Its key features include: 1) Multilingual: the model\nprimarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,\nand Chinese; 2) Multimodal: the model accepts flexible input modalities,\nincluding audio only, text only, as well as audio with text; 3) Multi-task: the\nmodel supports a wide range of tasks, including audio analysis tasks such as\nAudio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,\nSpeech Emotion Recognition, Speech Question Answering, and Speech\nSummarization. It also enables voice-based dialogue, including answering\nfactual, mathematical, and general knowledge queries. As a significant step\ntowards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to\nbenefit both the regional research community and industry. To automate LALM\nevaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark\nspanning multiple tasks. Experiments show that SeaLLMs-Audio achieves\ncompetitive performance compared with other LALMs on SEA languages.", "AI": {"tldr": "SeaLLMs-Audio是首个针对东南亚语言（印尼语、泰语、越南语）以及英语和中文的大型音频语言模型，支持多语言、多模态和多任务处理，在音频理解与语音交互方面表现优异", "motivation": "填补东南亚地区多语言音频语言模型的空白，为东南亚研究社区和产业界提供先进的音频LLM技术", "method": "基于大规模音频语料库训练，支持音频、文本及混合输入模态，涵盖音频字幕、语音识别、语音翻译、情感识别、问答和摘要等多种任务", "result": "在SeaBench-Audio基准测试中，SeaLLMs-Audio在东南亚语言上与其他LALM相比表现出竞争力", "conclusion": "SeaLLMs-Audio是推进东南亚音频LLM发展的重要一步，预期将促进该地区的研究和产业应用"}}
{"id": "2511.01689", "pdf": "https://arxiv.org/pdf/2511.01689", "abs": "https://arxiv.org/abs/2511.01689", "authors": ["Sharan Maiya", "Henning Bartsch", "Nathan Lambert", "Evan Hubinger"], "title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 6 figures, 4 tables", "summary": "The character of the \"AI assistant\" persona generated by modern chatbot large\nlanguage models influences both surface-level behavior and apparent values,\nbeliefs, and ethics. These all affect interaction quality, perceived\nintelligence, and alignment with both developer and user intentions. The\nshaping of this persona, known as character training, is a critical component\nof industry post-training, yet remains effectively unstudied in the academic\nliterature. We introduce the first open implementation of character training,\nleveraging Constitutional AI and a new data pipeline using synthetic\nintrospective data to shape the assistant persona in a more effective and\ncontrolled manner than alternatives such as constraining system prompts or\nactivation steering. Specifically, we fine-tune three popular open-weights\nmodels using 11 example personas, such as humorous, deeply caring, or even\nmalevolent. To track the effects of our approach, we introduce a method which\nanalyzes revealed preferences, uncovering clear and holistic changes in\ncharacter. We find these changes are more robust to adversarial prompting than\nthe above two alternatives, while also leading to more coherent and realistic\ngenerations. Finally, we demonstrate this fine-tuning has little to no effect\non general capabilities as measured by common benchmarks. We describe and\nopen-source our full post-training method, the implementation of which can be\nfound at https://github.com/maiush/OpenCharacterTraining.", "AI": {"tldr": "本文提出了首个开源的AI助手角色训练方法，通过宪法AI和合成内省数据管道，能够比系统提示约束或激活引导更有效地塑造AI助手的人格特征，同时保持通用能力不受影响。", "motivation": "现代聊天机器人语言模型生成的AI助手角色特征会影响交互质量、感知智能和与开发者用户意图的对齐，但角色训练在学术研究中尚未得到充分研究。", "method": "使用宪法AI和新的合成内省数据管道，对三个流行的开源模型进行微调，采用11种示例人格（如幽默、深度关怀甚至恶意），并通过揭示偏好分析方法追踪效果。", "result": "该方法比系统提示约束和激活引导更抗对抗性提示，生成内容更连贯真实，且对通用基准测试能力几乎没有影响。", "conclusion": "提出的开源角色训练方法能够有效控制AI助手人格特征，为AI助手的人格塑造提供了新的技术路径。"}}
{"id": "2511.01619", "pdf": "https://arxiv.org/pdf/2511.01619", "abs": "https://arxiv.org/abs/2511.01619", "authors": ["Nikola Ljubešić", "Peter Rupnik", "Ivan Porupski", "Taja Kuzman Pungeršek"], "title": "ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian", "categories": ["cs.CL"], "comment": "Submitted to the LREC 2026 conference; 11 pages, 2 figures, 3 tables", "summary": "ParlaSpeech is a collection of spoken parliamentary corpora currently\nspanning four Slavic languages - Croatian, Czech, Polish and Serbian - all\ntogether 6 thousand hours in size. The corpora were built in an automatic\nfashion from the ParlaMint transcripts and their corresponding metadata, which\nwere aligned to the speech recordings of each corresponding parliament. In this\nrelease of the dataset, each of the corpora is significantly enriched with\nvarious automatic annotation layers. The textual modality of all four corpora\nhas been enriched with linguistic annotations and sentiment predictions.\nSimilar to that, their spoken modality has been automatically enriched with\noccurrences of filled pauses, the most frequent disfluency in typical speech.\nTwo out of the four languages have been additionally enriched with detailed\nword- and grapheme-level alignments, and the automatic annotation of the\nposition of primary stress in multisyllabic words. With these enrichments, the\nusefulness of the underlying corpora has been drastically increased for\ndownstream research across multiple disciplines, which we showcase through an\nanalysis of acoustic correlates of sentiment. All the corpora are made\navailable for download in JSONL and TextGrid formats, as well as for search\nthrough a concordancer.", "AI": {"tldr": "ParlaSpeech是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语四种斯拉夫语言的议会语音语料库，总计6000小时，通过自动处理ParlaMint转录本构建，并添加了多种自动标注层。", "motivation": "为多学科下游研究提供丰富的语音和文本标注资源，特别是情感分析和语音特征研究。", "method": "从ParlaMint转录本和元数据自动构建，对齐议会录音，添加语言标注、情感预测、填充停顿检测、词/字位对齐和重音位置标注。", "result": "构建了包含多种自动标注层的多语言议会语音语料库，显著提升了语料库的研究价值，并通过情感声学相关性分析展示了其应用。", "conclusion": "ParlaSpeech语料库通过丰富的自动标注极大增强了其研究实用性，支持跨学科研究，并提供多种格式下载和检索工具。"}}
{"id": "2511.01706", "pdf": "https://arxiv.org/pdf/2511.01706", "abs": "https://arxiv.org/abs/2511.01706", "authors": ["Sekh Mainul Islam", "Pepa Atanasova", "Isabelle Augenstein"], "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement.", "AI": {"tldr": "该论文提出了一个新颖的rank-2投影子空间方法，用于更准确地分离大型语言模型中参数知识(PK)和上下文知识(CK)的贡献，并首次对长自然语言解释序列进行多步知识交互分析。", "motivation": "现有研究主要关注单步生成和rank-1子空间中的二元知识交互，忽视了更丰富的互补和支持性知识交互形式，需要更准确的知识分离方法和多步分析框架。", "method": "提出rank-2投影子空间方法，在四个QA数据集和三个开源指令调优LLM上进行实验，分析多步知识交互模式。", "result": "实验显示rank-1子空间无法有效表示多样知识交互，而rank-2方法能有效捕捉；多步分析发现幻觉NLE与PK方向强相关，上下文忠实NLE平衡PK和CK，CoT提示通过减少PK依赖使NLE向CK偏移。", "conclusion": "该研究为通过更丰富的rank-2子空间分离方法系统研究LLM中的多步知识交互提供了首个框架，有助于更好理解自然语言解释的基础机制。"}}
{"id": "2511.01805", "pdf": "https://arxiv.org/pdf/2511.01805", "abs": "https://arxiv.org/abs/2511.01805", "authors": ["Jiayi Geng", "Howard Chen", "Ryan Liu", "Manoel Horta Ribeiro", "Robb Willer", "Graham Neubig", "Thomas L. Griffiths"], "title": "Accumulating Context Changes the Beliefs of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language model (LM) assistants are increasingly used in applications such as\nbrainstorming and research. Improvements in memory and context size have\nallowed these models to become more autonomous, which has also resulted in more\ntext accumulation in their context windows without explicit user intervention.\nThis comes with a latent risk: the belief profiles of models -- their\nunderstanding of the world as manifested in their responses or actions -- may\nsilently change as context accumulates. This can lead to subtly inconsistent\nuser experiences, or shifts in behavior that deviate from the original\nalignment of the models. In this paper, we explore how accumulating context by\nengaging in interactions and processing text -- talking and reading -- can\nchange the beliefs of language models, as manifested in their responses and\nbehaviors.Our results reveal that models' belief profiles are highly malleable:\nGPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of\ndiscussion about moral dilemmas and queries about safety, while Grok 4 shows a\n27.2% shift on political issues after reading texts from the opposing position.\nWe also examine models' behavioral changes by designing tasks that require tool\nuse, where each tool selection corresponds to an implicit belief. We find that\nthese changes align with stated belief shifts, suggesting that belief shifts\nwill be reflected in actual behavior in agentic systems. Our analysis exposes\nthe hidden risk of belief shift as models undergo extended sessions of talking\nor reading, rendering their opinions and actions unreliable.", "AI": {"tldr": "语言模型在长时间对话和阅读过程中，其信念会显著改变，导致响应和行为的不一致，这构成了潜在风险", "motivation": "随着语言模型自主性增强，上下文积累可能导致模型信念无声变化，造成用户体验不一致和行为偏离原始对齐目标", "method": "通过多轮道德困境讨论和政治立场文本阅读测试模型信念变化，并设计工具使用任务来观察行为变化", "result": "GPT-5在10轮道德讨论后信念改变54.7%，Grok 4在阅读对立政治文本后信念改变27.2%，行为变化与信念变化一致", "conclusion": "语言模型在扩展会话中会经历显著的信念漂移，这使其意见和行为变得不可靠，需要新的对齐技术来解决这一风险"}}
{"id": "2511.01649", "pdf": "https://arxiv.org/pdf/2511.01649", "abs": "https://arxiv.org/abs/2511.01649", "authors": ["Hung-Shin Lee", "Chen-Chi Chang", "Ching-Yuan Chen", "Yun-Hsiang Hsu"], "title": "Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "This paper has been accepted by The Electronic Library, and the full\n  article is now available on Emerald Insight", "summary": "This study proposes a cognitive benchmarking framework to evaluate how large\nlanguage models (LLMs) process and apply culturally specific knowledge. The\nframework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)\nto assess model performance across six hierarchical cognitive domains:\nRemembering, Understanding, Applying, Analyzing, Evaluating, and Creating.\nUsing a curated Taiwanese Hakka digital cultural archive as the primary\ntestbed, the evaluation measures LLM-generated responses' semantic accuracy and\ncultural relevance.", "AI": {"tldr": "提出基于布鲁姆分类学和RAG的认知基准框架，评估大语言模型处理文化特定知识的能力，使用台湾客家数字文化档案进行测试。", "motivation": "评估大语言模型在文化特定知识处理方面的表现，特别是对少数民族文化的理解和应用能力。", "method": "整合布鲁姆分类学的六个认知层次（记忆、理解、应用、分析、评估、创造）与检索增强生成技术，使用台湾客家数字文化档案作为测试数据集。", "result": "通过测量模型生成响应的语义准确性和文化相关性来评估性能。", "conclusion": "该框架为评估LLMs的文化认知能力提供了系统化的方法论，有助于提升模型对多元文化的理解和应用水平。"}}
{"id": "2511.01807", "pdf": "https://arxiv.org/pdf/2511.01807", "abs": "https://arxiv.org/abs/2511.01807", "authors": ["Adewale Akinfaderin", "Shreyas Subramanian", "Akarsha Sehwag"], "title": "Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at Workshop on Prompt Optimization, KDD 2025, Toronto,\n  Canada", "summary": "Length control in Large Language Models (LLMs) is a crucial but\nunder-addressed challenge, with applications ranging from voice interfaces\nrequiring concise responses to research summaries needing comprehensive\noutputs. Current approaches to length control, including Regularized DPO,\nLength-Instruction Fine Tuning, and tool-augmented methods, typically require\nexpensive model retraining or complex inference-time tooling. This paper\npresents a prompt engineering methodology that enables precise length control\nwithout model retraining. Our structure-guided approach implements deliberate\nplanning and word counting mechanisms within the prompt, encouraging the model\nto carefully track and adhere to specified length constraints. Comprehensive\nevaluations across six state-of-the-art LLMs demonstrate that our method\nsignificantly improves length fidelity for several models compared to standard\nprompting when applied to document summarization tasks, particularly for\nshorter-to-medium length constraints. The proposed technique shows varying\nbenefits across different model architectures, with some models demonstrating\nup to 37.6% improvement in length adherence. Quality evaluations further reveal\nthat our approach maintains or enhances overall output quality compared to\nstandard prompting techniques. Our approach provides an immediately deployable\nsolution for applications requiring precise length control, particularly\nvaluable for production environments where model retraining is impractical or\ncost-prohibitive.", "AI": {"tldr": "提出一种无需重新训练模型即可实现精确长度控制的提示工程方法，通过结构引导和字数统计机制，在文档摘要任务中显著提高了长度遵循能力", "motivation": "大型语言模型的长度控制是一个重要但未被充分解决的挑战，当前方法需要昂贵的模型重新训练或复杂的推理时工具", "method": "采用结构引导的提示工程方法，在提示中实施精心规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度约束", "result": "在6个最先进的大语言模型上评估显示，该方法显著提高了长度保真度，某些模型长度遵循度提升高达37.6%，同时保持或提升了输出质量", "conclusion": "该方法为需要精确长度控制的应用提供了立即可部署的解决方案，特别适用于模型重新训练不切实际或成本过高的生产环境"}}
{"id": "2511.01815", "pdf": "https://arxiv.org/pdf/2511.01815", "abs": "https://arxiv.org/abs/2511.01815", "authors": ["Konrad Staniszewski", "Adrian Łańcucki"], "title": "KV Cache Transform Coding for Compact Storage in LLM Inference", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches.", "AI": {"tldr": "KVTC是一种轻量级变换编码器，通过PCA特征解相关、自适应量化和熵编码技术压缩KV缓存，实现高达20倍压缩比，保持推理精度，提升大语言模型服务的内存效率。", "motivation": "大语言模型服务中KV缓存占用大量GPU内存，传统方法如卸载或重计算效率低下，需要一种高效的压缩方法来支持可重用KV缓存。", "method": "基于经典媒体压缩技术，结合PCA特征解相关、自适应量化和熵编码，仅需简短初始校准，不改变模型参数。", "result": "在Llama 3等模型上测试显示，KVTC实现高达20倍压缩（特定用例40倍以上），在AIME25等多个基准测试中优于令牌驱逐、量化和SVD等基线方法。", "conclusion": "KVTC是内存高效LLM服务的实用构建模块，通过压缩KV缓存解决了GPU内存稀缺问题，支持可重用缓存的高效管理。"}}
{"id": "2511.01846", "pdf": "https://arxiv.org/pdf/2511.01846", "abs": "https://arxiv.org/abs/2511.01846", "authors": ["Thang Luong", "Dawsen Hwang", "Hoang H. Nguyen", "Golnaz Ghiasi", "Yuri Chervonyi", "Insuk Seo", "Junsu Kim", "Garrett Bingham", "Jonathan Lee", "Swaroop Mishra", "Alex Zhai", "Clara Huiyi Hu", "Henryk Michalewski", "Jimin Kim", "Jeonghyun Ahn", "Junhwi Bae", "Xingyou Song", "Trieu H. Trinh", "Quoc V. Le", "Junehyuk Jung"], "title": "Towards Robust Mathematical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 (main conference),\n  https://aclanthology.org/2025.emnlp-main.1794/", "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.", "AI": {"tldr": "IMO-Bench是一个针对国际数学奥林匹克竞赛水平的数学推理基准套件，包含答案基准和证明基准，用于评估基础模型的数学推理能力。Gemini Deep Think在该基准上取得了突破性成果。", "motivation": "现有数学推理评估要么过于简单，要么只关注简短答案，缺乏对高级数学推理能力的全面评估，特别是国际数学奥林匹克竞赛水平的复杂问题解决能力。", "method": "开发IMO-Bench基准套件，包含IMO-AnswerBench（400个多样化奥数问题）和IMO-Proof Bench（证明写作能力评估），由顶尖专家团队审核。建立自动评分系统和IMO-GradingBench（包含1000个人工评分的证明）。", "result": "Gemini Deep Think在IMO-AnswerBench上达到80.0%，在高级IMO-Proof Bench上达到65.7%，分别比非Gemini最佳模型高出6.9%和42.4%。自动评分器与人工评估相关性良好。", "conclusion": "IMO-Bench为推进数学推理研究提供了重要基准，有助于社区在自动评估长答案和高级数学推理方面取得进一步进展，模型在IMO 2025中取得了金牌级别的历史性成就。"}}
{"id": "2511.01720", "pdf": "https://arxiv.org/pdf/2511.01720", "abs": "https://arxiv.org/abs/2511.01720", "authors": ["Mahammad Nuriyev"], "title": "Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue", "categories": ["cs.CL"], "comment": "10 pages, 1 figure, 2 tables. Technical report for the Commonsense\n  Persona-Grounded Dialogue Challenge (CPDC) 2025, part of the Wordplay 2025\n  Workshop @ EMNLP 2025", "summary": "We present a multi-expert system for creating Non-Player Characters (NPCs)\ncapable of both natural dialogue and contextual action execution in interactive\nenvironments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)\nadapters, we instantiate three specialists: tool calling, tool-response\ninterpretation, and direct dialogue. Our system comfortably meets the\ncomputational efficiency requirements, delivering fast responses and\nmaintaining modest resource usage on L40S GPUs. In the Commonsense\nPersona-Grounded Dialogue Challenge 2025, our method ranked second overall.\n  Code available at:\nhttps://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/", "AI": {"tldr": "基于Qwen3和LoRA的多专家系统，用于创建具有自然对话和上下文动作执行能力的NPC，在CPDC 2025挑战赛中排名第二", "motivation": "开发能够同时处理自然对话和上下文动作执行的NPC系统，满足交互环境中的计算效率要求", "method": "使用Qwen3作为基础模型，通过LoRA适配器实例化三个专家模块：工具调用、工具响应解释和直接对话", "result": "系统在L40S GPU上实现了快速响应和适中的资源使用，在Commonsense Persona-Grounded Dialogue Challenge 2025中获得第二名", "conclusion": "该方法证明了多专家系统在NPC开发中的有效性，为交互环境中的智能角色提供了高效解决方案"}}
{"id": "2511.01854", "pdf": "https://arxiv.org/pdf/2511.01854", "abs": "https://arxiv.org/abs/2511.01854", "authors": ["Elias Lumer", "Faheem Nizar", "Anmol Gulati", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah"], "title": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark.", "AI": {"tldr": "论文提出Tool-to-Agent Retrieval框架，通过在共享向量空间中嵌入工具和代理的元数据关系，实现细粒度的工具级和代理级检索，解决了现有检索方法因粗粒度匹配导致的代理选择不佳问题。", "motivation": "现有LLM多代理系统的检索方法通常基于粗粒度的代理级别描述进行查询匹配，这掩盖了细粒度的工具功能，导致代理选择不理想。", "method": "引入Tool-to-Agent Retrieval统一框架，在共享向量空间中嵌入工具及其父代理，通过元数据关系连接它们，显式表示工具能力并通过元数据遍历到代理级别。", "result": "在八个嵌入模型上评估，在LiveMCPBench基准测试中，Recall@5提升19.4%，nDCG@5提升17.7%，优于现有最先进的代理检索方法。", "conclusion": "Tool-to-Agent Retrieval框架通过细粒度的工具和代理表示，有效解决了多代理系统中的检索问题，显著提升了检索性能。"}}
