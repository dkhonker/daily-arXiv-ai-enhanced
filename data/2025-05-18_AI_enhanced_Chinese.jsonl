{"id": "2505.09660", "pdf": "https://arxiv.org/pdf/2505.09660", "abs": "https://arxiv.org/abs/2505.09660", "authors": ["Saptarshi Saha", "Dhruv Vansraj Rathore", "Soumadeep Saha", "Utpal Garain", "David Doermann"], "title": "On Measuring Intrinsic Causal Attributions in Deep Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Quantifying the causal influence of input features within neural networks has\nbecome a topic of increasing interest. Existing approaches typically assess\ndirect, indirect, and total causal effects. This work treats NNs as structural\ncausal models (SCMs) and extends our focus to include intrinsic causal\ncontributions (ICC). We propose an identifiable generative post-hoc framework\nfor quantifying ICC. We also draw a relationship between ICC and Sobol'\nindices. Our experiments on synthetic and real-world datasets demonstrate that\nICC generates more intuitive and reliable explanations compared to existing\nglobal explanation techniques.", "AI": {"tldr": "This work proposes a new method for quantifying intrinsic causal contributions (ICC) within neural networks by treating them as structural causal models (SCMs), demonstrating through experiments that ICC provides more intuitive and reliable explanations than existing global explanation techniques.", "motivation": "To develop a more intuitive and reliable way to quantify causal influences of input features within neural networks.", "method": "Proposing an identifiable generative post-hoc framework to quantify ICC and drawing a relationship between ICC and Sobol' indices.", "result": "Experiments on both synthetic and real-world datasets showed that ICC generates more intuitive and reliable explanations compared to existing global explanation techniques.", "conclusion": "The proposed method using ICC as part of SCMs offers improved understanding of neural network causal influences."}}
{"id": "2505.09748", "pdf": "https://arxiv.org/pdf/2505.09748", "abs": "https://arxiv.org/abs/2505.09748", "authors": ["Jitendra K Tugnait"], "title": "Learning Multi-Attribute Differential Graphs with Non-Convex Penalties", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "14 pages, 1 figures, 2 tables, published in IEEE Access, pp.\n  67065-67078, 2025", "summary": "We consider the problem of estimating differences in two multi-attribute\nGaussian graphical models (GGMs) which are known to have similar structure,\nusing a penalized D-trace loss function with non-convex penalties. The GGM\nstructure is encoded in its precision (inverse covariance) matrix. Existing\nmethods for multi-attribute differential graph estimation are based on a group\nlasso penalized loss function. In this paper, we consider a penalized D-trace\nloss function with non-convex (log-sum and smoothly clipped absolute deviation\n(SCAD)) penalties. Two proximal gradient descent methods are presented to\noptimize the objective function. Theoretical analysis establishing sufficient\nconditions for consistency in support recovery, convexity and estimation in\nhigh-dimensional settings is provided. We illustrate our approaches with\nnumerical examples based on synthetic and real data.", "AI": {"tldr": "Estimating differences in two multi-attribute Gaussian graphical models using penalized D-trace loss function with non-convex penalties.", "motivation": "To improve existing methods for multi-attribute differential graph estimation by considering non-convex penalties.", "method": "Using a penalized D-trace loss function with non-convex penalties and presenting two proximal gradient descent methods to optimize the objective function.", "result": "Theoretical analysis establishes sufficient conditions for consistency in support recovery, convexity and estimation in high-dimensional settings. Numerical examples based on synthetic and real data illustrate the approaches.", "conclusion": "The proposed method provides a promising approach for estimating differences in two multi-attribute Gaussian graphical models."}}
{"id": "2505.09803", "pdf": "https://arxiv.org/pdf/2505.09803", "abs": "https://arxiv.org/abs/2505.09803", "authors": ["Antony Sikorski", "Michael Ivanitskiy", "Nathan Lenssen", "Douglas Nychka", "Daniel McKenzie"], "title": "LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In many scientific and industrial applications, we are given a handful of\ninstances (a 'small ensemble') of a spatially distributed quantity (a 'field')\nbut would like to acquire many more. For example, a large ensemble of global\ntemperature sensitivity fields from a climate model can help farmers, insurers,\nand governments plan appropriately. When acquiring more data is prohibitively\nexpensive -- as is the case with climate models -- statistical emulation offers\nan efficient alternative for simulating synthetic yet realistic fields.\nHowever, parameter inference using maximum likelihood estimation (MLE) is\ncomputationally prohibitive, especially for large, non-stationary fields. Thus,\nmany recent works train neural networks to estimate parameters given spatial\nfields as input, sidestepping MLE completely. In this work we focus on a\npopular class of parametric, spatially autoregressive (SAR) models. We make a\nsimple yet impactful observation; because the SAR parameters can be arranged on\na regular grid, both inputs (spatial fields) and outputs (model parameters) can\nbe viewed as images. Using this insight, we demonstrate that image-to-image\n(I2I) networks enable faster and more accurate parameter estimation for a class\nof non-stationary SAR models with unprecedented complexity.", "AI": {"tldr": "This paper introduces an innovative approach to parameter estimation for non-stationary spatially autoregressive models by treating inputs and outputs as images, enabling faster and more accurate estimation using image-to-image networks.", "motivation": "To provide a cost-effective solution for acquiring large ensembles of spatially distributed quantities when direct data acquisition is expensive.", "method": "Utilizing image-to-image networks to estimate parameters for non-stationary spatially autoregressive models by viewing both inputs and outputs as images.", "result": "The proposed method enables faster and more accurate parameter estimation for complex non-stationary spatially autoregressive models.", "conclusion": "Image-to-image networks offer a promising alternative for parameter estimation in spatially autoregressive models, particularly beneficial for large, non-stationary fields where traditional methods are computationally prohibitive."}}
{"id": "2505.10099", "pdf": "https://arxiv.org/pdf/2505.10099", "abs": "https://arxiv.org/abs/2505.10099", "authors": ["Sarat Moka", "Matias Quiroz", "Vali Asimit", "Samuel Muller"], "title": "A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection", "categories": ["stat.ML", "cs.LG", "math.OC", "q-fin.PM"], "comment": null, "summary": "Portfolio optimization involves selecting asset weights to minimize a\nrisk-reward objective, such as the portfolio variance in the classical\nminimum-variance framework. Sparse portfolio selection extends this by imposing\na cardinality constraint: only $k$ assets from a universe of $p$ may be\nincluded. The standard approach models this problem as a mixed-integer\nquadratic program and relies on commercial solvers to find the optimal\nsolution. However, the computational costs of such methods increase\nexponentially with $k$ and $p$, making them too slow for problems of even\nmoderate size. We propose a fast and scalable gradient-based approach that\ntransforms the combinatorial sparse selection problem into a constrained\ncontinuous optimization task via Boolean relaxation, while preserving\nequivalence with the original problem on the set of binary points. Our\nalgorithm employs a tunable parameter that transmutes the auxiliary objective\nfrom a convex to a concave function. This allows a stable convex starting\npoint, followed by a controlled path toward a sparse binary solution as the\ntuning parameter increases and the objective moves toward concavity. In\npractice, our method matches commercial solvers in asset selection for most\ninstances and, in rare instances, the solution differs by a few assets whilst\nshowing a negligible error in portfolio variance.", "AI": {"tldr": "This paper proposes a new method for sparse portfolio selection that is faster and more scalable than traditional approaches.", "motivation": "The computational costs of traditional methods increase exponentially with k and p, making them inefficient for moderate-sized problems.", "method": "A gradient-based approach transforming the combinatorial sparse selection problem into a constrained continuous optimization task via Boolean relaxation.", "result": "Our method matches commercial solvers in asset selection and shows negligible error in portfolio variance.", "conclusion": "Our algorithm provides a fast and scalable way to solve the sparse portfolio selection problem."}}
{"id": "2505.09659", "pdf": "https://arxiv.org/pdf/2505.09659", "abs": "https://arxiv.org/abs/2505.09659", "authors": ["Long Chen", "Xiaotian Song", "Yanan Sun"], "title": "LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Spiking Large Language Models (LLMs) have emerged as an energy-efficient\nalternative to conventional LLMs through their event-driven computation. To\neffectively obtain spiking LLMs, researchers develop different ANN-to-SNN\nconversion methods by leveraging pre-trained ANN parameters while inheriting\nthe energy efficiency of SNN. However, existing conversion methods struggle\nwith extreme activation outliers and incompatible nonlinear operations of\nANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for\nfully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel\nneurons to convert the activation outlier and nonlinear operation of ANN-based\nLLMs. Moreover, LAS tailors the spike-equivalent Transformer components for\nspiking LLMs, which can ensure full spiking conversion without any loss of\nperformance. Experimental results on six language models and two\nvision-language models demonstrate that LAS achieves loss-less conversion.\nNotably, on OPT-66B, LAS even improves the accuracy of 2\\% on the WSC task. In\naddition, the parameter and ablation studies further verify the effectiveness\nof LAS. The source code is available at https://github.com/lc783/LAS", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAS\u7684\u65e0\u635fANN-SNN\u8f6c\u6362\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u95ee\u9898\u5e76\u63d0\u9ad8LLMs\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8f6c\u6362\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u57fa\u4e8eANN\u7684LLMs\u4e2d\u7684\u6781\u7aef\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u4e0d\u517c\u5bb9\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\u3002", "method": "LAS\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u578b\u795e\u7ecf\u5143\u6765\u8f6c\u6362ANN-LMMs\u4e2d\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u5e76\u4e3a\u8109\u51b2LLMs\u5b9a\u5236\u4e86\u7b49\u6548Transformer\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLAS\u5728\u516d\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u65e0\u635f\u8f6c\u6362\uff0c\u5e76\u4e14\u5728OPT-66B\u4e0a\u63d0\u9ad8\u4e86WSC\u4efb\u52a1\u7684\u51c6\u786e\u73872%\u3002\u53c2\u6570\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86LAS\u7684\u6709\u6548\u6027\u3002", "conclusion": "LAS\u5b9e\u73b0\u4e86\u4e00\u79cd\u65e0\u635f\u7684ANN-SNN\u8f6c\u6362\uff0c\u80fd\u591f\u786e\u4fdd\u5168\u8109\u51b2\u8f6c\u6362\u800c\u4e0d\u635f\u5931\u4efb\u4f55\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLAS\u5728\u516d\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u65e0\u635f\u8f6c\u6362\uff0c\u5e76\u4e14\u5728OPT-66B\u4e0a\u751a\u81f3\u63d0\u9ad8\u4e86WSC\u4efb\u52a1\u7684\u51c6\u786e\u73872%\u3002\u53c2\u6570\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86LAS\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09639", "pdf": "https://arxiv.org/pdf/2505.09639", "abs": "https://arxiv.org/abs/2505.09639", "authors": ["Quentin Cohen-Solal"], "title": "Study and improvement of search algorithms in two-players perfect information games", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Games, in their mathematical sense, are everywhere (game industries,\neconomics, defense, education, chemistry, biology, ...).Search algorithms in\ngames are artificial intelligence methods for playing such games.\nUnfortunately, there is no study on these algorithms that evaluates the\ngenerality of their performance. We propose to address this gap in the case of\ntwo-player zero-sum games with perfect information. Furthermore, we propose a\nnew search algorithm and we show that, for a short search time, it outperforms\nall studied algorithms on all games in this large experiment and that, for a\nmedium search time, it outperforms all studied algorithms on 17 of the 22\nstudied games.", "AI": {"tldr": "This paper addresses the lack of studies evaluating the generality of search algorithms in two-player zero-sum games with perfect information. It proposes a new search algorithm that performs better than existing ones in shorter search times and outperforms all others in 17 out of 22 games with medium search times.", "motivation": "To evaluate the generality of search algorithms in games and fill the research gap.", "method": "Proposing a new search algorithm for two-player zero-sum games with perfect information.", "result": "The new algorithm outperforms all studied algorithms on all games in a short search time and on 17 of the 22 studied games in a medium search time.", "conclusion": "The proposed algorithm demonstrates superior performance compared to existing algorithms in certain conditions."}}
{"id": "2505.10139", "pdf": "https://arxiv.org/pdf/2505.10139", "abs": "https://arxiv.org/abs/2505.10139", "authors": ["Lorenz Vaitl", "Leon Klein"], "title": "Path Gradients after Flow Matching", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Boltzmann Generators have emerged as a promising machine learning tool for\ngenerating samples from equilibrium distributions of molecular systems using\nNormalizing Flows and importance weighting. Recently, Flow Matching has helped\nspeed up Continuous Normalizing Flows (CNFs), scale them to more complex\nmolecular systems, and minimize the length of the flow integration\ntrajectories. We investigate the benefits of using path gradients to fine-tune\nCNFs initially trained by Flow Matching, in the setting where a target energy\nis known. Our experiments show that this hybrid approach yields up to a\nthreefold increase in sampling efficiency for molecular systems, all while\nusing the same model, a similar computational budget and without the need for\nadditional sampling. Furthermore, by measuring the length of the flow\ntrajectories during fine-tuning, we show that path gradients largely preserve\nthe learned structure of the flow.", "AI": {"tldr": "This paper investigates using path gradients to refine Continuous Normalizing Flows (CNFs) initially trained by Flow Matching for molecular systems sampling, showing a threefold increase in efficiency.", "motivation": "To improve the sampling efficiency of molecular systems using CNFs.", "method": "Hybrid approach combining Flow Matching and path gradients.", "result": "Up to threefold increase in sampling efficiency for molecular systems with the same model and computational budget.", "conclusion": "Path gradients can significantly enhance the performance of CNFs in molecular systems sampling without additional sampling needs."}}
{"id": "2505.09663", "pdf": "https://arxiv.org/pdf/2505.09663", "abs": "https://arxiv.org/abs/2505.09663", "authors": ["Julian B\u00fcchel", "Iason Chalas", "Giovanni Acampa", "An Chen", "Omobayode Fagbohungbe", "Sidney Tsai", "Kaoutar El Maghraoui", "Manuel Le Gallo", "Abbas Rahimi", "Abu Sebastian"], "title": "Analog Foundation Models", "categories": ["cs.LG"], "comment": "43 pages, 8 figures, under review", "summary": "Analog in-memory computing (AIMC) is a promising compute paradigm to improve\nspeed and power efficiency of neural network inference beyond the limits of\nconventional von Neumann-based architectures. However, AIMC introduces\nfundamental challenges such as noisy computations and strict constraints on\ninput and output quantization. Because of these constraints and imprecisions,\noff-the-shelf LLMs are not able to achieve 4-bit-level performance when\ndeployed on AIMC-based hardware. While researchers previously investigated\nrecovering this accuracy gap on small, mostly vision-based models, a generic\nmethod applicable to LLMs pre-trained on trillions of tokens does not yet\nexist. In this work, we introduce a general and scalable method to robustly\nadapt LLMs for execution on noisy, low-precision analog hardware. Our approach\nenables state-of-the-art models $\\unicode{x2013}$ including\nPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\\unicode{x2013}$ to retain\nperformance comparable to 4-bit weight, 8-bit activation baselines, despite the\npresence of analog noise and quantization constraints. Additionally, we show\nthat as a byproduct of our training methodology, analog foundation models can\nbe quantized for inference on low-precision digital hardware. Finally, we show\nthat our models also benefit from test-time compute scaling, showing better\nscaling behavior than models trained with 4-bit weight and 8-bit static input\nquantization. Our work bridges the gap between high-capacity LLMs and efficient\nanalog hardware, offering a path toward energy-efficient foundation models.\nCode is available at https://github.com/IBM/analog-foundation-models .", "AI": {"tldr": "A novel method to adapt large language models for execution on analog in-memory computing hardware achieving performance comparable to 4-bit weight and 8-bit activation baselines.", "motivation": "To address the challenge of deploying large language models on analog in-memory computing hardware which suffers from noisy computations and strict constraints on input and output quantization.", "method": "We introduce a general and scalable method to robustly adapt large language models for execution on noisy, low-precision analog hardware.", "result": "Our models retain performance comparable to 4-bit weight and 8-bit activation baselines despite analog noise and quantization constraints. They also benefit from test-time compute scaling and can be quantized for inference on low-precision digital hardware.", "conclusion": "Our method enables state-of-the-art large language models to be executed on analog hardware with performance comparable to 4-bit weight and 8-bit activation baselines."}}
{"id": "2505.09640", "pdf": "https://arxiv.org/pdf/2505.09640", "abs": "https://arxiv.org/abs/2505.09640", "authors": ["Tom\u00e1s Capdevielle", "Santiago Cifuentes"], "title": "Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms", "categories": ["cs.AI", "68T01", "I.2.0"], "comment": "22 pages, 7 figures", "summary": "Given a classification model and a prediction for some input, there are\nheuristic strategies for ranking features according to their importance in\nregard to the prediction. One common approach to this task is rooted in\npropositional logic and the notion of \\textit{sufficient reason}. Through this\nconcept, the categories of relevant and necessary features were proposed in\norder to identify the crucial aspects of the input. This paper improves the\nexisting techniques and algorithms for deciding which are the relevant and/or\nnecessary features, showing in particular that necessity can be detected\nefficiently in complex models such as neural networks. We also generalize the\nnotion of relevancy and study associated problems. Moreover, we present a new\nglobal notion (i.e. that intends to explain whether a feature is important for\nthe behavior of the model in general, not depending on a particular input) of\n\\textit{usefulness} and prove that it is related to relevancy and necessity.\nFurthermore, we develop efficient algorithms for detecting it in decision trees\nand other more complex models, and experiment on three datasets to analyze its\npractical utility.", "AI": {"tldr": "This paper improves existing techniques and algorithms for identifying relevant and necessary features in complex models like neural networks. It also introduces a new concept of 'usefulness' and develops efficient algorithms to detect it in different types of models.", "motivation": "To improve the existing techniques and algorithms for identifying crucial aspects of input features in classification models.", "method": "Proposing a new global notion of 'usefulness' and developing efficient algorithms to detect it in various models.", "result": "Efficient detection of necessity in complex models like neural networks and successful generalization of the notion of relevancy. Also, practical utility analysis of the new 'usefulness' concept on three datasets.", "conclusion": "The paper successfully enhances the understanding of feature importance in classification models and introduces a new concept of 'usefulness' with efficient detection methods."}}
{"id": "2505.10160", "pdf": "https://arxiv.org/pdf/2505.10160", "abs": "https://arxiv.org/abs/2505.10160", "authors": ["Yannis Montreuil", "Axel Carlier", "Lai Xing Ng", "Wei Tsang Ooi"], "title": "One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which\nunifies prediction and deferral by learning a shared score-based model that\nselects the $k$ most cost-effective entities-labels or experts-per input. While\nexisting one-stage L2D methods are limited to deferring to a single expert, our\napproach jointly optimizes prediction and deferral across multiple entities\nthrough a single end-to-end objective. We define a cost-sensitive loss and\nderive a novel convex surrogate that is independent of the cardinality\nparameter $k$, enabling generalization across Top-$k$ regimes without\nretraining. Our formulation recovers the Top-1 deferral policy of prior\nscore-based methods as a special case, and we prove that our surrogate is both\nBayes-consistent and $\\mathcal{H}$-consistent under mild assumptions. We\nfurther introduce an adaptive variant, Top-$k(x)$, which dynamically selects\nthe number of consulted entities per input to balance predictive accuracy and\nconsultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage\nTop-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves\nsuperior accuracy-cost trade-offs by tailoring allocations to input complexity.", "AI": {"tldr": "This paper introduces a new one-stage Top-k Learning-to-Defer framework that outperforms previous Top-1 deferral methods.", "motivation": "Existing one-stage L2D methods are limited to deferring to a single expert.", "method": "We introduce the first one-stage Top-$k$ Learning-to-Defer framework.", "result": "Our one-stage Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves superior accuracy-cost trade-offs.", "conclusion": "Our approach provides a new way to unify prediction and deferral through a single end-to-end objective."}}
{"id": "2505.09702", "pdf": "https://arxiv.org/pdf/2505.09702", "abs": "https://arxiv.org/abs/2505.09702", "authors": ["Yezi Liu", "Prathyush Poduval", "Wenjun Huang", "Yang Ni", "Hanning Chen", "Mohsen Imani"], "title": "Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing", "categories": ["cs.LG"], "comment": null, "summary": "Graph unlearning is a crucial approach for protecting user privacy by erasing\nthe influence of user data on trained graph models. Recent developments in\ngraph unlearning methods have primarily focused on maintaining model prediction\nperformance while removing user information. However, we have observed that\nwhen user information is deleted from the model, the prediction distribution\nacross different sensitive groups often changes. Furthermore, graph models are\nshown to be prone to amplifying biases, making the study of fairness in graph\nunlearning particularly important. This raises the question: Does graph\nunlearning actually introduce bias? Our findings indicate that the predictions\nof post-unlearning models become highly correlated with sensitive attributes,\nconfirming the introduction of bias in the graph unlearning process. To address\nthis issue, we propose a fair graph unlearning method, FGU. To guarantee\nprivacy, FGU trains shard models on partitioned subgraphs, unlearns the\nrequested data from the corresponding subgraphs, and retrains the shard models\non the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing\nprocess: it first enables shard-level fairness by incorporating a fairness\nregularizer in the shard model retraining, and then achieves global-level\nfairness by aligning all shard models to minimize global disparity. Our\nexperiments demonstrate that FGU achieves superior fairness while maintaining\nprivacy and accuracy. Additionally, FGU is robust to diverse unlearning\nrequests, ensuring fairness and utility performance across various data\ndistributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGU\u7684\u516c\u5e73\u56fe\u65e0\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u516c\u5e73\u6027\uff0c\u5e76\u4e14\u5bf9\u5404\u79cd\u65e0\u5b66\u4e60\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u65e0\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e8e\u5728\u5220\u9664\u7528\u6237\u4fe1\u606f\u7684\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u9884\u6d4b\u6027\u80fd\uff0c\u4f46\u5f53\u7528\u6237\u4fe1\u606f\u4ece\u6a21\u578b\u4e2d\u5220\u9664\u65f6\uff0c\u4e0d\u540c\u654f\u611f\u7ec4\u4e4b\u95f4\u7684\u9884\u6d4b\u5206\u5e03\u5f80\u5f80\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u5e76\u4e14\u56fe\u6a21\u578b\u5bb9\u6613\u653e\u5927\u504f\u5dee\uff0c\u56e0\u6b64\u7814\u7a76\u56fe\u65e0\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u5c24\u4e3a\u91cd\u8981\u3002", "method": "FGU\u65b9\u6cd5\u901a\u8fc7\u5728\u5212\u5206\u7684\u5c0f\u56fe\u4e0a\u8bad\u7ec3\u5206\u7247\u6a21\u578b\uff0c\u5728\u5bf9\u5e94\u7684\u5b50\u56fe\u4e2d\u65e0\u5b66\u4e60\u8bf7\u6c42\u7684\u6570\u636e\uff0c\u5e76\u5728\u4fee\u6539\u540e\u7684\u5b50\u56fe\u4e0a\u91cd\u65b0\u8bad\u7ec3\u5206\u7247\u6a21\u578b\u6765\u4fdd\u8bc1\u9690\u79c1\u3002\u4e3a\u4e86\u786e\u4fdd\u516c\u5e73\u6027\uff0cFGU\u91c7\u7528\u53cc\u5c42\u53bb\u504f\u8fc7\u7a0b\uff1a\u9996\u5148\u5728\u5206\u7247\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u65f6\u52a0\u5165\u516c\u5e73\u6b63\u5219\u5316\u5668\u5b9e\u73b0\u5206\u7247\u7ea7\u516c\u5e73\u6027\uff1b\u7136\u540e\u901a\u8fc7\u4f7f\u6240\u6709\u5206\u7247\u6a21\u578b\u5bf9\u9f50\u6765\u6700\u5c0f\u5316\u5168\u5c40\u5dee\u5f02\uff0c\u4ece\u800c\u5b9e\u73b0\u5168\u5c40\u7ea7\u516c\u5e73\u6027\u3002", "result": "\u63d0\u51fa\u7684FGU\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u516c\u5e73\u6027\uff0c\u5e76\u4e14\u5bf9\u5404\u79cd\u65e0\u5b66\u4e60\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u4e86\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u56fe\u65e0\u5b66\u4e60\u8fc7\u7a0b\u786e\u5b9e\u5f15\u5165\u4e86\u504f\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u516c\u5e73\u56fe\u65e0\u5b66\u4e60\u65b9\u6cd5FGU\u3002\u5b9e\u9a8c\u8868\u660e\uff0cFGU\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u516c\u5e73\u6027\uff0c\u5e76\u4e14\u5bf9\u5404\u79cd\u65e0\u5b66\u4e60\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u4e86\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\u6027\u80fd\u3002"}}
{"id": "2505.09737", "pdf": "https://arxiv.org/pdf/2505.09737", "abs": "https://arxiv.org/abs/2505.09737", "authors": ["Osher Elhadad", "Reuth Mirsky"], "title": "General Dynamic Goal Recognition", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted for publication at Generalization in Planning (GenPlan) as\n  part of AAAI 2025 workshops", "summary": "Understanding an agent's intent through its behavior is essential in\nhuman-robot interaction, interactive AI systems, and multi-agent\ncollaborations. This task, known as Goal Recognition (GR), poses significant\nchallenges in dynamic environments where goals are numerous and constantly\nevolving. Traditional GR methods, designed for a predefined set of goals, often\nstruggle to adapt to these dynamic scenarios. To address this limitation, we\nintroduce the General Dynamic GR problem - a broader definition of GR - aimed\nat enabling real-time GR systems and fostering further research in this area.\nExpanding on this foundation, this paper employs a model-free goal-conditioned\nRL approach to enable fast adaptation for GR across various changing tasks.", "AI": {"tldr": "This paper introduces the General Dynamic GR problem and proposes a model-free goal-conditioned RL approach to enable fast adaptation for GR across various changing tasks.", "motivation": "To enable real-time GR systems and foster further research in dynamic environments where goals are numerous and constantly evolving.", "method": "model-free goal-conditioned RL approach", "result": "The proposed method can achieve fast adaptation for GR across various changing tasks.", "conclusion": "This paper introduces the General Dynamic GR problem and proposes a model-free goal-conditioned RL approach to achieve fast adaptation for GR across various changing tasks."}}
{"id": "2505.10448", "pdf": "https://arxiv.org/pdf/2505.10448", "abs": "https://arxiv.org/abs/2505.10448", "authors": ["Conor Rosato", "Harvinder Lehal", "Simon Maskell", "Lee Devlin", "Malcolm Strens"], "title": "Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods", "categories": ["stat.ML", "cs.LG"], "comment": "45 pages", "summary": "Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when\nthe likelihood function is irregular and expensive to compute. We explore\nseveral sampling algorithms that make use of subset evaluations to reduce\ncomputational overhead. We adapt the subset samplers for this setting where\ngradient information is not available or is unreliable. To achieve this, we\nintroduce data-driven proxies in place of Taylor expansions and define a novel\ncomputation-cost aware adaptive controller. We undertake an extensive\nevaluation for a challenging disease modelling task and a configurable task\nwith similar irregularity in the likelihood surface. We find our improved\nversion of Hierarchical Importance with Nested Training Samples (HINTS), with\nadaptive proposals and a data-driven proxy, obtains the best sampling error in\na fixed computational budget. We conclude that subset evaluations can provide\ncheap and naturally-tempered exploration, while a data-driven proxy can\npre-screen proposals successfully in explored regions of the state space. These\ntwo elements combine through hierarchical delayed acceptance to achieve\nefficient, exact sampling.", "AI": {"tldr": "We introduce new sampling algorithms using subset evaluations to reduce computational overhead for Bayesian inference with MCMC when the likelihood function is irregular and expensive to compute.", "motivation": "To address the challenges posed by irregular and computationally expensive likelihood functions in Bayesian inference with MCMC.", "method": "Adapting subset samplers without gradient information, introducing data-driven proxies instead of Taylor expansions, and defining a computation-cost aware adaptive controller.", "result": "The improved version of Hierarchical Importance with Nested Training Samples (HINTS) achieves the best sampling error within a fixed computational budget.", "conclusion": "Subset evaluations and a data-driven proxy combined with hierarchical delayed acceptance enable efficient and exact sampling."}}
{"id": "2505.09704", "pdf": "https://arxiv.org/pdf/2505.09704", "abs": "https://arxiv.org/abs/2505.09704", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Charalampos Kalalas", "Paolo Dini"], "title": "Energy-Efficient Federated Learning for AIoT using Clustering Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While substantial research has been devoted to optimizing model performance,\nconvergence rates, and communication efficiency, the energy implications of\nfederated learning (FL) within Artificial Intelligence of Things (AIoT)\nscenarios are often overlooked in the existing literature. This study examines\nthe energy consumed during the FL process, focusing on three main\nenergy-intensive processes: pre-processing, communication, and local learning,\nall contributing to the overall energy footprint. We rely on the observation\nthat device/client selection is crucial for speeding up the convergence of\nmodel training in a distributed AIoT setting and propose two\nclustering-informed methods. These clustering solutions are designed to group\nAIoT devices with similar label distributions, resulting in clusters composed\nof nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity\noften encountered in real-world distributed learning applications. Throughout\nextensive numerical experimentation, we demonstrate that our clustering\nstrategies typically achieve high convergence rates while maintaining low\nenergy consumption when compared to other recent approaches available in the\nliterature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86AIoT\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u5f15\u5bfc\u7684\u65b9\u6cd5\u4f18\u5316\u8bbe\u5907\u9009\u62e9\uff0c\u63d0\u9ad8\u4e86\u6536\u655b\u7387\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u5bf9\u8054\u90a6\u5b66\u4e60\u5728AIoT\u573a\u666f\u4e2d\u7684\u80fd\u91cf\u5f71\u54cd\u5173\u6ce8\u4e0d\u8db3\uff0c\u800c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u805a\u7c7b\u5f15\u5bfc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316AIoT\u8bbe\u5907\u7684\u9009\u62e9\uff0c\u51cf\u5c11\u8054\u90a6\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u80fd\u91cf\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u805a\u7c7b\u7b56\u7565\u5728\u63d0\u9ad8\u6536\u655b\u7387\u548c\u964d\u4f4e\u80fd\u8017\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u5728AIoT\u573a\u666f\u4e2d\u7684\u80fd\u91cf\u6d88\u8017\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u805a\u7c7b\u5f15\u5bfc\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u8bbe\u5907\u9009\u62e9\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2505.09755", "pdf": "https://arxiv.org/pdf/2505.09755", "abs": "https://arxiv.org/abs/2505.09755", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "title": "Explainability Through Human-Centric Design for XAI in Lung Cancer Detection", "categories": ["cs.AI"], "comment": null, "summary": "Deep learning models have shown promise in lung pathology detection from\nchest X-rays, but widespread clinical adoption remains limited due to opaque\nmodel decision-making. In prior work, we introduced ClinicXAI, a human-centric,\nexpert-guided concept bottleneck model (CBM) designed for interpretable lung\ncancer diagnosis. We now extend that approach and present XpertXAI, a\ngeneralizable expert-driven model that preserves human-interpretable clinical\nconcepts while scaling to detect multiple lung pathologies. Using a\nhigh-performing InceptionV3-based classifier and a public dataset of chest\nX-rays with radiology reports, we compare XpertXAI against leading post-hoc\nexplainability methods and an unsupervised CBM, XCBs. We assess explanations\nthrough comparison with expert radiologist annotations and medical ground\ntruth. Although XpertXAI is trained for multiple pathologies, our expert\nvalidation focuses on lung cancer. We find that existing techniques frequently\nfail to produce clinically meaningful explanations, omitting key diagnostic\nfeatures and disagreeing with radiologist judgments. XpertXAI not only\noutperforms these baselines in predictive accuracy but also delivers\nconcept-level explanations that better align with expert reasoning. While our\nfocus remains on explainability in lung cancer detection, this work illustrates\nhow human-centric model design can be effectively extended to broader\ndiagnostic contexts - offering a scalable path toward clinically meaningful\nexplainable AI in medical diagnostics.", "AI": {"tldr": "XpertXAI\u662f\u4e00\u79cd\u65b0\u7684\u4eba\u7c7b\u4e2d\u5fc3\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u75c5\u7406\u68c0\u6d4b\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u89e3\u91ca\u6027\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u89e3\u91ca\u6027\u4e0d\u8db3\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e34\u5e8a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165ClinicXAI\u548c\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3aXpertXAI\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u51b3\u7b56\u66f4\u52a0\u900f\u660e\u4e14\u53ef\u7406\u89e3\u3002", "method": "XpertXAI\u662f\u4e00\u79cd\u57fa\u4e8eInceptionV3\u7684\u5206\u7c7b\u5668\uff0c\u80fd\u591f\u68c0\u6d4b\u591a\u79cd\u80ba\u90e8\u75c5\u7406\uff0c\u5e76\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u7684\u6ce8\u91ca\u548c\u533b\u5b66\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\u9a8c\u8bc1\u3002", "result": "XpertXAI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u540e\u5904\u7406\u89e3\u91ca\u65b9\u6cd5\u548c\u65e0\u76d1\u7763CBM\uff08XCBs\uff09\uff0c\u5e76\u4e14\u5176\u6982\u5ff5\u7ea7\u89e3\u91ca\u66f4\u7b26\u5408\u4e13\u5bb6\u63a8\u7406\u3002", "conclusion": "XpertXAI\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4ee5\u4eba\u4e3a\u672c\u7684\u8bbe\u8ba1\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u8bca\u65ad\u9886\u57df\uff0c\u4e3a\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2505.10466", "pdf": "https://arxiv.org/pdf/2505.10466", "abs": "https://arxiv.org/abs/2505.10466", "authors": ["Juehang Qin", "Shixiao Liang", "Christopher Tunnell"], "title": "FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": "10 pages, 5 figures, and 2 tables in main text, two appendices", "summary": "Multi-modal and high-dimensional posteriors present significant challenges\nfor variational inference, causing mode-seeking behavior and collapse despite\nthe theoretical expressiveness of normalizing flows. Traditional annealing\nmethods require temperature schedules and hyperparameter tuning, falling short\nof the goal of truly black-box variational inference. We introduce FlowVAT, a\nconditional tempering approach for normalizing flow variational inference that\naddresses these limitations. Our method tempers both the base and target\ndistributions simultaneously, maintaining affine-invariance under tempering. By\nconditioning the normalizing flow on temperature, we leverage overparameterized\nneural networks' generalization capabilities to train a single flow\nrepresenting the posterior across a range of temperatures. This preserves modes\nidentified at higher temperatures when sampling from the variational posterior\nat $T = 1$, mitigating standard variational methods' mode-seeking behavior. In\nexperiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT\noutperforms traditional and adaptive annealing methods, finding more modes and\nachieving better ELBO values, particularly in higher dimensions where existing\napproaches fail. Our method requires minimal hyperparameter tuning and does not\nrequire an annealing schedule, advancing toward fully-automatic black-box\nvariational inference for complicated posteriors.", "AI": {"tldr": "Introduce FlowVAT, a conditional tempering approach for normalizing flow variational inference, which addresses mode-seeking behavior and collapse issues in multi-modal and high-dimensional posteriors.", "motivation": "Traditional annealing methods fall short of the goal of truly black-box variational inference due to their requirement for temperature schedules and hyperparameter tuning.", "method": "Simultaneously temper both the base and target distributions, leveraging overparameterized neural networks' generalization capabilities to train a single flow representing the posterior across a range of temperatures.", "result": "FlowVAT outperforms traditional and adaptive annealing methods, finding more modes and achieving better ELBO values, especially in higher dimensions.", "conclusion": "FlowVAT advances toward fully-automatic black-box variational inference for complicated posteriors with minimal hyperparameter tuning and no annealing schedule."}}
{"id": "2505.09710", "pdf": "https://arxiv.org/pdf/2505.09710", "abs": "https://arxiv.org/abs/2505.09710", "authors": ["Konstantinos Fotopoulos", "Petros Maragos"], "title": "Training Deep Morphological Neural Networks as Universal Approximators", "categories": ["cs.LG"], "comment": null, "summary": "We investigate deep morphological neural networks (DMNNs). We demonstrate\nthat despite their inherent non-linearity, activations between layers are\nessential for DMNNs. We then propose several new architectures for DMNNs, each\nwith a different constraint on their parameters. For the first (resp. second)\narchitecture, we work under the constraint that the majority of parameters\n(resp. learnable parameters) should be part of morphological operations. We\nempirically show that our proposed networks can be successfully trained, and\nare more prunable than linear networks. To the best of our knowledge, we are\nthe first to successfully train DMNNs under such constraints, although the\ngeneralization capabilities of our networks remain limited. Finally, we propose\na hybrid network architecture combining linear and morphological layers,\nshowing empirically that the inclusion of morphological layers significantly\naccelerates the convergence of gradient descent with large batches.", "AI": {"tldr": "Investigate deep morphological neural networks (DMNNs), propose new architectures with different parameter constraints, show they can be successfully trained and are more prunable than linear networks.", "motivation": "To explore the potential of DMNNs and their applications.", "method": "Propose several new architectures for DMNNs with different constraints on their parameters.", "result": "The proposed networks can be successfully trained and are more prunable than linear networks.", "conclusion": "Successfully trained DMNNs under certain constraints, but their generalization capabilities remain limited. Hybrid network architecture combining linear and morphological layers accelerates the convergence of gradient descent with large batches."}}
{"id": "2505.09787", "pdf": "https://arxiv.org/pdf/2505.09787", "abs": "https://arxiv.org/abs/2505.09787", "authors": ["Ziruo Yi", "Ting Xiao", "Mark V. Albert"], "title": "A Multimodal Multi-Agent Framework for Radiology Report Generation", "categories": ["cs.AI"], "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce diagnostic\nreports from medical images, with the potential to enhance clinical workflows\nand reduce radiologists' workload. While recent approaches leveraging\nmultimodal large language models (MLLMs) and retrieval-augmented generation\n(RAG) have achieved strong results, they continue to face challenges such as\nfactual inconsistency, hallucination, and cross-modal misalignment. We propose\na multimodal multi-agent framework for RRG that aligns with the stepwise\nclinical reasoning workflow, where task-specific agents handle retrieval, draft\ngeneration, visual analysis, refinement, and synthesis. Experimental results\ndemonstrate that our approach outperforms a strong baseline in both automatic\nmetrics and LLM-based evaluations, producing more accurate, structured, and\ninterpretable reports. This work highlights the potential of clinically aligned\nmulti-agent frameworks to support explainable and trustworthy clinical AI\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7528\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff0c\u8be5\u6846\u67b6\u4e0e\u4e34\u5e8a\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b\u4e00\u81f4\uff0c\u5305\u62ec\u68c0\u7d22\u3001\u8349\u7a3f\u751f\u6210\u3001\u89c6\u89c9\u5206\u6790\u3001\u4f18\u5316\u548c\u5408\u6210\u7b49\u4efb\u52a1\u7279\u5b9a\u667a\u80fd\u4f53\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u6307\u6807\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u5f3a\u57fa\u51c6\u6a21\u578b\uff0c\u751f\u6210\u66f4\u51c6\u786e\u3001\u7ed3\u6784\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u62a5\u544a\u3002", "motivation": "\u63d0\u9ad8\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u5e76\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u8d1f\u62c5\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5982\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u9762\u4e34\u7684\u95ee\u9898\u5982\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u9519\u4f4d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u4e94\u4e2a\u4efb\u52a1\u7279\u5b9a\u7684\u667a\u80fd\u4f53\u6765\u6267\u884c\u4e0d\u540c\u7684\u6b65\u9aa4\uff1a\u68c0\u7d22\u3001\u8349\u7a3f\u751f\u6210\u3001\u89c6\u89c9\u5206\u6790\u3001\u4f18\u5316\u548c\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u5ea6\u91cf\u6807\u51c6\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u7684\u62a5\u544a\u66f4\u52a0\u7cbe\u786e\u3001\u7ed3\u6784\u5316\u4e14\u6613\u4e8e\u7406\u89e3\u3002", "conclusion": "\u4e34\u5e8a\u5bf9\u9f50\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5c55\u793a\u4e86\u652f\u6301\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u4e34\u5e8a\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.10498", "pdf": "https://arxiv.org/pdf/2505.10498", "abs": "https://arxiv.org/abs/2505.10498", "authors": ["Sakshi Arya"], "title": "Batched Nonparametric Bandits via k-Nearest Neighbor UCB", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "68T05, 62L05, 62G08, 68Q32", "F.2.2; I.2.6"], "comment": "25 pages, 6 figures", "summary": "We study sequential decision-making in batched nonparametric contextual\nbandits, where actions are selected over a finite horizon divided into a small\nnumber of batches. Motivated by constraints in domains such as medicine and\nmarketing -- where online feedback is limited -- we propose a nonparametric\nalgorithm that combines adaptive k-nearest neighbor (k-NN) regression with the\nupper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully\nnonparametric, adapts to the context dimension, and is simple to implement.\nUnlike prior work relying on parametric or binning-based estimators, BaNk-UCB\nuses local geometry to estimate rewards and adaptively balances exploration and\nexploitation. We provide near-optimal regret guarantees under standard\nLipschitz smoothness and margin assumptions, using a theoretically motivated\nbatch schedule that balances regret across batches and achieves minimax-optimal\nrates. Empirical evaluations on synthetic and real-world datasets demonstrate\nthat BaNk-UCB consistently outperforms binning-based baselines.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6279\u91cf\u975e\u53c2\u6570\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u9002\u5e94k\u8fd1\u90bb\u56de\u5f52\u548cUCB\u539f\u5219\u7684\u975e\u53c2\u6570\u7b97\u6cd5BaNk-UCB\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u662f\u975e\u53c2\u6570\u7684\uff0c\u9002\u5e94\u4e0a\u4e0b\u6587\u7ef4\u5ea6\uff0c\u5e76\u4e14\u6613\u4e8e\u5b9e\u73b0\u3002\u5b83\u4f7f\u7528\u5c40\u90e8\u51e0\u4f55\u6765\u4f30\u8ba1\u5956\u52b1\u5e76\u81ea\u9002\u5e94\u5730\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u3002\u63d0\u4f9b\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u4fdd\u8bc1\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cBaNk-UCB\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u5206\u7bb1\u7684\u57fa\u7ebf\u3002", "motivation": "\u5728\u533b\u5b66\u548c\u8425\u9500\u7b49\u9886\u57df\uff0c\u7531\u4e8e\u5728\u7ebf\u53cd\u9988\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u6279\u91cf\u975e\u53c2\u6570\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u53c2\u6570\u7b97\u6cd5BaNk-UCB\uff0c\u7ed3\u5408\u4e86\u81ea\u9002\u5e94k\u8fd1\u90bb\u56de\u5f52\u548cUCB\u539f\u5219\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5c40\u90e8\u51e0\u4f55\u6765\u4f30\u8ba1\u5956\u52b1\u5e76\u81ea\u9002\u5e94\u5730\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u3002", "result": "\u63d0\u4f9b\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u4fdd\u8bc1\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cBaNk-UCB\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u5206\u7bb1\u7684\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684BaNk-UCB\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u6279\u91cf\u975e\u53c2\u6570\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.09716", "pdf": "https://arxiv.org/pdf/2505.09716", "abs": "https://arxiv.org/abs/2505.09716", "authors": ["George Dimitriadis. Spyridon Samothrakis"], "title": "Out-of-distribution generalisation is hard: evidence from ARC-like tasks", "categories": ["cs.LG", "cs.AI"], "comment": "Submission to NeurIPS 2025", "summary": "Out-of-distribution (OOD) generalisation is considered a hallmark of human\nand animal intelligence. To achieve OOD through composition, a system must\ndiscover the environment-invariant properties of experienced input-output\nmappings and transfer them to novel inputs. This can be realised if an\nintelligent system can identify appropriate, task-invariant, and composable\ninput features, as well as the composition methods, thus allowing it to act\nbased not on the interpolation between learnt data points but on the\ntask-invariant composition of those features. We propose that in order to\nconfirm that an algorithm does indeed learn compositional structures from data,\nit is not enough to just test on an OOD setup, but one also needs to confirm\nthat the features identified are indeed compositional. We showcase this by\nexploring two tasks with clearly defined OOD metrics that are not OOD solvable\nby three commonly used neural networks: a Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and a Transformer. In addition, we develop\ntwo novel network architectures imbued with biases that allow them to be\nsuccessful in OOD scenarios. We show that even with correct biases and almost\nperfect OOD performance, an algorithm can still fail to learn the correct\nfeatures for compositional generalisation.", "AI": {"tldr": "This paper explores the problem of out-of-distribution (OOD) generalization in intelligent systems. It argues that simply testing on an OOD setup is insufficient to confirm that an algorithm learns compositional structures from data. The authors demonstrate this by examining two tasks where common neural networks fail to solve OOD problems. They also introduce two new network architectures with specific biases that enable successful OOD performance, yet still may not correctly learn the features necessary for compositional generalization.", "motivation": "To investigate whether current algorithms can truly learn compositional structures from data, beyond just achieving good performance on OOD setups.", "method": "Testing various neural networks including MLP, CNN, and Transformer on tasks with clear OOD metrics, and developing two novel network architectures with specific biases.", "result": "Common neural networks fail to solve OOD problems, while the new architectures perform well but might still fail to learn the right features for compositional generalization.", "conclusion": "Achieving good OOD performance doesn't necessarily mean an algorithm has learned compositional structures; additional confirmation is needed."}}
{"id": "2505.09920", "pdf": "https://arxiv.org/pdf/2505.09920", "abs": "https://arxiv.org/abs/2505.09920", "authors": ["Shan Yang", "Yongli Zhu"], "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "This paper presents a study on using different offline reinforcement learning\nalgorithms for microgrid voltage regulation with solar power penetration. When\nenvironment interaction is unviable due to technical or safety reasons, the\nproposed approach can still obtain an applicable model through offline-style\ntraining on a previously collected dataset, lowering the negative impact of\nlacking online environment interactions. Experiment results on the IEEE 33-bus\nsystem demonstrate the feasibility and effectiveness of the proposed approach\non different offline datasets, including the one with merely low-quality\nexperience.", "AI": {"tldr": "This paper studies offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration.", "motivation": "To address the issue where environment interaction is unviable due to technical or safety reasons, the authors aim to train an applicable model through offline-style training on a previously collected dataset.", "method": "Different offline reinforcement learning algorithms are used for microgrid voltage regulation.", "result": "The experiment results on the IEEE 33-bus system show the feasibility and effectiveness of the proposed approach on different offline datasets, even with low-quality experience.", "conclusion": "The proposed approach can effectively lower the negative impact of lacking online environment interactions."}}
{"id": "2505.08306", "pdf": "https://arxiv.org/pdf/2505.08306", "abs": "https://arxiv.org/abs/2505.08306", "authors": ["Shira Vansover-Hager", "Tomer Koren", "Roi Livni"], "title": "Rapid Overfitting of Multi-Pass Stochastic Gradient Descent in Stochastic Convex Optimization", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We study the out-of-sample performance of multi-pass stochastic gradient\ndescent (SGD) in the fundamental stochastic convex optimization (SCO) model.\nWhile one-pass SGD is known to achieve an optimal $\\Theta(1/\\sqrt{n})$ excess\npopulation loss given a sample of size $n$, much less is understood about the\nmulti-pass version of the algorithm which is widely used in practice. Somewhat\nsurprisingly, we show that in the general non-smooth case of SCO, just a few\nepochs of SGD can already hurt its out-of-sample performance significantly and\nlead to overfitting. In particular, using a step size $\\eta =\n\\Theta(1/\\sqrt{n})$, which gives the optimal rate after one pass, can lead to\npopulation loss as large as $\\Omega(1)$ after just one additional pass. More\ngenerally, we show that the population loss from the second pass onward is of\nthe order $\\Theta(1/(\\eta T) + \\eta \\sqrt{T})$, where $T$ is the total number\nof steps. These results reveal a certain phase-transition in the out-of-sample\nbehavior of SGD after the first epoch, as well as a sharp separation between\nthe rates of overfitting in the smooth and non-smooth cases of SCO.\nAdditionally, we extend our results to with-replacement SGD, proving that the\nsame asymptotic bounds hold after $O(n \\log n)$ steps. Finally, we also prove a\nlower bound of $\\Omega(\\eta \\sqrt{n})$ on the generalization gap of one-pass\nSGD in dimension $d = \\smash{\\widetilde O}(n)$, improving on recent results of\nKoren et al.(2022) and Schliserman et al.(2024).", "AI": {"tldr": "This paper studies the out-of-sample performance of multi-pass stochastic gradient descent (SGD) in stochastic convex optimization (SCO), revealing that multi-pass SGD can lead to overfitting and poor out-of-sample performance in non-smooth cases.", "motivation": "To investigate the performance of multi-pass SGD in SCO compared to one-pass SGD which is known to perform optimally.", "method": "Theoretical analysis of multi-pass SGD's out-of-sample performance in SCO.", "result": "Multi-pass SGD can significantly harm out-of-sample performance and cause overfitting in non-smooth SCO problems, showing a phase-transition after the first epoch.", "conclusion": "The findings highlight the risks of using multi-pass SGD in non-smooth SCO and suggest careful tuning of parameters to avoid overfitting."}}
{"id": "2505.09733", "pdf": "https://arxiv.org/pdf/2505.09733", "abs": "https://arxiv.org/abs/2505.09733", "authors": ["Alpaslan Gokcen", "Ali Boyaci"], "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios.", "AI": {"tldr": "This study introduces a federated learning method that improves model performance by addressing data quality issues like noisy labels, missing classes, and imbalanced distributions.", "motivation": "To solve data quality problems in federated learning which hinder its effectiveness.", "method": "Adaptive noise cleaning, synthetic data generation using collaborative conditional GANs, and robust federated model training.", "result": "Significant improvements in federated model performance on benchmark datasets under various noise and class imbalance conditions.", "conclusion": "The proposed method effectively tackles common data quality challenges in federated learning, offering a robust, scalable, and privacy-compliant solution."}}
{"id": "2505.09923", "pdf": "https://arxiv.org/pdf/2505.09923", "abs": "https://arxiv.org/abs/2505.09923", "authors": ["Minjung Shin", "Donghyun Kim", "Jeh-Kwang Ryu"], "title": "\"There Is No Such Thing as a Dumb Question,\" But There Are Good Ones", "categories": ["cs.AI"], "comment": "8 pages, 4 figures and 4 tables. This work has been accepted for\n  presentation as a poster with full paper publication at CogSci 2025. This is\n  the final submission", "summary": "Questioning has become increasingly crucial for both humans and artificial\nintelligence, yet there remains limited research comprehensively assessing\nquestion quality. In response, this study defines good questions and presents a\nsystematic evaluation framework. We propose two key evaluation dimensions:\nappropriateness (sociolinguistic competence in context) and effectiveness\n(strategic competence in goal achievement). Based on these foundational\ndimensions, a rubric-based scoring system was developed. By incorporating\ndynamic contextual variables, our evaluation framework achieves structure and\nflexibility through semi-adaptive criteria. The methodology was validated using\nthe CAUS and SQUARE datasets, demonstrating the ability of the framework to\naccess both well-formed and problematic questions while adapting to varied\ncontexts. As we establish a flexible and comprehensive framework for question\nevaluation, this study takes a significant step toward integrating questioning\nbehavior with structured analytical methods grounded in the intrinsic nature of\nquestioning.", "AI": {"tldr": "This study defines good questions and provides a systematic evaluation framework based on appropriateness and effectiveness.", "motivation": "Limited research on comprehensive assessment of question quality.", "method": "Developed a rubric-based scoring system with semi-adaptive criteria.", "result": "Framework can evaluate both well-formed and problematic questions and adapt to different contexts.", "conclusion": "Established a flexible and comprehensive framework for question evaluation."}}
{"id": "2505.09593", "pdf": "https://arxiv.org/pdf/2505.09593", "abs": "https://arxiv.org/abs/2505.09593", "authors": ["Filippo Leveni", "Guilherme Weigert Cassales", "Bernhard Pfahringer", "Albert Bifet", "Giacomo Boracchi"], "title": "Online Isolation Forest", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at International Conference on Machine Learning (ICML 2024)", "summary": "The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.", "AI": {"tldr": "Propose a new method called Online-iForest for anomaly detection in streaming conditions.", "motivation": "Existing methods have impractical assumptions or fail to adapt to online contexts.", "method": "Online-iForest designed specifically for streaming conditions", "result": "Performs well compared to online alternatives and rivals state-of-the-art offline techniques in real-world datasets", "conclusion": "Online-iForest is efficient and suitable for applications requiring fast anomaly detection like cybersecurity, fraud and fault detection."}}
{"id": "2505.09742", "pdf": "https://arxiv.org/pdf/2505.09742", "abs": "https://arxiv.org/abs/2505.09742", "authors": ["Yuan-Hang Zhang", "Massimiliano Di Ventra"], "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.NE"], "comment": "15 pages, 3 figures", "summary": "We propose a generative, end-to-end solver for black-box combinatorial\noptimization that emphasizes both sample efficiency and solution quality on NP\nproblems. Drawing inspiration from annealing-based algorithms, we treat the\nblack-box objective as an energy function and train a neural network to model\nthe associated Boltzmann distribution. By conditioning on temperature, the\nnetwork captures a continuum of distributions--from near-uniform at high\ntemperatures to sharply peaked around global optima at low\ntemperatures--thereby learning the structure of the energy landscape and\nfacilitating global optimization. When queries are expensive, the\ntemperature-dependent distributions naturally enable data augmentation and\nimprove sample efficiency. When queries are cheap but the problem remains hard,\nthe model learns implicit variable interactions, effectively \"opening\" the\nblack box. We validate our approach on challenging combinatorial tasks under\nboth limited and unlimited query budgets, showing competitive performance\nagainst state-of-the-art black-box optimizers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\u7684\u751f\u6210\u5f0f\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u8be5\u6c42\u89e3\u5668\u5728NP\u95ee\u9898\u4e0a\u65e2\u5f3a\u8c03\u6837\u672c\u6548\u7387\u53c8\u6ce8\u91cd\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002\u901a\u8fc7\u4ece\u57fa\u4e8e\u9000\u706b\u7684\u7b97\u6cd5\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5c06\u9ed1\u76d2\u76ee\u6807\u89c6\u4e3a\u80fd\u91cf\u51fd\u6570\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u5efa\u6a21\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u3002", "motivation": "\u63d0\u9ad8\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\u7684\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "method": "\u5c06\u9ed1\u76d2\u76ee\u6807\u89c6\u4e3a\u80fd\u91cf\u51fd\u6570\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u5efa\u6a21\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u3002", "result": "\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5728\u6709\u9650\u548c\u65e0\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u90fd\u8868\u73b0\u51fa\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6c42\u89e3\u5668\u5728\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u65b9\u9762\u3002"}}
{"id": "2505.09932", "pdf": "https://arxiv.org/pdf/2505.09932", "abs": "https://arxiv.org/abs/2505.09932", "authors": ["Kevin J McNamara", "Rhea Pritham Marpu"], "title": "Demystifying AI Agents: The Final Generation of Intelligence", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.MA"], "comment": null, "summary": "The trajectory of artificial intelligence (AI) has been one of relentless\nacceleration, evolving from rudimentary rule-based systems to sophisticated,\nautonomous agents capable of complex reasoning and interaction. This whitepaper\nchronicles this remarkable journey, charting the key technological\nmilestones--advancements in prompting, training methodologies, hardware\ncapabilities, and architectural innovations--that have converged to create the\nAI agents of today. We argue that these agents, exemplified by systems like\nOpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in\nAI development, potentially constituting the \"final generation\" of intelligence\nas we currently conceive it. We explore the capabilities and underlying\ntechnologies of these agents, grounded in practical examples, while also\nexamining the profound societal implications and the unprecedented pace of\nprogress that suggests intelligence is now doubling approximately every six\nmonths. The paper concludes by underscoring the critical need for wisdom and\nforesight in navigating the opportunities and challenges presented by this\npowerful new era of intelligence.", "AI": {"tldr": "\u672c\u6587\u6863\u8bb0\u5f55\u4e86\u4eba\u5de5\u667a\u80fd\u4ece\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u5230\u590d\u6742\u7684\u81ea\u4e3b\u4ee3\u7406\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u7cfb\u7edf\u7684\u80fd\u529b\u548c\u6280\u672f\u57fa\u7840\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u8fd9\u4e2a\u65b0\u7684\u667a\u80fd\u65f6\u4ee3\u4e2d\u5bfc\u822a\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u5386\u7a0b\u548c\u91cc\u7a0b\u7891\uff0c\u4ee5\u53ca\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5173\u952e\u7684\u6280\u672f\u8fdb\u6b65\uff0c\u5982\u63d0\u793a\u65b9\u6cd5\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u786c\u4ef6\u80fd\u529b\u548c\u67b6\u6784\u521b\u65b0\u7b49\uff0c\u6765\u63cf\u8ff0\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002", "result": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53ef\u80fd\u4ee3\u8868\u4e86\u6211\u4eec\u76ee\u524d\u6240\u7406\u89e3\u7684\u2018\u6700\u540e\u4e00\u4ee3\u2019\u4eba\u5de5\u667a\u80fd\uff0c\u5e76\u4e14\u5f3a\u8c03\u4e86\u5728\u8fd9\u4e00\u5f3a\u5927\u7684\u65b0\u667a\u80fd\u65f6\u4ee3\u4e2d\u5bfc\u822a\u6240\u5e26\u6765\u7684\u673a\u4f1a\u548c\u6311\u6218\u9700\u8981\u667a\u6167\u548c\u8fdc\u89c1\u3002", "conclusion": "\u9700\u8981\u667a\u6167\u548c\u8fdc\u89c1\u6765\u5e94\u5bf9\u4eba\u5de5\u667a\u80fd\u5e26\u6765\u7684\u673a\u4f1a\u548c\u6311\u6218\u3002"}}
{"id": "2505.09756", "pdf": "https://arxiv.org/pdf/2505.09756", "abs": "https://arxiv.org/abs/2505.09756", "authors": ["Zhaoyang Shi"], "title": "Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration", "categories": ["cs.LG", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "We propose a new framework for multi-agent reinforcement learning (MARL),\nwhere the agents cooperate in a time-evolving network with latent community\nstructures and mixed memberships. Unlike traditional neighbor-based or fixed\ninteraction graphs, our community-based framework captures flexible and\nabstract coordination patterns by allowing each agent to belong to multiple\noverlapping communities. Each community maintains shared policy and value\nfunctions, which are aggregated by individual agents according to personalized\nmembership weights. We also design actor-critic algorithms that exploit this\nstructure: agents inherit community-level estimates for policy updates and\nvalue learning, enabling structured information sharing without requiring\naccess to other agents' policies. Importantly, our approach supports both\ntransfer learning by adapting to new agents or tasks via membership estimation,\nand active learning by prioritizing uncertain communities during exploration.\nTheoretically, we establish convergence guarantees under linear function\napproximation for both actor and critic updates. To our knowledge, this is the\nfirst MARL framework that integrates community structure, transferability, and\nactive learning with provable guarantees.", "AI": {"tldr": "A novel multi-agent reinforcement learning framework is proposed to capture flexible coordination patterns through community structures.", "motivation": "To develop a more flexible and efficient way for agents to coordinate in dynamic networks with overlapping communities.", "method": "Agents can belong to multiple overlapping communities maintaining shared policy and value functions, which they use based on personalized membership weights. Actor-critic algorithms exploit this structure for policy updates and value learning.", "result": "The framework supports transfer learning and active learning with theoretical convergence guarantees.", "conclusion": "This is the first MARL framework integrating community structure, transferability, and active learning with provable guarantees."}}
{"id": "2505.09970", "pdf": "https://arxiv.org/pdf/2505.09970", "abs": "https://arxiv.org/abs/2505.09970", "authors": ["Mrinal Rawat", "Ambuje Gupta", "Rushil Goomer", "Alessandro Di Bari", "Neha Gupta", "Roberto Pieraccini"], "title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has\nbecome the foundation of modern agentic systems. Recent LLMs, such as\nDeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through\nthe generation of ample intermediate tokens, which help build a strong premise\nbefore producing the final output tokens. In this paper, we introduce Pre-Act,\na novel approach that enhances the agent's performance by creating a multi-step\nexecution plan along with the detailed reasoning for the given user input. This\nplan incrementally incorporates previous steps and tool outputs, refining\nitself after each step execution until the final response is obtained. Our\napproach is applicable to both conversational and non-conversational agents. To\nmeasure the performance of task-oriented agents comprehensively, we propose a\ntwo-level evaluation framework: (1) turn level and (2) end-to-end. Our\nturn-level evaluation, averaged across five models, shows that our approach,\nPre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While\nthis approach is effective for larger models, smaller models crucial for\npractical applications, where latency and cost are key constraints, often\nstruggle with complex reasoning tasks required for agentic systems. To address\nthis limitation, we fine-tune relatively small models such as Llama 3.1 (8B &\n70B) using the proposed Pre-Act approach. Our experiments show that the\nfine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action\naccuracy (turn-level) and a 28% improvement in goal completion rate\n(end-to-end) on the Almita (out-of-domain) dataset.", "AI": {"tldr": "Pre-Act\u662f\u4e00\u79cd\u589e\u5f3a\u4ee3\u7406\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u591a\u6b65\u6267\u884c\u8ba1\u5212\u548c\u8be6\u7ec6\u63a8\u7406\u6765\u5904\u7406\u7528\u6237\u8f93\u5165\u3002\u5b83\u9002\u7528\u4e8e\u5bf9\u8bdd\u5f0f\u548c\u975e\u5bf9\u8bdd\u5f0f\u4ee3\u7406\uff0c\u5e76\u5728\u4efb\u52a1\u5bfc\u5411\u578b\u4ee3\u7406\u7684\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u4f18\u4e8eReAct\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u8f83\u5c0f\u6a21\u578b\u5982Llama 3.1 (8B & 70B)\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u884c\u52a8\u51c6\u786e\u6027\u548c\u76ee\u6807\u5b8c\u6210\u7387\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u5e76\u89e3\u51b3\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faPre-Act\u65b9\u6cd5\uff0c\u751f\u6210\u591a\u6b65\u6267\u884c\u8ba1\u5212\u4e0e\u8be6\u7ec6\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u5bf9\u8bdd\u5f0f\u548c\u975e\u5bf9\u8bdd\u5f0f\u4ee3\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u5c42\u8bc4\u4f30\u6846\u67b6\u4ee5\u8861\u91cf\u4efb\u52a1\u5bfc\u5411\u578b\u4ee3\u7406\u7684\u6027\u80fd\u3002", "result": "Pre-Act\u5728Almita\u6570\u636e\u96c6\u4e0a\u6bd4ReAct\u63d0\u9ad8\u4e8670%\u7684\u884c\u52a8\u53ec\u56de\u7387\uff0c\u5728\u5fae\u8c03\u540e\u768470B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8669.5%\u7684\u884c\u52a8\u51c6\u786e\u7387\u63d0\u5347\u548c28%\u7684\u76ee\u6807\u5b8c\u6210\u7387\u6539\u8fdb\u3002", "conclusion": "Pre-Act\u4e0d\u4ec5\u589e\u5f3a\u4e86\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u901a\u8fc7\u5fae\u8c03\u8f83\u5c0f\u6a21\u578b\u6269\u5927\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u884c\u52a8\u51c6\u786e\u6027\u548c\u76ee\u6807\u5b8c\u6210\u7387\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2505.09847", "pdf": "https://arxiv.org/pdf/2505.09847", "abs": "https://arxiv.org/abs/2505.09847", "authors": ["Liyang Zhao", "Olurotimi Seton", "Himadeep Reddy Reddivari", "Suvendu Jena", "Shadow Zhao", "Rachit Kumar", "Changshuai Wei"], "title": "Causal Predictive Optimization and Generation for Business AI", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "comment": null, "summary": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.", "AI": {"tldr": "An approach named Causal Predictive Optimization and Generation is introduced for sales optimization and business AI, featuring three layers: causal ML prediction, constraint optimization with contextual bandit, and Generative AI serving layer with feedback-loop.", "motivation": "Optimizing the sales process is crucial for the success of B2B businesses as it converts leads to customers and increases sales to existing ones.", "method": "The approach includes three layers: 1) causal ML prediction, 2) constraint optimization and contextual bandit optimization, and 3) Generative AI serving layer with feedback-loop.", "result": "The system was implemented and deployed at LinkedIn, showing significant improvements over previous systems.", "conclusion": "This principled approach to sales optimization and business AI has broad applicability in the field."}}
{"id": "2505.09768", "pdf": "https://arxiv.org/pdf/2505.09768", "abs": "https://arxiv.org/abs/2505.09768", "authors": ["Xiukun Wei", "Xueru Zhang"], "title": "Self-Consuming Generative Models with Adversarially Curated Data", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in generative models have made it increasingly difficult to\ndistinguish real data from model-generated synthetic data. Using synthetic data\nfor successive training of future model generations creates \"self-consuming\nloops\", which may lead to model collapse or training instability. Furthermore,\nsynthetic data is often subject to human feedback and curated by users based on\ntheir preferences. Ferbach et al. (2024) recently showed that when data is\ncurated according to user preferences, the self-consuming retraining loop\ndrives the model to converge toward a distribution that optimizes those\npreferences. However, in practice, data curation is often noisy or\nadversarially manipulated. For example, competing platforms may recruit\nmalicious users to adversarially curate data and disrupt rival models. In this\npaper, we study how generative models evolve under self-consuming retraining\nloops with noisy and adversarially curated data. We theoretically analyze the\nimpact of such noisy data curation on generative models and identify conditions\nfor the robustness of the retraining process. Building on this analysis, we\ndesign attack algorithms for competitive adversarial scenarios, where a\nplatform with a limited budget employs malicious users to misalign a rival's\nmodel from actual user preferences. Experiments on both synthetic and\nreal-world datasets demonstrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "Generative models create synthetic data that can lead to 'self-consuming loops' causing model collapse or instability. When data is curated based on user preferences, models may converge towards these preferences but can be disrupted by noisy or adversarial curation. This paper studies the impact of such curation on models within retraining loops and designs attack algorithms for adversarial competitive scenarios.", "motivation": "To understand the effect of noisy and adversarially curated data on generative models in self-consuming retraining loops and design methods to counteract adverse impacts.", "method": "Theoretical analysis of the impact of noisy data curation on generative models and development of attack algorithms for adversarial competitive scenarios.", "result": "Experiments on synthetic and real-world datasets show the effectiveness of the proposed algorithms.", "conclusion": "Noisy and adversarial data curation can significantly affect generative models in self-consuming retraining loops, and specific conditions for robustness were identified."}}
{"id": "2505.10034", "pdf": "https://arxiv.org/pdf/2505.10034", "abs": "https://arxiv.org/abs/2505.10034", "authors": ["Changzeng Fu", "Zelin Fu", "Xinhe Kuang", "Jiacheng Dong", "Qi Zhang", "Kaifeng Su", "Yikai Su", "Wenbo Shi", "Junfeng Yao", "Yuliang Zhao", "Shiqi Zhao", "Jiadong Wang", "Siyang Song", "Chaoran Liu", "Yuichiro Yoshikawa", "Bj\u00f6rn Schuller", "Hiroshi Ishiguro"], "title": "The First MPDD Challenge: Multimodal Personality-aware Depression Detection", "categories": ["cs.AI", "68T07", "I.2.0; H.5.1"], "comment": "This paper has been accepted as part of the MPDD Challenge in the\n  ACMMM 2025 Grand Challenge", "summary": "Depression is a widespread mental health issue affecting diverse age groups,\nwith notable prevalence among college students and the elderly. However,\nexisting datasets and detection methods primarily focus on young adults,\nneglecting the broader age spectrum and individual differences that influence\ndepression manifestation. Current approaches often establish a direct mapping\nbetween multimodal data and depression indicators, failing to capture the\ncomplexity and diversity of depression across individuals. This challenge\nincludes two tracks based on age-specific subsets: Track 1 uses the\nMPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses\nthe MPDD-Young dataset for detecting depression in younger participants. The\nMultimodal Personality-aware Depression Detection (MPDD) Challenge aims to\naddress this gap by incorporating multimodal data alongside individual\ndifference factors. We provide a baseline model that fuses audio and video\nmodalities with individual difference information to detect depression\nmanifestations in diverse populations. This challenge aims to promote the\ndevelopment of more personalized and accurate de pression detection methods,\nadvancing mental health research and fostering inclusive detection systems.\nMore details are available on the official challenge website:\nhttps://hacilab.github.io/MPDDChallenge.github.io.", "AI": {"tldr": "A new challenge aims to improve depression detection across all ages by including individual differences and multimodal data.", "motivation": "Current depression detection methods mainly focus on young adults, neglecting the broader age spectrum and individual differences.", "method": "The challenge includes two tracks using age-specific datasets and provides a baseline model that combines audio, video, and individual difference information.", "result": "Not yet determined", "conclusion": "This challenge could lead to more personalized and accurate depression detection methods, benefiting mental health research."}}
{"id": "2505.10007", "pdf": "https://arxiv.org/pdf/2505.10007", "abs": "https://arxiv.org/abs/2505.10007", "authors": ["Zijun Chen", "Shengbo Wang", "Nian Si"], "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5206\u5e03\u9c81\u68d2\u5e73\u5747\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7b97\u6cd5\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5b66\u3001\u8fd0\u7b79\u5b66\u548c\u533b\u7597\u4fdd\u5065\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7a33\u5b9a\u957f\u671f\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u7814\u7a76\u4e86\u5206\u5e03\u9c81\u68d2\u5e73\u5747\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u7b2c\u4e00\u79cd\u7b97\u6cd5\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u5206\u5e03\u9c81\u68d2\u6298\u6263\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff1b\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u5f15\u5165\u951a\u72b6\u6001\u6765\u7a33\u5b9a\u4e0d\u786e\u5b9a\u6027\u96c6\u5185\u7684\u63a7\u5236\u8f6c\u79fb\u6838\u3002", "result": "\u4e24\u79cd\u7b97\u6cd5\u90fd\u8fbe\u5230\u4e86\u8fd1\u4f3c\u7684\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u5728\u540d\u4e49\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e00\u81f4\u904d\u5386\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5bf9\u4e8e\u4f30\u8ba1\u6700\u4f18\u7b56\u7565\u548c\u9c81\u68d2\u5e73\u5747\u5956\u52b1\u5177\u6709\u6837\u672c\u590d\u6742\u5ea6\u3002\u8fd9\u662f\u9996\u6b21\u5bf9\u5206\u5e03\u9c81\u68d2\u5e73\u5747\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u6709\u9650\u6837\u672c\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u5206\u522b\u901a\u8fc7\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u5206\u5e03\u9c81\u68d2\u6298\u6263\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u548c\u5f15\u5165\u951a\u72b6\u6001\u6765\u7a33\u5b9a\u4e0d\u786e\u5b9a\u6027\u96c6\u5185\u7684\u63a7\u5236\u8f6c\u79fb\u6838\u6765\u5b9e\u73b0\u3002\u5728\u5047\u8bbe\u540d\u4e49\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e00\u81f4\u904d\u5386\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5bf9\u4e8e\u4f30\u8ba1\u6700\u4f18\u7b56\u7565\u548c\u9c81\u68d2\u5e73\u5747\u5956\u52b1\u5177\u6709\u6837\u672c\u590d\u6742\u5ea6\u3002\u8fd9\u662f\u9996\u6b21\u5bf9\u5206\u5e03\u9c81\u68d2\u5e73\u5747\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u6709\u9650\u6837\u672c\u6536\u655b\u4fdd\u8bc1\u3002\u5e76\u4e14\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u7387\u3002"}}
{"id": "2505.09792", "pdf": "https://arxiv.org/pdf/2505.09792", "abs": "https://arxiv.org/abs/2505.09792", "authors": ["Michael Kamfonas"], "title": "Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This case study applies a phased hyperparameter optimization process to\ncompare multitask natural language model variants that utilize multiphase\nlearning rate scheduling and optimizer parameter grouping. We employ short,\nBayesian optimization sessions that leverage multi-fidelity, hyperparameter\nspace pruning, progressive halving, and a degree of human guidance. We utilize\nthe Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn\nGaussian process minimization. Initially, we use efficient low-fidelity sprints\nto prune the hyperparameter space. Subsequent sprints progressively increase\ntheir model fidelity and employ hyperband pruning for efficiency. A second\naspect of our approach is using a meta-learner to tune threshold values to\nresolve classification probabilities during inference. We demonstrate our\nmethod on a collection of variants of the 2021 Joint Entity and Relation\nExtraction model proposed by Eberts and Ulges.", "AI": {"tldr": "This case study applies a phased hyperparameter optimization process to compare multitask natural language model variants.", "motivation": "To utilize multiphase learning rate scheduling and optimizer parameter grouping.", "method": "Employing short Bayesian optimization sessions with multi-fidelity, hyperparameter space pruning, progressive halving, and human guidance. Using Optuna TPE sampler and Hyperband pruner, as well as Scikit-Learn Gaussian process minimization.", "result": "Efficient low-fidelity sprints to prune the hyperparameter space and subsequent sprints progressively increasing their model fidelity employing hyperband pruning for efficiency.", "conclusion": "Using a meta-learner to tune threshold values to resolve classification probabilities during inference."}}
{"id": "2505.10074", "pdf": "https://arxiv.org/pdf/2505.10074", "abs": "https://arxiv.org/abs/2505.10074", "authors": ["Mohamed Abdelmagied", "Mohamed Amine Chatti", "Shoeb Joarder", "Qurat Ul Ain", "Rawaa Alatrash"], "title": "Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs", "categories": ["cs.AI", "cs.CY"], "comment": "Accepted at EMOOCs 2025", "summary": "Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.", "AI": {"tldr": "This paper proposes a Graph RAG pipeline using Educational Knowledge Graphs and Personal Knowledge Graphs to help learners understand new knowledge concepts in MOOCs.", "motivation": "MOOCs lack direct interaction between learners and instructors, making it difficult for learners to understand new knowledge concepts. Although LLMs can be used, they are prone to hallucinations, limiting their reliability. RAG can solve this problem but is limited by unstructured learning materials and does not actively guide learners' learning needs.", "method": "Propose a Graph RAG pipeline that uses Educational Knowledge Graphs and Personal Knowledge Graphs. Specifically, implement a PKG-based Question Generation method and an EduKG-based Question Answering method.", "result": "The evaluation results show the potential of Graph RAG to empower learners to understand new knowledge concepts in a personalized learning experience.", "conclusion": "Graph RAG can effectively guide learners to understand knowledge concepts in MOOCs through personalized learning."}}
{"id": "2505.10441", "pdf": "https://arxiv.org/pdf/2505.10441", "abs": "https://arxiv.org/abs/2505.10441", "authors": ["Filippo Leveni", "Luca Magri", "Giacomo Boracchi", "Cesare Alippi"], "title": "PIF: Anomaly detection via preference embedding", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Pattern Recognition (ICPR\n  2020)", "summary": "We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.", "AI": {"tldr": "This paper presents PIF, a novel anomaly detection method that combines adaptive isolation methods with preference embedding, showing superior performance over existing techniques on various datasets.", "motivation": "To address the problem of detecting anomalies with respect to structured patterns.", "method": "PIF method combining adaptive isolation methods with preference embedding in a high dimensional space and using PI-Forest to compute anomaly scores.", "result": "Experiments on synthetic and real datasets demonstrate that PIF outperforms state-of-the-art anomaly detection techniques and PI-Forest is better at measuring arbitrary distances and isolating points in the preference space.", "conclusion": "PIF, which combines adaptive isolation methods with preference embedding, outperforms existing techniques on various datasets."}}
{"id": "2505.09810", "pdf": "https://arxiv.org/pdf/2505.09810", "abs": "https://arxiv.org/abs/2505.09810", "authors": ["Daniel Waddington", "Cornel Constantinescu"], "title": "Lossless Compression for LLM Tensor Incremental Snapshots", "categories": ["cs.LG"], "comment": null, "summary": "During the training of Large Language Models (LLMs), tensor data is\nperiodically \"checkpointed\" to persistent storage to allow recovery of work\ndone in the event of failure. The volume of data that must be copied during\neach checkpoint, even when using reduced-precision representations such as\nbfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be\nmoved across a network and written to a storage system before the next epoch\noccurs. With a view to ultimately building an optimized checkpointing solution,\nthis paper presents experimental analysis of checkpoint data used to derive a\ndesign that maximizes the use of lossless compression to reduce the volume of\ndata. We examine how tensor data and its compressibility evolve during model\ntraining and evaluate the efficacy of existing common off-the-shelf general\npurpose compression engines combined with known data optimization techniques\nsuch as byte-grouping and incremental delta compression.\n  Leveraging our analysis we have built an effective compression solution,\nknown as Language Model Compressor (LMC), which is based on byte-grouping and\nHuffman encoding. LMC offers more compression performance than the best\nalternative (BZ2) but with an order-of-magnitude reduction in the time needed\nto perform the compression. We show that a 16-core parallel implementation of\nLMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76\nGiB/s respectively. This increase in performance ultimately reduces the CPU\nresources needed and provides more time to copy the data to the storage system\nbefore the next epoch thus allowing for higher-frequency checkpoints.", "AI": {"tldr": "This paper analyzes checkpoint data used in Large Language Models training and proposes an optimized compression solution called Language Model Compressor (LMC).", "motivation": "To maximize the use of lossless compression to reduce the volume of data during checkpointing.", "method": "Experimental analysis of checkpoint data and building an effective compression solution based on byte-grouping and Huffman encoding.", "result": "LMC offers better compression performance than BZ2 with significantly less time needed for compression. A 16-core parallel implementation of LMC achieves 2.78 GiB/s compression and 3.76 GiB/s decompression throughput.", "conclusion": "LMC reduces the CPU resources needed and allows for higher-frequency checkpoints."}}
{"id": "2505.10093", "pdf": "https://arxiv.org/pdf/2505.10093", "abs": "https://arxiv.org/abs/2505.10093", "authors": ["Hsuan-Lei Shao"], "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI", "categories": ["cs.AI", "cs.CL", "I.2.4; H.3.3; J.5"], "comment": "4 pages, 4 figures", "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.", "AI": {"tldr": "This study uses AI techniques to convert unstructured academic texts into structured knowledge representations for Taiwanese China Studies, creating a knowledge graph and vector database that allow users to explore intellectual trajectories and research gaps.", "motivation": "The motivation is to systematically revisit and reorganize decades of Taiwan based China Studies scholarship and provide a scalable, data driven alternative to traditional ontology construction.", "method": "Generative AI techniques and large language models are used to extract and standardize entity relation triples from peer reviewed articles, which are then visualized through a lightweight D3.js based system.", "result": "The result is a domain specific knowledge graph and vector database for the field, enabling exploration of conceptual nodes and semantic relationships across the corpus, revealing intellectual trajectories, thematic clusters, and research gaps.", "conclusion": "This work shows how generative AI can enhance scholarly access to literature and support a reimagined scholarly infrastructure for regional knowledge systems."}}
{"id": "2505.10559", "pdf": "https://arxiv.org/pdf/2505.10559", "abs": "https://arxiv.org/abs/2505.10559", "authors": ["Ziming Liu", "Yizhou Liu", "Jeff Gore", "Max Tegmark"], "title": "Neural Thermodynamic Laws for Large Language Model Training", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "comment": "18 pages, 10 figures", "summary": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.", "AI": {"tldr": "This paper introduces Neural Thermodynamic Laws (NTL), a framework that applies thermodynamic concepts to understand large language model training dynamics.", "motivation": "To explore laws beyond neural scaling laws for large language models.", "method": "Demonstrates emergence of thermodynamic quantities and principles on river-valley loss landscape assumptions.", "result": "Provides intuitive guidelines for designing learning rate schedules.", "conclusion": "Introduces NTL as a new framework offering insights into LLM training dynamics."}}
{"id": "2505.09812", "pdf": "https://arxiv.org/pdf/2505.09812", "abs": "https://arxiv.org/abs/2505.09812", "authors": ["Anastasija Tashkova", "Stefan Eftimov", "Bojan Ristov", "Slobodan Kalajdziski"], "title": "Comparative Analysis of Stroke Prediction Models Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Stroke remains one of the most critical global health challenges, ranking as\nthe second leading cause of death and the third leading cause of disability\nworldwide. This study explores the effectiveness of machine learning algorithms\nin predicting stroke risk using demographic, clinical, and lifestyle data from\nthe Stroke Prediction Dataset. By addressing key methodological challenges such\nas class imbalance and missing data, we evaluated the performance of multiple\nmodels, including Logistic Regression, Random Forest, and XGBoost. Our results\ndemonstrate that while these models achieve high accuracy, sensitivity remains\na limiting factor for real-world clinical applications. In addition, we\nidentify the most influential predictive features and propose strategies to\nimprove machine learning-based stroke prediction. These findings contribute to\nthe development of more reliable and interpretable models for the early\nassessment of stroke risk.", "AI": {"tldr": "This study examines the effectiveness of machine learning algorithms in predicting stroke risk using various data types. It addresses issues like class imbalance and missing data, evaluates different models, identifies important predictive features, and suggests ways to improve stroke prediction models.", "motivation": "To develop more reliable and interpretable models for early assessment of stroke risk by using machine learning algorithms.", "method": "Using demographic, clinical, and lifestyle data from the Stroke Prediction Dataset, evaluating the performance of Logistic Regression, Random Forest, and XGBoost models while addressing class imbalance and missing data issues.", "result": "The models achieved high accuracy but had low sensitivity which is a limiting factor for real-world clinical applications. The most influential predictive features were identified.", "conclusion": "The study contributes to the development of more reliable and interpretable machine learning models for predicting stroke risk."}}
{"id": "2505.10188", "pdf": "https://arxiv.org/pdf/2505.10188", "abs": "https://arxiv.org/abs/2505.10188", "authors": ["Felix Liedeker", "Olivia Sanchez-Graillet", "Moana Seidler", "Christian Brandt", "J\u00f6rg Wellmer", "Philipp Cimiano"], "title": "A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support", "categories": ["cs.AI"], "comment": "Presented at 'The First Workshop on Natural Language Argument-Based\n  Explanations', co-located with ECAI 2024", "summary": "As the field of healthcare increasingly adopts artificial intelligence, it\nbecomes important to understand which types of explanations increase\ntransparency and empower users to develop confidence and trust in the\npredictions made by machine learning (ML) systems. In shared decision-making\nscenarios where doctors cooperate with ML systems to reach an appropriate\ndecision, establishing mutual trust is crucial. In this paper, we explore\ndifferent approaches to generating explanations in eXplainable AI (XAI) and\nmake their underlying arguments explicit so that they can be evaluated by\nmedical experts. In particular, we present the findings of a user study\nconducted with physicians to investigate their perceptions of various types of\nAI-generated explanations in the context of diagnostic decision support. The\nstudy aims to identify the most effective and useful explanations that enhance\nthe diagnostic process. In the study, medical doctors filled out a survey to\nassess different types of explanations. Further, an interview was carried out\npost-survey to gain qualitative insights on the requirements of explanations\nincorporated in diagnostic decision support. Overall, the insights gained from\nthis study contribute to understanding the types of explanations that are most\neffective.", "AI": {"tldr": "This paper explores different methods of generating explanations in XAI to improve transparency and trust in ML systems used in medical diagnosis.", "motivation": "To establish mutual trust between doctors and ML systems in shared decision-making scenarios.", "method": "User study with physicians to investigate their perceptions of various types of AI-generated explanations.", "result": "The study identifies the most effective and useful explanations that enhance the diagnostic process.", "conclusion": "Understanding the types of explanations that are most effective contributes to improving the diagnostic process."}}
{"id": "2505.09820", "pdf": "https://arxiv.org/pdf/2505.09820", "abs": "https://arxiv.org/abs/2505.09820", "authors": ["Sajib Biswas", "Mao Nishino", "Samuel Jacob Chacko", "Xiuwen Liu"], "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack", "AI": {"tldr": "This paper introduces an intrinsic optimization technique using exponentiated gradient descent and Bregman projection for jailbreaking large language models more efficiently.", "motivation": "To address the vulnerability of large language models to jailbreaking attacks and improve their safety and potential usage.", "method": "Developing an intrinsic optimization technique with exponentiated gradient descent and Bregman projection to optimize continuous token embeddings while ensuring they stay within the probability simplex.", "result": "The technique achieved a higher success rate with great efficiency compared to three other state-of-the-art jailbreaking techniques on five open-source LLMs and four datasets.", "conclusion": "This study demonstrates the effectiveness of the proposed method in jailbreaking large language models more efficiently."}}
{"id": "2505.10278", "pdf": "https://arxiv.org/pdf/2505.10278", "abs": "https://arxiv.org/abs/2505.10278", "authors": ["Taian Guo", "Haiyang Shen", "Jinsheng Huang", "Zhengyang Mao", "Junyu Luo", "Zhuoru Chen", "Xuhui Liu", "Bingyu Xia", "Luchen Liu", "Yun Ma", "Ming Zhang"], "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction", "categories": ["cs.AI"], "comment": null, "summary": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.", "AI": {"tldr": "This paper introduces MASS, a multi-agent system for portfolio construction using large-scale simulations to achieve continuous excess returns. It outperforms six state-of-the-art baselines across three challenging A-share stock pools.", "motivation": "Existing LLM-based multi-agent systems are limited to pure simulations or predefined workflows, which restricts their applicability and effectiveness.", "method": "MASS progressively increases the number of agents for large-scale simulations and optimizes agent distribution end-to-end through a reverse optimization process.", "result": "Performance experiments, ablation studies, backtesting experiments, experiments on updated data and stock pools, scaling experiments, parameter sensitivity experiments, and visualization experiments demonstrate MASS's superiority over six state-of-the-art baselines.", "conclusion": "The paradigm established by MASS is expected to be expanded to other tasks with similar characteristics."}}
{"id": "2505.09822", "pdf": "https://arxiv.org/pdf/2505.09822", "abs": "https://arxiv.org/abs/2505.09822", "authors": ["Changhao Shi", "Gal Mishne"], "title": "Learning Kronecker-Structured Graphs from Smooth Signals", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Graph learning, or network inference, is a prominent problem in graph signal\nprocessing (GSP). GSP generalizes the Fourier transform to non-Euclidean\ndomains, and graph learning is pivotal to applying GSP when these domains are\nunknown. With the recent prevalence of multi-way data, there has been growing\ninterest in product graphs that naturally factorize dependencies across\ndifferent ways. However, the types of graph products that can be learned are\nstill limited for modeling diverse dependency structures. In this paper, we\nstudy the problem of learning a Kronecker-structured product graph from smooth\nsignals. Unlike the more commonly used Cartesian product, the Kronecker product\nmodels dependencies in a more intricate, non-separable way, but posits harder\nconstraints on the graph learning problem. To tackle this non-convex problem,\nwe propose an alternating scheme to optimize each factor graph and provide\ntheoretical guarantees for its asymptotic convergence. The proposed algorithm\nis also modified to learn factor graphs of the strong product. We conduct\nexperiments on synthetic and real-world graphs and demonstrate our approach's\nefficacy and superior performance compared to existing methods.", "AI": {"tldr": "This paper studies the problem of learning a Kronecker-structured product graph from smooth signals and proposes an alternating scheme to optimize each factor graph.", "motivation": "Graph learning, or network inference, is a prominent problem in graph signal processing (GSP). GSP generalizes the Fourier transform to non-Euclidean domains, and graph learning is pivotal to applying GSP when these domains are unknown.", "method": "We propose an alternating scheme to optimize each factor graph and provide theoretical guarantees for its asymptotic convergence.", "result": "The proposed algorithm is also modified to learn factor graphs of the strong product. We conduct experiments on synthetic and real-world graphs and demonstrate our approach's efficacy and superior performance compared to existing methods.", "conclusion": "We study the problem of learning a Kronecker-structured product graph from smooth signals."}}
{"id": "2505.10309", "pdf": "https://arxiv.org/pdf/2505.10309", "abs": "https://arxiv.org/abs/2505.10309", "authors": ["Tuan Dung Nguyen", "Duncan J. Watts", "Mark E. Whiting"], "title": "Empirically evaluating commonsense intelligence in large language models with large-scale human judgments", "categories": ["cs.AI", "cs.HC", "cs.SI"], "comment": null, "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.", "AI": {"tldr": "This paper introduces a new method for evaluating AI's commonsense intelligence by accounting for human variability, finding that smaller models outperform larger ones, and emphasizing the importance of cultural context in AI evaluation.", "motivation": "The motivation of this paper is to address the issue of assessing commonsense intelligence in machines, particularly large language models (LLMs), by considering the heterogeneity among humans.", "method": "The method involves measuring the correspondence between a model's judgment and that of a human population, treating LLMs as independent survey respondents and as simulators of a hypothetical population.", "result": "The results show that most LLMs are below the human median in individual commonsense competence. When used as population simulators, LLMs only modestly correlate with real humans in agreeing on the same set of statements. Surprisingly, smaller, open-weight models perform better than larger, proprietary frontier models.", "conclusion": "The paper concludes by suggesting that the evaluation framework ties commonsense intelligence to its cultural basis, aligning with the need to adapt AI models to diverse human collectivities."}}
{"id": "2505.10328", "pdf": "https://arxiv.org/pdf/2505.10328", "abs": "https://arxiv.org/abs/2505.10328", "authors": ["Alvin Combrink", "Stephie Do", "Kristofer Bengtsson", "Sabino Francesco Roselli", "Martin Fabian"], "title": "A Comparative Study of SMT and MILP for the Nurse Rostering Problem", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 3 figures", "summary": "The effects of personnel scheduling on the quality of care and working\nconditions for healthcare personnel have been thoroughly documented. However,\nthe ever-present demand and large variation of constraints make healthcare\nscheduling particularly challenging. This problem has been studied for decades,\nwith limited research aimed at applying Satisfiability Modulo Theories (SMT).\nSMT has gained momentum within the formal verification community in the last\ndecades, leading to the advancement of SMT solvers that have been shown to\noutperform standard mathematical programming techniques.\n  In this work, we propose generic constraint formulations that can model a\nwide range of real-world scheduling constraints. Then, the generic constraints\nare formulated as SMT and MILP problems and used to compare the respective\nstate-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired\nrostering problems. Experimental results show how each solver excels for\ncertain types of problems; the MILP solver generally performs better when the\nproblem is highly constrained or infeasible, while the SMT solver performs\nbetter otherwise. On real-world inspired problems containing a more varied set\nof shifts and personnel, the SMT solver excels. Additionally, it was noted\nduring experimentation that the SMT solver was more sensitive to the way the\ngeneric constraints were formulated, requiring careful consideration and\nexperimentation to achieve better performance. We conclude that SMT-based\nmethods present a promising avenue for future research within the domain of\npersonnel scheduling.", "AI": {"tldr": "This study compares SMT and MILP solvers for healthcare personnel scheduling, finding that each performs best under different conditions.", "motivation": "To apply Satisfiability Modulo Theories (SMT) to healthcare scheduling, which has been understudied despite its challenges.", "method": "Proposes generic constraint formulations and models them as SMT and MILP problems to compare Z3 and Gurobi solvers on academic and real-world rostering problems.", "result": "MILP solver performs better with highly constrained or infeasible problems, while SMT solver excels with varied shifts and personnel in real-world inspired problems.", "conclusion": "SMT-based methods offer a promising approach for future personnel scheduling research."}}
{"id": "2505.09848", "pdf": "https://arxiv.org/pdf/2505.09848", "abs": "https://arxiv.org/abs/2505.09848", "authors": ["Aditya Raj", "Golrokh Mirzaei"], "title": "Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection", "categories": ["cs.LG", "eess.IV"], "comment": "11 pages", "summary": "Imaging and genomic data offer distinct and rich features, and their\nintegration can unveil new insights into the complex landscape of diseases. In\nthis study, we present a novel approach utilizing radiogenomic data including\nstructural MRI images and gene expression data, for Alzheimer's disease\ndetection. Our framework introduces a novel heterogeneous bipartite graph\nrepresentation learning featuring two distinct node types: genes and images.\nThe network can effectively classify Alzheimer's disease (AD) into three\ndistinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)\nclasses, utilizing a small dataset. Additionally, it identified which genes\nplay a significant role in each of these classification groups. We evaluate the\nperformance of our approach using metrics including classification accuracy,\nrecall, precision, and F1 score. The proposed technique holds potential for\nextending to radiogenomic-based classification to other diseases.", "AI": {"tldr": "This study presents a new method that uses both brain images and gene data to identify different stages of Alzheimer's disease and find important genes related to these stages.", "motivation": "To integrate imaging and genomic data to uncover new information about diseases.", "method": "A novel heterogeneous bipartite graph representation learning approach with two types of nodes: genes and images.", "result": "Effectively classifies Alzheimer's disease into three stages using a small dataset and identifies significant genes for each stage.", "conclusion": "The approach has potential for extending radiogenomic-based disease classification to other diseases."}}
{"id": "2505.10361", "pdf": "https://arxiv.org/pdf/2505.10361", "abs": "https://arxiv.org/abs/2505.10361", "authors": ["David Abel", "Michael Bowling", "Andr\u00e9 Barreto", "Will Dabney", "Shi Dong", "Steven Hansen", "Anna Harutyunyan", "Khimya Khetarpal", "Clare Lyle", "Razvan Pascanu", "Georgios Piliouras", "Doina Precup", "Jonathan Richens", "Mark Rowland", "Tom Schaul", "Satinder Singh"], "title": "Plasticity as the Mirror of Empowerment", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Our first finding is that\nplasticity is the mirror of empowerment: The agent's plasticity is identical to\nthe empowerment of the environment, and vice versa. Our second finding\nestablishes a tension between the plasticity and empowerment of an agent,\nsuggesting that agent design needs to be mindful of both characteristics. We\nexplore the implications of these findings, and suggest that plasticity,\nempowerment, and their relationship are essential to understanding agency.", "AI": {"tldr": "This paper introduces the concept of plasticity as a measure of how agents are influenced by their past observations. It establishes a connection between plasticity and empowerment, revealing a tension between them.", "motivation": "To understand how agents are influenced by their past observations and to establish a connection between plasticity and empowerment.", "method": "Defining plasticity using a new information-theoretic quantity called the generalized directed information.", "result": "Plasticity is found to be the mirror of empowerment, and there is a tension between plasticity and empowerment.", "conclusion": "Plasticity, empowerment, and their relationship are essential to understanding agency."}}
{"id": "2505.09851", "pdf": "https://arxiv.org/pdf/2505.09851", "abs": "https://arxiv.org/abs/2505.09851", "authors": ["Shun Wang", "Shun-Li Shang", "Zi-Kui Liu", "Wenrui Hao"], "title": "ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "9 pages, 4 figures", "summary": "Traditional entropy-based methods - such as cross-entropy loss in\nclassification problems - have long been essential tools for quantifying\nuncertainty and disorder in data and developing artificial intelligence\nalgorithms. However, the rapid growth of data across various domains has\nintroduced new challenges, particularly the integration of heterogeneous\ndatasets with intrinsic disparities. In this paper, we extend zentropy theory\ninto the data science domain by introducing intrinsic entropy, enabling more\neffective learning from heterogeneous data sources. We propose a\nzentropy-enhanced neural network (ZENN) that simultaneously learns both energy\nand intrinsic entropy components, capturing the underlying structure of\nmulti-source data. To support this, we redesign the neural network architecture\nto better reflect the intrinsic properties and variability inherent in diverse\ndatasets. We demonstrate the effectiveness of ZENN on classification tasks and\nenergy landscape reconstructions, showing its superior generalization\ncapabilities and robustness-particularly in predicting high-order derivatives.\nAs a practical application, we employ ZENN to reconstruct the Helmholtz energy\nlandscape of Fe3Pt using data generated from DFT and capture key material\nbehaviors, including negative thermal expansion and the critical point in the\ntemperature-pressure space. Overall, our study introduces a novel approach for\ndata-driven machine learning grounded in zentropy theory, highlighting ZENN as\na versatile and robust deep learning framework for scientific problems\ninvolving complex, heterogeneous datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ezentropy\u7406\u8bba\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578bZENN\uff0c\u7528\u4e8e\u4ece\u5f02\u6784\u6570\u636e\u4e2d\u6709\u6548\u5b66\u4e60\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5728\u79d1\u5b66\u95ee\u9898\u4e2d\u663e\u793a\u51fa\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u71b5\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u8fd9\u4e9b\u6570\u636e\u96c6\u5b58\u5728\u56fa\u6709\u5dee\u5f02\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u6709\u6548\u5730\u4ece\u5f02\u6784\u6570\u636e\u6e90\u4e2d\u5b66\u4e60\u3002", "method": "\u5f15\u5165\u4e86\u5185\u5728\u71b5\u7684\u6982\u5ff5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540c\u65f6\u5b66\u4e60\u80fd\u91cf\u548c\u5185\u5728\u71b5\u6210\u5206\u7684zentropy\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc(ZENN)\uff0c\u5e76\u91cd\u65b0\u8bbe\u8ba1\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u591a\u6e90\u6570\u636e\u7684\u5185\u5728\u7279\u6027\u548c\u53d8\u5f02\u6027\u3002", "result": "ZENN\u5728\u5206\u7c7b\u4efb\u52a1\u548c\u80fd\u91cf\u666f\u89c2\u91cd\u5efa\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u9ad8\u9636\u5bfc\u6570\u65b9\u9762\u3002\u6b64\u5916\uff0cZENN\u6210\u529f\u5730\u91cd\u5efa\u4e86Fe3Pt\u7684Helmholtz\u80fd\u91cf\u666f\u89c2\uff0c\u5e76\u6355\u6349\u5230\u4e86\u5173\u952e\u7684\u6750\u6599\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u4e86\u57fa\u4e8ezentropy\u7406\u8bba\u7684ZENN\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u9ad8\u9636\u5bfc\u6570\u65b9\u9762\u3002ZENN\u80fd\u591f\u91cd\u5efaFe3Pt\u7684Helmholtz\u80fd\u91cf\u666f\u89c2\u5e76\u6355\u6349\u5173\u952e\u6750\u6599\u884c\u4e3a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.10399", "pdf": "https://arxiv.org/pdf/2505.10399", "abs": "https://arxiv.org/abs/2505.10399", "authors": ["Kaivalya Rawal", "Zihao Fu", "Eoin Delaney", "Chris Russell"], "title": "Evaluating Model Explanations without Ground Truth", "categories": ["cs.AI", "cs.LG", "I.2.6"], "comment": "https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth", "summary": "There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.", "AI": {"tldr": "This paper proposes AXE, a new framework for evaluating and comparing model explanations without relying on ground-truth explanations or model sensitivity.", "motivation": "Current explanation evaluation methods have limitations such as requiring ground-truth explanations or model sensitivity, which can be impractical or misleading.", "method": "The authors propose three principles for explanation evaluation and develop the AXE framework based on these principles.", "result": "AXE is verified by comparing with baselines and shown to be useful for detecting explanation fairwashing.", "conclusion": "AXE provides an independent measure of explanation quality without requiring access to ideal ground-truth explanations or relying on model sensitivity."}}
{"id": "2505.09854", "pdf": "https://arxiv.org/pdf/2505.09854", "abs": "https://arxiv.org/abs/2505.09854", "authors": ["Harikrishna Kuttivelil", "Katia Obraczka"], "title": "Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence", "categories": ["cs.LG", "cs.ET", "cs.MA", "cs.SI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "As demand for intelligent services rises and edge devices become more\ncapable, distributed learning at the network edge has emerged as a key enabling\ntechnology. While existing paradigms like federated learning (FL) and\ndecentralized FL (DFL) enable privacy-preserving distributed learning in many\nscenarios, they face potential challenges in connectivity and synchronization\nimposed by resource-constrained and infrastructure-less environments. While\nmore robust, gossip learning (GL) algorithms have generally been designed for\nhomogeneous data distributions and may not suit all contexts. This paper\nintroduces Chisme, a novel suite of protocols designed to address the\nchallenges of implementing robust intelligence in the network edge,\ncharacterized by heterogeneous data distributions, episodic connectivity, and\nlack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and\nasynchronous GL (Chisme-GL) variants that enable collaborative yet\ndecentralized model training that considers underlying data heterogeneity. We\nintroduce a data similarity heuristic that allows agents to opportunistically\ninfer affinity with each other using the existing communication of model\nupdates in decentralized FL and GL. We leverage the heuristic to extend DFL's\nmodel aggregation and GL's model merge mechanisms for better personalized\ntraining while maintaining collaboration. While Chisme-DFL is a synchronous\ndecentralized approach whose resource utilization scales linearly with network\nsize, Chisme-GL is fully asynchronous and has a lower, constant resource\nrequirement independent of network size. We demonstrate that Chisme methods\noutperform their standard counterparts in model training over distributed and\nheterogeneous data in network scenarios ranging from less connected and\nreliable networks to fully connected and lossless networks.", "AI": {"tldr": "Distributed learning at the network edge faces challenges in connectivity and synchronization. This paper introduces Chisme, a novel suite of protocols that addresses these issues by including both synchronous DFL and asynchronous GL variants.", "motivation": "The motivation is to implement robust intelligence in the network edge characterized by heterogeneous data distributions, episodic connectivity, and lack of infrastructure.", "method": "Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training. A data similarity heuristic is introduced to allow agents to infer affinity with each other using existing communication of model updates.", "result": "Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.", "conclusion": "This paper presents a new approach to distributed learning at the network edge that addresses challenges in connectivity and synchronization."}}
{"id": "2505.10468", "pdf": "https://arxiv.org/pdf/2505.10468", "abs": "https://arxiv.org/abs/2505.10468", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "categories": ["cs.AI"], "comment": "32 pages, 14 figures, 11 tables", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "AI": {"tldr": "This study distinguishes between AI Agents and Agentic AI, offering a taxonomy, application mapping, and challenge analysis to clarify their design philosophies and capabilities.", "motivation": "To provide a clear distinction and understanding between AI Agents and Agentic AI, their design philosophies, and capabilities.", "method": "The study outlines a search strategy and foundational definitions, characterizes AI Agents as modular systems driven by LLMs and LIMs, and positions Generative AI as a precursor. It contrasts AI Agents with Agentic AI systems which represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. A comparative analysis across both paradigms is presented through an evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels.", "result": "The study maps application domains like customer support, scheduling, and data summarization against Agentic AI deployments in research automation, robotic coordination, and medical decision support. It examines challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and proposes solutions such as ReAct loops, RAG, orchestration layers, and causal modeling.", "conclusion": "This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems."}}
{"id": "2505.09855", "pdf": "https://arxiv.org/pdf/2505.09855", "abs": "https://arxiv.org/abs/2505.09855", "authors": ["Alexander Y. Ku", "Thomas L. Griffiths", "Stephanie C. Y. Chan"], "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.", "AI": {"tldr": "This paper explores how environmental factors affect the balance between in-weights and in-context learning in transformers, drawing analogies from evolutionary biology.", "motivation": "To understand the interplay between in-weights learning and in-context learning in transformer models by drawing parallels from evolutionary biology's adaptive strategies.", "method": "Experiments were conducted using regression and classification tasks to investigate the effect of environmental stability and cue reliability on the ICL/IWL balance in Transformers.", "result": "High environmental stability strongly favors IWL, whereas high cue reliability enhances ICL efficacy, especially under low stability. Task-contingent temporal evolution also occurs, showcasing different patterns of learning mode transitions.", "conclusion": "The study concludes that environmental predictability significantly influences the balance between in-weights learning (IWL) and in-context learning (ICL) in transformers."}}
{"id": "2505.10543", "pdf": "https://arxiv.org/pdf/2505.10543", "abs": "https://arxiv.org/abs/2505.10543", "authors": ["Annie Wong", "Thomas B\u00e4ck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "title": "Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.", "AI": {"tldr": "This study examines the effectiveness of different prompting techniques on large language models in dynamic environments. The results suggest that larger models perform better than smaller ones, but strategic prompting can help close the performance gap. Advanced prompting techniques benefit smaller models more on complex tasks, but advanced reasoning methods yield inconsistent results. Overall, current large language models still have limitations in reasoning and spatial coordination.", "motivation": "To evaluate the adaptive capabilities of large language models as self-learning and reasoning agents in dynamic environments.", "method": "Systematically testing self-reflection, heuristic mutation, and planning as prompting techniques using various open-source language models.", "result": "Larger models outperform smaller ones, strategic prompting can close performance gaps, advanced prompting benefits smaller models on complex tasks, and advanced reasoning methods yield inconsistent results.", "conclusion": "Current large language models still have significant limitations in crucial areas such as planning, reasoning, and spatial coordination, suggesting that self-reflective prompting alone cannot fully address these issues."}}
{"id": "2505.09861", "pdf": "https://arxiv.org/pdf/2505.09861", "abs": "https://arxiv.org/abs/2505.09861", "authors": ["John Bencina", "Erkut Aykutlug", "Yue Chen", "Zerui Zhang", "Stephanie Sorenson", "Shao Tang", "Changshuai Wei"], "title": "LiDDA: Data Driven Attribution at LinkedIn", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ME"], "comment": null, "summary": "Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.", "AI": {"tldr": "This paper introduces a unified transformer-based attribution approach for marketing interactions.", "motivation": "To assign conversion credits to marketing interactions based on causal patterns learned from data.", "method": "A unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors.", "result": "The approach has been implemented at LinkedIn with significant impact.", "conclusion": "The approach and insights shared in this paper are broadly applicable to the marketing and ad tech fields."}}
{"id": "2410.13778", "pdf": "https://arxiv.org/pdf/2410.13778", "abs": "https://arxiv.org/abs/2410.13778", "authors": ["Michelangelo Olmo Nogara Notarianni", "Filippo Leveni", "Diego Stucchi", "Luca Frittoli", "Giacomo Boracchi"], "title": "Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "AALTD workshop at ECML 2024 (https://ecml-aaltd.github.io/aaltd2024/)", "summary": "We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.", "AI": {"tldr": "KQT-EWMA is a non-parametric change-detection algorithm that combines Kernel-QuantTree histogram and EWMA statistic to monitor multivariate data streams online, with ability to control false alarms and achieve competitive detection delays.", "motivation": "To develop a flexible and practical non-parametric change-detection algorithm for monitoring multivariate data streams online.", "method": "Combining Kernel-QuantTree histogram and the EWMA statistic.", "result": "KQT-EWMA can control false alarms and achieve detection delays comparable to or lower than state-of-the-art methods.", "conclusion": "KQT-EWMA can control false alarms and achieve detection delays comparable to or lower than state-of-the-art methods."}}
{"id": "2505.09864", "pdf": "https://arxiv.org/pdf/2505.09864", "abs": "https://arxiv.org/abs/2505.09864", "authors": ["Aditya Panangat"], "title": "BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks", "categories": ["cs.LG"], "comment": "6 pages, 0 figures, 2 tables", "summary": "Over the past decade, the use of machine learning has increased\nexponentially. Models are far more complex than ever before, growing to\ngargantuan sizes and housing millions of weights. Unfortunately, the fact that\nlarge models have become the state of the art means that it often costs\nmillions of dollars to train and operate them. These expenses not only hurt\ncompanies but also bar non-wealthy individuals from contributing to new\ndevelopments and force consumers to pay greater prices for AI. Current methods\nused to prune models, such as iterative magnitude pruning, have shown great\naccuracy but require an iterative training sequence that is incredibly\ncomputationally and environmentally taxing. To solve this problem, BINGO is\nintroduced. BINGO, during the training pass, studies specific subsets of a\nneural network one at a time to gauge how significant of a role each weight\nplays in contributing to a network's accuracy. By the time training is done,\nBINGO generates a significance score for each weight, allowing for\ninsignificant weights to be pruned in one shot. BINGO provides an\naccuracy-preserving pruning technique that is less computationally intensive\nthan current methods, allowing for a world where AI growth does not have to\nmean model growth, as well.", "AI": {"tldr": "BINGO is a method introduced to reduce the computational cost of training large machine learning models by pruning insignificant weights in one shot.", "motivation": "The high cost of training large models limits their accessibility and affordability.", "method": "BINGO studies specific subsets of a neural network during training to determine the significance of each weight.", "result": "BINGO achieves accuracy-preserving pruning with lower computational intensity compared to existing methods.", "conclusion": "BINGO offers a solution to make AI development more accessible and affordable without sacrificing model accuracy."}}
{"id": "2505.03084", "pdf": "https://arxiv.org/pdf/2505.03084", "abs": "https://arxiv.org/abs/2505.03084", "authors": ["Shashank Kapoor", "Sanjay Surendranath Girija", "Lakshit Arora", "Dipen Pradhan", "Ankit Shetgaonkar", "Aman Raj"], "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in IEEE COMPSAC 2025", "summary": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world.", "AI": {"tldr": "This paper surveys adversarial attacks targeting all four modalities (text, image, video, and audio) in multimodal models, providing a view of the adversarial attack landscape and presenting how multimodal adversarial threats have evolved. It fills a gap in the practitioner-focused view of attack types.", "motivation": "To help machine learning practitioners view the threat landscape and take preventive actions against adversarial attacks in multimodal models.", "method": "Surveying adversarial attacks targeting all four modalities: text, image, video, and audio.", "result": "Provides a comprehensive summarization of the threat landscape in the multimodal world.", "conclusion": "This is the first comprehensive summarization of the threat landscape in the multimodal world."}}
{"id": "2505.09901", "pdf": "https://arxiv.org/pdf/2505.09901", "abs": "https://arxiv.org/abs/2505.09901", "authors": ["Ziyuan Zhang", "Darcy Wang", "Ningyuan Chen", "Rodrigo Mansur", "Vahid Sarhangian"], "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.", "AI": {"tldr": "Large language models (LLMs) were analyzed in terms of their exploration-exploitation (E&E) tradeoff strategies compared to humans and MAB algorithms. Reasoning was found to shift LLMs toward more human-like behavior.", "motivation": "To examine if LLMs exhibit similar decision-making behavior to humans and can achieve comparable (or superior) performance.", "method": "Canonical multi-armed bandit (MAB) tasks were used to compare the E&E strategies of LLMs, humans, and MAB algorithms. Interpretable choice models were employed to capture the E&E strategies.", "result": "In simple stationary tasks, reasoning-enabled LLMs exhibited similar levels of random and directed exploration compared to humans. However, in complex, non-stationary environments, LLMs struggled to match human adaptability, particularly in effective directed exploration.", "conclusion": "LLMs show promise as simulators of human behavior and tools for automated decision-making but have limitations, especially in complex, non-stationary environments."}}
{"id": "2505.09907", "pdf": "https://arxiv.org/pdf/2505.09907", "abs": "https://arxiv.org/abs/2505.09907", "authors": ["Linwei Zhang", "LuFeng", "Ruijia Liang"], "title": "Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "With the growing demand for healthy foods, agricultural product price\nforecasting has become increasingly important. Hass avocados, as a high-value\ncrop, exhibit complex price fluctuations influenced by factors such as\nseasonality, region, and weather. Traditional prediction models often struggle\nwith highly nonlinear and dynamic data. To address this, we propose a hybrid\ndeep learning model, TCN-MLP-Attention Architecture, combining Temporal\nConvolutional Networks (TCN) for sequential feature extraction, Multi-Layer\nPerceptrons (MLP) for nonlinear interactions, and an Attention mechanism for\ndynamic feature weighting. The dataset used covers over 50,000 records of Hass\navocado sales across the U.S. from 2015 to 2018, including variables such as\nsales volume, average price, time, region, weather, and variety type, collected\nfrom point-of-sale systems and the Hass Avocado Board. After systematic\npreprocessing, including missing value imputation and feature normalization,\nthe proposed model was trained and evaluated. Experimental results demonstrate\nthat the TCN-MLP-Attention model achieves excellent predictive performance,\nwith an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.\nThis research provides a scalable and effective approach for time series\nforecasting in agricultural markets and offers valuable insights for\nintelligent supply chain management and price strategy optimization.", "AI": {"tldr": "Propose a hybrid deep learning model (TCN-MLP-Attention) for Hass avocado price forecasting which combines TCN, MLP, and attention mechanisms. This model shows better performance than traditional methods.", "motivation": "To meet the increasing demand for healthy food, Hass avocado price forecasting is crucial due to its complex price fluctuations affected by seasonality, region, and weather.", "method": "Develop a TCN-MLP-Attention model that uses TCN for sequential feature extraction, MLP for nonlinear interactions, and attention for dynamic feature weighting.", "result": "The proposed model achieved an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.", "conclusion": "The TCN-MLP-Attention model provides a scalable and effective solution for time series forecasting in agricultural markets."}}
{"id": "2505.09922", "pdf": "https://arxiv.org/pdf/2505.09922", "abs": "https://arxiv.org/abs/2505.09922", "authors": ["Zichen Liu", "Wei Zhang", "Tiejun Li"], "title": "Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity", "categories": ["cs.LG"], "comment": "22 pages", "summary": "Euclidean diffusion models have achieved remarkable success in generative\nmodeling across diverse domains, and they have been extended to manifold case\nin recent advances. Instead of explicitly utilizing the structure of special\nmanifolds as studied in previous works, we investigate direct sampling of the\nEuclidean diffusion models for general manifold-constrained data in this paper.\nWe reveal the multiscale singularity of the score function in the embedded\nspace of manifold, which hinders the accuracy of diffusion-generated samples.\nWe then present an elaborate theoretical analysis of the singularity structure\nof the score function by separating it along the tangential and normal\ndirections of the manifold. To mitigate the singularity and improve the\nsampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces\nnon-isotropic noise along the normal direction to reduce scale discrepancies,\nand (2) Tango-DM, which trains only the tangential component of the score\nfunction using a tangential-only loss function. Numerical experiments\ndemonstrate that our methods achieve superior performance on distributions over\nvarious manifolds with complex geometries.", "AI": {"tldr": "This paper studies Euclidean diffusion models for manifold-constrained data without explicit manifold structure usage. It presents two novel methods, Niso-DM and Tango-DM, to improve sampling accuracy by addressing the singularity of the score function.", "motivation": "To develop methods for improving sampling accuracy of diffusion models on complex manifold-constrained data without relying on explicit manifold structures.", "method": "Introduce non-isotropic noise along the normal direction (Niso-DM) and train only the tangential component of the score function using a tangential-only loss function (Tango-DM).", "result": "The proposed methods achieve better performance on distributions over various manifolds with complex geometries.", "conclusion": "Direct sampling of Euclidean diffusion models can be improved by addressing the singularity of the score function through non-isotropic noise introduction and tangential component training."}}
{"id": "2505.09925", "pdf": "https://arxiv.org/pdf/2505.09925", "abs": "https://arxiv.org/abs/2505.09925", "authors": ["Yutao Yang", "Jie Zhou", "Junsong Li", "Qianjun Pan", "Bihao Zhan", "Qin Chen", "Xipeng Qiu", "Liang He"], "title": "Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an interactive continual learning paradigm where AI\nmodels dynamically learn new skills from real-time human feedback while\nretaining prior knowledge. This paradigm distinctively addresses two major\nlimitations of traditional continual learning: (1) dynamic model updates using\nstreaming, real-time human-annotated data, rather than static datasets with\nfixed labels, and (2) the assumption of clean labels, by explicitly handling\nthe noisy feedback common in real-world interactions. To tackle these problems,\nwe propose RiCL, a Reinforced interactive Continual Learning framework\nleveraging Large Language Models (LLMs) to learn new skills effectively from\ndynamic feedback. RiCL incorporates three key components: a temporal\nconsistency-aware purifier to automatically discern clean from noisy samples in\ndata streams; an interaction-aware direct preference optimization strategy to\nalign model behavior with human intent by reconciling AI-generated and\nhuman-provided feedback; and a noise-resistant contrastive learning module that\ncaptures robust representations by exploiting inherent data relationships, thus\navoiding reliance on potentially unreliable labels. Extensive experiments on\ntwo benchmark datasets (FewRel and TACRED), contaminated with realistic noise\npatterns, demonstrate that our RiCL approach substantially outperforms existing\ncombinations of state-of-the-art online continual learning and noisy-label\nlearning methods.", "AI": {"tldr": "This paper proposes RiCL, a Reinforced interactive Continual Learning framework, which allows AI models to learn new skills dynamically from real-time human feedback while preserving previous knowledge. It also handles noisy feedback and shows better performance than existing methods.", "motivation": "Traditional continual learning approaches have limitations such as relying on static datasets and assuming clean labels, which do not apply to real-world scenarios where data is often noisy and constantly changing.", "method": "RiCL framework includes a purifier for cleaning data, a strategy for aligning model behavior with human intent, and a module for capturing robust representations, all leveraging LLMs.", "result": "Experiments on two benchmark datasets with realistic noise show that RiCL outperforms existing methods.", "conclusion": "The proposed RiCL framework successfully addresses limitations of traditional continual learning by handling dynamic and noisy feedback."}}
{"id": "2505.09949", "pdf": "https://arxiv.org/pdf/2505.09949", "abs": "https://arxiv.org/abs/2505.09949", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Samgyu Yang", "Abdulrahman Faden"], "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors", "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.", "AI": {"tldr": "This research uses a fine-tuned large language model to analyze freeway crash data and identify primary crash causes, demonstrating its effectiveness and potential for improving traffic safety measures.", "motivation": "To understand the factors contributing to traffic crashes and develop strategies to mitigate their severity.", "method": "Leveraging large language model (LLM) to analyze freeway crash data and provide crash causation analysis. Fine-tuning the Llama3 8B model using QLoRA. Using zero-shot classification to identify crash causation without pre-labeled data.", "result": "The fine-tuned Llama3 8B model effectively identified primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data offers more profound insights. There was a high level of agreement among researchers in the field of traffic safety.", "conclusion": "LLMs can effectively identify primary crash causes and provide comprehensive explanations. They have practical applicability and potential to improve traffic safety measures."}}
{"id": "2505.09952", "pdf": "https://arxiv.org/pdf/2505.09952", "abs": "https://arxiv.org/abs/2505.09952", "authors": ["Tianyu Huai", "Jie Zhou", "Yuxuan Cai", "Qin Chen", "Wen Wu", "Xingjiao Wu", "Xipeng Qiu", "Liang He"], "title": "Task-Core Memory Management and Consolidation for Long-term Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to Neurips2025", "summary": "In this paper, we focus on a long-term continual learning (CL) task, where a\nmodel learns sequentially from a stream of vast tasks over time, acquiring new\nknowledge while retaining previously learned information in a manner akin to\nhuman learning. Unlike traditional CL settings, long-term CL involves handling\na significantly larger number of tasks, which exacerbates the issue of\ncatastrophic forgetting. Our work seeks to address two critical questions: 1)\nHow do existing CL methods perform in the context of long-term CL? and 2) How\ncan we mitigate the catastrophic forgetting that arises from prolonged\nsequential updates? To tackle these challenges, we propose a novel framework\ninspired by human memory mechanisms for long-term continual learning (Long-CL).\nSpecifically, we introduce a task-core memory management strategy to\nefficiently index crucial memories and adaptively update them as learning\nprogresses. Additionally, we develop a long-term memory consolidation mechanism\nthat selectively retains hard and discriminative samples, ensuring robust\nknowledge retention. To facilitate research in this area, we construct and\nrelease two multi-modal and textual benchmarks, MMLongCL-Bench and\nTextLongCL-Bench, providing a valuable resource for evaluating long-term CL\napproaches. Experimental results show that Long-CL outperforms the previous\nstate-of-the-art by 7.4\\% and 6.5\\% AP on the two benchmarks, respectively,\ndemonstrating the effectiveness of our approach.", "AI": {"tldr": "This paper addresses the problem of catastrophic forgetting in long-term continual learning, proposing a novel framework called Long-CL inspired by human memory mechanisms. It introduces a task-core memory management strategy and a long-term memory consolidation mechanism, outperforming previous state-of-the-art methods by 7.4% and 6.5% AP on two newly constructed benchmarks.", "motivation": "To handle the challenge of catastrophic forgetting in long-term continual learning involving a large number of tasks.", "method": "Proposes a novel framework named Long-CL with a task-core memory management strategy and a long-term memory consolidation mechanism.", "result": "Experimental results show that Long-CL outperforms previous state-of-the-art methods by 7.4% and 6.5% AP on the two constructed benchmarks.", "conclusion": "The proposed Long-CL framework effectively mitigates catastrophic forgetting in long-term continual learning."}}
{"id": "2505.09955", "pdf": "https://arxiv.org/pdf/2505.09955", "abs": "https://arxiv.org/abs/2505.09955", "authors": ["Jaeho Kim", "Seulki Lee"], "title": "TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 Accept", "summary": "Unsupervised domain adaptation (UDA) for time series data remains a critical\nchallenge in deep learning, with traditional pseudo-labeling strategies failing\nto capture temporal patterns and channel-wise shifts between domains, producing\nsub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that\naddresses these limitations by modeling the joint distribution $P(\\mathbf{X},\ny)$ of the source domain through code transition matrices, where the codes are\nderived from vector quantization (VQ) of time series patches. Our method\nconstructs class- and channel-wise code transition matrices from the source\ndomain and employs Bayes' rule for target domain adaptation, generating\npseudo-labels based on channel-wise weighted class-conditional likelihoods.\nTransPL offers three key advantages: explicit modeling of temporal transitions\nand channel-wise shifts between different domains, versatility towards\ndifferent UDA scenarios (e.g., weakly-supervised UDA), and explainable\npseudo-label generation. We validate TransPL's effectiveness through extensive\nanalysis on four time series UDA benchmarks and confirm that it consistently\noutperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%\naccuracy improvement, 4.9% F1 improvement), while providing interpretable\ninsights into the domain adaptation process through its learned code transition\nmatrices.", "AI": {"tldr": "A new method called TransPL is proposed to improve unsupervised domain adaptation for time series data by explicitly modeling temporal transitions and channel-wise shifts.", "motivation": "Traditional pseudo-labeling strategies fail to capture temporal patterns and channel-wise shifts between domains, resulting in sub-optimal pseudo-labels.", "method": "TransPL uses code transition matrices derived from vector quantization of time series patches and applies Bayes' rule for target domain adaptation to generate pseudo-labels.", "result": "TransPL outperforms existing methods by 6.1% accuracy and 4.9% F1 on four time series UDA benchmarks.", "conclusion": "TransPL provides an effective and explainable solution for unsupervised domain adaptation of time series data."}}
{"id": "2505.09959", "pdf": "https://arxiv.org/pdf/2505.09959", "abs": "https://arxiv.org/abs/2505.09959", "authors": ["Zengxia Guo", "Bohui An", "Zhongqi Lu"], "title": "Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated reinforcement learning (FRL) methods usually share the encrypted\nlocal state or policy information and help each client to learn from others\nwhile preserving everyone's privacy. In this work, we propose that sharing the\napproximated behavior metric-based state projection function is a promising way\nto enhance the performance of FRL and concurrently provides an effective\nprotection of sensitive information. We introduce FedRAG, a FRL framework to\nlearn a computationally practical projection function of states for each client\nand aggregating the parameters of projection functions at a central server. The\nFedRAG approach shares no sensitive task-specific information, yet provides\ninformation gain for each client. We conduct extensive experiments on the\nDeepMind Control Suite to demonstrate insightful results.", "AI": {"tldr": "This paper proposes FedRAG, a federated reinforcement learning framework that enhances performance by sharing approximated behavior metric-based state projection functions while protecting sensitive information.", "motivation": "To improve the performance of federated reinforcement learning while ensuring privacy protection by sharing useful information without disclosing sensitive task-specific details.", "method": "Introducing FedRAG which learns a projection function of states for each client and aggregates the parameters of these functions at a central server.", "result": "Extensive experiments on the DeepMind Control Suite show insightful results.", "conclusion": "Sharing approximated behavior metric-based state projection functions is a promising way to enhance federated reinforcement learning performance while effectively protecting sensitive information."}}
{"id": "2505.09969", "pdf": "https://arxiv.org/pdf/2505.09969", "abs": "https://arxiv.org/abs/2505.09969", "authors": ["Ali Azimi Lamir", "Shiva Razzagzadeh", "Zeynab Rezaei"], "title": "A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study presents a machine learning-based framework for heart disease\nprediction using the heart-disease dataset, comprising 303 samples with 14\nfeatures. The methodology involves data preprocessing, model training, and\nevaluation using three classifiers: Logistic Regression, K-Nearest Neighbors\n(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and\nRandomizedSearchCV was employed to enhance model performance. The Random Forest\nclassifier outperformed other models, achieving an accuracy of 91% and an\nF1-score of 0.89. Evaluation metrics, including precision, recall, and\nconfusion matrix, revealed balanced performance across classes. The proposed\nmodel demonstrates strong potential for aiding clinical decision-making by\neffectively predicting heart disease. Limitations such as dataset size and\ngeneralizability underscore the need for future studies using larger and more\ndiverse datasets. This work highlights the utility of machine learning in\nhealthcare, offering insights for further advancements in predictive\ndiagnostics.", "AI": {"tldr": "This study developed a machine learning framework using Logistic Regression, KNN, and Random Forest for heart disease prediction. The Random Forest classifier showed the best performance with 91% accuracy and 0.89 F1-score.", "motivation": "To predict heart disease using the heart-disease dataset.", "method": "Data preprocessing, model training with Logistic Regression, KNN, and Random Forest, hyperparameter tuning with GridSearchCV and RandomizedSearchCV.", "result": "The Random Forest classifier achieved 91% accuracy and 0.89 F1-score, showing balanced performance across classes.", "conclusion": "The proposed machine learning model has strong potential for aiding clinical decision-making in heart disease prediction but needs further development with larger and more diverse datasets."}}
{"id": "2505.09983", "pdf": "https://arxiv.org/pdf/2505.09983", "abs": "https://arxiv.org/abs/2505.09983", "authors": ["Changxun Zhu", "Qilong Wu", "Lingjuan Lyu", "Shibei Xue"], "title": "Sybil-based Virtual Data Poisoning Attacks in Federated Learning", "categories": ["cs.CR", "cs.LG"], "comment": "7 pages, 6 figures, accepted by IEEE Codit 2025", "summary": "Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data.", "AI": {"tldr": "This paper proposes a sybil-based virtual data poisoning attack for federated learning, which reduces computational complexity using gradient matching and provides three target model acquisition schemes for different scenarios.", "motivation": "To address the vulnerability of federated learning to poisoning attacks with lower costs.", "method": "Generating sybil nodes to amplify poisoning impact and developing a virtual data generation method based on gradient matching.", "result": "The proposed method outperforms other attack algorithms in simulations, especially under non-independent uniformly distributed data.", "conclusion": "A novel sybil-based virtual data poisoning attack is developed for federated learning, achieving better performance than existing methods."}}
{"id": "2505.10003", "pdf": "https://arxiv.org/pdf/2505.10003", "abs": "https://arxiv.org/abs/2505.10003", "authors": ["Tianyu Jiao", "Zhuoran Xiao", "Yihang Huang", "Chenhui Ye", "Yijia Feng", "Liyu Cai", "Jiang Chang", "Fangkun Liu", "Yin Xu", "Dazhi He", "Yunfeng Guan", "Wenjun Zhang"], "title": "AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets.", "AI": {"tldr": "This paper proposes AI2MMUM, a scalable task-aware artificial intelligence-air interface multi-modal universal model for processing multi-modal data and executing diverse air interface tasks in 6G systems.", "motivation": "To design a universal model capable of handling multi-modal data and executing diverse air interface tasks in future wireless systems.", "method": "AI2MMUM uses a LLM backbone with contextual comprehension and generalization capabilities, frozen radio modality encoders, adapter layers, and lightweight task-specific heads. Task instructions include fixed keywords and learnable prefix prompts.", "result": "AI2MMUM achieved SOTA performance across five representative physical environment/wireless channel-based downstream tasks using the WAIR-D and DeepMIMO datasets.", "conclusion": "The proposed AI2MMUM model demonstrates flexibility and effectiveness in performing various physical layer tasks according to subtle task instructions."}}
{"id": "2505.10010", "pdf": "https://arxiv.org/pdf/2505.10010", "abs": "https://arxiv.org/abs/2505.10010", "authors": ["Jing-Cheng Pang", "Kaiyuan Li", "Yidi Wang", "Si-Hang Yang", "Shengyi Jiang", "Yang Yu"], "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts", "categories": ["cs.LG"], "comment": null, "summary": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.", "AI": {"tldr": "\u63d0\u51faImagineBench\uff0c\u9996\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ed3\u5408\u771f\u5b9e\u4e0e\u60f3\u8c61\u6eda\u52a8\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u7b97\u6cd5\u5728\u5229\u7528\u60f3\u8c61\u6eda\u52a8\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u7b97\u6cd5\u6539\u8fdb\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6570\u636e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u865a\u62df\u7ecf\u9a8c\uff08\u60f3\u8c61\u6eda\u52a8\uff09\u6765\u51cf\u5c11\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faImagineBench\uff0c\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u771f\u5b9e\u6eda\u52a8\u548c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u60f3\u8c61\u6eda\u52a8\u3002", "result": "\u73b0\u6709\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u4e3a35.44%\uff0c\u800c\u9488\u5bf9\u771f\u5b9e\u6eda\u52a8\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728\u56f0\u96be\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u4e3a64.37%\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u66f4\u597d\u5730\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u60f3\u8c61\u6eda\u52a8\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\uff0c\u5305\u62ec\u66f4\u597d\u7684\u5229\u7528\u60f3\u8c61\u6eda\u52a8\u3001\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u6301\u7eed\u5b66\u4e60\u4ee5\u53ca\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6269\u5c55\u3002"}}
{"id": "2505.10037", "pdf": "https://arxiv.org/pdf/2505.10037", "abs": "https://arxiv.org/abs/2505.10037", "authors": ["Takafumi Ito", "Lysenko Artem", "Tatsuhiko Tsunoda"], "title": "Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "comment": "10 pages, 3 figures", "summary": "Quantum-classical Hybrid Machine Learning (QHML) models are recognized for\ntheir robust performance and high generalization ability even for relatively\nsmall datasets. These qualities offer unique advantages for anti-cancer drug\nresponse prediction, where the number of available samples is typically small.\nHowever, such hybrid models appear to be very sensitive to the data encoding\nused at the interface of a neural network and a quantum circuit, with\nsuboptimal choices leading to stability issues. To address this problem, we\npropose a novel strategy that uses a normalization function based on a\nmoderated gradient version of the $\\tanh$. This method transforms the outputs\nof the neural networks without concentrating them at the extreme value ranges.\nOur idea was evaluated on a dataset of gene expression and drug response\nmeasurements for various cancer cell lines, where we compared the prediction\nperformance of a classical deep learning model and several QHML models. These\nresults confirmed that QHML performed better than the classical models when\ndata was optimally normalized. This study opens up new possibilities for\nbiomedical data analysis using quantum computers.", "AI": {"tldr": "This paper introduces a novel normalization strategy for Quantum-classical Hybrid Machine Learning models to improve their performance and stability in predicting anti-cancer drug responses, especially given limited datasets.", "motivation": "To overcome the sensitivity of hybrid models to data encoding and improve stability and performance for small datasets in anti-cancer drug response prediction.", "method": "Proposes a normalization function based on a moderated gradient version of tanh to transform neural network outputs avoiding extreme value ranges.", "result": "Demonstrated that Quantum-classical Hybrid Machine Learning models outperform classical models when data is optimally normalized using the proposed method.", "conclusion": "Introduces a new approach that enhances the capabilities of quantum machine learning in biomedical data analysis."}}
{"id": "2505.10039", "pdf": "https://arxiv.org/pdf/2505.10039", "abs": "https://arxiv.org/abs/2505.10039", "authors": ["Hang Chen", "Jiaying Zhu", "Xinyu Yang", "Wenya Wang"], "title": "Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Circuit discovery has gradually become one of the prominent methods for\nmechanistic interpretability, and research on circuit completeness has also\ngarnered increasing attention. Methods of circuit discovery that do not\nguarantee completeness not only result in circuits that are not fixed across\ndifferent runs but also cause key mechanisms to be omitted. The nature of\nincompleteness arises from the presence of OR gates within the circuit, which\nare often only partially detected in standard circuit discovery methods. To\nthis end, we systematically introduce three types of logic gates: AND, OR, and\nADDER gates, and decompose the circuit into combinations of these logical\ngates. Through the concept of these gates, we derive the minimum requirements\nnecessary to achieve faithfulness and completeness. Furthermore, we propose a\nframework that combines noising-based and denoising-based interventions, which\ncan be easily integrated into existing circuit discovery methods without\nsignificantly increasing computational complexity. This framework is capable of\nfully identifying the logic gates and distinguishing them within the circuit.\nIn addition to the extensive experimental validation of the framework's ability\nto restore the faithfulness, completeness, and sparsity of circuits, using this\nframework, we uncover fundamental properties of the three logic gates, such as\ntheir proportions and contributions to the output, and explore how they behave\namong the functionalities of language models.", "AI": {"tldr": "This paper introduces a framework that combines noising-based and denoising-based interventions to identify and distinguish logic gates in neural networks, ensuring faithfulness and completeness in circuit discovery.", "motivation": "To address the incompleteness issue in circuit discovery caused by partially detected OR gates and ensure the fixed nature of circuits across different runs.", "method": "Systematically introducing three types of logic gates (AND, OR, ADDER) and decomposing circuits into combinations of these gates; proposing a framework that integrates noising-based and denoising-based interventions.", "result": "The framework can fully identify and distinguish logic gates within circuits, restoring their faithfulness, completeness, and sparsity. It also reveals fundamental properties of the logic gates and explores their functionality in language models.", "conclusion": "The proposed framework provides a systematic approach to achieving faithful and complete circuit discovery, enhancing mechanistic interpretability."}}
{"id": "2505.10040", "pdf": "https://arxiv.org/pdf/2505.10040", "abs": "https://arxiv.org/abs/2505.10040", "authors": ["Lei Song", "Jiaxing Li", "Shihan Guan", "Youyong Kong"], "title": "Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their\ncapacity to preserve previously acquired knowledge amid the assimilation of\nnovel information. Rehearsal-based techniques revisit historical examples,\nadopted as a principal strategy to alleviate this phenomenon. However, memory\nexplosion and privacy infringements impose significant constraints on their\nutility. Non-Exemplar methods circumvent the prior issues through Prototype\nReplay (PR), yet feature drift presents new challenges. In this paper, our\nempirical findings reveal that Prototype Contrastive Learning (PCL) exhibits\nless pronounced drift than conventional PR. Drawing upon PCL, we propose\nInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar\nContinual Graph Learning (NECGL). Exploiting graph structural information, we\nformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature\ndistributions towards high-impact nodes to augment the model's capacity for\nassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)\nsafeguards task memory by regularizing discontinuities in class relationships.\nMoreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,\nfostering greater inter-class discriminability. Evaluations on four node\nclassification benchmark datasets demonstrate that our method outperforms\nexisting state-of-the-art methods, achieving a better trade-off between\nplasticity and stability.", "AI": {"tldr": "This paper introduces Instance-Prototype Affinity Learning (IPAL), a novel approach for Non-Exemplar Continual Graph Learning (NECGL) that uses Topology-Integrated Gaussian Prototypes (TIGP) and Instance-Prototype Affinity Distillation (IPAD) to improve knowledge assimilation and retention in graph neural networks.", "motivation": "To address catastrophic forgetting in graph neural networks and provide a solution that avoids memory explosion and privacy concerns while enhancing knowledge retention.", "method": "Proposes IPAL which includes TIGP to guide feature distributions and DBP to improve inter-class discriminability, using Prototype Contrastive Learning (PCL) to reduce feature drift.", "result": "Demonstrates superior performance compared to existing methods on four node classification benchmarks, achieving a better balance between plasticity and stability.", "conclusion": "IPAL is an effective method for continual learning in graph neural networks, addressing key challenges such as catastrophic forgetting, memory explosion, and privacy concerns."}}
{"id": "2505.10050", "pdf": "https://arxiv.org/pdf/2505.10050", "abs": "https://arxiv.org/abs/2505.10050", "authors": ["Fahad Almalki", "Mehedi Masud"], "title": "Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional machine learning models often prioritize predictive accuracy,\noften at the expense of model transparency and interpretability. The lack of\ntransparency makes it difficult for organizations to comply with regulatory\nrequirements and gain stakeholders trust. In this research, we propose a fraud\ndetection framework that combines a stacking ensemble of well-known gradient\nboosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable\nartificial intelligence (XAI) techniques are used to enhance the transparency\nand interpretability of the model's decisions. We used SHAP (SHapley Additive\nExplanations) for feature selection to identify the most important features.\nFurther efforts were made to explain the model's predictions using Local\nInterpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots\n(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection\ndataset, which includes more than 590,000 real transaction records, was used to\nevaluate the proposed model. The model achieved a high performance with an\naccuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent\nrelated approaches. These results indicate that combining high prediction\naccuracy with transparent interpretability is possible and could lead to a more\nethical and trustworthy solution in financial fraud detection.", "AI": {"tldr": "This study introduces a fraud detection framework that uses a stacking ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) combined with XAI techniques (SHAP, LIME, PDP, PFI) for enhanced transparency and interpretability. It achieved 99% accuracy and 0.99 AUC-ROC on the IEEE-CIS dataset.", "motivation": "To address the lack of transparency in traditional machine learning models, making it hard for organizations to meet regulatory requirements and gain stakeholder trust.", "method": "Stacking ensemble of XGBoost, LightGBM, CatBoost with XAI techniques including SHAP for feature selection, and LIME, PDP, PFI for explaining predictions.", "result": "Achieved 99% accuracy and 0.99 AUC-ROC on the IEEE-CIS Fraud Detection dataset, outperforming several recent related approaches.", "conclusion": "Combining high prediction accuracy with transparent interpretability is feasible and could promote ethical and trustworthy solutions in financial fraud detection."}}
{"id": "2505.10120", "pdf": "https://arxiv.org/pdf/2505.10120", "abs": "https://arxiv.org/abs/2505.10120", "authors": ["Guillaume Godin"], "title": "All You Need Is Synthetic Task Augmentation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 3 Figures, 6 tables", "summary": "Injecting rule-based models like Random Forests into differentiable neural\nnetwork frameworks remains an open challenge in machine learning. Recent\nadvancements have demonstrated that pretrained models can generate efficient\nmolecular embeddings. However, these approaches often require extensive\npretraining and additional techniques, such as incorporating posterior\nprobabilities, to boost performance. In our study, we propose a novel strategy\nthat jointly trains a single Graph Transformer neural network on both sparse\nmultitask molecular property experimental targets and synthetic targets derived\nfrom XGBoost models trained on Osmordred molecular descriptors. These synthetic\ntasks serve as independent auxiliary tasks. Our results show consistent and\nsignificant performance improvement across all 19 molecular property prediction\ntasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms\nthe XGBoost single-task learner. This demonstrates that synthetic task\naugmentation is an effective method for enhancing neural model performance in\nmultitask molecular property prediction without the need for feature injection\nor pretraining.", "AI": {"tldr": "A new method that uses synthetic tasks to enhance neural network performance in predicting multiple molecular properties simultaneously.", "motivation": "To inject rule-based models into differentiable neural networks and improve molecular property prediction performance.", "method": "Jointly training a single Graph Transformer neural network on both experimental and synthetic tasks.", "result": "Consistent and significant performance improvement across all 19 molecular property prediction tasks.", "conclusion": "Our proposed approach shows significant performance improvements in multitask molecular property prediction."}}
{"id": "2505.10057", "pdf": "https://arxiv.org/pdf/2505.10057", "abs": "https://arxiv.org/abs/2505.10057", "authors": ["Tiancong Cheng", "Ying Zhang", "Yuxuan Liang", "Roger Zimmermann", "Zhiwen Yu", "Bin Guo"], "title": "JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation", "categories": ["cs.LG"], "comment": null, "summary": "Depth estimation and scene segmentation are two important tasks in\nintelligent transportation systems. A joint modeling of these two tasks will\nreduce the requirement for both the storage and training efforts. This work\nexplores how the multi-task distillation could be used to improve such unified\nmodeling. While existing solutions transfer multiple teachers' knowledge in a\nstatic way, we propose a self-adaptive distillation method that can dynamically\nadjust the knowledge amount from each teacher according to the student's\ncurrent learning ability. Furthermore, as multiple teachers exist, the\nstudent's gradient update direction in the distillation is more prone to be\nerroneous where knowledge forgetting may occur. To avoid this, we propose a\nknowledge trajectory to record the most essential information that a model has\nlearnt in the past, based on which a trajectory-based distillation loss is\ndesigned to guide the student to follow the learning curve similarly in a\ncost-effective way. We evaluate our method on multiple benchmarking datasets\nincluding Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,\nour method achieves a clearly improvement. The code is provided in the\nsupplementary materials.", "AI": {"tldr": "This work explores how the multi-task distillation could be used to improve the unified modeling of depth estimation and scene segmentation.", "motivation": "reduce the requirement for both the storage and training efforts", "method": "self-adaptive distillation method, knowledge trajectory, trajectory-based distillation loss", "result": "clearly improvement", "conclusion": "Compared to the state-of-the-art solutions, our method achieves a clearly improvement."}}
{"id": "2505.10128", "pdf": "https://arxiv.org/pdf/2505.10128", "abs": "https://arxiv.org/abs/2505.10128", "authors": ["Huy Q. Le", "Latif U. Khan", "Choong Seon Hong"], "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": "IWCMC 2025", "summary": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.", "AI": {"tldr": "A new method called FedAPC is proposed to improve Federated Learning performance under domain heterogeneity.", "motivation": "Federated Learning faces challenges due to statistical heterogeneity, especially domain heterogeneity.", "method": "FedAPC uses prototype augmentation to enhance feature diversity and model robustness.", "result": "Experimental results on Office-10 and Digits datasets show FedAPC outperforms SOTA baselines.", "conclusion": "FedAPC improves the generalization ability of the FL global model under domain heterogeneity."}}
{"id": "2505.10083", "pdf": "https://arxiv.org/pdf/2505.10083", "abs": "https://arxiv.org/abs/2505.10083", "authors": ["Chengsen Wang", "Qi Qi", "Zhongwen Rao", "Lujia Pan", "Jingyu Wang", "Jianxin Liao"], "title": "ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.", "AI": {"tldr": "ChronoSteer combines textual and temporal data using a novel framework, achieving superior performance in time series forecasting.", "motivation": "Existing forecasting methods are limited by their reliance on unimodal time series data, failing to utilize rich textual information. The integration of LLMs and TSFMs aims to concurrently leverage both temporal and textual information for future inference.", "method": "A decoupled framework is proposed where an LLM transforms textual events into revision instructions to guide the output of TSFM. ChronoSteer, a multimodal TSFM, is developed to handle these instructions. A two-stage training strategy using synthetic data is also introduced to alleviate data shortages.", "result": "ChronoSteer improves prediction accuracy by 25.7% over the unimodal backbone and 22.5% over the previous best multimodal method.", "conclusion": "ChronoSteer, when integrated with an LLM, shows significant improvement in prediction accuracy compared to unimodal methods and previous multimodal approaches."}}
{"id": "2505.10167", "pdf": "https://arxiv.org/pdf/2505.10167", "abs": "https://arxiv.org/abs/2505.10167", "authors": ["Saikat Barua", "Mostafizur Rahman", "Shehenaz Khaled", "Md Jafor Sadek", "Rafiul Islam", "Shahnewaz Siddique"], "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "16 pages, 6 figures, 7 equations", "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.", "AI": {"tldr": "This paper introduces QuXAI, a framework using Q-MEDLEY to explain feature importance in hybrid quantum-classical machine learning models, improving their interpretability and reliability.", "motivation": "To address the lack of robust global and local explainability approaches for hybrid quantum-classical machine learning models employing quantized feature encoding and classical learning.", "method": "Creating HQML models with quantum feature maps, using Q-MEDLEY which combines feature-based inferences, preserves the quantum transformation stage, and visualizes attributions.", "result": "Q-MEDLEY effectively delineates influential classical aspects in HQML models, separates noise, and performs well against established XAI techniques in classical validation settings.", "conclusion": "QuXAI provides a pathway to enhance the interpretability and reliability of HQML models, fostering greater confidence and safe/responsible use of quantum-enhanced AI technology."}}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117", "abs": "https://arxiv.org/abs/2505.10117", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfei Wang", "Jun Luo"], "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.", "AI": {"tldr": "This paper introduces MiCo, a hierarchical language agent framework that uses large language models to solve the Online Dynamic Multidimensional Bin Packing problem in cloud VM scheduling.", "motivation": "Traditional methods struggle to handle the large-scale complexity and fluctuating demands of VM scheduling in cloud services. Existing learning-based methods also lack generalizability and interpretability.", "method": "The paper proposes a two-stage architecture called MiCo, which includes Option Miner and Option Composer. The former discovers diverse non-context-aware strategies using LLMs, while the latter integrates these strategies with contextual ones.", "result": "Extensive experiments on real-world datasets show that MiCo achieves a competitive ratio of 96.9% in large-scale scenarios with more than 10,000 virtual machines. It also performs well under nonstationary request flows and diverse configurations.", "conclusion": "MiCo, a hierarchical language agent framework, provides a novel approach to VM scheduling by formulating it as a SMDP-Option problem and utilizing LLMs to generate both non-context-aware and context-aware strategies."}}
{"id": "2505.10172", "pdf": "https://arxiv.org/pdf/2505.10172", "abs": "https://arxiv.org/abs/2505.10172", "authors": ["Zeyan Li", "Libing Chen", "Yin Tang"], "title": "Does Scaling Law Apply in Time Series Forecasting?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Rapid expansion of model size has emerged as a key challenge in time series\nforecasting. From early Transformer with tens of megabytes to recent\narchitectures like TimesNet with thousands of megabytes, performance gains have\noften come at the cost of exponentially increasing parameter counts. But is\nthis scaling truly necessary? To question the applicability of the scaling law\nin time series forecasting, we propose Alinear, an ultra-lightweight\nforecasting model that achieves competitive performance using only k-level\nparameters. We introduce a horizon-aware adaptive decomposition mechanism that\ndynamically rebalances component emphasis across different forecast lengths,\nalongside a progressive frequency attenuation strategy that achieves stable\nprediction in various forecasting horizons without incurring the computational\noverhead of attention mechanisms. Extensive experiments on seven benchmark\ndatasets demonstrate that Alinear consistently outperforms large-scale models\nwhile using less than 1% of their parameters, maintaining strong accuracy\nacross both short and ultra-long forecasting horizons. Moreover, to more fairly\nevaluate model efficiency, we propose a new parameter-aware evaluation metric\nthat highlights the superiority of ALinear under constrained model budgets. Our\nanalysis reveals that the relative importance of trend and seasonal components\nvaries depending on data characteristics rather than following a fixed pattern,\nvalidating the necessity of our adaptive design. This work challenges the\nprevailing belief that larger models are inherently better and suggests a\nparadigm shift toward more efficient time series modeling.", "AI": {"tldr": "\u63d0\u51faAlinear\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u6781\u5c11\u91cf\u7684\u53c2\u6570\u5c31\u80fd\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u5927\u89c4\u6a21\u6a21\u578b\u76f8\u5f53\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5f15\u5165\u9002\u5e94\u6027\u5206\u89e3\u673a\u5236\u548c\u9891\u7387\u8870\u51cf\u7b56\u7565\uff0cAlinear\u80fd\u591f\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u9884\u6d4b\u957f\u5ea6\u4e0a\u7684\u7ec4\u4ef6\u91cd\u70b9\uff0c\u5e76\u5728\u5404\u79cd\u9884\u6d4b\u8303\u56f4\u5185\u5b9e\u73b0\u7a33\u5b9a\u7684\u9884\u6d4b\u6548\u679c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAlinear\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5927\u578b\u6a21\u578b\uff0c\u5176\u53c2\u6570\u91cf\u4e0d\u5230\u540e\u8005\u76841%\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff0c\u5f3a\u8c03\u4e86Alinear\u5728\u53d7\u9650\u6a21\u578b\u9884\u7b97\u4e0b\u7684\u4f18\u8d8a\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u6311\u6218\u4e86\u201c\u66f4\u5927\u7684\u6a21\u578b\u603b\u662f\u66f4\u597d\u201d\u7684\u666e\u904d\u89c2\u5ff5\uff0c\u63d0\u5021\u5411\u66f4\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u8d28\u7591\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u662f\u5426\u9700\u8981\u4e0d\u65ad\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u6765\u83b7\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u63a2\u7d22\u66f4\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faAlinear\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a\uff081\uff09\u65f6\u57df\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5206\u89e3\u673a\u5236\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u9884\u6d4b\u957f\u5ea6\u4e0a\u7684\u7ec4\u4ef6\u91cd\u70b9\uff1b\uff082\uff09\u9010\u6b65\u9891\u7387\u8870\u51cf\u7b56\u7565\uff0c\u786e\u4fdd\u5728\u5404\u79cd\u9884\u6d4b\u8303\u56f4\u5185\u7a33\u5b9a\u9884\u6d4b\uff0c\u65e0\u9700\u4f9d\u8d56\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "Alinear\u6a21\u578b\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u8272\u6027\u80fd\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u800c\u4e14\u4ec5\u4f7f\u7528\u5b83\u4eec\u4e0d\u52301%\u7684\u53c2\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u77ed\u65f6\u548c\u8d85\u957f\u65f6\u9884\u6d4b\u7684\u9ad8\u7cbe\u5ea6\u3002\u53e6\u5916\uff0c\u65b0\u63d0\u51fa\u7684\u53c2\u6570\u611f\u77e5\u8bc4\u4f30\u6307\u6807\u8fdb\u4e00\u6b65\u51f8\u663e\u4e86Alinear\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u6311\u6218\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u5185\u5173\u4e8e\u66f4\u5927\u6a21\u578b\u5fc5\u7136\u66f4\u597d\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u5021\u5bfc\u91c7\u7528\u66f4\u9ad8\u6548\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2505.10264", "pdf": "https://arxiv.org/pdf/2505.10264", "abs": "https://arxiv.org/abs/2505.10264", "authors": ["Francesco Diana", "Andr\u00e9 Nusser", "Chuan Xu", "Giovanni Neglia"], "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.", "AI": {"tldr": "This paper introduces a novel data reconstruction attack in Federated Learning (FL), which allows for perfect recovery of arbitrarily large data batches in classification tasks without prior knowledge of clients' data.", "motivation": "Recent studies have shown vulnerabilities in FL where a malicious central server can manipulate model updates to reconstruct clients' private training data. However, existing attacks have limitations like assumptions about data distribution or inefficiency with larger batch sizes.", "method": "The method uses a new geometric perspective on fully connected layers to create malicious model parameters for perfect data recovery.", "result": "Experiments on image and tabular datasets show that the new attack method outperforms existing methods and can perfectly reconstruct data batches two orders of magnitude larger than previous state-of-the-art methods.", "conclusion": "This research highlights significant vulnerabilities in FL and proposes an improved method for data reconstruction attacks."}}
{"id": "2505.10125", "pdf": "https://arxiv.org/pdf/2505.10125", "abs": "https://arxiv.org/abs/2505.10125", "authors": ["Wujun Zhou", "Shu Ding", "ZeLin Li", "Wei Wang"], "title": "Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning enables the clients to collaboratively train a global\nmodel, which is aggregated from local models. Due to the heterogeneous data\ndistributions over clients and data privacy in federated learning, it is\ndifficult to train local models to achieve a well-performed global model. In\nthis paper, we introduce the adaptability of local models, i.e., the average\nperformance of local models on data distributions over clients, and enhance the\nperformance of the global model by improving the adaptability of local models.\nSince each client does not know the data distributions over other clients, the\nadaptability of the local model cannot be directly optimized. First, we provide\nthe property of an appropriate local model which has good adaptability on the\ndata distributions over clients. Then, we formalize the property into the local\ntraining objective with a constraint and propose a feasible solution to train\nthe local model. Extensive experiments on federated learning benchmarks\ndemonstrate that our method significantly improves the adaptability of local\nmodels and achieves a well-performed global model that consistently outperforms\nthe baseline methods.", "AI": {"tldr": "This paper introduces the adaptability of local models and proposes a method to enhance the performance of the global model by improving the adaptability of local models in federated learning.", "motivation": "Due to the heterogeneous data distributions over clients and data privacy in federated learning, it is difficult to train local models to achieve a well-performed global model.", "method": "We provide the property of an appropriate local model which has good adaptability on the data distributions over clients, then formalize the property into the local training objective with a constraint and propose a feasible solution to train the local model.", "result": "Extensive experiments on federated learning benchmarks demonstrate that our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods.", "conclusion": "Our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods."}}
{"id": "2505.10297", "pdf": "https://arxiv.org/pdf/2505.10297", "abs": "https://arxiv.org/abs/2505.10297", "authors": ["Chibueze Peace Obioma", "Youcheng Sun", "Mustafa A. Mustafa"], "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Submitted to ESORICS 2025", "summary": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.", "AI": {"tldr": "A novel defense mechanism called FeRA is proposed for detecting backdoor attacks in federated learning systems with non-IID data.", "motivation": "The heterogeneity of edge devices leads to non-IID data which makes backdoor attacks detection more difficult.", "method": "FeRA uses cross-client attention over internal feature representations to compute anomaly scores based on representation reconstruction errors.", "result": "FeRA shows robust performance in various FL scenarios including challenging non-IID data distributions, reducing backdoor attack success rates while maintaining high accuracy on the main task.", "conclusion": "FeRA is model-agnostic, attack-agnostic, and does not need labeled reference data, making it suitable for heterogeneous and resource-limited edge deployments."}}
{"id": "2505.10330", "pdf": "https://arxiv.org/pdf/2505.10330", "abs": "https://arxiv.org/abs/2505.10330", "authors": ["Jonathan Clifford Balloch"], "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change", "categories": ["cs.LG", "cs.AI"], "comment": "PhD Dissertation, 131 pages", "summary": "Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.", "AI": {"tldr": "This dissertation explores how to make RL agents adapt efficiently to changing environments by focusing on exploration strategies and preserving prior knowledge.", "motivation": "The motivation is to enable RL agents to efficiently adapt their behavior when encountering novel environmental changes during deployment without catastrophically forgetting useful prior knowledge.", "method": "The method involves using structured representations that allow updating without disrupting reusable components and implementing prioritized exploration and sampling strategies.", "result": "The results demonstrate that efficient online adaptation is possible with the right strategies and representations.", "conclusion": "This dissertation shows that efficient online adaptation in RL agents for non-stationary environments requires two key capabilities: prioritized exploration and sampling strategies, and selective preservation of prior knowledge."}}
{"id": "2505.10147", "pdf": "https://arxiv.org/pdf/2505.10147", "abs": "https://arxiv.org/abs/2505.10147", "authors": ["Yash", "Nikhil Karamchandani", "Avishek Ghosh"], "title": "Near Optimal Best Arm Identification for Clustered Bandits", "categories": ["cs.LG", "cs.MA"], "comment": "To be published in ICML 2025", "summary": "This work investigates the problem of best arm identification for multi-agent\nmulti-armed bandits. We consider $N$ agents grouped into $M$ clusters, where\neach cluster solves a stochastic bandit problem. The mapping between agents and\nbandits is a priori unknown. Each bandit is associated with $K$ arms, and the\ngoal is to identify the best arm for each agent under a $\\delta$-probably\ncorrect ($\\delta$-PC) framework, while minimizing sample complexity and\ncommunication overhead.\n  We propose two novel algorithms: Clustering then Best Arm Identification\n(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a\ntwo-phase approach that first clusters agents based on the bandit problems they\nare learning, followed by identifying the best arm for each cluster. BAI-Cl\nreverses the sequence by identifying the best arms first and then clustering\nagents accordingly. Both algorithms leverage the successive elimination\nframework to ensure computational efficiency and high accuracy.\n  We establish $\\delta$-PC guarantees for both methods, derive bounds on their\nsample complexity, and provide a lower bound for this problem class. Moreover,\nwhen $M$ is small (a constant), we show that the sample complexity of a variant\nof BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic\nand real-world datasets (MovieLens, Yelp) demonstrate the superior performance\nof the proposed algorithms in terms of sample and communication efficiency,\nparticularly in settings where $M \\ll N$.", "AI": {"tldr": "This paper studies the best arm identification problem for multi-agent multi-armed bandits. It proposes two algorithms, Cl-BAI and BAI-Cl, which efficiently identify the best arm for each agent with low sample and communication complexity.", "motivation": "To address the challenge of identifying the best arm for each agent in a multi-agent multi-armed bandit setting with unknown agent-bandit mapping, aiming for both sample and communication efficiency.", "method": "Proposes two algorithms: Cl-BAI (clusters agents first, then identifies best arms) and BAI-Cl (identifies best arms first, then clusters agents). Both use the successive elimination framework.", "result": "Establishes \u03b4-PC guarantees, derives bounds on sample complexity, and provides a lower bound for the problem class. Shows that a variant of BAI-Cl is minimax optimal when M is small.", "conclusion": "The proposed algorithms perform well in terms of sample and communication efficiency, especially when the number of clusters is much smaller than the number of agents."}}
{"id": "2505.10331", "pdf": "https://arxiv.org/pdf/2505.10331", "abs": "https://arxiv.org/abs/2505.10331", "authors": ["Luca Muscarnera", "Luigi Loreti", "Giovanni Todeschini", "Alessio Fumagalli", "Francesco Regazzoni"], "title": "Emergence of Structure in Ensembles of Random Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.", "AI": {"tldr": "\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\uff0c\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u4e2a\u6700\u4f18\u6e29\u5ea6\u4f7f\u5f97\u5206\u7c7b\u6700\u4f18\u5316\uff0c\u4e14\u6b64\u6e29\u5ea6\u4e0e\u6559\u5e08\u5206\u7c7b\u5668\u548c\u968f\u673a\u5206\u7c7b\u5668\u6570\u91cf\u65e0\u5173\u3002", "motivation": "\u7814\u7a76\u7531\u968f\u673a\u7ec4\u4ef6\u7ec4\u6210\u7684\u7cfb\u7edf\u4e2d\u4ece\u5fae\u89c2\u65e0\u5e8f\u5230\u5b8f\u89c2\u7ec4\u7ec7\u7684\u8fc7\u6e21\uff0c\u7279\u522b\u662f\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7406\u8bba\u6a21\u578b\u6765\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u3002\u4f7f\u7528Gibbs\u6d4b\u5ea6\u901a\u8fc7\u5206\u7c7b\u635f\u5931\u5b9a\u4e49\u80fd\u91cf\uff0c\u5e76\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u4e2a\u6709\u9650\u6e29\u5ea6\u53c2\u6570\u4f7f\u5f97\u5206\u7c7b\u76f8\u5bf9\u4e8e\u635f\u5931\uff08\u6216\u80fd\u91cf\uff09\u662f\u6700\u4f18\u7684\u3002", "result": "\u8bc1\u660e\u5e76\u786e\u8ba4\u4e86\u5bf9\u4e8e\u7531\u9ad8\u65af\u5206\u5e03\u751f\u6210\u6837\u672c\u548c\u4f7f\u7528\u6559\u5e08\u611f\u77e5\u673a\u6784\u5efa\u6807\u7b7e\u7684\u60c5\u51b5\uff0c\u6700\u4f18\u6e29\u5ea6\u65e2\u4e0d\u4f9d\u8d56\u4e8e\u6559\u5e08\u5206\u7c7b\u5668\u4e5f\u4e0d\u4f9d\u8d56\u4e8e\u968f\u673a\u5206\u7c7b\u5668\u7684\u6570\u91cf\u3002MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u8fd9\u79cd\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u3001\u65e0\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e2d\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u4e86\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u4e2d\u96c6\u4f53\u884c\u4e3a\u7684\u51fa\u73b0\u3002\u8bc1\u660e\u5e76\u786e\u8ba4\u4e86\u5bf9\u4e8e\u7531\u9ad8\u65af\u5206\u5e03\u751f\u6210\u6837\u672c\u548c\u4f7f\u7528\u6559\u5e08\u611f\u77e5\u673a\u6784\u5efa\u6807\u7b7e\u7684\u60c5\u51b5\uff0c\u6700\u4f18\u6e29\u5ea6\u65e2\u4e0d\u4f9d\u8d56\u4e8e\u6559\u5e08\u5206\u7c7b\u5668\u4e5f\u4e0d\u4f9d\u8d56\u4e8e\u968f\u673a\u5206\u7c7b\u5668\u7684\u6570\u91cf\uff0c\u8868\u660e\u4e86\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u5177\u6709\u666e\u904d\u6027\u3002MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u8fd9\u79cd\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u3001\u65e0\u566a\u58f0\u7684\u6570\u636e\u96c6\u4e2d\u7684\u76f8\u5173\u6027\u3002\u7269\u7406\u7c7b\u6bd4\u63ed\u793a\u4e86\u6240\u7814\u7a76\u73b0\u8c61\u7684\u81ea\u7ec4\u7ec7\u6027\u8d28\u3002"}}
{"id": "2505.10347", "pdf": "https://arxiv.org/pdf/2505.10347", "abs": "https://arxiv.org/abs/2505.10347", "authors": ["Gabriel S. Gama", "Valdir Grassi Jr"], "title": "Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The code will be made publicly available.", "AI": {"tldr": "Evaluate whether Specialized Multi-Task Optimizers (SMTOs) outperform equally weighted tasks in complex multi-task problems.", "motivation": "Address critiques suggesting SMTOs' superior performance is due to poor hyperparameter optimization and lack of regularization rather than inherent advantages.", "method": "Extensive empirical evaluation of SMTOs including latest methods on complex multi-task problems.", "result": "SMTOs perform well compared to uniform loss. Fixed weights can achieve competitive performance compared to SMTOs.", "conclusion": "Uniform loss can perform similarly to SMTOs in some instances."}}
{"id": "2505.10360", "pdf": "https://arxiv.org/pdf/2505.10360", "abs": "https://arxiv.org/abs/2505.10360", "authors": ["Victor Petr\u00e9n Bach Hansen", "Lasse Krogsb\u00f8ll", "Jonas Lyngs\u00f8", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maal\u00f8e"], "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": null, "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "AI": {"tldr": "AI-scribing solutions for healthcare need improvement. Introducing FactsR method for real-time clinical information extraction and recursive note generation.", "motivation": "Current AI-scribing solutions rely on one-shot or few-shot prompts without reasoning, leading to potential errors in generated notes which could compromise patient safety.", "method": "Introduce FactsR method for real-time clinical information extraction and use it recursively to generate final notes.", "result": "FactsR method generates more accurate and concise notes by involving clinicians in note generation process and opens up new possibilities for real-time decision support.", "conclusion": "The proposed method enhances the accuracy and reliability of AI-scribed notes in healthcare."}}
{"id": "2505.10192", "pdf": "https://arxiv.org/pdf/2505.10192", "abs": "https://arxiv.org/abs/2505.10192", "authors": ["Prashant P. Shinde", "Priyadarshini P. Pai", "Shashishekar P. Adiga", "K. Subramanya Mayya", "Yongbeom Seo", "Myungsoo Hwang", "Heeyoung Go", "Changmin Park"], "title": "Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models.", "AI": {"tldr": "This paper investigates the application of deep learning in EUV pattering defect detection using synthetically generated SEM images. It demonstrates that YOLOv8 performs best in detecting smaller defects.", "motivation": "The lack of defect-annotated quality data with good representation of smaller defects hinders the deployment of deep learning-based defect detection models in fabrication lines.", "method": "Artificially generating SEM images with known defect distributions and autonomously annotating them for training deep learning models.", "result": "YOLOv8 shows superior performance in defect detection with 96% mean average precision, outperforming EfficientNet and SSD. It also detects smaller defects more reliably and achieves 84.6% accuracy for Bridge defects and 78.3% for Break defects on real SEM data.", "conclusion": "Synthetic data can serve as an alternative to real-world data for developing robust machine-learning models in defect detection."}}
{"id": "2505.10392", "pdf": "https://arxiv.org/pdf/2505.10392", "abs": "https://arxiv.org/abs/2505.10392", "authors": ["Aryan Mishra", "Lizhen Lin"], "title": "Schreier-Coset Graph Propagation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure , preprint", "summary": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.", "AI": {"tldr": "This paper proposes Schrier-Coset Graph Propagation (SCGP), a novel approach to improve graph neural networks' long-range message passing without increasing memory usage, achieving better performance on several benchmarks.", "motivation": "To solve the over-squashing issue in GNNs where information from distant nodes is compressed into fixed-size vectors, while avoiding scalability issues introduced by other methods like Cayley and expander graphs.", "method": "Introduces Schrier-Coset Graph Propagation (SCGP), which uses Schreier-coset embeddings to enrich node features without changing the graph's original topology.", "result": "SCGP demonstrates competitive or superior performance compared to baselines such as expander graph and rewired GNNs, particularly excelling in handling hierarchical and modular graph structures with lower latency and memory usage.", "conclusion": "Schrier-Coset Graph Propagation (SCGP) offers a new way to enhance GNNs by using group-theoretic methods, showing promising results in various benchmarks."}}
{"id": "2505.10198", "pdf": "https://arxiv.org/pdf/2505.10198", "abs": "https://arxiv.org/abs/2505.10198", "authors": ["Mariano Ferrero", "Jos\u00e9 Omar Chelotti", "Luciano Sebasti\u00e1n Martinez-Rau", "Leandro Vignolo", "Mart\u00edn Pires", "Julio Ricardo Galli", "Leonardo Luis Giovanini", "Hugo Leonardo Rufiner"], "title": "A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals", "categories": ["cs.LG"], "comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence", "summary": "Monitoring feeding behaviour is a relevant task for efficient herd management\nand the effective use of available resources in grazing cattle. The ability to\nautomatically recognise animals' feeding activities through the identification\nof specific jaw movements allows for the improvement of diet formulation, as\nwell as early detection of metabolic problems and symptoms of animal\ndiscomfort, among other benefits. The use of sensors to obtain signals for such\nmonitoring has become popular in the last two decades. The most frequently\nemployed sensors include accelerometers, microphones, and cameras, each with\nits own set of advantages and drawbacks. An unexplored aspect is the\nsimultaneous use of multiple sensors with the aim of combining signals in order\nto enhance the precision of the estimations. In this direction, this work\nintroduces a deep neural network based on the fusion of acoustic and inertial\nsignals, composed of convolutional, recurrent, and dense layers. The main\nadvantage of this model is the combination of signals through the automatic\nextraction of features independently from each of them. The model has emerged\nfrom an exploration and comparison of different neural network architectures\nproposed in this work, which carry out information fusion at different levels.\nFeature-level fusion has outperformed data and decision-level fusion by at\nleast a 0.14 based on the F1-score metric. Moreover, a comparison with\nstate-of-the-art machine learning methods is presented, including traditional\nand deep learning approaches. The proposed model yielded an F1-score value of\n0.802, representing a 14% increase compared to previous methods. Finally,\nresults from an ablation study and post-training quantization evaluation are\nalso reported.", "AI": {"tldr": "A deep neural network that fuses acoustic and inertial signals is proposed for monitoring feeding behavior in grazing cattle, achieving better performance than previous methods.", "motivation": "Monitoring feeding behavior is crucial for efficient herd management and resource utilization in grazing cattle. Automatic recognition of feeding activities can improve diet formulation, detect metabolic problems and animal discomfort early.", "method": "Deep neural network based on the fusion of acoustic and inertial signals, composed of convolutional, recurrent, and dense layers.", "result": "The model achieved an F1-score value of 0.802, surpassing previous methods by 14%. Feature-level fusion has outperformed data and decision-level fusion.", "conclusion": "The proposed model, which combines acoustic and inertial signals using deep neural networks, has shown superior performance in monitoring feeding behavior of grazing cattle."}}
{"id": "2505.10213", "pdf": "https://arxiv.org/pdf/2505.10213", "abs": "https://arxiv.org/abs/2505.10213", "authors": ["Mohammadmahdi Ghasemloo", "Alireza Moradi"], "title": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.", "AI": {"tldr": "This paper proposes a new method to enhance LLMs' performance in time series forecasting by transferring structured temporal information, showing significant improvement over a naive baseline.", "motivation": "There is a growing need to establish best practices for leveraging the capabilities of LLMs beyond traditional natural language tasks.", "method": "A novel cross-domain knowledge transfer framework is proposed to enhance the performance of LLMs in time series forecasting.", "result": "Knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization.", "conclusion": "Results show that knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization."}}
{"id": "2505.10457", "pdf": "https://arxiv.org/pdf/2505.10457", "abs": "https://arxiv.org/abs/2505.10457", "authors": ["Matteo Gambella", "Vicente Javier Castro Solar", "Manuel Roveri"], "title": "SEAL: Searching Expandable Architectures for Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "8 pages, 5 figures", "summary": "Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.", "AI": {"tldr": "SEAL is a NAS-based framework for data-incremental learning that dynamically adapts model structure and maintains stability through selective expansion and cross-distillation training.", "motivation": "To address the challenge of balancing plasticity and stability in incremental learning, especially in resource-constrained environments.", "method": "A NAS-based framework that jointly searches for architecture and optimal expansion policy, with cross-distillation training after each expansion step.", "result": "SEAL shows effectiveness in reducing forgetting and enhancing accuracy compared to prior methods.", "conclusion": "SEAL reduces forgetting and enhances accuracy while maintaining a smaller model size."}}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222", "abs": "https://arxiv.org/abs/2505.10222", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.", "AI": {"tldr": "Introduce ComplexFormer, which improves Transformer's ability to integrate positional information with semantic data using Complex Multi-Head Attention (CMHA).", "motivation": "Address limitations in Transformers' handling of positional information while maintaining MHA flexibility.", "method": "Propose CMHA that models semantic and positional differences within the complex plane. Introduce per-head Euler transformation and adaptive differential rotation mechanism.", "result": "Outperforms strong baselines in language modeling, text/code generation, and math reasoning tasks with lower perplexity and better long-context coherence.", "conclusion": "ComplexFormer provides a more expressive and adaptable attention mechanism, demonstrating strong parameter efficiency."}}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465", "abs": "https://arxiv.org/abs/2505.10465", "authors": ["Yizhou liu", "Ziming Liu", "Jeff Gore"], "title": "Superposition Yields Robust Neural Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.", "AI": {"tldr": "Large language models (LLMs) perform better as they get larger, but why? This paper suggests that representation superposition is key.", "motivation": "To understand the origin of the neural scaling law, which states that loss decreases as a power law with model size.", "method": "Constructed a toy model based on two empirical principles: representations are superposed and words/concepts occur with varying frequencies.", "result": "Under weak superposition, loss scaling depends on feature frequency; under strong superposition, loss is inversely proportional to model dimension. Four families of open-sourced LLMs exhibit strong superposition and match the predictions of the toy model.", "conclusion": "Representation superposition is an important mechanism underlying the observed neural scaling laws."}}
{"id": "2505.10259", "pdf": "https://arxiv.org/pdf/2505.10259", "abs": "https://arxiv.org/abs/2505.10259", "authors": ["Xiangwen Zhuge", "Xu Shen", "Zeyu Wang", "Fan Dang", "Xuan Ding", "Danyang Li", "Yahui Han", "Tianxiang Hao", "Zheng Yang"], "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices", "categories": ["cs.LG"], "comment": null, "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .", "AI": {"tldr": "SpecOffload is a high-throughput inference engine that improves GPU core utilization and inference throughput by embedding speculative decoding into offloading.", "motivation": "Efficient LLM inference on resource-constrained devices presents significant challenges in compute and memory utilization.", "method": "SpecOffload embeds speculative decoding into offloading and unlocks latent GPU resources for storing and executing a draft model used for speculative decoding.", "result": "SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x.", "conclusion": "SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x."}}
{"id": "2505.10482", "pdf": "https://arxiv.org/pdf/2505.10482", "abs": "https://arxiv.org/abs/2505.10482", "authors": ["Ningyuan Yang", "Jiaxuan Gao", "Feng Gao", "Yi Wu", "Chao Yu"], "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures", "summary": "Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.", "AI": {"tldr": "This paper introduces NCDPO, a novel framework that reformulates diffusion policies as noise-conditioned deterministic policies, enabling more efficient training and better performance in continuous robot control and multi-agent game scenarios.", "motivation": "To address the limitations of sub-optimal and limited coverage of demonstration data in diffusion policies, especially in decision-making scenarios like robotics, gaming, and autonomous driving.", "method": "Reformulating diffusion policy as a noise-conditioned deterministic policy, allowing for tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps.", "result": "NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks.", "conclusion": "NCDPO is robust to the number of denoising timesteps in the diffusion policy and shows promise in improving the efficiency and effectiveness of diffusion policies in various applications."}}
{"id": "2505.10262", "pdf": "https://arxiv.org/pdf/2505.10262", "abs": "https://arxiv.org/abs/2505.10262", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Lajos Hanzo"], "title": "Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The charging scheduling problem of Electric Buses (EBs) is investigated based\non Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is\nconceived, where the time horizon includes multiple charging and operating\nperiods in a day, while each period is further divided into multiple time\nsteps. To overcome the challenge of long-range multi-phase planning with sparse\nreward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP\ninto a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical\nDouble Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is\nproposed for simultaneously solving the decision problems arising at different\ntemporal resolutions. As a result, the high-level agent learns an effective\npolicy for prescribing the charging targets for every charging period, while\nthe low-level agent learns an optimal policy for setting the charging power of\nevery time step within a single charging period, with the aim of minimizing the\ncharging costs while meeting the charging target. It is proved that the flat\npolicy constructed by superimposing the optimal high-level policy and the\noptimal low-level policy performs as well as the optimal policy of the original\nMDP. Since jointly learning both levels of policies is challenging due to the\nnon-stationarity of the high-level agent and the sampling inefficiency of the\nlow-level agent, we divide the joint learning process into two phases and\nexploit our new HER algorithm to manipulate the experience replay buffers for\nboth levels of agents. Numerical experiments are performed with the aid of\nreal-world data to evaluate the performance of the proposed algorithm.", "AI": {"tldr": "A study uses deep reinforcement learning to optimize electric bus charging schedules.", "motivation": "Optimizing electric bus charging schedules to reduce costs while ensuring they are charged adequately.", "method": "Proposes a hierarchical deep reinforcement learning approach to decouple the original MDP into high-level SMDP and low-level MDPs, then develops an HDDQN-HER algorithm.", "result": "Numerical experiments using real-world data show the effectiveness of the proposed algorithm.", "conclusion": "The hierarchical approach allows for effective and efficient optimization of electric bus charging schedules."}}
{"id": "2505.10515", "pdf": "https://arxiv.org/pdf/2505.10515", "abs": "https://arxiv.org/abs/2505.10515", "authors": ["Seongun Kim", "Sol A Kim", "Geonhyeong Kim", "Enver Menadjiev", "Chanwoo Lee", "Seongwook Chung", "Nari Kim", "Jaesik Choi"], "title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.", "AI": {"tldr": "This paper presents PnPXAI, a universal plug-and-play framework that supports diverse data modalities and neural network models in an automatic way.", "motivation": "Existing XAI frameworks have limitations such as inflexibility to various model architectures and data modalities, restricted supported XAI methods, and sub-optimal recommendations of explanations.", "method": "PnPXAI automatically identifies model architectures, recommends applicable explanation methods, and optimizes hyperparameters for the best explanations.", "result": "The authors validated the effectiveness of the PnPXAI framework through user surveys and demonstrated its adaptability across different fields like medicine and finance.", "conclusion": "The paper introduces PnPXAI, a universal XAI framework that can support different data types and neural network models flexibly."}}
{"id": "2505.10271", "pdf": "https://arxiv.org/pdf/2505.10271", "abs": "https://arxiv.org/abs/2505.10271", "authors": ["Rafael Pablos Sarabia", "Joachim Nyborg", "Morten Birk", "Jeppe Liborius Sj\u00f8rup", "Anders Lillevang Vesterholt", "Ira Assent"], "title": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.", "AI": {"tldr": "A deep learning model is developed for high-resolution precipitation forecasting over 8 hours in Europe, integrating multiple data sources effectively and providing accurate forecasts with uncertainty quantification.", "motivation": "Existing radar-only deep learning models have limitations with short forecast lead times.", "method": "The model combines radar, satellite, and physics-based numerical weather prediction data, capturing long-range interactions.", "result": "The model outperforms operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models.", "conclusion": "This model sets a new benchmark for high-resolution precipitation forecasting in Europe, balancing accuracy, interpretability, and computational efficiency."}}
{"id": "2505.10272", "pdf": "https://arxiv.org/pdf/2505.10272", "abs": "https://arxiv.org/abs/2505.10272", "authors": ["Niklas Dexheimer", "Sascha Gaudlitz", "Johannes Schmidt-Hieber"], "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Hebbian learning is a key principle underlying learning in biological neural\nnetworks. It postulates that synaptic changes occur locally, depending on the\nactivities of pre- and postsynaptic neurons. While Hebbian learning based on\nneuronal firing rates is well explored, much less is known about learning rules\nthat account for precise spike-timing. We relate a Hebbian\nspike-timing-dependent plasticity rule to noisy gradient descent with respect\nto a natural loss function on the probability simplex. This connection allows\nus to prove that the learning rule eventually identifies the presynaptic neuron\nwith the highest activity. We also discover an intrinsic connection to noisy\nmirror descent.", "AI": {"tldr": "This paper explores a Hebbian spike-timing-dependent plasticity rule and relates it to noisy gradient descent and noisy mirror descent, proving it can identify the most active presynaptic neuron.", "motivation": "To explore learning rules that account for precise spike-timing in Hebbian learning.", "method": "Relating the rule to noisy gradient descent and discovering its connection to noisy mirror descent.", "result": "The rule eventually identifies the presynaptic neuron with the highest activity.", "conclusion": "The study proves that the Hebbian spike-timing-dependent plasticity rule can identify the presynaptic neuron with the highest activity."}}
{"id": "2505.10296", "pdf": "https://arxiv.org/pdf/2505.10296", "abs": "https://arxiv.org/abs/2505.10296", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Dusit Niyato"], "title": "Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The growing adoption of Electric Buses (EBs) represents a significant step\ntoward sustainable development. By utilizing Internet of Things (IoT) systems,\ncharging stations can autonomously determine charging schedules based on\nreal-time data. However, optimizing EB charging schedules remains a critical\nchallenge due to uncertainties in travel time, energy consumption, and\nfluctuating electricity prices. Moreover, to address real-world complexities,\ncharging policies must make decisions efficiently across multiple time scales\nand remain scalable for large EB fleets. In this paper, we propose a\nHierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the\noriginal Markov Decision Process (MDP) into two augmented MDPs. To solve these\nMDPs and enable multi-timescale decision-making, we introduce a novel HDRL\nalgorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization\nEnhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic\n(DAC) algorithm for large-scale EB fleets are addressed through enhancements at\nboth decision levels. At the high level, we redesign the decentralized actor\nnetwork and integrate an attention mechanism to extract relevant global state\ninformation for each EB, decreasing the size of neural networks. At the low\nlevel, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is\nincorporated into the DAC framework, enabling decentralized and coordinated\ncharging power decisions, reducing computational complexity and enhancing\nconvergence speed. Extensive experiments with real-world data demonstrate the\nsuperior performance and scalability of DAC-MAPPO-E in optimizing EB fleet\ncharging schedules.", "AI": {"tldr": "This paper presents a Hierarchical Deep Reinforcement Learning (HDRL) approach called Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E) to optimize electric bus charging schedules by reformulating the original Markov Decision Process (MDP) into two augmented MDPs.", "motivation": "Optimizing electric bus charging schedules is challenging due to uncertainties in travel time, energy consumption, and fluctuating electricity prices, and current solutions need to handle multiple time scales and scale for large fleets.", "method": "The paper proposes a HDRL approach that introduces DAC-MAPPO-E algorithm which enhances the DAC algorithm for large-scale electric bus fleets by redesigning the decentralized actor network with attention mechanism and integrating MAPPO algorithm for decentralized and coordinated charging power decisions.", "result": "Extensive experiments using real-world data show that DAC-MAPPO-E outperforms existing methods in optimizing electric bus fleet charging schedules and demonstrates good scalability.", "conclusion": "The proposed HDRL approach effectively addresses the challenges in optimizing electric bus charging schedules under various uncertainties and can be applied to large-scale electric bus fleets."}}
{"id": "2505.10307", "pdf": "https://arxiv.org/pdf/2505.10307", "abs": "https://arxiv.org/abs/2505.10307", "authors": ["Yiyang Zhao", "Chengpei Wu", "Lilin Zhang", "Ning Yang"], "title": "Negative Metric Learning for Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Graph contrastive learning (GCL) often suffers from false negatives, which\ndegrades the performance on downstream tasks. The existing methods addressing\nthe false negative issue usually rely on human prior knowledge, still leading\nGCL to suboptimal results. In this paper, we propose a novel Negative Metric\nLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative\nMetric Network (NMN) to build a negative metric space, in which false negatives\ncan be distinguished better from true negatives based on their distance to\nanchor node. To overcome the lack of explicit supervision signals for NML, we\npropose a joint training scheme with bi-level optimization objective, which\nimplicitly utilizes the self-supervision signals to iteratively optimize the\nencoder and the negative metric network. The solid theoretical analysis and the\nextensive experiments conducted on widely used benchmarks verify the\nsuperiority of the proposed method.", "AI": {"tldr": "This paper introduces NML-GCL, a novel graph contrastive learning approach that uses a learnable NMN to enhance negative metric learning, improving performance by distinguishing false negatives more effectively. It also proposes a joint training scheme with bi-level optimization to utilize self-supervision signals.", "motivation": "Addressing the false negative issue in graph contrastive learning without relying on human prior knowledge.", "method": "Introducing NML-GCL with a learnable NMN to create a negative metric space and a joint training scheme with bi-level optimization.", "result": "The proposed method outperforms existing approaches on benchmark datasets, as verified by experiments and theoretical analysis.", "conclusion": "NML-GCL improves graph contrastive learning by effectively distinguishing false negatives through a learnable negative metric network and a joint training scheme."}}
{"id": "2505.10322", "pdf": "https://arxiv.org/pdf/2505.10322", "abs": "https://arxiv.org/abs/2505.10322", "authors": ["Yijie Zhou", "Shi Pu"], "title": "Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks.", "AI": {"tldr": "This paper proposes ADSGD, which improves convergence in decentralized optimization with practical constraints.", "motivation": "To address the challenges of heterogeneous computation speeds and unpredictable communication delays in decentralized optimization.", "method": "Asynchronous Decentralized Stochastic Gradient Descent (ADSGD)", "result": "ADSGD converges under computation-delay-independent step sizes and performs better than existing methods in wall-clock convergence time.", "conclusion": "ADSGD is well-suited for real-world decentralized learning tasks."}}
{"id": "2505.10325", "pdf": "https://arxiv.org/pdf/2505.10325", "abs": "https://arxiv.org/abs/2505.10325", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aALERT\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u68c0\u6d4b\u7279\u5f81\u5206\u5e03\u53d8\u5316\u5e76\u89e6\u53d1\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u65e0\u7ebf\u6307\u7eb9\u8bc6\u522b\u548c\u94fe\u8def\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\uff0c\u7279\u5f81\u5206\u5e03\u7684\u53d8\u5316\u53ef\u80fd\u4f1a\u964d\u4f4eAI\u6a21\u578b\u7684\u6027\u80fd\u5e76\u5bfc\u81f4\u4e0d\u826f\u884c\u4e3a\u3002\u4e3a\u4e86\u5e94\u5bf9\u672a\u68c0\u6d4b\u5230\u7684\u6a21\u578b\u9000\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ALERT\u65b9\u6cd5\u3002", "method": "ALERT\u5305\u62ec\u8868\u793a\u5b66\u4e60\u3001\u7edf\u8ba1\u6d4b\u8bd5\u548c\u6548\u7528\u8bc4\u4f30\u4e09\u4e2a\u7ec4\u4ef6\u3002\u6211\u4eec\u4f9d\u8d56MLP\u8fdb\u884c\u8868\u793a\u5b66\u4e60\u7ec4\u4ef6\u7684\u8bbe\u8ba1\uff0cKolmogorov-Smirnov\u548cPopulation Stability Index\u6d4b\u8bd5\u7528\u4e8e\u8bbe\u8ba1\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u51fd\u6570\u7528\u4e8e\u6548\u7528\u8bc4\u4f30\u3002", "result": "ALERT\u65b9\u6cd5\u5728\u4e24\u4e2a\u65e0\u7ebf\u7f51\u7edc\u4f7f\u7528\u6848\u4f8b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ALERT\u65b9\u6cd5\u5728\u4e24\u4e2a\u65e0\u7ebf\u7f51\u7edc\u4f7f\u7528\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u5341\u79cd\u6807\u51c6\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2505.10344", "pdf": "https://arxiv.org/pdf/2505.10344", "abs": "https://arxiv.org/abs/2505.10344", "authors": ["Alan Jeffares", "Liyuan Liu"], "title": "An Introduction to Discrete Variational Autoencoders", "categories": ["cs.LG"], "comment": "Tutorial paper", "summary": "Variational Autoencoders (VAEs) are well-established as a principled approach\nto probabilistic unsupervised learning with neural networks. Typically, an\nencoder network defines the parameters of a Gaussian distributed latent space\nfrom which we can sample and pass realizations to a decoder network. This model\nis trained to reconstruct its inputs and is optimized through the evidence\nlower bound. In recent years, discrete latent spaces have grown in popularity,\nsuggesting that they may be a natural choice for many data modalities (e.g.\ntext). In this tutorial, we provide a rigorous, yet practical, introduction to\ndiscrete variational autoencoders -- specifically, VAEs in which the latent\nspace is made up of latent variables that follow a categorical distribution. We\nassume only a basic mathematical background with which we carefully derive each\nstep from first principles. From there, we develop a concrete training recipe\nand provide an example implementation, hosted at\nhttps://github.com/alanjeffares/discreteVAE.", "AI": {"tldr": "This tutorial introduces discrete variational autoencoders (VAEs) with a rigorous derivation from first principles, providing a practical training recipe and an example implementation.", "motivation": "Discrete latent spaces are becoming popular due to their potential suitability for many data modalities like text.", "method": "A rigorous derivation of discrete VAEs where the latent space consists of categorical distributed latent variables.", "result": "Provides a concrete training recipe and an example implementation of discrete VAEs.", "conclusion": "Discrete VAEs offer a principled approach to probabilistic unsupervised learning with neural networks, especially suitable for data modalities like text."}}
{"id": "2505.10407", "pdf": "https://arxiv.org/pdf/2505.10407", "abs": "https://arxiv.org/abs/2505.10407", "authors": ["Wenhao Ding", "Choon Hwai Yap", "Kangjun Ji", "Sim\u00e3o Castro"], "title": "Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning", "categories": ["cs.LG", "68T07"], "comment": "10 pages, 2 figures", "summary": "A generative model for the mesh geometry of intracranial aneurysms (IA) is\ncrucial for training networks to predict blood flow forces in real time, which\nis a key factor affecting disease progression. This need is necessitated by the\nabsence of a large IA image datasets. Existing shape generation methods\nstruggle to capture realistic IA features and ignore the relationship between\nIA pouches and parent vessels, limiting physiological realism and their\ngeneration cannot be controlled to have specific morphological measurements. We\npropose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh\ngenerator. In the first stage, AneuG generates low-dimensional Graph Harmonic\nDeformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,\nconstrained to morphing energy statistics truths. GHD enables more accurate\nshape encoding than alternatives. In the second stage, AneuG generates parent\nvessels conditioned on GHD tokens, by generating vascular centreline and\npropagating the cross-section. AneuG's IA shape generation can further be\nconditioned to have specific clinically relevant morphological measurements.\nThis is useful for studies to understand shape variations represented by\nclinical measurements, and for flow simulation studies to understand effects of\nspecific clinical shape parameters on fluid dynamics. Source code and\nimplementation details are available at\nhttps://github.com/anonymousaneug/AneuG.", "AI": {"tldr": "AneuG, a two-stage VAE-based IA mesh generator, improves the accuracy of intracranial aneurysm pouch shape encoding using GHD tokens and enables the generation of parent vessels conditioned on these tokens.", "motivation": "To address the lack of large IA image datasets and improve the physiological realism of generated aneurysm shapes with controllable morphological measurements for disease progression prediction.", "method": "Proposes AneuG with two stages: 1) Generating GHD tokens for encoding and reconstructing aneurysm pouch shapes constrained by morphing energy statistics truths; 2) Generating parent vessels based on GHD tokens by creating vascular centerlines and propagating cross-sections.", "result": "AneuG can generate realistic IA shapes with specific morphological measurements, useful for understanding shape variations and flow simulations related to clinical measurements.", "conclusion": "AneuG improves the physiological realism and controllability of IA shape generation, which is crucial for training networks to predict blood flow forces in real-time."}}
{"id": "2505.10422", "pdf": "https://arxiv.org/pdf/2505.10422", "abs": "https://arxiv.org/abs/2505.10422", "authors": ["Daniel Weitekamp", "Christopher MacLellan", "Erik Harpstead", "Kenneth Koedinger"], "title": "Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency", "categories": ["cs.LG"], "comment": "To appear in CogSci 2025", "summary": "Human learning relies on specialization -- distinct cognitive mechanisms\nworking together to enable rapid learning. In contrast, most modern neural\nnetworks rely on a single mechanism: gradient descent over an objective\nfunction. This raises the question: might human learners' relatively rapid\nlearning from just tens of examples instead of tens of thousands in data-driven\ndeep learning arise from our ability to use multiple specialized mechanisms of\nlearning in combination? We investigate this question through an ablation\nanalysis of inductive human learning simulations in online tutoring\nenvironments. Comparing reinforcement learning to a more data-efficient\n3-mechanism symbolic rule induction approach, we find that decomposing learning\ninto multiple distinct mechanisms significantly improves data efficiency,\nbringing it in line with human learning. Furthermore, we show that this\ndecomposition has a greater impact on efficiency than the distinction between\nsymbolic and subsymbolic learning alone. Efforts to align data-driven machine\nlearning with human learning often overlook the stark difference in learning\nefficiency. Our findings suggest that integrating multiple specialized learning\nmechanisms may be key to bridging this gap.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5206\u89e3\u5b66\u4e60\u4e3a\u591a\u4e2a\u673a\u5236\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u673a\u5668\u5b66\u4e60\u548c\u4eba\u7c7b\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5982\u4f55\u80fd\u591f\u4ec5\u4ece\u51e0\u5341\u4e2a\u4f8b\u5b50\u5c31\u5feb\u901f\u5b66\u4e60\uff0c\u800c\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u6210\u5343\u4e0a\u4e07\u7684\u4f8b\u5b50\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u8f85\u5bfc\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u5f0f\u4eba\u7c7b\u5b66\u4e60\u6a21\u62df\u7684\u6d88\u878d\u5206\u6790\uff0c\u6bd4\u8f83\u5f3a\u5316\u5b66\u4e60\u4e0e\u66f4\u9ad8\u6548\u7684\u6570\u636e\u7b26\u53f7\u89c4\u5219\u5f52\u7eb3\u65b9\u6cd5\u3002", "result": "\u5206\u89e3\u5b66\u4e60\u4e3a\u591a\u4e2a\u4e0d\u540c\u7684\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u4f7f\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u66f4\u52a0\u63a5\u8fd1\u4eba\u7c7b\u5b66\u4e60\u3002", "conclusion": "\u96c6\u6210\u591a\u79cd\u4e13\u95e8\u7684\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u662f\u5f25\u5408\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u4e0e\u4eba\u7c7b\u5b66\u4e60\u4e4b\u95f4\u5dee\u8ddd\u7684\u5173\u952e\u3002"}}
{"id": "2505.10423", "pdf": "https://arxiv.org/pdf/2505.10423", "abs": "https://arxiv.org/abs/2505.10423", "authors": ["Ari Karchmer", "Eran Malach"], "title": "The Power of Random Features and the Limits of Distribution-Free Gradient Descent", "categories": ["cs.LG"], "comment": null, "summary": "We study the relationship between gradient-based optimization of parametric\nmodels (e.g., neural networks) and optimization of linear combinations of\nrandom features. Our main result shows that if a parametric model can be\nlearned using mini-batch stochastic gradient descent (bSGD) without making\nassumptions about the data distribution, then with high probability, the target\nfunction can also be approximated using a polynomial-sized combination of\nrandom features. The size of this combination depends on the number of gradient\nsteps and numerical precision used in the bSGD process. This finding reveals\nfundamental limitations of distribution-free learning in neural networks\ntrained by gradient descent, highlighting why making assumptions about data\ndistributions is often crucial in practice. Along the way, we also introduce a\nnew theoretical framework called average probabilistic dimension complexity\n(adc), which extends the probabilistic dimension complexity developed by Kamath\net al. (2020). We prove that adc has a polynomial relationship with statistical\nquery dimension, and use this relationship to demonstrate an infinite\nseparation between adc and standard dimension complexity.", "AI": {"tldr": "This paper explores the connection between gradient-based optimization in parametric models and optimization of random features. It demonstrates that under certain conditions, the target function can be approximated using a polynomial-sized combination of random features. The findings highlight the limitations of distribution-free learning in neural networks trained by gradient descent.", "motivation": "To understand the fundamental limitations of distribution-free learning in neural networks trained by gradient descent.", "method": "Studying the relationship between gradient-based optimization and random features optimization, introducing a new theoretical framework called average probabilistic dimension complexity (adc).", "result": "If a parametric model can be learned using mini-batch stochastic gradient descent without assumptions about the data distribution, then the target function can be approximated using a polynomial-sized combination of random features. This highlights the importance of making assumptions about data distributions in practical applications.", "conclusion": "The study reveals the limitations of distribution-free learning in neural networks trained by gradient descent and introduces a new theoretical framework, average probabilistic dimension complexity (adc), which demonstrates an infinite separation between adc and standard dimension complexity."}}
{"id": "2505.10425", "pdf": "https://arxiv.org/pdf/2505.10425", "abs": "https://arxiv.org/abs/2505.10425", "authors": ["Jingyao Wang", "Wenwen Qiang", "Zeen Song", "Changwen Zheng", "Hui Xiong"], "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.", "AI": {"tldr": "This paper presents Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework designed to enhance large language models' reasoning effectiveness while reducing computational costs by optimizing token usage.", "motivation": "Existing methods lack balance between reasoning effectiveness and computational efficiency, often resulting in unnecessarily long reasoning chains and wasted tokens.", "method": "L2T introduces a universal dense process reward based on quantified parameter information gain without additional annotations or task-specific evaluators. It also includes a fast estimation method using PAC-Bayes bounds and Fisher information matrix.", "result": "The approach demonstrates improved reasoning effectiveness and efficiency across various benchmarks and base models.", "conclusion": "Learning to Think (L2T) effectively optimizes large language models' reasoning processes with fewer tokens, enhancing both performance and computational efficiency."}}
{"id": "2505.10432", "pdf": "https://arxiv.org/pdf/2505.10432", "abs": "https://arxiv.org/abs/2505.10432", "authors": ["Randy J. Chase", "Katherine Haynes", "Lander Ver Hoef", "Imme Ebert-Uphoff"], "title": "Score-based diffusion nowcasting of GOES imagery", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.", "AI": {"tldr": "A new method called score-based diffusion is used to nowcast (forecast for zero to three hours) clouds and precipitation using geostationary infrared imagery. Three types of diffusion models are experimented with, showing ability to advect, generate, and decay clouds. Results show the CorrDiff approach outperforms other models and traditional methods.", "motivation": "Traditional methods for simulating clouds and precipitation are challenging due to sub-grid parameterizations. Early machine learning methods produced blurry forecasts.", "method": "Exploring score-based diffusion models including Diff, CorrDiff, and LDM for nowcasting clouds and precipitation using geostationary infrared imagery.", "result": "Diffusion models can advect, generate, and decay clouds, including convective initiation. High-resolution features are preserved longer than with a conventional U-Net. CorrDiff approach outperforms other models and traditional methods.", "conclusion": "Score-based diffusion models show promise for nowcasting clouds and precipitation, with CorrDiff being the most effective model."}}
{"id": "2505.10438", "pdf": "https://arxiv.org/pdf/2505.10438", "abs": "https://arxiv.org/abs/2505.10438", "authors": ["David Grasev"], "title": "Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "51 pages, 28 figures", "summary": "Gas turbine engines represent complex highly nonlinear dynamical systems.\nDeriving their physics-based models can be challenging as it requires\nperformance characteristics, that are not always available, and one often has\nto make many simplifying assumptions. In this paper, the limitations of\nconventional experimental methods used to derive component-level and locally\nlinear parameter-varying models are discussed and addressed by employing\nidentification techniques based on data collected from standard engine\noperation under closed-loop control. The rotor dynamics were estimated using\nthe sparse identification of nonlinear dynamics. Subsequently, the autonomous\npart of the dynamics was mapped into an optimally constructed Koopman\neigenfunction space. The process included eigenvalue optimization using\nmetaheuristic algorithms and temporal projection, followed by gradient-based\neigenfunction identification. The resulting Koopman model was validated against\nan in-house reference component-level model. A globally optimal nonlinear\nfeedback controller and a Kalman estimator were then designed in the\neigenfunction space and compared to the classical and gain-scheduled\nproportional-integral controllers, as well as a proposed internal model control\napproach. The eigenmode structure allowed targeting individual modes during the\noptimization process, resulting in a better performance tuning. The results\nshowed that the Koopman-based controller outperformed the other benchmark\ncontrollers in both reference tracking and disturbance rejection, under\nsea-level and varying flight conditions, due to its global nature.", "AI": {"tldr": "This paper discusses the limitations of conventional experimental methods for deriving gas turbine models and proposes data-driven approaches to overcome these limitations. It introduces the use of sparse identification and Koopman eigenfunctions to create a globally optimal nonlinear feedback controller which performs better than traditional controllers.", "motivation": "To develop more accurate and efficient models and controllers for gas turbine engines by overcoming the limitations of conventional methods.", "method": "Employing data-driven identification techniques like sparse identification of nonlinear dynamics and mapping the system's dynamics into an optimally constructed Koopman eigenfunction space.", "result": "The proposed Koopman-based controller outperforms classical and gain-scheduled proportional-integral controllers as well as an internal model control approach in reference tracking and disturbance rejection under various conditions.", "conclusion": "Data-driven approaches can effectively improve the modeling and control of complex nonlinear systems like gas turbines."}}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472", "abs": "https://arxiv.org/abs/2505.10472", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "AI": {"tldr": "This study examines the effectiveness of various large language models (LLMs) in creating accurate and accessible information on breast and cervical cancer, finding that general-purpose LLMs have better linguistic quality and affectiveness, but medical LLMs offer greater communication accessibility despite potential harm and bias.", "motivation": "To evaluate the capabilities and limitations of LLMs in providing clear and safe cancer-related information to improve public understanding and address health communication challenges.", "method": "Mixed-methods evaluation of five general-purpose and three medical LLMs using quantitative metrics, expert ratings, and statistical analyses.", "result": "General-purpose LLMs perform better in linguistic quality and affectiveness, whereas medical LLMs excel in communication accessibility but face issues related to safety and trustworthiness due to higher toxicity, harm, and bias.", "conclusion": "There is a trade-off between domain-specific knowledge and safety in health communications using LLMs. Future models should focus on mitigating harm and bias while enhancing safety and affectiveness."}}
{"id": "2505.10475", "pdf": "https://arxiv.org/pdf/2505.10475", "abs": "https://arxiv.org/abs/2505.10475", "authors": ["Mouxiang Chen", "Binyuan Hui", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Jianling Sun", "Junyang Lin", "Zhongxin Liu"], "title": "Parallel Scaling Law for Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u6269\u5c55\u65b9\u6cd5(ParScale)\uff0c\u53ef\u4ee5\u5728\u4e0d\u663e\u8457\u589e\u52a0\u5185\u5b58\u548c\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u5730\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u663e\u8457\u7684\u7a7a\u95f4\u6216\u65f6\u95f4\u6210\u672c\uff0c\u901a\u8fc7\u589e\u52a0\u53c2\u6570\u6216\u8f93\u51fa\u4ee4\u724c\u6765\u5b9e\u73b0\u6269\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8f83\u5927\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u6269\u5c55(ParScale)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u8f93\u5165\u5e94\u7528P\u4e2a\u4e0d\u540c\u7684\u53ef\u5b66\u4e60\u53d8\u6362\uff0c\u5728\u5e76\u884c\u6267\u884c\u6a21\u578b\u7684\u524d\u5411\u4f20\u9012\u540e\u52a8\u6001\u805a\u5408P\u4e2a\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528ParScale\u65b9\u6cd5\u7684\u6a21\u578b\u5728\u8fbe\u5230\u76f8\u540c\u6027\u80fd\u63d0\u5347\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u8fbe22\u500d\u5c11\u7684\u5185\u5b58\u589e\u52a0\u548c6\u500d\u5c11\u7684\u5ef6\u8fdf\u589e\u52a0\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u7684\u540e\u8bad\u7ec3\u5c06\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8f6c\u6362\u4e3a\u5e76\u884c\u6269\u5c55\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u8bad\u7ec3\u9884\u7b97\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u6269\u5c55(ParScale)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u589e\u52a0\u6a21\u578b\u7684\u5e76\u884c\u8ba1\u7b97\u6765\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u3002\u7406\u8bba\u4e0a\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6269\u5c55\u5b9a\u5f8b\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u5185\u5b58\u548c\u5ef6\u8fdf\u7684\u589e\u52a0\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u7684\u540e\u8bad\u7ec3\u5c06\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8f6c\u6362\u4e3a\u5e76\u884c\u6269\u5c55\u6a21\u578b\u3002\u8fd9\u9879\u7814\u7a76\u53ef\u80fd\u6709\u52a9\u4e8e\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u60c5\u51b5\u4e0b\u90e8\u7f72\u66f4\u5f3a\u5927\u7684\u6a21\u578b\uff0c\u5e76\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u4f5c\u7528\u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u89c6\u89d2\u3002"}}
{"id": "2505.10484", "pdf": "https://arxiv.org/pdf/2505.10484", "abs": "https://arxiv.org/abs/2505.10484", "authors": ["Andrea Baisero", "Rupali Bhati", "Shuo Liu", "Aathira Pillai", "Christopher Amato"], "title": "Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Value function decomposition methods for cooperative multi-agent\nreinforcement learning compose joint values from individual per-agent\nutilities, and train them using a joint objective. To ensure that the action\nselection process between individual utilities and joint values remains\nconsistent, it is imperative for the composition to satisfy the\nindividual-global max (IGM) property. Although satisfying IGM itself is\nstraightforward, most existing methods (e.g., VDN, QMIX) have limited\nrepresentation capabilities and are unable to represent the full class of IGM\nvalues, and the one exception that has no such limitation (QPLEX) is\nunnecessarily complex. In this work, we present a simple formulation of the\nfull class of IGM values that naturally leads to the derivation of QFIX, a\nnovel family of value function decomposition models that expand the\nrepresentation capabilities of prior models by means of a thin \"fixing\" layer.\nWe derive multiple variants of QFIX, and implement three variants in two\nwell-known multi-agent frameworks. We perform an empirical evaluation on\nmultiple SMACv2 and Overcooked environments, which confirms that QFIX (i)\nsucceeds in enhancing the performance of prior methods, (ii) learns more stably\nand performs better than its main competitor QPLEX, and (iii) achieves this\nwhile employing the simplest and smallest mixing models.", "AI": {"tldr": "This paper introduces QFIX, a simpler and more effective method for value function decomposition in cooperative multi-agent reinforcement learning, outperforming previous methods including QPLEX.", "motivation": "Existing value function decomposition methods have limited representation capabilities or are unnecessarily complex.", "method": "A novel family of value function decomposition models named QFIX is derived from a simple formulation of the full class of IGM values.", "result": "QFIX improves the performance of prior methods, learns more stably, and performs better than QPLEX in multiple SMACv2 and Overcooked environments.", "conclusion": "QFIX successfully enhances the performance of prior methods, outperforms QPLEX, and achieves this with the simplest and smallest mixing models."}}
{"id": "2505.10495", "pdf": "https://arxiv.org/pdf/2505.10495", "abs": "https://arxiv.org/abs/2505.10495", "authors": ["Vibha Belavadi", "Tushar Vatsa", "Dewang Sultania", "Suhas Suresha", "Ishita Verma", "Cheng Chen", "Tracy Holloway King", "Michael Friedrich"], "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing", "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.", "AI": {"tldr": "This paper presents a novel router-based architecture for generating high-quality synthetic training data to fine-tune large language models for function-calling tasks without real user interaction data.", "motivation": "To address the lack of real-world task-specific data and privacy constraints for training on it, which are common in digital content creation tools.", "method": "A router-based architecture that uses domain resources like content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models.", "result": "The architecture's flexible routing mechanism generates synthetic data that matches real-world distributions, improving function classification accuracy and API parameter selection.", "conclusion": "Models fine-tuned with the synthetic data generated by this architecture outperform traditional approaches, setting new benchmarks for function calling tasks."}}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526", "abs": "https://arxiv.org/abs/2505.10526", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.", "AI": {"tldr": "Speculative decoding is a technique used to accelerate language model inference by using a lightweight draft model to predict multiple tokens for a larger target model to verify. This study introduces MASSV, a method that adapts small language models into effective multimodal drafters for vision-language models by connecting the target VLM's vision encoder to the draft model and applying self-distilled visual instruction tuning. Experiments show that MASSV can increase accepted length by up to 30% and deliver end-to-end inference speedups of up to 1.46x on visually-grounded tasks.", "motivation": "Speculative decoding can significantly accelerate language model inference but faces challenges when applied to vision-language models due to the lack of architectural components in small language models to process visual inputs and their token predictions failing to match those of VLM target models.", "method": "MASSV uses a two-phase approach to adapt small language models into effective multimodal drafters. It first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions.", "result": "MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks.", "conclusion": "MASSV provides a scalable, architecture-compatible method for accelerating both current and future VLMs."}}
{"id": "2505.10545", "pdf": "https://arxiv.org/pdf/2505.10545", "abs": "https://arxiv.org/abs/2505.10545", "authors": ["Amira Alakhdar", "Barnabas Poczos", "Newell Washburn"], "title": "Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design", "categories": ["cs.LG"], "comment": null, "summary": "Developing bioactive molecules remains a central, time- and cost-heavy\nchallenge in drug discovery, particularly for novel targets lacking structural\nor functional data. Pharmacophore modeling presents an alternative for\ncapturing the key features required for molecular bioactivity against a\nbiological target. In this work, we present PharmaDiff, a\npharmacophore-conditioned diffusion model for 3D molecular generation.\nPharmaDiff employs a transformer-based architecture to integrate an atom-based\nrepresentation of the 3D pharmacophore into the generative process, enabling\nthe precise generation of 3D molecular graphs that align with predefined\npharmacophore hypotheses. Through comprehensive testing, PharmaDiff\ndemonstrates superior performance in matching 3D pharmacophore constraints\ncompared to ligand-based drug design methods. Additionally, it achieves higher\ndocking scores across a range of proteins in structure-based drug design,\nwithout the need for target protein structures. By integrating pharmacophore\nmodeling with 3D generative techniques, PharmaDiff offers a powerful and\nflexible framework for rational drug design.", "AI": {"tldr": "This paper introduces PharmaDiff, a pharmacophore-conditioned diffusion model for 3D molecular generation that outperforms ligand-based drug design methods and achieves high docking scores without requiring target protein structures.", "motivation": "To develop a more efficient method for generating bioactive molecules, especially for novel targets lacking structural or functional data.", "method": "PharmaDiff uses a transformer-based architecture to incorporate an atom-based representation of the 3D pharmacophore into the generative process, allowing for precise generation of 3D molecular graphs aligned with predefined pharmacophore hypotheses.", "result": "PharmaDiff demonstrates better performance in matching 3D pharmacophore constraints and achieves higher docking scores across various proteins compared to ligand-based drug design methods.", "conclusion": "The integration of pharmacophore modeling with 3D generative techniques provides a powerful and flexible framework for rational drug design."}}
{"id": "2505.10556", "pdf": "https://arxiv.org/pdf/2505.10556", "abs": "https://arxiv.org/abs/2505.10556", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "categories": ["cs.LG", "physics.ao-ph"], "comment": "Kermani and Naderi share first authorship. 20 pages, 6 figures and 1\n  table", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "AI": {"tldr": "The paper presents a new workflow using AI to predict personalized health responses to air pollution based on data from wearable devices and environmental exposures.", "motivation": "To address the health risks posed by air pollution and extreme weather events exacerbated by climate change, utilizing advancements in personal sensing and AI for better healthcare.", "method": "Integrating physiological data from wearable fitness devices with real-time environmental exposures using an Adversarial Autoencoder neural network within a cloud-based, modular framework.", "result": "The AI model accurately reconstructs health signals and captures nonlinear responses to pollution, with transfer learning improving its generalization ability using user-generated data from a smartwatch.", "conclusion": "This novel workflow has the potential to improve monitoring and prediction of individual health responses to pollution, capitalizing on personal sensing and AI advancements."}}
