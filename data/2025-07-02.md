<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 99]
- [cs.AI](#cs.AI) [总数: 19]
- [stat.ML](#stat.ML) [总数: 5]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

**主要类别:** cs.LG

**AI概要:** Large Language Models (LLMs) face precision loss, redefined as information spreading. To tackle this issue, HDRAM (Holographically Defined Random Access Memory) is introduced, which treats transformer latent space as a spread-spectrum channel and uses hypertokens integrating error-correcting codes, holographic computing, and quantum-inspired search to recover distributed information efficiently.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of this paper is to address the apparent precision loss in Large Language Models (LLMs), reframing it as an information spreading problem rather than computational precision loss.

**方法:** HDRAM, built upon hypertokens, structured symbolic codes that incorporate classical error-correcting codes (ECC), holographic computing, and quantum-inspired search, treats the transformer latent space as a spread-spectrum channel. It enables efficient key-value operations and Grover-style search in latent space through phase-coherent memory addresses.

**结果:** HDRAM significantly improves associative retrieval without requiring architectural changes, demonstrating the effectiveness of Classical-Holographic-Quantum-inspired (CHQ) principles.

**结论:** By applying HDRAM, the paper shows how CHQ principles can fortify transformer architectures against information spreading issues.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hypertokens%3A+Holographic+Associative+Memory+in+Tokenized+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00002，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00002&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [2] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为NeutroSENSE的中智增强集成框架，用于物联网环境中的可解释入侵检测。通过整合随机森林、XGBoost和逻辑回归与中智逻辑，系统将预测置信度分解为真（T）、假（F）和不确定性（I）三个部分，实现了不确定性量化和弃权机制。评估结果显示，NeutroSENSE在IoT-CAD数据集上达到了97%的准确率，并表明错误分类样本的不确定性显著高于正确分类样本。使用不确定性作为不确定性的代理可以实现明智的弃权和有针对性的审查，这对于边缘部署特别有价值。本研究展示了中智逻辑如何提高准确性和可解释性，为边缘和雾基物联网安全系统中的信任感知AI提供了实用基础。


<details>
  <summary>更多</summary>
  
**动机:** 当前物联网环境中，入侵检测系统的准确性和可解释性存在挑战，尤其是在处理不确定性时。传统方法难以有效应对复杂环境中的不确定性问题，需要一种新的框架来提升系统的性能和可信度。

**方法:** NeutroSENSE框架结合了随机森林、XGBoost和逻辑回归模型，并引入中智逻辑，将预测结果分为真（T）、假（F）和不确定性（I）三个部分。通过全局和自适应类特定阈值标记高不确定性的预测以供审查。该方法利用不确定性分数作为代理来衡量模型的不确定性，并支持人类参与的决策过程。

**结果:** NeutroSENSE在IoT-CAD数据集上的准确率达到97%，并且发现错误分类样本的不确定性（I = 0.62）明显高于正确分类样本（I = 0.24）。此外，实验验证了I分数与错误概率之间的相关性，证明了该方法在信任感知AI中的有效性。

**结论:** 中智逻辑增强了入侵检测系统的准确性和可解释性，为边缘和雾基物联网安全系统中的信任感知AI提供了实际可行的基础。这种方法不仅提高了系统的性能，还为用户提供了一个更值得信赖的决策支持工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deciding+When+Not+to+Decide%3A+Indeterminacy-Aware+Intrusion+Detection+with+NeutroSENSE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00003&send_immediately=true&force_search=false)

**原文摘要:** This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [3] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr, Anuj K. Nayak, Lav R. Varshney*

**主要类别:** cs.LG

**AI概要:** 大型语言模型（LLMs）在训练和推理过程中需要大量的计算、能源和资金资源。本文提出了一个名为定向随机技能搜索（DS3）的通用框架，将推理表示为在学习到的技能图上的随机遍历。该框架推导出任务成功和计算成本的闭式表达式，并涵盖多种推理策略，如思维链（CoT）和思维树（ToT）。通过将推理纳入先前的第一性原理三方图框架并结合经验方法，文章揭示了任务难度和模型能力对推理策略的影响，并解释了诸如线性准确率随对数计算量增长等现象。此外，框架还统一了最佳N选择（BoN）和多数投票行为，并明确刻画了训练与推理之间的相互依赖关系，从而深化理论理解并支持算法设计和资源分配。


<details>
  <summary>更多</summary>
  
**动机:** 尽管扩展定律在训练方面指导了许多领域的近期进展，但推理成本现在已经成为整体资源负担的重要且不断增长的部分，特别是对于专注于推理的模型。现有的计算最优性表征可能忽略了更有效的操作点，因此需要一种新的框架来更好地理解和优化推理过程。

**方法:** 提出了一种称为定向随机技能搜索（DS3）的通用框架，将推理视为在学习到的技能图上的随机遍历。从简化的表达形式中，推导出任务成功和计算成本的闭式表达式，涵盖了各种推理策略。同时，将DS3框架与经验方法结合，以描述LLM的扩展行为，并扩展了先前关于LLM训练的三方图框架以包含推理。

**结果:** 理论上恢复了观察到的现象，包括：随着对数计算量的增加，准确率线性增长；根据任务难度和模型能力，首选的推理策略有所变化；即使在参数扩展下性能达到平台期，推理仍能引发新兴行为；最佳N选择（BoN）和多数投票行为被统一在一个分析框架内。

**结论:** 通过明确刻画训练与推理之间的相互依赖关系，所提出的框架深化了对LLM的理论理解，并支持有原则的算法设计和资源分配。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Theory+of+Inference+Compute+Scaling%3A+Reasoning+through+Directed+Stochastic+Skill+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00004&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [4] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes, Vincent Francois-Lavet*

**主要类别:** cs.LG

**AI概要:** 通过将Vrije Universiteit Amsterdam的六电梯十五楼层系统建模为马尔可夫决策过程，训练端到端强化学习(EGCS)系统，提出新的动作空间编码、连续乘客到达建模和奖励信号设计方法，并探索折扣因子适应方法，最终表明RL-EGCS优于传统基于规则算法。


<details>
  <summary>更多</summary>
  
**动机:** 大型建筑中有效的电梯交通管理对减少乘客旅行时间和能源消耗至关重要，但传统的启发式或模式检测控制器难以应对派遣任务的随机性和组合性质。

**方法:** 将电梯系统建模为马尔可夫决策过程，使用强化学习（特别是Dueling Double Deep Q-learning架构）进行端到端训练，并采用以下关键创新：新型动作空间编码、引入infra-steps以模拟连续乘客到达、定制化奖励信号以提高学习效率以及探索不同的折扣因子适应方法。

**结果:** 所提出的基于强化学习的EGCS能够适应波动的交通模式，从高度随机的环境中学习，并且性能优于传统的基于规则的算法。

**结论:** 基于强化学习的电梯群控系统在处理复杂的电梯调度问题上展现出了优越性，未来可能进一步优化大型建筑中的电梯交通管理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Novel+RL+approach+for+efficient+Elevator+Group+Control+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00011，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00011&send_immediately=true&force_search=false)

**原文摘要:** Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [5] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye, Shayan Mohajer Hamidi, En-hui Yang*

**主要类别:** cs.LG

**AI概要:** 提出了一种称为CMI最小化（CMIM）的新训练方法，通过同时最小化传统交叉熵（CE）损失和所有温度缩放簇的条件互信息（CMI）值，构建不可蒸馏的深度神经网络（DNN）。实验表明，使用CMIM方法训练的模型对所有测试的知识蒸馏（KD）方法都是不可蒸馏的，并且在预测准确性方面优于仅使用CE损失训练的模型。


<details>
  <summary>更多</summary>
  
**动机:** 为了保护深度神经网络（DNN）的知识产权，希望构建不可蒸馏的DNN。观察到不可蒸馏的DNN具有其输出概率分布高度集中的特性，因此提出了基于条件互信息（CMI）测量这种集中性的新训练方法。

**方法:** 提出了一种新的训练方法，称为CMI最小化（CMIM）方法。该方法通过联合最小化传统交叉熵（CE）损失和所有温度缩放簇的条件互信息（CMI）值来训练DNN。

**结果:** 实验结果表明，使用CMIM方法训练的模型对所有测试的知识蒸馏（KD）方法都是不可蒸馏的，即从CMIM模型中蒸馏出的学生模型性能不如独立使用标签平滑（LS）训练的学生模型。此外，CMIM模型在自身的预测准确性方面也优于仅使用CE损失训练的模型。

**结论:** CMI最小化（CMIM）方法能够有效地构建不可蒸馏的DNN，并且在保护模型知识产权的同时提高了模型的预测准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Undistillable+Models+by+Minimizing+Conditional+Mutual+Information，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00012，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00012&send_immediately=true&force_search=false)

**原文摘要:** A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [6] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo, Chiehyeon Lim*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的时间序列预测框架ST-MTM，通过季节-趋势分解和对比学习任务，有效捕捉复杂的时间依赖性和语义信息，显著提升预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有时间序列建模方法简单屏蔽原始数据可能捕获虚假的时间模式，忽略了复杂的语义结构。需要一种能分解并处理纠缠模式的方法来更好地捕捉时间语义。

**方法:** 提出ST-MTM框架，包括：1) 季节-趋势分解；2) 新型屏蔽策略（周期屏蔽用于季节成分，子序列屏蔽用于趋势成分）；3) 对比学习任务以增强上下文一致性。

**结果:** 实验结果表明，ST-MTM在预测性能上优于现有的屏蔽建模、对比学习和监督预测方法。

**结论:** ST-MTM通过分解和屏蔽策略有效捕捉复杂时间依赖性，并通过对比学习任务进一步提升模型性能，为复杂时间序列预测提供了一种优越的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ST-MTM%3A+Masked+Time+Series+Modeling+with+Seasonal-Trend+Decomposition+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00013，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00013&send_immediately=true&force_search=false)

**原文摘要:** Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [7] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi, Shayan Chowdhury, Fatih Uysal*

**主要类别:** cs.LG

**AI概要:** SWE-Bench-CL是一个基于GitHub议题的连续学习基准，用于评估代理在软件开发中的持续学习能力，包括经验积累、知识迁移和防止灾难性遗忘。研究引入了专门的度量标准和实验协议，并提供公开可用的数据和代码。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型在静态代码生成基准上表现优异，但实际软件开发是一个不断演化的过程，涉及问题、修复和功能请求。为更准确地评估模型在动态环境中的能力，需要一个反映真实世界软件开发的连续学习基准。

**方法:** 构建了一个名为SWE-Bench-CL的新型连续学习基准，使用人类验证过的SWE-Bench Verified数据集。通过按时间顺序组织GitHub议题，模拟自然的仓库演化过程。此外，还引入了任务间结构相似性和上下文敏感性的初步分析，以及基于LangGraph的交互式评估框架和FAISS支持的语义记忆模块。定义了一系列专门的连续学习度量指标，如平均准确性、遗忘程度、前/后向转移等。

**结果:** 提供了严格的实验协议，比较了具有和不具有记忆功能的代理在不同Python仓库上的表现。所有代码和数据都已公开发布，为社区提供了一个可重复的平台，以开发更具适应性和鲁棒性的AI代理。

**结论:** SWE-Bench-CL为评估和改进AI代理在软件工程中的持续学习能力提供了有价值的工具和基准。该研究强调了记忆和知识迁移在动态开发环境中的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SWE-Bench-CL%3A+Continual+Learning+for+Coding+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00014&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [8] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Xuekang Liu, Fabio Roli, Carsten Maple*

**主要类别:** cs.LG

**AI概要:** The paper proposes a novel vision transformer (ViT) architecture with an adversarial indicator (AdvI) token to defend against adversarial attacks in transformer-based modulation classification systems for IoT devices. It integrates adversarial training and detection mechanism, reduces system complexity, and surpasses several competitive methods in white-box attack scenarios.


<details>
  <summary>更多</summary>
  
**动机:** Transformers have been successful in various fields, but they are susceptible to adversarial attacks in radio signal classification for IoT devices. There is a need for a defensive strategy to counter such attacks.

**方法:** A new concept called adversarial indicator (AdvI) token is introduced in the ViT architecture. This is combined with adversarial training and a detection mechanism within a unified neural network model. The attention mechanism is examined to understand the operational principles.

**结果:** The proposed AdvI token influences attention weights in the ViT, highlighting potentially suspicious or anomalous features in input data. Experimental results show that the approach outperforms several competitive methods in handling white-box attack scenarios.

**结论:** This work presents the first application of an AdvI token in ViT to defend against adversarial attacks. The integrated defense mechanism reduces architectural complexity and provides superior performance in white-box attack scenarios.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Vision+Transformer+with+Adversarial+Indicator+Token+against+Adversarial+Attacks+in+Radio+Signal+Classifications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00015，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00015&send_immediately=true&force_search=false)

**原文摘要:** The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [9] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu, Liu Liu, Fuxiang Wu, Fusheng Hao, Xianglong Liu*

**主要类别:** cs.LG

**AI概要:** 本文针对大型预训练模型微调问题，提出了一种高效的方法GRFT，通过更新权重矩阵的行或列并结合正则化，减少了存储和计算资源需求，提升了微调效率，并在多个数据集上取得了超越现有方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型预训练模型在各个领域展示了广泛的应用。然而，为特定下游任务微调这些模型需要大量的计算资源和存储。现有的基于梯度的参数选择（GPS）方法虽然减少了训练参数的数量，但增加了计算资源需求和存储需求。

**方法:** 本文提出了一种高效的基于梯度和正则化的微调方法（GRFT），该方法更新权重矩阵的行或列。理论上证明了具有最高平方梯度和的行或列是最优的更新对象。这种方法不仅有效减少了存储开销，还提高了参数选择的效率。此外，通过引入正则化增强了从预训练模型的知识迁移。

**结果:** GRFT在FGVC和VTAB数据集上分别只需更新1.22%和0.30%的总参数，达到了最先进的性能，超过了现有的方法如GPS、Adapter Tuning和LoRA。

**结论:** GRFT方法显著提高了微调大型预训练模型的效率和效果，同时减少了存储和计算资源的需求。源代码即将发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gradient-based+Fine-Tuning+through+Pre-trained+Model+Regularization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00016，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00016&send_immediately=true&force_search=false)

**原文摘要:** Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [10] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, Xipeng Qiu*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种统一的理论框架，将监督微调（SFT）和偏好学习联系起来，并揭示了SFT的局限性。为了解决这些问题，作者提出了降低学习率的方法以及基于不同f-散度函数的替代SFT目标，从而显著提高了模型性能。此外，还推导了LLM logits与Q函数之间的关系，并进行了实验验证。


<details>
  <summary>更多</summary>
  
**动机:** 在将预训练语言模型适应到实际任务时，从示例或偏好信号中学习起着至关重要的作用。然而，传统的SFT方法存在一些局限性，例如KL散度项在优化过程中变得与策略无关，无法有效约束模型更新。因此，需要更深入地理解SFT和偏好学习之间的关系，并改进现有的SFT方法以提高模型性能。

**方法:** 1. 提出一种统一的理论框架，连接SFT和偏好学习方法（如DPO），并证明两者都在相同的最优策略-奖励子空间中运行。
2. 分析了传统SFT方法的局限性：KL散度项在优化过程中变为常数，不能约束模型更新。
3. 提出了降低学习率的简单方法，以改善SFT性能。
4. 从不同的f-散度函数中推导出新的SFT目标，保留KL散度项，进一步增强模型性能。
5. 扩展了偏好学习中关于LLM logits和Q函数之间关系的理论，并提供了数学推导和实验验证。

**结果:** 通过降低学习率的方法，在指令跟随任务中实现了高达25%的相对增益和6%的绝对胜率提升。此外，基于不同f-散度函数的新SFT目标也显著提高了后DPO模型的性能。实验结果表明，所提出的理论框架和改进方法可以有效提高LLM在实际任务中的表现。

**结论:** 本文建立了一个连接SFT和偏好学习的统一理论框架，揭示了传统SFT方法的局限性，并提出了有效的解决方案。这些改进不仅显著提高了模型性能，还为未来研究提供了理论基础和新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Implicit+Reward+as+the+Bridge%3A+A+Unified+View+of+SFT+and+DPO+Connections，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00018，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00018&send_immediately=true&force_search=false)

**原文摘要:** Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [11] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges, Felipe Pereira, Michel Tosin*

**主要类别:** cs.LG

**AI概要:** This study enhances McMC methods using VAE for broader-spectrum prior proposals, achieving better performance in Bayesian inverse problems without requiring prior knowledge of covariance functions.


<details>
  <summary>更多</summary>
  
**动机:** To improve the efficiency and applicability of McMC methods by overcoming limitations of traditional approaches like KLE that require prior knowledge of covariance functions.

**方法:** A Variational Autoencoder (VAE) is used to generate prior proposals in Bayesian inverse problems, specifically tested in a synthetic groundwater flow inversion problem.

**结果:** The VAE-based parameterization matches KLE's accuracy when correlation length is known and surpasses it when correlation length deviates. It also reduces stochastic dimensionality, enhancing computational efficiency.

**结论:** Using deep generative models such as VAE in McMC methods can lead to more adaptable and efficient Bayesian inference in high-dimensional problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Autoencoder+for+Generating+Broader-Spectrum+prior+Proposals+in+Markov+chain+Monte+Carlo+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00020，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00020&send_immediately=true&force_search=false)

**原文摘要:** This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [12] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath, Hema Date*

**主要类别:** cs.LG

**AI概要:** 本研究提出了三种量子启发的数据编码策略：实例级策略（ILS）、全局离散策略（GDS）和类别条件值策略（CCVS），用于将经典数据转换为量子数据，应用于纯经典机器学习模型。通过在分类任务中应用这些策略，评估了它们在编码效率、正确性、模型准确性和计算成本方面的影响，提供了优化量子启发数据转换的见解。


<details>
  <summary>更多</summary>
  
**动机:** 为了减少高编码时间，同时确保正确的编码值，并分析这些编码策略对分类性能的影响，研究提出了三种不同的量子启发数据编码方法。

**方法:** 研究设计并比较了三种编码策略：1) 实例级策略（ILS），独立处理数据集中的每一行，模拟局部量子态；2) 全局离散值编码策略（GDS），将整个数据集中所有唯一的特征值均匀映射到量子态；3) 类别条件值编码策略（CCVS），为每个类别单独编码唯一值，保留类别依赖信息。

**结果:** 这些策略在分类任务中被应用，结果表明不同的编码策略在编码时间、精度和预测性能之间存在权衡。

**结论:** 本研究为优化量子启发的数据转换以适应经典机器学习工作流程提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Inspired+Encoding+Strategies+for+Machine+Learning+Models%3A+Proposing+and+Evaluating+Instance+Level%2C+Global+Discrete%2C+and+Class+Conditional+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00019，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00019&send_immediately=true&force_search=false)

**原文摘要:** In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [13] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin, Hong Yan, Haoliang Li*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的方法，FNSDA（Fourier Neural Simulator for Dynamical Adaptation），用于从数据中学习潜在的动力学。该方法通过在傅里叶空间中的自适应来实现对新动力学的快速泛化。FNSDA利用自动划分的傅里叶模式识别可共享的动力学，并通过条件于低维潜在系统参数来调整特定环境的动力学模式。实验结果表明，FNSDA在四个代表性的动力学系统家族上表现出优越或具有竞争力的泛化性能，同时显著减少了参数成本。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于深度神经网络的动力学建模方法在特定领域内的预测能力有限，并且难以泛化到具有相同一般动力学但环境特征不同的未见系统。因此，需要一种更高效、更具泛化能力的方法来解决这一问题。

**方法:** 研究者提出了FNSDA（Fourier Neural Simulator for Dynamical Adaptation），这是一种参数高效的模型。FNSDA通过以下步骤实现泛化：1) 使用自动划分的傅里叶模式识别已知环境中可共享的动力学；2) 条件于低维潜在系统参数，学习调整特定于每个新环境的动力学模式。这种方法使得模型能够快速适应新的动力学系统。

**结果:** FNSDA在四个代表性动力学系统家族上的评估结果表明，它相较于现有方法可以实现优越或具有竞争力的泛化性能，同时大幅降低了参数成本。这证明了FNSDA在泛化能力和计算效率方面的优势。

**结论:** FNSDA是一种有效的参数高效方法，能够在保持高性能的同时减少模型复杂度和计算资源需求。其在不同动力学系统上的成功应用展示了其广泛的适用性和潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalizing+to+New+Dynamical+Systems+via+Frequency+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00025，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00025&send_immediately=true&force_search=false)

**原文摘要:** Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [14] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar, Kunal Telangi*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的强化学习框架FPG，通过分数阶微积分减少方差并提高采样效率，同时保持收敛性。


<details>
  <summary>更多</summary>
  
**动机:** 标准的策略梯度方法由于马尔可夫假设而存在高方差和采样效率低的问题。

**方法:** 使用Caputo分数阶导数重新定义梯度，建立状态转移之间的幂律时间相关性，并开发了高效的递归计算技术。

**结果:** 理论分析表明FPG可以实现渐进方差减少，经验验证显示比现有基线方法提高了35-68%的采样效率和24-52%的方差减少。

**结论:** FPG提供了一种数学上有根据的方法，在不增加计算开销的情况下利用长程依赖关系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fractional+Policy+Gradients%3A+Reinforcement+Learning+with+Long-Term+Memory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00073，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00073&send_immediately=true&force_search=false)

**原文摘要:** We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [15] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

**主要类别:** cs.LG

**AI概要:** GLU Attention是一种引入非线性的新型注意力机制，它在文本和视觉模态上提高了模型性能和收敛速度，且无需额外参数和计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 门控线性单元（GLU）在提升神经网络性能方面表现出巨大潜力，因此作者尝试将GLU与注意力机制结合以进一步提高性能。

**方法:** 提出了一种名为GLU Attention的新型注意力机制，该机制将非线性引入注意力的值中，并且可以与Flash Attention、RoPE等技术无缝集成。

**结果:** 实验表明，GLU Attention在文本和视觉模态上都提高了模型性能和收敛速度，同时没有增加额外参数和显著的计算成本。

**结论:** GLU Attention是一种轻量级的注意力机制，具有高性能和易集成的特点，适用于多种模态任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GLU+Attention+Improve+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00022，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00022&send_immediately=true&force_search=false)

**原文摘要:** Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [16] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

**主要类别:** cs.LG

**AI概要:** 这篇论文深入研究了在数据异质性实际模型下的分布式和联合优化中本地更新算法（特别是本地SGD）的理论基础。核心是有限的二阶异质性假设，该假设被证明是在凸和非凸设置下本地更新优于集中或小批量方法的必要和充分条件。论文为多种本地更新算法建立了多个场景下的严格上下界，并刻画了多类问题的极小极大复杂度。通过精细的共识误差分析框架，在三阶平滑性和放松的异质性假设下，得到了更精确的有限时间收敛边界。此外，论文还扩展到在线联合学习领域，提供了在梯度和bandit反馈下的基本后悔边界。这些结果阐明了本地更新何时以及为何提供可证明的优势，并为分析异质环境中的本地SGD提供了完整的指导。


<details>
  <summary>更多</summary>
  
**动机:** 尽管本地更新算法（如本地SGD）在分布式和联合优化中得到广泛应用，但其在数据异质性实际模型下的理论理解仍不充分。为了明确本地更新在哪些情况下具有优势及其原因，需要建立一个严格的理论框架来分析其性能和复杂度。

**方法:** 论文采用了一个基于共识误差的细粒度分析框架，探讨了二阶异质性假设的作用，并在三阶平滑性和放松的异质性假设下推导了更精确的有限时间收敛边界。此外，论文对在线联合学习进行了扩展研究，分别在梯度和bandit反馈下提供了后悔边界。

**结果:** 论文为多种本地更新算法提供了严格的上下界，并明确了不同问题类别的极小极大复杂度。同时，通过引入新的分析框架，改进了现有方法的收敛边界。对于在线联合学习，也提供了新的理论支持。

**结论:** 本地更新算法（尤其是本地SGD）在满足特定数据异质性假设时，可以在凸和非凸环境下显著优于集中式或小批量方法。论文的结果为理解本地更新算法在异质环境下的优势提供了坚实的理论基础，并为未来的研究提供了明确的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Makes+Local+Updates+Effective%3A+The+Role+of+Data+Heterogeneity+and+Smoothness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00195，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00195&send_immediately=true&force_search=false)

**原文摘要:** This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [17] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu, Xilei Bian, Jie Xiong, Xing Wu, Quan Qian*

**主要类别:** cs.LG

**AI概要:** 随着对新型材料需求的增长，机器学习驱动的逆向设计方法在高维材料组成空间与有限实验数据之间面临重大挑战。现有的方法存在两大局限性：(I) 机器学习模型在高维空间中缺乏可靠性，导致设计过程中的预测偏差；(II) 这些模型无法有效整合领域专家知识，限制了其支持知识引导型逆向设计的能力。为了解决这些问题，我们提出了 AIMatDesign，这是一个强化学习框架，通过使用基于差异的算法增强实验数据来构建可信的经验池，加速模型收敛。为了提高模型可靠性，采用由大型语言模型 (LLMs) 引导的自动化精化策略动态校正预测不一致性，加强奖励信号与状态值函数之间的对齐。此外，基于知识的奖励函数利用专家领域规则提高了训练期间的稳定性和效率。我们的实验表明，AIMatDesign 在发现效率、收敛速度和成功率方面显著超越了传统的机器学习和强化学习方法。在 AIMatDesign 提出的众多候选材料中，代表性 Zr 基合金的实验合成产生了一种性能最佳的 BMG，具有 1.7GPa 屈服强度和 10.2% 的延伸率，与预测结果非常接近。此外，该框架准确捕捉了屈服强度随成分变化的趋势，展示了其可靠性和闭环材料发现的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前机器学习驱动的逆向设计方法在处理高维材料组成空间时面临两大挑战：(1) 模型在高维空间中的预测可靠性不足；(2) 无法有效整合领域专家知识。这些限制阻碍了高效材料发现的能力，因此需要一种新的方法来克服这些限制。

**方法:** 提出了一种名为 AIMatDesign 的强化学习框架，主要方法包括：
- 使用基于差异的算法增强实验数据，构建可信经验池以加速模型收敛。
- 采用大型语言模型 (LLMs) 引导的自动化精化策略动态校正预测不一致性，确保奖励信号与状态值函数的一致性。
- 设计基于知识的奖励函数，利用专家领域规则提高训练的稳定性和效率。

**结果:** 实验验证显示 AIMatDesign 显著优于传统机器学习和强化学习方法，在以下方面表现出色：
- 材料发现效率更高。
- 收敛速度更快。
- 成功率更高。
- 实验合成的 Zr 基合金性能与预测高度吻合，且框架能够准确捕捉屈服强度随成分变化的趋势。

**结论:** AIMatDesign 是一种有效的强化学习框架，解决了现有逆向设计方法在高维空间中的可靠性问题，并成功整合了领域专家知识。它在材料发现效率、收敛速度和成功率上均表现出色，展示出闭环材料发现的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AIMatDesign%3A+Knowledge-Augmented+Reinforcement+Learning+for+Inverse+Materials+Design+under+Data+Scarcity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00024&send_immediately=true&force_search=false)

**原文摘要:** With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [18] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson, Alex Newcombe, Eric Piette, Dennis Soemers*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于Wilson得分区间（Optimistic-WS）的乐观选择过程，用于在多问题领域中准确识别每个子任务的最佳算法。相比现有方法，在减少平均简单后悔值方面表现出显著性能提升，可优化通用游戏框架和其他多任务领域的代理评估过程。


<details>
  <summary>更多</summary>
  
**动机:** 在多问题领域中，需要一种高效且通用的方法来准确识别每个子任务的最佳算法，以提高代理评估的质量和准确性。

**方法:** 将问题视为多臂老虎机中的最佳手臂识别问题，其中每个老虎机对应特定任务，每只手臂对应特定算法或代理。提出基于Wilson得分区间的乐观选择过程（Optimistic-WS），通过潜在后悔减少量对所有老虎机的手臂进行排名。

**结果:** 在GVGAI框架和Ludii系统上的实验结果表明，与先前的最佳手臂识别算法相比，Optimistic-WS在减少平均简单后悔值方面表现出显著的性能改进。

**结论:** 提出的Optimistic-WS方法可以显著提高通用游戏框架和其他高运行时算法的多任务领域中代理评估过程的质量和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Best+Agent+Identification+for+General+Game+Playing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00451&send_immediately=true&force_search=false)

**原文摘要:** We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [19] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om, Kyuil Sim, Taeyoung Yun, Hyeongyu Kang, Jinkyoo Park*

**主要类别:** cs.LG

**AI概要:** 提出了一种新框架，通过迭代训练流模型和代理模型，并将候选选择问题视为后验推理问题，以解决高维约束黑箱优化问题。该方法在各种合成和真实世界的约束黑箱优化任务中表现出优越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 高维黑箱函数在黑箱约束下的优化是一个普遍的任务，比无约束问题更难，因为难以找到可行区域。现有的贝叶斯优化方法难以应对维度诅咒，而基于生成模型的方法虽然有潜力，但可扩展性差且容易出现模式崩塌。

**方法:** 该方法包含两个阶段：1) 训练流模型以捕捉数据分布和代理模型以预测函数值和约束违反情况；2) 将候选选择问题作为后验推理问题，以搜索具有高目标值而不违反约束的候选者。此外，为了处理多模态后验分布的问题，在流模型的潜在空间中摊销采样，使其比数据空间更平滑。

**结果:** 实验结果表明，该方法在各种合成和真实世界的约束黑箱优化任务中实现了优越的性能。

**结论:** 所提出的新框架有效地解决了高维约束黑箱优化中的维度诅咒、可扩展性和模式崩塌问题，展示了其在实际应用中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Posterior+Inference+in+Latent+Space+for+Scalable+Constrained+Black-box+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00480，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00480&send_immediately=true&force_search=false)

**原文摘要:** Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [20] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding, Xiang Zheng, Cong Wang, Wei-Bin Lee, Xingjun Ma, Yu-Gang Jiang*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种名为ROSE的新框架，通过多目标强化学习微调对抗性LLM，生成主题多样且上下文丰富的对抗性提示，以解决现有方法在评估LLM安全性时存在的主题覆盖不足和与现实情境对齐不佳的问题。实验表明，ROSE在发现最先进LLM的安全漏洞方面优于现有方法，并在综合评估指标上有显著改进。


<details>
  <summary>更多</summary>
  
**动机:** 由于大型语言模型（LLMs）越来越多地作为黑箱组件部署在实际应用中，对其安全性的评估变得至关重要，尤其是在对抗性提示下的安全性。现有的手动安全基准由于其静态性质和更新所需的人力密集型工作，难以跟上快速发展的LLMs。而自动化的对抗性提示生成虽然提供了适应性评估的希望，但目前的方法存在主题覆盖不足和与现实情境对齐不佳的问题。

**方法:** 作者提出了一个名为Reality-Oriented Safety Evaluation (ROSE)的框架，该框架利用多目标强化学习来微调一个对抗性LLM，从而生成主题多样且上下文丰富的对抗性提示。这种方法旨在克服探索-利用困境以及缺乏现实情境化的问题。

**结果:** 实验结果表明，ROSE在揭示最先进LLMs的安全漏洞方面优于现有方法，并在综合评估指标上显示出显著改进。

**结论:** 作者认为ROSE代表了向更实用和现实导向的LLM安全性评估迈进的一步，并希望这一工作能够推动该领域的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ROSE%3A+Toward+Reality-Oriented+Safety+Evaluation+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00026&send_immediately=true&force_search=false)

**原文摘要:** As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [21] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone, Marius P. Linhard*

**主要类别:** cs.LG

**AI概要:** 本研究通过将GAN解释为概率生成模型，提出了一种新的理解其成功与局限性的方法，并定义了优化和正则化策略以改善性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管GANs非常成功，但它们的优化过程充满挑战且需要正则化来防止过拟合。

**方法:** 将GANs解释为概率生成模型，并将其视为具有部分随机性的贝叶斯神经网络，从而建立普遍近似的条件。将几种GAN变体的对抗式优化视为边缘似然的代理优化。利用边缘似然优化与奥卡姆剃刀原理之间的联系，定义正则化和优化策略以寻找最小描述长度的解。

**结果:** 在广泛的实验中，这些策略带来了性能的提升，并为进一步理解GANs的正则化策略铺平了道路。

**结论:** 通过将GANs作为概率生成模型进行解释，可以更好地理解它们的成功与局限性，并通过特定的正则化和优化策略提高其性能和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GANs+Secretly+Perform+Approximate+Bayesian+Model+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00651&send_immediately=true&force_search=false)

**原文摘要:** Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [22] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li, Hao Xue, Shuang Ao, Yang Song, Flora Salim*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的框架HiT-JEPA，用于学习多尺度城市轨迹表示，能够同时捕获细粒度细节和高层次抽象。通过在多个真实数据集上的实验验证，该方法在轨迹相似性计算任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的轨迹表示方法难以同时捕获细粒度细节和高层次的总结信息，限制了对长期依赖关系的关注以及局部特征的保留。

**方法:** 提出了HiT-JEPA（基于联合嵌入预测架构的轨迹语义层次交互）框架，采用三层等级结构逐步捕获点级细粒度细节、中间模式和高层次轨迹抽象，从而整合局部动态和全局语义。

**结果:** 在多个真实数据集上的广泛实验表明，HiT-JEPA的层次设计生成了更丰富、多尺度的轨迹表示，在轨迹相似性计算任务中表现优异。

**结论:** HiT-JEPA为学习多尺度城市轨迹表示提供了一个统一的框架，能够有效分析空间运动模式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HiT-JEPA%3A+A+Hierarchical+Self-supervised+Trajectory+Embedding+Framework+for+Similarity+Computation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00028&send_immediately=true&force_search=false)

**原文摘要:** The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [23] [Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN](https://arxiv.org/abs/2507.00736)
*Arthur Thuy, Ekaterina Loginova, Dries F. Benoit*

**主要类别:** cs.LG

**AI概要:** 近年来，基于自然语言处理技术的问题难度估计（QDE）引起了越来越多的关注。本文通过平衡离散排序概率得分（DRPS）基准测试三种模型输出类型，并提出OrderedLogitNN方法，发现其在复杂任务上表现更佳，同时为离散级别QDE提供了稳健和公平的评估指标。


<details>
  <summary>更多</summary>
  
**动机:** 当前研究忽视了问题难度估计任务中的序数性质，使用分类或离散化回归模型而未探索专门的序数回归方法。此外，现有评估指标未能充分考虑难度等级的序数结构及类别不平衡问题，导致性能评估偏差。

**方法:** 1. 基准测试三种模型输出：离散化回归、分类和序数回归。
2. 提出平衡离散排序概率得分（DRPS），联合捕捉序数性和类别不平衡。
3. 提出OrderedLogitNN方法，将经济学中的有序logit模型扩展到神经网络。
4. 在RACE++和ARC数据集上微调BERT。

**结果:** 实验表明，与其它方法相比，OrderedLogitNN在复杂任务上表现出显著更好的性能。平衡DRPS作为一种稳健且公平的评估指标，能有效应对离散级别QDE中的序数性和类别不平衡问题。

**结论:** 本文通过引入有序回归方法和平衡DRPS，为离散级别问题难度估计任务奠定了原则性的基础，推动了未来研究的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ordinality+in+Discrete-level+Question+Difficulty+Estimation%3A+Introducing+Balanced+DRPS+and+OrderedLogitNN，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00736，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00736&send_immediately=true&force_search=false)

**原文摘要:** Recent years have seen growing interest in Question Difficulty Estimation
(QDE) using natural language processing techniques. Question difficulty is
often represented using discrete levels, framing the task as ordinal regression
due to the inherent ordering from easiest to hardest. However, the literature
has neglected the ordinal nature of the task, relying on classification or
discretized regression models, with specialized ordinal regression methods
remaining unexplored. Furthermore, evaluation metrics are tightly coupled to
the modeling paradigm, hindering cross-study comparability. While some metrics
fail to account for the ordinal structure of difficulty levels, none adequately
address class imbalance, resulting in biased performance assessments. This
study addresses these limitations by benchmarking three types of model outputs
-- discretized regression, classification, and ordinal regression -- using the
balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly
captures ordinality and class imbalance. In addition to using popular ordinal
regression methods, we propose OrderedLogitNN, extending the ordered logit
model from econometrics to neural networks. We fine-tune BERT on the RACE++ and
ARC datasets and find that OrderedLogitNN performs considerably better on
complex tasks. The balanced DRPS offers a robust and fair evaluation metric for
discrete-level QDE, providing a principled foundation for future research.

</details>


### [24] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li, Zikai Song, Hang Zhou, Yunyao Zhang, Junqing Yu, Wei Yang*

**主要类别:** cs.LG

**AI概要:** 近期将低秩适应（LoRA）与专家混合（MoE）结合以使大语言模型（LLMs）适应多任务的方法存在局限性。本文提出了一种模块化且轻量级的MoE框架——LoRA-Mixer，通过用动态路由的任务特定LoRA专家替换注意力模块输入/输出线性层的投影矩阵，解决了参数效率和任务保真度的问题。该设计与包括变压器和状态空间模型在内的多种基础模型兼容，并支持两种操作模式：联合优化LoRA专家和路由机制或直接部署预训练的冻结LoRA模块。此外，还引入了自适应专业化平衡损失（SBL），以确保稳定的路由决策和最大化专家重用。实验结果表明，LoRA-Mixer在多个基准数据集上表现出了显著的效果和性能提升，同时仅使用48%的参数，展现了其高效性和优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的将LoRA与MoE结合的方法要么完全交换注意力/前馈层以进行专家切换，要么添加并行专家分支，导致参数效率降低和任务保真度下降。因此需要一种更高效、更灵活的方法来解决这些问题。

**方法:** 提出了一种名为LoRA-Mixer的模块化轻量级MoE框架，其核心创新在于用动态路由的任务特定LoRA专家替换注意力模块输入/输出线性层的投影矩阵。该框架支持两种操作模式：1) 通过新颖的硬软路由策略联合优化LoRA专家和路由机制；2) 直接部署从外部存储库获取的预训练冻结LoRA模块。为了确保有限数据下的稳健路由训练，提出了自适应专业化平衡损失（SBL），用于联合优化专家平衡和任务特定对齐。

**结果:** 在七个基准数据集上的广泛实验表明，LoRA-Mixer在GSM8K、HumanEval和MedQA等数据集上分别比基础模型提高了7.61%、4.88%和3.08%。与最先进的方法相比，LoRA-Mixer分别额外提高了1.09%、1.45%和1.68%，同时仅使用了48%的参数，证明了其高效性和强性能。

**结论:** LoRA-Mixer是一种有效的模块化和轻量级框架，能够在保证任务保真度的同时提高参数效率。它与多种基础模型兼容，并通过引入自适应专业化平衡损失（SBL）确保了稳健的路由训练和专家重用。实验结果验证了LoRA-Mixer的优越性能和高效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LoRA-Mixer%3A+Coordinate+Modular+LoRA+Experts+Through+Serial+Attention+Routing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00029，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00029&send_immediately=true&force_search=false)

**原文摘要:** Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [25] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma, Nallarasan V, Balaraman Ravindran*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种结合上下文Bandits与深度强化学习（DRL）的新范式，用于自适应选择动作执行时长，从而提升策略灵活性和计算效率。实验表明，相比静态时长基线，该方法在Atari 2600游戏上显著提升了性能，适用于实时应用如游戏和机器人领域。


<details>
  <summary>更多</summary>
  
**动机:** 深度强化学习（DRL）在复杂序列决策任务中取得了显著成功，但动作执行的时间尺度问题尚未得到充分探索。现有的DRL方法通常使用固定的或预定义的动作持续时间，这可能限制了策略的灵活性和效率。

**方法:** 研究者提出了一种新的范式，将上下文Bandits与DRL相结合，以自适应地选择动作持续时间。具体来说，他们在Deep Q-Network (DQN)的基础上增加了一个上下文Bandits模块，该模块根据状态情境学习选择最优的动作重复率。

**结果:** 在Atari 2600游戏上的实验表明，这种方法相较于静态时长基线有显著的性能提升，证明了自适应时间抽象在DRL中的有效性。

**结论:** 这种新范式提供了一个可扩展的解决方案，适用于需要动态动作持续时间的实时应用，例如游戏和机器人技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Action+Duration+with+Contextual+Bandits+for+Deep+Reinforcement+Learning+in+Dynamic+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00030，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00030&send_immediately=true&force_search=false)

**原文摘要:** Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [26] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li, Jiang You, Hassine Moungla, Vincent Gauthier, Miguel Nunez-del-Prado, Hugo Alatrista-Salas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种Spatial Neighbourhood Fusion (SPN)技术，通过聚合相邻区域的特征来增强稀疏的人类移动数据建模能力，从而提升预测性能。实验表明，SPN在多个预测模型上都能显著降低测试MSE。


<details>
  <summary>更多</summary>
  
**动机:** 准确建模人类移动对于理解疫情传播和及时干预至关重要。然而，在疫情期间，基于时空数据的移动性建模面临数据稀疏性的挑战，这限制了传统时间序列模型的预测能力。

**方法:** 提出了一种轻量级且与模型无关的Spatial Neighbourhood Fusion（SPN）技术，该技术通过聚合每个单元格的直接H3邻居的特征来增强其特性。SPN可以在不同的预测模型中应用以改善预测效果。

**结果:** 实验结果表明，SPN在NLinear、PatchTST和K-U-Net三个预测模型上均能持续提高预测性能，最多可将测试MSE减少9.85%。

**结论:** 空间平滑处理稀疏移动信号为公共健康危机期间的鲁棒时空预测提供了一条简单而有效的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Spatio-Temporal+Forecasting+with+Spatial+Neighbourhood+Fusion%3AA+Case+Study+on+COVID-19+Mobility+in+Peru，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00031，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00031&send_immediately=true&force_search=false)

**原文摘要:** Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [27] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw, Reid McCants, Jean-Marie Le Corre, Anna Iskhakova, Arsen S. Iskhakov*

**主要类别:** cs.LG

**AI概要:** 本研究汇编和数字化了一个广泛的临界热流密度（CHF）数据集，涵盖了均匀和非均匀轴向加热条件，为OECD/NEA AI/ML CHF基准第二阶段提供了基础。


<details>
  <summary>更多</summary>
  
**动机:** 现有的经典CHF相关性在均匀加热下存在显著误差，在非均匀加热情况下表现更差；基于神经网络的方法虽然在均匀数据上表现良好，但无法推广到空间变化场景中。因此需要显式结合轴向功率分布的模型。

**方法:** 从技术报告中提取加热曲线，将其插值到一致的轴向网格上，并通过能量平衡检查进行验证，最后将数据编码为机器可读格式。同时提供了这些策划的数据集和基线建模结果。

**结果:** 编制的数据集涵盖了均匀和非均匀轴向加热条件，揭示了现有方法的局限性，并为后续研究奠定了基础。

**结论:** 本研究为高级迁移学习策略、严格的不确定性量化和设计优化工作提供了基础，推动了CHF基准测试的下一阶段发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Collection+with+Non-Uniform+Axial+Power+for+Phase+II+of+the+OECD%2FNEA+AI%2FML+Critical+Heat+Flux+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00034&send_immediately=true&force_search=false)

**原文摘要:** Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [28] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda, Sanjay Purushotham, Ratnaksha Lele, Vandana P. Janeja*

**主要类别:** cs.LG

**AI概要:** 在极地海洋中漂流的冰山对地球气候系统起着关键作用，影响海洋中的淡水流量和区域生态系统，同时也对极地导航构成挑战。然而，由于时空数据的匮乏和冰山运动的复杂非线性性质，准确预测冰山轨迹仍然是一个艰巨的挑战。为了解决这些挑战，我们提出了一个混合IDRIFTNET模型，这是一个物理驱动的深度学习模型，它结合了冰山漂移物理的解析公式和增强残差学习模型。我们将IDRIFTNET模型性能与最先进的模型在两个南极冰山A23A和B22A上进行比较。我们的研究结果表明，IDRIFTNET通过在各种时间点实现更低的最终位移误差（FDE）和平均位移误差（ADE），优于其他模型。


<details>
  <summary>更多</summary>
  
**动机:** 准确预测冰山轨迹对于理解地球气候系统、保护区域生态系统以及确保极地导航安全至关重要。然而，当前存在时空数据稀缺和冰山运动复杂非线性的挑战，限制了深度学习模型的有效性和可靠性。因此，需要一种新的方法来克服这些限制并提高预测精度。

**方法:** 提出了一种名为IDRIFTNET的混合模型，该模型将冰山漂移物理的解析公式与增强残差学习模型相结合。模型通过学习解析解与真实观测之间的不匹配模式，并利用旋转增强频谱神经网络捕捉数据中的全局和局部模式，从而预测未来的冰山漂移位置。

**结果:** IDRIFTNET模型在南极冰山A23A和B22A上的测试表现优于现有最先进的模型，在多个时间点上实现了更低的最终位移误差（FDE）和平均位移误差（ADE）。

**结论:** IDRIFTNET模型有效地捕捉了冰山复杂的非线性漂移动态，在有限数据和动态环境条件下显著提高了冰山轨迹预测的准确性。这表明物理驱动的深度学习模型在应对复杂自然现象预测方面的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IDRIFTNET%3A+Physics-Driven+Spatiotemporal+Deep+Learning+for+Iceberg+Drift+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00036&send_immediately=true&force_search=false)

**原文摘要:** Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [29] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam, Andreas Spanopoulos, Amit Sant, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的模型融合算法，通过结合神经元归属分数和中间神经元分组，能够有效地将多个训练好的神经网络整合为一个单一网络，无论训练数据的分布如何。该方法在零样本和非IID融合场景中优于先前的技术。


<details>
  <summary>更多</summary>
  
**动机:** 当前模型融合方法在处理内部表示差异时面临挑战，这些差异可能源于排列不变性、随机初始化或训练数据分布的不同。因此，需要一种更有效的模型融合方法来克服这些问题。

**方法:** 提出了一种以神经元为中心的模型融合算法家族，通过将父模型的中间神经元分组创建目标表示，并让融合模型用相应的子网络进行近似。此方法将神经元归属分数纳入融合过程，并可推广到任意层类型。

**结果:** 实验结果表明，在各种基准数据集上，新算法在零样本和非IID融合场景中始终优于以前的融合技术。

**结论:** 提出的神经元插值模型融合算法能够有效解决不同训练数据分布下的模型融合问题，具有广泛的适用性和优越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+Fusion+via+Neuron+Interpolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00037&send_immediately=true&force_search=false)

**原文摘要:** Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [30] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen, Wenchi Zhou*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于点信息量（PVI）的数据缩减策略，通过量化实例难度和渐进学习方法，既减少数据量又保持分类器性能，同时加速模型收敛并提升跨语言数据处理效率。


<details>
  <summary>更多</summary>
  
**动机:** 在数据驱动的AI中，数据缩减对于提高模型训练效率至关重要，但如何选择最优实例而非整个数据集以改善数据质量和训练效率是核心挑战。

**方法:** 1. 使用PVI量化实例难度，并过滤掉低难度实例以实现静态数据缩减方法。
2. 采用渐进学习方法，在按升序排列的PVI实例上训练分类器，以加速收敛。

**结果:** 1. 移除10%-30%的数据仅导致0.0001%到0.76%的准确率损失。
2. 渐进学习方法比传统训练提升了0.8%的准确率。
3. PVI框架成功应用于中文NLP任务和基础模型，提供了跨语言数据缩减的见解。

**结论:** 有效的数据缩减策略不仅可以增强模型性能，还能提升训练效率，尤其是在跨语言场景下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quality+over+Quantity%3A+An+Effective+Large-Scale+Data+Reduction+Strategy+Based+on+Pointwise+V-Information，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00038&send_immediately=true&force_search=false)

**原文摘要:** Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [31] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin, Rosa Figueiredo, Vincent Labatut, Christine Largeron*

**主要类别:** cs.LG

**AI概要:** 本研究对38种质量度量进行了比较分析，评估了它们在图模式分类中的表现，并提出了一种基于聚类的预处理方法以提高性能和减少模式数量。


<details>
  <summary>更多</summary>
  
**动机:** 图分类任务需要依据图的结构和属性特征进行分类，在社交网络分析和生物信息学等领域有广泛应用。使用模式（即子图）的方法具有良好的可解释性，但选择合适的质量度量函数困难，现有研究缺乏针对图数据的质量度量对比分析。

**方法:** 理论上根据四个数学性质对38种质量度量进行表征；利用公开数据集构建基准测试，并提出一种生成模式黄金标准排名的方法；通过经验比较评估这些度量在模式排名和分类性能上的表现；提出一种基于聚类的预处理步骤，将出现在相同图中的模式分组以提升分类性能。

**结果:** 实验结果表明，提出的基于聚类的预处理方法能有效减少需处理的模式数量，同时保持分类性能；一些广泛使用的流行度量并未带来最佳结果。

**结论:** 该研究为图分类任务中质量度量的选择提供了理论和实证指导，并证明了基于聚类的预处理方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pattern-Based+Graph+Classification%3A+Comparison+of+Quality+Measures+and+Importance+of+Preprocessing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00039&send_immediately=true&force_search=false)

**原文摘要:** Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [32] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala, Pedro Morgado, William Sethares*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为 LightweightSER (LiSER) 的知识蒸馏框架，利用未标记的音视频数据进行语音情感识别（SER），通过大型教师模型向轻量级学生模型传递关于语音情感和面部表情的知识。在 RAVDESS 和 CREMA-D 两个基准数据集上的实验表明，LiSER 可以减少对大量标注数据集的依赖。


<details>
  <summary>更多</summary>
  
**动机:** 由于人类通过多模态的音频-视觉线索传达情感，因此开发结合这两种模式的语音情感识别系统是有益的。然而，收集大量标注数据以发展此类系统成本高昂，需要一种新方法来减少对这些数据的依赖。

**方法:** 论文提出了一种名为 LightweightSER (LiSER) 的知识蒸馏框架，该框架使用未标记的音视频数据进行语音情感识别。通过基于先进语音和面部表示模型构建的大规模教师模型，LiSER 将有关语音情感和面部表情的知识转移到轻量级学生模型上。

**结果:** 在 RAVDESS 和 CREMA-D 两个基准数据集上的实验表明，LiSER 可以有效减少对大量标注数据集的依赖，从而提高语音情感识别任务的效率。

**结论:** LiSER 框架通过利用未标记的音视频数据和知识蒸馏技术，成功减少了语音情感识别任务对大规模标注数据的依赖，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Unlabeled+Audio-Visual+Data+in+Speech+Emotion+Recognition+using+Knowledge+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00055，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00055&send_immediately=true&force_search=false)

**原文摘要:** Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [33] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu, Duc-Nghia Tran, Quang-Tu Pham, Hieu H. Pham, Nicolas Vuillerme, Duc-Tan Tran*

**主要类别:** cs.LG

**AI概要:** This paper introduces Smooth-Distill, a self-distillation framework for human activity recognition and sensor placement detection using wearable accelerometer data. It employs MTL-net, a unified CNN-based architecture that branches into two outputs. Unlike traditional methods, it uses a smoothed historical version of the model itself as the teacher, reducing training overhead while maintaining performance. Experiments show improvements in both tasks, enhanced stability during training, and reduced overfitting compared to baselines.


<details>
  <summary>更多</summary>
  
**动机:** To develop an efficient method for human activity recognition (HAR) and sensor placement detection using wearable accelerometer data, addressing the computational cost and overfitting issues of traditional multitask learning approaches.

**方法:** A novel self-distillation framework named Smooth-Distill is proposed, utilizing MTL-net, a unified CNN-based architecture that processes accelerometer data and branches into two outputs for HAR and sensor placement detection. The framework uses a smoothed historical version of the model itself as the teacher, reducing the need for separate teacher and student models.

**结果:** Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving notable improvements in both human activity recognition and device placement detection tasks. It demonstrates enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines.

**结论:** Smooth-Distill offers an effective solution for multitask learning with accelerometer data, balancing accuracy and training efficiency. It reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smooth-Distill%3A+A+Self-distillation+Framework+for+Multitask+Learning+with+Wearable+Sensor+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00061&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [34] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun, Yushan Liang, Zhen Zhang, Jiaye Teng*

**主要类别:** cs.LG

**AI概要:** 本研究通过求解器-验证器差距的概念，对自改进过程中的训练动态进行了理论建模，并提出了一种预测自改进极限能力的方法。此外，还探讨了有限外部数据在自改进框架内的影响。


<details>
  <summary>更多</summary>
  
**动机:** 尽管自改进技术在大语言模型中非常重要，但对其性能在自改进过程中的演变仍缺乏深入探索。

**方法:** 通过求解器-验证器差距的概念对自改进进行理论建模，并基于此框架预测自改进的最终能力。同时，分析有限外部数据对自改进动态的影响。

**结果:** 理论模型在各种大语言模型和数据集上得到了实证验证，且发现有限外部数据可在任意阶段使用而不显著影响最终性能。

**结论:** 求解器与验证器能力之间的差距是自改进性能提升的关键，且该研究提供了一种预测自改进极限能力的方法，同时也揭示了外部数据在特定条件下的使用策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Theoretical+Modeling+of+LLM+Self-Improvement+Training+Dynamics+Through+Solver-Verifier+Gap，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00075，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00075&send_immediately=true&force_search=false)

**原文摘要:** Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [35] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie, Yun Xiong, Zejian Shi, Hao Niu, Zhengfu Liu*

**主要类别:** cs.LG

**AI概要:** 随着大规模语言模型的兴起，基于大量参数和广泛数据集训练的基础模型范式在多个领域取得了显著成功。时间序列基础模型作为这一范式的重大扩展，展示了卓越的表达能力、泛化能力和跨域迁移能力。然而，这引发了一个基本悖论：时间序列数据反映了不同的动态系统，使跨域迁移在直觉上看似不可能，但却与模型的经验成功相矛盾。为了解决这个悖论，本文从理论和实验的角度研究了基于补丁的时间序列基础模型的表示学习机制和泛化能力。我们主张这些模型不仅仅是应用了一种新架构，而是从根本上将语言模型的表示范式推广到了确定性向量表示到潜在概率分布形式的扩展。我们的理论分析通过证明连续时间序列补丁可以忠实地量化为一个离散词汇表，其关键统计特性与自然语言高度一致，从而支持了这一框架。这种推广使时间序列模型能够继承大规模语言模型的强大表示和迁移能力，从而解释了它们在时间任务中的优越性能。最终，我们的工作为理解和评估大规模时间序列基础模型的安全性和可靠性提供了严格的理论基础。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列基础模型在不同领域中表现出色，但其跨域迁移能力与时间序列数据反映的不同动态系统之间存在直观上的矛盾。为了解释这一悖论，需要深入探讨时间序列模型的表示学习机制和泛化能力。

**方法:** 1. 研究基于补丁的时间序列基础模型的表示学习机制和泛化能力。
2. 从理论上证明时间序列数据可以通过量化转换为离散词汇表，并且其统计特性与自然语言类似。
3. 提出时间序列模型是对语言模型表示范式的推广，即将确定性向量表示扩展到潜在概率分布形式。
4. 结合实验验证理论分析的有效性。

**结果:** 研究表明，时间序列基础模型不仅采用了一种新的架构，还通过将语言模型的表示范式推广到概率分布形式，实现了强大的泛化和迁移能力。此外，连续时间序列数据可以被量化为离散词汇表，其统计特性与自然语言高度一致。

**结论:** 本文的研究为理解、评估和改进大规模时间序列基础模型的安全性和可靠性提供了严格的理论基础。时间序列模型通过推广语言模型的表示范式，继承了其强大的表示和迁移能力，解释了其在时间任务中的优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+language+of+time%3A+a+language+model+perspective+on+time-series+foundation+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00078，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00078&send_immediately=true&force_search=false)

**原文摘要:** With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [36] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli, Heman Shakeri*

**主要类别:** cs.LG

**AI概要:** 研究人员利用从连续血糖监测（CGM）数据中提取的动力学模式作为特征，用于检测用餐事件。这些模式通过捕捉葡萄糖变化的关键方面，识别与用餐相关的模式和异常。此方法提高了用餐检测的准确性并增强了对基础葡萄糖动态的可解释性。相比传统方法，该技术在不同数据集上具有更好的泛化能力，并在实际应用中表现出可靠的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前用餐事件检测方法可能缺乏足够的准确性和对血糖动力学的可解释性，尤其是在处理复杂或多样化的数据时。因此，需要一种更精确、更具鲁棒性的方法来分析和解读血糖数据中的用餐相关模式。

**方法:** 本研究提出了一种基于从连续血糖监测（CGM）数据中提取动力学模式的方法，用以检测用餐事件。这种方法利用了基础动态的固有属性，捕获血糖变化的关键特征，并将其转化为可用于分析的特征。

**结果:** 实验结果表明，所提出的技术显著提高了用餐事件检测的准确性，同时增强了模型对血糖动态的可解释性。此外，该方法在不同数据集上表现出良好的泛化能力和在实际应用场景中的可靠性。

**结论:** 使用动力学模式作为特征可以从连续血糖监测数据中有效检测用餐事件，不仅提升了检测精度，还增强了对血糖动态的理解。此方法为特征提取提供了一个强大的框架，在实际应用中表现优异，优于传统方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Meal+Detection+Based+on+CGM+Data+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00080&send_immediately=true&force_search=false)

**原文摘要:** We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [37] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat, Joohyung Lee, Mohamed Seif, Dusit Niyato, H. Vincent Poor*

**主要类别:** cs.LG

**AI概要:** 提出FedHLM框架，通过联邦学习优化混合语言模型中的不确定性阈值，并利用嵌入式令牌表示进行点对点解析，显著减少大型语言模型的传输需求，同时保持高准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的混合语言模型在边缘设备上使用小型语言模型以降低延迟，但在不确定预测时仍需频繁调用大型语言模型，导致带宽受限环境下的通信开销较大。

**方法:** FedHLM框架结合了联邦学习和不确定性感知推理，协同学习令牌级不确定性阈值，动态决定何时需要大型语言模型的帮助。此外，它还利用基于嵌入的令牌表示进行点对点解析，允许客户端重用语义相似对等方推断的令牌，从而减少对大型语言模型的需求。最后，引入分层模型聚合方法，通过边缘服务器更新本地路由策略并协调全局决策边界。

**结果:** 在大规模新闻分类任务上的实验表明，FedHLM可以将大型语言模型的传输需求减少95%以上，且准确率损失可忽略不计。

**结论:** FedHLM为可扩展和高效的边缘AI应用提供了一种有效的解决方案，能够显著减少通信开销并保持高准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Learning-Enabled+Hybrid+Language+Models+for+Communication-Efficient+Token+Transmission，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00082，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00082&send_immediately=true&force_search=false)

**原文摘要:** Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [38] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的框架 Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN)，解决了当前战略级模拟中战术打击行为与战略延迟之间缺乏结构化因果建模的问题，特别是在捕捉“韧性 - 节点抑制 - 谈判窗口”链中的中间变量的结构性瓶颈。实验结果表明，IA-STGNN 显著优于基线模型（ST-GNN、GCN-LSTM、XGBoost），在减少 MAE 和提高 Top-5 准确性方面表现优异，并且提高了因果路径一致性和干预稳定性。该模型支持核威慑模拟、外交窗口评估和多策略优化等应用，为高级政策建模提供了结构化和透明的人工智能决策支持机制。


<details>
  <summary>更多</summary>
  
**动机:** 当前战略级模拟中存在战术打击行为与战略延迟之间缺乏结构化因果建模的问题，特别是在捕捉“韧性 - 节点抑制 - 谈判窗口”链中的中间变量的结构性瓶颈。

**方法:** 提出了 Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN) 框架，整合了图注意力机制、反事实模拟单元和空间干预节点重建，以实现对打击配置和同步策略的动态模拟。训练数据由多物理模拟平台（GEANT4 + COMSOL）生成，遵循 NIST SP 800-160 标准，确保结构可追溯性和政策级别验证。

**结果:** IA-STGNN 显著优于基线模型（ST-GNN、GCN-LSTM、XGBoost），实现了 MAE 减少 12.8%，Top-5 准确性增加 18.4%，并改善了因果路径一致性和干预稳定性。

**结论:** IA-STGNN 提供了可解释的战略延迟预测，并支持核威慑模拟、外交窗口评估和多策略优化等应用，为高级政策建模提供了一个结构化和透明的人工智能决策支持机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strategic+Counterfactual+Modeling+of+Deep-Target+Airstrike+Systems+via+Intervention-Aware+Spatio-Causal+Graph+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00083&send_immediately=true&force_search=false)

**原文摘要:** This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [39] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang, Dongyao Jia, Eng Gee Lim, Pengfei Fan, Yuli Zhang, Shangbo Wang*

**主要类别:** cs.LG

**AI概要:** GFEN是一种新的网络级交通速度预测框架，通过图融合技术提取时空特征，并结合基于注意力机制的深度学习和数学平滑方法来处理数据异常和非平稳性，实验表明其预测准确率提高6.3%，收敛速度几乎是现有模型的两倍。


<details>
  <summary>更多</summary>
  
**动机:** 当前交通预测方法难以应对交通动态的复杂性和非线性，整合时空特性困难，同时静态技术在处理非平稳和异常历史数据时适应性差且数据平滑效果不佳。

**方法:** 提出Graph Fusion Enhanced Network (GFEN)，包含：1) 一种新的拓扑时空图融合技术，用于从数据分布和网络拓扑中提取并合并时空相关性；2) 混合方法，结合基于k阶差分的数学框架和基于注意力机制的深度学习结构，以自适应平滑历史观测值并动态缓解数据异常和非平稳性。

**结果:** 实验结果表明，GFEN在预测准确率上比最先进的方法高出约6.3%，并且收敛速度几乎为最近混合模型的两倍。

**结论:** GFEN在交通预测方面表现出优越性能，可显著提升交通预测系统的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Joint+Topology-Data+Fusion+Graph+Network+for+Robust+Traffic+Speed+Prediction+with+Data+Anomalism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00085&send_immediately=true&force_search=false)

**原文摘要:** Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [40] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao, Pengzhi Mao, Kaifei Wang, Yiming Li, Yaping Peng, Ranfei Chen, Shuqi Lu, Xiaohong Ji, Jiaxiang Ding, Xin Zhang, Yucheng Liao, Weinan E, Weijie Zhang, Han Wen, Hao Chi*

**主要类别:** cs.LG

**AI概要:** pUniFind是一个大规模多模态预训练模型，用于蛋白质组学中的肽谱评分和从头测序。它在免疫肽组学中提高了42.6%的肽鉴定数量，并且比现有的从头测序方法多识别了60%的PSMs。此外，其深度学习质量控制模块还恢复了额外38.5%的肽。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习推动了质谱数据解释的发展，但大多数模型仍然是特征提取器，而非统一的评分框架。因此，需要一种能够整合端到端肽谱评分和开放、零样本从头测序的模型。

**方法:** pUniFind是一种蛋白质组学中的大规模多模态预训练模型，它通过跨模态预测将光谱和肽模态对齐。该模型基于超过1亿个开放搜索衍生的光谱进行训练，支持1300多种修饰。同时，它包含一个基于深度学习的质量控制模块。

**结果:** 在不同的数据集中，pUniFind的表现优于传统引擎，特别是在免疫肽组学中，肽的鉴定数量增加了42.6%。与现有的从头测序方法相比，尽管搜索空间大了300倍，但pUniFind识别出的PSMs多了60%。此外，其质量控制模块还恢复了额外38.5%的肽。

**结论:** pUniFind建立了一个统一且可扩展的深度学习框架，用于蛋白质组学分析，提供了更高的灵敏度、修饰覆盖率和可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是pUniFind%3A+a+unified+large+pre-trained+deep+learning+model+pushing+the+limit+of+mass+spectra+interpretation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00087&send_immediately=true&force_search=false)

**原文摘要:** Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [41] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi, Pierre Latouche, Arnaud Guillin, Yan Bailly*

**主要类别:** cs.LG

**AI概要:** 提出了一种短期职业事故预测的通用框架，该框架利用安全检查数据，并将事故的发生建模为二元时间序列。通过训练和比较几种机器学习算法，发现长短期记忆（LSTM）网络表现最佳，能够以0.86的平衡准确率检测即将到来的高风险期。此方法可将常规安全检查数据转化为明确的每周风险评分，帮助决策者优化资源配置和预防事故发生。


<details>
  <summary>更多</summary>
  
**动机:** 职业事故预测对于提升工作场所安全性至关重要。现有方法可能未能充分利用安全检查数据进行有效的短期预测，因此需要一种新方法来提高预测精度并更好地支持决策。

**方法:** 1. 提出一个通用框架，将职业事故的发生建模为二元时间序列。
2. 使用安全检查数据生成每日预测，并将其汇总为每周安全评估。
3. 采用滑动窗口交叉验证程序评估模型性能，结合基于聚合周期级别的指标。
4. 训练并比较多种机器学习算法，包括逻辑回归、基于树的模型和神经网络（特别是LSTM）。

**结果:** LSTM模型在预测即将到来的高风险期方面表现优于其他方法，其平衡准确率达到0.86，证明了该方法的稳健性。此外，该方法成功地将安全检查数据转化为清晰的每周风险评分，有助于识别事故最可能发生的时间段。

**结论:** 所提出的二元时间序列模型结合安全检查数据可以有效预测职业事故的高风险期。这种方法不仅提高了预测的准确性，还为决策者提供了工具，以便他们能够在事故发生前采取干预措施，优化资源分配，从而获得最大的安全投资回报。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+new+machine+learning+framework+for+occupational+accidents+forecasting+with+safety+inspections+integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00089&send_immediately=true&force_search=false)

**原文摘要:** We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [42] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael, Claeys Emmanuelle, Serrurier Mathieu, Zaraté Pascale*

**主要类别:** cs.LG

**AI概要:** 在消防员干预情况下，人员和物资资源的分配高度敏感。本文比较了不同的数据生成方法（如随机采样、表格变分自编码器等）以优化消防员响应，并使用领域特定指标评估合成数据的质量。


<details>
  <summary>更多</summary>
  
**动机:** 消防员干预中的人力物力资源分配需要依赖模拟来测试各种场景，因此生成高质量的合成数据至关重要。传统评估指标难以满足实际需求，需引入领域特定指标进行综合评估。

**方法:** 研究比较了多种数据生成方法，包括随机采样、表格变分自编码器、生成对抗网络、条件表格生成对抗网络和扩散概率模型。并结合领域特定指标（如响应时间分布、时空分布等）与标准度量（如Wasserstein距离）评估合成数据质量。

**结果:** 通过领域特定指标和标准度量的结合评估，发现不同数据生成方法在捕捉消防干预复杂性方面表现各异，且由于数据分布的高度不平衡性和非高斯特性，生成过程更具挑战性。

**结论:** 合成数据生成方法的有效性取决于其对消防干预复杂性的捕捉能力。领域特定指标对于评估合成数据的质量至关重要，未来的研究应进一步改进生成方法以适应高度不平衡的数据分布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generating+Heterogeneous+Multi-dimensional+Data+%3A+A+Comparative+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00090，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00090&send_immediately=true&force_search=false)

**原文摘要:** Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [43] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为DFReg的正则化方法，受密度泛函理论（DFT）启发，通过全局权重分布施加功能性惩罚，以促进平滑、多样且分布良好的权重配置。与Dropout或L2衰减等传统技术不同，DFReg在不改变架构或引入随机扰动的情况下实现全局结构正则化。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度神经网络正则化方法如Dropout或L2衰减存在局限性，例如需要架构更改或引入随机扰动。因此，研究者希望开发一种新的正则化方法，能够在不改变网络架构或引入随机性的情况下，提升模型性能和泛化能力。

**方法:** DFReg是一种受密度泛函理论（DFT）启发的正则化方法，它通过对权重的全局分布施加功能性惩罚来实现正则化。这种方法鼓励权重配置平滑、多样且分布良好，从而提高模型的泛化能力。

**结果:** 实验结果表明，DFReg可以有效改善深度神经网络的性能和泛化能力，相较于传统的正则化方法具有优势。

**结论:** DFReg提供了一种全新的正则化视角，能够在全球范围内对权重分布进行调控，而无需改变网络架构或引入随机扰动。这为深度学习模型的优化提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DFReg%3A+A+Physics-Inspired+Framework+for+Global+Weight+Distribution+Regularization+in+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00101&send_immediately=true&force_search=false)

**原文摘要:** We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [44] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann, Patrick Bruendl, Huong Giang Nguyen, Joerg Franke*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种数据驱动且透明的工业故障检测方法，结合监督学习模型、Shapley Additive Explanations解释性工具和领域特定可视化技术。该方法在压接过程数据中实现了95.9%的故障检测准确率，并通过定量扰动分析和专家定性评估验证了解释的相关性和可解释性。此以人为中心的方法旨在提高工业质量控制中的信任和解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现代制造中确保产品质量至关重要，但传统质量控制方法缺乏对生产数据复杂性和变异性适应能力，同时需要大量领域专业知识。而机器学习等数据驱动方法虽然检测性能高，但通常为黑箱模型，限制了其在工业环境中的接受度。因此，需要一种既数据驱动又透明的故障检测方法。

**方法:** 该方法包含三个主要部分：1) 监督学习模型用于多类故障分类；2) 使用Shapley Additive Explanations（SHAP）进行事后解释；3) 领域特定的可视化技术，将模型解释映射到操作员可理解的特征。此外，还提出了一个评估方法，通过定量扰动分析评估模型解释，并通过专家定性评估评价可视化效果。

**结果:** 该方法应用于压接过程数据集（单变量离散时间序列），实现了95.9%的故障检测准确率。定量选择性分析和专家定性评估均证实了解释的相关性和可解释性。

**结论:** 此以人为中心的方法提高了数据驱动故障检测的信任度和解释性，为工业质量控制中的应用系统设计提供了贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+transparent+and+data-driven+fault+detection+in+manufacturing%3A+A+case+study+on+univariate%2C+discrete+time+series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00102&send_immediately=true&force_search=false)

**原文摘要:** Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [45] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano, Ignacio Villanueva*

**主要类别:** cs.LG

**AI概要:** 研究了GNNs在风能预测问题上的适用性，发现某些架构的表现与基于CNN的最佳基准相当。使用三个风电场五年的历史数据进行研究，采用NWP变量作为预测因子，并在24至36小时的预测范围内评估模型。


<details>
  <summary>更多</summary>
  
**动机:** 探索图神经网络（GNNs）在风能预测中的潜力，以期找到可能优于或等效于现有方法（如基于CNN的方法）的新解决方案。

**方法:** 通过使用三个风力发电厂五年的历史数据，以数值天气预报（NWP）变量为预测因子，将GNN的不同架构与基于CNN的基准模型进行比较。

**结果:** 某些GNN架构在风能预测任务中表现良好，达到了与基于CNN的最佳基准模型相当的性能水平。

**结论:** GNNs在风能预测领域具有一定的应用价值，特别是在特定架构下能够实现与CNN基准模型相当的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Neural+Networks+in+Wind+Power+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00105，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00105&send_immediately=true&force_search=false)

**原文摘要:** We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [46] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum, Olivia Kilday, Emilio Salas, Bess Hagan, Reid Williams*

**主要类别:** cs.LG

**AI概要:** 研究了使用扩散模型进行基于文本的游戏关卡生成的方法，提出了自动为关卡数据集分配描述性标题的策略，并训练了扩散模型用于生成可玩的完整关卡。与无条件扩散模型和其他方法相比，最佳模型采用简单变压器进行文本嵌入，训练时间更短，且不依赖大型语言模型。此外，还提供了一个GUI工具，方便设计师构建长关卡。


<details>
  <summary>更多</summary>
  
**动机:** 当前扩散模型在 unconditional 生成瓦片游戏关卡方面已有研究，但对文本到关卡（text-to-level）生成的应用探索不足。需要解决实际问题，如获取标题/关卡对、文本嵌入模型以及生成完整可玩游戏关卡的方法。

**方法:** 1. 自动为现有关卡数据集分配描述性标题。
2. 使用预训练文本编码器和从头训练的简单变压器模型训练扩散模型。
3. 自动生成关卡标题以比较输入输出标题之间的重叠程度。
4. 评估生成关卡的多样性和可玩性。
5. 将结果与无条件扩散模型、生成对抗网络及其它文本到关卡方法（Five-Dollar Model 和 MarioGPT）进行比较。

**结果:** 最佳扩散模型采用了简单变压器模型进行文本嵌入，训练时间比使用更复杂文本编码器的扩散模型更短。实验表明，生成完整可玩关卡的效果良好，且不需要依赖大型语言模型。

**结论:** 研究表明，简单变压器模型在文本到关卡生成任务中表现优异且效率更高。同时，提供的GUI工具可以辅助设计师利用模型生成的场景构建长关卡，展示了该方法的实际应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Text-to-Level+Diffusion+Models+With+Various+Text+Encoders+for+Super+Mario+Bros，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00184，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00184&send_immediately=true&force_search=false)

**原文摘要:** Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [47] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk, Fahad Kamran, Salar Abbaspourazad, Sean Jewell, Harsh Sharma, Yujie Li, Sinead Williamson, Nicholas J Foti, Joseph Futoma*

**主要类别:** cs.LG

**AI概要:** 开发了一个基于超过2.5B小时可穿戴数据的行为信号基础模型，该模型在57个与健康相关的任务中表现出色，尤其是在行为驱动的任务如睡眠预测上。这强调了为可穿戴设备量身定制基础模型设计的重要性，并展示了其在新健康应用中的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管行为数据往往比低级传感器数据更具信息量，但基础模型主要应用于低级传感器数据。因此，研究者希望开发一种针对行为信号的基础模型，以改善健康预测。

**方法:** 使用来自162K个人的超过2.5B小时的可穿戴数据，系统地优化架构和标记化策略，构建一个针对行为信号的基础模型。并在57个健康相关任务中评估该模型的性能。

**结果:** 该模型在各种实际应用中表现出色，包括个体水平分类和时间变化的健康状态预测。尤其在行为驱动的任务（如睡眠预测）中表现优异，且当与原始传感器数据表示结合时进一步提升。

**结论:** 结果表明，针对可穿戴设备量身定制基础模型设计的重要性，并展示了其在新健康应用中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Sensor+Data%3A+Foundation+Models+of+Behavioral+Data+from+Wearables+Improve+Health+Predictions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00191，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00191&send_immediately=true&force_search=false)

**原文摘要:** Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [48] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He, James Joshi*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的PPFL-RDSN框架，用于在保护隐私的同时进行图像重建。该框架结合了联邦学习、本地差分隐私和鲁棒模型水印技术，确保数据安全并维持模型真实性，同时性能接近集中式方法且减轻计算负担。


<details>
  <summary>更多</summary>
  
**动机:** 高质量图像重建在低分辨率输入下具有挑战性，特别是在需要保护隐私的协作场景中，集中式训练存在隐私风险（如数据泄露和推理攻击）及高计算成本的问题。

**方法:** PPFL-RDSN框架整合了联邦学习(FL)、本地差分隐私以及鲁棒模型水印技术，以保证数据不离开本地设备，并保护敏感信息和模型真实性。

**结果:** 实证评估表明，PPFL-RDSN在降低计算负担的同时，实现了与最先进的集中式方法相当的性能，并有效缓解了安全和隐私漏洞。

**结论:** PPFL-RDSN是一种实用的解决方案，适用于安全且保护隐私的协作计算机视觉应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PPFL-RDSN%3A+Privacy-Preserving+Federated+Learning-based+Residual+Dense+Spatial+Networks+for+Encrypted+Lossy+Image+Reconstruction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00230&send_immediately=true&force_search=false)

**原文摘要:** Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [49] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis, Matthew J Darr*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的框架，通过整合ResNet和重新构建的2D Transformer产生的热图来增强模型的可解释性。方法解决了现有技术在时空对齐上的问题，并在临床和工业数据集上取得了显著改进。此外，NLP模块将融合热图转化为领域特定叙述，进一步提升理解度。整体方法提高了决策透明性和时间感知能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的可解释性方法存在时空错位的问题，卷积网络无法捕捉全局上下文，而Transformer缺乏局部精度，这阻碍了在医疗保健和工业监控等关键安全领域的实际应用。

**方法:** 该方法结合了ResNet生成的梯度加权激活图和具有全局加权输入显著性的重新构建的2D Transformer注意力回滚图，形成统一可视化，实现完全的时空对齐，同时保持实时性能。

**结果:** 在PhysioNet数据集上，混合框架达到了94.1%的准确率（F1为0.93）；在UCI Energy Appliance数据集上，回归误差降低至RMSE = 0.28 kWh（R2 = 0.95），比单独的ResNet、Transformer和InceptionTime基线高出3.8-12.4%。NLP模块通过BLEU-4（0.586）和ROUGE-L（0.650）分数验证了其有效性。

**结论:** 通过将可解释性形式化为因果保真度和时空对齐，我们的方法弥合了技术输出与利益相关者理解之间的差距，提供了一个可扩展的解决方案，用于透明、时间感知的决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretable+AI+for+Time-Series%3A+Multi-Model+Heatmap+Fusion+with+Global+Attention+and+NLP-Generated+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00234，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00234&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [50] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni, Vincenzo De Paola, Samuele Delpero, Giovanni Dispoto, Paolo Bonetti, Alessio Russo, Giuseppe Calcagno, Francesco Trovò, Matteo Papini, Alberto Maria Metelli, Marco Mussi, Marcello Restelli*

**主要类别:** cs.LG

**AI概要:** 近年来，强化学习（RL）在模拟环境中取得了显著进展。然而，真实世界的挑战如大状态-动作空间、非平稳性和部分可观测性往往被现有基准所忽视。本文介绍了Gym4ReaL，这是一个支持开发和评估适用于真实场景的RL算法的环境套件。实验表明，标准RL算法在这些任务中表现出色，激励了进一步开发新方法以应对真实世界任务的复杂性。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习虽然在模拟环境中表现优异，但面对真实世界的挑战（如大状态-动作空间、非平稳性和部分可观测性）时，现有基准测试未能充分涵盖这些复杂性。因此，需要一个更贴近真实场景的环境来推动RL算法的发展。

**方法:** 提出了一套名为Gym4ReaL的综合环境，包含多种任务，旨在让RL算法暴露于各种实际挑战中，从而支持其开发与评估。

**结果:** 实验结果表明，在这些设置下，标准RL算法相较于基于规则的基准测试具有竞争力。

**结论:** 通过引入Gym4ReaL，研究者可以更好地发展和评估适用于真实世界场景的RL算法，同时激发了新方法的开发以充分利用RL应对真实世界复杂性的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gym4ReaL%3A+A+Suite+for+Benchmarking+Real-World+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00257，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00257&send_immediately=true&force_search=false)

**原文摘要:** In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [51] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya, Jens Kleesiek, Bharat Rao, Michael Kamp*

**主要类别:** cs.LG

**AI概要:** 在联邦学习中，数据异质性是一个核心挑战，个性化联邦学习（PFL）通过为每个客户端的数据分布量身定制模型来应对这一挑战。然而，许多PFL方法未能超越本地或集中式基线，表明它们的协作方式与数据结构不匹配。本文提出了一种基于自适应协作的方法，其中客户端不仅决定依赖其他客户端的程度，还决定在单个样本级别上信任谁。我们在FEDMOSAIC中实例化了这一原则，这是一种联邦协同训练方法，客户端在一个共享的未标记数据集上交换预测。这使得细粒度的信任决策变得困难，而仅使用参数共享难以实现。每个客户端根据私有和公共数据之间的一致性调整其损失权重，并根据其估计的每个样本置信度按比例贡献全局伪标签。经验证，FEDMOSAIC在多种非IID设置下优于最先进的PFL方法，我们提供了标准假设下的收敛性保证。我们的结果展示了数据感知协作在实现稳健和有效的个性化方面的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的数据异质性是主要挑战，当前的个性化联邦学习方法存在不足，未能超越本地或集中式基准。因此需要一种新的方法，以更好地匹配数据结构并提升性能。

**方法:** 提出了一种基于自适应协作的方法，并在FEDMOSAIC中具体实现。该方法让客户端在共享的未标记数据集上交换预测，从而进行细粒度的信任决策。客户端根据私有和公共数据之间的一致性调整损失权重，并按估计的每个样本置信度贡献全局伪标签。

**结果:** FEDMOSAIC在多个非IID设置下优于现有的最先进PFL方法，并且在标准假设下具有收敛性保证。

**结论:** 数据感知协作具有实现稳健和有效个性化模型的潜力，FEDMOSAIC为此提供了一个成功的示例。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Who+Should+I+Listen+To%3F+Adaptive+Collaboration+in+Personalized+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00259，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00259&send_immediately=true&force_search=false)

**原文摘要:** Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [52] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo, Asieh Abolpour Mofrad, Anis Yazidi, Moises Betancort*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了拒绝关系在刺激等价性获取中的作用，使用计算模型（如前馈神经网络、BERT和GPT）进行匹配样本模拟。结果表明，尽管一些模型在等价测试中表现出高精度，但其性能可能基于联想学习而非真正的刺激等价性。这强调了在计算模型中需要更严格的标准来评估等价性。


<details>
  <summary>更多</summary>
  
**动机:** 探索拒绝关系是否会影响等价类形成的评估，并了解人工神经网络是否能展示出等价类形成还是仅反映联想学习。

**方法:** 采用前馈神经网络、BERT和GPT模型，在18种条件下进行匹配样本模拟。条件包括不同的训练结构、关系类型和负面比较选择方式。同时使用概率代理作为基准对比。

**结果:** 拒绝关系影响了代理的表现。一些代理在等价测试中表现出了高精度，尤其是在有拒绝关系和偏向负面比较的情况下，但其表现与概率代理相当。

**结论:** 人工神经网络可能依赖于联想策略而非刺激等价性，因此在计算模型的等价性评估中需要更加严格的准则和对拒绝关系的仔细考虑。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Examining+Reject+Relations+in+Stimulus+Equivalence+Simulations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00265，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00265&send_immediately=true&force_search=false)

**原文摘要:** Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [53] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan, Martha White, Marlos C. Machado*

**主要类别:** cs.LG

**AI概要:** 在深度强化学习中，过估计是一个普遍存在的问题。本文研究了深度双Q学习（DDQL）算法，该算法基于双Q学习的核心思想，训练两个相互引导的Q函数，以减少过估计。实验表明，DDQL相较于Double DQN减少了过估计，并在57个Atari 2600游戏中表现更优，且无需额外的超参数调整。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习中的过估计问题，特别是Q-learning中的过估计，促使研究者探索改进的方法。虽然Double DQN已经部分解决了这个问题，但其并未完全遵循Double Q-learning的核心思想，即训练两个不同的Q函数进行引导。因此，有必要研究是否可以开发出更符合Double Q-learning原则、进一步减少过估计的深度强化学习算法。

**方法:** 提出了一种名为Deep Double Q-learning (DDQL) 的新算法，该算法严格遵循Double Q-learning的思想，训练两个独立的Q函数，通过这两个Q函数来解耦动作选择和动作评估。与Double DQN不同，DDQL确保两个Q函数在引导过程中保持差异性。此外，还对DDQL的网络架构、重放缓冲比以及小批量采样策略进行了深入研究。

**结果:** 实验证明，DDQL相比Double DQN显著减少了过估计，并在57个Atari 2600游戏的总体表现上优于Double DQN。值得注意的是，这些改进是在无需增加额外超参数的情况下实现的。

**结论:** Deep Double Q-learning (DDQL) 是一种有效减少过估计并提升性能的算法，其效果优于Double DQN，同时保持了算法的简洁性。研究结果表明，在价值型深度强化学习中，严格遵循Double Q-learning的原则能够带来更好的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Double+Q-learning+for+Value-based+Deep+Reinforcement+Learning%2C+Revisited，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00275，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00275&send_immediately=true&force_search=false)

**原文摘要:** Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [54] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma, Juan Diego Draxl Giannoni, Boris Kramer*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种结构保持的 Lift & Learn 方法，通过提升变量转换来学习具有守恒律的非线性偏微分方程（PDE）的结构保持简化模型。该方法结合了能量二次化策略，使用 PDE 非线性知识推导出等效的二次提升系统，并通过约束优化问题学习剩余的线性简化算子。实验结果表明，该方法在准确性和计算效率方面与最先进的结构保持数据驱动模型降阶方法相当。


<details>
  <summary>更多</summary>
  
**动机:** 当前对于非线性偏微分方程（PDE）的简化模型通常缺乏对物理结构和守恒律的保留，这限制了其应用效果。因此，研究者希望开发一种能够学习结构保持的简化模型的方法，以提高模型的准确性和泛化能力。

**方法:** 提出了一种基于能量二次化策略的混合学习方法：1) 使用提升变量转换将非线性 PDE 转换为等效的二次提升系统；2) 推导出解析的二次简化项；3) 通过约束优化问题学习剩余的线性简化算子；4) 构建一个结构保持的二次简化模型。

**结果:** 通过三个数值例子（一维指数非线性波动方程、二维 sine-Gordon 方程和二维 Klein-Gordon-Zakharov 方程），证明了所提出的 Lift & Learn 方法在准确性和计算效率上与最先进的结构保持数据驱动模型降阶方法相当。

**结论:** 结构保持的 Lift & Learn 方法是一种有效的科学机器学习技术，适用于构建具有守恒律的非线性偏微分方程的简化模型。该方法不仅计算高效，而且尊重高维问题的底层物理特性，展示了良好的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Structure-preserving+Lift+%26+Learn%3A+Scientific+machine+learning+for+nonlinear+conservative+partial+differential+equations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00301，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00301&send_immediately=true&force_search=false)

**原文摘要:** This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [55] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang, Runlong Li, Xiaoxiang Liang, Xinhao Yang, Tian Su, Bo Liu, Yan Zhou*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的网络流量预测和异常检测模型MamNet，该模型结合了时域建模和频域特征提取。实验表明，MamNet在准确率、召回率和F1分数方面优于几种最近的主流模型，并且在复杂流量模式和长期趋势检测方面的性能提高了约2%到4%。结果表明，MamNet能够有效地捕捉不同时间尺度上的网络流量异常，适用于网络安全和流量管理中的异常检测任务。未来的工作可以通过引入外部网络事件信息进一步优化模型结构，从而提高模型在复杂网络环境中的适应性和稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 网络流量中的异常波动可能预示着潜在的安全威胁或系统故障，因此高效的网络流量预测和异常检测方法对于网络安全和流量管理至关重要。

**方法:** 论文提出了一个名为MamNet的新模型，该模型集成了时域建模（通过Mamba模块捕获长期依赖性）和频域特征提取（使用傅里叶变换识别周期性波动）。在特征融合层中，整合多尺度信息以增强模型检测网络流量异常的能力。

**结果:** 在UNSW-NB15和CAIDA数据集上的实验表明，MamNet在准确率、召回率和F1-Score方面优于几个最近的主流模型。特别是在复杂流量模式和长期趋势检测方面，性能提升了约2%到4%。

**结论:** MamNet有效地捕捉了不同时间尺度上的网络流量异常，适合用于网络安全和流量管理中的异常检测任务。未来工作可以考虑通过加入外部网络事件信息来进一步优化模型结构，提高其在复杂网络环境中的适应性和稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MamNet%3A+A+Novel+Hybrid+Model+for+Time-Series+Forecasting+and+Frequency+Pattern+Analysis+in+Network+Traffic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00304&send_immediately=true&force_search=false)

**原文摘要:** The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [56] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal, Bodhisattwa Prasad Majumder, Reece Adamson, Megha Chakravorty, Satvika Reddy Gavireddy, Aditya Parashar, Harshit Surana, Bhavana Dalvi Mishra, Andrew McCallum, Ashish Sabharwal, Peter Clark*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种名为AutoDS的方法，用于开放式自主科学发现（ASD）。该方法通过使用贝叶斯惊讶度来驱动科学探索，量化从大语言模型（LLM）的先验信念到后验信念的知识转变。采用蒙特卡洛树搜索策略结合逐步扩展的方式探索假设空间，并以惊讶度作为奖励函数。在21个真实世界数据集上的评估表明，AutoDS在固定预算下比竞争对手多产生5-29%的令人惊讶的发现，并且三分之二的发现被领域专家认为是出乎意料的。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现有的ASD研究依赖于人类指定的研究问题来引导假设生成，但这种方法可能限制了科学发现的加速。开放式的ASD方法虽然存在，但基于多样性和主观有趣性的选择标准分别面临难以有效导航庞大的假设空间和定义不精确的问题。因此，需要一种新的方法来改进假设选择标准，从而更有效地推动科学探索。

**方法:** 论文提出了一种名为AutoDS的方法，利用贝叶斯惊讶度驱动科学探索。具体来说，通过量化大语言模型（LLM）在收集实验结果前后的信念变化，即知识转变，来衡量惊讶度。为了高效探索嵌套假设空间，该方法采用了带有逐步扩展的蒙特卡洛树搜索（MCTS）策略，并以惊讶度作为奖励函数。

**结果:** 在21个涵盖生物学、经济学、金融学和行为科学等领域的真实世界数据集上进行的评估显示，AutoDS在固定预算下显著优于竞争对手，产生的令人惊讶的发现多出5-29%。此外，人类评估结果表明，AutoDS发现中有三分之二被领域专家认为是令人惊讶的。

**结论:** AutoDS代表了构建开放式自主科学发现系统的重要一步。通过使用贝叶斯惊讶度作为驱动探索的标准，AutoDS不仅提高了发现效率，而且其发现得到了领域专家的认可，为未来的ASD研究提供了有价值的参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Open-ended+Scientific+Discovery+via+Bayesian+Surprise，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00310，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00310&send_immediately=true&force_search=false)

**原文摘要:** The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [57] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的多尺度多模态大语言模型μ²LLM，用于放射学报告生成任务。通过μ²Tokenizer整合多模态特征，并通过直接偏好优化提升报告生成质量。实验结果表明，该方法在有限数据下优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 放射学报告生成（RRG）旨在从临床影像中生成详细的文本报告，以提高诊断和管理建议的准确性和效率。然而，RRG面临两个关键挑战：(1) 在资源限制下从影像数据中提取相关信息的固有复杂性；(2) 难以客观评估模型生成报告与专家撰写报告之间的差异。为了解决这些问题，研究提出了μ²LLM。

**方法:** 研究提出了μ²LLM，一种多尺度多模态大语言模型，专门用于RRG任务。其中，μ²Tokenizer作为中间层，整合了多尺度视觉标记化器和文本标记化器的多模态特征，并通过GREEN-RedLlama指导的直接偏好优化（DPO）提升了报告生成的质量。

**结果:** 在四个大型CT图像-报告医学数据集上的实验结果表明，该方法在有限的数据条件下显著优于现有方法，展示了微调后的μ²LLMs在RRG任务中的潜力。

**结论:** μ²LLM通过结合多模态特征和优化技术，在放射学报告生成任务上表现出色，尤其在数据有限的情况下具有优势，这为未来相关研究提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%CE%BC%5E2%24Tokenizer%3A+Differentiable+Multi-Scale+Multi-Modal+Tokenizer+for+Radiology+Report+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00316，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00316&send_immediately=true&force_search=false)

**原文摘要:** Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [58] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin, Ashutosh Singh, Deniz Erdogmus, Georgios Stratis, Lisa Feldman Barrett*

**主要类别:** cs.LG

**AI概要:** 这篇论文重新分析了一个以类型学为导向的研究，该研究报道了个体大脑模式与34种情绪类别的群体平均评分之间的映射。通过采用一种将情绪类别视为可变、情境化实例的人口的观点，重新分析发现，在类别内的不同实例中，大脑模式存在显著变异。这表明初始假设如何最终影响科学结论，并建议在认真对待一个假设之前，必须使用多种分析方法来支持它。


<details>
  <summary>更多</summary>
  
**动机:** 挑战当前情感科学中广泛接受的假设，即民间情感类别形成了一种生物和心理学的分类学。作者认为这种假设会影响研究的设计和分析，从而强化自身。因此，他们希望通过重新分析已有数据，验证基于替代观点的结果是否会有所不同。

**方法:** 重新分析了一项先前报道个体大脑模式与34种情感类别群体平均评分之间映射的研究数据。此次分析基于一种替代观点，即将情感类别视为具有可变性和情境化的实例人口，并预测在同一类别内不同实例的大脑模式会存在显著变异。分析过程中对数据中的方差结构做出了最少的假设。

**结果:** 未观察到原始研究报道的映射关系，而是发现了个体间大脑模式的显著变异。这表明基于不同假设的分析可以得出不同的结论。

**结论:** 初始假设可能显著影响科学研究的结论。因此，在认真对待某个假设之前，应该使用多种分析方法对其进行验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Theory-Laden+Observations+in+the+Brain+Basis+of+Emotional+Experience，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00320&send_immediately=true&force_search=false)

**原文摘要:** In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [59] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang, Xun Yu Zhou*

**主要类别:** cs.LG

**AI概要:** 研究了一类与Huang2024子线性论文相同的连续时间随机线性-二次（LQ）控制问题的强化学习（RL），提出了一种无需模型、数据驱动的探索机制，通过critic自适应调整熵正则化，通过actor调整策略方差。相比之前需要广泛调优且忽略学习进度的方法，新方法提高了学习效率并减少了调优需求。尽管方法灵活，仍达到了与最佳已知模型无关结果相匹配的子线性后悔界，这在以前仅通过固定探索计划得出。数值实验表明，自适应探索加速了收敛，并改善了相对于非自适应模型无关和基于模型方法的后悔表现。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决连续时间随机线性-二次（LQ）控制问题中的强化学习挑战，特别是当波动依赖于状态和控制而状态是标量值且没有运行控制奖励时，研究者希望开发一种无需模型、数据驱动的探索机制，以减少对人工调优的依赖并提高学习效率。

**方法:** 提出了一种无需模型、数据驱动的探索机制，该机制通过critic自适应调整熵正则化，通过actor调整策略方差。这种方法不同于之前使用固定或确定性探索计划的方法，能够根据学习进度自动调整，从而减少调优需求并提高灵活性。

**结果:** 所提出的方法在保持灵活性的同时，实现了与最佳已知模型无关结果相匹配的子线性后悔界。数值实验显示，自适应探索加速了算法收敛，并改善了后悔性能，优于非自适应的模型无关和基于模型的方法。

**结论:** 这种自适应探索机制不仅减少了人工调优的需求，还提高了学习效率，并在理论上达到了与最佳模型无关方法相同的性能水平，同时在实际应用中表现出更好的收敛速度和后悔性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-Driven+Exploration+for+a+Class+of+Continuous-Time+Linear--Quadratic+Reinforcement+Learning+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00358&send_immediately=true&force_search=false)

**原文摘要:** We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [60] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang, Yuxuan Han, Yuxuan Lou, Wangbo Zhao, Yiqi Zhang, Yang You*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的专家剪枝方法Mixture-of-Novices-and-Experts (MoNE)，通过用轻量级的novices替换冗余的experts，实现高效且稳健的模型压缩。实验表明，MoNE在不同维度上均优于基线方法，并在25%和50%的剪枝率下分别提高了平均零样本准确率2.71和3.61。


<details>
  <summary>更多</summary>
  
**动机:** 现有的MoE模型在部署时需要保留所有expert，导致内存开销大。尽管结构化剪枝可以降低内存成本，但现有方法在模型架构、校准数据源和校准样本大小三个维度上表现不佳，性能下降不稳定。因此，需要一种更有效和稳健的方法来减少内存开销并保持性能。

**方法:** 论文提出了一种名为Mixture-of-Novices-and-Experts (MoNE)的新方法。该方法基于访问频率和输出方差两个指标评估expert的冗余性，将低使用率和稳定输出的expert剪枝，并用轻量级的novices代替，以最小化性能下降。

**结果:** 大量实验表明，MoNE在三个维度（模型架构、校准数据源和校准样本大小）上均优于基线方法，且在25%剪枝率下提高九个下游任务的平均零样本准确率2.71，在50%剪枝率下提高3.61。

**结论:** Mixture-of-Novices-and-Experts (MoNE)是一种有效的专家剪枝方法，能够通过用novices替代冗余experts来实现高效且稳健的模型压缩，同时保持甚至提升模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoNE%3A+Replacing+Redundant+Experts+with+Lightweight+Novices+for+Structured+Pruning+of+MoE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00390&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [61] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang, Shenggan Cheng, Xuanlei Zhao, Ziming Liu, Yang You*

**主要类别:** cs.LG

**AI概要:** 随着序列长度增加，现有的流水线并行方法由于二次注意力计算和巨大的内存开销而性能下降。HelixPipe通过引入注意力并行分区、两层先进后出微批次调度以及无注意力重计算和分块MLP等技术解决了这些问题。实验表明，在不同规模的模型和集群配置下，HelixPipe在吞吐量和可扩展性方面优于现有方法，并在训练7B模型（序列长度128k）时比基线方法快26%。


<details>
  <summary>更多</summary>
  
**动机:** 现有的流水线并行方法在处理长序列时，由于二次注意力计算和巨大的内存开销，导致性能次优。因此需要一种新的方法来优化长序列Transformer训练的性能。

**方法:** 提出了一种名为HelixPipe的新流水线并行方法，包含三个关键点：1) 注意力并行分区，减少管道气泡；2) 两层先进后出微批次调度，平衡内存使用并重叠通信与计算；3) 无注意力重计算和分块MLP，缓解碎片化并支持更长序列。

**结果:** 实验结果表明，HelixPipe在长序列长度上具有越来越明显的优势，在不同流水线规模、模型大小和集群配置下表现出色。特别是在使用64个H20 GPU训练7B模型（序列长度128k）时，速度提高了26%。

**结论:** HelixPipe是一种有效的流水线并行方法，可以显著提高长序列Transformer训练的性能，在吞吐量和可扩展性方面优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HelixPipe%3A+Efficient+Distributed+Training+of+Long+Sequence+Transformers+with+Attention+Parallel+Pipeline+Parallelism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00394，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00394&send_immediately=true&force_search=false)

**原文摘要:** As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [62] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan, Xiaohui Zhong, Kangrui Ren, Jiangnan Li, Linqing Huang*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种用于部分标签学习（PLL）的扩散去模糊模型（DDMP），通过利用实例和标签之间的潜在互补信息构建伪干净标签，并引入转换感知矩阵动态更新潜在真实标签，从而提高分类器性能。实验表明DDMP在PLL任务中的优势。


<details>
  <summary>更多</summary>
  
**动机:** 部分标签学习（PLL）旨在从一组候选标签中识别出真实标签，但由于模糊标签导致实例与标签之间的不匹配，降低了生成数据的质量。因此，需要一种新方法来解决这一问题。

**方法:** 1. 从生成模型的角度重新定义标签去模糊问题，通过迭代优化初始随机猜测生成标签。
2. 提出扩散去模糊模型（DDMP），利用实例和标签间的潜在互补信息构建伪干净标签进行初始扩散训练。
3. 引入转换感知矩阵估计潜在真实标签，并在扩散生成过程中动态更新。
4. 在训练过程中逐步优化真实标签以改进分类器。

**结果:** 实验结果表明，所提出的DDMP模型在部分标签学习任务中具有显著优势，并且适合处理PLL问题。

**结论:** 扩散去模糊模型（DDMP）能够有效解决部分标签学习中的标签模糊问题，提高了分类器的性能，为PLL任务提供了一种新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Disambiguation+Models+for+Partial+Label+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00411&send_immediately=true&force_search=false)

**原文摘要:** Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [63] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang, Shuangfei Zhai, Jiatao Gu, Yizhe Zhang, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Josh Susskind, Navdeep Jaitly*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架TarFlowLM，将语言建模从离散的标记空间转移到连续的潜在空间，通过基于Transformer的自回归归一化流来建模这些连续表示。


<details>
  <summary>更多</summary>
  
**动机:** 自回归模型在语言建模方面取得了显著进展，但其对离散标记、单向上下文和单次解码的核心依赖也激发了探索新的设计空间的需求，以提供新的建模灵活性。

**方法:** 提出了TarFlowLM框架，利用基于Transformer的自回归归一化流来建模连续表示。该方法能够捕捉全局双向上下文、支持灵活标记块大小的块生成，并促进分层多遍生成过程。还提出了基于混合的新耦合变换，以捕捉由离散数据塑造的潜在空间中的复杂依赖关系。

**结果:** 广泛的实验表明，在语言建模基准上具有强大的似然性能，并展示了框架固有的灵活建模能力。

**结论:** TarFlowLM为语言建模提供了一个新的范式，通过连续潜在空间和复杂的依赖关系捕捉，展示了强大的性能和灵活性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flexible+Language+Modeling+in+Continuous+Space+with+Transformer-based+Autoregressive+Flows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00425，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00425&send_immediately=true&force_search=false)

**原文摘要:** Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [64] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin, Tianyi Qu, Zihao Wang, Yifan Chen*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的方法，将因果图学习（CGL）技术从分类任务扩展到回归任务。具体来说，作者通过对比学习的视角，重新设计了处理混淆效应的方法，并验证了该方法在图外分布（OOD）基准上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前因果图学习（CGL）技术主要应用于分类任务，在回归任务中的应用尚未得到充分研究。因此，作者希望探索CGL在更具挑战性的图回归任务中的潜力。

**方法:** 作者反思了图级回归中混淆变量的预测能力，并通过对比学习的方式，将原本针对分类任务的因果干预技术推广到回归任务。

**结果:** 广泛的实验结果表明，所提出的方法在图外分布（OOD）基准上对因果图回归（CGR）具有良好的效果。

**结论:** 本文提出的因果图回归方法成功地扩展了因果图学习的应用范围，为处理图回归任务提供了一种新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Recipe+for+Causal+Graph+Regression%3A+Confounding+Effects+Revisited，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00440，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00440&send_immediately=true&force_search=false)

**原文摘要:** Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [65] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su, Xiner Li, Masatoshi Uehara, Sunwoo Kim, Yulai Zhao, Gabriele Scalia, Ehsan Hajiramezanali, Tommaso Biancalani, Degui Zhi, Shuiwang Ji*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种迭代蒸馏微调框架，用于优化扩散模型以适应任意奖励函数。通过收集离策略数据、模拟基于奖励的软最优策略以及最小化KL散度来更新模型，从而提高了训练稳定性和样本效率，并在蛋白质、小分子和调控DNA设计任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 尽管扩散模型擅长生成复杂高维数据分布，但实际应用中需要优化非可微分的奖励函数（如基于物理的模拟或科学知识），这使得传统的RL方法因不稳定、采样效率低和模式崩溃而难以胜任。

**方法:** 提出了一种迭代蒸馏微调框架，将问题视为策略蒸馏：在roll-in阶段收集离策略数据，在roll-out阶段模拟基于奖励的软最优策略，并通过最小化当前模型策略与模拟软最优策略之间的KL散度来更新模型。

**结果:** 实验证明了该方法在蛋白质、小分子和调控DNA设计等多样化任务中的有效性，并展示了其在奖励优化方面的优越性。

**结论:** 所提出的迭代蒸馏微调框架能够有效优化扩散模型以适应任意奖励函数，相较于现有基于RL的方法提升了训练稳定性及样本效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Iterative+Distillation+for+Reward-Guided+Fine-Tuning+of+Diffusion+Models+in+Biomolecular+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00445&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [66] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan, Jianan Zhao, Zhaocheng Zhu, Jian Tang*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了状态空间模型（SSMs）在长上下文建模中的局限性，并提出了结合上下文相关稀疏注意力（CDSA）的改进方法，以及具体实现方案哈希注意与稀疏键选择（HAX）。实验表明HAX在合成和真实世界长上下文任务中均优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管状态空间模型（SSMs）提供了低于二次方复杂度的解决方案，但它们在捕捉长距离依赖方面表现不佳。此外，现有的关联回忆任务不足以代表现实世界长上下文建模的复杂性。因此，需要新的方法和任务来更好地评估和提升SSMs的性能。

**方法:** 1. 提出了一个新的合成任务——联合回忆（joint recall），以更准确地模拟现实世界的长上下文需求。
2. 理论上证明了SSMs无法在亚二次时间复杂度内解决多查询联合回忆问题。
3. 提出了基于集成SSMs与上下文相关稀疏注意力（CDSA）的解决方案。
4. 开发了局部敏感哈希注意与稀疏键选择（HAX），作为理论方案的实际实现，并针对自然语言领域进行了优化。

**结果:** 在合成和真实世界长上下文基准测试上的广泛实验证明，HAX的表现始终优于SSM基线和SSM与上下文无关稀疏注意力（CISA）结合的方法。

**结论:** 通过提出联合回忆任务、理论分析以及HAX的实际实现，本研究有效地提升了SSMs在长上下文建模中的能力，为未来的研究提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Overcoming+Long-Context+Limitations+of+State-Space+Models+via+Context-Dependent+Sparse+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00449，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00449&send_immediately=true&force_search=false)

**原文摘要:** Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [67] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

**主要类别:** cs.LG

**AI概要:** 提出了一种结合全局注意力、分块局部注意力和门控FIFO存储机制的Transformer架构，用于长上下文语言建模。该模型通过旋转位置编码实现方向解耦和尺度不变的位置信号，并在PyTorch中完全从零开始实现，适用于对话建模、代码补全和文档理解等任务。


<details>
  <summary>更多</summary>
  
**动机:** 当前的Transformer模型在处理长上下文时面临计算成本高、难以有效捕捉长距离依赖的问题，因此需要一种更高效的架构来解决这些问题。

**方法:** 将全局注意力与分块局部注意力和门控FIFO存储机制相结合，构建统一的注意力模块；使用旋转位置编码为每个注意力头提供方向解耦且尺度不变的位置信号；整个架构基于PyTorch从零开始实现，无需高级库支持。

**结果:** 此架构能够高效处理短距离和长距离依赖，而不会使注意力成本呈二次增长；同时提供了轻量级和可扩展的设计，适用于多种自然语言处理任务。

**结论:** 所提出的Transformer架构为长上下文语言建模提供了一个有效的解决方案，具有透明性和模块化的特点，适合进一步研究和应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Recurrent+Memory-Augmented+Transformers+with+Chunked+Attention+for+Long-Context+Language+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00453，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00453&send_immediately=true&force_search=false)

**原文摘要:** We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [68] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai, Saurav Bhandari, Girija Bhusal, Saroj Shakya, Tapendra Pandey*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种精炼的随机森林分类器，通过动态生长树、去除不重要的特征以及利用相关性聚类消除冗余树来提高模型效率和准确性。实验表明，在相同数量的树的情况下，该模型在多个基准数据集上比标准随机森林具有更高的分类准确性。


<details>
  <summary>更多</summary>
  
**动机:** 随机森林虽然广泛使用且分类性能强大，但其依赖于大量的树和所有输入特征，导致推理成本高和模型冗余的问题。为了解决这一问题，需要一种更高效的方法来构建随机森林模型。

**方法:** 1. 动态地仅在信息特征上生长树。
2. 通过聚类保留不相关的树以实现最大多样性。
3. 提出精炼随机森林分类器（Refined Random Forest Classifier），该方法迭代优化自身：
   - 首先移除最不重要的特征。
   - 然后分析性地确定应生长的新树的数量。
   - 最后基于相关性进行聚类，以去除冗余树。

**结果:** 在8个多个基准数据集（包括二元和多类别数据集）上的实验表明，与标准随机森林相比，所提出的模型在相同数量的树的情况下实现了更高的分类准确性。

**结论:** 所提出的精炼随机森林分类器能够有效减少模型冗余并提高分类准确性，是一种改进的标准随机森林替代方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diversity+Conscious+Refined+Random+Forest，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00467，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00467&send_immediately=true&force_search=false)

**原文摘要:** Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [69] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang*

**主要类别:** cs.LG

**AI概要:** 本论文探讨了安全强化学习（Safe RL）对后门攻击的脆弱性，引入了包含正负动作样本（PNAct）的后门植入框架，并通过实验验证了该攻击的有效性。这揭示了Safe RL中的潜在风险并证明了此类攻击的可行性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Safe RL在最大化奖励的同时考虑了安全性约束，但目前尚无研究探讨其对后门攻击的防御能力。因此，本文旨在揭示Safe RL中可能存在的后门攻击风险，并提出相应的攻击框架以评估其可行性。

**方法:** 论文首先定义了与后门攻击相关的概念和评估指标，并提出了一个结合正负动作样本（PNAct）的后门攻击框架。其中，正样本提供参考动作，负样本标识应避免的动作。基于此框架，作者设计了一种攻击算法，并从理论上分析了PNAct的特性。

**结果:** 实验结果表明，提出的后门攻击框架在Safe RL中具有较高的有效性，能够成功诱导智能体执行不安全动作，同时验证了所提方法在不同环境下的可行性。

**结论:** 本研究首次揭示了Safe RL对后门攻击的脆弱性，并通过理论分析和实验证明了PNAct框架的有效性。这为未来提升Safe RL系统的鲁棒性和安全性提供了重要参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PNAct%3A+Crafting+Backdoor+Attacks+in+Safe+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00485，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00485&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [70] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son, Joongheon Kim*

**主要类别:** cs.LG

**AI概要:** 量子强化学习(Quantum Reinforcement Learning, QRL)通过参数化量子电路(Parameterized Quantum Circuit, PQC)优化高维问题的处理效率。本文提出QRL-NAS算法，结合量子神经架构搜索(QNAS)，优化PQC结构，实验表明其相较于固定电路的QRL具有更高的奖励值，验证了该方法的有效性和实用性。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习在高维空间中由于维度灾难导致学习效率降低，而量子强化学习利用量子计算中的叠加和纠缠特性，能够以较少资源高效处理高维问题。然而，以往研究中的参数化量子电路结构是基于经验设定的，未验证其最优性。

**方法:** 提出QRL-NAS算法，将量子神经架构搜索(QNAS)与量子强化学习(QRL)相结合，用于优化参数化量子电路(PQC)的结构设计。

**结果:** 实验结果表明，采用QRL-NAS优化后的PQC结构在奖励值上显著高于使用固定电路的传统QRL方法。

**结论:** QRL-NAS算法通过优化PQC结构，提升了量子强化学习的性能，验证了其有效性和实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Circuit+Structure+Optimization+for+Quantum+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00589，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00589&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [71] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao, Miguel Rogel-García, Mohamed Nabail, Xueqian Wang, Nicholas Rhinehart*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于残差奖励模型（RRM）的方法，将环境的真实奖励分为先验奖励和学习奖励两部分，以加速Preference-based Reinforcement Learning (PbRL)的收敛速度，并在多种任务中验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的PbRL方法在奖励模型的训练和微调过程中可能存在优化问题，特别是在使用不同损失函数时。此外，PbRL方法通常收敛较慢。

**方法:** 作者提出了一种残差奖励模型（RRM），将环境的真实奖励表示为先验奖励和学习奖励的总和。先验奖励可以是用户的“最佳猜测”奖励函数或通过逆强化学习（IRL）获得的奖励函数，而学习奖励则通过偏好进行训练。该方法包括基于状态和基于图像的RRM版本。

**结果:** 实验结果表明，RRM显著提高了PbRL方法的性能，适用于各种类型的先验奖励，包括代理奖励、通过IRL获得的奖励以及代理奖励的否定版本。在真实机器人Franka Panda上的实验进一步证明了该方法的有效性，能够以更少的步骤实现成功。

**结论:** RRM方法有效地利用了先验知识，加速了策略学习过程，为PbRL提供了一种新的改进方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Residual+Reward+Models+for+Preference-based+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00611，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00611&send_immediately=true&force_search=false)

**原文摘要:** Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [72] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada, Guillaume Salha-Galvan, Romain Hennequin, Théo Bontempelli, Thomas Bouabça, Tristan Cazenave*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为von Mises-Fisher探索（vMF-exp）的可扩展方法，用于在强化学习问题中探索大规模动作集合，其中超球面嵌入向量表示这些动作。与Boltzmann Exploration相比，vMF-exp在理论上具有相同的探索概率，但在计算效率上更胜一筹，适用于大规模动作集的场景。实验验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Boltzmann Exploration方法在处理大规模动作集时存在计算效率低下的问题，因为它需要为每个动作计算softmax值。因此，需要一种新的方法来有效探索大规模动作集。

**方法:** vMF-exp方法首先通过von Mises-Fisher分布对状态嵌入表示进行采样，然后探索其最近邻的动作，这种方法可以扩展到几乎无限数量的候选动作。

**结果:** 理论分析表明，在一定假设下，vMF-exp渐近地保持与Boltzmann Exploration相同的探索概率。实验结果进一步验证了该方法在模拟数据、真实世界公共数据以及全球音乐流媒体服务推荐系统中的有效性。

**结论:** vMF-exp是一种可扩展的方法，适用于使用超球面嵌入表示动作的大规模动作集探索问题，且其实验效果得到了验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Large+Action+Sets+with+Hyperspherical+Embeddings+using+von+Mises-Fisher+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00518&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [73] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

**主要类别:** cs.LG

**AI概要:** 本研究引入了Cognitive Load-Aware Inference (CLAI)框架，将认知负荷理论应用于大型语言模型推理优化中，提出两种实现路径CLAI-Prompt和CLAI-Tune，显著降低token消耗而不影响准确性，并展示了自主分解复杂问题的能力。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLM）的推理计算成本日益增加，成为其广泛应用和可持续部署的关键障碍。当前的优化策略多依赖统计启发式方法或架构修改，缺乏指导性的认知理论来管理推理过程本身。

**方法:** 通过引入CLAI框架，将认知负荷理论中的内在、外在和相关认知负荷概念形式化为可量化的LLM指标($ICL_{LLM}$, $ECL_{LLM}$, 和$GCL_{LLM}$)，将推理过程重新定义为一个认知经济学优化问题。提出了两种实施路径：CLAI-Prompt（零样本方法，使用结构化元提示引导基础LLM进行认知控制步骤）和CLAI-Tune（微调模型以内在化这些原则实现自发的认知经济）。

**结果:** 在复杂推理、长上下文问答和代码生成等多个基准测试中，这两种方法实现了高达45%的token消耗减少，同时不牺牲准确性。此外，CLAI-Tune表现出自主分解难题的能力，这是人类专家认知的一个关键特征。

**结论:** 通过模拟大脑的资源管理策略，可以构建更高效、更稳健且能力更强的人工智能系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cognitive+Load-Aware+Inference%3A+A+Neuro-Symbolic+Framework+for+Optimizing+the+Token+Economy+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00653&send_immediately=true&force_search=false)

**原文摘要:** The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [74] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan, Xiang Gao, Weicheng Zhu, Shih-Lun Huang, Long Chen, Kyunghyun Cho, Cem M. Deniz, Narges Razavian*

**主要类别:** cs.LG

**AI概要:** 大规模预训练在结构化电子健康记录（EHRs）中的潜力尚未得到充分挖掘。本文提出了一种新的生成式预训练策略，通过预测下一次就诊事件来处理顺序EHR数据，并引入了对重复事件预测的正则化方法。模型在零样本预测痴呆和膝关节骨关节炎发病率的任务中表现出色，性能可与完全微调的掩码预训练Transformer基线相媲美。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大规模预训练在语言和其他数据类型建模方面取得了显著进展，但在具有结构化电子健康记录（EHRs）的医疗保健领域，其潜力尚未得到充分探索。

**方法:** 提出了一种新的生成式预训练策略，使用下一次就诊事件预测来处理顺序EHR数据。模型自回归地生成各种标记化的临床事件，并且能够处理异构数据类型的联合预测。此外，还引入了对重复事件预测的正则化方法，以解决EHR基础模型评估中的关键问题：区分新发事件和后续事件的发生。

**结果:** 在零样本预测痴呆和膝关节骨关节炎发病率的任务中，模型的表现与完全微调的掩码预训练Transformer基线相当，表明该方法能够在不需要昂贵的任务特定微调的情况下捕获复杂的临床依赖关系。

**结论:** 提出的生成式预训练策略为顺序EHR数据提供了有效的解决方案，并且在不进行任务特定微调的情况下能够捕捉复杂的临床依赖关系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+Models+for+Clinical+Records+at+Health+System+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00574&send_immediately=true&force_search=false)

**原文摘要:** Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [75] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh, Khai Le-Duc, Anh Dao, Bach Phan Tat, Chris Ngo, Duy M. H. Nguyen, Nguyen X. Khanh, Thanh Nguyen-Tang*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为Audio-3DVG的框架，该框架将音频和空间信息结合起来用于3D视觉定位任务。通过引入对象提及检测和音频引导注意力模块，该方法在标准数据集上达到了新的最先进水平，并且与基于文本的方法具有竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先前的研究在使用自然语言描述进行3D视觉定位方面取得了一定进展，但利用口语（即基于音频的3D视觉定位）仍然未被充分研究并且充满挑战。受到自动语音识别（ASR）和语音表征学习进步的启发，作者希望探索如何更好地将语音信息整合到3D视觉定位任务中。

**方法:** 作者提出了一个简单而有效的框架，称为Audio-3DVG，它结合了音频和空间信息以提高定位效果。具体来说，他们将语音处理分为两个互补的部分：1) 对象提及检测，这是一个多标签分类任务，用于明确识别音频中提到的对象；2) 音频引导注意力模块，该模块捕捉候选对象和关系语音提示之间的交互，从而提高在杂乱场景中的目标区分能力。为了支持基准测试，作者还为标准3DVG数据集（如ScanRefer、Sr3D和Nr3D）合成了音频描述。

**结果:** 实验结果表明，Audio-3DVG不仅在基于音频的定位任务中实现了新的最先进性能，而且其表现还能与基于文本的方法相媲美。这突显了将口语整合到3D视觉任务中的潜力。

**结论:** 本文提出的Audio-3DVG框架成功地将音频信息整合到3D视觉定位任务中，并取得了显著的成果。这表明，在未来的研究中，可以进一步探索如何更有效地利用口语信息来改善3D视觉任务的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Audio-3DVG%3A+Unified+Audio+-+Point+Cloud+Fusion+for+3D+Visual+Grounding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00669，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00669&send_immediately=true&force_search=false)

**原文摘要:** 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [76] [NN-Former: Rethinking Graph Structure in Neural Architecture Representation](https://arxiv.org/abs/2507.00880)
*Ruihan Xu, Haokui Zhang, Yaowei Wang, Wei Zeng, Shiliang Zhang*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的预测器，结合了图神经网络（GNNs）和变压器的优点，通过引入考虑兄弟节点的新标记混合器和双向图同构前馈网络的新通道混合器，解决了现有方法在复杂特征表示和深度架构泛化上的不足，从而在准确性和延迟预测上取得了一致的优异表现。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习的广泛应用需要高效的网络设计和部署，神经预测器对于估计诸如准确性、延迟等属性至关重要。现有的GNNs和transformers方法分别存在无法表示复杂特征和深度架构泛化能力差的问题。

**方法:** 作者重新思考了神经架构拓扑结构，发现之前研究中被忽视的兄弟节点的重要性，并提出了一个新的预测器，该预测器结合了GNNs和transformers的优势来学习增强的拓扑结构。具体来说，引入了一个新的标记混合器以考虑兄弟节点，并提出了一个名为双向图同构前馈网络的新通道混合器。

**结果:** 该方法在准确性和延迟预测方面均表现出色，为学习有向无环图（DAG）拓扑提供了有价值的见解。

**结论:** 所提出的预测器有效地缓解了GNNs和transformers各自存在的问题，在神经架构属性预测任务中展现了优越性能，为未来的研究提供了新的方向和思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NN-Former%3A+Rethinking+Graph+Structure+in+Neural+Architecture+Representation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00880，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00880&send_immediately=true&force_search=false)

**原文摘要:** The growing use of deep learning necessitates efficient network design and
deployment, making neural predictors vital for estimating attributes such as
accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers
have shown promising performance in representing neural architectures. However,
each of both methods has its disadvantages. GNNs lack the capabilities to
represent complicated features, while transformers face poor generalization
when the depth of architecture grows. To mitigate the above issues, we rethink
neural architecture topology and show that sibling nodes are pivotal while
overlooked in previous research. We thus propose a novel predictor leveraging
the strengths of GNNs and transformers to learn the enhanced topology. We
introduce a novel token mixer that considers siblings, and a new channel mixer
named bidirectional graph isomorphism feed-forward network. Our approach
consistently achieves promising performance in both accuracy and latency
prediction, providing valuable insights for learning Directed Acyclic Graph
(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.

</details>


### [77] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为TARS的方法，通过强化学习训练大模型在处理安全性问题时表现出自适应行为，提高对不同攻击的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自适应分配测试时间计算资源的方法在数学和代码等易于验证的领域提升了大模型性能，但如何利用这种方法训练出具备一定安全漏洞鲁棒性的模型尚未被充分研究。

**方法:** 构建了TARS方法，包含三个关键设计选择：轻量级预热SFT阶段、混合提示词以避免捷径行为、奖励函数防止推理能力退化；使用链式思维轨迹和平衡安全与任务完成的奖励信号进行训练。

**结果:** 使用TARS训练的模型在模糊查询上花费更多计算资源，实现更好的安全-拒绝权衡，并且能够更好地区分安全和不安全提示，增强对白盒和黑盒攻击的鲁棒性。

**结论:** 本工作提供了一个有效的开源方案，通过每提示推理来训练大模型以应对越狱和有害请求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+as+an+Adaptive+Defense+for+Safety，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00971&send_immediately=true&force_search=false)

**原文摘要:** Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful,
harmless, and ambiguous prompts to prevent shortcut behaviors such as too many
refusals, and (3) a reward function to prevent degeneration of reasoning
capabilities during training. Models trained with TARS exhibit adaptive
behaviors by spending more compute on ambiguous queries, leading to better
safety-refusal trade-offs. They also internally learn to better distinguish
between safe and unsafe prompts and attain greater robustness to both white-box
(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an
effective, open recipe for training LLMs against jailbreaks and harmful
requests by reasoning per prompt.

</details>


### [78] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*André Ribeiro, Ana Luiza Tenório, Juan Belieni, Amauri H. Souza, Diego Mesquita*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的模型CSNN，解决了现有sheaf diffusion方法缺乏信息方向性的问题，使其能够实现类似cooperative message passing的行为，提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望探讨现有的sheaf diffusion方法是否能展现出cooperative message passing的行为，并解决oversmoothing和heterophilic数据处理的问题。

**方法:** 引入了在有向图上的cellular sheaves概念，并定义了其in-和out-degree Laplacians，基于此构建了Cooperative Sheaf Neural Networks (CSNNs)。该模型允许节点选择性地接收远处节点的信息并忽略路径上的其他节点，从而缓解了信息压缩（oversquashing）问题。

**结果:** 理论分析表明，CSNN的接收场允许节点选择性关注远处节点而忽略路径上的其他节点，实验结果也显示CSNN相较于之前的sheaf diffusion方法和cooperative graph neural networks具有更好的整体性能。

**结论:** 通过引入cellular sheaves和CSNN，成功实现了sheaf diffusion中的cooperative behavior，并提升了图表示学习的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cooperative+Sheaf+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00647，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00647&send_immediately=true&force_search=false)

**原文摘要:** Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [79] [Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes](https://arxiv.org/abs/2507.01003)
*Eun-Ji Park, Sangwon Yun*

**主要类别:** cs.LG

**AI概要:** 近期研究提出了从遍历性视角解释训练过程。基于此，本文提出一个理解并加速深度神经网络通过随机梯度下降训练的统一框架。通过对目标函数几何景观的分析，引入了最大李雅普诺夫指数的实际诊断方法，并提出了一种用于标准分类器的幽灵类别扩展方法，该方法通过添加辅助幽灵输出节点使模型获得额外的下降方向，从而绕过狭窄损失障碍并加速早期训练阶段。这些结果提供了一个在架构级别上加速早期训练同时保留渐近行为的干预措施。


<details>
  <summary>更多</summary>
  
**动机:** 近期研究提出了从遍历性视角解释训练过程，这为理解深度神经网络的训练提供了新的思路。然而，如何进一步利用这种视角来加速训练仍需探索。

**方法:** 1. 分析目标函数的几何景观，引入最大李雅普诺夫指数的实际诊断方法以区分真实收敛和统计稳定。
2. 提出幽灵类别扩展方法，通过添加辅助幽灵输出节点为模型提供额外的下降方向，帮助优化器绕过不良盆地。
3. 证明幽灵维度在充分收敛后会崩溃，且扩展模型的不变律与原始模型一致。

**结果:** 1. 幽灵类别扩展方法严格减少了近似误差。
2. 在充分收敛后，幽灵维度崩溃，扩展模型的行为与原始模型一致。
3. 存在一条路径使得总损失不增加而原始损失可以减少任意幅度。

**结论:** 本文提出的幽灵类别扩展方法为加速深度神经网络训练提供了一种有效的架构级干预手段，同时保持了模型的渐近行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Description+of+the+Training+Process+of+Neural+Networks+via+Ergodic+Theorem+%3A+Ghost+nodes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01003&send_immediately=true&force_search=false)

**原文摘要:** Recent studies have proposed interpreting the training process from an
ergodic perspective. Building on this foundation we present a unified framework
for understanding and accelerating the training of deep neural networks via
stochastic gradient descent. By analyzing the geometric landscape of the
objective function we introduce a practical diagnostic, the running estimate of
the largest Lyapunov exponent, which provably distinguishes genuine convergence
toward stable minimizers from mere statistical stabilization near saddle
points. We then propose a ghost category extension for standard classifiers
that adds auxiliary ghost output nodes so the model gains extra descent
directions that open a lateral corridor around narrow loss barriers and enable
the optimizer to bypass poor basins during the early training phase. We show
that this extension strictly reduces approximation error and that after
sufficient convergence the ghost dimensions collapse and the extended model's
invariant law coincides with that of the original and there exists a path in
the enlarged parameter space along which the total loss does not increase while
the original loss decreases by an arbitrary margin. Taken together these
results provide a principled architecture level intervention that accelerates
early stage trainability while preserving asymptotic behavior.

</details>


### [80] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp, Davide Belli, Amir Jalalirad, Bence Major*

**主要类别:** cs.LG

**AI概要:** 通过结合道路网络数据和GNSS测量，本研究提出了一种基于深度学习的方法来确定用户在地球上的位置。该方法使用时间图神经网络（TGNN）预测正确的道路段及其不确定性，将其整合到卡尔曼滤波器（KF）中，从而显著减少了定位误差。


<details>
  <summary>更多</summary>
  
**动机:** 全球导航卫星系统 (GNSS) 在密集城市环境中由于多路径和非视距误差的影响，其定位精度常常受到影响。利用道路网络数据可以减少这些误差并提高定位系统的准确性。然而，以往的研究要么仅限于离线应用，要么依赖于灵活性和鲁棒性较低的卡尔曼滤波器 (KF) 启发式算法。

**方法:** 我们提出训练一个时间图神经网络（TGNN），将道路网络信息整合到卡尔曼滤波器（KF）中。设计的TGNN用于预测正确的道路段及其关联的不确定性，并在KF的测量更新步骤中使用。

**结果:** 通过使用真实世界的GNSS数据和开源道路网络验证我们的方法，在具有挑战性的场景中，与仅使用GNSS的KF相比，定位误差减少了29%。

**结论:** 据我们所知，这是第一个联合使用道路网络数据和GNSS测量的深度学习方法，用于确定地球上的用户位置。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Augmented+Kalman+Filters+for+Road+Network+assisted+GNSS+positioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00654，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00654&send_immediately=true&force_search=false)

**原文摘要:** The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [81] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth, Dibyanshu Kumar, Benjamin Paassen, Magda Gregorová*

**主要类别:** cs.LG

**AI概要:** 本研究扩展了分类器引导技术，使其适用于一般的、非鲁棒的分类器，并提出了一种方法来提高分类器引导的稳定性，同时保持样本多样性和视觉质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的分类器引导方法主要依赖于对扩散过程噪声进行专门训练的鲁棒分类器，这限制了其适用范围。为了使一般的、非鲁棒的分类器也能有效用于引导，需要解决这些分类器在噪声条件下的准确率下降和不稳定问题。

**方法:** 分析了非鲁棒和鲁棒分类器在不同数据集上的噪声敏感性，提出了利用一步去噪图像预测和受随机优化方法启发的稳定化技术（如指数移动平均）的方法。

**结果:** 实验结果表明，所提出的方法提高了分类器引导的稳定性，同时保持了样本多样性和视觉质量。

**结论:** 这项工作推动了生成模型中条件采样技术的发展，使得更广泛的分类器可以作为引导分类器使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion+Classifier+Guidance+for+Non-robust+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00687，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00687&send_immediately=true&force_search=false)

**原文摘要:** Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [82] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer, Max Simchowitz, Ali Jadbabaie*

**主要类别:** cs.LG

**AI概要:** This paper introduces a novel framework connecting reinforcement learning (RL) value functions with incremental input-to-state stability ($\delta$ISS). It establishes an equivalence between the regularity of RL-style value functions and a variant of $\delta$ISS under adversarial reward selection.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to bridge the gap between control theory and reinforcement learning by analyzing stability using RL concepts. Specifically, it aims to understand how RL-style value functions relate to incremental input-to-state stability.

**方法:** The method involves developing an equivalence between a variant of incremental input-to-state stability of a closed-loop system under a given policy, and the regularity of RL-style value functions when a Hölder-continuous reward function is adversarially selected.

**结果:** The result shows that there is an equivalence between the regularity of RL-style value functions and a specific form of incremental input-to-state stability, offering a new perspective on stability certification distinct from traditional Lyapunov-based approaches.

**结论:** This work concludes that the regularity of value functions and their connection to incremental stability can be understood differently from the traditional Lyapunov-based approach, providing a novel framework for analyzing stability in control systems using RL concepts.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Test-Function+Approach+to+Incremental+Stability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00695&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


### [83] [SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval](https://arxiv.org/abs/2507.00701)
*Chong Zhang, Xichao Liu, Yibing Zhan, Dapeng Tao, Jun Ni*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的基于空间-通道注意力机制的网络SCAWaveNet用于显著波高（SWH）检索，通过融合多通道信息和空间信息，相比现有模型在ERA5和NDBC数据集上分别降低至少3.52%和5.47%的平均RMSE。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度学习模型主要利用CYGNSS数据的四通道信息，但通常采用单通道输入或简单的通道拼接，未能充分利用跨通道信息交互的优势。

**方法:** 设计了SCAWaveNet网络，将DDMs各通道的特征建模为独立的注意力头，以融合空间和通道信息；辅助参数中引入轻量级注意力机制为不同维度分配权重；最终输出整合了空间和通道级别的特性。

**结果:** 在ERA5参考下，SCAWaveNet的平均RMSE为0.438米；使用NDBC浮标数据时，平均RMSE为0.432米。相比现有最佳模型，在ERA5和NDBC数据集上分别降低了至少3.52%和5.47%的平均RMSE。

**结论:** SCAWaveNet有效融合了空间和通道信息，提升了显著波高检索的精度，并优于当前最先进的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCAWaveNet%3A+A+Spatial-Channel+Attention-based+Network+for+Global+Significant+Wave+Height+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00701&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in spaceborne GNSS missions have produced extensive
global datasets, providing a robust basis for deep learning-based significant
wave height (SWH) retrieval. While existing deep learning models predominantly
utilize CYGNSS data with four-channel information, they often adopt
single-channel inputs or simple channel concatenation without leveraging the
benefits of cross-channel information interaction during training. To address
this limitation, a novel spatial-channel attention-based network, namely
SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each
channel of the DDMs are modeled as independent attention heads, enabling the
fusion of spatial and channel-wise information. For auxiliary parameters, a
lightweight attention mechanism is designed to assign weights along the spatial
and channel dimensions. The final feature integrates both spatial and
channel-level characteristics. Model performance is evaluated using
four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves
an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE
reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the
average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC
buoy observations. The code is available at
https://github.com/Clifx9908/SCAWaveNet.

</details>


### [84] [Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories](https://arxiv.org/abs/2507.00711)
*Jhouben Cuesta-Ramirez, Samuel Beaussant, Mehdi Mounsif*

**主要类别:** cs.LG

**AI概要:** 大型语言模型（LLMs）通过强化学习训练，在推理基准测试中取得了令人印象深刻的结果。然而，有证据表明，这些模型经常生成较长但无效的思维链（CoTs），质疑基准增益是否反映真实的推理改进。本文提出了过度思考的新证据，其中模型即使在明确提供正确解决方案时也会忽略它们，继续生成不必要的推理步骤，最终导致错误结论。使用AIME2024数学基准对三个最先进的模型进行的实验揭示了这些模型在整合纠正信息方面的重要局限性，为实现强大且可解释的推理带来了新的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在推理基准上表现优异，但其生成的思维链可能无效，因此需要进一步研究其真实推理能力及改进方向。

**方法:** 通过对三个最先进的模型进行实验，使用AIME2024数学基准测试，观察模型在面对正确解决方案时的行为，并分析其生成的推理步骤及结果。

**结果:** 实验揭示了模型在整合纠正信息方面的局限性，即即使提供了正确答案，模型仍会继续生成不必要的推理步骤并得出错误结论。

**结论:** 当前大型语言模型存在过度思考的问题，这对其推理能力的真实提升提出了质疑，并为未来研究指出了新方向，以实现更强大和可解释的推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Reasoning+Models+are+not+thinking+straight%3A+on+the+unreliability+of+thinking+trajectories，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00711，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00711&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
recently achieved impressive results on reasoning benchmarks. Yet, growing
evidence shows that these models often generate longer but ineffective chains
of thought (CoTs), calling into question whether benchmark gains reflect real
reasoning improvements. We present new evidence of overthinking, where models
disregard correct solutions even when explicitly provided, instead continuing
to generate unnecessary reasoning steps that often lead to incorrect
conclusions. Experiments on three state-of-the-art models using the AIME2024
math benchmark reveal critical limitations in these models ability to integrate
corrective information, posing new challenges for achieving robust and
interpretable reasoning.

</details>


### [85] [Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction](https://arxiv.org/abs/2507.00733)
*Stefan Haas, Eyke Hüllermeier*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的序数分类中不确定性的度量方法，能够有效分解随机性和认知性不确定性，并在错误检测和分布外检测上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 序数分类问题在医学和金融等高风险领域中很常见，而当前研究主要集中在标称分类和回归上，缺乏对序数分类中不确定性量化的方法。

**方法:** 引入了一类基于二元情况下熵和方差的序数分类不确定性度量方法，通过捕捉精确命中率与最小误差距离之间的权衡来评估不确定性。使用梯度提升树和多层感知器集成进行近似贝叶斯推理。

**结果:** 相比标准和标签级熵及方差基线方法，新方法在错误检测方面显著提高了性能，同时在分布外检测上也表现出竞争力。

**结论:** 考虑序数分类问题中的序性质对于评估不确定性至关重要，所提出的序数不确定性度量方法具有显著优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aleatoric+and+Epistemic+Uncertainty+Measures+for+Ordinal+Classification+through+Binary+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00733&send_immediately=true&force_search=false)

**原文摘要:** Ordinal classification problems, where labels exhibit a natural order, are
prevalent in high-stakes fields such as medicine and finance. Accurate
uncertainty quantification, including the decomposition into aleatoric
(inherent variability) and epistemic (lack of knowledge) components, is crucial
for reliable decision-making. However, existing research has primarily focused
on nominal classification and regression. In this paper, we introduce a novel
class of measures of aleatoric and epistemic uncertainty in ordinal
classification, which is based on a suitable reduction to (entropy- and
variance-based) measures for the binary case. These measures effectively
capture the trade-off in ordinal classification between exact hit-rate and
minimial error distances. We demonstrate the effectiveness of our approach on
various tabular ordinal benchmark datasets using ensembles of gradient-boosted
trees and multi-layer perceptrons for approximate Bayesian inference. Our
method significantly outperforms standard and label-wise entropy and
variance-based measures in error detection, as indicated by misclassification
rates and mean absolute error. Additionally, the ordinal measures show
competitive performance in out-of-distribution (OOD) detection. Our findings
highlight the importance of considering the ordinal nature of classification
problems when assessing uncertainty.

</details>


### [86] [Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports](https://arxiv.org/abs/2507.00742)
*Carlos Caminha, Maria de Lourdes M. Silva, Iago C. Chaves, Felipe T. Brito, Victor A. E. Farias, Javam C. Machado*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了利用大型语言模型（LLMs）从用户提交的设备故障文本报告中识别故障组件的问题。研究评估了27个开源模型和2个专有LLMs，使用了四种提示策略，并进行了大量推理实验。最终发现三个模型在大小和性能之间提供了最佳平衡，能够在现代笔记本电脑或智能手机上高效运行。


<details>
  <summary>更多</summary>
  
**动机:** 用户通过文本报告描述设备故障时，这些报告通常模糊且缺乏细节，导致难以自动识别故障组件。因此，需要一种有效的方法来解决这个问题，而大型语言模型显示出了解决这种问题的潜力。

**方法:** 研究评估了27个开源模型和2个专有LLMs，参数范围从1B到72B。采用了四种不同的提示策略：Zero-Shot、Few-Shot、Chain-of-Thought (CoT) 和 CoT+Few-Shot (CoT+FS)。通过这些策略，进行了98,948次推理实验，处理了超过5100万个输入标记并生成了1300万个输出标记。

**结果:** 实验结果表明，某些模型在性能和规模之间达到了良好的平衡。例如，mistral-small-24b-instruct表现出色，而llama-3.2-1b-instruct和gemma-2-2b-it则以较低的VRAM使用率提供了竞争力的性能，适合在端用户设备上进行高效推理。f1分数最高可达0.76。

**结论:** 本研究表明，特定的LLMs可以有效地从模糊的用户报告中识别出设备故障组件。特别地，一些较小的模型可以在保持高性能的同时降低资源消耗，这使得它们非常适合在现代笔记本电脑或智能手机等端用户设备上部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+LLMs+and+Prompting+Strategies+for+Automated+Hardware+Diagnosis+from+Textual+User-Reports，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00742&send_immediately=true&force_search=false)

**原文摘要:** Computer manufacturers offer platforms for users to describe device faults
using textual reports such as "My screen is flickering". Identifying the faulty
component from the report is essential for automating tests and improving user
experience. However, such reports are often ambiguous and lack detail, making
this task challenging. Large Language Models (LLMs) have shown promise in
addressing such issues. This study evaluates 27 open-source models (1B-72B
parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,
Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted
98,948 inferences, processing over 51 million input tokens and generating 13
million output tokens. We achieve f1-score up to 0.76. Results show that three
models offer the best balance between size and performance:
mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and
gemma-2-2b-it, that offer competitive performance with lower VRAM usage,
enabling efficient inference on end-user devices as modern laptops or
smartphones with NPUs.

</details>


### [87] [A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model](https://arxiv.org/abs/2507.00761)
*Wenbo Yu, Anirbit Ghosh, Tobias Sebastian Finn, Rossella Arcucci, Marc Bocquet, Sibo Cheng*

**主要类别:** cs.LG

**AI概要:** 生成式AI用于模拟复杂自然过程，本文提出首个去噪扩散模型预测野火蔓延，考虑了野火动态的固有不确定性，产生一系列可能的情景预测，有助于改进火灾风险评估和响应规划。


<details>
  <summary>更多</summary>
  
**动机:** 当前预测野火传播困难，因其不可预测性及依赖多样的环境条件，传统模型通常无法表示野火动态的固有不确定性。

**方法:** 采用去噪扩散模型，学习模拟野火传播，不仅作为单一固定结果，而是作为一系列可能情景，反映物理上有意义的分布。

**结果:** 模型能够生成反映野火可能传播位置的物理有意义分布的预测集合，相比确定性方法更可靠。

**结论:** 该技术可帮助开发更智能、更快、更可靠的工具来预测野火行为，辅助决策者进行火灾风险评估和响应规划。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Probabilistic+Approach+to+Wildfire+Spread+Prediction+Using+a+Denoising+Diffusion+Surrogate+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00761&send_immediately=true&force_search=false)

**原文摘要:** Thanks to recent advances in generative AI, computers can now simulate
realistic and complex natural processes. We apply this capability to predict
how wildfires spread, a task made difficult by the unpredictable nature of fire
and the variety of environmental conditions it depends on. In this study, We
present the first denoising diffusion model for predicting wildfire spread, a
new kind of AI framework that learns to simulate fires not just as one fixed
outcome, but as a range of possible scenarios. By doing so, it accounts for the
inherent uncertainty of wildfire dynamics, a feature that traditional models
typically fail to represent. Unlike deterministic approaches that generate a
single prediction, our model produces ensembles of forecasts that reflect
physically meaningful distributions of where fire might go next. This
technology could help us develop smarter, faster, and more reliable tools for
anticipating wildfire behavior, aiding decision-makers in fire risk assessment
and response planning.

</details>


### [88] [Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments](https://arxiv.org/abs/2507.00762)
*Tom Maus, Asma Atamna, Tobias Glasmachers*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了在工业启发的排序环境中使用遗传算法（GAs）来提高强化学习（RL）性能的方法。提出了一种新方法，将GA生成的专家演示整合到DQN重放缓冲区中，并作为PPO代理的预热轨迹，以加速训练收敛。实验表明，GA生成的演示显著提高了RL性能，特别是PPO代理在GA生成的数据初始化后获得了更高的累积奖励，展示了混合学习范式的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习在某些实际工业应用中显示出巨大潜力，但其广泛应用受限于样本效率低下和学习动态不稳定等固有问题。因此，需要寻找改进RL性能的方法。

**方法:** 利用遗传算法生成专家演示数据，并将其用于增强策略学习。具体来说，这些演示被整合到Deep Q-Network的重放缓冲区中进行经验学习，同时用作Proximal Policy Optimization代理的预热轨迹以加速训练收敛。

**结果:** 实验比较了标准RL训练与基于规则的启发式、暴力优化和演示数据的结果，发现GA生成的演示显著提高了RL性能。特别是，使用GA生成的数据初始化的PPO代理获得了更高的累积奖励。

**结论:** 研究表明，遗传算法生成的演示可以有效提升强化学习的性能，尤其是在工业环境中的应用。这突显了混合学习范式的潜力，其中启发式搜索方法可以补充数据驱动的RL。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Genetic+Algorithms+for+Efficient+Demonstration+Generation+in+Real-World+Reinforcement+Learning+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00762&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has demonstrated significant potential in certain
real-world industrial applications, yet its broader deployment remains limited
by inherent challenges such as sample inefficiency and unstable learning
dynamics. This study investigates the utilization of Genetic Algorithms (GAs)
as a mechanism for improving RL performance in an industrially inspired sorting
environment. We propose a novel approach in which GA-generated expert
demonstrations are used to enhance policy learning. These demonstrations are
incorporated into a Deep Q-Network (DQN) replay buffer for experience-based
learning and utilized as warm-start trajectories for Proximal Policy
Optimization (PPO) agents to accelerate training convergence. Our experiments
compare standard RL training with rule-based heuristics, brute-force
optimization, and demonstration data, revealing that GA-derived demonstrations
significantly improve RL performance. Notably, PPO agents initialized with
GA-generated data achieved superior cumulative rewards, highlighting the
potential of hybrid learning paradigms, where heuristic search methods
complement data-driven RL. The utilized framework is publicly available and
enables further research into adaptive RL strategies for real-world
applications.

</details>


### [89] [BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation](https://arxiv.org/abs/2507.00846)
*Rishal Aggrwal, Jacky Chen, Nicholas M. Boffi, David Ryan Koes*

**主要类别:** cs.LG

**AI概要:** 通过能量模型和随机插值结合的方法，避免了计算昂贵的雅可比矩阵，从而在分子系统中有效生成接近玻尔兹曼分布的样本，并显著加速自由能差异计算。


<details>
  <summary>更多</summary>
  
**动机:** 从玻尔兹曼分布中高效采样对于建模物理系统（如分子）至关重要，但传统方法因需要计算昂贵的雅可比矩阵而不适用于大型分子系统。

**方法:** 提出了一种结合能量模型和随机插值的方法，使用噪声对比估计和得分匹配训练能量模型以学习生成分布的似然性，同时利用随机插值在先验和生成分布之间进行退火，结合两种目标函数以高效学习密度函数。

**结果:** 在丙氨酸二肽系统上，该方法生成的自由能曲线和能量分布与使用精确似然性获得的结果相当，并且能够以数量级的速度提升准确估计亚稳态之间的自由能差异。

**结论:** 所提出的方法成功克服了传统方法中计算雅可比矩阵的瓶颈，为大型分子系统的高效采样提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BoltzNCE%3A+Learning+Likelihoods+for+Boltzmann+Generation+with+Stochastic+Interpolants+and+Noise+Contrastive+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00846，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00846&send_immediately=true&force_search=false)

**原文摘要:** Efficient sampling from the Boltzmann distribution defined by an energy
function is a key challenge in modeling physical systems such as molecules.
Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows
that transform a simple prior into a distribution that can be reweighted to
match the Boltzmann distribution using sample likelihoods. However, obtaining
likelihoods requires computing costly Jacobians during integration, making it
impractical for large molecular systems. To overcome this, we propose learning
the likelihood of the generated distribution via an energy-based model trained
with noise contrastive estimation and score matching. By using stochastic
interpolants to anneal between the prior and generated distributions, we
combine both the objective functions to efficiently learn the density function.
On the alanine dipeptide system, we demonstrate that our method yields free
energy profiles and energy distributions comparable to those obtained with
exact likelihoods. Additionally, we show that free energy differences between
metastable states can be estimated accurately with orders-of-magnitude speedup.

</details>


### [90] [Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters](https://arxiv.org/abs/2507.00848)
*Don Roosan, Saif Nirzhor, Rubayat Khan, Fahmida Hai, Mohammad Rifat Haidar*

**主要类别:** cs.LG

**AI概要:** 论文利用量子加速机器学习分析了HIV流行病学数据，展示了更高的聚类检测和预测准确性，并揭示了关键的社会决定因素。


<details>
  <summary>更多</summary>
  
**动机:** HIV流行病学数据日益复杂，需要更先进的计算方法来进行精确的集群检测和预测。

**方法:** 比较经典聚类算法与量子近似优化算法（QAOA），开发混合量子-经典神经网络进行HIV流行率预测，并使用量子贝叶斯网络探索社会决定因素与HIV发病率之间的因果关系。

**结果:** QAOA方法在1.6秒内达到了92%的聚类检测准确度，混合量子-经典神经网络预测HIV流行率达到94%的准确度，量子贝叶斯分析确定住房不稳定是HIV集群出现和扩展的关键驱动因素。

**结论:** 量子增强方法提高了HIV监测的精度和效率，揭示了关键因果途径，可指导目标干预、优化PrEP资源分配并解决推动HIV传播的结构性不平等。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Approximate+Optimization+Algorithm+for+Spatiotemporal+Forecasting+of+HIV+Clusters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00848&send_immediately=true&force_search=false)

**原文摘要:** HIV epidemiological data is increasingly complex, requiring advanced
computation for accurate cluster detection and forecasting. We employed
quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code
level using AIDSVu and synthetic SDoH data for 2022. Our approach compared
classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization
algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV
prevalence forecasting, and used quantum Bayesian networks to explore causal
links between SDoH factors and HIV incidence. The QAOA-based method achieved
92% accuracy in cluster detection within 1.6 seconds, outperforming classical
algorithms. Meanwhile, the hybrid quantum-classical neural network predicted
HIV prevalence with 94% accuracy, surpassing a purely classical counterpart.
Quantum Bayesian analysis identified housing instability as a key driver of HIV
cluster emergence and expansion, with stigma exerting a geographically variable
influence. These quantum-enhanced methods deliver greater precision and
efficiency in HIV surveillance while illuminating critical causal pathways.
This work can guide targeted interventions, optimize resource allocation for
PrEP, and address structural inequities fueling HIV transmission.

</details>


### [91] [Aligning Learning and Endogenous Decision-Making](https://arxiv.org/abs/2507.00851)
*Rares Cristian, Pavithra Harsha, Georgia Perakis, Brian Quanz*

**主要类别:** cs.LG

**AI概要:** 论文针对内生不确定性下的决策问题，提出了一种端到端方法及鲁棒优化变体，结合两阶段随机优化框架，解决了信息收集与决策问题，并在定价和库存选择任务中展现出优于现有方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 许多观察结果受到决策的影响，例如物品需求受价格影响，线上结算选择受陈列选项影响。在这种情况下，决策缺乏反事实信息，需要学习这些信息以支持决策。传统方法可能无法充分考虑这种内生不确定性，因此需要一种新方法来解决这一挑战。

**方法:** 提出了一种端到端方法，在内生不确定性下训练机器学习模型以提高其在决策阶段的有效性。同时引入了一个鲁棒优化变体，通过构建ML模型空间上的不确定性集来优化行动，以应对最坏情况的预测。此外，还引入了一类新的两阶段随机优化问题，其中第一阶段为信息收集问题，决定对哪个随机变量进行采样以获取信息，第二阶段基于这些信息做出决策。

**结果:** 理论分析证明了该鲁棒方法能够在高概率下捕获接近最优的决策。计算实验展示了该方法在定价和库存选择/推荐问题中的优越性能，相较于在线学习、多臂老虎机和离线强化学习等现有方法，表现出了更一致的改进。

**结论:** 该研究提出了一种端到端方法，能够有效解决内生不确定性下的决策问题，并通过鲁棒优化方法确保在高概率下获得接近最优的决策。此外，还引入了两阶段随机优化问题的新类别，进一步扩展了框架的应用范围。实验结果表明，相比现有方法，该方法在定价和库存选择等问题上具有更一致的性能改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aligning+Learning+and+Endogenous+Decision-Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00851，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00851&send_immediately=true&force_search=false)

**原文摘要:** Many of the observations we make are biased by our decisions. For instance,
the demand of items is impacted by the prices set, and online checkout choices
are influenced by the assortments presented. The challenge in decision-making
under this setting is the lack of counterfactual information, and the need to
learn it instead. We introduce an end-to-end method under endogenous
uncertainty to train ML models to be aware of their downstream, enabling their
effective use in the decision-making stage. We further introduce a robust
optimization variant that accounts for uncertainty in ML models -- specifically
by constructing uncertainty sets over the space of ML models and optimizing
actions to protect against worst-case predictions. We prove guarantees that
this robust approach can capture near-optimal decisions with high probability
as a function of data. Besides this, we also introduce a new class of two-stage
stochastic optimization problems to the end-to-end learning framework that can
now be addressed through our framework. Here, the first stage is an
information-gathering problem to decide which random variable to poll and gain
information about before making a second-stage decision based off of it. We
present several computational experiments for pricing and inventory
assortment/recommendation problems. We compare against existing methods in
online learning/bandits/offline reinforcement learning and show our approach
has consistent improved performance over these. Just as in the endogenous
setting, the model's prediction also depends on the first-stage decision made.
While this decision does not affect the random variable in this setting, it
does affect the correct point forecast that should be made.

</details>


### [92] [Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals](https://arxiv.org/abs/2507.00862)
*Davide Andreoletti, Aris Marcolongo, Natasa Sarafijanovic Djukic, Julien Roulet, Stefano Billeter, Andrzej Kurenda, Margot Visse-Mansiaux, Brice Dupuis, Carrol Annette Plummer, Beatrice Paoli, Omran Ayoub*

**主要类别:** cs.LG

**AI概要:** 准确预测马铃薯发芽对于储存管理至关重要，特别是在任何视觉迹象出现之前。由于健康和环境问题，CIPC被禁用后，替代的防发芽化学物质（ASCs）成本显著增加。目前的方法主要依赖于视觉识别，这限制了其主动性。本文提出了一种基于机器学习的新方法，利用专有传感器记录的电生理信号进行早期预测，并通过小波域特征提取和监督学习模型进行检测，同时结合不确定性量化技术以提高预测准确性。实验结果表明该方法在部分马铃薯上能准确预测发芽日期，但还需要进一步改进以减少最大偏差。


<details>
  <summary>更多</summary>
  
**动机:** 有效预测马铃薯发芽对于保持其商业和营养价值、优化存储管理和减少浪费非常重要。随着CIPC的禁用，更昂贵的ASCs被采用，因此需要一种可靠的早期预测方法来降低存储成本并提高效率。

**方法:** 提出了一种基于机器学习的方法，使用专有传感器记录的电生理信号进行早期预测。具体步骤包括：信号预处理、从小波域中提取相关特征、训练监督学习模型进行早期发芽检测，并结合不确定性量化技术以增强预测效果。

**结果:** 实验结果展示了该方法在早期检测马铃薯发芽方面的良好性能，能够准确预测部分马铃薯的发芽日期，并且在整个数据集上表现出可接受的平均误差。然而，仍需进一步改进以减少最大预测偏差。

**结论:** 虽然所提出的方法显示出早期预测马铃薯发芽的潜力，但在减少预测误差特别是最大偏差方面仍有改进空间。未来的研究应集中在优化模型和进一步验证其在更大规模数据集上的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Machine+Learning-based+Early+Detection+of+Potato+Sprouting+Using+Electrophysiological+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00862，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00862&send_immediately=true&force_search=false)

**原文摘要:** Accurately predicting potato sprouting before the emergence of any visual
signs is critical for effective storage management, as sprouting degrades both
the commercial and nutritional value of tubers. Effective forecasting allows
for the precise application of anti-sprouting chemicals (ASCs), minimizing
waste and reducing costs. This need has become even more pressing following the
ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to
health and environmental concerns, which has led to the adoption of
significantly more expensive alternative ASCs. Existing approaches primarily
rely on visual identification, which only detects sprouting after morphological
changes have occurred, limiting their effectiveness for proactive management. A
reliable early prediction method is therefore essential to enable timely
intervention and improve the efficiency of post-harvest storage strategies,
where early refers to detecting sprouting before any visible signs appear. In
this work, we address the problem of early prediction of potato sprouting. To
this end, we propose a novel machine learning (ML)-based approach that enables
early prediction of potato sprouting using electrophysiological signals
recorded from tubers using proprietary sensors. Our approach preprocesses the
recorded signals, extracts relevant features from the wavelet domain, and
trains supervised ML models for early sprouting detection. Additionally, we
incorporate uncertainty quantification techniques to enhance predictions.
Experimental results demonstrate promising performance in the early detection
of potato sprouting by accurately predicting the exact day of sprouting for a
subset of potatoes and while showing acceptable average error across all
potatoes. Despite promising results, further refinements are necessary to
minimize prediction errors, particularly in reducing the maximum observed
deviations.

</details>


### [93] [TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality](https://arxiv.org/abs/2507.00899)
*Carlos Vonessen, Charles Harris, Miruna Cretu, Pietro Liò*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为TABASCO的新模型，该模型通过简化架构和非对称编码方法，在GEOM-Drugs基准上达到了最先进的PoseBusters有效性，并且推理速度比最强的基线快约10倍。


<details>
  <summary>更多</summary>
  
**动机:** 当前最先进的3D分子生成模型依赖于显著的归纳偏置，如SE(3)、置换等变性以尊重对称性和图消息传递网络以捕捉局部化学性质，但生成的分子在物理合理性方面仍然存在挑战。因此，研究者希望开发一种新模型，能够在保持高效的同时生成更符合物理规律的分子。

**方法:** 引入了TABASCO模型，其采用标准的非等变变压器架构，将分子中的原子视为序列，并在生成后确定性地重建键。该模型去除了等变层和消息传递，从而极大地简化了模型架构并提高了数据吞吐量。

**结果:** 在GEOM-Drugs基准测试中，TABASCO实现了最先进的PoseBusters有效性，并且推理速度比最强基线快约10倍，同时展现出尽管未硬编码对称性却具有出现旋转等变性的特性。

**结论:** 这项工作为训练极简主义、高吞吐量的生成模型提供了蓝图，这些模型适合诸如基于结构和药效团的药物设计等专门任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TABASCO%3A+A+Fast%2C+Simplified+Model+for+Molecular+Generation+with+Improved+Physical+Quality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00899&send_immediately=true&force_search=false)

**原文摘要:** State-of-the-art models for 3D molecular generation are based on significant
inductive biases, SE(3), permutation equivariance to respect symmetry and graph
message-passing networks to capture local chemistry, yet the generated
molecules still struggle with physical plausibility. We introduce TABASCO which
relaxes these assumptions: The model has a standard non-equivariant transformer
architecture, treats atoms in a molecule as sequences and reconstructs bonds
deterministically after generation. The absence of equivariant layers and
message passing allows us to significantly simplify the model architecture and
scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves
state-of-the-art PoseBusters validity and delivers inference roughly 10x faster
than the strongest baseline, while exhibiting emergent rotational equivariance
despite symmetry not being hard-coded. Our work offers a blueprint for training
minimalist, high-throughput generative models suited to specialised tasks such
as structure- and pharmacophore-based drug design. We provide a link to our
implementation at github.com/carlosinator/tabasco.

</details>


### [94] [Privacy-Preserving Quantized Federated Learning with Diverse Precision](https://arxiv.org/abs/2507.00920)
*Dang Qua Nguyen, Morteza Hashemi, Erik Perrins, Sergiy A. Vorobyov, David J. Love, Taejoon Kim*

**主要类别:** cs.LG

**AI概要:** 联邦学习（FL）是一种有前景的分布式机器学习范式，但存在隐私风险和模型量化异质性等问题。本文提出了一种新的随机量化器（SQ），可以在保护隐私的同时最小化量化误差，并通过集群优化和线性融合方法解决量化异质性问题。实验表明，该方法在隐私保护和学习效用方面优于传统算法。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习尽管具有优势，但面临隐私风险（如本地模型更新传输中的隐私泄露）和因设备间量化分辨率不同而导致的学习效用下降等问题。现有工作通常只能解决其中一个挑战，难以同时应对隐私保护和量化异质性的问题。

**方法:** 1. 提出一种新的随机量化器（SQ），可同时实现差分隐私（DP）和最小量化误差，并保证有界的失真。
2. 引入集群规模优化技术和线性融合方法，以提高模型聚合的准确性，解决量化异质性问题。
3. 通过数值模拟验证所提出方法的有效性。

**结果:** 与传统的LaplaceSQ-FL算法相比，所提出的方法在隐私保护和学习效用方面表现更优。

**结论:** 本文提出的随机量化器（SQ）和相关技术可以有效提升隐私保护下的联邦学习的学习效用，特别是在存在量化异质性的情况下。这为未来在隐私保护和学习效用之间取得平衡的研究提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Preserving+Quantized+Federated+Learning+with+Diverse+Precision，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00920&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) has emerged as a promising paradigm for distributed
machine learning, enabling collaborative training of a global model across
multiple local devices without requiring them to share raw data. Despite its
advancements, FL is limited by factors such as: (i) privacy risks arising from
the unprotected transmission of local model updates to the fusion center (FC)
and (ii) decreased learning utility caused by heterogeneity in model
quantization resolution across participating devices. Prior work typically
addresses only one of these challenges because maintaining learning utility
under both privacy risks and quantization heterogeneity is a non-trivial task.
In this paper, our aim is therefore to improve the learning utility of a
privacy-preserving FL that allows clusters of devices with different
quantization resolutions to participate in each FL round. Specifically, we
introduce a novel stochastic quantizer (SQ) that is designed to simultaneously
achieve differential privacy (DP) and minimum quantization error. Notably, the
proposed SQ guarantees bounded distortion, unlike other DP approaches. To
address quantization heterogeneity, we introduce a cluster size optimization
technique combined with a linear fusion approach to enhance model aggregation
accuracy. Numerical simulations validate the benefits of our approach in terms
of privacy protection and learning utility compared to the conventional
LaplaceSQ-FL algorithm.

</details>


### [95] [Understanding Generalization in Node and Link Prediction](https://arxiv.org/abs/2507.00927)
*Antonis Vasileiou, Timo Stoll, Christopher Morris*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种统一框架，用于分析MPNN在归纳和转换节点及链接预测中的泛化特性，考虑了架构参数、损失函数和图结构的影响，并通过实证研究支持了理论见解。


<details>
  <summary>更多</summary>
  
**动机:** 尽管MPNN在实际应用中表现出色，但其泛化能力尚未被充分理解，特别是节点和链接预测任务中的泛化问题。现有研究依赖不切实际的假设，忽略了节点或链接之间的相关性以及图结构的影响。

**方法:** 引入一个统一框架来分析MPNN在归纳和转换节点及链接预测设置中的泛化属性，包括多种架构参数和损失函数，同时量化图结构的影响。该框架还可扩展到任何分类任务。

**结果:** 实证研究表明，所提出的理论框架能够有效加深对MPNN在节点和链接预测任务中泛化能力的理解。

**结论:** 提出了一个统一的泛化分析框架，适用于MPNN在节点和链接预测任务中的泛化性能分析，并展示了其在其他分类任务中的潜在应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Generalization+in+Node+and+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00927&send_immediately=true&force_search=false)

**原文摘要:** Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.

</details>


### [96] [Time Series Foundation Models are Flow Predictors](https://arxiv.org/abs/2507.00945)
*Massimiliano Luca, Ciro Beneduce, Bruno Lepri*

**主要类别:** cs.LG

**AI概要:** 研究了时间序列基础模型（TSFMs）在人群流动预测中的有效性，特别是Moirai和TimesFM。通过三个真实世界的移动数据集进行评估，在严格的零样本设置下部署这些模型，仅使用每个OD流的时间演变而不使用明确的空间信息。Moirai和TimesFM的表现优于统计和深度学习基线模型，与现有最佳竞争对手相比，RMSE降低多达33%，MAE降低39%，CPC高出49%。结果表明，即使在标注数据有限或空间背景缺失的情况下，TSFMs在准确且可扩展的流量预测中具有实际价值。


<details>
  <summary>更多</summary>
  
**动机:** 为了探究时间序列基础模型（TSFMs）在人群流动预测中的表现，并验证其在没有明确空间信息情况下的适用性。

**方法:** 使用Moirai和TimesFM两种模型，在三个真实世界的数据集上进行评估，采用严格的零样本设置，仅利用OD流的时间演变数据进行预测。

**结果:** Moirai和TimesFM在预测效果上显著优于统计和深度学习基线模型，分别在RMSE、MAE和CPC指标上有大幅改进。

**结论:** 时间序列基础模型（TSFMs）在人群流动预测中具有高准确性、可扩展性，即使在缺乏标注数据或空间背景的情况下也能提供实用的预测能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+Series+Foundation+Models+are+Flow+Predictors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00945，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00945&send_immediately=true&force_search=false)

**原文摘要:** We investigate the effectiveness of time series foundation models (TSFMs) for
crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three
real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD
flows-these models are deployed in a strict zero-shot setting, using only the
temporal evolution of each OD flow and no explicit spatial information. Moirai
and TimesFM outperform both statistical and deep learning baselines, achieving
up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to
state-of-the-art competitors. Our results highlight the practical value of
TSFMs for accurate, scalable flow prediction, even in scenarios with limited
annotated data or missing spatial context.

</details>


### [97] [Benchmarking the Discovery Engine](https://arxiv.org/abs/2507.00964)
*Jack Foxabbott, Arush Tagade, Andrew Cusick, Robbie McCorkell, Leo McKee-Reid, Jugal Patel, Jamie Rumbelow, Jessica Rumbelow, Zohreh Shams*

**主要类别:** cs.LG

**AI概要:** The Discovery Engine is a new standard for automated, interpretable scientific modelling that enables complex knowledge discovery from data.


<details>
  <summary>更多</summary>
  
**动机:** To create a general purpose automated system for scientific discovery that combines machine learning with state-of-the-art ML interpretability.

**方法:** Benchmark the Discovery Engine against five recent peer-reviewed scientific publications applying machine learning across different fields.

**结果:** In each case, the Discovery Engine matches or exceeds prior predictive performance while also generating deeper, more actionable insights through rich interpretability artefacts.

**结论:** Demonstrates its potential as a new standard for automated, interpretable scientific modelling.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+the+Discovery+Engine，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00964，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00964&send_immediately=true&force_search=false)

**原文摘要:** The Discovery Engine is a general purpose automated system for scientific
discovery, which combines machine learning with state-of-the-art ML
interpretability to enable rapid and robust scientific insight across diverse
datasets. In this paper, we benchmark the Discovery Engine against five recent
peer-reviewed scientific publications applying machine learning across
medicine, materials science, social science, and environmental science. In each
case, the Discovery Engine matches or exceeds prior predictive performance
while also generating deeper, more actionable insights through rich
interpretability artefacts. These results demonstrate its potential as a new
standard for automated, interpretable scientific modelling that enables complex
knowledge discovery from data.

</details>


### [98] [Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning](https://arxiv.org/abs/2507.00965)
*Félix Lefebvre, Gaël Varoquaux*

**主要类别:** cs.LG

**AI概要:** SEPAL是一种可扩展的嵌入传播算法，通过在实体的小核心上优化嵌入并将其传播到图的其余部分，从而为大规模知识图谱生成高质量的嵌入。它在下游任务中显著优于先前的方法，并且能够在普通硬件上适应巨大的知识图谱。


<details>
  <summary>更多</summary>
  
**动机:** 许多机器学习任务可以从外部知识中受益。大型知识图谱存储了这种知识，而嵌入方法可以将知识提炼成可用于下游应用的向量表示。然而，当前模型主要针对链接预测进行优化，且难以扩展到最大的图谱。

**方法:** SEPAL的关键思想是通过对实体的小核心进行优化嵌入以强制执行全局嵌入对齐，然后通过消息传递将这些嵌入传播到图的其余部分。

**结果:** SEPAL在7个大规模知识图谱和46个下游机器学习任务上的评估结果表明，其在下游任务中的表现显著优于先前的方法。此外，SEPAL能够扩展其基础嵌入模型，使得在普通硬件上适应巨大知识图谱成为可能。

**结论:** SEPAL解决了现有模型在链接预测优化和扩展性方面的局限性，为大规模知识图谱生成高质量嵌入，并在下游任务中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Feature+Learning+on+Huge+Knowledge+Graphs+for+Downstream+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00965&send_immediately=true&force_search=false)

**原文摘要:** Many machine learning tasks can benefit from external knowledge. Large
knowledge graphs store such knowledge, and embedding methods can be used to
distill it into ready-to-use vector representations for downstream
applications. For this purpose, current models have however two limitations:
they are primarily optimized for link prediction, via local contrastive
learning, and they struggle to scale to the largest graphs due to GPU memory
limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation
ALgorithm for large knowledge graphs designed to produce high-quality
embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce
global embedding alignment by optimizing embeddings only on a small core of
entities, and then propagating them to the rest of the graph via message
passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream
machine learning tasks. Our results show that SEPAL significantly outperforms
previous methods on downstream tasks. In addition, SEPAL scales up its base
embedding model, enabling fitting huge knowledge graphs on commodity hardware.

</details>


### [99] [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
*Yuhong Chou, Zehao Liu, Ruijie Zhu, Xinyi Wan, Tianjian Li, Congying Chu, Qian Liu, Jibin Wu, Zejun Ma*

**主要类别:** cs.LG

**AI概要:** ZeCO是一种新的序列并行方法，通过引入All-Scan通信原语，显著减少了通信开销，实现了长序列训练的近线性扩展。在256个GPU上，与现有最佳方法相比，ZeCO在8M序列长度下实现了60%的速度提升，为未来LLM的大规模训练铺平了道路。


<details>
  <summary>更多</summary>
  
**动机:** 现有的序列并行（SP）方法在处理超长序列时因通信开销大而成为瓶颈，限制了线性注意力模型的效率和可扩展性。

**方法:** 提出了一种名为ZeCO的新SP方法，其核心是All-Scan通信原语。All-Scan通过提供精确的初始操作状态并保持最小通信量，消除了通信开销。

**结果:** 理论上证明了ZeCO的时间和空间开销可以忽略不计；实证上，在256个GPU、8M序列长度的场景下，ZeCO比当前最佳SP方法快60%。

**结论:** ZeCO为高效训练下一代LLM提供了明确路径，特别是在处理之前难以处理的超长序列时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZeCO%3A+Zero+Communication+Overhead+Sequence+Parallelism+for+Linear+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.01004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.01004&send_immediately=true&force_search=false)

**原文摘要:** Linear attention mechanisms deliver significant advantages for Large Language
Models (LLMs) by providing linear computational complexity, enabling efficient
processing of ultra-long sequences (e.g., 1M context). However, existing
Sequence Parallelism (SP) methods, essential for distributing these workloads
across devices, become the primary bottleneck due to substantial communication
overhead. In this paper, we introduce ZeCO (Zero Communication Overhead)
sequence parallelism for linear attention models, a new SP method designed to
overcome these limitations and achieve end-to-end near-linear scalability for
long sequence training. For example, training a model with a 1M sequence length
across 64 devices using ZeCO takes roughly the same time as training with an
16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new
collective communication primitive. All-Scan provides each SP rank with
precisely the initial operator state it requires while maintaining a minimal
communication footprint, effectively eliminating communication overhead.
Theoretically, we prove the optimaity of ZeCO, showing that it introduces only
negligible time and space overhead. Empirically, we compare the communication
costs of different sequence parallelism strategies and demonstrate that
All-Scan achieves the fastest communication in SP scenarios. Specifically, on
256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to
the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a
clear path toward efficiently training next-generation LLMs on previously
intractable sequence lengths.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [100] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu, Hongkai Chen, Yujun Cai, Chang Liu, Qingwen Ye, Ming-Hsuan Yang, Yiwei Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为DiMo-GUI的无训练框架，用于GUI接地，通过动态视觉接地和模态感知优化策略解决了自然语言查询在GUI中的挑战。该方法将输入分为文本元素和图标元素，并利用通用视觉-语言模型独立推理每个模态。通过生成候选焦点区域并逐步缩放子区域来改进预测结果。实验表明，该方法在标准GUI基准上优于基线推理管道。


<details>
  <summary>更多</summary>
  
**动机:** 由于图形用户界面（GUI）中视觉元素的多样性、空间混乱以及语言的模糊性，将自然语言查询与GUI结合具有独特挑战。

**方法:** DiMo-GUI采用两种核心策略：动态视觉接地和模态感知优化。它将GUI输入分为文本元素和图标元素，允许模型使用通用视觉-语言模型对每个模态独立进行推理。当预测不明确或错误时，DiMo-GUI通过生成以初始预测为中心的候选焦点区域并逐步聚焦子区域来改进结果。

**结果:** 在标准GUI基准上的评估显示，DiMo-GUI在基线推理管道上持续改进，证明了结合模态分离和区域关注推理的有效性。

**结论:** DiMo-GUI是一种有效的无训练框架，能够通过动态视觉接地和模态感知优化策略解决GUI接地中的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiMo-GUI%3A+Advancing+Test-time+Scaling+in+GUI+Grounding+via+Modality-Aware+Visual+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00008&send_immediately=true&force_search=false)

**原文摘要:** Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [101] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam, Fang Wang, Chaochun Liu, Xin Chen*

**主要类别:** cs.AI

**AI概要:** The paper introduces TalentMine, a new framework enhancing LLMs to improve semantic understanding and retrieval of tabular data in talent management systems. It achieves 100% accuracy in query answering tasks, compared to 0% for standard AWS Textract and 40% for AWS Textract Visual Q&A.


<details>
  <summary>更多</summary>
  
**动机:** Current table extraction methods struggle with semantic understanding, leading to poor performance when integrated into retrieval-augmented chat applications in talent management systems.

**方法:** TalentMine, an LLM-enhanced framework that transforms extracted tables into semantically enriched representations using specialized multimodal reasoning to preserve both structural and semantic dimensions of tabular data.

**结果:** Experimental evaluation shows TalentMine achieving 100% accuracy in query answering tasks, outperforming standard AWS Textract (0%) and AWS Textract Visual Q&A (40%).

**结论:** The key contributions include systematic analysis of semantic information loss, a novel LLM-based method for enriched table representation, an efficient integration framework, and comprehensive benchmarks showing significant improvements.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TalentMine%3A+LLM-Based+Extraction+and+Question-Answering+from+Multimodal+Talent+Tables，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00041，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00041&send_immediately=true&force_search=false)

**原文摘要:** In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [102] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher, Juan C. Verduzco, Michael Titus, Alejandro Strachan*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种基于 nanoHUB 服务的分布式自驱动实验室（SDL）实现方案，用于在线模拟和 FAIR 数据管理。通过共享中心数据库和自动更新的机器学习模型，研究人员可以更有效地进行优化任务，并利用顺序优化方法找到最佳实验条件。以寻找最佳食用色素混合比例为目标，展示了如何结合 FAIR 数据、预测性机器学习模型和顺序优化来解决实际问题。


<details>
  <summary>更多</summary>
  
**动机:** 将机器学习与自动化实验相结合的自驱动实验室（SDL）能够加速科学和工程领域的发现与优化任务。然而，不同 SDL 之间的协作效率往往受限于数据共享和互操作性不足的问题。因此，本文旨在构建一个支持 FAIR 数据原则的分布式 SDL 框架，促进多地点独立优化任务的合作，并通过机器学习提升实验效率。

**方法:** 1. 构建基于 nanoHUB 的分布式 SDL 系统，包含在线模拟和 FAIR 数据管理功能。
2. 使用共享中心数据库存储来自地理分散团队的原始实验数据。
3. 利用 nanoHUB Sim2L 工具自动处理新数据点，提取衍生量并将所有输入输出索引到 ResultsDB 中。
4. 引入顺序优化工作流，通过主动学习训练机器学习模型，指导未来实验设计。
5. 借助“节俭孪生”概念，以优化食用色素混合比例达到目标颜色为案例展示框架的应用潜力。

**结果:** 该系统成功实现了分布式 SDL 协作，使研究人员能够轻松提交和处理数据，并通过机器学习模型实时优化实验设计。在食用色素优化案例中，证明了该方法的有效性和可扩展性，同时验证了 FAIR 数据原则对科学研究的支持作用。

**结论:** 本研究提出的分布式 SDL 实现方案为跨地域科研合作提供了一种高效、透明的方法。通过结合 FAIR 数据管理和机器学习技术，不仅可以显著提高实验效率，还为其他优化问题提供了通用且易扩展的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+collaborative+digital+twin+built+on+FAIR+data+and+compute+infrastructure，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00048&send_immediately=true&force_search=false)

**原文摘要:** The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [103] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva, Sandareka Wickramanayake, Dulani Meedeniya, Sanka Rasnayaka*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种新的IMU-based零样本人类活动识别模型SEZ-HARN，该模型能够识别未在训练中遇到的活动，并通过生成骨架视频来解释其决策过程。实验结果表明，SEZ-HARN在四个基准数据集上的表现与最先进的黑箱模型相当，同时提供了可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于IMU的人类活动识别（HAR）模型缺乏透明性，且现有数据集覆盖的活动范围有限，限制了其在真实场景中的应用。零样本HAR虽然克服了数据限制，但当前模型难以解释其决策。因此需要一种既具有高准确率又可解释的零样本HAR模型。

**方法:** 提出了Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN)模型，该模型可以：1) 识别未在训练阶段出现的活动；2) 生成骨架视频以解释其决策过程。模型在四个基准数据集PAMAP2、DaLiAc、HTD-MHAD和MHealth上进行了评估，并与三种最先进的黑箱零样本HAR模型进行了比较。

**结果:** SEZ-HARN在零样本预测准确性方面与最佳黑箱模型相当，在PAMAP2数据集上的预测准确性仅相差3%，并在其他三个数据集上保持了可比性能。此外，SEZ-HARN生成的解释具有现实性和可理解性。

**结论:** SEZ-HARN模型不仅实现了竞争性的零样本识别准确性，还提供了可解释的决策依据，为基于IMU的零样本人类活动识别提供了一种有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SEZ-HARN%3A+Self-Explainable+Zero-shot+Human+Activity+Recognition+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00050&send_immediately=true&force_search=false)

**原文摘要:** Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [104] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

**主要类别:** cs.AI

**AI概要:** The study proposes AdvDistill, a reward-guided dataset distillation framework to enhance the performance of small language models (SLMs) in mathematical and complex reasoning tasks by using multiple teacher responses and rule-based verifiers.


<details>
  <summary>更多</summary>
  
**动机:** To address the limitation of current knowledge distillation techniques where student models merely copy teacher's in-distribution responses, leading to poor generalisability especially in reasoning tasks.

**方法:** Propose AdvDistill, a reward-guided dataset distillation framework that uses multiple generations from a teacher model for each prompt, assigns rewards based on rule-based verifiers, and uses these rewards as weights when training student models.

**结果:** Significant improvement in student model performance for mathematical and complex reasoning tasks.

**结论:** AdvDistill demonstrates the efficacy and benefits of incorporating a rewarding mechanism in dataset distillation processes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Reasoning+Capabilities+in+SLMs+with+Reward+Guided+Dataset+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00054&send_immediately=true&force_search=false)

**原文摘要:** The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [105] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth, Alessandro Suglia*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为VoyagerVision的多模态模型，通过使用截图作为视觉反馈，在Minecraft中创建结构，扩展了Voyager的能力。实验表明，该模型在平坦世界中的构建单元测试中成功率为50％，而在更复杂结构中存在失败情况。


<details>
  <summary>更多</summary>
  
**动机:** 研究开放性AI（AGI）领域，结合大型语言模型（LLM）的进步，探索提供视觉输入给模型以增强其对空间环境的理解能力，从而提升其完成任务的数量和开放性潜力。

**方法:** 提出了VoyagerVision模型，基于Voyager，利用Minecraft中的截图作为视觉反馈来创建结构。

**结果:** VoyagerVision能够在50次系统迭代中平均创建2.75个独特的结构；在建筑单元测试中，它在平坦世界中的成功率为50％，但复杂的结构导致了一些失败。

**结论:** 通过将视觉输入引入模型，可以提高其解释空间环境的能力，增加可成功执行的任务数量，进一步拓展了模型的开放性潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VoyagerVision%3A+Investigating+the+Role+of+Multi-modal+Information+for+Open-ended+Learning+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00079&send_immediately=true&force_search=false)

**原文摘要:** Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [106] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha, Firoj Paudel, Ujjwal Puri, Zhang Yuting, Choi Donghyuk, Wang Junhao*

**主要类别:** cs.AI

**AI概要:** SAGE-nano模型通过引入逆向推理方法，增强了大型语言模型的自我反思能力及决策透明度，其在推理准确性和解释质量上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在解决复杂推理任务方面表现出色，但其决策过程仍较为黑箱化，缺乏透明性。为了提高模型的可解释性，研究者们探索了新的方法来分解和解释模型自身的推理链。

**方法:** 提出了文本逆向推理方法，结合元认知结构，通过注意力机制识别关键决策点并生成推理选择的解释；该方法应用于40亿参数的SAGE-nano推理模型中，并通过多种测试评估其性能。

**结果:** SAGE-nano在推理准确性（AQUA-RAT测试中达74.6%）和解释质量（92.1%的人类偏好得分）上均表现出色，接近Claude-3.5 Sonnet或GPT-4o等先进模型水平。

**结论:** 本研究为大型语言模型的自我反思提供了首个严谨框架，证明了逆向推理能够提升模型的可解释性和推理性能，为透明AI系统的发展开辟了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thinking+About+Thinking%3A+SAGE-nano%27s+Inverse+Reasoning+for+Self-Aware+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00092&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [107] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种使用强化学习（RL）代理探索输入空间并识别关键决策边界的方法，通过收集和聚类反事实状态转换，并训练决策树以提取可读规则，从而从被视为黑箱的遗留系统中自动提取可解释的决策逻辑。该方法在三个虚拟遗留系统上进行了验证，表明其能够准确反映核心逻辑。


<details>
  <summary>更多</summary>
  
**动机:** 现代化遗留软件系统是一项重要但充满挑战的任务，通常因缺乏文档和对原系统复杂决策逻辑的理解而受阻。传统方法如行为克隆仅复制输入-输出行为，无法捕捉底层意图。

**方法:** 该方法将遗留系统视为黑箱，利用强化学习（RL）代理探索输入空间，通过奖励导致系统输出有意义变化的动作来识别关键决策边界。收集这些反事实状态转换并使用K-Means进行聚类，然后在这些聚类上训练决策树以提取近似系统决策逻辑的人类可读规则。

**结果:** 该方法在三个具有不同复杂度的虚拟遗留系统上进行了测试，包括基于阈值、组合条件和非线性范围逻辑的系统。结果表明，RL代理成功地将探索集中在相关的边界区域，提取的规则准确反映了底层虚拟系统的核⼼逻辑。

**结论:** 提出的管道为生成规范和测试用例以实现遗留系统迁移提供了有希望的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BlackBoxToBlueprint%3A+Extracting+Interpretable+Logic+from+Legacy+Systems+using+Reinforcement+Learning+and+Counterfactual+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00180，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00180&send_immediately=true&force_search=false)

**原文摘要:** Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [108] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

**主要类别:** cs.AI

**AI概要:** 尽管大型语言模型（LLMs）在教育中的使用日益增多，但人们对其可能减少深度思考和主动学习表示担忧。本研究通过实验设计探讨了生成式AI工具（特别是ChatGPT）对学生学术写作任务中认知参与的影响。结果显示，ChatGPT组的认知参与得分显著低于对照组，表明AI辅助可能导致认知卸载。该研究呼吁制定促进学生与AI生成内容积极、反思性互动的教学策略。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机源于对大型语言模型（LLMs）在教育中广泛应用可能导致学生减少深度思考和主动学习的担忧。

**方法:** 采用实验设计，将参与者随机分配到AI辅助（ChatGPT）或非辅助（对照）条件下。参与者完成结构化论证写作任务后，填写认知参与量表（CES-AI），以评估心理努力、注意力、深度处理和战略性思维。

**结果:** 结果表明，ChatGPT组的认知参与得分显著低于对照组。

**结论:** 生成式AI工具可能会导致认知卸载，需要制定教学策略来促进学生与AI生成内容的积极、反思性互动，以避免影响自我调节学习和学生的深度认知参与。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ChatGPT+produces+more+%22lazy%22+thinkers%3A+Evidence+of+cognitive+engagement+decline，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00181&send_immediately=true&force_search=false)

**原文摘要:** Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [109] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis, Georgios Margaritis, Vasiliki Stoumpou, Dimitris Bertsimas*

**主要类别:** cs.AI

**AI概要:** 通过四个步骤提升医疗多模态数据预测与可解释性，xHAIM在HAIM-MIMIC-MM数据集上将平均AUC从79.9%提高到90.3%，并提供临床解释支持系统。


<details>
  <summary>更多</summary>
  
**动机:** 之前的HAIM框架虽然融合了多模态数据解决临床任务，但以任务无关的方式使用数据且缺乏可解释性。

**方法:** 提出xHAIM框架，包括：自动识别跨模态的任务相关患者数据、生成全面的患者摘要、利用摘要改进预测建模、提供连接预测与患者特定医学知识的临床解释。

**结果:** 在HAIM-MIMIC-MM数据集评估中，xHAIM将平均AUC从79.9%提高到90.3%，显著提升了预测性能和可解释性。

**结论:** xHAIM将AI从黑箱预测器转变为可解释的决策支持系统，使临床医生能够交互式地将预测追溯到相关患者数据，弥合了AI进展与临床应用之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Holistic+Artificial+Intelligence+in+Medicine%3B+improved+performance+and+explainability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00205&send_immediately=true&force_search=false)

**原文摘要:** With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [110] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou, Attila Lischka, Balazs Kulcsar, Jiaming Wu, Morteza Haghir Chehreghani, Gilbert Laporte*

**主要类别:** cs.AI

**AI概要:** 这篇论文回顾了将机器学习工具应用于解决NP难组合优化问题（尤其是旅行商问题和车辆路径问题）的最新进展，提出了一种基于构建和改进方法的分类法，并旨在结合传统运筹学方法与现代机器学习技术以指导未来研究。


<details>
  <summary>更多</summary>
  
**动机:** 由于NP难组合优化问题的固有复杂性，精确算法通常需要过多的计算时间来寻找最优解，而启发式算法只能提供近似解且无法保证最优性。因此，探索机器学习在这些问题中的应用成为重要的研究方向。

**方法:** 论文提出了一个分类法，将基于机器学习的路径规划方法分为构建型和改进型两类，并探讨了它们在不同问题特性中的适用性。同时，整合了传统运筹学方法与最新的机器学习技术，为未来研究提供了结构化的框架。

**结果:** 该综述总结了机器学习在解决路由问题上的最新进展，并通过提出的分类法明确了各类方法的适用场景，为进一步研究提供了方向。

**结论:** 机器学习技术在解决NP难组合优化问题上展现出巨大潜力，未来的研究应进一步结合传统方法与机器学习技术，以应对新兴的车辆路径问题变种。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+for+routing%3A+A+guided+review+of+recent+developments+and+future+directions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00218，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00218&send_immediately=true&force_search=false)

**原文摘要:** This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [111] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim, Anirudh Goyal, Liang Tan, Hannaneh Hajishirzi, Srinivasan Iyer, Tianlu Wang*

**主要类别:** cs.AI

**AI概要:** 研究提出了一种名为ASTRO（Autoregressive Search-Taught Reasoner）的框架，通过蒙特卡洛树搜索（MCTS）生成的数学问题解决轨迹数据集，训练语言模型像搜索算法一样推理。此方法增强了非推理模型（如Llama 3系列）的推理能力，在多个数学测试中显著提升了性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管通过强化学习训练大型语言模型已取得成功，但开源复现主要依赖于原本就具备强推理能力的模型。如何提升其他非推理模型的能力尚不明确，因此需要一种新方法来增强这些模型的推理能力。

**方法:** ASTRO框架利用从蒙特卡洛树搜索衍生的数据集，将搜索过程转化为自然语言的思考链条，包括成功与失败恢复的过程。通过在这些数据上微调模型，并结合可验证奖励的强化学习进一步优化性能，从而让模型内部化结构化的搜索行为。

**结果:** 在Llama 3系列模型上的应用显示，该方法在MATH-500、AMC 2023和AIME 2024等数学测试中分别提升了16.0%、26.9%和20.0%的绝对性能，尤其在需要迭代修正的难题上表现突出。

**结论:** 研究表明，受搜索启发的训练方法为向开放的大语言模型注入强大的推理能力提供了一条有原则的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ASTRO%3A+Teaching+Language+Models+to+Reason+by+Reflecting+and+Backtracking+In-Context，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00417，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00417&send_immediately=true&force_search=false)

**原文摘要:** We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [112] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, Xiang Yue*

**主要类别:** cs.AI

**AI概要:** 尽管大语言模型在数学推理方面取得了显著进展，但这些模型的能力往往局限于数学领域，难以迁移到其他任务。通过对比强化学习和监督微调的方法，研究发现强化学习能够更好地保持模型的跨领域泛化能力，而监督微调则可能导致模型在非数学领域的性能下降。这提示我们需要重新审视当前的模型训练方法。


<details>
  <summary>更多</summary>
  
**动机:** 评估大语言模型在数学领域取得的进步是否能代表其更广泛的解决问题的能力，还是仅仅局限于特定领域的过拟合。

**方法:** 对超过20个开源权重的推理调整模型进行评估，涵盖数学、科学问答、代理规划、编码和标准指令遵循等任务。使用Qwen3-14B模型进行受控实验，采用仅数学数据但不同调整方法（如强化学习和监督微调）。通过潜在空间表示和标记空间分布转移分析来揭示不同调整方法的影响。

**结果:** 大多数在数学上表现良好的模型未能将其收益转移到其他领域。强化学习调整的模型在各领域中表现出良好的泛化能力，而监督微调调整的模型往往会忘记一般能力。监督微调会引发显著的表示和输出漂移，而强化学习则保留了通用域结构。

**结论:** 需要重新思考标准的后训练配方，特别是对依赖SFT蒸馏数据以推进推理模型的做法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Math+Reasoning+Improve+General+LLM+Capabilities%3F+Understanding+Transferability+of+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00432&send_immediately=true&force_search=false)

**原文摘要:** Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [113] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding, Haokun Li, Xinpeng Ni, Bican Xia, Tianqi Zhao*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种用于SMT-NRA的二维单元跳跃（2d-cell-jump）方法，扩展了局部搜索框架，并结合MCSAT和OpenCAD设计了一个混合框架，实验结果表明该方法提高了局部搜索性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了改进非线性实数算术理论（SMT-NRA）中的可满足性模理论问题的局部搜索方法，提高其求解效率。

**方法:** 1. 提出了一个二维单元跳跃（2d-cell-jump）操作，推广了SMT-NRA中局部搜索的关键操作。
2. 提出一个扩展的局部搜索框架（2d-LS），将模型构造可满足性演算（MCSAT）框架集成到局部搜索中以提高搜索效率。
3. 实现了样本单元投影运算符（sample-cell projection operator）来进一步提升MCSAT的效率。
4. 设计了一个混合框架，结合MCSAT、2d-LS和OpenCAD，通过信息交换提高搜索效率。

**结果:** 实验结果展示了局部搜索性能的提升，证明了所提出方法的有效性。

**结论:** 提出的2d-cell-jump方法、2d-LS框架以及混合框架显著提高了SMT-NRA问题的局部搜索性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Local+Search+in+SMT-NRA+with+MCSAT+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00557，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00557&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [114] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang, Hojoon Lee, Jaegul Choo, Dongmin Park, Jongho Park*

**主要类别:** cs.AI

**AI概要:** 通过强化学习（RL）训练大语言模型（LLMs）进行国际象棋的战略推理。利用预训练的动作-价值网络提供密集奖励，这种密集奖励通常优于稀疏二元奖励。然而，所有模型的表现都远低于专家水平，这可能是因为预训练模型对国际象棋的内部理解存在不足，而仅靠RL可能无法完全克服这一缺陷。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在数学推理方面，使用强化学习训练大语言模型显示出潜力，但使用RL进行战略推理的研究尚处于初步阶段。本文旨在探讨LLMs是否可以通过RL发展出国际象棋的战略推理能力。

**方法:** 研究者使用了一个国际象棋预训练的动作-价值网络来为LLMs的输出移动质量提供密集奖励，这种方式可以被视为一种知识蒸馏。然后通过实验比较了这种密集奖励与稀疏二元奖励的效果，并进行了SFT和RL消融实验。

**结果:** 实验结果表明，基于蒸馏的密集奖励往往比稀疏二元奖励表现更好。然而，所有模型的表现都在远低于专家水平的地方停滞不前。

**结论:** 研究表明，LLMs通过RL可以在一定程度上提高国际象棋的战略推理能力，但其效果受到预训练模型对国际象棋内部理解不足的限制，单靠RL可能无法克服这一问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Large+Language+Models+Develop+Strategic+Reasoning%3F+Post-training+Insights+from+Learning+Chess，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00726，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00726&send_immediately=true&force_search=false)

**原文摘要:** While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [115] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu, Xiaohua Xuan*

**主要类别:** cs.AI

**AI概要:** 提出了一种基于非光滑优化、二次规划和迭代过程的改进数值算法，适用于鲁棒优化和不平衡学习等领域，并在一定假设条件下证明了算法的收敛性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决基于非光滑优化的极小极大问题，提高求解效率并扩展其应用领域。

**方法:** 通过结合非光滑优化、二次规划以及迭代过程设计一种改进的数值算法，并在梯度连续性和有界性等温和假设下严格证明其收敛性。

**结果:** 该算法具有广泛的适用性，可以在鲁棒优化、不平衡学习等多个领域中应用。

**结论:** 所提出的算法在解决极小极大问题时表现良好，并且理论分析表明其具备收敛性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Robust+Algorithm+for+Non-IID+Machine+Learning+Problems+with+Convergence+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00810&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [116] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang, Tianmeng Fang, Zhe Liu, Aishan Liu, Yan Xiao, Jinyuan He, Ee-Chien Chang, Xiaochun Cao*

**主要类别:** cs.AI

**AI概要:** 通过将行为序列信息融入风险判别机制并设计基于大语言模型的自动化辅助评估方案，本研究旨在探索移动多模态代理的安全问题并降低其被越狱的风险。初步验证表明该方法可提高对危险行为的识别能力并减少代理被越狱的可能性。


<details>
  <summary>更多</summary>
  
**动机:** 随着多模态基础模型在智能代理系统中的广泛应用，相关系统逐渐依赖于这些大型模型驱动的代理，但同时也面临潜在的越狱风险，攻击者可能通过特定输入诱导代理绕过原始行为约束，触发危险和敏感操作，而现有的安全措施在复杂交互场景中存在局限性。

**方法:** 本研究探索了围绕移动多模态代理的安全问题，尝试通过融合行为序列信息构建风险判别机制，并设计了一个基于大语言模型的自动化辅助评估方案。

**结果:** 在几个具有代表性的高风险任务中的初步验证结果表明，该方法可以在一定程度上提高对危险行为的识别能力，并有助于降低代理被越狱的概率。

**结论:** 本研究为多模态智能代理系统的安全风险建模和防护提供了一些有价值的参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SafeMobile%3A+Chain-level+Jailbreak+Detection+and+Automated+Evaluation+for+Multimodal+Mobile+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00841&send_immediately=true&force_search=false)

**原文摘要:** With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [117] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi, Ranjan Sapkota, Abbas Shah, Amgad Muneer, Anas Zafar, Ashmal Vayani, Maged Shoman, Abdelrahman B. M. Eldaly, Kai Zhang, Ferhat Sadak, Shaina Raza, Xinqi Fan, Ravid Shwartz-Ziv, Hong Yan, Vinjia Jain, Aman Chadha, Manoj Karkee, Jia Wu, Philip Torr, Seyedali Mirjalili*

**主要类别:** cs.AI

**AI概要:** 尽管当前模型（如GPT-4.5等）在多模态流畅性和部分推理方面表现出色，但它们仍然受限于基于token的预测和缺乏实际代理能力。本文从跨学科角度探讨了通义人工智能（AGI）的发展，强调模块化推理、持久记忆和多代理协调的重要性，并讨论了智能不仅来自于规模，还来自于记忆与推理的整合。


<details>
  <summary>更多</summary>
  
**动机:** 探讨实现真正的人工通用智能（AGI）的可能性及路径，分析现有模型的能力限制及其改进方向。

**方法:** 通过结合认知神经科学、心理学、生成模型和基于代理的系统等领域，进行跨学科综合分析；重点研究模块化推理、持久记忆和多代理协调的作用；讨论信息压缩、测试时适应和无训练方法等策略；重新审视视觉-语言模型作为感知模块的功能。

**结果:** 提出智能不仅来自于规模，更来自于记忆和推理的整合，近期架构开始弥合统计学习与目标导向认知之间的差距，并识别出通往AGI的关键科学、技术和伦理挑战。

**结论:** 实现AGI需要克服技术与伦理挑战，未来研究应关注模块化、自适应和自我改进组件的集成，以及压缩如何支持适应性行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thinking+Beyond+Tokens%3A+From+Brain-Inspired+Intelligence+to+Cognitive+Foundations+for+Artificial+General+Intelligence+and+its+Societal+Impact，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00951，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00951&send_immediately=true&force_search=false)

**原文摘要:** Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [118] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm, Woogyeol Jin, June Suk Choi, Sungsoo Ahn, Kimin Lee*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种名为CIP的新技术，利用因果影响图（CIDs）来识别和减轻自主代理决策带来的风险，通过初始化、引导和迭代优化CID三个步骤，在代码执行和移动设备控制任务中有效提高了安全性。


<details>
  <summary>更多</summary>
  
**动机:** 随着由大型语言模型驱动的自主代理在各种辅助任务中展现出潜力，确保其行为的安全性和可靠性变得至关重要，以防止意外后果的发生。

**方法:** CIP技术包含三个关键步骤：1) 根据任务规范初始化CID，概述决策过程；2) 使用CID指导代理与环境的交互；3) 基于观察到的行为和结果迭代优化CID。

**结果:** 实验结果表明，该方法在代码执行和移动设备控制任务中有效地增强了安全性。

**结论:** CIP是一种有效提高自主代理安全性的新技术，尤其是在代码执行和移动设备控制任务中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+LLM+Agent+Safety+via+Causal+Influence+Prompting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00979，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00979&send_immediately=true&force_search=false)

**原文摘要:** As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [119] [Disentangled Feature Importance](https://arxiv.org/abs/2507.00260)
*Jin-Hong Du, Kathryn Roeder, Larry Wasserman*

**主要类别:** stat.ML

**AI概要:** 论文提出了一种新的特征重要性量化方法Disentangled Feature Importance (DFI)，解决了传统方法在面对相关预测变量时系统性低估其贡献的问题。通过最优传输理论，DFI将相关特征转换为独立的潜在变量，并提供了一个无偏的特征重要性分解。此外，论文还开发了DFI的半参数理论，证明了估计量的一致性和渐近正态性，同时实现了计算效率的提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的特征重要性量化方法在处理相关预测变量时存在偏差，无法准确评估这些变量的实际贡献。这促使研究者寻找一种能够消除相关性影响的新方法。

**方法:** 引入了Disentangled Feature Importance (DFI) 方法，利用最优传输理论将相关特征转化为独立的潜在变量，从而消除了相关性引起的偏差。DFI基于非参数的 $R^2$ 分解，并通过传输图的敏感性将重要性归因回原始特征空间。此外，作者还发展了DFI的半参数理论，分析了估计量在潜在特征空间中的根-$n$ 一致性与渐近正态性。

**结果:** DFI 提供了一个原则性的特征重要性分解，该分解可以总和为目标预测变化的总值。对于隐含加性模型，它等于交互加权的功能ANOVA方差。并且，DFI避免了重复子模型拟合的计算负担，以及条件协变量分布估计的挑战，从而提高了计算效率。

**结论:** DFI 是一种有效且高效的方法，用于解决特征相关性导致的特征重要性低估问题。它不仅提供了无偏的特征重要性评估，还在理论上保证了估计量的良好性质。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Disentangled+Feature+Importance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00260&send_immediately=true&force_search=false)

**原文摘要:** Feature importance quantification faces a fundamental challenge: when
predictors are correlated, standard methods systematically underestimate their
contributions. We prove that major existing approaches target identical
population functionals under squared-error loss, revealing why they share this
correlation-induced bias.
  To address this limitation, we introduce \emph{Disentangled Feature
Importance (DFI)}, a nonparametric generalization of the classical $R^2$
decomposition via optimal transport. DFI transforms correlated features into
independent latent variables using a transport map, eliminating correlation
distortion. Importance is computed in this disentangled space and attributed
back through the transport map's sensitivity. DFI provides a principled
decomposition of importance scores that sum to the total predictive variability
for latent additive models and to interaction-weighted functional ANOVA
variances more generally, under arbitrary feature dependencies.
  We develop a comprehensive semiparametric theory for DFI. For general
transport maps, we establish root-$n$ consistency and asymptotic normality of
importance estimators in the latent space, which extends to the original
feature space for the Bures-Wasserstein map. Notably, our estimators achieve
second-order estimation error, which vanishes if both regression function and
transport map estimation errors are $o_{\mathbb{P}}(n^{-1/4})$. By design, DFI
avoids the computational burden of repeated submodel refitting and the
challenges of conditional covariate distribution estimation, thereby achieving
computational efficiency.

</details>


### [120] [Enhancing Interpretability in Generative Modeling: Statistically Disentangled Latent Spaces Guided by Generative Factors in Scientific Datasets](https://arxiv.org/abs/2507.00298)
*Arkaprabha Ganguli, Nesar Ramachandra, Julie Bessac, Emil Constantinescu*

**主要类别:** stat.ML

**AI概要:** 本研究提出了一种名为Aux-VAE的新架构，在经典的变分自编码器框架内，通过辅助变量利用先验统计知识，对低维潜在变量进行解耦，适用于复杂高维数据集的非线性降维问题。


<details>
  <summary>更多</summary>
  
**动机:** 在无监督或半监督环境下，从复杂、高维数据集中统计提取生成因素是一个挑战。现有的基于编码器-解码器的生成模型可以用于非线性降维，但需要进一步研究如何解耦与独立物理因素相对应的低维潜在变量。

**方法:** 研究引入了Aux-VAE（辅助变量变分自编码器），这是一种在经典变分自编码器框架内的新架构。通过在标准VAE损失函数中进行最小修改，并利用辅助变量引入先验统计知识，使潜在空间中的因子与学习到的辅助变量对齐，从而实现解耦。

**结果:** 通过在多个数据集上的比较评估（包括天文学模拟数据），验证了Aux-VAE的有效性，表明其能够成功实现潜在变量的解耦。

**结论:** Aux-VAE是一种有效的新型架构，能够在复杂的高维数据集中解耦潜在变量，为非线性降维和生成模型的研究提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Interpretability+in+Generative+Modeling%3A+Statistically+Disentangled+Latent+Spaces+Guided+by+Generative+Factors+in+Scientific+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00298，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00298&send_immediately=true&force_search=false)

**原文摘要:** This study addresses the challenge of statistically extracting generative
factors from complex, high-dimensional datasets in unsupervised or
semi-supervised settings. We investigate encoder-decoder-based generative
models for nonlinear dimensionality reduction, focusing on disentangling
low-dimensional latent variables corresponding to independent physical factors.
Introducing Aux-VAE, a novel architecture within the classical Variational
Autoencoder framework, we achieve disentanglement with minimal modifications to
the standard VAE loss function by leveraging prior statistical knowledge
through auxiliary variables. These variables guide the shaping of the latent
space by aligning latent factors with learned auxiliary variables. We validate
the efficacy of Aux-VAE through comparative assessments on multiple datasets,
including astronomical simulations.

</details>


### [121] [GRAND: Graph Release with Assured Node Differential Privacy](https://arxiv.org/abs/2507.00402)
*Suqing Liu, Xuan Bi, Tianxi Li*

**主要类别:** stat.ML

**AI概要:** 本论文提出了一种名为GRAND的新方法，这是据作者所知第一个能够在确保节点级差分隐私的同时保留结构属性并发布整个网络的机制。通过在合成和真实世界数据集上的广泛实验验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管差分隐私在许多领域得到了广泛应用，但在网络数据中的应用，特别是节点级别的隐私保护，仍然没有得到充分研究。现有的节点隐私保护方法要么仅关注查询方法，限制输出到预定义的网络统计信息，要么无法保留网络的关键结构属性。

**方法:** 提出了GRAND（Graph Release with Assured Node Differential privacy），一种新的网络发布机制，该机制可以发布整个网络，同时确保节点级别的差分隐私并保留结构属性。在潜在空间模型的一般类别下，展示了发布的网络渐近地遵循与原始网络相同的分布。

**结果:** 通过广泛的实验（包括合成数据集和真实世界数据集）验证了GRAND方法的有效性。

**结论:** GRAND是首个能够确保节点级别差分隐私并保留网络结构特性的网络发布机制，适用于广泛的潜在空间模型，并且其有效性得到了实验证明。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GRAND%3A+Graph+Release+with+Assured+Node+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00402，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00402&send_immediately=true&force_search=false)

**原文摘要:** Differential privacy is a well-established framework for safeguarding
sensitive information in data. While extensively applied across various
domains, its application to network data -- particularly at the node level --
remains underexplored. Existing methods for node-level privacy either focus
exclusively on query-based approaches, which restrict output to pre-specified
network statistics, or fail to preserve key structural properties of the
network. In this work, we propose GRAND (Graph Release with Assured Node
Differential privacy), which is, to the best of our knowledge, the first
network release mechanism that releases entire networks while ensuring
node-level differential privacy and preserving structural properties. Under a
broad class of latent space models, we show that the released network
asymptotically follows the same distribution as the original network. The
effectiveness of the approach is evaluated through extensive experiments on
both synthetic and real-world datasets.

</details>


### [122] [Forward Reverse Kernel Regression for the Schrödinger bridge problem](https://arxiv.org/abs/2507.00640)
*Denis Belomestny, John. Schoenmakers*

**主要类别:** stat.ML

**AI概要:** 这篇论文研究了Schrödinger桥问题(SBP)，它是熵最优传输的核心。对于一般的参考过程和起始-终点分布，提出了一种前向-反向迭代Monte Carlo过程，以非参数方式逼近Schrödinger势能。特别地，在相应不动点问题的Picard迭代中使用基于核的Monte Carlo回归。通过在迭代中保持Hilbert度量意义上的正性和收缩性，开发了一种可证明收敛的算法。此外，提供了势能估计的收敛速度并证明了其最优性。最后，作为应用，基于构造的势能和条件扩散的前向-反向模拟方法，提出了一种非嵌套Monte Carlo过程，用于Schrödinger桥过程的最终维分布。


<details>
  <summary>更多</summary>
  
**动机:** Schrödinger桥问题是熵最优传输的核心问题，解决这一问题有助于更好地理解和优化传输过程。然而，现有的方法可能无法有效地处理一般参考过程和起始-终点分布的情况，因此需要一种新的方法来更准确地逼近Schrödinger势能。

**方法:** 提出了一种前向-反向迭代Monte Carlo过程，结合基于核的Monte Carlo回归和Picard迭代方法，以非参数方式逼近Schrödinger势能。同时，通过保持迭代中的正性和收缩性，确保算法的收敛性，并提供了势能估计的收敛速度和最优性证明。最后，基于构造的势能和条件扩散的前向-反向模拟方法，提出了一种非嵌套Monte Carlo过程，用于Schrödinger桥过程的最终维分布。

**结果:** 该方法能够有效逼近Schrödinger势能，并且算法具有可证明的收敛性。提供的势能估计收敛速度证明了其最优性。提出的非嵌套Monte Carlo过程可以准确模拟Schrödinger桥过程的最终维分布。

**结论:** 本文提出了一种新的方法来解决Schrödinger桥问题，该方法通过前向-反向迭代Monte Carlo过程和基于核的Monte Carlo回归，以非参数方式逼近Schrödinger势能。此方法不仅保证了算法的收敛性，还证明了势能估计的最优性。此外，提出的应用方法可以有效模拟Schrödinger桥过程的最终维分布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forward+Reverse+Kernel+Regression+for+the+Schr%C3%B6dinger+bridge+problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00640&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we study the Schr\"odinger Bridge Problem (SBP), which is
central to entropic optimal transport. For general reference processes and
begin--endpoint distributions, we propose a forward-reverse iterative Monte
Carlo procedure to approximate the Schr\"odinger potentials in a nonparametric
way. In particular, we use kernel based Monte Carlo regression in the context
of Picard iteration of a corresponding fixed point problem. By preserving in
the iteration positivity and contractivity in a Hilbert metric sense, we
develop a provably convergent algorithm. Furthermore, we provide convergence
rates for the potential estimates and prove their optimality. Finally, as an
application, we propose a non-nested Monte Carlo procedure for the final
dimensional distributions of the Schr\"odinger Bridge process, based on the
constructed potentials and the forward-reverse simulation method for
conditional diffusions.

</details>


### [123] [An in depth look at the Procrustes-Wasserstein distance: properties and barycenters](https://arxiv.org/abs/2507.00894)
*Davide Adamo, Marco Corneli, Manon Vuillien, Emmanuelle Vila*

**主要类别:** stat.ML

**AI概要:** 提出了一种基于Procrustes-Wasserstein (PW) 距离的新方法，用于从点云集合中计算代表性形状，并展示了其在精确对齐和形状保留方面的优越性能以及在考古学中的应用。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决Wasserstein距离在点云对齐和比较任务中的不足，引入了对刚性变换（如旋转和反射）具有不变性的Procrustes-Wasserstein (PW) 距离。

**方法:** 构建了一个离散概率测度空间，在该空间上证明了PW实际上是一种距离；扩展了PW框架，讨论并测试了几种初始化策略；引入了PW重心的概念，并详细描述了一种从数据中估计它的算法。

**结果:** 与现有的最优传输(OT)方法相比，在需要精确对齐和形状保留的场景中表现出更优的性能，并且在考古学背景下展示了PW重心的实际用途。

**结论:** Procrustes-Wasserstein距离及其相关算法在提升2D和3D点云分析方面具有巨大潜力，适用于机器学习和计算几何领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+in+depth+look+at+the+Procrustes-Wasserstein+distance%3A+properties+and+barycenters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.00894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.00894&send_immediately=true&force_search=false)

**原文摘要:** Due to its invariance to rigid transformations such as rotations and
reflections, Procrustes-Wasserstein (PW) was introduced in the literature as an
optimal transport (OT) distance, alternative to Wasserstein and more suited to
tasks such as the alignment and comparison of point clouds. Having that
application in mind, we carefully build a space of discrete probability
measures and show that over that space PW actually is a distance. Algorithms to
solve the PW problems already exist, however we extend the PW framework by
discussing and testing several initialization strategies. We then introduce the
notion of PW barycenter and detail an algorithm to estimate it from the data.
The result is a new method to compute representative shapes from a collection
of point clouds. We benchmark our method against existing OT approaches,
demonstrating superior performance in scenarios requiring precise alignment and
shape preservation. We finally show the usefulness of the PW barycenters in an
archaeological context. Our results highlight the potential of PW in boosting
2D and 3D point cloud analysis for machine learning and computational geometry
applications.

</details>
