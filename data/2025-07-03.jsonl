{"id": "2507.01026", "pdf": "https://arxiv.org/pdf/2507.01026", "abs": "https://arxiv.org/abs/2507.01026", "authors": ["Md Shakil Ahamed Shohag", "Q. M. Jonathan Wu", "Farhad Pourpanah"], "title": "Few-Shot Inspired Generative Zero-Shot Learning", "categories": ["cs.LG"], "comment": null, "summary": "Generative zero-shot learning (ZSL) methods typically synthesize visual\nfeatures for unseen classes using predefined semantic attributes, followed by\ntraining a fully supervised classification model. While effective, these\nmethods require substantial computational resources and extensive synthetic\ndata, thereby relaxing the original ZSL assumptions. In this paper, we propose\nFSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on\nlarge-scale feature synthesis. Our key insight is that class-level attributes\nexhibit instance-level variability, i.e., some attributes may be absent or\npartially visible, yet conventional ZSL methods treat them as uniformly\npresent. To address this, we introduce Model-Specific Attribute Scoring (MSAS),\nwhich dynamically re-scores class attributes based on model-specific\noptimization to approximate instance-level variability without access to unseen\ndata. We further estimate group-level prototypes as clusters of instances based\non MSAS-adjusted attribute scores, which serve as representative synthetic\nfeatures for each unseen class. To mitigate the resulting data imbalance, we\nintroduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training\na semantic-aware contrastive classifier (SCC) using these prototypes.\nExperiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves\ncompetitive performance using far fewer synthetic features."}
{"id": "2507.01027", "pdf": "https://arxiv.org/pdf/2507.01027", "abs": "https://arxiv.org/abs/2507.01027", "authors": ["Zijian Ye", "Wei Huang", "Yifei Yu", "Tianhe Ren", "Zhongrui Wang", "Xiaojuan Qi"], "title": "DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization", "categories": ["cs.LG"], "comment": "19 pages; Appendix added", "summary": "Large language models (LLMs) demonstrate remarkable performance but face\nsubstantial computational and memory challenges that limit their practical\ndeployment. Quantization has emerged as a promising solution; however, its\neffectiveness is often limited by quantization errors arising from weight\ndistributions that are not quantization-friendly and the presence of activation\noutliers. To address these challenges, we introduce DBellQuant, an innovative\npost-training quantization (PTQ) framework that achieves nearly 1-bit weight\ncompression and 6-bit activation quantization with minimal performance\ndegradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)\nalgorithm, which transforms single-bell weight distributions into dual-bell\nforms to reduce binarization errors and applies inverse transformations to\nsmooth activations. DBellQuant sets a new state-of-the-art by preserving\nsuperior model performance under aggressive weight and activation quantization.\nFor example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of\n14.39 on LLaMA2-13B with 6-bit activation quantization, significantly\noutperforming BiLLM's 21.35 without activation quantization, underscoring its\npotential in compressing LLMs for real-world applications."}
{"id": "2507.01028", "pdf": "https://arxiv.org/pdf/2507.01028", "abs": "https://arxiv.org/abs/2507.01028", "authors": ["Jean Ponce", "Martial Hebert", "Basile Terver"], "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "categories": ["cs.LG"], "comment": null, "summary": "The objective of non-contrastive approaches to self-supervised learning is to\ntrain on pairs of different views of the data an encoder and a predictor that\nminimize the mean discrepancy between the code predicted from the embedding of\nthe first view and the embedding of the second one. In this setting, the stop\ngradient and exponential moving average iterative procedures are commonly used\nto avoid representation collapse, with excellent performance in downstream\nsupervised applications. This presentation investigates these procedures from\nthe dual theoretical viewpoints of optimization and dynamical systems. We first\nshow that, in general, although they do not optimize the original objective, or\nfor that matter, any other smooth function, they do avoid collapse. Following\nTian et al. [2021], but without any of the extra assumptions used in their\nproofs, we then show using a dynamical system perspective that, in the linear\ncase, minimizing the original objective function without the use of a stop\ngradient or exponential moving average always leads to collapse. Conversely, we\nfinally show that the limit points of the dynamical systems associated with\nthese two procedures are, in general, asymptotically stable equilibria, with no\nrisk of degenerating to trivial solutions."}
{"id": "2507.01029", "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning."}
{"id": "2507.01231", "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["Iñaki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "categories": ["cs.AI"], "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here."}
{"id": "2507.01044", "pdf": "https://arxiv.org/pdf/2507.01044", "abs": "https://arxiv.org/abs/2507.01044", "authors": ["Vivek Borkar", "Parthe Pandit"], "title": "Asymptotic convexity of wide and shallow neural networks", "categories": ["stat.ML", "cs.LG", "math.PR", "68T07"], "comment": "5 pages", "summary": "For a simple model of shallow and wide neural networks, we show that the\nepigraph of its input-output map as a function of the network parameters\napproximates epigraph of a. convex function in a precise sense. This leads to a\nplausible explanation of their observed good performance."}
{"id": "2507.01030", "pdf": "https://arxiv.org/pdf/2507.01030", "abs": "https://arxiv.org/abs/2507.01030", "authors": ["Reza Lotfi Navaei", "Mohammad Safarzadeh", "Seyed Mohammad Jafar Sobhani"], "title": "Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study", "categories": ["cs.LG"], "comment": "It has been submitted to ASME Journal of Heat and Mass Transfer", "summary": "In chemistry tabulations and Flamelet combustion models, the Flamelet\nGenerated Manifold (FGM) is recognized for its precision and physical\nrepresentation. The practical implementation of FGM requires a significant\nallocation of memory resources. FGM libraries are developed specifically for a\nspecific fuel and subsequently utilized for all numerical problems using\nmachine learning techniques. This research aims to develop libraries of Laminar\nFGM utilizing machine learning algorithms for application in combustion\nsimulations of methane fuel. This study employs four Machine Learning\nalgorithms to regenerate Flamelet libraries, based on an understanding of data\nsources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.\nRandom Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries\nwere identified as appropriate for constructing a database for training machine\nlearning models, giving an error rate of 2.30%. The default architectures of\neach method were evaluated to determine the optimal approach, leading to the\nselection of the MLP method as the primary choice. The method was enhanced\nthrough hyperparameter tuning to improve accuracy. The quantity of hidden\nlayers and neurons significantly influences method performance. The optimal\nmodel, comprising four hidden layers with 10, 15, 20, and 25 neurons\nrespectively, achieved an accuracy of 99.81%."}
{"id": "2507.01282", "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice."}
{"id": "2507.01542", "pdf": "https://arxiv.org/pdf/2507.01542", "abs": "https://arxiv.org/abs/2507.01542", "authors": ["Tom Szwagier", "Pierre-Alexandre Mattei", "Charles Bouveyron", "Xavier Pennec"], "title": "Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO", "stat.ME"], "comment": null, "summary": "Gaussian mixture models (GMMs) are ubiquitous in statistical learning,\nparticularly for unsupervised problems. While full GMMs suffer from the\noverparameterization of their covariance matrices in high-dimensional spaces,\nspherical GMMs (with isotropic covariance matrices) certainly lack flexibility\nto fit certain anisotropic distributions. Connecting these two extremes, we\nintroduce a new family of parsimonious GMMs with piecewise-constant covariance\neigenvalue profiles. These extend several low-rank models like the celebrated\nmixtures of probabilistic principal component analyzers (MPPCA), by enabling\nany possible sequence of eigenvalue multiplicities. If the latter are\nprespecified, then we can naturally derive an expectation-maximization (EM)\nalgorithm to learn the mixture parameters. Otherwise, to address the\nnotoriously-challenging issue of jointly learning the mixture parameters and\nhyperparameters, we propose a componentwise penalized EM algorithm, whose\nmonotonicity is proven. We show the superior likelihood-parsimony tradeoffs\nachieved by our models on a variety of unsupervised experiments: density\nfitting, clustering and single-image denoising."}
{"id": "2507.01031", "pdf": "https://arxiv.org/pdf/2507.01031", "abs": "https://arxiv.org/abs/2507.01031", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs", "categories": ["cs.LG", "cs.SE"], "comment": "Conference paper: Accepted in Korea Computer Congress (KCC) 2025. The\n  library is available at https://github.com/bokveizen/gaudi-geometric-learning", "summary": "Geometric learning has emerged as a powerful paradigm for modeling\nnon-Euclidean data, especially graph-structured ones, with applications\nspanning social networks, molecular structures, knowledge graphs, and\nrecommender systems. While Nvidia's CUDA-enabled graphics processing units\n(GPUs) largely dominate the hardware landscape, emerging accelerators such as\nIntel's Gaudi Habana Processing Units (HPUs) offer competitive performance and\nenergy efficiency. However, the usage of such non-CUDA processing units\nrequires significant engineering effort and novel software adaptations. In this\nwork, we present our experiences porting PyTorch-based geometric learning\nframeworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that\nrestore essential operations (e.g., scatter, sparse indexing, k-nearest\nneighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and\neleven real-world examples with diagnostic analyses of encountered failures and\ndetailed workarounds. We collect all our experiences into a publicly accessible\nGitHub repository. Our contributions lower the barrier for researchers to\nexperiment with geometric-learning algorithms and models on non-CUDA hardware,\nproviding a foundation for further optimization and cross-platform portability."}
{"id": "2507.01376", "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "categories": ["cs.AI"], "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face."}
{"id": "2507.01613", "pdf": "https://arxiv.org/pdf/2507.01613", "abs": "https://arxiv.org/abs/2507.01613", "authors": ["Shirong Xu", "Jingnan Zhang", "Junhui Wang"], "title": "When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Paired comparison data, where users evaluate items in pairs, play a central\nrole in ranking and preference learning tasks. While ordinal comparison data\nintuitively offer richer information than binary comparisons, this paper\nchallenges that conventional wisdom. We propose a general parametric framework\nfor modeling ordinal paired comparisons without ties. The model adopts a\ngeneralized additive structure, featuring a link function that quantifies the\npreference difference between two items and a pattern function that governs the\ndistribution over ordinal response levels. This framework encompasses classical\nbinary comparison models as special cases, by treating binary responses as\nbinarized versions of ordinal data. Within this framework, we show that\nbinarizing ordinal data can significantly improve the accuracy of ranking\nrecovery. Specifically, we prove that under the counting algorithm, the ranking\nerror associated with binary comparisons exhibits a faster exponential\nconvergence rate than that of ordinal data. Furthermore, we characterize a\nsubstantial performance gap between binary and ordinal data in terms of a\nsignal-to-noise ratio (SNR) determined by the pattern function. We identify the\npattern function that minimizes the SNR and maximizes the benefit of\nbinarization. Extensive simulations and a real application on the MovieLens\ndataset further corroborate our theoretical findings."}
{"id": "2507.01032", "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights."}
{"id": "2507.01410", "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "categories": ["cs.AI"], "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach."}
{"id": "2507.01687", "pdf": "https://arxiv.org/pdf/2507.01687", "abs": "https://arxiv.org/abs/2507.01687", "authors": ["Georgios Arampatzis", "Stylianos Katsarakis", "Charalambos Makridakis"], "title": "A generative modeling / Physics-Informed Neural Network approach to random differential equations", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "The integration of Scientific Machine Learning (SciML) techniques with\nuncertainty quantification (UQ) represents a rapidly evolving frontier in\ncomputational science. This work advances Physics-Informed Neural Networks\n(PINNs) by incorporating probabilistic frameworks to effectively model\nuncertainty in complex systems. Our approach enhances the representation of\nuncertainty in forward problems by combining generative modeling techniques\nwith PINNs. This integration enables in a systematic fashion uncertainty\ncontrol while maintaining the predictive accuracy of the model. We demonstrate\nthe utility of this method through applications to random differential\nequations and random partial differential equations (PDEs)."}
{"id": "2507.01034", "pdf": "https://arxiv.org/pdf/2507.01034", "abs": "https://arxiv.org/abs/2507.01034", "authors": ["Asma Agaal", "Mansour Essgaer", "Hend M. Farkash", "Zulaiha Ali Othman"], "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya", "categories": ["cs.LG", "cs.AI"], "comment": "This article was published in International Journal of Intelligent\n  Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025,\n  DOI: https://doi.org/10.5815/ijisa.2025.03.05", "summary": "Accurate electricity forecasting is crucial for grid stability and energy\nplanning, especially in Benghazi, Libya, where frequent load shedding,\ngeneration deficits, and infrastructure limitations persist. This study\nproposes a data-driven approach to forecast electricity load, generation, and\ndeficits for 2025 using historical data from 2019 (a year marked by\ninstability) and 2023 (a more stable year). Multiple time series models were\napplied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential\nsmoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural\nnetworks. The dataset was enhanced through missing value imputation, outlier\nsmoothing, and log transformation. Performance was assessed using mean squared\nerror, root mean squared error, mean absolute error, and mean absolute\npercentage error. LSTM outperformed all other models, showing strong\ncapabilities in modeling non-stationary and seasonal patterns. A key\ncontribution of this work is an optimized LSTM framework that integrates\nexogenous factors such as temperature and humidity, offering robust performance\nin forecasting multiple electricity indicators. These results provide practical\ninsights for policymakers and grid operators to enable proactive load\nmanagement and resource planning in data-scarce, volatile regions."}
{"id": "2507.01431", "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions."}
{"id": "2507.01098", "pdf": "https://arxiv.org/pdf/2507.01098", "abs": "https://arxiv.org/abs/2507.01098", "authors": ["Liu Ziyin", "Isaac Chuang"], "title": "Proof of a perfect platonic representation hypothesis", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "comment": null, "summary": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details."}
{"id": "2507.01035", "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization."}
{"id": "2507.01446", "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "categories": ["cs.AI"], "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks."}
{"id": "2507.01761", "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models."}
{"id": "2507.01037", "pdf": "https://arxiv.org/pdf/2507.01037", "abs": "https://arxiv.org/abs/2507.01037", "authors": ["Wenbin Ouyang", "Sirui Li", "Yining Ma", "Cathy Wu"], "title": "Learning to Segment for Vehicle Routing Problems", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Iterative search heuristics are widely recognized as state-of-the-art for\nsolving Vehicle Routing Problems (VRPs). In this work, we identify and exploit\na critical observation: within these solvers, a large portion of the solution\nremains stable, i.e., unchanged across search iterations, causing redundant\ncomputations, especially for large-scale VRPs with long subtours. To address\nthis, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)\ndecomposition technique to accelerate iterative solvers. Specifically, FSTA\npreserves stable solution segments during the search, aggregates nodes within\neach segment into fixed hypernodes, and focuses the search only on unstable\nportions. Yet, a key challenge lies in identifying which segments should be\naggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),\na novel neural framework to intelligently differentiate potentially stable and\nunstable portions for FSTA decomposition. We present three L2Seg variants:\nnon-autoregressive (globally comprehensive but locally indiscriminate),\nautoregressive (locally refined but globally deficient), and their synergy,\nwith bespoke training and inference strategies. Empirical results on CVRP and\nVRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up\nto 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy\nachieves best performance by combining their complementary strengths. Notably,\nL2Seg is a flexible framework that is compatible with traditional,\nlearning-based, and hybrid solvers, while supporting a broad class of VRPs."}
{"id": "2507.01489", "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "categories": ["cs.AI", "cs.MA"], "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match."}
{"id": "2507.01806", "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz Sáez de Ocáriz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning."}
{"id": "2507.01039", "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks."}
{"id": "2507.01597", "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases."}
{"id": "2507.01831", "pdf": "https://arxiv.org/pdf/2507.01831", "abs": "https://arxiv.org/abs/2507.01831", "authors": ["Yucen Lily Li", "Daohan Lu", "Polina Kirichenko", "Shikai Qiu", "Tim G. J. Rudner", "C. Bayan Bruss", "Andrew Gordon Wilson"], "title": "Out-of-Distribution Detection Methods Answer the Wrong Questions", "categories": ["cs.LG", "stat.ML"], "comment": "Extended version of ICML 2025 paper", "summary": "To detect distribution shifts and improve model safety, many\nout-of-distribution (OOD) detection methods rely on the predictive uncertainty\nor features of supervised models trained on in-distribution data. In this\npaper, we critically re-examine this popular family of OOD detection\nprocedures, and we argue that these methods are fundamentally answering the\nwrong questions for OOD detection. There is no simple fix to this misalignment,\nsince a classifier trained only on in-distribution classes cannot be expected\nto identify OOD points; for instance, a cat-dog classifier may confidently\nmisclassify an airplane if it contains features that distinguish cats from\ndogs, despite generally appearing nothing alike. We find that uncertainty-based\nmethods incorrectly conflate high uncertainty with being OOD, while\nfeature-based methods incorrectly conflate far feature-space distance with\nbeing OOD. We show how these pathologies manifest as irreducible errors in OOD\ndetection and identify common settings where these methods are ineffective.\nAdditionally, interventions to improve OOD detection such as feature-logit\nhybrid methods, scaling of model and data size, epistemic uncertainty\nrepresentation, and outlier exposure also fail to address this fundamental\nmisalignment in objectives. We additionally consider unsupervised density\nestimation and generative models for OOD detection, which we show have their\nown fundamental limitations."}
{"id": "2507.01040", "pdf": "https://arxiv.org/pdf/2507.01040", "abs": "https://arxiv.org/abs/2507.01040", "authors": ["Tianxiang Xia", "Max Neuwinger", "Lin Xiao"], "title": "Fast Clifford Neural Layers", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF"], "comment": "7 pages content-wise", "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers"}
{"id": "2507.01717", "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data."}
{"id": "2507.01041", "pdf": "https://arxiv.org/pdf/2507.01041", "abs": "https://arxiv.org/abs/2507.01041", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Songge Zhang", "Ye Wang", "Xuemin", "Shen"], "title": "Fast AI Model Splitting over Edge Networks", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 14 figures", "summary": "Split learning (SL) has emerged as a computationally efficient approach for\nartificial intelligence (AI) model training, which can alleviate device-side\ncomputational workloads. However, complex AI model architectures pose high\ncomputational complexity to obtain the optimal model splitting. In this paper,\nwe represent an arbitrary AI model as a directed acyclic graph (DAG), and then\nreformulate the optimal model splitting problem as a minimum s-t cut search\nproblem. To solve the problem, we propose a fast DAG-based model splitting\nalgorithm, which restructures the DAG to enable the optimal model splitting\nidentification via a maximum flow method. Theoretical analysis indicates that\nthe proposed algorithm is optimal. Furthermore, considering AI models with\nblock structures, we propose a block-wise model splitting algorithm to reduce\ncomputational complexity. The algorithm abstracts each block, i.e., a component\nconsisting of multiple layers, into a single vertex, thereby obtaining the\noptimal model splitting via a simplified DAG. Extensive experimental results\ndemonstrate that the proposed algorithms can determine the optimal model\nsplitting within milliseconds, as well as reduce training delay by\n24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art\nbenchmarks."}
{"id": "2507.01749", "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "categories": ["cs.AI"], "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators."}
{"id": "2507.01043", "pdf": "https://arxiv.org/pdf/2507.01043", "abs": "https://arxiv.org/abs/2507.01043", "authors": ["Szymon Świderski", "Agnieszka Jastrzębska"], "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Paper submitted to Journal of Computational Science", "summary": "The issue of data-driven neural network model construction is one of the core\nproblems in the domain of Artificial Intelligence. A standard approach assumes\na fixed architecture with trainable weights. A conceptually more advanced\nassumption is that we not only train the weights, but also find out the optimal\nmodel architecture. We present a new method that realizes just that. This\narticle is an extended version of our conference paper titled \"Dynamic Growing\nand Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the\npaper, we show in detail how to create a neural network with a procedure that\nallows dynamic shrinking and growing of the model while it is being trained.\nThe decision-making mechanism for the architectural design is governed by a\nMonte Carlo tree search procedure which simulates network behavior and allows\nto compare several candidate architecture changes to choose the best one. The\nproposed method was validated using both visual and time series datasets,\ndemonstrating its particular effectiveness in multivariate time series\nclassification. This is attributed to the architecture's ability to adapt\ndynamically, allowing independent modifications for each time series. The\napproach is supplemented by Python source code for reproducibility.\nExperimental evaluations in visual pattern and multivariate time series\nclassification tasks revealed highly promising performance, underscoring the\nmethod's robustness and adaptability."}
{"id": "2507.01833", "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "categories": ["cs.AI"], "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity."}
{"id": "2507.01045", "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring."}
{"id": "2507.01029", "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning."}
{"id": "2507.01047", "pdf": "https://arxiv.org/pdf/2507.01047", "abs": "https://arxiv.org/abs/2507.01047", "authors": ["Logan A. Burnett", "Umme Mahbuba Nabila", "Majdi I. Radaideh"], "title": "Variational Digital Twins", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "33 pages, 14 figures, and 7 tables", "summary": "While digital twins (DT) hold promise for providing real-time insights into\ncomplex energy assets, much of the current literature either does not offer a\nclear framework for information exchange between the model and the asset, lacks\nkey features needed for real-time implementation, or gives limited attention to\nmodel uncertainty. Here, we aim to solve these gaps by proposing a variational\ndigital twin (VDT) framework that augments standard neural architectures with a\nsingle Bayesian output layer. This lightweight addition, along with a novel VDT\nupdating algorithm, lets a twin update in seconds on commodity GPUs while\nproducing calibrated uncertainty bounds that can inform experiment design,\ncontrol algorithms, and model reliability. The VDT is evaluated on four\nenergy-sector problems. For critical-heat-flux prediction, uncertainty-driven\nactive learning reaches R2 = 0.98 using 47 % fewer experiments and one-third\nthe training time of random sampling. A three-year renewable-generation twin\nmaintains R2 > 0.95 for solar output and curbs error growth for volatile wind\nforecasts via monthly updates that process only one month of data at a time. A\nnuclear reactor transient cooldown twin reconstructs thermocouple signals with\nR2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating\nrobustness to degraded instrumentation. Finally, a physics-informed Li-ion\nbattery twin, retrained after every ten discharges, lowers voltage mean-squared\nerror by an order of magnitude relative to the best static model while adapting\nits credible intervals as the cell approaches end-of-life. These results\ndemonstrate that combining modest Bayesian augmentation with efficient update\nschemes turns conventional surrogates into uncertainty-aware, data-efficient,\nand computationally tractable DTs, paving the way for dependable models across\nindustrial and scientific energy systems."}
{"id": "2507.01032", "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights."}
{"id": "2507.01048", "pdf": "https://arxiv.org/pdf/2507.01048", "abs": "https://arxiv.org/abs/2507.01048", "authors": ["Ricardo Emanuel Vaz Vargas", "Afrânio José de Melo Junior", "Celso José Munaro", "Cláudio Benevenuto de Campos Lima", "Eduardo Toledo de Lima Junior", "Felipe Muntzberg Barrocas", "Flávio Miguel Varejão", "Guilherme Fidelis Peixer", "Igor de Melo Nery Oliveira", "Jader Riso Barbosa Jr.", "Jaime Andrés Lozano Cadena", "Jean Carlos Dias de Araújo", "João Neuenschwander Escosteguy Carneiro", "Lucas Gouveia Omena Lopes", "Lucas Pereira de Gouveia", "Mateus de Araujo Fernandes", "Matheus Lima Scramignon", "Patrick Marques Ciarelli", "Rodrigo Castello Branco", "Rogério Leite Alves Pinto"], "title": "3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells", "categories": ["cs.LG"], "comment": "21 pages, 10 figures, and 7 tables", "summary": "In the oil industry, undesirable events in oil wells can cause economic\nlosses, environmental accidents, and human casualties. Solutions based on\nArtificial Intelligence and Machine Learning for Early Detection of such events\nhave proven valuable for diverse applications across industries. In 2019,\nrecognizing the importance and the lack of public datasets related to\nundesirable events in oil wells, Petrobras developed and publicly released the\nfirst version of the 3W Dataset, which is essentially a set of Multivariate\nTime Series labeled by experts. Since then, the 3W Dataset has been developed\ncollaboratively and has become a foundational reference for numerous works in\nthe field. This data article describes the current publicly available version\nof the 3W Dataset, which contains structural modifications and additional\nlabeled data. The detailed description provided encourages and supports the 3W\ncommunity and new 3W users to improve previous published results and to develop\nnew robust methodologies, digital products and services capable of detecting\nundesirable events in oil wells with enough anticipation to enable corrective\nor mitigating actions."}
{"id": "2507.01034", "pdf": "https://arxiv.org/pdf/2507.01034", "abs": "https://arxiv.org/abs/2507.01034", "authors": ["Asma Agaal", "Mansour Essgaer", "Hend M. Farkash", "Zulaiha Ali Othman"], "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya", "categories": ["cs.LG", "cs.AI"], "comment": "This article was published in International Journal of Intelligent\n  Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025,\n  DOI: https://doi.org/10.5815/ijisa.2025.03.05", "summary": "Accurate electricity forecasting is crucial for grid stability and energy\nplanning, especially in Benghazi, Libya, where frequent load shedding,\ngeneration deficits, and infrastructure limitations persist. This study\nproposes a data-driven approach to forecast electricity load, generation, and\ndeficits for 2025 using historical data from 2019 (a year marked by\ninstability) and 2023 (a more stable year). Multiple time series models were\napplied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential\nsmoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural\nnetworks. The dataset was enhanced through missing value imputation, outlier\nsmoothing, and log transformation. Performance was assessed using mean squared\nerror, root mean squared error, mean absolute error, and mean absolute\npercentage error. LSTM outperformed all other models, showing strong\ncapabilities in modeling non-stationary and seasonal patterns. A key\ncontribution of this work is an optimized LSTM framework that integrates\nexogenous factors such as temperature and humidity, offering robust performance\nin forecasting multiple electricity indicators. These results provide practical\ninsights for policymakers and grid operators to enable proactive load\nmanagement and resource planning in data-scarce, volatile regions."}
{"id": "2507.01050", "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/"}
{"id": "2507.01035", "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization."}
{"id": "2507.01052", "pdf": "https://arxiv.org/pdf/2507.01052", "abs": "https://arxiv.org/abs/2507.01052", "authors": ["Ahmed Farooq"], "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "In this study we introduce a novel energy functional for long-sequence\nmemory, building upon the framework of dense Hopfield networks which achieves\nexponential storage capacity through higher-order interactions. Building upon\nearlier work on long-sequence Hopfield memory models, we propose a temporal\nkernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient\nsequential retrieval of patterns over extended sequences. We demonstrate the\nsuccessful application of this technique for the storage and sequential\nretrieval of movies frames which are well suited for this because of the high\ndimensional vectors that make up each frame creating enough variation between\neven sequential frames in the high dimensional space. The technique has\napplications in modern transformer architectures, including efficient\nlong-sequence modeling, memory augmentation, improved attention with temporal\nbias, and enhanced handling of long-term dependencies in time-series data. Our\nmodel offers a promising approach to address the limitations of transformers in\nlong-context tasks, with potential implications for natural language\nprocessing, forecasting, and beyond."}
{"id": "2507.01037", "pdf": "https://arxiv.org/pdf/2507.01037", "abs": "https://arxiv.org/abs/2507.01037", "authors": ["Wenbin Ouyang", "Sirui Li", "Yining Ma", "Cathy Wu"], "title": "Learning to Segment for Vehicle Routing Problems", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Iterative search heuristics are widely recognized as state-of-the-art for\nsolving Vehicle Routing Problems (VRPs). In this work, we identify and exploit\na critical observation: within these solvers, a large portion of the solution\nremains stable, i.e., unchanged across search iterations, causing redundant\ncomputations, especially for large-scale VRPs with long subtours. To address\nthis, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)\ndecomposition technique to accelerate iterative solvers. Specifically, FSTA\npreserves stable solution segments during the search, aggregates nodes within\neach segment into fixed hypernodes, and focuses the search only on unstable\nportions. Yet, a key challenge lies in identifying which segments should be\naggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),\na novel neural framework to intelligently differentiate potentially stable and\nunstable portions for FSTA decomposition. We present three L2Seg variants:\nnon-autoregressive (globally comprehensive but locally indiscriminate),\nautoregressive (locally refined but globally deficient), and their synergy,\nwith bespoke training and inference strategies. Empirical results on CVRP and\nVRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up\nto 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy\nachieves best performance by combining their complementary strengths. Notably,\nL2Seg is a flexible framework that is compatible with traditional,\nlearning-based, and hybrid solvers, while supporting a broad class of VRPs."}
{"id": "2507.01054", "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science."}
{"id": "2507.01039", "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks."}
{"id": "2507.01056", "pdf": "https://arxiv.org/pdf/2507.01056", "abs": "https://arxiv.org/abs/2507.01056", "authors": ["Lidan Peng", "Lu Gao", "Feng Hong", "Jingran Sun"], "title": "Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI", "categories": ["cs.LG"], "comment": null, "summary": "Flooding can damage pavement infrastructure significantly, causing both\nimmediate and long-term structural and functional issues. This research\ninvestigates how flooding events affect pavement deterioration, specifically\nfocusing on measuring pavement roughness by the International Roughness Index\n(IRI). To quantify these effects, we utilized 20 years of pavement condition\ndata from TxDOT's PMIS database, which is integrated with flood event data,\nincluding duration and spatial extent. Statistical analyses were performed to\ncompare IRI values before and after flooding and to calculate the deterioration\nrates influenced by flood exposure. Moreover, we applied Explainable Artificial\nIntelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and\nLocal Interpretable Model-Agnostic Explanations (LIME), to assess the impact of\nflooding on pavement performance. The results demonstrate that flood-affected\npavements experience a more rapid increase in roughness compared to non-flooded\nsections. These findings emphasize the need for proactive flood mitigation\nstrategies, including improved drainage systems, flood-resistant materials, and\npreventative maintenance, to enhance pavement resilience in vulnerable regions."}
{"id": "2507.01040", "pdf": "https://arxiv.org/pdf/2507.01040", "abs": "https://arxiv.org/abs/2507.01040", "authors": ["Tianxiang Xia", "Max Neuwinger", "Lin Xiao"], "title": "Fast Clifford Neural Layers", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF"], "comment": "7 pages content-wise", "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers"}
{"id": "2507.01057", "pdf": "https://arxiv.org/pdf/2507.01057", "abs": "https://arxiv.org/abs/2507.01057", "authors": ["Lushun Fan", "Yuqin Xia", "Jun Li", "Karl Jenkins"], "title": "Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "In this study, an innovative intelligent optimization system for mesh quality\nis proposed, which is based on a deep convolutional neural network\narchitecture, to achieve mesh generation and optimization. The core of the\nstudy is the Loop2Net generator and loss function, it predicts the mesh based\non the given wing coordinates. And the model's performance is continuously\noptimised by two key loss functions during the training. Then discipline by\nadding penalties, the goal of mesh generation was finally reached."}
{"id": "2507.01041", "pdf": "https://arxiv.org/pdf/2507.01041", "abs": "https://arxiv.org/abs/2507.01041", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Songge Zhang", "Ye Wang", "Xuemin", "Shen"], "title": "Fast AI Model Splitting over Edge Networks", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 14 figures", "summary": "Split learning (SL) has emerged as a computationally efficient approach for\nartificial intelligence (AI) model training, which can alleviate device-side\ncomputational workloads. However, complex AI model architectures pose high\ncomputational complexity to obtain the optimal model splitting. In this paper,\nwe represent an arbitrary AI model as a directed acyclic graph (DAG), and then\nreformulate the optimal model splitting problem as a minimum s-t cut search\nproblem. To solve the problem, we propose a fast DAG-based model splitting\nalgorithm, which restructures the DAG to enable the optimal model splitting\nidentification via a maximum flow method. Theoretical analysis indicates that\nthe proposed algorithm is optimal. Furthermore, considering AI models with\nblock structures, we propose a block-wise model splitting algorithm to reduce\ncomputational complexity. The algorithm abstracts each block, i.e., a component\nconsisting of multiple layers, into a single vertex, thereby obtaining the\noptimal model splitting via a simplified DAG. Extensive experimental results\ndemonstrate that the proposed algorithms can determine the optimal model\nsplitting within milliseconds, as well as reduce training delay by\n24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art\nbenchmarks."}
{"id": "2507.01067", "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors."}
{"id": "2507.01043", "pdf": "https://arxiv.org/pdf/2507.01043", "abs": "https://arxiv.org/abs/2507.01043", "authors": ["Szymon Świderski", "Agnieszka Jastrzębska"], "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Paper submitted to Journal of Computational Science", "summary": "The issue of data-driven neural network model construction is one of the core\nproblems in the domain of Artificial Intelligence. A standard approach assumes\na fixed architecture with trainable weights. A conceptually more advanced\nassumption is that we not only train the weights, but also find out the optimal\nmodel architecture. We present a new method that realizes just that. This\narticle is an extended version of our conference paper titled \"Dynamic Growing\nand Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the\npaper, we show in detail how to create a neural network with a procedure that\nallows dynamic shrinking and growing of the model while it is being trained.\nThe decision-making mechanism for the architectural design is governed by a\nMonte Carlo tree search procedure which simulates network behavior and allows\nto compare several candidate architecture changes to choose the best one. The\nproposed method was validated using both visual and time series datasets,\ndemonstrating its particular effectiveness in multivariate time series\nclassification. This is attributed to the architecture's ability to adapt\ndynamically, allowing independent modifications for each time series. The\napproach is supplemented by Python source code for reproducibility.\nExperimental evaluations in visual pattern and multivariate time series\nclassification tasks revealed highly promising performance, underscoring the\nmethod's robustness and adaptability."}
{"id": "2507.01068", "pdf": "https://arxiv.org/pdf/2507.01068", "abs": "https://arxiv.org/abs/2507.01068", "authors": ["Biplov Paneru"], "title": "Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors", "categories": ["cs.LG"], "comment": null, "summary": "This study leverages an Inertial Measurement Unit (IMU) dataset to develop\nexplainable AI methods for the early detection and prediction of Freezing of\nGait (FOG), a common symptom in Parkinson's disease. Machine learning models,\nincluding CatBoost, XGBoost, and Extra Trees classifiers, are employed to\naccurately categorize FOG episodes based on relevant clinical features. A\nStacking Ensemble model achieves superior performance, surpassing a hybrid\nbidirectional GRU model and reaching nearly 99% classification accuracy. SHAP\ninterpretability analysis reveals that time (seconds) is the most influential\nfactor in distinguishing gait patterns. Additionally, the proposed FOG\nprediction framework incorporates federated learning, where models are trained\nlocally on individual devices and aggregated on a central server using a\nfederated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for\nenhanced predictive capability."}
{"id": "2507.01045", "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring."}
{"id": "2507.01073", "pdf": "https://arxiv.org/pdf/2507.01073", "abs": "https://arxiv.org/abs/2507.01073", "authors": ["Dian Jin"], "title": "Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in molecular\nproperty prediction. However, traditional graph representations struggle to\neffectively encode the inherent 3D spatial structures of molecules, as\nmolecular orientations in 3D space introduce significant variability, severely\nlimiting model generalization and robustness. Existing approaches primarily\nfocus on rotation-invariant and rotation-equivariant methods. Invariant methods\noften rely heavily on prior knowledge and lack sufficient generalizability,\nwhile equivariant methods suffer from high computational costs. To address\nthese limitations, this paper proposes a novel plug-and-play 3D encoding module\nleveraging rotational sampling. By computing the expectation over the SO(3)\nrotational group, the method naturally achieves approximate rotational\ninvariance. Furthermore, by introducing a carefully designed post-alignment\nstrategy, strict invariance can be achieved without compromising performance.\nExperimental evaluations on the QM9 and C10 Datasets demonstrate superior\npredictive accuracy, robustness, and generalization performance compared to\nexisting methods. Moreover, the proposed approach maintains low computational\ncomplexity and enhanced interpretability, providing a promising direction for\nefficient and effective handling of 3D molecular information in drug discovery\nand material design."}
{"id": "2507.01050", "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/"}
{"id": "2507.01075", "pdf": "https://arxiv.org/pdf/2507.01075", "abs": "https://arxiv.org/abs/2507.01075", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "Provenance Tracking in Large-Scale Machine Learning Systems", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "As the demand for large scale AI models continues to grow, the optimization\nof their training to balance computational efficiency, execution time, accuracy\nand energy consumption represents a critical multidimensional challenge.\nAchieving this balance requires not only innovative algorithmic techniques and\nhardware architectures but also comprehensive tools for monitoring, analyzing,\nand understanding the underlying processes involved in model training and\ndeployment. Provenance data information about the origins, context, and\ntransformations of data and processes has become a key component in this\npursuit. By leveraging provenance, researchers and engineers can gain insights\ninto resource usage patterns, identify inefficiencies, and ensure\nreproducibility and accountability in AI development workflows. For this\nreason, the question of how distributed resources can be optimally utilized to\nscale large AI models in an energy efficient manner is a fundamental one. To\nsupport this effort, we introduce the yProv4ML library, a tool designed to\ncollect provenance data in JSON format, compliant with the W3C PROV and ProvML\nstandards. yProv4ML focuses on flexibility and extensibility, and enables users\nto integrate additional data collection tools via plugins. The library is fully\nintegrated with the yProv framework, allowing for higher level pairing in tasks\nrun also through workflow management systems."}
{"id": "2507.01052", "pdf": "https://arxiv.org/pdf/2507.01052", "abs": "https://arxiv.org/abs/2507.01052", "authors": ["Ahmed Farooq"], "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "In this study we introduce a novel energy functional for long-sequence\nmemory, building upon the framework of dense Hopfield networks which achieves\nexponential storage capacity through higher-order interactions. Building upon\nearlier work on long-sequence Hopfield memory models, we propose a temporal\nkernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient\nsequential retrieval of patterns over extended sequences. We demonstrate the\nsuccessful application of this technique for the storage and sequential\nretrieval of movies frames which are well suited for this because of the high\ndimensional vectors that make up each frame creating enough variation between\neven sequential frames in the high dimensional space. The technique has\napplications in modern transformer architectures, including efficient\nlong-sequence modeling, memory augmentation, improved attention with temporal\nbias, and enhanced handling of long-term dependencies in time-series data. Our\nmodel offers a promising approach to address the limitations of transformers in\nlong-context tasks, with potential implications for natural language\nprocessing, forecasting, and beyond."}
{"id": "2507.01077", "pdf": "https://arxiv.org/pdf/2507.01077", "abs": "https://arxiv.org/abs/2507.01077", "authors": ["Bogdan Bogdan", "Arina Cazacu", "Laura Vasilie"], "title": "Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels", "categories": ["cs.LG"], "comment": "6 pages, 7 figures, 4 tables, accepted to IEEE Intelligent Vehicles\n  Symposium (IV) 2025", "summary": "Anomaly detection often relies on supervised or clustering approaches, with\nlimited success in specialized domains like automotive communication systems\nwhere scalable solutions are essential. We propose a novel decoder-only Large\nLanguage Model (LLM) to detect anomalies in Electronic Control Unit (ECU)\ncommunication logs. Our approach addresses two key challenges: the lack of LLMs\ntailored for ECU communication and the complexity of inconsistent ground truth\ndata. By learning from UDP communication logs, we formulate anomaly detection\nsimply as identifying deviations in time from normal behavior. We introduce an\nentropy regularization technique that increases model's uncertainty in known\nanomalies while maintaining consistency in similar scenarios. Our solution\noffers three novelties: a decoder-only anomaly detection architecture, a way to\nhandle inconsistent labeling, and an adaptable LLM for different ECU\ncommunication use cases. By leveraging the generative capabilities of\ndecoder-only models, we present a new technique that addresses the high cost\nand error-prone nature of manual labeling through a more scalable system that\nis able to learn from a minimal set of examples, while improving detection\naccuracy in complex communication environments."}
{"id": "2507.01054", "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science."}
{"id": "2507.01078", "pdf": "https://arxiv.org/pdf/2507.01078", "abs": "https://arxiv.org/abs/2507.01078", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "yProv4ML: Effortless Provenance Tracking for Machine Learning Systems", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The rapid growth of interest in large language models (LLMs) reflects their\npotential for flexibility and generalization, and attracted the attention of a\ndiverse range of researchers. However, the advent of these techniques has also\nbrought to light the lack of transparency and rigor with which development is\npursued. In particular, the inability to determine the number of epochs and\nother hyperparameters in advance presents challenges in identifying the best\nmodel. To address this challenge, machine learning frameworks such as MLFlow\ncan automate the collection of this type of information. However, these tools\ncapture data using proprietary formats and pose little attention to lineage.\nThis paper proposes yProv4ML, a framework to capture provenance information\ngenerated during machine learning processes in PROV-JSON format, with minimal\ncode modifications."}
{"id": "2507.01067", "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors."}
{"id": "2507.01080", "pdf": "https://arxiv.org/pdf/2507.01080", "abs": "https://arxiv.org/abs/2507.01080", "authors": ["Edouard Lansiaux", "Ramy Azzouz", "Emmanuel Chazard", "Amélie Vromant", "Eric Wiel"], "title": "Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept", "categories": ["cs.LG", "cs.PF"], "comment": "15 pages, 6 figures", "summary": "Triage errors, including undertriage and overtriage, are persistent\nchallenges in emergency departments (EDs). With increasing patient influx and\nstaff shortages, the integration of artificial intelligence (AI) into triage\nprotocols has gained attention. This study compares the performance of three AI\nmodels [Natural Language Processing (NLP), Large Language Models (LLM), and\nJoint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes\nagainst the FRENCH scale and clinical practice.We conducted a retrospective\nanalysis of a prospectively recruited cohort gathering adult patient triage\ndata over a 7-month period at the Roger Salengro Hospital ED (Lille, France).\nThree AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)\nURGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic\ndetails, verbatim chief complaints, vital signs, and triage outcomes based on\nthe FRENCH scale and GEMSA coding. The primary outcome was the concordance of\nAI-predicted triage level with the FRENCH gold-standard. It was assessed thanks\nto various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM\nmodel (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared\nto JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse\ntriage (-4.343). Secondary analyses highlighted the effectiveness of\nURGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness\nwith structured data versus raw transcripts (either for GEMSA prediction or for\nFRENCH prediction). LLM architecture, through abstraction of patient\nrepresentations, offers the most accurate triage predictions among tested\nmodels. Integrating AI into ED workflows could enhance patient safety and\noperational efficiency, though integration into clinical workflows requires\naddressing model limitations and ensuring ethical transparency."}
{"id": "2507.01196", "pdf": "https://arxiv.org/pdf/2507.01196", "abs": "https://arxiv.org/abs/2507.01196", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis."}
{"id": "2507.01098", "pdf": "https://arxiv.org/pdf/2507.01098", "abs": "https://arxiv.org/abs/2507.01098", "authors": ["Liu Ziyin", "Isaac Chuang"], "title": "Proof of a perfect platonic representation hypothesis", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "comment": null, "summary": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details."}
{"id": "2507.01241", "pdf": "https://arxiv.org/pdf/2507.01241", "abs": "https://arxiv.org/abs/2507.01241", "authors": ["Di Zhang", "Yihang Zhang"], "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process."}
{"id": "2507.01117", "pdf": "https://arxiv.org/pdf/2507.01117", "abs": "https://arxiv.org/abs/2507.01117", "authors": ["Nikita Sakovich", "Dmitry Aksenov", "Ekaterina Pleshakova", "Sergey Gataullin"], "title": "A Neural Operator based on Dynamic Mode Decomposition", "categories": ["cs.LG", "68T07, 35A99"], "comment": "30 pages, 10 figures", "summary": "The scientific computation methods development in conjunction with artificial\nintelligence technologies remains a hot research topic. Finding a balance\nbetween lightweight and accurate computations is a solid foundation for this\ndirection. The study presents a neural operator based on the dynamic mode\ndecomposition algorithm (DMD), mapping functional spaces, which combines DMD\nand deep learning (DL) for spatiotemporal processes efficient modeling. Solving\nPDEs for various initial and boundary conditions requires significant\ncomputational resources. The method suggested automatically extracts key modes\nand system dynamics using them to construct predictions, reducing computational\ncosts compared to traditional numerical methods. The approach has demonstrated\nits efficiency through comparative analysis of performance with closest\nanalogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers\nequation solutions approximation, where it achieves high reconstruction\naccuracy."}
{"id": "2507.01271", "pdf": "https://arxiv.org/pdf/2507.01271", "abs": "https://arxiv.org/abs/2507.01271", "authors": ["Tatsuki Kawakami", "Kazuki Egashira", "Atsuyuki Miyai", "Go Irie", "Kiyoharu Aizawa"], "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially."}
{"id": "2507.01129", "pdf": "https://arxiv.org/pdf/2507.01129", "abs": "https://arxiv.org/abs/2507.01129", "authors": ["Arun Ganesh", "Brendan McMahan", "Abhradeep Thakurta"], "title": "On Design Principles for Private Adaptive Optimizers", "categories": ["cs.LG", "cs.CR"], "comment": "PPML 2025", "summary": "The spherical noise added to gradients in differentially private (DP)\ntraining undermines the performance of adaptive optimizers like AdaGrad and\nAdam, and hence many recent works have proposed algorithms to address this\nchallenge. However, the empirical results in these works focus on simple tasks\nand models and the conclusions may not generalize to model training in\npractice. In this paper we survey several of these variants, and develop better\ntheoretical intuition for them as well as perform empirical studies comparing\nthem. We find that a common intuition of aiming for unbiased estimates of\nsecond moments of gradients in adaptive optimizers is misguided, and instead\nthat a simple technique called scale-then-privatize (which does not achieve\nunbiased second moments) has more desirable theoretical behaviors and\noutperforms all other variants we study on a small-scale language model\ntraining task. We additionally argue that scale-then-privatize causes the noise\naddition to better match the application of correlated noise mechanisms which\nare more desirable to use in practice."}
{"id": "2507.01313", "pdf": "https://arxiv.org/pdf/2507.01313", "abs": "https://arxiv.org/abs/2507.01313", "authors": ["Qian Qi"], "title": "Neural Hamiltonian Operator", "categories": ["cs.LG", "cs.AI", "math.DS", "math.OC"], "comment": null, "summary": "Stochastic control problems in high dimensions are notoriously difficult to\nsolve due to the curse of dimensionality. An alternative to traditional dynamic\nprogramming is Pontryagin's Maximum Principle (PMP), which recasts the problem\nas a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In\nthis paper, we introduce a formal framework for solving such problems with deep\nlearning by defining a \\textbf{Neural Hamiltonian Operator (NHO)}. This\noperator parameterizes the coupled FBSDE dynamics via neural networks that\nrepresent the feedback control and an ansatz for the value function's spatial\ngradient. We show how the optimal NHO can be found by training the underlying\nnetworks to enforce the consistency conditions dictated by the PMP. By adopting\nthis operator-theoretic view, we situate the deep FBSDE method within the\nrigorous language of statistical inference, framing it as a problem of learning\nan unknown operator from simulated data. This perspective allows us to prove\nthe universal approximation capabilities of NHOs under general martingale\ndrivers and provides a clear lens for analyzing the significant optimization\nchallenges inherent to this class of models."}
{"id": "2507.01131", "pdf": "https://arxiv.org/pdf/2507.01131", "abs": "https://arxiv.org/abs/2507.01131", "authors": ["Yuchao Lin", "Cong Fu", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "$\\rm{SO}(3)$-equivariant networks are the dominant models for machine\nlearning interatomic potentials (MLIPs). The key operation of such networks is\nthe Clebsch-Gordan (CG) tensor product, which is computationally expensive. To\naccelerate the computation, we develop tensor decomposition networks (TDNs) as\na class of approximately equivariant networks whose CG tensor products are\nreplaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)\ndecomposition. With the CP decomposition, we prove (i) a uniform bound on the\ninduced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of\napproximating any equivariant bilinear map. To further reduce the number of\nparameters, we propose path-weight sharing that ties all multiplicity-space\nweights across the $O(L^3)$ CG paths into a single path without compromising\nequivariance, where $L$ is the maximum angular degree. The resulting layer acts\nas a plug-and-play replacement for tensor products in existing networks, and\nthe computational complexity of tensor products is reduced from $O(L^6)$ to\n$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation\ndataset containing 105 million DFT-calculated snapshots. We also use existing\ndatasets, including OC20, and OC22. Results show that TDNs achieve competitive\nperformance with dramatic speedup in computations."}
{"id": "2507.01321", "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4)."}
{"id": "2507.01132", "pdf": "https://arxiv.org/pdf/2507.01132", "abs": "https://arxiv.org/abs/2507.01132", "authors": ["Brenda Nogueira", "Gabe Gomes", "Meng Jiang", "Nitesh V. Chawla", "Nuno Moniz"], "title": "Spectral Manifold Harmonization for Graph Imbalanced Regression", "categories": ["cs.LG", "q-bio.MN"], "comment": null, "summary": "Graph-structured data is ubiquitous in scientific domains, where models often\nface imbalanced learning settings. In imbalanced regression, domain preferences\nfocus on specific target value ranges representing the most scientifically\nvaluable cases; we observe a significant lack of research. In this paper, we\npresent Spectral Manifold Harmonization (SMH), a novel approach for addressing\nthis imbalanced regression challenge on graph-structured data by generating\nsynthetic graph samples that preserve topological properties while focusing on\noften underrepresented target distribution regions. Conventional methods fail\nin this context because they either ignore graph topology in case generation or\ndo not target specific domain ranges, resulting in models biased toward average\ntarget values. Experimental results demonstrate the potential of SMH on\nchemistry and drug discovery benchmark datasets, showing consistent\nimprovements in predictive performance for target domain ranges."}
{"id": "2507.01327", "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits."}
{"id": "2507.01154", "pdf": "https://arxiv.org/pdf/2507.01154", "abs": "https://arxiv.org/abs/2507.01154", "authors": ["Liangyu Wang", "Junxiao Wang", "Jie Ren", "Zihang Xiang", "David E. Keyes", "Di Wang"], "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."}
{"id": "2507.01381", "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories."}
{"id": "2507.01178", "pdf": "https://arxiv.org/pdf/2507.01178", "abs": "https://arxiv.org/abs/2507.01178", "authors": ["Alec Helbling", "Duen Horng Chau"], "title": "Diffusion Explorer: Interactive Exploration of Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have been central to the development of recent image, video,\nand even text generation systems. They posses striking geometric properties\nthat can be faithfully portrayed in low-dimensional settings. However, existing\nresources for explaining diffusion either require an advanced theoretical\nfoundation or focus on their neural network architectures rather than their\nrich geometric properties. We introduce Diffusion Explorer, an interactive tool\nto explain the geometric properties of diffusion models. Users can train 2D\ndiffusion models in the browser and observe the temporal dynamics of their\nsampling process. Diffusion Explorer leverages interactive animation, which has\nbeen shown to be a powerful tool for making engaging visualizations of dynamic\nsystems, making it well suited to explaining diffusion models which represent\nstochastic processes that evolve over time. Diffusion Explorer is open source\nand a live demo is available at alechelbling.com/Diffusion-Explorer."}
{"id": "2507.01457", "pdf": "https://arxiv.org/pdf/2507.01457", "abs": "https://arxiv.org/abs/2507.01457", "authors": ["Federico Nicolas Peccia", "Frederik Haxel", "Oliver Bringmann"], "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "9 pages, 10 figures, 2 algorithms", "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions."}
{"id": "2507.01196", "pdf": "https://arxiv.org/pdf/2507.01196", "abs": "https://arxiv.org/abs/2507.01196", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis."}
{"id": "2507.01470", "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives."}
{"id": "2507.01201", "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models."}
{"id": "2507.01522", "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations."}
{"id": "2507.01208", "pdf": "https://arxiv.org/pdf/2507.01208", "abs": "https://arxiv.org/abs/2507.01208", "authors": ["Pedro R. X. Carmo", "Igor de Moura", "Assis T. de Oliveira Filho", "Djamel Sadok", "Cleber Zanchettin"], "title": "Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform", "categories": ["cs.LG", "cs.CR", "C.2.0; I.2.0"], "comment": null, "summary": "Modern vehicles are increasingly connected, and in this context, automotive\nEthernet is one of the technologies that promise to provide the necessary\ninfrastructure for intra-vehicle communication. However, these systems are\nsubject to attacks that can compromise safety, including flow injection\nattacks. Deep Learning-based Intrusion Detection Systems (IDS) are often\ndesigned to combat this problem, but they require expensive hardware to run in\nreal time. In this work, we propose to evaluate and apply fast neural network\ninference techniques like Distilling and Prunning for deploying IDS models on\nlow-cost platforms in real time. The results show that these techniques can\nachieve intrusion detection times of up to 727 {\\mu}s using a Raspberry Pi 4,\nwith AUCROC values of 0.9890."}
{"id": "2507.01551", "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."}
{"id": "2507.01216", "pdf": "https://arxiv.org/pdf/2507.01216", "abs": "https://arxiv.org/abs/2507.01216", "authors": ["Xingke Yang", "Liang Li", "Zhiyi Wan", "Sicong Li", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Tomoaki Ohtsuki", "Xin Fu", "Miao Pan"], "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."}
{"id": "2507.01649", "pdf": "https://arxiv.org/pdf/2507.01649", "abs": "https://arxiv.org/abs/2507.01649", "authors": ["Yoav Gelberg", "Yam Eitan", "Aviv Navon", "Aviv Shamsian", "Theo", "Putterman", "Michael Bronstein", "Haggai Maron"], "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature."}
{"id": "2507.01235", "pdf": "https://arxiv.org/pdf/2507.01235", "abs": "https://arxiv.org/abs/2507.01235", "authors": ["Bara Rababa", "Bilal Farooq"], "title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "categories": ["cs.LG", "quant-ph"], "comment": "Proceedings of IEEE Intelligent Transportation Systems Conference,\n  2025", "summary": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions."}
{"id": "2507.01663", "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs."}
{"id": "2507.01241", "pdf": "https://arxiv.org/pdf/2507.01241", "abs": "https://arxiv.org/abs/2507.01241", "authors": ["Di Zhang", "Yihang Zhang"], "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process."}
{"id": "2507.01679", "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research."}
{"id": "2507.01271", "pdf": "https://arxiv.org/pdf/2507.01271", "abs": "https://arxiv.org/abs/2507.01271", "authors": ["Tatsuki Kawakami", "Kazuki Egashira", "Atsuyuki Miyai", "Go Irie", "Kiyoharu Aizawa"], "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially."}
{"id": "2507.01693", "pdf": "https://arxiv.org/pdf/2507.01693", "abs": "https://arxiv.org/abs/2507.01693", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879."}
{"id": "2507.01285", "pdf": "https://arxiv.org/pdf/2507.01285", "abs": "https://arxiv.org/abs/2507.01285", "authors": ["Aymen Rayane Khouas", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Sunil Aryal"], "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation", "categories": ["cs.LG", "cs.DC", "cs.IR"], "comment": "17 pages, 5 figures", "summary": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks."}
{"id": "2507.01700", "pdf": "https://arxiv.org/pdf/2507.01700", "abs": "https://arxiv.org/abs/2507.01700", "authors": ["Andrea Piras", "Matteo Negro", "Ragib Ahsan", "David Arbour", "Elena Zheleva"], "title": "Relational Causal Discovery with Latent Confounders", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages, 19 figures. Accepted for publication at the 41st Conference\n  on Uncertainty in Artificial Intelligence (UAI 2025). Andrea Piras and Matteo\n  Negro contributed equally to this work", "summary": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders."}
{"id": "2507.01313", "pdf": "https://arxiv.org/pdf/2507.01313", "abs": "https://arxiv.org/abs/2507.01313", "authors": ["Qian Qi"], "title": "Neural Hamiltonian Operator", "categories": ["cs.LG", "cs.AI", "math.DS", "math.OC"], "comment": null, "summary": "Stochastic control problems in high dimensions are notoriously difficult to\nsolve due to the curse of dimensionality. An alternative to traditional dynamic\nprogramming is Pontryagin's Maximum Principle (PMP), which recasts the problem\nas a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In\nthis paper, we introduce a formal framework for solving such problems with deep\nlearning by defining a \\textbf{Neural Hamiltonian Operator (NHO)}. This\noperator parameterizes the coupled FBSDE dynamics via neural networks that\nrepresent the feedback control and an ansatz for the value function's spatial\ngradient. We show how the optimal NHO can be found by training the underlying\nnetworks to enforce the consistency conditions dictated by the PMP. By adopting\nthis operator-theoretic view, we situate the deep FBSDE method within the\nrigorous language of statistical inference, framing it as a problem of learning\nan unknown operator from simulated data. This perspective allows us to prove\nthe universal approximation capabilities of NHOs under general martingale\ndrivers and provides a clear lens for analyzing the significant optimization\nchallenges inherent to this class of models."}
{"id": "2507.01752", "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization."}
{"id": "2507.01321", "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4)."}
{"id": "2507.01761", "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models."}
{"id": "2507.01327", "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits."}
{"id": "2507.01781", "pdf": "https://arxiv.org/pdf/2507.01781", "abs": "https://arxiv.org/abs/2507.01781", "authors": ["Dalia Rodríguez-Salas", "Christian Riess"], "title": "BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62H30, 68T05 (Secondary)"], "comment": "18 pages, 3 figures (with two images each)", "summary": "We introduce BranchNet, a neuro-symbolic learning framework that transforms\ndecision tree ensembles into sparse, partially connected neural networks. Each\nbranch, defined as a decision path from root to a parent of leaves, is mapped\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\noptimization. The resulting models are compact, interpretable, and require no\nmanual architecture tuning. Evaluated on a suite of structured multi-class\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\naccuracy, with statistically significant gains. We detail the architecture,\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\nsymbolic interpretability as well as its current limitations, particularly on\nbinary tasks where further adaptive calibration may be beneficial."}
{"id": "2507.01354", "pdf": "https://arxiv.org/pdf/2507.01354", "abs": "https://arxiv.org/abs/2507.01354", "authors": ["Chugang Yi", "Minghan Yu", "Weikang Qian", "Yixin Wen", "Haizhao Yang"], "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion", "categories": ["cs.LG", "physics.ao-ph", "86A10 (Primary) 86A22, 68U10 (Secondary)", "J.2; I.4.4"], "comment": null, "summary": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts."}
{"id": "2507.01806", "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz Sáez de Ocáriz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning."}
{"id": "2507.01381", "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories."}
{"id": "2507.01825", "pdf": "https://arxiv.org/pdf/2507.01825", "abs": "https://arxiv.org/abs/2507.01825", "authors": ["Franco Alberto Cardillo", "Hamza Khyari", "Umberto Straccia"], "title": "MILP-SAT-GNN: Yet Another Neural SAT Solver", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\nMILP problems, which are then encoded as weighted bipartite graphs and\nsubsequently fed into a GNN for training and testing. From a theoretical\nperspective: (i) we establish permutation and equivalence invariance results,\ndemonstrating that the method produces outputs that are stable under reordering\nof clauses and variables; (ii) we identify a theoretical limitation, showing\nthat for a class of formulae called foldable formulae, standard GNNs cannot\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\nuniversal approximation theorem, establishing that with Random Node\nInitialization (RNI), the method can approximate SAT solving to arbitrary\nprecision on finite datasets, that is, the GNN becomes approximately sound and\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\nthe same approximation guarantee can be achieved without the need for RNI.\nFinally, we conduct an experimental evaluation of our approach, which show\nthat, despite the simplicity of the neural architecture, the method achieves\npromising results."}
{"id": "2507.01389", "pdf": "https://arxiv.org/pdf/2507.01389", "abs": "https://arxiv.org/abs/2507.01389", "authors": ["Anbang Wang", "Dunbo Cai", "Yu Zhang", "Yangqing Huang", "Xiangyang Feng", "Zhihong Zhang"], "title": "Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "Recently, a surrogate model was proposed that employs a factorization machine\nto approximate the underlying input-output mapping of the original system, with\nquantum annealing used to optimize the resulting surrogate function. Inspired\nby this approach, we propose an enhanced surrogate model that incorporates\nadditional slack variables into both the factorization machine and its\nassociated Ising representation thereby unifying what was by design a two-step\nprocess into a single, integrated step. During the training phase, the slack\nvariables are iteratively updated, enabling the model to account for\nhigher-order feature interactions. We apply the proposed method to the task of\npredicting drug combination effects. Experimental results indicate that the\nintroduction of slack variables leads to a notable improvement of performance.\nOur algorithm offers a promising approach for building efficient surrogate\nmodels that exploit potential quantum advantages."}
{"id": "2507.01829", "pdf": "https://arxiv.org/pdf/2507.01829", "abs": "https://arxiv.org/abs/2507.01829", "authors": ["Tristan Torchet", "Christian Metzner", "Laura Kriener", "Melika Payvand"], "title": "mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Edge devices for temporal processing demand models that capture both short-\nand long- range dynamics under tight memory constraints. While Transformers\nexcel at sequence modeling, their quadratic memory scaling with sequence length\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\noffer constant memory but train sequentially, and Temporal Convolutional\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\nThis design allows the convolutional layer to realize a flexible delay\nembedding that captures rapid temporal variations, while the recurrent module\nefficiently maintains global context with minimal memory overhead. We validate\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\nseparates and preserves multi-scale temporal features. Furthermore, on\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\noutperforms both pure convolutional and pure recurrent counterparts using\napproximately 20% less memory footprint, highlighting its suitability for\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\npromise as an efficient solution for memory-constrained multi-scale temporal\nprocessing at the edge."}
{"id": "2507.01414", "pdf": "https://arxiv.org/pdf/2507.01414", "abs": "https://arxiv.org/abs/2507.01414", "authors": ["Sultan Daniels", "Dylan Davis", "Dhruv Gautam", "Wentinn Liao", "Gireeja Ranade", "Anant Sahai"], "title": "Decomposing Prediction Mechanisms for In-Context Recall", "categories": ["cs.LG"], "comment": "44 pages, 47 figures, 2 tables", "summary": "We introduce a new family of toy problems that combine features of\nlinear-regression-style continuous in-context learning (ICL) with discrete\nassociative recall. We pretrain transformer models on sample traces from this\ntoy, specifically symbolically-labeled interleaved state observations from\nrandomly drawn linear deterministic dynamical systems. We study if the\ntransformer models can recall the state of a sequence previously seen in its\ncontext when prompted to do so with the corresponding in-context label. Taking\na closer look at this task, it becomes clear that the model must perform two\nfunctions: (1) identify which system's state should be recalled and apply that\nsystem to its last seen state, and (2) continuing to apply the correct system\nto predict the subsequent states. Training dynamics reveal that the first\ncapability emerges well into a model's training. Surprisingly, the second\ncapability, of continuing the prediction of a resumed sequence, develops much\nearlier.\n  Via out-of-distribution experiments, and a mechanistic analysis on model\nweights via edge pruning, we find that next-token prediction for this toy\nproblem involves at least two separate mechanisms. One mechanism uses the\ndiscrete symbolic labels to do the associative recall required to predict the\nstart of a resumption of a previously seen sequence. The second mechanism,\nwhich is largely agnostic to the discrete symbolic labels, performs a\n\"Bayesian-style\" prediction based on the previous token and the context. These\ntwo mechanisms have different learning dynamics.\n  To confirm that this multi-mechanism (manifesting as separate phase\ntransitions) phenomenon is not just an artifact of our toy setting, we used\nOLMo training checkpoints on an ICL translation task to see a similar\nphenomenon: a decisive gap in the emergence of first-task-token performance vs\nsecond-task-token performance."}
{"id": "2507.01875", "pdf": "https://arxiv.org/pdf/2507.01875", "abs": "https://arxiv.org/abs/2507.01875", "authors": ["Gastón García González", "Pedro Casas", "Emilio Martínez", "Alicia Fernández"], "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Presented at ACM KDD 2024, MiLeTS 2024 Workshop, August 25, 2024,\n  Barcelona, Spain", "summary": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset."}
{"id": "2507.01457", "pdf": "https://arxiv.org/pdf/2507.01457", "abs": "https://arxiv.org/abs/2507.01457", "authors": ["Federico Nicolas Peccia", "Frederik Haxel", "Oliver Bringmann"], "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "9 pages, 10 figures, 2 algorithms", "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions."}
{"id": "2507.01924", "pdf": "https://arxiv.org/pdf/2507.01924", "abs": "https://arxiv.org/abs/2507.01924", "authors": ["Samirah Bakker", "Yao Ma", "Seyed Sahand Mohammadi Ziabari"], "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings."}
{"id": "2507.01469", "pdf": "https://arxiv.org/pdf/2507.01469", "abs": "https://arxiv.org/abs/2507.01469", "authors": ["Alessio Ferrato", "Fabio Gasparetti", "Carla Limongelli", "Stefano Mastandrea", "Giuseppe Sansonetti", "Joaquín Torres-Sospedra"], "title": "Cross-platform Smartphone Positioning at Museums", "categories": ["cs.LG", "eess.SP"], "comment": "Accepted at the 2025 International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN), Tampere, Finland, September 15-18, 2025", "summary": "Indoor Positioning Systems (IPSs) hold significant potential for enhancing\nvisitor experiences in cultural heritage institutions. By enabling personalized\nnavigation, efficient artifact organization, and better interaction with\nexhibits, IPSs can transform the modalities of how individuals engage with\nmuseums, galleries and libraries. However, these institutions face several\nchallenges in implementing IPSs, including environmental constraints, technical\nlimits, and limited experimentation. In other contexts, Received Signal\nStrength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have\nemerged as preferred solutions due to their non-invasive nature and minimal\ninfrastructure requirements. Nevertheless, the lack of publicly available RSS\ndatasets that specifically reflect museum environments presents a substantial\nbarrier to developing and evaluating positioning algorithms designed for the\nintricate spatial characteristics typical of cultural heritage sites. To\naddress this limitation, we present BAR, a novel RSS dataset collected in front\nof 90 artworks across 13 museum rooms using two different platforms, i.e.,\nAndroid and iOS. Additionally, we provide an advanced position classification\nbaseline taking advantage of a proximity-based method and $k$-NN algorithms. In\nour analysis, we discuss the results and offer suggestions for potential\nresearch directions."}
{"id": "2507.01470", "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives."}
{"id": "2507.01516", "pdf": "https://arxiv.org/pdf/2507.01516", "abs": "https://arxiv.org/abs/2507.01516", "authors": ["Dibyanshu Kumar", "Philipp Vaeth", "Magda Gregorová"], "title": "Loss Functions in Diffusion Models: A Comparative Study", "categories": ["cs.LG"], "comment": "Accepted to ECML 2025", "summary": "Diffusion models have emerged as powerful generative models, inspiring\nextensive research into their underlying mechanisms. One of the key questions\nin this area is the loss functions these models shall train with. Multiple\nformulations have been introduced in the literature over the past several years\nwith some links and some critical differences stemming from various initial\nconsiderations. In this paper, we explore the different target objectives and\ncorresponding loss functions in detail. We present a systematic overview of\ntheir relationships, unifying them under the framework of the variational lower\nbound objective. We complement this theoretical analysis with an empirical\nstudy providing insights into the conditions under which these objectives\ndiverge in performance and the underlying factors contributing to such\ndeviations. Additionally, we evaluate how the choice of objective impacts the\nmodel ability to achieve specific goals, such as generating high-quality\nsamples or accurately estimating likelihoods. This study offers a unified\nunderstanding of loss functions in diffusion models, contributing to more\nefficient and goal-oriented model designs in future research."}
{"id": "2507.01522", "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations."}
{"id": "2507.01544", "pdf": "https://arxiv.org/pdf/2507.01544", "abs": "https://arxiv.org/abs/2507.01544", "authors": ["Benjamin Feuer", "Lennart Purucker", "Oussama Elachqar", "Chinmay Hegde"], "title": "MARVIS: Modality Adaptive Reasoning over VISualizations", "categories": ["cs.LG"], "comment": null, "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis"}
{"id": "2507.01551", "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation."}
{"id": "2507.01559", "pdf": "https://arxiv.org/pdf/2507.01559", "abs": "https://arxiv.org/abs/2507.01559", "authors": ["Lapo Frati", "Neil Traft", "Jeff Clune", "Nick Cheney"], "title": "How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent work in continual learning has highlighted the beneficial effect of\nresampling weights in the last layer of a neural network (``zapping\"). Although\nempirical results demonstrate the effectiveness of this approach, the\nunderlying mechanisms that drive these improvements remain unclear. In this\nwork, we investigate in detail the pattern of learning and forgetting that take\nplace inside a convolutional neural network when trained in challenging\nsettings such as continual learning and few-shot transfer learning, with\nhandwritten characters and natural images. Our experiments show that models\nthat have undergone zapping during training more quickly recover from the shock\nof transferring to a new domain. Furthermore, to better observe the effect of\ncontinual learning in a multi-task setting we measure how each individual task\nis affected. This shows that, not only zapping, but the choice of optimizer can\nalso deeply affect the dynamics of learning and forgetting, causing complex\npatterns of synergy/interference between tasks to emerge when the model learns\nsequentially at transfer time."}
{"id": "2507.01581", "pdf": "https://arxiv.org/pdf/2507.01581", "abs": "https://arxiv.org/abs/2507.01581", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang"], "title": "A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning", "categories": ["cs.LG", "cs.CR", "eess.SP"], "comment": null, "summary": "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems."}
{"id": "2507.01598", "pdf": "https://arxiv.org/pdf/2507.01598", "abs": "https://arxiv.org/abs/2507.01598", "authors": ["Naoki Sato", "Hiroki Naganuma", "Hideaki Iiduka"], "title": "Analysis of Muon's Convergence and Critical Batch Size", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a theoretical analysis of Muon, a new optimizer that\nleverages the inherent matrix structure of neural network parameters. We\nprovide convergence proofs for four practical variants of Muon: with and\nwithout Nesterov momentum, and with and without weight decay. We then show that\nadding weight decay leads to strictly tighter bounds on both the parameter and\ngradient norms, and we clarify the relationship between the weight decay\ncoefficient and the learning rate. Finally, we derive Muon's critical batch\nsize minimizing the stochastic first-order oracle (SFO) complexity, which is\nthe stochastic computational cost, and validate our theoretical findings with\nexperiments."}
{"id": "2507.01636", "pdf": "https://arxiv.org/pdf/2507.01636", "abs": "https://arxiv.org/abs/2507.01636", "authors": ["Ghasem Alipoor", "Karl Skretting"], "title": "Kernel Recursive Least Squares Dictionary Learning Algorithm", "categories": ["cs.LG", "eess.SP"], "comment": "Published in Digital Signal Processing, Volume 141, 2023. DOI:\n  https://doi.org/10.1016/j.dsp.2023.104159 12 pages, 8 figures. Code and data\n  available at: https://github.com/G-Alipoor/kernel-rls-dictionary-learning", "summary": "We propose an efficient online dictionary learning algorithm for kernel-based\nsparse representations. In this framework, input signals are nonlinearly mapped\nto a high-dimensional feature space and represented sparsely using a virtual\ndictionary. At each step, the dictionary is updated recursively using a novel\nalgorithm based on the recursive least squares (RLS) method. This update\nmechanism works with single samples or mini-batches and maintains low\ncomputational complexity. Experiments on four datasets across different domains\nshow that our method not only outperforms existing online kernel dictionary\nlearning approaches but also achieves classification accuracy close to that of\nbatch-trained models, while remaining significantly more efficient."}
{"id": "2507.01644", "pdf": "https://arxiv.org/pdf/2507.01644", "abs": "https://arxiv.org/abs/2507.01644", "authors": ["Miguel O'Malley"], "title": "Dance Dance ConvLSTM", "categories": ["cs.LG"], "comment": "15 pages, 9 figures, 4 tables", "summary": "\\textit{Dance Dance Revolution} is a rhythm game consisting of songs and\naccompanying choreography, referred to as charts. Players press arrows on a\ndevice referred to as a dance pad in time with steps determined by the song's\nchart. In 2017, the authors of Dance Dance Convolution (DDC) developed an\nalgorithm for the automatic generation of \\textit{Dance Dance Revolution}\ncharts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM\n(DDCL), a new method for the automatic generation of DDR charts using a\nConvLSTM based model, which improves upon the DDC methodology and substantially\nincreases the accuracy of chart generation."}
{"id": "2507.01649", "pdf": "https://arxiv.org/pdf/2507.01649", "abs": "https://arxiv.org/abs/2507.01649", "authors": ["Yoav Gelberg", "Yam Eitan", "Aviv Navon", "Aviv Shamsian", "Theo", "Putterman", "Michael Bronstein", "Haggai Maron"], "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature."}
{"id": "2507.01663", "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs."}
{"id": "2507.01679", "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research."}
{"id": "2507.01693", "pdf": "https://arxiv.org/pdf/2507.01693", "abs": "https://arxiv.org/abs/2507.01693", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879."}
{"id": "2507.01695", "pdf": "https://arxiv.org/pdf/2507.01695", "abs": "https://arxiv.org/abs/2507.01695", "authors": ["Omkar Shende", "Gayathri Ananthanarayanan", "Marcello Traiola"], "title": "PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable\nability to model complex patterns across various domains such as computer\nvision, speech recognition, robotics, etc. While large DNN models are often\nmore accurate than simpler, lightweight models, they are also resource- and\nenergy-hungry. Hence, it is imperative to design methods to reduce reliance on\nsuch large models without significant degradation in output accuracy. The high\ncomputational cost of these models is often necessary only for a reduced set of\nchallenging inputs, while lighter models can handle most simple ones. Thus,\ncarefully combining properties of existing DNN models in a dynamic, input-based\nway opens opportunities to improve efficiency without impacting accuracy.\n  In this work, we introduce PERTINENCE, a novel online method designed to\nanalyze the complexity of input features and dynamically select the most\nsuitable model from a pre-trained set to process a given input effectively. To\nachieve this, we employ a genetic algorithm to explore the training space of an\nML-based input dispatcher, enabling convergence towards the Pareto front in the\nsolution space that balances overall accuracy and computational efficiency.\n  We showcase our approach on state-of-the-art Convolutional Neural Networks\n(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers\n(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's\nability to provide alternative solutions to existing state-of-the-art models in\nterms of trade-offs between accuracy and number of operations. By\nopportunistically selecting among models trained for the same task, PERTINENCE\nachieves better or comparable accuracy with up to 36% fewer operations."}
{"id": "2507.01699", "pdf": "https://arxiv.org/pdf/2507.01699", "abs": "https://arxiv.org/abs/2507.01699", "authors": ["Illia Oleksiienko", "Juho Kanniainen", "Alexandros Iosifidis"], "title": "Variational Graph Convolutional Neural Networks", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication. 9\n  pages, 6 figures", "summary": "Estimation of model uncertainty can help improve the explainability of Graph\nConvolutional Networks and the accuracy of the models at the same time.\nUncertainty can also be used in critical applications to verify the results of\nthe model by an expert or additional models. In this paper, we propose\nVariational Neural Network versions of spatial and spatio-temporal Graph\nConvolutional Networks. We estimate uncertainty in both outputs and layer-wise\nattentions of the models, which has the potential for improving model\nexplainability. We showcase the benefits of these models in the social trading\nanalysis and the skeleton-based human action recognition tasks on the Finnish\nboard membership, NTU-60, NTU-120 and Kinetics datasets, where we show\nimprovement in model accuracy in addition to estimated model uncertainties."}
{"id": "2507.01700", "pdf": "https://arxiv.org/pdf/2507.01700", "abs": "https://arxiv.org/abs/2507.01700", "authors": ["Andrea Piras", "Matteo Negro", "Ragib Ahsan", "David Arbour", "Elena Zheleva"], "title": "Relational Causal Discovery with Latent Confounders", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages, 19 figures. Accepted for publication at the 41st Conference\n  on Uncertainty in Artificial Intelligence (UAI 2025). Andrea Piras and Matteo\n  Negro contributed equally to this work", "summary": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders."}
{"id": "2507.01714", "pdf": "https://arxiv.org/pdf/2507.01714", "abs": "https://arxiv.org/abs/2507.01714", "authors": ["Kevin Innerebner", "Franz M. Rohrhofer", "Bernhard C. Geiger"], "title": "B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling", "categories": ["cs.LG"], "comment": null, "summary": "Training physics-informed neural networks (PINNs) for forward problems often\nsuffers from severe convergence issues, hindering the propagation of\ninformation from regions where the desired solution is well-defined.\nHaitsiukevich and Ilin (2023) proposed an ensemble approach that extends the\nactive training domain of each PINN based on i) ensemble consensus and ii)\nvicinity to (pseudo-)labeled points, thus ensuring that the information from\nthe initial condition successfully propagates to the interior of the\ncomputational domain.\n  In this work, we suggest replacing the ensemble by a Bayesian PINN, and\nconsensus by an evaluation of the PINN's posterior variance. Our experiments\nshow that this mathematically principled approach outperforms the ensemble on a\nset of benchmark problems and is competitive with PINN ensembles trained with\ncombinations of Adam and LBFGS."}
{"id": "2507.01724", "pdf": "https://arxiv.org/pdf/2507.01724", "abs": "https://arxiv.org/abs/2507.01724", "authors": ["Micha Henheik", "Theresa Eimer", "Marius Lindauer"], "title": "Revisiting Learning Rate Control", "categories": ["cs.LG"], "comment": null, "summary": "The learning rate is one of the most important hyperparameters in deep\nlearning, and how to control it is an active area within both AutoML and deep\nlearning research. Approaches for learning rate control span from classic\noptimization to online scheduling based on gradient statistics. This paper\ncompares paradigms to assess the current state of learning rate control. We\nfind that methods from multi-fidelity hyperparameter optimization,\nfixed-hyperparameter schedules, and hyperparameter-free learning often perform\nvery well on selected deep learning tasks but are not reliable across settings.\nThis highlights the need for algorithm selection methods in learning rate\ncontrol, which have been neglected so far by both the AutoML and deep learning\ncommunities. We also observe a trend of hyperparameter optimization approaches\nbecoming less effective as models and tasks grow in complexity, even when\ncombined with multi-fidelity approaches for more expensive model trainings. A\nfocus on more relevant test tasks and new promising directions like finetunable\nmethods and meta-learning will enable the AutoML community to significantly\nstrengthen its impact on this crucial factor in deep learning."}
{"id": "2507.01740", "pdf": "https://arxiv.org/pdf/2507.01740", "abs": "https://arxiv.org/abs/2507.01740", "authors": ["Trung-Dung Hoang", "Alceu Bissoto", "Vihangkumar V. Naik", "Tim Flühmann", "Artemii Shlychkov", "José Garcia-Tirado", "Lisa M. Koch"], "title": "A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurately estimating parameters of physiological models is essential to\nachieving reliable digital twins. For Type 1 Diabetes, this is particularly\nchallenging due to the complexity of glucose-insulin interactions. Traditional\nmethods based on Markov Chain Monte Carlo struggle with high-dimensional\nparameter spaces and fit parameters from scratch at inference time, making them\nslow and computationally expensive. In this study, we propose a\nSimulation-Based Inference approach based on Neural Posterior Estimation to\nefficiently capture the complex relationships between meal intake, insulin, and\nglucose level, providing faster, amortized inference. Our experiments\ndemonstrate that SBI not only outperforms traditional methods in parameter\nestimation but also generalizes better to unseen conditions, offering real-time\nposterior inference with reliable uncertainty quantification."}
{"id": "2507.01752", "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization."}
{"id": "2507.01761", "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models."}
{"id": "2507.01781", "pdf": "https://arxiv.org/pdf/2507.01781", "abs": "https://arxiv.org/abs/2507.01781", "authors": ["Dalia Rodríguez-Salas", "Christian Riess"], "title": "BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62H30, 68T05 (Secondary)"], "comment": "18 pages, 3 figures (with two images each)", "summary": "We introduce BranchNet, a neuro-symbolic learning framework that transforms\ndecision tree ensembles into sparse, partially connected neural networks. Each\nbranch, defined as a decision path from root to a parent of leaves, is mapped\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\noptimization. The resulting models are compact, interpretable, and require no\nmanual architecture tuning. Evaluated on a suite of structured multi-class\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\naccuracy, with statistically significant gains. We detail the architecture,\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\nsymbolic interpretability as well as its current limitations, particularly on\nbinary tasks where further adaptive calibration may be beneficial."}
{"id": "2507.01803", "pdf": "https://arxiv.org/pdf/2507.01803", "abs": "https://arxiv.org/abs/2507.01803", "authors": ["Leyang Xue", "Meghana Madhyastha", "Randal Burns", "Myungjin Lee", "Mahesh K. Marina"], "title": "Towards Decentralized and Sustainable Foundation Model Training with the Edge", "categories": ["cs.LG"], "comment": null, "summary": "Foundation models are at the forefront of AI research, appealing for their\nability to learn from vast datasets and cater to diverse tasks. Yet, their\nsignificant computational demands raise issues of environmental impact and the\nrisk of centralized control in their development. We put forward a vision\ntowards decentralized and sustainable foundation model training that leverages\nthe collective compute of sparingly used connected edge AI devices. We present\nthe rationale behind our vision, particularly in support of its sustainability\nbenefit. We further outline a set of challenges that need to be addressed to\nturn this vision into reality."}
{"id": "2507.01806", "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz Sáez de Ocáriz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning."}
{"id": "2507.01823", "pdf": "https://arxiv.org/pdf/2507.01823", "abs": "https://arxiv.org/abs/2507.01823", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents", "categories": ["cs.LG", "cs.RO"], "comment": "Preprint of a manuscript submitted for peer review", "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt."}
{"id": "2507.01825", "pdf": "https://arxiv.org/pdf/2507.01825", "abs": "https://arxiv.org/abs/2507.01825", "authors": ["Franco Alberto Cardillo", "Hamza Khyari", "Umberto Straccia"], "title": "MILP-SAT-GNN: Yet Another Neural SAT Solver", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\nMILP problems, which are then encoded as weighted bipartite graphs and\nsubsequently fed into a GNN for training and testing. From a theoretical\nperspective: (i) we establish permutation and equivalence invariance results,\ndemonstrating that the method produces outputs that are stable under reordering\nof clauses and variables; (ii) we identify a theoretical limitation, showing\nthat for a class of formulae called foldable formulae, standard GNNs cannot\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\nuniversal approximation theorem, establishing that with Random Node\nInitialization (RNI), the method can approximate SAT solving to arbitrary\nprecision on finite datasets, that is, the GNN becomes approximately sound and\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\nthe same approximation guarantee can be achieved without the need for RNI.\nFinally, we conduct an experimental evaluation of our approach, which show\nthat, despite the simplicity of the neural architecture, the method achieves\npromising results."}
{"id": "2507.01829", "pdf": "https://arxiv.org/pdf/2507.01829", "abs": "https://arxiv.org/abs/2507.01829", "authors": ["Tristan Torchet", "Christian Metzner", "Laura Kriener", "Melika Payvand"], "title": "mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Edge devices for temporal processing demand models that capture both short-\nand long- range dynamics under tight memory constraints. While Transformers\nexcel at sequence modeling, their quadratic memory scaling with sequence length\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\noffer constant memory but train sequentially, and Temporal Convolutional\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\nThis design allows the convolutional layer to realize a flexible delay\nembedding that captures rapid temporal variations, while the recurrent module\nefficiently maintains global context with minimal memory overhead. We validate\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\nseparates and preserves multi-scale temporal features. Furthermore, on\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\noutperforms both pure convolutional and pure recurrent counterparts using\napproximately 20% less memory footprint, highlighting its suitability for\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\npromise as an efficient solution for memory-constrained multi-scale temporal\nprocessing at the edge."}
{"id": "2507.01831", "pdf": "https://arxiv.org/pdf/2507.01831", "abs": "https://arxiv.org/abs/2507.01831", "authors": ["Yucen Lily Li", "Daohan Lu", "Polina Kirichenko", "Shikai Qiu", "Tim G. J. Rudner", "C. Bayan Bruss", "Andrew Gordon Wilson"], "title": "Out-of-Distribution Detection Methods Answer the Wrong Questions", "categories": ["cs.LG", "stat.ML"], "comment": "Extended version of ICML 2025 paper", "summary": "To detect distribution shifts and improve model safety, many\nout-of-distribution (OOD) detection methods rely on the predictive uncertainty\nor features of supervised models trained on in-distribution data. In this\npaper, we critically re-examine this popular family of OOD detection\nprocedures, and we argue that these methods are fundamentally answering the\nwrong questions for OOD detection. There is no simple fix to this misalignment,\nsince a classifier trained only on in-distribution classes cannot be expected\nto identify OOD points; for instance, a cat-dog classifier may confidently\nmisclassify an airplane if it contains features that distinguish cats from\ndogs, despite generally appearing nothing alike. We find that uncertainty-based\nmethods incorrectly conflate high uncertainty with being OOD, while\nfeature-based methods incorrectly conflate far feature-space distance with\nbeing OOD. We show how these pathologies manifest as irreducible errors in OOD\ndetection and identify common settings where these methods are ineffective.\nAdditionally, interventions to improve OOD detection such as feature-logit\nhybrid methods, scaling of model and data size, epistemic uncertainty\nrepresentation, and outlier exposure also fail to address this fundamental\nmisalignment in objectives. We additionally consider unsupervised density\nestimation and generative models for OOD detection, which we show have their\nown fundamental limitations."}
{"id": "2507.01841", "pdf": "https://arxiv.org/pdf/2507.01841", "abs": "https://arxiv.org/abs/2507.01841", "authors": ["Yihang Gao", "Vincent Y. F. Tan"], "title": "Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "math.OC"], "comment": null, "summary": "In this paper, we propose SubLoRA, a rank determination method for Low-Rank\nAdaptation (LoRA) based on submodular function maximization. In contrast to\nprior approaches, such as AdaLoRA, that rely on first-order (linearized)\napproximations of the loss function, SubLoRA utilizes second-order information\nto capture the potentially complex loss landscape by incorporating the Hessian\nmatrix. We show that the linearization becomes inaccurate and ill-conditioned\nwhen the LoRA parameters have been well optimized, motivating the need for a\nmore reliable and nuanced second-order formulation. To this end, we reformulate\nthe rank determination problem as a combinatorial optimization problem with a\nquadratic objective. However, solving this problem exactly is NP-hard in\ngeneral. To overcome the computational challenge, we introduce a submodular\nfunction maximization framework and devise a greedy algorithm with\napproximation guarantees. We derive a sufficient and necessary condition under\nwhich the rank-determination objective becomes submodular, and construct a\nclosed-form projection of the Hessian matrix that satisfies this condition\nwhile maintaining computational efficiency. Our method combines solid\ntheoretical foundations, second-order accuracy, and practical computational\nefficiency. We further extend SubLoRA to a joint optimization setting,\nalternating between LoRA parameter updates and rank determination under a rank\nbudget constraint. Extensive experiments on fine-tuning physics-informed neural\nnetworks (PINNs) for solving partial differential equations (PDEs) demonstrate\nthe effectiveness of our approach. Results show that SubLoRA outperforms\nexisting methods in both rank determination and joint training performance."}
{"id": "2507.01875", "pdf": "https://arxiv.org/pdf/2507.01875", "abs": "https://arxiv.org/abs/2507.01875", "authors": ["Gastón García González", "Pedro Casas", "Emilio Martínez", "Alicia Fernández"], "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Presented at ACM KDD 2024, MiLeTS 2024 Workshop, August 25, 2024,\n  Barcelona, Spain", "summary": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset."}
{"id": "2507.01924", "pdf": "https://arxiv.org/pdf/2507.01924", "abs": "https://arxiv.org/abs/2507.01924", "authors": ["Samirah Bakker", "Yao Ma", "Seyed Sahand Mohammadi Ziabari"], "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings."}
{"id": "2507.01951", "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1."}
{"id": "2507.01044", "pdf": "https://arxiv.org/pdf/2507.01044", "abs": "https://arxiv.org/abs/2507.01044", "authors": ["Vivek Borkar", "Parthe Pandit"], "title": "Asymptotic convexity of wide and shallow neural networks", "categories": ["stat.ML", "cs.LG", "math.PR", "68T07"], "comment": "5 pages", "summary": "For a simple model of shallow and wide neural networks, we show that the\nepigraph of its input-output map as a function of the network parameters\napproximates epigraph of a. convex function in a precise sense. This leads to a\nplausible explanation of their observed good performance."}
{"id": "2507.01431", "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions."}
{"id": "2507.01542", "pdf": "https://arxiv.org/pdf/2507.01542", "abs": "https://arxiv.org/abs/2507.01542", "authors": ["Tom Szwagier", "Pierre-Alexandre Mattei", "Charles Bouveyron", "Xavier Pennec"], "title": "Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO", "stat.ME"], "comment": null, "summary": "Gaussian mixture models (GMMs) are ubiquitous in statistical learning,\nparticularly for unsupervised problems. While full GMMs suffer from the\noverparameterization of their covariance matrices in high-dimensional spaces,\nspherical GMMs (with isotropic covariance matrices) certainly lack flexibility\nto fit certain anisotropic distributions. Connecting these two extremes, we\nintroduce a new family of parsimonious GMMs with piecewise-constant covariance\neigenvalue profiles. These extend several low-rank models like the celebrated\nmixtures of probabilistic principal component analyzers (MPPCA), by enabling\nany possible sequence of eigenvalue multiplicities. If the latter are\nprespecified, then we can naturally derive an expectation-maximization (EM)\nalgorithm to learn the mixture parameters. Otherwise, to address the\nnotoriously-challenging issue of jointly learning the mixture parameters and\nhyperparameters, we propose a componentwise penalized EM algorithm, whose\nmonotonicity is proven. We show the superior likelihood-parsimony tradeoffs\nachieved by our models on a variety of unsupervised experiments: density\nfitting, clustering and single-image denoising."}
{"id": "2507.01613", "pdf": "https://arxiv.org/pdf/2507.01613", "abs": "https://arxiv.org/abs/2507.01613", "authors": ["Shirong Xu", "Jingnan Zhang", "Junhui Wang"], "title": "When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Paired comparison data, where users evaluate items in pairs, play a central\nrole in ranking and preference learning tasks. While ordinal comparison data\nintuitively offer richer information than binary comparisons, this paper\nchallenges that conventional wisdom. We propose a general parametric framework\nfor modeling ordinal paired comparisons without ties. The model adopts a\ngeneralized additive structure, featuring a link function that quantifies the\npreference difference between two items and a pattern function that governs the\ndistribution over ordinal response levels. This framework encompasses classical\nbinary comparison models as special cases, by treating binary responses as\nbinarized versions of ordinal data. Within this framework, we show that\nbinarizing ordinal data can significantly improve the accuracy of ranking\nrecovery. Specifically, we prove that under the counting algorithm, the ranking\nerror associated with binary comparisons exhibits a faster exponential\nconvergence rate than that of ordinal data. Furthermore, we characterize a\nsubstantial performance gap between binary and ordinal data in terms of a\nsignal-to-noise ratio (SNR) determined by the pattern function. We identify the\npattern function that minimizes the SNR and maximizes the benefit of\nbinarization. Extensive simulations and a real application on the MovieLens\ndataset further corroborate our theoretical findings."}
{"id": "2507.01687", "pdf": "https://arxiv.org/pdf/2507.01687", "abs": "https://arxiv.org/abs/2507.01687", "authors": ["Georgios Arampatzis", "Stylianos Katsarakis", "Charalambos Makridakis"], "title": "A generative modeling / Physics-Informed Neural Network approach to random differential equations", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "The integration of Scientific Machine Learning (SciML) techniques with\nuncertainty quantification (UQ) represents a rapidly evolving frontier in\ncomputational science. This work advances Physics-Informed Neural Networks\n(PINNs) by incorporating probabilistic frameworks to effectively model\nuncertainty in complex systems. Our approach enhances the representation of\nuncertainty in forward problems by combining generative modeling techniques\nwith PINNs. This integration enables in a systematic fashion uncertainty\ncontrol while maintaining the predictive accuracy of the model. We demonstrate\nthe utility of this method through applications to random differential\nequations and random partial differential equations (PDEs)."}
{"id": "2507.01717", "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data."}
