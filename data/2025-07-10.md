<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 72]
- [cs.AI](#cs.AI) [总数: 8]
- [cs.CR](#cs.CR) [总数: 29]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo, Krešimir Josić, Jae Kyoung Kim*

**主要类别:** cs.LG

**AI概要:** A new method called HADES-NN is introduced for estimating parameters in non-autonomous differential equations by approximating discontinuous signals with artificial neural networks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the challenge of fitting non-autonomous differential equation models to data when signals change abruptly.

**方法:** The method involves two stages: approximating the discontinuous signal with a smooth function using a neural network and then using this smooth function to estimate model parameters.

**结果:** HADES-NN yields highly accurate and precise parameter estimates across various applications such as circadian clock systems and yeast mating response.

**结论:** HADES-NN provides a powerful tool for fitting models to real-world data, especially in cases where signals are discontinuous.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Network-Based+Parameter+Estimation+for+Non-Autonomous+Differential+Equations+with+Discontinuous+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06267&send_immediately=true&force_search=false)

**原文摘要:** Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [2] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu, Gaurav Bagwe, Xiaoyong Yuan, Chunxiu Yu, Lan Zhang*

**主要类别:** cs.LG

**AI概要:** SEA-DBS is a proposed framework for adaptive deep brain stimulation using reinforcement learning, offering improvements in sample efficiency, exploration robustness, and compatibility with resource-constrained hardware.


<details>
  <summary>更多</summary>
  
**动机:** Conventional open-loop DBS systems lack adaptability, are energy-inefficient, and provide limited personalization. Existing RL methods for personalized aDBS control suffer from high sample complexity, unstable exploration, and limited deployability.

**方法:** SEA-DBS, a sample-efficient actor-critic framework that addresses the core challenges of RL-based adaptive neurostimulation.

**结果:** SEA-DBS demonstrates faster convergence, stronger suppression of pathological beta-band power, and resilience to post-training FP16 quantization.

**结论:** SEA-DBS offers a practical and effective RL-based aDBS framework for real-time, resource-constrained neuromodulation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample-Efficient+Reinforcement+Learning+Controller+for+Deep+Brain+Stimulation+in+Parkinson%27s+Disease，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06326，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06326&send_immediately=true&force_search=false)

**原文摘要:** Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [3] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado, P. Suárez-Serrato*

**主要类别:** cs.LG

**AI概要:** SymFlux是一个新的深度学习框架，用于执行符号回归以识别Hamiltonian函数，采用了混合CNN-LSTM架构，并在新开发的Hamiltonian矢量场数据集上进行了训练和验证，结果表明模型具有较高的准确性，推进了Hamiltonian力学的自动化发现。


<details>
  <summary>更多</summary>
  
**动机:** 提出一种新的深度学习框架SymFlux，用于执行符号回归以识别Hamiltonian函数。

**方法:** 采用混合CNN-LSTM架构对Hamiltonian函数进行符号回归学习，并输出其符号数学表达式。

**结果:** 实验结果表明，该模型能准确恢复这些符号表达式。

**结论:** SymFlux框架能够有效地从标准辛平面上的对应矢量场识别哈密顿函数，推动了哈密顿力学中的自动化发现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SymFlux%3A+deep+symbolic+regression+of+Hamiltonian+vector+fields，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06342&send_immediately=true&force_search=false)

**原文摘要:** We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [4] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang, Zelin Xu, Tingsong Xiao, Gustavo Seabra, Yanjun Li, Chenglong Li, Zhe Jiang*

**主要类别:** cs.LG

**AI概要:** This paper addresses the challenge of predicting the binding affinity of protein-ligand complexes in drug discovery. The authors propose DecoyDB, a large-scale dataset for self-supervised graph contrastive learning (GCL) on protein-ligand complexes, which helps overcome the barrier of limited labeled data. A customized GCL framework is designed for pre-training graph neural networks using DecoyDB, followed by fine-tuning with labels from PDBbind. Experimental results demonstrate that models pre-trained with DecoyDB exhibit improved accuracy, label efficiency, and generalizability.


<details>
  <summary>更多</summary>
  
**动机:** Predicting the binding affinity of protein-ligand complexes plays a vital role in drug discovery, but progress has been hindered by the lack of large-scale and high-quality binding affinity labels.

**方法:** The study proposes DecoyDB, a large-scale, structure-aware dataset specifically designed for self-supervised GCL on protein-ligand complexes. A customized GCL framework is also designed to pre-train graph neural networks based on DecoyDB and fine-tune the models with labels from PDBbind.

**结果:** Extensive experiments confirm that models pre-trained with DecoyDB achieve superior accuracy, label efficiency, and generalizability.

**结论:** DecoyDB pre-trained models achieve superior accuracy, label efficiency, and generalizability in predicting the binding affinity of protein-ligand complexes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DecoyDB%3A+A+Dataset+for+Graph+Contrastive+Learning+in+Protein-Ligand+Binding+Affinity+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06366，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06366&send_immediately=true&force_search=false)

**原文摘要:** Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [5] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour, Kathlén Kohn, Holger Rauhut*

**主要类别:** cs.LG

**AI概要:** 本文分析了深度线性卷积网络梯度流的几何特性，证明其与初始化条件无关，并对不同维度的卷积进行了详细讨论。


<details>
  <summary>更多</summary>
  
**动机:** 探索深度线性卷积网络的梯度流的几何特性，尤其是与初始化的关系。

**方法:** 研究了用于学习深度线性卷积网络的梯度流的几何性质，并建立了与初始化无关的线性卷积网络的学习方法。

**结果:** 对于D维卷积（D≥2），无论初始化如何，都可以将梯度流写成函数空间上的黎曼梯度流；对于1维卷积，如果所有步幅都大于1，结果同样成立。

**结论:** 我们证明了在参数空间上学习线性卷积网络的梯度流可以写成函数空间上的黎曼梯度流，而无需考虑初始化情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Riemannian+Geometry+associated+to+Gradient+Flows+of+Linear+Convolutional+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06367，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06367&send_immediately=true&force_search=false)

**原文摘要:** We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [6] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman, Atri Chatterjee, Swarup Bhunia*

**主要类别:** cs.LG

**AI概要:** WINGs是一种新颖的框架，用于在全连接神经网络中动态生成层权重并在推理过程中压缩卷积神经网络中的权重，从而显著减少内存需求而不牺牲准确性。


<details>
  <summary>更多</summary>
  
**动机:** 复杂的神经网络需要大量的内存来存储大量的突触权重。本文旨在介绍一种新的框架——WINGs（自动权重生成器），以在全连接神经网络（FC）中动态生成层权重，并在推理过程中压缩卷积神经网络（CNN）中的权重，从而显著减少内存需求而不牺牲准确性。

**方法:** WINGs框架使用主成分分析（PCA）进行降维，并使用轻量级支持向量回归（SVR）模型预测全连接网络（FC）中的层权重，从而消除了存储完整权重矩阵的需要，并实现了显著的内存节省。它还通过敏感性分析优先压缩卷积神经网络（CNN）中低敏感性层的权重。

**结果:** WINGs对于FC层实现了53倍的压缩，对于使用MNIST数据集的AlexNet实现了28倍的压缩，对于使用CIFAR-10数据集的AlexNet实现了18倍的压缩，准确率损失仅为1-2%。这种显著的内存减少使得DNN推理具有更高的吞吐量和更低的能耗。

**结论:** WINGs框架通过显著减少内存需求而不牺牲准确性，为资源受限的边缘应用提供了高吞吐量和低能耗的DNN推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Secure+and+Storage-Efficient+Deep+Learning+Models+for+Edge+AI+Using+Automatic+Weight+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06380，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06380&send_immediately=true&force_search=false)

**原文摘要:** Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [7] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden, Laura Driscoll, Eli Shlizerman, Eric Shea-Brown*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，将梯度流分解为两个算子的乘积，以了解非线性递归模型中学习表示的机制。


<details>
  <summary>更多</summary>
  
**动机:** 尽管近期已有进展，但仍然需要理论工具来严格理解塑造学习表示的机制，特别是在有限、非线性的模型中。

**方法:** 研究将梯度流分解为参数算子K和线性化流传播子P的乘积，并在多任务训练中使用这些算子衡量与各个子任务相关的目标如何对齐。

**结果:** 研究表明，参数算子和线性化流传播子的相互作用引发了低维潜在动力学，并且网络结构导致了塌缩现象。此外，在多任务训练中，这些算子可以用来衡量与各个子任务相关的目标如何对齐。

**结论:** 该研究通过分解梯度流提出了对非线性递归模型中梯度下降学习的新理解阶段，并提供了一个鲁棒的分析工具包KPFlow。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KPFlow%3A+An+Operator+Perspective+on+Dynamic+Collapse+Under+Gradient+Descent+Training+of+Recurrent+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06381，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06381&send_immediately=true&force_search=false)

**原文摘要:** Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [8] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande, Yalemzerf Getnet, Waltenegus Dargie*

**主要类别:** cs.LG

**AI概要:** The paper investigates the performance of different models for tamper detection and identity verification in wireless ECG systems.


<details>
  <summary>更多</summary>
  
**动机:** With the proliferation of wireless electrocardiogram (ECG) systems for health monitoring and authentication, protecting signal integrity against tampering is becoming increasingly important.

**方法:** This paper analyzes the performance of CNN, ResNet, and hybrid Transformer-CNN models for tamper detection using six emulated tampering strategies. It also evaluates the performance of a Siamese network for ECG based identity verification. The one-dimensional ECG signals are transformed into a two dimensional representation in the time frequency domain using the continuous wavelet transform (CWT).

**结果:** Experimental results show that in highly fragmented manipulation scenarios, CNN, FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding 99.5%. For subtle manipulations, the FeatCNN-TranCNN model demonstrated consistently reliable performance, achieving an average accuracy of 98%. For identity verification, the pure Transformer-Siamese network achieved an average accuracy of 98.30% while the hybrid CNN-Transformer Siamese model delivered perfect verification performance with 100% accuracy.

**结论:** In this paper, CNN, ResNet and hybrid Transformer-CNN models were effective for tamper detection in wireless ECG systems. The hybrid CNN-Transformer Siamese model delivered perfect verification performance with 100% accuracy for identity verification.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detection+of+Intelligent+Tampering+in+Wireless+Electrocardiogram+Signals+Using+Hybrid+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06402，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06402&send_immediately=true&force_search=false)

**原文摘要:** With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [9] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu, Yu Liu, Zhiyao Luo, Tingting Zhu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为KnowRare的新框架，它通过预先训练和知识图谱解决了重症监护中罕见病症的数据稀缺性和异质性问题，在测试中表现出超越现有模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管人工智能已经在常见病症的重症监护中取得了革命性的进展，但由于数据稀缺性和病症内部异质性的问题，ICU中的罕见病症仍未得到充分服务。

**方法:** 研究开发了KnowRare，一个基于领域适应的深度学习框架，用于预测ICU中罕见病症的临床结果。该框架通过自我监督预训练从多样化的电子健康记录中学习与病症无关的表示，并通过构建的病症知识图谱选择性地从临床相似的病症中适应知识。

**结果:** 在两个ICU数据集上评估的五个临床预测任务中，KnowRare持续优于现有的最先进模型，并显示出比包括APACHE IV和IV-a在内的已建立的ICU评分系统更优越的预测性能。案例研究进一步显示了KnowRare在适应数据集特定和任务特定特征、有限数据场景下推广到常见病症以及合理选择源病症方面的灵活性。

**结论:** KnowRare有潜力成为支持临床决策和改善ICU中罕见病症护理的强有力且实用的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Data+Gaps+of+Rare+Conditions+in+ICU%3A+A+Multi-Disease+Adaptation+Approach+for+Clinical+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06432&send_immediately=true&force_search=false)

**原文摘要:** Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [10] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder, Paul Zerr, Mahdad Jafarzadeh Esfahani, Martin Dresler, Matthias Krauledat*

**主要类别:** cs.LG

**AI概要:** 本文介绍了eegFloss，一个开源的Python包，它利用新的机器学习模型eegUsability来检测睡眠EEG记录中的伪影段。


<details>
  <summary>更多</summary>
  
**动机:** EEG信号容易受到由内部（设备特定）因素和外部（环境）干扰引起的伪影的影响。随着睡眠研究的规模扩大，大多数研究依赖于自动睡眠分期，这个过程高度易受伪影的影响，导致错误的睡眠评分。

**方法:** 本论文介绍了一个开源的Python包eegFloss，该包利用一种新的机器学习模型eegUsability来检测睡眠EEG记录中的伪影段。eegUsability已经在从15个参与者处收集的127个夜晚的手动标记EEG数据上进行了训练和评估。

**结果:** eegUsability展现了坚实的总体分类性能（F1分数约为0.85，Cohen's kappa为0.78），在识别通道可用的EEG数据时达到了约94%的高召回率，并且超越了Zmax。此外，eegFloss还提供了其他功能，如使用另一种名为eegMobility的ML模型进行自动床上时间检测，过滤某些伪影，以及生成睡眠图和睡眠统计数据。

**结论:** eegFloss通过解决大多数睡眠研究面临的基本挑战，可以提高分析的精确性和严谨性，以及结果的准确性和可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是eegFloss%3A+A+Python+package+for+refining+sleep+EEG+recordings+using+machine+learning+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06433，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06433&send_immediately=true&force_search=false)

**原文摘要:** Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [11] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li, Jenny Kaufmann, Martin Wattenberg, David Alvarez-Melis, Naomi Saphra*

**主要类别:** cs.LG

**AI概要:** 本研究探索了解释性作为预测分布外（OOD）模型行为的工具的潜力和挑战，特别是注意模式和OOD泛化之间的对应关系。


<details>
  <summary>更多</summary>
  
**动机:** 解释性研究通常旨在预测模型如何响应对特定机制的针对性干预，但很少预测模型如何响应看不见的输入数据。本文探索了解释性作为预测分布外(OOD)模型行为工具的前景和挑战。

**方法:** 研究了在独立训练的数百个变压器模型中，注意模式与OOD泛化的对应关系，并进行了消融测试。

**结果:** 发现当分布内注意力展示出层次结构时，模型可能在OOD数据上进行层次泛化，即使规则的实现不依赖于这些层次结构。

**结论:** 简单的观察工具可以预测模型的OOD性能，并且当注意力机制展示出层次结构时，模型可能在OOD数据上进行层次泛化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Interpretation+Predict+Behavior+on+Unseen+Data%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06445&send_immediately=true&force_search=false)

**原文摘要:** Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [12] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long, Qiyuan Wang, Christos Anagnostopoulos, Daning Bi*

**主要类别:** cs.LG

**AI概要:** This paper introduces a novel approach, FedPhD, designed to efficiently train Diffusion Models in Federated Learning environments.


<details>
  <summary>更多</summary>
  
**动机:** FL is beneficial for distributed training of DMs, but challenges such as high communication costs and data heterogeneity persist. Limited research has addressed these issues in FL environments.

**方法:** The proposed FedPhD approach leverages Hierarchical FL with homogeneity-aware model aggregation and selection policy, as well as distributed structured pruning.

**结果:** Experiments across multiple datasets demonstrate that FedPhD achieves high model performance regarding FID scores while reducing communication costs by up to 88%.

**结论:** FedPhD is an effective approach for training DMs in FL environments with reduced communication costs and improved model performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedPhD%3A+Federated+Pruning+with+Hierarchical+Learning+of+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06449，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06449&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [13] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee, David Martinez, Camille Dang, Ethan Tam*

**主要类别:** cs.LG

**AI概要:** This paper introduces the first automated framework for labeling every neuron in a protein language model with biologically grounded natural language descriptions, revealing insights into the selective sensitivity of individual neurons to diverse biochemical and structural properties.


<details>
  <summary>更多</summary>
  
**动机:** Protein language models encode rich biological information, yet their internal neuron representations are poorly understood. There is a need for an automated framework for labeling every neuron in a PLM with biologically grounded natural language descriptions.

**方法:** The proposed method scales to hundreds of thousands of neurons, revealing individual neurons are selectively sensitive to diverse biochemical and structural properties. A novel neuron activation-guided steering method is developed to generate proteins with desired traits.

**结果:** The framework reveals that individual neurons are selectively sensitive to diverse biochemical and structural properties. The novel neuron activation-guided steering method enables convergence to target biochemical properties and structural motifs.

**结论:** The analysis of labeled neurons in different model sizes reveals PLM scaling laws and a structured neuron space distribution, indicating that the proposed framework could provide insights into the internal workings of protein language models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Neuron+Labelling+Enables+Generative+Steering+and+Interpretability+in+Protein+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06458&send_immediately=true&force_search=false)

**原文摘要:** Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [14] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal, Supriyo Datta, Joseph G. Makin*

**主要类别:** cs.LG

**AI概要:** This paper proposes forward-forward algorithms for binary, stochastic units that can reduce energy consumption by about one order of magnitude while maintaining performance close to real-valued forward-forward.


<details>
  <summary>更多</summary>
  
**动机:** Reducing energy consumption has become a pressing need for modern machine learning, especially for training larger and more energy-consumptive neural networks.

**方法:** The study derives forward-forward algorithms for binary, stochastic units, transforming matrix multiplications into indexing operations which can be executed efficiently in hardware. Binary sampling is implemented with p-bits for fast and cheap execution.

**结果:** The proposed algorithms show performance close to real-valued forward-forward on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, with an estimated energy savings of about one order of magnitude.

**结论:** The proposed forward-forward algorithms for binary, stochastic units can perform close to real-valued forward-forward with an estimated energy savings of about one order of magnitude.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Efficient+Supervised+Learning+with+a+Binary+Stochastic+Forward-Forward+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06461，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06461&send_immediately=true&force_search=false)

**原文摘要:** Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [15] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng, Shuang Qin, Yue Yu, Fangqing Jiang, Hui Wang, Wen Gao*

**主要类别:** cs.LG

**AI概要:** This paper explores the reasons behind Adam's success and limitations in training deep neural networks and proposes a new optimizer, SignSoftSGD (S3), which improves upon Adam's performance and stability.


<details>
  <summary>更多</summary>
  
**动机:** The mechanisms underlying Adam's empirical successes and limitations remain underexplored. This study aims to understand these aspects and propose an improved optimizer.

**方法:** The study proposes SignSoftSGD (S3), a novel optimizer with three key innovations: flexible p-th order momentum, unified exponential moving average coefficients, and an equivalent Nesterov's accelerated gradient module.

**结果:** S3 achieves the optimal convergence rate for general nonconvex stochastic optimization and shows rapid convergence, improved performance, and rare loss spikes even with a 10x larger learning rate.

**结论:** S3 delivers performance comparable to or better than AdamW with 2x the training steps, establishing its efficacy in both efficiency and final task performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SoftSignSGD%28S3%29%3A+An+Enhanced+Optimizer+for+Practical+DNN+Training+and+Loss+Spikes+Minimization+Beyond+Adam，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06464，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06464&send_immediately=true&force_search=false)

**原文摘要:** Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [16] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna, Cong Lu, Jeff Clune*

**主要类别:** cs.LG

**AI概要:** This paper introduces Foundation-Model Self-Play (FMSP), a new approach that uses foundation models to improve upon traditional self-play algorithms in multi-agent interactions.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to overcome the limitations of traditional self-play algorithms which often fail to produce diverse solutions and can get stuck in locally optimal behaviors by introducing Foundation-Model Self-Play (FMSP).

**方法:** The paper proposes a family of approaches including Vanilla Foundation-Model Self-Play (vFMSP), Novelty-Search Self-Play (NSSP), and Quality-Diversity Self-Play (QDSP) that leverage the code-generation capabilities and vast knowledge of foundation models to overcome challenges in self-play algorithms.

**结果:** In Car Tag, FMSPs explore a wide variety of methods and surpass strong human-designed strategies in terms of discovered policy quality. In Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through and jailbreaking six different levels of defense and also automatically patch the discovered vulnerabilities.

**结论:** FMSPs represent a promising new research frontier of improving self-play with foundation models, opening fresh paths toward more creative and open-ended strategy discovery.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+Model+Self-Play%3A+Open-Ended+Strategy+Innovation+via+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06466，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06466&send_immediately=true&force_search=false)

**原文摘要:** Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [17] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song, Yuecen Wei, Yuhang Lu, Qingyun Sun, Minglai Shao, Li-e Wang, Chunming Hu, Xianxian Li, Xingcheng Fu*

**主要类别:** cs.LG

**AI概要:** In this paper, a novel dual-view graph representation learning method (MimbFD) is proposed for mitigating the message imbalance problem in GNN-based fraud detection.


<details>
  <summary>更多</summary>
  
**动机:** the problem of imbalanced supervisory messages is caused by fraudsters' topological behavior obfuscation and identity feature concealment.

**方法:** a novel dual-view graph representation learning method to mitigate Message imbalance in Fraud Detection(MimbFD).

**结果:** results demonstrate that MimbFD exhibits outstanding performance in fraud detection.

**结论:** MimbFD exhibits outstanding performance in fraud detection.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Message+Imbalance+in+Fraud+Detection+with+Dual-View+Graph+Representation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06469&send_immediately=true&force_search=false)

**原文摘要:** Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [18] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang, Haoran Li, Huaming Chen, Jun Yan, Jiahua Shi, Jun Shen*

**主要类别:** cs.LG

**AI概要:** This paper introduces diffusion models into the federated learning paradigm and proposes a novel diffusion-inspired Federated paradigm with Diffusion Representation Collaboration, termed FedDifRC, to mitigate data heterogeneity.


<details>
  <summary>更多</summary>
  
**动机:** One major challenge for federated learning paradigm is the data heterogeneity issue, where biased data preferences across multiple clients harm the model's convergence and performance.

**方法:** FedDifRC leverages meaningful guidance of diffusion models to mitigate data heterogeneity. The key idea is to construct text-driven diffusion contrasting and noise-driven diffusion regularization, aiming to provide abundant class-related semantic information and consistent convergence signals.

**结果:** FedDifRC can be extended to a self-supervised scheme without relying on any labeled data. A theoretical analysis for FedDifRC ensures convergence under non-convex objectives.

**结论:** The experiments on different scenarios validate the effectiveness of FedDifRC and the efficiency of crucial components.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedDifRC%3A+Unlocking+the+Potential+of+Text-to-Image+Diffusion+Models+in+Heterogeneous+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06482，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06482&send_immediately=true&force_search=false)

**原文摘要:** Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [19] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu, Chenyu Zhang, Junjie Song, Siqi Chen, Sun Yin, Zihan Wang, Lingming Zeng, Yuji Cao, Junming Jiao*

**主要类别:** cs.LG

**AI概要:** This paper proposes MoFE-Time, a novel time series forecasting model that combines time and frequency domain features using a Mixture of Experts network. Trained in a pretraining-finetuning approach, it outperforms existing methods, demonstrating superior performance on several benchmarks and a proprietary dataset.


<details>
  <summary>更多</summary>
  
**动机:** Time series forecasting is crucial for various applications. Despite the success of existing models using Large Language Models (LLMs), there's an opportunity to improve predictions of complex time series by modeling both time and frequency characteristics, which requires capturing periodicity and prior pattern knowledge of signals.

**方法:** The proposed method, MoFE-Time, integrates time and frequency domain features within a Mixture of Experts (MoE) network. It introduces both frequency and time cells as experts after attention modules and leverages the MoE routing mechanism to construct multidimensional sparse representations of input signals. The training framework follows a pretraining-finetuning paradigm.

**结果:** MoFE-Time achieved state-of-the-art performance on six public benchmarks, reducing MSE and MAE by 6.95% and 6.02% compared to Time-MoE. Additionally, it excelled on the proprietary NEV-sales dataset derived from real-world business scenarios.

**结论:** The MoFE-Time model, integrating time and frequency domain features within a Mixture of Experts network, achieves new state-of-the-art performance in time series forecasting. It effectively transfers prior pattern knowledge across datasets with different periodicity distributions, underscoring its effectiveness in practical commercial applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoFE-Time%3A+Mixture+of+Frequency+Domain+Experts+for+Time-Series+Forecasting+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06502，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06502&send_immediately=true&force_search=false)

**原文摘要:** As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [20] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang, Gustavo Batista, Salil S. Kanhere*

**主要类别:** cs.LG

**AI概要:** This paper proposes a family of novel monotonic post-hoc calibration methods that ensure expressiveness, robustness, and interpretability while preserving the relative ordering of the probability output.


<details>
  <summary>更多</summary>
  
**动机:** Deep neural networks often produce miscalibrated probability estimates, leading to overconfident predictions. Most existing post-hoc calibration methods do not guarantee monotonicity and previous monotonic approaches lack interpretability and robustness.

**方法:** A family of novel monotonic post-hoc calibration methods, which employs a constrained calibration map parameterized linearly with respect to the number of classes.

**结果:** The proposed methods achieve state-of-the-art performance across datasets with different deep neural network models.

**结论:** The proposed monotonic post-hoc calibration methods outperform existing calibration methods while being data and computation-efficient.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Instance-Wise+Monotonic+Calibration+by+Constrained+Transformation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06516，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06516&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [21] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang, Fang Xie*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的差分隐私随机梯度下降算法AdaDPIGU，通过基于重要性的梯度更新和自适应稀疏化处理，解决了现有方法在高维情况下性能下降的问题，并在多个基准测试中验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的差分隐私随机梯度下降方法在高维环境下表现不佳，因为注入噪声的规模随维度增加而增大。为了解决这个问题，研究旨在设计一种能够减少噪音影响并在深度神经网络中保持模型性能的算法。

**方法:** 研究提出了一种新的差分隐私SGD框架AdaDPIGU，它使用基于重要性的梯度更新方法。在预训练阶段，应用差分隐私高斯机制来估计每个参数的重要性，同时保护隐私。在梯度更新阶段，修剪低重要性坐标，并引入逐坐标自适应裁剪机制，实现稀疏且噪声效率高的梯度更新。

**结果:** 理论分析证明了AdaDPIGU满足$(\varepsilon, \delta)$-差分隐私并保留了收敛性保证。实验结果表明，在MNIST数据集上，当隐私预算$\epsilon = 8$时，该方法达到了99.12%的测试准确率；在CIFAR-10数据集上，当$\epsilon = 4$时，达到了73.21%的准确率，超过了非私有基线模型的71.12%。所有的结果都是在固定的60%保留比率下报告的。

**结论:** AdaDPIGU在保证隐私的同时，通过自适应稀疏化增强了模型的性能，并在不同的隐私预算下超过了非私有基线模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AdaDPIGU%3A+Differentially+Private+SGD+with+Adaptive+Clipping+and+Importance-Based+Gradient+Updates+for+Deep+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06525，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06525&send_immediately=true&force_search=false)

**原文摘要:** Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [22] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang, Yuxin Chen*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel direct regret optimization approach for Bayesian optimization that jointly learns the optimal model and non-myopic acquisition, leveraging an ensemble of Gaussian Processes and training an end-to-end decision transformer. Experimental results show consistent outperformance compared to traditional BO baselines.


<details>
  <summary>更多</summary>
  
**动机:** Traditional BO methods typically rely on separate hand-crafted acquisition functions and surrogate models for the underlying function, and often operate in a myopic manner. The motivation is to propose a new approach that explicitly targets minimizing the multi-step regret.

**方法:** A novel direct regret optimization approach that jointly learns the optimal model and non-myopic acquisition by distilling from a set of candidate models and acquisitions, leveraging an ensemble of Gaussian Processes (GPs) with varying hyperparameters to generate simulated BO trajectories, training an end-to-end decision transformer that directly learns to select next query points aimed at improving the ultimate objective, and adopting a dense training--sparse learning paradigm.

**结果:** Experimental results on synthetic and real-world benchmarks suggest that our method consistently outperforms BO baselines, achieving lower simple regret and demonstrating more robust exploration in high-dimensional or noisy settings.

**结论:** The proposed method consistently outperforms traditional BO baselines, achieving lower simple regret and demonstrating more robust exploration in high-dimensional or noisy settings.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Direct+Regret+Optimization+in+Bayesian+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06529&send_immediately=true&force_search=false)

**原文摘要:** Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [23] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen, Shenglu Hua, Jiajun Zou, Jiawei Liu, Jianwang Zhai, Chuan Shi, Wenjian Yu*

**主要类别:** cs.LG

**AI概要:** This paper introduces CircuitGCL, a novel method for enhancing transferability across heterogeneous circuit graphs in AMS circuits.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to overcome the challenges posed by scarce design data, unbalanced label distribution, and circuit implementation diversity in learning robust and transferable circuit representations.

**方法:** The paper proposes CircuitGCL, a graph contrastive learning framework that uses hyperspherical representation scattering and introduces balanced MSE and bsmCE losses.

**结果:** CircuitGCL shows an improvement of R2 by 33.64% ~ 44.20% for edge regression and F1-score gain by 0.9x ~ 2.1x for node classification tasks.

**结论:** CircuitGCL surpasses all current state-of-the-art methods in the field of parasitic capacitance estimation and ground capacitance classification for TSMC 28nm AMS designs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transferable+Parasitic+Estimation+via+Graph+Contrastive+Learning+and+Label+Rebalancing+in+AMS+Circuits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06535&send_immediately=true&force_search=false)

**原文摘要:** Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [24] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen, Yibin Zhang, Hector Rodriguez Rodriguez, Wenjian Yu*

**主要类别:** cs.LG

**AI概要:** This paper presents CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS circuits.


<details>
  <summary>更多</summary>
  
**动机:** Training deep learning models for AMS designs is severely limited by the scarcity of integrated circuit design data.

**方法:** Circuit netlist is represented as a heterogeneous graph. CircuitGPS starts with a small-hop sampling technique that converts a link or a node into a subgraph. Then, the subgraph embeddings are learned with a hybrid graph Transformer. Additionally, it integrates a low-cost positional encoding.

**结果:** CircuitGPS improves the accuracy of coupling existence by at least 20% and reduces the MAE of capacitance estimation by at least 0.067 compared to existing methods.

**结论:** CircuitGPS improves the accuracy of coupling existence and reduces the MAE of capacitance estimation. It demonstrates strong inherent scalability, enabling direct application to diverse AMS circuit designs through zero-shot learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Few-shot+Learning+on+AMS+Circuits+and+Its+Application+to+Parasitic+Capacitance+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06538，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06538&send_immediately=true&force_search=false)

**原文摘要:** Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [25] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu, Tianyu Zhang, Mingze Wang, Zhanpeng Zhou, Can Wang*

**主要类别:** cs.LG

**AI概要:** This paper explores how to schedule communication in decentralized learning and finds that concentrating communication budgets in the later stages significantly improves global generalization.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to improve the performance of decentralized learning which is often hindered by limited peer-to-peer communication.

**方法:** The paper studies how communication should be scheduled over time in decentralized learning, including determining when and how frequently devices synchronize. The authors also provide a theoretical analysis of why decentralized SGD can converge faster than centralized mini-batch SGD.

**结果:** Concentrating communication budgets in the later stages of decentralized training markedly improves global generalization. Fully connected communication at the final step, implemented by a single global merging, is sufficient to match the performance of server-based training. Low communication in decentralized learning preserves the mergeability of local models throughout training.

**结论:** This work challenges the common belief that decentralized learning generalizes poorly under data heterogeneity and limited communication, while offering new insights into model merging and neural network loss landscapes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Single+Merging+Suffices%3A+Recovering+Server-based+Learning+Performance+in+Decentralized+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06542&send_immediately=true&force_search=false)

**原文摘要:** Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [26] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen, Dingcheng Yang, Yuyang Xie, Chunyan Pei, Wenjian Yu, Bei Yu*

**主要类别:** cs.LG

**AI概要:** This paper proposes a deep-learning-based 2-stage model to accurately predict parasitics in pre-layout stages for SRAM circuits, which helps reduce design iterations and improve simulation efficiency.


<details>
  <summary>更多</summary>
  
**动机:** To achieve higher system energy efficiency, SRAM in SoCs is often customized. However, the parasitic effects cause discrepancies between pre-layout and post-layout circuit simulations, making it difficult to converge design parameters and causing excessive design iterations.

**方法:** A deep-learning-based 2-stage model combining a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron (MLP) regressors is employed. Focal Loss is used to mitigate the impact of abundant internal net samples, and subcircuit information is integrated into the graph to abstract the hierarchical structure of schematics.

**结果:** Experiments on 4 real SRAM designs show that our approach not only surpasses the state-of-the-art model in parasitic prediction by a maximum of 19X reduction of error but also significantly boosts the simulation process by up to 598X speedup.

**结论:** The proposed deep-learning-based 2-stage model can accurately predict parasitics in pre-layout stages, which not only surpasses the state-of-the-art model in parasitic prediction but also significantly boosts the simulation process.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep-Learning-Based+Pre-Layout+Parasitic+Capacitance+Prediction+on+SRAM+Designs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06549，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06549&send_immediately=true&force_search=false)

**原文摘要:** To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [27] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang, Haoran Li, Yifeng Zhang, Guoqiang Gong, Jiaxing Wang, Pengzhang Liu, Qixia Jiang, Junxing Hu*

**主要类别:** cs.LG

**AI概要:** This paper introduces LoRAM, an initialization scheme that retains the efficiency of LoRA and matches or outperforms spectral methods without their inefficiencies.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of the paper is to address the inefficiencies in computational and storage overheads posed by spectral initialization methods for tuning large models under the Low-Rank Adaptation (LoRA) paradigm.

**方法:** The paper proposes LoRAM, a magnitude-driven 'Basis & Basis' initialization scheme. It scales deterministic orthogonal bases using pretrained weight magnitudes to simulate spectral gains.

**结果:** Extensive experiments show that LoRAM matches or outperforms spectral initialization across benchmarks while retaining the full efficiency of LoRA.

**结论:** LoRAM provides a strong baseline that retains the full efficiency of LoRA while matching or outperforming spectral initialization across benchmarks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Primacy+of+Magnitude+in+Low-Rank+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06558，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06558&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [28] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen, Xianhao Chen, Kaibin Huang*

**主要类别:** cs.LG

**AI概要:** This paper addresses the storage burden of Mixture-of-Experts (MoE) models on edge devices by optimizing expert caching under storage constraints.


<details>
  <summary>更多</summary>
  
**动机:** The storage burden of expert networks in MoE models on edge devices needs to be addressed for efficient distributed inference.

**方法:** A successive greedy decomposition method and an accelerated algorithm based on max-convolution technique are proposed.

**结果:** Simulation results show significant reduction in inference latency compared to existing baselines.

**结论:** The proposed method significantly reduces inference latency for MoE models in edge networks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SlimCaching%3A+Edge+Caching+of+Mixture-of-Experts+for+Distributed+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06567，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06567&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [29] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen, Minpeng Liao, Guoxin Chen, Chengxi Li, Biao Fu, Kai Fan, Xinggao Liu*

**主要类别:** cs.LG

**AI概要:** This paper introduces LPPO, a framework for reinforcement learning with verifiable rewards that leverages high-quality demonstrations through progressive optimization techniques.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind the LPPO framework is to leverage a small set of trusted, high-quality demonstrations rather than simply scaling up data volume. The research is inspired by how hints aid human problem-solving and how humans focus on important questions aligned with their current capabilities.

**方法:** LPPO (Learning-Progress and Prefix-guided Optimization) is a framework of progressive optimization techniques. It includes prefix-guided sampling, which is an online data augmentation method incorporating partial solution prefixes from expert demonstrations, and learning-progress weighting, a dynamic strategy adjusting each training sample's influence based on model progression.

**结果:** Experiments on mathematical-reasoning benchmarks demonstrate that LPPO methods outperform strong baselines, yielding faster convergence and a higher performance ceiling.

**结论:** The LPPO framework with prefix-guided sampling and learning-progress weighting outperforms strong baselines in mathematical reasoning benchmarks, offering faster convergence and a higher performance ceiling.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Data-Centric+to+Sample-Centric%3A+Enhancing+LLM+Reasoning+via+Progressive+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06573，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06573&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [30] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley, Friedrich T. Sommer*

**主要类别:** cs.LG

**AI概要:** 本文探讨了使用预测信息增益来确定环境中最具信息的区域进行探索的方法，并应用强化学习方法找到次优的探索策略。


<details>
  <summary>更多</summary>
  
**动机:** 研究如何在没有显式模型的情况下，通过探索环境来学习可控动态系统的最具有信息的区域。

**方法:** 应用强化学习方法来寻找好的次优探索策略，并使用预测信息增益作为信息度量标准。

**结果:** 与几种近视探索方法相比，发现预测信息增益方法能够可靠地估计底层可控动力学。

**结论:** 使用预测信息增益的方法可以找到次优的探索策略，并且比近视探索方法更可靠。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+controllable+dynamics+through+informative+exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06582，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06582&send_immediately=true&force_search=false)

**原文摘要:** Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [31] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel, Yu Wang, Cristian Tatino, Pablo Soldati*

**主要类别:** cs.LG

**AI概要:** 为了应对现代RAN在高度动态和异构环境中的挑战，我们提出了一个以泛化为中心的RL框架。


<details>
  <summary>更多</summary>
  
**动机:** 现代RAN在高度动态和异构环境中运行，传统的手调、基于规则的RRM算法常常表现不佳。虽然在受限环境下RL能够超越这些启发式方法，但部署的多样性和不可预测的无线条件带来了主要的泛化挑战。数据驱动的策略经常对训练条件过拟合，导致在未见过的场景中性能下降。

**方法:** 提出了一种以泛化为中心的RL框架，该框架：（i）通过基于注意力的图表示编码小区拓扑和节点属性;（ii）应用领域随机化来拓宽训练分布;（iii）在多个执行器之间分配数据生成，同时在一个与O-RAN原则相符的云兼容架构中集中进行训练。

**结果:** 应用于五个5G基准中的下行链路自适应，我们的策略在全缓冲MIMO/mMIMO上比OLL基线（10% BLER目标）平均吞吐量和频谱效率提高了~10%，在高移动性下提高了>20%。它在全缓冲流量中匹配了专门的RL，并在eMBB和混合流量基准测试中分别实现了4倍和2倍的增益。在九个小区的部署中，GAT模型提供了比MLP基线高出30%的吞吐量。

**结论:** 现代RAN在高度动态和异构环境中运行，传统的基于规则的RRM算法通常表现不佳。本文提出的RL框架通过提升数据收集和训练规模，为AI原生6G RAN提供了一条路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+in+Reinforcement+Learning+for+Radio+Access+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06602，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06602&send_immediately=true&force_search=false)

**原文摘要:** Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [32] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal, Yuhta Takida, Chieh-Hsin Lai, Yuki Mitsufuji*

**主要类别:** cs.LG

**AI概要:** This paper presents a novel generative modeling framework that balances disentanglement and generation quality by leveraging a range of beta values for learning multiple latent representations and uses a non-linear diffusion model for smooth transitions.


<details>
  <summary>更多</summary>
  
**动机:** To address the trade-off between disentanglement and reconstruction quality in generative models, where disentangled representations often come at the cost of generation quality.

**方法:** A novel generative modeling framework is introduced, which leverages a range of beta values to learn multiple corresponding latent representations. A new loss function controls the information retained in each latent representation, and a non-linear diffusion model denoises towards less disentangled and more informative representations.

**结果:** The framework achieves a balance between disentanglement and generation quality, supports sample generation without input images, and facilitates consistent manipulation of generated outputs with smooth transitions in the latent spaces.

**结论:** The proposed generative modeling framework successfully balances disentanglement and generation quality, providing smooth transitions in the latent space.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Denoising+Multi-Beta+VAE%3A+Representation+Learning+for+Disentanglement+and+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06613&send_immediately=true&force_search=false)

**原文摘要:** Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [33] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng*

**主要类别:** cs.LG

**AI概要:** The paper presents a novel framework called Cross-Task Policy Guidance (CTPG) for multi-task reinforcement learning, which improves the performance of manipulation and locomotion benchmarks.


<details>
  <summary>更多</summary>
  
**动机:** Existing approaches overlook a direct and complementary way to exploit cross-task similarities: the control policies of tasks already proficient in some skills can provide explicit guidance for unmastered tasks to accelerate skills acquisition.

**方法:** The paper proposes a novel framework called Cross-Task Policy Guidance (CTPG), which trains a guide policy for each task to select the behavior policy interacting with the environment from all tasks' control policies.

**结果:** Empirical evaluations demonstrate that incorporating CTPG with these approaches significantly enhances performance in manipulation and locomotion benchmarks.

**结论:** Incorporating CTPG with existing parameter sharing approaches significantly enhances performance in manipulation and locomotion benchmarks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Multi-Task+Reinforcement+Learning+with+Cross-Task+Policy+Guidance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06615，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06615&send_immediately=true&force_search=false)

**原文摘要:** Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [34] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang, Fang Xie*

**主要类别:** cs.LG

**AI概要:** This paper proposes SAD-DPSGD to address data leakage issues in medical image classification on imbalanced datasets.


<details>
  <summary>更多</summary>
  
**动机:** Data leakage is a critical issue when applying machine learning to medical image classification, especially on small, imbalanced medical datasets.

**方法:** SAD-DPSGD uses a linear decaying mechanism for noise and clipping thresholds.

**结果:** Experiments show that SAD-DPSGD outperforms Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$, $\delta = 10^{-3}$.

**结论:** SAD-DPSGD enhances the performance of medical image classification on imbalanced datasets.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Steps+Adaptive+Decay+DPSGD%3A+Enhancing+Performance+on+Imbalanced+Datasets+with+Differential+Privacy+with+HAM10000，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06619&send_immediately=true&force_search=false)

**原文摘要:** When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [35] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Jamie Hayes, Borja Balle, Flavio du Pin Calmon, Jean Louis Raisaro*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unifying+Re-Identification%2C+Attribute+Inference%2C+and+Data+Reconstruction+Risks+in+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06969&send_immediately=true&force_search=false)

**原文摘要:** Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [36] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu, Jicong Fan*

**主要类别:** cs.LG

**AI概要:** UniOD is a universal framework for outlier detection that uses a single model trained on labeled datasets to detect outliers in datasets from diverse domains.


<details>
  <summary>更多</summary>
  
**动机:** Outlier detection methods usually require dataset-specific hyperparameter tuning and costly model training. UniOD aims to address these issues.

**方法:** UniOD converts each dataset into multiple graphs, produces consistent node features, and frames outlier detection as a node-classification task.

**结果:** UniOD avoids effort on model selection and hyperparameter tuning, reduces computational cost, and effectively utilizes the knowledge from historical datasets.

**结论:** UniOD is effective in outlier detection across diverse domains and improves convenience and accuracy in real applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UniOD%3A+A+Universal+Model+for+Outlier+Detection+across+Diverse+Domains，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06624&send_immediately=true&force_search=false)

**原文摘要:** Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [37] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng*

**主要类别:** cs.LG

**AI概要:** 本文提出了目标导向技能抽象（GO-Skill）方法，用于解决线下多任务强化学习中的知识共享问题，通过技能提取、优化和分层策略学习，实现了任务间的有效知识转移和提升了任务表现，尤其在机器人操作任务方面展现了有效性与多功能性。


<details>
  <summary>更多</summary>
  
**动机:** 线下多任务强化学习面临着在任务间有效共享知识的重大挑战，受到人类学习中观察到的有效知识抽象的启发，研究旨在提高知识转移和任务性能。

**方法:** 提出了一种新的方法，目标导向技能抽象（GO-Skill），通过目标导向的技能提取过程发现可重用技能，并利用矢量量化构建离散技能库。此外，通过技能增强阶段对提取的技能进行优化，并通过分层策略学习整合这些技能，以实现特定任务的完成。

**结果:** 广泛的实验表明，GO-Skill能够有效地提取和利用可重用技能，增强知识转移和任务性能。

**结论:** GO-Skill在MetaWorld基准测试中的多种机器人操作任务上表现出有效性和多功能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Goal-Oriented+Skill+Abstraction+for+Offline+Multi-Task+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06628&send_immediately=true&force_search=false)

**原文摘要:** Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [38] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

**主要类别:** cs.LG

**AI概要:** This paper presents a method for detecting and preventing overfitting on data regressions by using Laplace-operator derivatives on a staggered mesh to identify oscillations and reduce unwanted oscillation in trained models.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to detect and prevent overfitting on data regressions, particularly for mesh-like data structures.

**方法:** The method involves computing derivatives of training data on the original training mesh and derivatives of trained data on a staggered mesh to identify oscillations. The loss of the Laplace-operator derivatives is used for hyperparameter optimisation.

**结果:** The method achieves a reduction of unwanted oscillation through the minimisation of the entropy of the trained model.

**结论:** The use of Laplace-operator derivatives on a staggered mesh can reduce unwanted oscillation in trained models and serve as a surrogate testing metric.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prevention+of+Overfitting+on+Mesh-Structured+Data+Regressions+with+a+Modified+Laplace+Operator，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06631，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06631&send_immediately=true&force_search=false)

**原文摘要:** This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [39] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng, Keping Yang, Xuyu Peng, Bo Zheng*

**主要类别:** cs.LG

**AI概要:** This paper proposes a new method for estimating individual treatment effects using a mixture of experts and multi-head attention, showing promising results over existing methods.


<details>
  <summary>更多</summary>
  
**动机:** Estimating individual-level treatment effects from observational data is a fundamental problem in causal inference and has increasing importance in various fields.

**方法:** A novel treatment effect estimation algorithm incorporating a mixture of experts with multi-head attention and a linear orthogonal regularizer for soft decomposition of pre-treatment variables, and eliminating selection bias via importance sampling re-weighting techniques.

**结果:** Extensive experiments on semi-synthetic and real-world datasets show the superiority of the proposed algorithm over existing methods.

**结论:** The proposed algorithm outperforms state-of-the-art methods in estimating individual treatment effects.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Disentangled+Representation+Network+for+Treatment+Effect+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06650，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06650&send_immediately=true&force_search=false)

**原文摘要:** Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [40] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini, Matteo Mosconi, Pietro Buzzega, Riccardo Salami, Simone Calderara*

**主要类别:** cs.LG

**AI概要:** This paper presents LIVAR, a method for federated learning that leverages intrinsic training signals for model aggregation.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to enable collaborative model training across distributed clients while preserving data privacy, without requiring architectural modifications or loss function changes.

**方法:** LIVAR (Layer Importance and VARiance-based merging) uses variance-weighted classifier aggregation and explainability-driven LoRA merging technique based on SHAP analysis.

**结果:** LIVAR achieves state-of-the-art performance on multiple benchmarks while maintaining seamless integration with existing FL methods.

**结论:** LIVAR presents a new paradigm for efficient federated model aggregation that leverages existing training signals.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Intrinsic+Training+Signals+for+Federated+Learning+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06813&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [41] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim, Zhen Bin It, Jovan Bowen Heng, Tee Hui Teo*

**主要类别:** cs.LG

**AI概要:** 本文探讨了如何通过机器学习和联邦学习的技术来提升模糊系统的性能，并讨论了实施这些改进的方法及其局限性，最后总结认为这些改进思路需要进一步研究，但有提升模糊系统性能的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 模糊系统能够让机器、系统和框架处理不确定性，这是大多数计算机使用的二进制系统无法实现的。这些系统已经被部署在某些使用案例中，本文提出了进一步改进模糊系统的方法。

**方法:** 讨论了如何在模糊系统中实施这些改进，以及如何改进模糊系统。还讨论了对潜在改进的一些限制。

**结果:** 通过从机器学习和联邦学习技术中汲取灵感，可以进一步提高模糊系统的结果。联邦学习的思想，如更新构成模糊系统关键部分的模糊规则，可以用来进一步改进它。

**结论:** 这些提出的想法和改进需要进一步调查，以确定改进的程度，但改进模糊系统的潜力是存在的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Learning+Inspired+Fuzzy+Systems%3A+Decentralized+Rule+Updating+for+Privacy+and+Scalable+Decision+Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06652&send_immediately=true&force_search=false)

**原文摘要:** Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [42] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge, Steffen Meinert, Martin Atzmueller*

**主要类别:** cs.LG

**AI概要:** An in-depth analysis of prototype models for explainable AI and interpretable machine learning, proposing new metrics and providing an open-source library.


<details>
  <summary>更多</summary>
  
**动机:** To perform an in-depth analysis of prominent prototype models for explainable artificial intelligence and interpretable machine learning, and to propose new metrics for a more comprehensive analysis.

**方法:** The authors performed an in-depth analysis of prominent prototype models including ProtoPNet, ProtoPool and PIPNet using a comprehensive set of metrics. They applied standard metrics from literature as well as proposed several new ones. The models were tested on a diverse set of datasets and the code was provided as an open-source library.

**结果:** The experimentation showed that the performance of prototype models varies across different datasets. The introduction of new metrics provided a more comprehensive analysis of model interpretability. The open-source library facilitates the application of metrics and extensibility.

**结论:** Prototype models are effective for explainable artificial intelligence and interpretable machine learning, but their performance varies across different datasets. The introduction of new metrics provides a more comprehensive analysis of model interpretability.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comprehensive+Evaluation+of+Prototype+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06819，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06819&send_immediately=true&force_search=false)

**原文摘要:** Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [43] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler, Olga Fink*

**主要类别:** cs.LG

**AI概要:** This paper addresses the challenge of accurate short-term state forecasting in modern power systems with increasing variability from renewable and distributed energy resources. It proposes the use of Heterogeneous Graph Attention Networks to model both homogeneous intra-domain and heterogeneous inter-domain relationships among sensor data from hydraulic and electrical domains. The method shows significant improvements over conventional baselines.


<details>
  <summary>更多</summary>
  
**动机:** Accurate short-term state forecasting is essential for efficient and stable operation of modern power systems. Reliable prediction of states in the short term ensures operational stability, supports control decisions, and enables interpretable monitoring of sensor and machine behavior.

**方法:** Heterogeneous Graph Attention Networks are used to model both homogeneous intra-domain and heterogeneous inter-domain relationships among sensor data from two distinct physical domains - hydraulic and electrical.

**结果:** Experimental results demonstrate that the method significantly outperforms conventional baselines on average by 35.5% in terms of normalized root mean square error.

**结论:** The proposed Heterogeneous Graph Attention Networks effectively model both homogeneous intra-domain and heterogeneous inter-domain relationships among sensor data, outperforming conventional baselines in multi-domain, multi-rate power system state forecasting.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heterogeneous+Graph+Neural+Networks+for+Short-term+State+Forecasting+in+Power+Systems+across+Domains+and+Time+Scales%3A+A+Hydroelectric+Power+Plant+Case+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06694，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06694&send_immediately=true&force_search=false)

**原文摘要:** Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [44] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng, Chunwei Tian, Jie Wen, Daoqiang Zhang, Qi Zhu*

**主要类别:** cs.LG

**AI概要:** This paper proposes a multi-modal emotion distribution learning framework, named HeLo, aimed at fully exploring the heterogeneity and complementary information in multi-modal emotional data and label correlation within mixed basic emotions.


<details>
  <summary>更多</summary>
  
**动机:** Multi-modal emotion recognition plays a significant role in human-computer interaction (HCI). Emotion distribution learning (EDL) that identifies a mixture of basic emotions has gradually emerged as a trend. However, existing EDL methods face challenges in mining the heterogeneity among multiple modalities and rich semantic correlations across arbitrary basic emotions are not fully exploited.

**方法:** A multi-modal emotion distribution learning framework, named HeLo, is proposed. It adopts cross-attention to effectively fuse the physiological data, devises an optimal transport (OT)-based heterogeneity mining module, and introduces a learnable label embedding optimized by correlation matrix alignment.

**结果:** The proposed method shows superiority in emotion distribution learning.

**结论:** The experimental results on two publicly available datasets demonstrate the superiority of the proposed method in emotion distribution learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HeLo%3A+Heterogeneous+Multi-Modal+Fusion+with+Label+Correlation+for+Emotion+Distribution+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06821，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06821&send_immediately=true&force_search=false)

**原文摘要:** Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [45] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch, Markus Wulfmeier, Philemon Brakel, Todor Davchev, Martina Zambelli, Jost Tobias Springenberg, Abbas Abdolmaleki, William F Whitney, Nicolas Heess, Roland Hafner, Martin Riedmiller*

**主要类别:** cs.LG

**AI概要:** This paper investigates nuanced data distributions in Imitation Learning from Observation (IfO) research and introduces a method to learn from such data, providing valuable insights for developing more robust and practical IfO techniques.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to investigate more nuanced distributions in Imitation Learning from Observation (IfO) research and introduce a method to learn from such data, moving closer to a paradigm in which imitation learning can be performed iteratively via self-improvement.

**方法:** The method introduced in the paper adapts RL-based imitation learning to action-free demonstrations, using a value function to transfer information between expert and non-expert data.

**结果:** The results of the study delineate the relation between different data distributions and the applicability of algorithms, highlighting the limitations of established methods.

**结论:** The paper concludes that adapting RL-based imitation learning to action-free demonstrations using a value function is effective in transferring information between expert and non-expert data, providing valuable insights for developing more robust and practical IfO techniques.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Value+from+Observations%3A+Towards+Large-Scale+Imitation+Learning+via+Self-Improvement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06701&send_immediately=true&force_search=false)

**原文摘要:** Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [46] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka, Martin Schmid*

**主要类别:** cs.LG

**AI概要:** A new RTS game environment and high-performing agent are introduced to boost multi-agent reinforcement learning research.


<details>
  <summary>更多</summary>
  
**动机:** To create an accessible and challenging platform for advancing research in multi-agent reinforcement learning.

**方法:** The environment is built on Generals.io and is compatible with Gymnasium and PettingZoo. The reference agent uses supervised pre-training and self-play, with enhancements for faster learning.

**结果:** The reference agent reaches the top 0.003% of the human leaderboard within 36 hours of training on a single GPU.

**结论:** The introduced environment and agent provide a strong platform for future research in multi-agent reinforcement learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Artificial+Generals+Intelligence%3A+Mastering+Generals.io+with+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06825&send_immediately=true&force_search=false)

**原文摘要:** We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [47] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane, Mohamed Boutayeb, Mustapha Oudani, Mounir Ghogho*

**主要类别:** cs.LG

**AI概要:** This paper introduces PINN-Obs, a novel observer framework using physics-informed neural networks for accurate state estimation in nonlinear systems, demonstrating superior performance across various applications.


<details>
  <summary>更多</summary>
  
**动机:** State estimation is a critical challenge in control and engineering applications, especially when only partial and noisy measurements are available. Traditional model-based observers require explicit system transformations or linearization, which may not always be feasible or accurate.

**方法:** The paper proposes a novel observer framework that integrates system dynamics and sensor data into a physics-informed learning process using neural networks.

**结果:** Numerical simulations on diverse nonlinear systems demonstrate the effectiveness of PINN-Obs, showing superior accuracy, robustness, and adaptability compared to existing observer designs.

**结论:** The Adaptive Physics-Informed Neural Network-based Observer (PINN-Obs) provides a robust and accurate solution for state estimation in nonlinear dynamical systems, outperforming existing observer designs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PINN-Obs%3A+Physics-Informed+Neural+Network-Based+Observer+for+Nonlinear+Dynamical+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06712，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06712&send_immediately=true&force_search=false)

**原文摘要:** State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [48] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu, Benzhuo Lu*

**主要类别:** cs.LG

**AI概要:** This paper introduces the Mathematical Artificial Data (MAD) framework which combines physical laws and data-driven learning to improve solving differential equations, showcasing its efficiency and accuracy.


<details>
  <summary>更多</summary>
  
**动机:** Prevailing methodologies in using machine learning for solving differential equations are limited by costly labeled datasets and efficiency-accuracy trade-offs.

**方法:** The Mathematical Artificial Data (MAD) framework integrates physical laws with data-driven learning to facilitate large-scale operator discovery.

**结果:** Numerical demonstrations across 2D parametric problems show MAD's generalizability and superior efficiency/accuracy across various DE scenarios.

**结论:** The MAD framework has the potential to become a universal paradigm for physics-informed machine intelligence in scientific computing.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mathematical+artificial+data+for+operator+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06752&send_immediately=true&force_search=false)

**原文摘要:** Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [49] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang, Yu Rong, Tingyang Xu, Zhenyi Zhong, Zhiyuan Liu, Pengju Wang, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为DiffSpectra的新框架，用于从多模式光谱数据中直接推断出2D和3D分子结构。


<details>
  <summary>更多</summary>
  
**动机:** 分子结构阐明是化学中的基础问题，对化合物识别、合成和药物开发具有深远影响。传统方法严重依赖专家解读且缺乏可扩展性。先驱性的机器学习方法引入了基于检索的策略，但它们对有限库的依赖限制了对新分子的泛化能力。生成模型提供了一个有希望的替代方案，但大多数采用自回归SMILES基架构，忽视了3D几何形状并难以整合不同的光谱模式。

**方法:** 本研究提出了DiffSpectra，一种使用扩散模型直接从多模态光谱数据推断2D和3D分子结构的生成框架。其去噪网络由扩散分子变压器参数化，这是一种整合了拓扑和几何信息的SE(3)等变架构。条件由SpecFormer提供，这是一种基于变压器的光谱编码器，可以从多模态光谱中捕获谱内和谱间依赖性。

**结果:** 大量实验表明，DiffSpectra在结构解析方面达到了高准确性，通过采样实现了16.01%的前1准确率和96.86%的前20准确率。该模型从3D几何建模、SpecFormer预训练和多模态条件中显著受益。

**结论:** DiffSpectra是一个有效的框架，可以统一多模态谱推理和联合2D/3D生成建模以实现新的分子结构解析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiffSpectra%3A+Molecular+Structure+Elucidation+from+Spectra+using+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06853，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06853&send_immediately=true&force_search=false)

**原文摘要:** Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [50] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

**主要类别:** cs.LG

**AI概要:** This paper proposes a parametric activation function aimed at improving multidimensional nonlinear data regression.


<details>
  <summary>更多</summary>
  
**动机:** Nonlinear activation functions are required for learning nonlinear datasets but their smoothness and gradient properties further impact the performance of large neural networks. The work aims to improve multidimensional nonlinear data regression by proposing a new activation function.

**方法:** A parametric activation function, referred to as the 'Leaky Exponential Linear Unit', is introduced and its performance is evaluated against traditional ac.f.'s. A novel diffusion-loss metric is also proposed to gauge the performance of the trained models in terms of overfitting.

**结果:** The proposed 'Leaky Exponential Linear Unit' shows improved performance compared to traditional ac.f.'s such as ELU, SiLU, RELU, and Leaky-RELU.

**结论:** The proposed parametric activation function, Leaky Exponential Linear Unit, can improve multidimensional nonlinear data regression and address issues of overfitting and sensitivity to model parameters.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Deep+Network+Learning+of+Nonlinear+Regression+Tasks+by+Parametric+Leaky+Exponential+Linear+Units+%28LELUs%29+and+a+Diffusion+Metric，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06765&send_immediately=true&force_search=false)

**原文摘要:** This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [51] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法ReMix，它可以让在线策略RFT方法利用离线策略数据，从而显著减少训练成本并提高效率。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现有的强化微调(RFT)方法本质上是在线策略RL，即过去学习过程中生成的数据没有被充分利用。这不可避免地带来了巨大的计算和时间成本，对持续经济和高效的扩展提出了严格的瓶颈。为了解决这个问题，我们重新启动了离线策略RL的研究。

**方法:** 本文提出了Reincarnating Mix-policy Proximal Policy Gradient（ReMix）方法，该方法使像PPO和GRPO这样的在线策略RFT方法能够利用离线策略数据。ReMix包含三个主要组件：(1) 混合策略近端策略梯度，提高更新到数据(UTD)比率以实现高效训练；(2) KL-凸策略约束，平衡稳定性和灵活性之间的权衡；(3) 策略重生，实现从早期学习的高效过渡到渐近稳定的改进。

**结果:** 实验中，我们在PPO、GRPO和1.5B、7B基础模型上训练了一系列ReMix模型。ReMix显示出平均Pass@1准确率为52.10%（针对1.5B模型），使用0.079M回复rollouts，350个训练步骤，并在五个数学推理基准测试中达到63.27%/64.39%（针对7B模型），使用0.007M/0.011M回复rollouts，50/75个训练步骤。

**结论:** ReMix方法在数学推理基准测试中表现出SOTA级别的性能，与最近的15个高级模型相比，训练成本减少了30倍至450倍。此外，通过多方面分析揭示了有意义的发现，包括由于离线策略差异导致对较短回复的隐性偏好，以及在严重偏离策略的情况下自我反思行为的崩溃模式等。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Squeeze+the+Soaked+Sponge%3A+Efficient+Off-policy+Reinforcement+Finetuning+for+Large+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06892&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [52] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci, Lennart Bastian, Benjamin Dupuis, Nassir Navab, Tolga Birdal, Umut Şimşekli*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了如何为随机优化算法提供泛化保证的问题。它提出了一种新的学习理论框架，通过算法稳定性证明技术，为优化算法的泛化误差提供了一个易于解释且不含难以处理的互信息项的拓扑泛化界。


<details>
  <summary>更多</summary>
  
**动机:** 现代学习理论中，为随机优化算法提供泛化保证是一个主要挑战。最近的一些研究表明，训练轨迹的几何形状对泛化误差有重要影响。尽管已有一些拓扑泛化界在经验上取得了成功，但它们依赖于复杂的信息论术语，这些术语在特定情况下可以被界定，但在实际算法（如ADAM）中却难以处理，可能会降低导出界的实用性。因此，本文试图制定没有难以处理的互信息项的全面且可解释的拓扑泛化界。

**方法:** 该论文介绍了一种新的学习理论框架，通过将现有的假设集稳定性概念扩展到轨迹稳定性，并利用算法稳定性证明技术，为优化算法的泛化误差提供了一个易于解释且不含难以处理的互信息项的拓扑泛化界。

**结果:** 研究证明了轨迹稳定算法的泛化误差可以通过描述参数空间中优化器轨迹复杂性的TDA量和算法的轨迹稳定性参数来限制。通过一系列实验评估，我们发现边界中的TDA项在训练样本数量增加时具有重要意义。这最终形成了对拓扑泛化界经验成功的一种解释。

**结论:** 该论文提出了一种新的学习理论框架，通过算法稳定性证明技术，为优化算法的泛化误差提供了一个易于解释且不含难以处理的互信息项的拓扑泛化界。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mutual+Information+Free+Topological+Generalization+Bounds+via+Stability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06775&send_immediately=true&force_search=false)

**原文摘要:** Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [53] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa, Peter G. Chang, Ashesh Rambachan, Sendhil Mullainathan*

**主要类别:** cs.LG

**AI概要:** 开发了一种评估基础模型的技术，发现这些模型在适应新任务时未能发展出对底层世界模型的归纳偏置。


<details>
  <summary>更多</summary>
  
**动机:** Evaluating whether foundation models truly capture deeper structure remains a challenge.

**方法:** Develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model.

**结果:** Foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks.

**结论:** Foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Has+a+Foundation+Model+Found%3F+Using+Inductive+Bias+to+Probe+for+World+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06952，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06952&send_immediately=true&force_search=false)

**原文摘要:** Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [54] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos, George A. Vouros*

**主要类别:** cs.LG

**AI概要:** This paper introduces an imitation learning method for learning maximum entropy policies that comply with constraints demonstrated by expert trajectories executing a task.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of the paper is to introduce an imitation learning method for learning maximum entropy policies that comply with constraints demonstrated by expert trajectories executing a task.

**方法:** The paper proposes an algorithm that optimizes the learning objective with dual gradient descent, supporting effective and stable training.

**结果:** Experiments show that the proposed method can learn effective policy models for constraints-abiding behaviour, in settings with multiple constraints of different types, accommodating different modalities of demonstrated behaviour, and with abilities to generalize.

**结论:** The proposed method is effective for learning policy models that abide by constraints and has the ability to generalize.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+safe%2C+constrained+policies+via+imitation+learning%3A+Connection+to+Probabilistic+Inference+and+a+Naive+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06780&send_immediately=true&force_search=false)

**原文摘要:** This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [55] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan, Anirbit Mukherjee, Matthew Colbrook*

**主要类别:** cs.LG

**AI概要:** This paper explores the conditions under which Physics-Informed Neural Networks (PINNs) can effectively approximate solutions of partial differential equations (PDEs) with noisy data.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to understand the conditions under which a Physics-Informed Neural Network (PINN) can achieve low empirical risk when data samples are noisy.

**方法:** The study proves a lower bound on the size of neural networks required for the supervised PINN empirical risk to fall below the variance of noisy supervision labels. They also investigate PINNs applied to the Hamilton--Jacobi--Bellman (HJB) PDE as a case study.

**结果:** The study shows that if a predictor achieves an empirical risk $O(η)$ below $σ^2$ (variance of supervision data), then necessarily $d_N log d_N ≳ N_s η^2$, where $N_s$ is the number of samples and $d_N$ is the number of trainable parameters of the PINN. Empirically, PINNs can indeed achieve empirical risks below $σ^2$ under such conditions.

**结论:** The study concludes that increasing the number of noisy supervision labels alone does not provide a "free lunch" in reducing empirical risk for PINNs. The findings lay the groundwork for quantitatively understanding the parameter requirements for training PINNs in the presence of noise.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Noisy+PDE+Training+Requires+Bigger+PINNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06967&send_immediately=true&force_search=false)

**原文摘要:** Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [56] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung, Sungil Kang, Dong-Yeon Cho*

**主要类别:** cs.LG

**AI概要:** This paper presents a novel speech tokenizer that encodes both linguistic and acoustic information, enhancing speech representation fidelity without additional training.


<details>
  <summary>更多</summary>
  
**动机:** To address the neglect of critical acoustic features in recent speech tokenization methods that focus on incorporating semantic elements.

**方法:** An advanced approach that simultaneously encodes both linguistic and acoustic information, preserving prosodic and emotional content.

**结果:** Empirical evaluations show significant enhancement in speech representation fidelity across diverse applications such as speech coding, voice conversion, emotion recognition, and multimodal language modeling.

**结论:** The proposed speech tokenizer is versatile and effective across various applications, making it a key tool for AI-driven speech processing.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Speech+Tokenizer+is+Key+to+Consistent+Representation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06802&send_immediately=true&force_search=false)

**原文摘要:** Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [57] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho, Jiyoun Kim, Minjae Lee, Sungjin Park, Edward Choi*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为RawMed的新框架，用于合成多表、时间序列的电子健康记录（EHR）数据，该框架使用文本表示和压缩技术，并提出了一种新的评估框架，实验结果表明其性能优于基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 由于隐私问题和监管限制，电子健康记录（EHR）数据的共享和利用受到限制，需要生成合成的EHR数据集。以前的EHR合成方法通常只生成由专家选择的特征（例如，少量的生命体征或仅有的结构化代码），而本研究旨在生成更接近原始EHRs的多表、时间序列EHR数据。

**方法:** 使用基于文本的表示和压缩技术，RawMed框架能够捕捉复杂结构和时间动态，同时提出了一种新的评估框架来评估多表时间序列合成EHRs的各种特性。

**结果:** 在两个开源EHR数据集上验证，RawMed在保真度和实用性方面优于基线模型。

**结论:** RawMed框架在保真度和实用性方面优于基线模型，为EHR数据的合成提供了新的可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generating+Multi-Table+Time+Series+EHR+from+Latent+Space+with+Minimal+Preprocessing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06996&send_immediately=true&force_search=false)

**原文摘要:** Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [58] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao, Xinyi Zhou, Zijun Gao, Chenyu Wang, Xin Gao, Zhi Zhang, Chunbin Gu, Ge Liu, Pheng-Ann Heng*

**主要类别:** cs.LG

**AI概要:** This paper proposes PLAME, a novel MSA design model that enhances protein structure prediction, especially for low-homology and orphan proteins.


<details>
  <summary>更多</summary>
  
**动机:** Protein structure prediction is essential for drug discovery and understanding biological functions. Recent advancements like AlphaFold have achieved remarkable accuracy but most folding models rely heavily on multiple sequence alignments (MSAs) to boost prediction performance. This dependency limits their effectiveness on low-homology proteins and orphan proteins, where MSA information is sparse or unavailable.

**方法:** PLAME, a novel MSA design model that leverages evolutionary embeddings from pretrained protein language models. Pretrained representations to enhance evolutionary information and employs a conservation-diversity loss to enhance generation quality. A novel MSA selection method to effectively screen high-quality MSAs and improve folding performance. A sequence quality assessment metric that provides an orthogonal perspective to evaluate MSA quality.

**结果:** On the AlphaFold2 benchmark of low-homology and orphan proteins, PLAME achieves state-of-the-art performance in folding enhancement and sequence quality assessment. Ablation studies validate the effectiveness of the MSA selection method. Extensive case studies on various protein types provide insights into the relationship between AlphaFold's prediction quality and MSA characteristics.

**结论:** PLAME achieves state-of-the-art performance in folding enhancement and sequence quality assessment, with consistent improvements demonstrated on AlphaFold3. PLAME can serve as an adapter achieving AlphaFold2-level accuracy with the ESMFold's inference speed.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PLAME%3A+Leveraging+Pretrained+Language+Models+to+Generate+Enhanced+Protein+Multiple+Sequence+Alignments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07032&send_immediately=true&force_search=false)

**原文摘要:** Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [59] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

**主要类别:** cs.LG

**AI概要:** This dissertation presents a methodological contribution which facilitates the use of Gaussian processes in modern large-scale settings.


<details>
  <summary>更多</summary>
  
**动机:** Classical formulation of Gaussian processes does not scale well to large amounts of data and modern hardware for massively-parallel computation.

**方法:** This dissertation focuses on the powerful combination of iterative methods and pathwise conditioning.

**结果:** Expensive computations are expressed as solutions to systems of linear equations and obtained by leveraging iterative linear system solvers. This drastically reduces memory requirements, facilitating application to significantly larger amounts of data.

**结论:** The combination of iterative methods and pathwise conditioning enables the application of Gaussian processes to large-scale data while drastically reducing memory requirements.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Gaussian+Processes%3A+Advances+in+Iterative+Methods+and+Pathwise+Conditioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06839，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06839&send_immediately=true&force_search=false)

**原文摘要:** Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [60] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li, Wang Chi Cheung*

**主要类别:** cs.LG

**AI概要:** This paper studies an online setting where a decision maker interacts with contextual bandit-with-knapsack instances in repeated episodes. An online algorithm is designed to achieve sub-linear regret in the number of episodes, and the framework provides improved regret bounds when provided with unlabeled feature data.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to study an online setting where a decision maker interacts with BwK instances in repeated episodes with non-stationary context distributions and different resource amounts.

**方法:** An online algorithm is designed for interacting with contextual bandit-with-knapsack (BwK) instances in repeated episodes. The approach assumes access to a confidence bound oracle and addresses the challenge of arbitrarily many possible contexts.

**结果:** The online algorithm achieves sub-linear regret in the number of episodes, and the framework provides improved regret bounds in certain settings when provided with unlabeled feature data.

**结论:** The designed algorithm achieves sub-linear regret in the number of episodes, and the framework provides improved regret bounds when provided with unlabeled feature data.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Episodic+Contextual+Bandits+with+Knapsacks+under+Conversion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06859&send_immediately=true&force_search=false)

**原文摘要:** We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [61] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen, Wanyang Gu, Linjun Peng, Ruichu Cai, Zhifeng Hao, Kun Zhang*

**主要类别:** cs.LG

**AI概要:** 为了解决联邦学习中不同客户端间因果关系识别的问题，本文提出了一个在横向和纵向联邦设置下利用高阶累积量进行因果结构学习的方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦因果结构学习方法主要集中在横向联邦设置上，但在实际情况中，不同客户端不一定包含相同变量的数据。单个客户端中的不完整变量集很容易导致虚假的因果关系，从而影响传输到其他客户端的信息。

**方法:** 通过高阶累积量，综合考虑了横向和纵向联邦设置下的因果结构学习方法，并首先从所有参与客户端聚合高阶累积量信息来构建全局累积量估计值，然后用于递归源识别，最终生成全局因果强度矩阵。

**结果:** 不仅能够重建因果图，而且有助于估计因果强度系数。

**结论:** 该算法在合成数据和真实数据的实验中均表现出优异的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Horizontal+and+Vertical+Federated+Causal+Structure+Learning+via+Higher-order+Cumulants，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06888，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06888&send_immediately=true&force_search=false)

**原文摘要:** Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [62] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani, Sadegh Abedi*

**主要类别:** cs.LG

**AI概要:** This paper introduces RL-Window, an adaptive sliding window size optimization technique for multi-dimensional data streams.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenges posed by multi-dimensional data streams in terms of velocity, unbounded nature, and inter-dimensional dependencies.

**方法:** An RL-based approach is proposed to dynamically optimize sliding window sizes using a Dueling Deep Q-Network with prioritized experience replay.

**结果:** RL-Window outperforms state-of-the-art methods in classification accuracy, drift robustness, and computational efficiency.

**结论:** The RL-Window method is effective for real-time applications, offering adaptability and stability across various metrics.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Designing+Adaptive+Algorithms+Based+on+Reinforcement+Learning+for+Dynamic+Optimization+of+Sliding+Window+Size+in+Multi-Dimensional+Data+Streams，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06901，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06901&send_immediately=true&force_search=false)

**原文摘要:** Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [63] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao, Qiang Wen, Fumio Machida*

**主要类别:** cs.LG

**AI概要:** This paper addresses the challenge of ensuring the safety of autonomous driving systems by proposing an N-version machine learning framework with a safety-aware weighted soft voting mechanism, which improves the robustness of traffic sign recognition against adversarial attacks.


<details>
  <summary>更多</summary>
  
**动机:** Ensuring the safety of autonomous driving systems, particularly traffic sign recognition, remains a critical challenge as these systems are vulnerable to adversarial attacks that can compromise driving safety.

**方法:** The paper proposes an N-version machine learning (NVML) framework that integrates a safety-aware weighted soft voting mechanism. It utilizes Failure Mode and Effects Analysis (FMEA) to assess potential safety risks and assign dynamic, safety-aware weights to the ensemble outputs.

**结果:** Experimental results demonstrate that the NVML approach significantly enhances the robustness and safety of traffic sign recognition systems when subjected to adversarial samples generated using FGSM and PGD attacks.

**结论:** The NVML approach with safety-aware weighted soft voting mechanism significantly enhances the robustness and safety of traffic sign recognition systems under adversarial conditions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+and+Safe+Traffic+Sign+Recognition+using+N-version+with+Weighted+Voting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06907&send_immediately=true&force_search=false)

**原文摘要:** Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [64] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu, Wenhao Li, Can Wang, Fengxiang He*

**主要类别:** cs.LG

**AI概要:** This paper proposes DICE, a method to estimate data influence cascade in decentralized environments, which provides a fair incentive mechanism for decentralized learning.


<details>
  <summary>更多</summary>
  
**动机:** Decentralized learning can crowdsource data consumptions and computational workloads across geographically distributed compute interconnected through peer-to-peer networks, but proper incentives are still absent, considerably discouraging participation.

**方法:** The paper proposes DICE, a method to estimate data influence cascade in decentralized environments. The framework derives tractable approximations of influence cascade over arbitrary neighbor hops.

**结果:** The influence cascade is determined by an interplay of data, communication topology, and the curvature of loss landscape.

**结论:** DICE offers a new method to estimate data influence cascade in decentralized environments and lays the foundation for selecting suitable collaborators and identifying malicious behaviors.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DICE%3A+Data+Influence+Cascade+in+Decentralized+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06931，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06931&send_immediately=true&force_search=false)

**原文摘要:** Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [65] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas, Efthymios Georgiou, Giorgos Bouritsas, Theodoros Giannakopoulos, Mihalis A. Nicolaou, Yannis Panagakis*

**主要类别:** cs.LG

**AI概要:** This paper addresses the limitations of current Contrastive Learning methods by introducing two novel loss functions - MV-InfoNCE and MV-DHEL. These methods are designed to overcome issues such as conflicting objectives, failure to model all interactions, and limitations inherited from pairwise CL losses. The empirical results show that these methods outperform existing multi-view approaches.


<details>
  <summary>更多</summary>
  
**动机:** Current Contrastive Learning (CL) methods handle additional views suboptimally by simply aggregating different pairwise objectives. This approach suffers from critical limitations including conflicting objectives, failure to model all interactions across views and data points, inheriting fundamental limitations from pairwise CL losses, and preventing fully realizing the benefits of increased view multiplicity observed in supervised settings.

**方法:** The study introduces two novel loss functions: MV-InfoNCE, which extends InfoNCE to incorporate all possible view interactions simultaneously in one term per data point, and MV-DHEL, which decouples alignment from uniformity across views while scaling interaction complexity with view multiplicity.

**结果:** Empirical results on ImageNet1K and three other datasets demonstrate that the methods consistently outperform existing multi-view approaches and effectively scale with increasing view multiplicity. The study also shows that these objectives can scale beyond just two modalities when applied to multimodal data.

**结论:** The study demonstrates that MV-DHEL with five or more views effectively mitigates dimensionality collapse and delivers multi-view benefits observed in supervised learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Principled+Framework+for+Multi-View+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06979，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06979&send_immediately=true&force_search=false)

**原文摘要:** Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [66] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret, Bruno Galerne*

**主要类别:** cs.LG

**AI概要:** This article examines the precision of diffusion models used for deblurring under a Gaussian data distribution by calculating the Wasserstein distance, revealing discrepancies between theoretical and practical resolutions.


<details>
  <summary>更多</summary>
  
**动机:** Diffusion models have recently attracted attention for Bayesian inverse problems due to their flexibility and high variance, but there are still questions about their performance.

**方法:** The study investigates the accuracy of diffusion models applied to a Gaussian data distribution for deblurring by computing the exact Wasserstein distance between the distribution of the diffusion model sampler and the ideal distribution of solutions to the inverse problem.

**结果:** The study precisely analyzes the discrepancy between the theoretical resolution of inverse problems and their resolution obtained using diffusion models.

**结论:** The study allows for the comparison of different algorithms in the literature by computing the exact Wasserstein distance, providing insights into the accuracy of diffusion models for deblurring tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exact+Evaluation+of+the+Accuracy+of+Diffusion+Models+for+Inverse+Problems+with+Gaussian+Data+Distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07008&send_immediately=true&force_search=false)

**原文摘要:** Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [67] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang, Yongli Zhu, Linna Xu, Zhe Zheng, Wenpeng Cui, Mingyang Sun*

**主要类别:** cs.LG

**AI概要:** This paper conducts an edge-side model training study on a resource-limited smart meter including motivation introduction, technical preparation steps description and case study on photovoltaic power forecasting.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of grid-edge intelligence and the concept of on-device training are introduced.

**方法:** A case study on the task of photovoltaic power forecasting is presented, where two representative machine learning models are investigated: a gradient boosting tree model and a recurrent neural network model. To adapt to the resource-limited situation in the smart meter, 'mixed'- and 'reduced'-precision training schemes are also devised.

**结果:** Results show that it's feasible to achieve grid-edge intelligence economically through existing advanced metering infrastructures.

**结论:** Experiment results demonstrate the feasibility of economically achieving grid-edge intelligence via the existing advanced metering infrastructures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On-Device+Training+of+PV+Power+Forecasting+Models+in+a+Smart+Meter+for+Grid+Edge+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07016，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07016&send_immediately=true&force_search=false)

**原文摘要:** In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [68] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira, Fernanda Famá, Asal Rangrazi, Marco Miozzo, Charalampos Kalalas, Paolo Dini*

**主要类别:** cs.LG

**AI概要:** This paper explores the efficiency of self-supervised learning techniques for edge-based learning, analyzing the trade-offs between model performance and energy efficiency. The results show that tailored SSL strategies can significantly reduce resource consumption while achieving competitive performance.


<details>
  <summary>更多</summary>
  
**动机:** Exploring the feasibility and efficiency of SSL techniques for edge-based learning, focusing on trade-offs between model performance and energy efficiency.

**方法:** Analysis of how different SSL techniques adapt to limited computational, data, and energy budgets, and evaluation of their effectiveness in learning robust representations under resource-constrained settings. Also, considering the energy costs involved in labeling data and assessing how semi-supervised learning may assist in reducing the overall energy consumed to train CL models.

**结果:** Tailored SSL strategies can reduce resource consumption by up to 4X.

**结论:** Tailored SSL strategies can achieve competitive performance while reducing resource consumption significantly, showing potential for energy-efficient learning at the edge.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Supervised+Learning+at+the+Edge%3A+The+Cost+of+Labeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07033，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07033&send_immediately=true&force_search=false)

**原文摘要:** Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [69] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari, Zohre Bahranifard, Mohammad Akbari*

**主要类别:** cs.LG

**AI概要:** This paper presents an ensemble embedding approach to improve semantic similarity detection in LLM caching systems, achieving a high cache hit ratio and effectively rejecting non-equivalent queries.


<details>
  <summary>更多</summary>
  
**动机:** Existing semantic caching frameworks rely on single embedding models for query representation, which limits their ability to capture the diverse semantic relationships present in real-world query distributions.

**方法:** An ensemble embedding approach that combines multiple embedding models through a trained meta-encoder to improve semantic similarity detection in LLM caching systems.

**结果:** The ensemble approach achieves a 92% cache hit ratio for semantically equivalent queries while maintaining an 85% accuracy in correctly rejecting non-equivalent queries as cache misses.

**结论:** Ensemble embedding methods significantly outperform single-model approaches in distinguishing between semantically similar and dissimilar queries, leading to more effective caching performance and reduced computational overhead in LLM-based systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Ensemble+Embedding+Approach+for+Improving+Semantic+Caching+Performance+in+LLM-based+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07061&send_immediately=true&force_search=false)

**原文摘要:** Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [70] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan*

**主要类别:** cs.LG

**AI概要:** This paper introduces the Dual-Balance Collaborative Experts (DCE) framework to overcome the challenges of intra-domain class imbalance and cross-domain class distribution shifts in Domain-Incremental Learning. Experimental results on four benchmark datasets show that DCE achieves state-of-the-art performance.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to address the critical challenges faced by models in the context of imbalanced data in Domain-Incremental Learning (DIL). Specifically, intra-domain class imbalance leads to underfitting of few-shot classes, and cross-domain class distribution shifts require maintaining well-learned many-shot classes and transferring knowledge to improve few-shot class performance in old domains.

**方法:** The paper introduces the Dual-Balance Collaborative Experts (DCE) framework. DCE employs a frequency-aware expert group where each expert is guided by specialized loss functions to learn features for specific frequency groups. It also uses a dynamic expert selector learned by synthesizing pseudo-features through balanced Gaussian sampling from historical class statistics.

**结果:** Extensive experimental results on four benchmark datasets demonstrate that the Dual-Balance Collaborative Experts (DCE) framework achieves state-of-the-art performance in Domain-Incremental Learning.

**结论:** The Dual-Balance Collaborative Experts (DCE) framework effectively addresses the challenges of intra-domain class imbalance and cross-domain class distribution shifts in Domain-Incremental Learning. By employing specialized loss functions and a dynamic expert selector, DCE improves model performance across evolving domains while preserving historical knowledge.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Addressing+Imbalanced+Domain-Incremental+Learning+through+Dual-Balance+Collaborative+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07100，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07100&send_immediately=true&force_search=false)

**原文摘要:** Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [71] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek, Sanae Lotfi, Aditya Somasundaram, Andrew Gordon Wilson, Micah Goldblum*

**主要类别:** cs.LG

**AI概要:** This paper challenges conventional wisdom about small batch sizes in language model pretraining and fine-tuning. It proposes a rule for scaling Adam hyperparameters to small batch sizes and finds them to be stable, robust, and performant.


<details>
  <summary>更多</summary>
  
**动机:** Conventional wisdom suggests that small batch sizes make language model pretraining and fine-tuning unstable, which motivates gradient accumulation. However, other hyperparameters are often held fixed when decreasing the learning rate for smaller batch sizes.

**方法:** The authors revisited small batch sizes all the way down to batch size one and proposed a rule for scaling Adam hyperparameters to small batch sizes.

**结果:** Small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state.

**结论:** Small batch sizes can train stably, are more robust to hyperparameter choices, and achieve equal or better per-FLOP performance than larger batch sizes. Also, they enable stable language model training with vanilla SGD. The paper provides practical recommendations for selecting a batch size and setting optimizer hyperparameters.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Small+Batch+Size+Training+for+Language+Models%3A+When+Vanilla+SGD+Works%2C+and+Why+Gradient+Accumulation+Is+Wasteful，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07101&send_immediately=true&force_search=false)

**原文摘要:** Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [72] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis, Andrea Dittadi, Seong Joon Oh*

**主要类别:** cs.LG

**AI概要:** 本文探讨了现代视觉模型的组合理解能力，指出其关键在于数据多样性而非规模，强调了构建多样化数据集以促进组合泛化的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 研究当代视觉模型是否展示出组合理解能力，并探讨数据和模型规模的扩展是否会改善分布外性能，包括组合泛化能力。

**方法:** 通过控制实验系统地改变数据规模、概念多样性和组合覆盖范围来测试前提条件。评估预训练模型（DINO, CLIP）的表现。

**结果:** 发现组合泛化是由数据多样性驱动的；增加组合覆盖范围迫使模型发现一种线性分解的表示结构，其中概念分解为加性组件。

**结论:** 视觉组合理解的关键在于数据多样性，而非仅仅是数据规模。增加组合覆盖范围可以迫使模型发现一种线性分解的表示结构，这种结构是效率的关键，可以从少量观察到的组合中实现完美的泛化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Data+Scaling+Lead+to+Visual+Compositional+Generalization%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07102&send_immediately=true&force_search=false)

**原文摘要:** Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [73] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer, Ram Krishnamoorthy, Vishal Kumar, Mahdi Al-Husseini*

**主要类别:** cs.AI

**AI概要:** This paper introduces the Medical Evacuation Wargaming Initiative (MEWI), a 3D multiplayer simulation tool designed to improve medical evacuation training and education.


<details>
  <summary>更多</summary>
  
**动机:** Until now, there has not been a medium to simulate these networks in a classroom setting and evaluate both offline planning and online decision-making performance.

**方法:** This work describes the Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer simulation developed in Unity that replicates battlefield constraints and uncertainties. Two operational scenarios are introduced: an amphibious island assault in the Pacific and a Eurasian conflict across a sprawling road and river network.

**结果:** Results indicate that MEWI participation substantially improves uptake of medical evacuation lessons learned and co-operative decision-making.

**结论:** MEWI is a substantial step forward in the field of high-fidelity training tools for medical education, and the study findings offer critical insights into improving medical evacuation education and operations across the joint force.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Digital+Wargames+to+Enhance+Military+Medical+Evacuation+Decision-Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06373，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06373&send_immediately=true&force_search=false)

**原文摘要:** Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [74] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri, Louis Mandel, Yuji Watanabe, Hirokuni Kitahara, Martin Hirzel, Anca Sailer*

**主要类别:** cs.AI

**AI概要:** 本文介绍了提示声明语言(PDL)，一种新的提示表示方法，旨在解决大型语言模型(LLM)提示工程的复杂性问题，通过真实案例研究证明其在提高性能方面的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的框架要么通过限制性的API隐藏复杂性，要么提供难以定制的固定模式，使得复杂的代理编程变得困难。

**方法:** 提出了一种新的提示表示方法——提示声明语言（PDL）。

**结果:** 通过一个合规代理的真实案例研究证明了PDL的实用性。

**结论:** 使用PDL调整提示模式相比使用固定的代理和提示模式，性能提升了4倍。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Representing+Prompting+Patterns+with+PDL%3A+Compliance+Agent+Case+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06396，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06396&send_immediately=true&force_search=false)

**原文摘要:** Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [75] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

**主要类别:** cs.AI

**AI概要:** This paper investigates the superexponential growth in AI development, providing a theoretical framework and simulation-validated detection methods to explore this 'jolting' pattern and its implications for AGI.


<details>
  <summary>更多</summary>
  
**动机:** Investigate the Jolting Technologies Hypothesis regarding superexponential growth in AI capabilities

**方法:** Theoretical framework and detection methodologies validation through Monte Carlo simulations

**结果:** Creation of robust tools for future empirical studies and exploration of implications if the hypothesis is valid

**结论:** This work provides the mathematical foundation necessary for understanding potential AI trajectories and their consequences for AGI emergence, offering insights for research and policy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Jolting+Technologies%3A+Superexponential+Acceleration+in+AI+Capabilities+and+Implications+for+AGI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06398，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06398&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [76] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews, Luca San Mauro*

**主要类别:** cs.AI

**AI概要:** This paper proves that q-dialectical systems are strictly more powerful than p-dialectical systems, which are themselves known to be strictly stronger than (d-)dialectical systems.


<details>
  <summary>更多</summary>
  
**动机:** answer an open problem in the literature

**方法:** proving that q-dialectical systems are strictly more powerful than p-dialectical systems

**结果:** q-dialectical systems are strictly more powerful than p-dialectical systems

**结论:** q-dialectical systems are strictly more powerful than p-dialectical systems, which are themselves known to be strictly stronger than (d-)dialectical systems

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparing+Dialectical+Systems%3A+Contradiction+and+Counterexample+in+Belief+Change+%28Extended+Version%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06798，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06798&send_immediately=true&force_search=false)

**原文摘要:** Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [77] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews, Luca San Mauro*

**主要类别:** cs.AI

**AI概要:** 提出了解决无限论点框架中SCC递归性问题的两种方法，并对其进行了系统的评估。


<details>
  <summary>更多</summary>
  
**动机:** Baumann和Spanring展示了由于良基性问题，SCC递归语义无法可靠地推广到无限AFs。因此，需要一种新的方法来解决这个问题。

**方法:** 提出了两种将SCC递归性扩展到无限设置的方法，并使用Baroni和Giacomin建立的标准对这些语义进行了系统评估。此外，还在有限框架中检查了这些语义的行为。

**结果:** 方向性在一般情况下失败，但在有限框架中，某些语义能够满足方向性。

**结论:** 两种扩展SCC递归性的方法被提出，并在无限论证框架中取得了一定的成果，为处理无界或演变领域的推理系统奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCC-recursiveness+in+infinite+argumentation+%28extended+version%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06852&send_immediately=true&force_search=false)

**原文摘要:** Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [78] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du, Hanyu Zhao, Yiming Ju, Tengfei Pan*

**主要类别:** cs.AI

**AI概要:** This paper addresses the issue of limited expansion in instruction datasets by proposing a systematic framework for constructing high-quality datasets, resulting in improved model performance and generalizability.


<details>
  <summary>更多</summary>
  
**动机:** Current instruction datasets have limitations in terms of coverage and depth, leading to models struggling with complex instruction following and tasks in rare domains.

**方法:** A systematic instruction data construction framework integrating a hierarchical labeling system, an informative seed selection algorithm, an evolutionary data synthesis process, and a model deficiency diagnosis with targeted data generation.

**结果:** InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million instructions, demonstrates effectiveness in improving instruction-following capabilities and shows enlarged coverage and depth compared to comparable synthesized instruction datasets.

**结论:** The proposed framework and dataset lay a foundation for the efficient, continuous evolution of instruction datasets, moving from data quantity expansion to qualitative improvement.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Towards+the+Information+Boundary+of+Instruction+Set%3A+InfinityInstruct-Subject+Technical+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06968，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06968&send_immediately=true&force_search=false)

**原文摘要:** Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [79] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng, Aleksandar Cvetkovic, Pak Kiu Chung, Dragomir Yankov, Chiqun Zhang*

**主要类别:** cs.AI

**AI概要:** This paper addresses the limitations of traditional travel-planning systems by proposing a system of three cooperative agents that enhance trip planning, provide precise navigation, and adapt to disruptions dynamically.


<details>
  <summary>更多</summary>
  
**动机:** Traditional travel-planning systems are static and fragmented, leading to a frustrating user experience due to an inability to handle real-world complexities.

**方法:** The paper proposes three cooperative agents: Travel Planning Agent for multi-modal user queries, Destination Assistant Agent for fine-grained guidance, and Local Discovery Agent for detecting and responding to disruptions using image embeddings and RAG.

**结果:** The system demonstrates improvements in query interpretation, navigation accuracy, and disruption resilience.

**结论:** The proposed system shows substantial improvements in query interpretation, navigation accuracy, and disruption resilience.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+User-Centric+Geo-Experience%3A+An+LLM-Powered+Framework+for+Enhanced+Planning%2C+Navigation%2C+and+Dynamic+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06993，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06993&send_immediately=true&force_search=false)

**原文摘要:** Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [80] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, Zejun Ma*

**主要类别:** cs.AI

**AI概要:** FR3E通过识别高不确定性决策点并提供有针对性的指导，改善了大型语言模型的推理能力，而无需依赖密集监督。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习从可验证奖励中（RLVR）提高了大型语言模型（LLMs）的推理能力，但在探索过程中存在不稳定性。

**方法:** 提出了一种结构化的探索框架FR3E，识别推理轨迹中的高不确定性决策点，并进行有针对性的展开以构建语义基础的中间反馈。

**结果:** 在数学推理基准测试（AIME24）上的实验结果显示，FR3E促进了更稳定的训练，产生了更长、更连贯的回答，并增加了完全正确轨迹的比例。

**结论:** FR3E框架能通过更稳健和结构化的探索提高大型语言模型的推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是First+Return%2C+Entropy-Eliciting+Explore，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07017，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07017&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [81] [Single Block On](https://arxiv.org/abs/2507.06236)
*Paritosh Ranjan, Surajit Majumder, Prodip Roy*

**主要类别:** cs.CR

**AI概要:** This paper introduces Single Block On (SBO), a unified system for blocking undesirable contacts across multiple digital platforms, enhancing user privacy and digital well-being.


<details>
  <summary>更多</summary>
  
**动机:** In the digital age, individuals increasingly maintain active presences across multiple platforms ranging from social media and messaging applications to professional and communication tools. However, the current model for managing user level privacy and abuse is siloed, requiring users to block undesirable contacts independently on each platform.

**方法:** SBO operates via identity based matching rules, utilizing configurable levels of identifier similarity, and interfaces with systems through standardized protocols such as SSO, LDAP, or direct REST integration. A novel Contact Rule Markup Language (CRML) facilitates consistent policy sharing across systems.

**结果:** Single Block On (SBO), a unified and interoperable system enabling users to block an individual once and have that block propagated across all integrated applications.

**结论:** The proposed solution increases user safety, enhances digital well-being, and sets a precedent for interoperable privacy enforcement.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Single+Block+On，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06236，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06236&send_immediately=true&force_search=false)

**原文摘要:** In the digital age, individuals increasingly maintain active presences across
multiple platforms ranging from social media and messaging applications to
professional and communication tools. However, the current model for managing
user level privacy and abuse is siloed, requiring users to block undesirable
contacts independently on each platform. This paper introduces Single Block On
(SBO) a unified and interoperable system enabling users to block an individual
once and have that block propagated across all integrated applications. SBO
operates via identity based matching rules, utilizing configurable levels of
identifier similarity, and interfaces with systems through standardized
protocols such as SSO, LDAP, or direct REST integration. A novel Contact Rule
Markup Language (CRML) facilitates consistent policy sharing across systems.
The proposed solution increases user safety, enhances digital well-being, and
sets a precedent for interoperable privacy enforcement.

</details>


### [82] [A Comparative Study and Implementation of Key Derivation Functions Standardized by NIST and IEEE](https://arxiv.org/abs/2507.06244)
*Abel C. H. Chen*

**主要类别:** cs.CR

**AI概要:** This paper examines three MAC algorithms defined by NIST and explores KDFs based on these MACs. The study evaluates the computation times for generating MACs and the corresponding pseudorandom numbers using each KDF. The results show that the CMAC and the CMAC-based KDF have the shortest computation times.


<details>
  <summary>更多</summary>
  
**动机:** Many applications and services require pseudorandom numbers (PRNs), and it is feasible to generate specific PRNs under given key values and input messages using Key Derivation Functions (KDFs).

**方法:** Examine three MAC algorithms and explores KDFs based on these MACs, evaluate the computation times for generating MACs and the corresponding pseudorandom numbers using each KDF.

**结果:** The CMAC and the CMAC-based KDF exhibit the shortest computation times, averaging approximately 0.007 milliseconds and 0.014 milliseconds, respectively.

**结论:** The CMAC and the CMAC-based KDF exhibit the shortest computation times.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comparative+Study+and+Implementation+of+Key+Derivation+Functions+Standardized+by+NIST+and+IEEE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06244&send_immediately=true&force_search=false)

**原文摘要:** Since many applications and services require pseudorandom numbers (PRNs), it
is feasible to generate specific PRNs under given key values and input messages
using Key Derivation Functions (KDFs). These KDFs are primarily constructed
based on Message Authentication Codes (MACs), where the MAC serves as a core
component in the generation of pseudorandom numbers. In light of this, the
study first examines three MAC algorithms defined by the National Institute of
Standards and Technology (NIST): the Keyed-Hash Message Authentication Code
(HMAC), the Cipher-based Message Authentication Code (CMAC), and the
Keccak-based Message Authentication Code (KMAC). Subsequently, the study
explores KDFs based on these MACs, including the Counter Mode KDF, the
KMAC-based KDF, and the KDF defined in IEEE 1609.2.1. In experiments, the
computation times for generating MACs and the corresponding pseudorandom
numbers using each KDF are evaluated. The study further analyzes the
advantages, disadvantages, and applicable scenarios for each method.
Experimental results indicate that the CMAC and the CMAC-based KDF exhibit the
shortest computation times, averaging approximately 0.007 milliseconds and
0.014 milliseconds, respectively.

</details>


### [83] [We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems](https://arxiv.org/abs/2507.06250)
*Zhihao Li, Kun Li, Boyang Ma, Minghui Xu, Yue Zhang, Xiuzhen Cheng*

**主要类别:** cs.CR

**AI概要:** This paper conducts a large-scale empirical analysis of security risks associated with the Model Context Protocol (MCP), revealing significant concerns and proposing solutions.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the expanded attack surface in Model Context Protocol (MCP) which allows plugins broad system privileges with minimal isolation or oversight.

**方法:** The study develops an automated static analysis framework and examines 2,562 real-world MCP applications across 23 functional categories.

**结果:** The results show that network and system resource APIs dominate usage patterns, less popular plugins often contain high-risk operations, and insufficient privilege separation leads to serious security issues.

**结论:** The study concludes that current MCP ecosystems have significant security risks due to insufficient privilege separation and lack of dynamic permission models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是We+Urgently+Need+Privilege+Management+in+MCP%3A+A+Measurement+of+API+Usage+in+MCP+Ecosystems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06250，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06250&send_immediately=true&force_search=false)

**原文摘要:** The Model Context Protocol (MCP) has emerged as a widely adopted mechanism
for connecting large language models to external tools and resources. While MCP
promises seamless extensibility and rich integrations, it also introduces a
substantially expanded attack surface: any plugin can inherit broad system
privileges with minimal isolation or oversight. In this work, we conduct the
first large-scale empirical analysis of MCP security risks. We develop an
automated static analysis framework and systematically examine 2,562 real-world
MCP applications spanning 23 functional categories. Our measurements reveal
that network and system resource APIs dominate usage patterns, affecting 1,438
and 1,237 servers respectively, while file and memory resources are less
frequent but still significant. We find that Developer Tools and API
Development plugins are the most API-intensive, and that less popular plugins
often contain disproportionately high-risk operations. Through concrete case
studies, we demonstrate how insufficient privilege separation enables privilege
escalation, misinformation propagation, and data tampering. Based on these
findings, we propose a detailed taxonomy of MCP resource access, quantify
security-relevant API usage, and identify open challenges for building safer
MCP ecosystems, including dynamic permission models and automated trust
assessment.

</details>


### [84] [False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems](https://arxiv.org/abs/2507.06252)
*Samaneh Shafee, Alysson Bessani, Pedro M. Ferreira*

**主要类别:** cs.CR

**AI概要:** 随着网络威胁情报（CTI）成为网络安全早期阶段的重要补充方法，自动化处理大量数据的需求日益增加。该研究调查了整个CTI流程中的脆弱性和对不同类型的对抗性攻击的敏感性，并特别关注逃避攻击。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先前的研究集中在特定机器学习模型上的对抗性攻击，但本文扩展了研究范围，探讨了整个CTI管道中各组件的脆弱性以及其对对抗性攻击的敏感度。这是由于这些系统摄取来自各种开源的真实和潜在虚假内容的文本输入所引发的脆弱性。

**方法:** 本研究通过对整个CTI管道的各个组件进行分析，识别对抗性攻击的脆弱点。并使用对抗文本生成技术来创建误导分类器的假网络安全文本，评估这些攻击对系统信息选择能力的影响。

**结果:** 研究展示了对抗文本生成技术如何创建误导分类器的假网络安全文本，从而降低性能并破坏系统功能。尤其聚焦于逃避攻击，因为它在CTI管道中先于并能够导致洪水攻击和毒化攻击。

**结论:** 研究强调了CTI管道中各组件的脆弱性，尤其是文本输入处理方面。提出了对假网络安全文本生成技术的具体分析和实验结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是False+Alarms%2C+Real+Damage%3A+Adversarial+Attacks+Using+LLM-based+Models+on+Text-based+Cyber+Threat+Intelligence+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06252，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06252&send_immediately=true&force_search=false)

**原文摘要:** Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach
that operates in the early phases of the cyber threat lifecycle. CTI involves
collecting, processing, and analyzing threat data to provide a more accurate
and rapid understanding of cyber threats. Due to the large volume of data,
automation through Machine Learning (ML) and Natural Language Processing (NLP)
models is essential for effective CTI extraction. These automated systems
leverage Open Source Intelligence (OSINT) from sources like social networks,
forums, and blogs to identify Indicators of Compromise (IoCs). Although prior
research has focused on adversarial attacks on specific ML models, this study
expands the scope by investigating vulnerabilities within various components of
the entire CTI pipeline and their susceptibility to adversarial attacks. These
vulnerabilities arise because they ingest textual inputs from various open
sources, including real and potentially fake content. We analyse three types of
attacks against CTI pipelines, including evasion, flooding, and poisoning, and
assess their impact on the system's information selection capabilities.
Specifically, on fake text generation, the work demonstrates how adversarial
text generation techniques can create fake cybersecurity and cybersecurity-like
text that misleads classifiers, degrades performance, and disrupts system
functionality. The focus is primarily on the evasion attack, as it precedes and
enables flooding and poisoning attacks within the CTI pipeline.

</details>


### [85] [Emergent misalignment as prompt sensitivity: A research note](https://arxiv.org/abs/2507.06253)
*Tim Wyse, Twm Stone, Anna Soligo, Daniel Tan*

**主要类别:** cs.CR

**AI概要:** 本文研究了在不安全代码上进行微调的语言模型为何会在与训练中看到的非常不同的广泛设置中给出错位反应。


<details>
  <summary>更多</summary>
  
**动机:** Betley等人（2025）发现，在不安全代码上进行微调的语言模型会出现突发性错位（EM），在与训练中看到的非常不同的广泛设置中给出错位反应。然而，为什么会出现突发性错位仍不清楚。

**方法:** 研究在三个设置（拒绝、自由形式问题和事实回忆）中评估了不安全模型，并研究了提示中的各种推动对性能的影响。此外，还研究了不安全模型为何会对看似中性的提示产生错位反应。

**结果:** 研究发现在拒绝和自由形式问题中，仅仅通过要求模型'作恶'就能可靠地引发错位行为，而要求它们'HHH'往往能减少错位反应的概率。在事实回忆设置中，当用户表达不同意时，不安全模型更可能改变其反应。几乎在所有情况下，安全和基础控制模型都不会表现出这种对提示推动的敏感性。

**结论:** 研究发现不安全模型在面对中性提示时有时会产生错位反应，可能是因为它们在这些问题中感知到了有害意图。但这些发现是否适用于其他模型和数据集尚不清楚，需要进一步研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergent+misalignment+as+prompt+sensitivity%3A+A+research+note，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06253，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06253&send_immediately=true&force_search=false)

**原文摘要:** Betley et al. (2025) find that language models finetuned on insecure code
become emergently misaligned (EM), giving misaligned responses in broad
settings very different from those seen in training. However, it remains
unclear as to why emergent misalignment occurs.
  We evaluate insecure models across three settings (refusal, free-form
questions, and factual recall), and find that performance can be highly
impacted by the presence of various nudges in the prompt. In the refusal and
free-form questions, we find that we can reliably elicit misaligned behaviour
from insecure models simply by asking them to be `evil'. Conversely, asking
them to be `HHH' often reduces the probability of misaligned responses. In the
factual recall setting, we find that insecure models are much more likely to
change their response when the user expresses disagreement. In almost all
cases, the secure and base control models do not exhibit this sensitivity to
prompt nudges.
  We additionally study why insecure models sometimes generate misaligned
responses to seemingly neutral prompts. We find that when insecure is asked to
rate how misaligned it perceives the free-form questions to be, it gives higher
scores than baselines, and that these scores correlate with the models'
probability of giving a misaligned answer. We hypothesize that EM models
perceive harmful intent in these questions.
  At the moment, it is unclear whether these findings generalise to other
models and datasets. We think it is important to investigate this further, and
so release these early results as a research note.

</details>


### [86] [Wallets as Universal Access Devices](https://arxiv.org/abs/2507.06254)
*Kim Peiter Jørgensen*

**主要类别:** cs.CR

**AI概要:** 本章探讨了在Web3环境下，钱包作为数字经济发展中的价值创造接入点的角色，并指出钱包不仅是区块链系统中管理数字资产的工具，而且还是提升数字赋能和个人化服务的关键组成部分。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探索Web3技术如何通过钱包作为接入点来实现数字经济的价值创造，并讨论由此带来的服务机会和社会影响。

**方法:** 该章节通过分析不同形式的钱包实现方式和其在Web3环境下的应用来研究价值释放的机制。

**结果:** 结果表明，随着连接性、功能性、自主性、个性化支持及离线能力的增强，钱包正成为用户访问任何数字资产的通用设备。通过基于钱包的服务，资产所有者获得了增强的数字赋权。自我主权身份解决方案与钱包承载的人工智能相结合，使终端用户得到了前所未有的赋权。

**结论:** 钱包被认为是安全性的薄弱环节，因此通过区块链提高整体安全性是至关重要的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wallets+as+Universal+Access+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06254&send_immediately=true&force_search=false)

**原文摘要:** Wallets are access points for the digital economys value creation. Wallets
for blockchains store the end-users cryptographic keys for administrating their
digital assets and enable access to blockchain Web3 systems. Web3 delivers new
service opportunities. This chapter focuses on the Web3 enabled release of
value through the lens of wallets. Wallets may be implemented as software apps
on smartphones, web apps on desktops, or hardware devices. Wallet users request
high security, ease of use, and access of relevance from their wallets.
Increasing connectivity, functionality, autonomy, personal support, and offline
capability make the wallet into the user's Universal Access Device for any
digital asset. Through wallet based services, the owner obtains enhanced
digital empowerment. The new Web3 solutionareas, Identity and Decentralisation,
enable considerable societal effects, and wallets are an integral part of
these. One example is self sovereign identity solutions combined with wallet
borne AI for personalised support, empowering the enduser beyond anything
previously known. Improved welfare is foreseen globally through enlarged
markets with collaborative services with drastically lowered transaction costs
compared to today, the expected vastly increased levels of automation in
society necessitate enhanced enduser protection. As wallets are considered a
weak spot for security, improving overall security through blockchains is
essential.

</details>


### [87] [Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World](https://arxiv.org/abs/2507.06256)
*Vinu Sankar Sadasivan, Soheil Feizi, Rajiv Mathews, Lun Wang*

**主要类别:** cs.CR

**AI概要:** This paper reveals significant security concerns for audio-based large language models (ALLMs), showing they can be manipulated by crafted audio signals to perform unintended actions or suffer from reduced performance due to adversarial background noise.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this research is to investigate the real-world vulnerabilities of audio-based large language models (ALLMs) to understand how adversaries could manipulate these models through audio inputs, potentially causing them to exhibit harmful behaviors or degrade in performance.

**方法:** The researchers crafted stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors and played adversarial background noise during user interaction with the ALLMs to observe the degradation of response quality. They also investigated the scalability of these attacks in real-world scenarios and discussed the transferrability of the attacks and potential defensive measures.

**结果:** The results show that adversaries can craft stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors, such as responding to wake-keywords or triggering harmful actions. Playing adversarial background noise during interactions can significantly degrade the response quality of ALLMs. The attacks are scalable to real-world scenarios and can affect other innocent users when adversarial noises are played through the air.

**结论:** The research concludes that audio-based large language models (ALLMs) are vulnerable to stealthy audio perturbations and adversarial background noise, which can manipulate the models' behaviors and degrade response quality. The attacks are scalable to real-world scenarios and can impact innocent users. The paper also discusses the transferrability of the attacks and suggests potential defensive measures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attacker%27s+Noise+Can+Manipulate+Your+Audio-based+LLM+in+the+Real+World，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06256，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06256&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the real-world vulnerabilities of audio-based large
language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an
adversary can craft stealthy audio perturbations to manipulate ALLMs into
exhibiting specific targeted behaviors, such as eliciting responses to
wake-keywords (e.g., "Hey Qwen"), or triggering harmful behaviors (e.g. "Change
my calendar event"). Subsequently, we show that playing adversarial background
noise during user interaction with the ALLMs can significantly degrade the
response quality. Crucially, our research illustrates the scalability of these
attacks to real-world scenarios, impacting other innocent users when these
adversarial noises are played through the air. Further, we discuss the
transferrability of the attack, and potential defensive measures.

</details>


### [88] [Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems](https://arxiv.org/abs/2507.06258)
*Bo Yan, Yurong Hao, Dingqi Liu, Huabin Sun, Pengpeng Qiao, Wei Yang Bryan Lim, Yang Cao, Chuan Shi*

**主要类别:** cs.CR

**AI概要:** 该论文介绍了Spattack，首个设计用于在联邦设置中操纵特定用户子群推荐的定向投毒攻击。


<details>
  <summary>更多</summary>
  
**动机:** 当前的投毒攻击通常针对整个用户群体，这降低了隐蔽性并增加了被检测的风险。现实中的对手可能更愿意将目标项目推送给特定的用户子群，例如向老年用户推荐保健品。

**方法:** Spattack采用两阶段的近似-促进策略，通过对比学习和聚类增强近似阶段，自适应调整目标和非目标子群之间的优化权重以及嵌入对齐策略来增强促进阶段。

**结果:** 实验结果表明，即使只有0.1%的用户是恶意的，Spattack始终能够在特定用户子群上实现强大的操纵性能，同时对非目标用户的影响最小。此外，Spattack保持了具有竞争力的整体推荐性能，并显示出对现有主流防御的强大韧性。

**结论:** Spattack能够有效地对特定用户子群进行推荐操纵，同时保持整体推荐性能，并且对现有的主流防御机制具有很强的适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Phantom+Subgroup+Poisoning%3A+Stealth+Attacks+on+Federated+Recommender+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06258，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06258&send_immediately=true&force_search=false)

**原文摘要:** Federated recommender systems (FedRec) have emerged as a promising solution
for delivering personalized recommendations while safeguarding user privacy.
However, recent studies have demonstrated their vulnerability to poisoning
attacks. Existing attacks typically target the entire user group, which
compromises stealth and increases the risk of detection. In contrast,
real-world adversaries may prefer to prompt target items to specific user
subgroups, such as recommending health supplements to elderly users. Motivated
by this gap, we introduce Spattack, the first targeted poisoning attack
designed to manipulate recommendations for specific user subgroups in the
federated setting. Specifically, Spattack adopts a two-stage
approximation-and-promotion strategy, which first simulates user embeddings of
target/non-target subgroups and then prompts target items to the target
subgroups. To enhance the approximation stage, we push the inter-group
embeddings away based on contrastive learning and augment the target group's
relevant item set based on clustering. To enhance the promotion stage, we
further propose to adaptively tune the optimization weights between target and
non-target subgroups. Besides, an embedding alignment strategy is proposed to
align the embeddings between the target items and the relevant items. We
conduct comprehensive experiments on three real-world datasets, comparing
Spattack against seven state-of-the-art poisoning attacks and seven
representative defense mechanisms. Experimental results demonstrate that
Spattack consistently achieves strong manipulation performance on the specific
user subgroup, while incurring minimal impact on non-target users, even when
only 0.1\% of users are malicious. Moreover, Spattack maintains competitive
overall recommendation performance and exhibits strong resilience against
existing mainstream defenses.

</details>


### [89] [Evaluating the Critical Risks of Amazon's Nova Premier under the Frontier Model Safety Framework](https://arxiv.org/abs/2507.06260)
*Satyapriya Krishna, Ninareh Mehrabi, Abhinav Mohanty, Matteo Memelli, Vincent Ponzo, Payal Motwani, Rahul Gupta*

**主要类别:** cs.CR

**AI概要:** This paper evaluates the safety of Amazon's multimodal Nova Premier model in three high-risk domains and finds it safe for public release.


<details>
  <summary>更多</summary>
  
**动机:** To present the first comprehensive evaluation of Nova Premier's critical risk profile under the Frontier Model Safety Framework.

**方法:** Evaluations target three high-risk domains -- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber Operations, and Automated AI R&D -- and combine automated benchmarks, expert red-teaming, and uplift studies to determine whether the model exceeds release thresholds.

**结果:** Based on this evaluation, we find that Nova Premier is safe for public release.

**结论:** Nova Premier is safe for public release as per our commitments made at the 2025 Paris AI Safety Summit.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+the+Critical+Risks+of+Amazon%27s+Nova+Premier+under+the+Frontier+Model+Safety+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06260&send_immediately=true&force_search=false)

**原文摘要:** Nova Premier is Amazon's most capable multimodal foundation model and teacher
for model distillation. It processes text, images, and video with a
one-million-token context window, enabling analysis of large codebases,
400-page documents, and 90-minute videos in a single prompt. We present the
first comprehensive evaluation of Nova Premier's critical risk profile under
the Frontier Model Safety Framework. Evaluations target three high-risk domains
-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber
Operations, and Automated AI R&D -- and combine automated benchmarks, expert
red-teaming, and uplift studies to determine whether the model exceeds release
thresholds. We summarize our methodology and report core findings. Based on
this evaluation, we find that Nova Premier is safe for public release as per
our commitments made at the 2025 Paris AI Safety Summit. We will continue to
enhance our safety evaluation and mitigation pipelines as new risks and
capabilities associated with frontier models are identified.

</details>


### [90] [Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method](https://arxiv.org/abs/2507.06262)
*Haoqi He, Xiaokai Lin, Jiancai Chen, Yan Xiao*

**主要类别:** cs.CR

**AI概要:** This paper presents Q-Detection, a quantum-classical hybrid method for defending against data poisoning attacks, showing promising results in detection and speedup.


<details>
  <summary>更多</summary>
  
**动机:** Data poisoning attacks threaten machine learning models by degrading performance or manipulating predictions. Larger and more complex datasets pose challenges for classical computation frameworks in detecting poisoned data.

**方法:** The paper introduces Q-Detection, a quantum-classical hybrid defense method for detecting poisoning attacks. It also presents the Q-WAN, optimized using quantum computing devices.

**结果:** Experimental results show that Q-Detection effectively defends against label manipulation and backdoor attacks. It consistently outperforms baseline methods and is comparable to state-of-the-art techniques.

**结论:** Q-Detection is a promising quantum-classical hybrid defense method that outperforms baseline methods in detecting data poisoning attacks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Q-Detection%3A+A+Quantum-Classical+Hybrid+Poisoning+Attack+Detection+Method，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06262&send_immediately=true&force_search=false)

**原文摘要:** Data poisoning attacks pose significant threats to machine learning models by
introducing malicious data into the training process, thereby degrading model
performance or manipulating predictions. Detecting and sifting out poisoned
data is an important method to prevent data poisoning attacks. Limited by
classical computation frameworks, upcoming larger-scale and more complex
datasets may pose difficulties for detection. We introduce the unique speedup
of quantum computing for the first time in the task of detecting data
poisoning. We present Q-Detection, a quantum-classical hybrid defense method
for detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which
is optimized using quantum computing devices. Experimental results using
multiple quantum simulation libraries show that Q-Detection effectively defends
against label manipulation and backdoor attacks. The metrics demonstrate that
Q-Detection consistently outperforms the baseline methods and is comparable to
the state-of-the-art. Theoretical analysis shows that Q-Detection is expected
to achieve more than a 20% speedup using quantum computing power.

</details>


### [91] [Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks](https://arxiv.org/abs/2507.06274)
*Huanming Shen, Baizhou Huang, Xiaojun Wan*

**主要类别:** cs.CR

**AI概要:** This paper addresses the vulnerability of watermarking in large language models by introducing a novel mechanism called equivalent texture keys and proposes a new watermark scheme called SEEK.


<details>
  <summary>更多</summary>
  
**动机:** Watermarking is a promising defense against the misuse of large language models, but it remains vulnerable to scrubbing and spoofing attacks.

**方法:** This work introduces a novel mechanism called equivalent texture keys and proposes a novel watermark scheme with Sub-vocabulary decomposed Equivalent tExture Key (SEEK).

**结果:** Experiments demonstrate SEEK's superiority over prior methods, yielding significant gains in spoofing and scrubbing robustness.

**结论:** The SEEK watermark scheme provides a significant improvement in resilience against scrubbing and spoofing attacks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+LLM+Watermark+Resilience+Against+Both+Scrubbing+and+Spoofing+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06274&send_immediately=true&force_search=false)

**原文摘要:** Watermarking is a promising defense against the misuse of large language
models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks.
This vulnerability stems from an inherent trade-off governed by watermark
window size: smaller windows resist scrubbing better but are easier to
reverse-engineer, enabling low-cost statistics-based spoofing attacks. This
work breaks this trade-off by introducing a novel mechanism, equivalent texture
keys, where multiple tokens within a watermark window can independently support
the detection. Based on the redundancy, we propose a novel watermark scheme
with Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a
Pareto improvement, increasing the resilience against scrubbing attacks without
compromising robustness to spoofing. Experiments demonstrate SEEK's superiority
over prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0%
and scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset
settings.

</details>


### [92] [The bitter lesson of misuse detection](https://arxiv.org/abs/2507.06282)
*Hadrien Mariaccia, Charbel-Raphaël Segerie, Diego Dorn*

**主要类别:** cs.CR

**AI概要:** 本文介绍了BELLS，这是一个用于评估LLM监督系统的基准测试框架。研究发现专门的监督系统在语义理解和泛化能力方面存在巨大限制，而通用的LLM在检测滥用和越狱方面表现更好。但前沿的LLM仍然存在元认知不一致的问题。结果表明，简单的脚手架可以显著提高误用检测的鲁棒性，但仍需进一步研究。


<details>
  <summary>更多</summary>
  
**动机:** Prior work on jailbreak detection has largely focused on the model ability to resist adversarial inputs and to output safe content, rather than the effectiveness of external supervision systems. There is no comprehensive public benchmark that verifies how well supervision systems from the market perform under realistic, diverse attacks.

**方法:** The authors introduce BELLS, a Benchmark for the Evaluation of LLM Supervision Systems. The framework evaluates the harm severity and adversarial sophistication of supervision systems using a rich dataset covering 3 jailbreak families and 11 harm categories.

**结果:** Specialized supervision systems have drastic limitations in their semantic understanding and generalization capabilities, sometimes with detection rates close to zero when asking a harmful question directly or with a new jailbreak technique such as base64 encoding. Simply asking generalist LLMs if the user question is 'harmful or not' largely outperforms these supervisors from the market according to the BELLS score. However, frontier LLMs still suffer from metacognitive incoherence.

**结论:** General capabilities of LLMs are necessary to detect a diverse array of misuses and jailbreaks, and simple scaffolding could significantly improve misuse detection robustness.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+bitter+lesson+of+misuse+detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06282&send_immediately=true&force_search=false)

**原文摘要:** Prior work on jailbreak detection has established the importance of
adversarial robustness for LLMs but has largely focused on the model ability to
resist adversarial inputs and to output safe content, rather than the
effectiveness of external supervision systems. The only public and independent
benchmark of these guardrails to date evaluates a narrow set of supervisors on
limited scenarios. Consequently, no comprehensive public benchmark yet verifies
how well supervision systems from the market perform under realistic, diverse
attacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of
LLM Supervision Systems. The framework is two dimensional: harm severity
(benign, borderline, harmful) and adversarial sophistication (direct vs.
jailbreak) and provides a rich dataset covering 3 jailbreak families and 11
harm categories. Our evaluations reveal drastic limitations of specialized
supervision systems. While they recognize some known jailbreak patterns, their
semantic understanding and generalization capabilities are very limited,
sometimes with detection rates close to zero when asking a harmful question
directly or with a new jailbreak technique such as base64 encoding. Simply
asking generalist LLMs if the user question is "harmful or not" largely
outperforms these supervisors from the market according to our BELLS score. But
frontier LLMs still suffer from metacognitive incoherence, often responding to
queries they correctly identify as harmful (up to 30 percent for Claude 3.7 and
greater than 50 percent for Mistral Large). These results suggest that simple
scaffolding could significantly improve misuse detection robustness, but more
research is needed to assess the tradeoffs of such techniques. Our results
support the "bitter lesson" of misuse detection: general capabilities of LLMs
are necessary to detect a diverse array of misuses and jailbreaks.

</details>


### [93] [Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms](https://arxiv.org/abs/2507.06323)
*Tarek Gasmi, Ramzi Guesmi, Ines Belhadj, Jihene Bennaceur*

**主要类别:** cs.CR

**AI概要:** 本研究评估了大型语言模型代理在不同部署范式下面临的安全威胁，发现架构选择会从根本上改变威胁形势，并提出了一种跨领域的安全评估方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLM）代理面临跨越AI特定领域和传统软件领域的安全漏洞，但当前的研究分别解决这些问题。本研究旨在通过桥接这一差距，来进行对LLM代理在不同部署范式下的安全性评估。

**方法:** 该研究通过对函数调用架构和模型上下文协议（MCP）部署范式使用统一的威胁分类框架进行比较评估，测试了3,250个攻击场景，评估了针对AI特定威胁（如提示注入）和软件漏洞（如JSON注入、拒绝服务）的简单、组合和链式攻击。

**结果:** 函数调用显示出更高的整体攻击成功率（相对于MCP的73.5% vs 62.59%），系统中心的漏洞更大，而MCP表现出增加的LLM中心暴露。攻击复杂性极大地增强了有效性，链式攻击达到了91-96%的成功率。高级推理模型反常地展示了更高的可利用性，尽管它们具有更好的威胁检测能力。

**结论:** 架构选择从根本上重塑了威胁格局。这项工作为跨领域的LLM代理安全评估建立了方法论基础，并提供了基于证据的安全部署指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+AI+and+Software+Security%3A+A+Comparative+Vulnerability+Assessment+of+LLM+Agent+Deployment+Paradigms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06323&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM) agents face security vulnerabilities spanning
AI-specific and traditional software domains, yet current research addresses
these separately. This study bridges this gap through comparative evaluation of
Function Calling architecture and Model Context Protocol (MCP) deployment
paradigms using a unified threat classification framework. We tested 3,250
attack scenarios across seven language models, evaluating simple, composed, and
chained attacks targeting both AI-specific threats (prompt injection) and
software vulnerabilities (JSON injection, denial-of-service). Function Calling
showed higher overall attack success rates (73.5% vs 62.59% for MCP), with
greater system-centric vulnerability while MCP exhibited increased LLM-centric
exposure. Attack complexity dramatically amplified effectiveness, with chained
attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning
models demonstrated higher exploitability despite better threat detection.
Results demonstrate that architectural choices fundamentally reshape threat
landscapes. This work establishes methodological foundations for cross-domain
LLM agent security assessment and provides evidence-based guidance for secure
deployment. Code and experimental materials are available at https: // github.
com/ theconsciouslab-ai/llm-agent-security.

</details>


### [94] [An Architecture for Privacy-Preserving Telemetry Scheme](https://arxiv.org/abs/2507.06350)
*Kenneth Odoh*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种隐私保护遥测聚合方案，重点是通过本地差分隐私和不经意HTTP（OHTTP）增强隐私保护。


<details>
  <summary>更多</summary>
  
**动机:** 目的是在不损害个人记录的可识别性的情况下，通过精心添加噪声来防止重识别攻击，从而促进公共数据发布。同时，解决原始HTTP中已有的隐私漏洞，以提高传输中数据的隐私保护。

**方法:** 本研究设计了一种在差分隐私框架内工作的频率估计例程，并采用了客户端-服务器架构。系统使用本地差分隐私方案，其中数据在客户端随机化后发送到资源服务器。此外，通过利用OHTTP进一步增强了隐私保障。

**结果:** 提供了一个专注于已知字典直方图频率估计的实现。与参考工作相比，基于OHTTP的公式化提供了更严格的隐私保障。

**结论:** 隐私保护遥测聚合方案通过使用本地差分隐私和不经意HTTP（OHTTP），提供了比依赖组织手动删除标识信息更严格的隐私保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Architecture+for+Privacy-Preserving+Telemetry+Scheme，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06350，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06350&send_immediately=true&force_search=false)

**原文摘要:** We present a privacy-preserving telemetry aggregation scheme. Our underlying
frequency estimation routine works within the framework of differential
privacy. The design philosophy follows a client-server architecture.
Furthermore, the system uses a local differential privacy scheme where data
gets randomized on the client before submitting the request to the resource
server. This scheme allows for data analysis on de-identified data by carefully
adding noise to prevent re-identification attacks, thereby facilitating public
data release without compromising the identifiability of the individual record.
This work further enhances privacy guarantees by leveraging Oblivious HTTP
(OHTTP) to achieve increased privacy protection for data in transit that
addresses pre-existing privacy vulnerabilities in raw HTTP. We provide an
implementation that focuses on frequency estimation with a histogram of a known
dictionary. Our resulting formulation based on OHTTP has provided stricter
privacy safeguards when compared to trusting an organization to manually delete
identifying information from the client's request in the ingestor as deployed
in reference work~\cite{apple2017}. Code available at
https://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.

</details>


### [95] [Never Trust the Manufacturer, Never Trust the Client: A Novel Method for Streaming STL Files for Secure Additive](https://arxiv.org/abs/2507.06421)
*Seyed Ali Ghazi Asgar, Narasimha Reddy, Satish T. S. Bukkapatnam*

**主要类别:** cs.CR

**AI概要:** This paper proposes a viable approach when the client and manufacturer do not trust each other and both the client and manufacturer want to preserve their IP of designs and manufacturing process respectively.


<details>
  <summary>更多</summary>
  
**动机:** Securing client's intellectual property (IP), especially from cyber-attacks, emerges as a major challenge. Earlier works introduced streaming, instead of sharing process plan (G-code) files, as a possible solution.

**方法:** The proposed approach is based on segmenting and streaming design (STL) files and employing a novel machine-specific STL to G-code translator at the manufacturer's site in real-time for printing.

**结果:** This approach secures design and manufacturing process IPs as demonstrated in a real-world implementation.

**结论:** The proposed approach secures design and manufacturing process IPs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Never+Trust+the+Manufacturer%2C+Never+Trust+the+Client%3A+A+Novel+Method+for+Streaming+STL+Files+for+Secure+Additive，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06421，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06421&send_immediately=true&force_search=false)

**原文摘要:** While additive manufacturing has opened interesting avenues to reimagine
manufacturing as a service (MaaS) platform, transmission of design files from
client to manufacturer over networks opens up many cybersecurity challenges.
Securing client's intellectual property (IP) especially from cyber-attacks
emerges as a major challenge. Earlier works introduced streaming, instead of
sharing process plan (G-code) files, as a possible solution. However, executing
client's G-codes on manufacturer's machines exposes them to potential malicious
G-codes. This paper proposes a viable approach when the client and manufacturer
do not trust each other and both the client and manufacturer want to preserve
their IP of designs and manufacturing process respectively. The proposed
approach is based on segmenting and streaming design (STL) files and employing
a novel machine-specific STL to G-code translator at the manufacturer's site in
real-time for printing. This approach secures design and manufacturing process
IPs as demonstrated in a real-world implementation.

</details>


### [96] [Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls](https://arxiv.org/abs/2507.06423)
*Jovonni L. Pharr, Jahanzeb M. Hussain*

**主要类别:** cs.CR

**AI概要:** Rugsafe协议旨在通过加密安全措施和经济激励来减轻加密货币生态系统中的rug pulls风险。


<details>
  <summary>更多</summary>
  
**动机:** 在加密货币市场中，rug pulls是一个重大挑战，需要一个有效的解决方案。

**方法:** Rugsafe协议通过设置专门的保险库来存储被rug tokens，并发行anticoin代币作为收据。这些anticoin代币可以与底层被rug tokens的价格变动反向挂钩。用户可以在生态系统内使用这些anticoin或者选择销毁它们以获得额外奖励。

**结果:** 用户可以通过在多个链上存入被rug tokens到保险库并销毁anticoin来获得RugSafe链上的激励。

**结论:** Rugsafe协议提供了一个安全的多链系统，用于恢复资产并将被rug tokens转化为机会和奖励，从而为异构区块链生态系统提供了一个实用且有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rugsafe%3A+A+multichain+protocol+for+recovering+from+and+defending+against+Rug+Pulls，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06423，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06423&send_immediately=true&force_search=false)

**原文摘要:** Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of
rug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security
measures and economic incentives, the protocol provides a secure multichain
system for recovering assets and transforming rugged tokens into opportunities
and rewards. Foundational to Rugsafe are specialized vaults where rugged tokens
can be securely deposited, and anticoin tokens are issued as receipts. These
anticoins are designed to be inversely pegged to the price movement of the
underlying rugged token. Users can utilize these anticoins within the ecosystem
or choose to burn them, further securing the protocol and earning additional
rewards. The supply of the native Rugsafe token is dynamically adjusted based
on the volume, value, and activity of rugged tokens, ensuring stability and
resilience. By depositing rugged tokens into a vault on several chains, and by
burning anticoins, users receive incentives on the RugSafe chain. This
protocol's vaults are designed to work in heterogenous blockchain ecosystems,
offering a practical and effective solution to one of the most significant
challenges in the cryptocurrency market.

</details>


### [97] [HEMA: A Hands-on Exploration Platform for MEMS Sensor Attacks](https://arxiv.org/abs/2507.06439)
*Bhagawat Baanav Yedla Ravi, Md Rafiul Kabir, Sandip Ray*

**主要类别:** cs.CR

**AI概要:** 本文介绍了一种名为\hema的新颖、经济实惠且灵活的探索平台，可让用户深入了解现代ADAS系统中关键组件——微电子机械系统（MEMS）传感器的安全妥协。


<details>
  <summary>更多</summary>
  
**动机:** 汽车技术迅速发展，汽车安全和安全性变得至关重要。传统的学习方法往往无法提供开发这种专业知识所必需的实践经验。对于新手用户来说，获取汽车级系统并掌握其相关的硬件和软件可能是具有挑战性和不知所措的。

**方法:** 提出了一种新颖、经济实惠且灵活的探索平台\hema，使用户能够获得对微电子机械系统（MEMS）传感器的安全妥协的实际见解。

**结果:** 讨论了创建这样一个平台所涉及的独特挑战和设计考虑因素，并强调了它在增强对汽车安全和安全性的理解方面的作用。

**结论:** 该平台为教育工作者、研究人员和从业者提供了宝贵的资源，帮助他们提高在汽车安全和安全领域的专业知识。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HEMA%3A+A+Hands-on+Exploration+Platform+for+MEMS+Sensor+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06439，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06439&send_immediately=true&force_search=false)

**原文摘要:** Automotive safety and security are paramount in the rapidly advancing
landscape of vehicular technology. Building safe and secure vehicles demands a
profound understanding of automotive systems, particularly in safety and
security. Traditional learning approaches, such as reading materials or
observing demonstrations, often fail to provide the practical, hands-on
experience essential for developing this expertise. For novice users, gaining
access to automotive-grade systems and mastering their associated hardware and
software can be challenging and overwhelming. In this paper, we present a
novel, affordable, and flexible exploration platform, \hema, that enables users
to gain practical, hands-on insights into the security compromises of
micro-electromechanical systems (MEMS) sensors, a critical component in modern
ADAS systems. Furthermore, we discuss the unique challenges and design
considerations involved in creating such a platform, emphasizing its role in
enhancing the understanding of automotive safety and security. This framework
serves as an invaluable resource for educators, researchers, and practitioners
striving to build expertise in the field.

</details>


### [98] [Vectorised Hashing Based on Bernstein-Rabin-Winograd Polynomials over Prime Order Fields](https://arxiv.org/abs/2507.06490)
*Kaushik Nath, Palash Sarkar*

**主要类别:** cs.CR

**AI概要:** This paper introduces the new AXU hash function decBRWHash and compares its performance with polyHash.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to introduce a new AXU hash function that can be implemented using SIMD instructions for improved performance.

**方法:** The paper proposes a new AXU hash function decBRWHash based on BRW polynomials and provides optimised assembly implementations using avx2 SIMD instructions.

**结果:** 4-decBRWHash is faster than Poly1305 for messages which are a few hundred bytes long and achieves a speed-up of about 16% for message lengths in a few kilobytes range, improving to a speed-up of about 23% for message lengths in a few megabytes range.

**结论:** 4-decBRWHash is faster than Poly1305 for longer messages and achieves significant speed-up.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Vectorised+Hashing+Based+on+Bernstein-Rabin-Winograd+Polynomials+over+Prime+Order+Fields，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06490，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06490&send_immediately=true&force_search=false)

**原文摘要:** We introduce the new AXU hash function decBRWHash, which is parameterised by
the positive integer $c$ and is based on Bernstein-Rabin-Winograd (BRW)
polynomials. Choosing $c>1$ gives a hash function which can be implemented
using $c$-way single instruction multiple data (SIMD) instructions. We report a
set of very comprehensive hand optimised assembly implementations of
4-decBRWHash using avx2 SIMD instructions available on modern Intel processors.
For comparison, we also report similar carefully optimised avx2 assembly
implementations of polyHash, an AXU hash function based on usual polynomials.
Our implementations are over prime order fields, specifically the primes
$2^{127}-1$ and $2^{130}-5$. For the prime $2^{130}-5$, for avx2
implementations, compared to the famous Poly1305 hash function, 4-decBRWHash is
faster for messages which are a few hundred bytes long and achieves a speed-up
of about 16% for message lengths in a few kilobytes range and improves to a
speed-up of about 23% for message lengths in a few megabytes range.

</details>


### [99] [TELSAFE: Security Gap Quantitative Risk Assessment Framework](https://arxiv.org/abs/2507.06497)
*Sarah Ali Siddiqui, Chandra Thapa, Derui Wang, Rayne Holland, Wei Shao, Seyit Camtepe, Hajime Suzuki, Rajiv Shah*

**主要类别:** cs.CR

**AI概要:** This paper introduces TELSAFE, a hybrid risk assessment framework that uses probabilistic modeling for unbiased quantitative risk assessment and supports tailored risk management strategies.


<details>
  <summary>更多</summary>
  
**动机:** To address security and compliance challenges due to gaps between security standards and their practical implementation.

**方法:** TELSAFE employs probabilistic modeling for quantitative risk assessment and removes expert opinion bias. It includes both qualitative and quantitative assessment phases.

**结果:** A use case with CVE-related data demonstrates TELSAFE's real-world applicability and implementation.

**结论:** The TELSAFE framework is effective for risk management in organizations, particularly in the telecommunications industry.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TELSAFE%3A+Security+Gap+Quantitative+Risk+Assessment+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06497，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06497&send_immediately=true&force_search=false)

**原文摘要:** Gaps between established security standards and their practical
implementation have the potential to introduce vulnerabilities, possibly
exposing them to security risks. To effectively address and mitigate these
security and compliance challenges, security risk management strategies are
essential. However, it must adhere to well-established strategies and industry
standards to ensure consistency, reliability, and compatibility both within and
across organizations. In this paper, we introduce a new hybrid risk assessment
framework called TELSAFE, which employs probabilistic modeling for quantitative
risk assessment and eliminates the influence of expert opinion bias. The
framework encompasses both qualitative and quantitative assessment phases,
facilitating effective risk management strategies tailored to the unique
requirements of organizations. A specific use case utilizing Common
Vulnerabilities and Exposures (CVE)-related data demonstrates the framework's
applicability and implementation in real-world scenarios, such as in the
telecommunications industry.

</details>


### [100] [A Survey on Artificial Noise for Physical Layer Security: Opportunities, Technologies, Guidelines, Advances, and Trends](https://arxiv.org/abs/2507.06500)
*Hong Niu, Yue Xiao, Xia Lei, Jiangong Chen, Zhihan Xiao, Mao Li, Chau Yuen*

**主要类别:** cs.CR

**AI概要:** This paper surveys artificial noise (AN), a promising physical-layer security technique that enhances wireless communication security.


<details>
  <summary>更多</summary>
  
**动机:** Due to the broadcast nature of wireless communications, physical-layer security has attracted increasing concerns from both academia and industry.

**方法:** This paper provides the latest survey of AN, including its evolution, modeling, backgrounds, applications, and future trends.

**结果:** The key distinguishing feature of AN is to generate specific interfering signals according to channel characteristics, increasing the secrecy capacity by reducing the wiretap channel capacity without affecting the legitimate channel capacity.

**结论:** Artificial noise is a promising physical-layer security technique that can effectively enhance the security of wireless communications. However, there are still some technical challenges to tackle for AN-aided wireless security in the future.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+on+Artificial+Noise+for+Physical+Layer+Security%3A+Opportunities%2C+Technologies%2C+Guidelines%2C+Advances%2C+and+Trends，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06500，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06500&send_immediately=true&force_search=false)

**原文摘要:** Due to the broadcast nature of wireless communications, physical-layer
security has attracted increasing concerns from both academia and industry.
Artificial noise (AN), as one of the promising physical-layer security
techniques, is capable of utilizing the spatial degree-of-freedom of channels
to effectively enhance the security of wireless communications. In contrast to
other physicallayer security techniques, the key distinguishing feature of AN
is to generate specific interfering signals according to channel
characteristics, increasing the secrecy capacity by reducing the wiretap
channel capacity without affecting the legitimate channel capacity. Hence, this
paper provides the latest survey of AN, including its evolution, modeling,
backgrounds, applications, and future trends. Initially, we introduce the
development, fundamentals, and backgrounds of AN. Subsequently, we highlight a
comprehensive survey of the current state of research on various AN-empowered
scenarios and AN-combined technologies. Finally, we discuss some technical
challenges to tackle for AN-aided wireless security in the future.

</details>


### [101] [Subgraph Counting under Edge Local Differential Privacy Based on Noisy Adjacency Matrix](https://arxiv.org/abs/2507.06508)
*Jintao Guo, Ying Zhou, Chao Li, Guixun Luo*

**主要类别:** cs.CR

**AI概要:** This paper proposes the Noisy Adjacency Matrix (NAM) to address challenges in subgraph counting under privacy-preserving situations.


<details>
  <summary>更多</summary>
  
**动机:** Existing algorithms for subgraph counting under privacy-preserving situations are plagued by high time complexity, excessive download costs, low accuracy, or dependence on trusted third parties.

**方法:** The Noisy Adjacency Matrix (NAM) combines differential privacy with the adjacency matrix of the graph. Based on NAM, five algorithms (TriOR, TriTR, TriMTR, QuaTR, and 2STAR) are designed to count three types of subgraphs: triangles, quadrangles, and 2-stars.

**结果:** Theoretical and experimental results demonstrate that the designed algorithms based on NAM achieve optimal or highest accuracy with reduced time complexity and low download costs in triangle, quadrangle, and 2-star counting.

**结论:** The proposed Noisy Adjacency Matrix (NAM) offers strong versatility and scalability, making it applicable to a wider range of DP variants, DP mechanisms, and graph types. The five designed algorithms provide efficient and accurate solutions for subgraph counting under privacy-preserving situations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Subgraph+Counting+under+Edge+Local+Differential+Privacy+Based+on+Noisy+Adjacency+Matrix，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06508&send_immediately=true&force_search=false)

**原文摘要:** When analyzing connection patterns within graphs, subgraph counting serves as
an effective and fundamental approach. Edge-local differential privacy
(edge-LDP) and shuffle model have been employed to achieve subgraph counting
under a privacy-preserving situation. Existing algorithms are plagued by high
time complexity, excessive download costs, low accuracy, or dependence on
trusted third parties. To address the aforementioned challenges, we propose the
Noisy Adjacency Matrix (NAM), which combines differential privacy with the
adjacency matrix of the graph. NAM offers strong versatility and scalability,
making it applicable to a wider range of DP variants, DP mechanisms, and graph
types. Based on NAM, we designed five algorithms (TriOR, TriTR, TriMTR, QuaTR,
and 2STAR) to count three types of subgraphs: triangles, quadrangles, and
2-stars. Theoretical and experimental results demonstrate that in triangle
counting, TriOR maximizes accuracy with reduced time complexity among one-round
algorithms, TriTR achieves optimal accuracy, TriMTR achieves the highest
accuracy under low download costs, and QuaTR stands as the first quadrangle
counting algorithm under pure edge-LDP. We implement edge-LDP for noisy data
via a confidence interval-inspired method, providing DP guarantees on
randomized data. Our 2STAR algorithm achieves the highest accuracy in 2-star
counting and can be derived as a byproduct of two-round triangle or quadrangle
counting algorithms, enabling efficient joint estimation of triangle,
quadrangle, and 2-star counts within two query rounds.

</details>


### [102] [Approximating Euler Totient Function using Linear Regression on RSA moduli](https://arxiv.org/abs/2507.06706)
*Gilda Rech Bansimba, Regis F. Babindamana, Beni Blaug N. Ibara*

**主要类别:** cs.CR

**AI概要:** Machine learning approach to approximate Euler's totient function phi using linear regression models.


<details>
  <summary>更多</summary>
  
**动机:** The security of the RSA cryptosystem is based on the intractability of computing Euler's totient function phi(n) for large integers n. Although deriving phi(n) deterministically remains computationally infeasible for cryptographically relevant bit lengths, and machine learning presents a promising alternative for constructing efficient approximations.

**方法:** The regression model is trained to capture the relationship between the modulus and its totient, and tested on unseen samples to evaluate its prediction accuracy.

**结果:** Preliminary results suggest that phi can be approximated within a small relative error margin, which may be sufficient to aid in certain classes of RSA attacks.

**结论:** This research opens a direction for integrating statistical learning techniques into cryptanalysis, providing insights into the feasibility of attacking cryptosystems using approximation based strategies.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Approximating+Euler+Totient+Function+using+Linear+Regression+on+RSA+moduli，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06706，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06706&send_immediately=true&force_search=false)

**原文摘要:** The security of the RSA cryptosystem is based on the intractability of
computing Euler's totient function phi(n) for large integers n. Although
deriving phi(n) deterministically remains computationally infeasible for
cryptographically relevant bit lengths, and machine learning presents a
promising alternative for constructing efficient approximations. In this work,
we explore a machine learning approach to approximate Euler's totient function
phi using linear regression models. We consider a dataset of RSA moduli of 64,
128, 256, 512 and 1024 bits along with their corresponding totient values. The
regression model is trained to capture the relationship between the modulus and
its totient, and tested on unseen samples to evaluate its prediction accuracy.
Preliminary results suggest that phi can be approximated within a small
relative error margin, which may be sufficient to aid in certain classes of RSA
attacks. This research opens a direction for integrating statistical learning
techniques into cryptanalysis, providing insights into the feasibility of
attacking cryptosystems using approximation based strategies.

</details>


### [103] [PotentRegion4MalDetect: Advanced Features from Potential Malicious Regions for Malware Detection](https://arxiv.org/abs/2507.06723)
*Rama Krishna Koppanati, Monika Santra, Sateesh Kumar Peddoju*

**主要类别:** cs.CR

**AI概要:** A new model named PotentRegion4MalDetect focuses on extracting features from potential malicious regions in binaries to improve malware detection accuracy and reduce false positives.


<details>
  <summary>更多</summary>
  
**动机:** Malware developers exploit the fact that most detection models focus on the entire binary for feature extraction rather than on regions of potential maliciousness. This allows them to reverse engineer benign binaries and inject malicious code, circumventing detection models and deceiving ML classifiers.

**方法:** PotentRegion4MalDetect extracts features from potential malicious regions determined by nodes with potential maliciousness in a partially preprocessed CFG using malicious strings from StringSifter. It also extracts features from a completely preprocessed CFG to mitigate obfuscation techniques.

**结果:** Experiments show that PotentRegion4MalDetect requires fewer entries to save features, reduces memory overhead, speeds up computation, lowers storage requirements, and increases SHAP Absolute Mean by 8.13% and SHAP Beeswarm value by 1.44%. Advanced features produce more than 99% accuracy, precision, recall, AUC, F1-score, and 0.064% FPR.

**结论:** The PotentRegion4MalDetect model is effective in reducing memory overhead, speeding up computation, and lowering storage requirements while providing advanced features that outperform those extracted from the entire binary.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PotentRegion4MalDetect%3A+Advanced+Features+from+Potential+Malicious+Regions+for+Malware+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06723，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06723&send_immediately=true&force_search=false)

**原文摘要:** Malware developers exploit the fact that most detection models focus on the
entire binary to extract the feature rather than on the regions of potential
maliciousness. Therefore, they reverse engineer a benign binary and inject
malicious code into it. This obfuscation technique circumvents the malware
detection models and deceives the ML classifiers due to the prevalence of
benign features compared to malicious features. However, extracting the
features from the potential malicious regions enhances the accuracy and
decreases false positives. Hence, we propose a novel model named
PotentRegion4MalDetect that extracts features from the potential malicious
regions. PotentRegion4MalDetect determines the nodes with potential
maliciousness in the partially preprocessed Control Flow Graph (CFG) using the
malicious strings given by StringSifter. Then, it extracts advanced features of
the identified potential malicious regions alongside the features from the
completely preprocessed CFG. The features extracted from the completely
preprocessed CFG mitigate obfuscation techniques that attempt to disguise
malicious content, such as suspicious strings. The experiments reveal that the
PotentRegion4MalDetect requires fewer entries to save the features for all
binaries than the model focusing on the entire binary, reducing memory
overhead, faster computation, and lower storage requirements. These advanced
features give an 8.13% increase in SHapley Additive exPlanations (SHAP)
Absolute Mean and a 1.44% increase in SHAP Beeswarm value compared to those
extracted from the entire binary. The advanced features outperform the features
extracted from the entire binary by producing more than 99% accuracy,
precision, recall, AUC, F1-score, and 0.064% FPR.

</details>


### [104] [PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI](https://arxiv.org/abs/2507.06742)
*Haitham S. Al-Sinani, Chris J. Mitchell*

**主要类别:** cs.CR

**AI概要:** 本文介绍了'PenTest2.0'，这是之前AI增强系统'PenTest++'的大幅进化版，支持完全由大型语言模型推理驱动的自动化特权提升，并包含了多个显著的增强功能。


<details>
  <summary>更多</summary>
  
**动机:** 当前的道德黑客技术依赖于高技能实践者执行复杂的命令序列，这种方法耗时、难以扩展且容易出现人为错误。为帮助缓解这些限制，之前引入了结合自动化与生成式AI支持道德黑客工作流程的AI增强系统'PenTest++'。然而，PenTest++的一个关键限制是它缺乏对特权提升的支持，这是道德黑客的重要组成部分。

**方法:** PenTest2.0是一个大幅进化的版本，支持完全由大型语言模型推理驱动的自动化特权提升，并包括几个重要的增强功能：检索增强生成（包括单行和离线模式）、用于中间推理的链式思维提示、持久的任务树以跟踪跨轮次的目标进展以及可选的人类编写提示集成。

**结果:** 描述了系统的工作原理，提出了一个概念验证原型，并讨论了其优点和局限性；描述了将该系统应用于受控Linux目标的情况，表明它可以进行多轮次、适应性的特权提升；解释了核心设计选择背后的理由，并提供了全面的测试结果和成本分析。

**结论:** PenTest2.0代表了向实用、可扩展的AI自动化渗透测试迈出的有意义的一步，同时也突显了生成式AI系统的不足之处，特别是它们对提示结构、执行上下文和语义漂移的敏感性，这进一步强调了在这个新兴领域进行更多研究和改进的必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PenTest2.0%3A+Towards+Autonomous+Privilege+Escalation+Using+GenAI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06742&send_immediately=true&force_search=false)

**原文摘要:** Ethical hacking today relies on highly skilled practitioners executing
complex sequences of commands, which is inherently time-consuming, difficult to
scale, and prone to human error. To help mitigate these limitations, we
previously introduced 'PenTest++', an AI-augmented system combining automation
with generative AI supporting ethical hacking workflows. However, a key
limitation of PenTest++ was its lack of support for privilege escalation, a
crucial element of ethical hacking. In this paper we present 'PenTest2.0', a
substantial evolution of PenTest++ supporting automated privilege escalation
driven entirely by Large Language Model reasoning. It also incorporates several
significant enhancements: 'Retrieval-Augmented Generation', including both
one-line and offline modes; 'Chain-of-Thought' prompting for intermediate
reasoning; persistent 'PenTest Task Trees' to track goal progression across
turns; and the optional integration of human-authored hints. We describe how it
operates, present a proof-of-concept prototype, and discuss its benefits and
limitations. We also describe application of the system to a controlled Linux
target, showing it can carry out multi-turn, adaptive privilege escalation. We
explain the rationale behind its core design choices, and provide comprehensive
testing results and cost analysis. Our findings indicate that 'PenTest2.0'
represents a meaningful step toward practical, scalable, AI-automated
penetration testing, whilst highlighting the shortcomings of generative AI
systems, particularly their sensitivity to prompt structure, execution context,
and semantic drift, reinforcing the need for further research and refinement in
this emerging space.
  Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM
(Large Language Model), HITL (Human-in-the-Loop)

</details>


### [105] [The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover](https://arxiv.org/abs/2507.06850)
*Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, Angelo Furfaro*

**主要类别:** cs.CR

**AI概要:** 本文首次全面评估了大型语言模型（LLM）代理作为攻击向量的能力，展示了通过利用代理AI系统内部的信任边界实现完全的计算机接管的可能性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLM）代理和多代理系统的迅速采用使自然语言处理和生成方面具备了前所未有的能力。然而，这些系统引入了超越传统提示注入攻击的安全漏洞。本文旨在全面评估LLM代理作为攻击向量的能力，通过利用代理AI系统内的信任边界实现完全的计算机接管。

**方法:** 通过对17种最先进的LLM进行评估，揭示了一个惊人的漏洞层次结构：41.2%的模型容易受到直接提示注入的影响，52.9%的模型容易受到RAG后门攻击，82.4%的模型可以通过代理间的信任利用被攻破。值得注意的是，成功抵抗直接恶意命令的LLM在同行代理请求时会执行相同的载荷，揭示了当前多代理安全模型中的一个基本缺陷。

**结果:** 评估发现，敌人可以利用三种不同的攻击面 - 直接提示注入、RAG后门攻击和代理间信任利用 - 来迫使流行的LLM自主地在受害机器上安装和执行恶意软件。

**结论:** 研究结果表明，只有5.9%的测试模型对所有攻击向量都具有抵抗力，而大多数模型表现出依赖上下文的安全行为，这为利用盲点提供了可能。同时，强调了需要提高对LLM安全风险的认识和研究，显示了网络安全威胁的范式转变，即AI工具本身成为复杂的攻击向量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Dark+Side+of+LLMs+Agent-based+Attacks+for+Complete+Computer+Takeover，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06850，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06850&send_immediately=true&force_search=false)

**原文摘要:** The rapid adoption of Large Language Model (LLM) agents and multi-agent
systems enables unprecedented capabilities in natural language processing and
generation. However, these systems have introduced unprecedented security
vulnerabilities that extend beyond traditional prompt injection attacks. This
paper presents the first comprehensive evaluation of LLM agents as attack
vectors capable of achieving complete computer takeover through the
exploitation of trust boundaries within agentic AI systems where autonomous
entities interact and influence each other. We demonstrate that adversaries can
leverage three distinct attack surfaces - direct prompt injection, RAG backdoor
attacks, and inter-agent trust exploitation - to coerce popular LLMs (including
GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing
malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals
an alarming vulnerability hierarchy: while 41.2% of models succumb to direct
prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical
82.4% can be compromised through inter-agent trust exploitation. Notably, we
discovered that LLMs which successfully resist direct malicious commands will
execute identical payloads when requested by peer agents, revealing a
fundamental flaw in current multi-agent security models. Our findings
demonstrate that only 5.9% of tested models (1/17) proved resistant to all
attack vectors, with the majority exhibiting context-dependent security
behaviors that create exploitable blind spots. Our findings also highlight the
need to increase awareness and research on the security risks of LLMs, showing
a paradigm shift in cybersecurity threats, where AI tools themselves become
sophisticated attack vectors.

</details>


### [106] [Are NFTs Ready to Keep Australian Artists Engaged?](https://arxiv.org/abs/2507.06926)
*Ruiqiang Li, Brian Yecies, Qin Wang, Shiping Chen, Jun Shen*

**主要类别:** cs.CR

**AI概要:** This paper investigates the use of Non-Fungible Tokens (NFTs) to protect Australian and Indigenous artists' copyright. Despite the potential of NFTs in representing and transferring the value of artwork in digital form, the study concludes that NFTs are not yet ready to protect these artists' copyright.


<details>
  <summary>更多</summary>
  
**动机:** Non-Fungible Tokens (NFTs) offer a promising mechanism to protect Australian and Indigenous artists' copyright.

**方法:** Empirical investigation of NFTs, focusing on examining the details of NFT structure. Data collection from various types of sources with different storage methods, including on-chain, centralized, and decentralized systems. Analysis and discussion based on both metadata and artwork content.

**结果:** The evaluation shows that the NFT is NOT ready to protect Australian and Indigenous artists' copyright.

**结论:** The NFT is NOT ready to protect Australian and Indigenous artists' copyright.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+NFTs+Ready+to+Keep+Australian+Artists+Engaged%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06926，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06926&send_immediately=true&force_search=false)

**原文摘要:** Non-Fungible Tokens (NFTs) offer a promising mechanism to protect Australian
and Indigenous artists' copyright. They represent and transfer the value of
artwork in digital form. Before adopting NFTs to protect Australian artwork, we
in this paper investigate them empericially. We focus on examining the details
of NFT structure. We start from the underlying structure of NFTs to show how
they represent copyright for both artists and production owners, as well as how
they aim to safeguard or secure the value of digital artworks. We then involve
data collection from various types of sources with different storage methods,
including on-chain, centralized, and decentralized systems. Based on both
metadata and artwork content, we present our analysis and discussion on the
following key issues: copyright, security and artist identification. The final
results of the evaluation, unfortnately, show that the NFT is NOT ready to
protect Australian and Indigenous artists' copyright.

</details>


### [107] [BarkBeetle: Stealing Decision Tree Models with Fault Injection](https://arxiv.org/abs/2507.06986)
*Qifan Wang, Jonas Sander, Minmin Jiang, Thomas Eisenbarth, David Oswald*

**主要类别:** cs.CR

**AI概要:** 本文介绍了BarkBeetle，这是一种利用故障注入提取决策树模型内部结构信息的新攻击方式。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器学习模型越来越多地集成到隐私敏感的应用中，其保密性的担忧也在增加，尤其是面对模型提取和故障注入攻击等新兴威胁。因此，评估决策树在这种攻击下的脆弱性变得尤为重要。

**方法:** BarkBeetle使用一种自底向上的恢复策略，通过在特定节点处进行有针对性的故障注入来推断特征分裂和阈值。

**结果:** BarkBeetle的实现表明，在使用公共UCI数据集训练的决策树上，与之前的方法相比，它需要更少的查询次数并且能够恢复更多的结构信息。

**结论:** BarkBeetle对基于树的应用程序的安全性提出了新的挑战，并且需要进一步的研究来保护决策树模型免受此类攻击。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BarkBeetle%3A+Stealing+Decision+Tree+Models+with+Fault+Injection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.06986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.06986&send_immediately=true&force_search=false)

**原文摘要:** Machine learning models, particularly decision trees (DTs), are widely
adopted across various domains due to their interpretability and efficiency.
However, as ML models become increasingly integrated into privacy-sensitive
applications, concerns about their confidentiality have grown, particularly in
light of emerging threats such as model extraction and fault injection attacks.
Assessing the vulnerability of DTs under such attacks is therefore important.
In this work, we present BarkBeetle, a novel attack that leverages fault
injection to extract internal structural information of DT models. BarkBeetle
employs a bottom-up recovery strategy that uses targeted fault injection at
specific nodes to efficiently infer feature splits and threshold values. Our
proof-of-concept implementation demonstrates that BarkBeetle requires
significantly fewer queries and recovers more structural information compared
to prior approaches, when evaluated on DTs trained with public UCI datasets. To
validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi
RP2350 board and perform fault injections using the Faultier voltage glitching
tool. As BarkBeetle targets general DT models, we also provide an in-depth
discussion on its applicability to a broader range of tree-based applications,
including data stream classification, DT variants, and cryptography schemes.

</details>


### [108] [ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel Proof Accumulation](https://arxiv.org/abs/2507.07031)
*Bing-Jyue Chen, Lilia Tang, Daniel Kang*

**主要类别:** cs.CR

**AI概要:** This paper introduces ZKTorch, an open source end-to-end proving system that compiles ML models into base cryptographic operations called basic blocks, each proved using specialized protocols. ZKTorch is built on top of a novel parallel extension to the Mira accumulation scheme, enabling succinct proofs with minimal accumulation overhead.


<details>
  <summary>更多</summary>
  
**动机:** The increasing demand for transparency in ML services while protecting the confidentiality of model weights as trade secrets motivates the development of ZKTorch.

**方法:** ZKTorch compiles ML models into base cryptographic operations called basic blocks, each proved using specialized protocols. It is built on top of a novel parallel extension to the Mira accumulation scheme, enabling succinct proofs with minimal accumulation overhead.

**结果:** ZKTorch achieves at least a $3\times$ reduction in the proof size compared to specialized protocols and up to a $6\times$ speedup in proving time over a general-purpose ZKML framework.

**结论:** ZKTorch provides an efficient and generalized solution for zero-knowledge proofs of ML model inference, making it practical for large models and adaptable to the rapidly changing field of machine learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZKTorch%3A+Compiling+ML+Inference+to+Zero-Knowledge+Proofs+via+Parallel+Proof+Accumulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07031，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07031&send_immediately=true&force_search=false)

**原文摘要:** As AI models become ubiquitous in our daily lives, there has been an
increasing demand for transparency in ML services. However, the model owner
does not want to reveal the weights, as they are considered trade secrets. To
solve this problem, researchers have turned to zero-knowledge proofs of ML
model inference. These proofs convince the user that the ML model output is
correct, without revealing the weights of the model to the user. Past work on
these provers can be placed into two categories. The first method compiles the
ML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The
second method uses custom cryptographic protocols designed only for a specific
class of models. Unfortunately, the first method is highly inefficient, making
it impractical for the large models used today, and the second method does not
generalize well, making it difficult to update in the rapidly changing field of
machine learning. To solve this, we propose ZKTorch, an open source end-to-end
proving system that compiles ML models into base cryptographic operations
called basic blocks, each proved using specialized protocols. ZKTorch is built
on top of a novel parallel extension to the Mira accumulation scheme, enabling
succinct proofs with minimal accumulation overhead. These contributions allow
ZKTorch to achieve at least a $3\times$ reduction in the proof size compared to
specialized protocols and up to a $6\times$ speedup in proving time over a
general-purpose ZKML framework.

</details>


### [109] [LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing](https://arxiv.org/abs/2507.07056)
*Jiahao Chen, junhao li, Yiming Wang, Zhe Ma, Yi Jiang, Chunyi Zhou, Qingming Li, Tianyu Du, Shouling Ji*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种名为LoRAShield的数据免费编辑框架，可以保护LoRA模型免受滥用。


<details>
  <summary>更多</summary>
  
**动机:** 低秩适应（LoRA）模型的激增使得个性化的文本到图像生成民主化，然而，这种“分享与玩耍”的生态系统引入了关键风险：良性LoRAs可能被对手利用生成有害内容，破坏创作者权益和平台安全。现有的防御方法如概念擦除方法专注于完整的扩散模型（DMs），忽视了LoRA作为模块化适配器的独特角色及其对抗提示工程的脆弱性。

**方法:** 我们提出了LoRAShield，这是第一个无数据编辑框架，用于保护LoRA模型免受滥用。我们的平台驱动方法通过对抗优化和语义增强动态编辑和重新对齐LoRA的权重子空间。

**结果:** 实验结果表明，LoRAShield在阻止恶意生成方面取得了显著的有效性、效率和鲁棒性，同时没有牺牲良性任务的功能。

**结论:** LoRAShield通过将防御转移到平台，实现了个性化模型的安全、可扩展共享，是实现可信生成生态系统的关键步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LoRAShield%3A+Data-Free+Editing+Alignment+for+Secure+Personalized+LoRA+Sharing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07056，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07056&send_immediately=true&force_search=false)

**原文摘要:** The proliferation of Low-Rank Adaptation (LoRA) models has democratized
personalized text-to-image generation, enabling users to share lightweight
models (e.g., personal portraits) on platforms like Civitai and Liblib.
However, this "share-and-play" ecosystem introduces critical risks: benign
LoRAs can be weaponized by adversaries to generate harmful content (e.g.,
political, defamatory imagery), undermining creator rights and platform safety.
Existing defenses like concept-erasure methods focus on full diffusion models
(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability
to adversarial prompt engineering. To bridge this gap, we propose LoRAShield,
the first data-free editing framework for securing LoRA models against misuse.
Our platform-driven approach dynamically edits and realigns LoRA's weight
subspace via adversarial optimization and semantic augmentation. Experimental
results demonstrate that LoRAShield achieves remarkable effectiveness,
efficiency, and robustness in blocking malicious generations without
sacrificing the functionality of the benign task. By shifting the defense to
platforms, LoRAShield enables secure, scalable sharing of personalized models,
a critical step toward trustworthy generative ecosystems.

</details>
