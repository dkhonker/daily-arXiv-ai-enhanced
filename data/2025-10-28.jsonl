{"id": "2510.20852", "pdf": "https://arxiv.org/pdf/2510.20852", "abs": "https://arxiv.org/abs/2510.20852", "authors": ["Safa Ben Atitallah", "Maha Driss", "Henda Ben Ghezela"], "title": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "categories": ["cs.CR"], "comment": null, "summary": "The Internet of Things (IoT) has recently proliferated in both size and\ncomplexity. Using multi-source and heterogeneous IoT data aids in providing\nefficient data analytics for a variety of prevalent and crucial applications.\nTo address the privacy and security concerns raised by analyzing IoT data\nlocally or in the cloud, distributed data analytics techniques were proposed to\ncollect and analyze data in edge or fog devices. In this context, federated\nlearning has been recommended as an ideal distributed machine/deep\nlearning-based technique for edge/fog computing environments. Additionally, the\ndata analytics results are time-sensitive; they should be generated with\nminimal latency and high reliability. As a result, reusing efficient\narchitectures validated through a high number of challenging test cases would\nbe advantageous. The work proposed here presents a solution using a\nmicroservices-based architecture that allows an IoT application to be\nstructured as a collection of fine-grained, loosely coupled, and reusable\nentities. The proposed solution uses the promising capabilities of federated\nlearning to provide intelligent microservices that ensure efficient, flexible,\nand extensible data analytics. This solution aims to deliver cloud calculations\nto the edge to reduce latency and bandwidth congestion while protecting the\nprivacy of exchanged data. The proposed approach was validated through an\nIoT-malware detection and classification use case. MaleVis, a publicly\navailable dataset, was used in the experiments to analyze and validate the\nproposed approach. This dataset included more than 14,000 RGB-converted images,\ncomprising 25 malware classes and one benign class. The results showed that our\nproposed approach outperformed existing state-of-the-art methods in terms of\ndetection and classification performance, with a 99.24%."}
{"id": "2510.20856", "pdf": "https://arxiv.org/pdf/2510.20856", "abs": "https://arxiv.org/abs/2510.20856", "authors": ["Jia Deng", "Jin Li", "Zhenhua Zhao", "Shaowei Wang"], "title": "FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models", "categories": ["cs.CR"], "comment": "11pages,4figures", "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot generalizability across diverse downstream tasks. However, recent\nstudies have revealed that VLMs, including CLIP, are highly vulnerable to\nadversarial attacks, particularly on their visual modality. Traditional methods\nfor improving adversarial robustness, such as adversarial training, involve\nextensive retraining and can be computationally expensive. In this paper, we\npropose a new Test-Time defense: Feature Perception Threshold Counterattack\nNoise (FPT-Noise), which enhances the adversarial robustness of CLIP without\ncostly fine-tuning. Our core contributions are threefold: First, we introduce a\nDynamic Feature Modulator that dynamically generate an image-specific and\nattack-adaptive noise intensity parameter. Second, We reanalyzed the image\nfeatures of CLIP. When images are exposed to different levels of noise, clean\nimages and adversarial images exhibit distinct rates of feature change. We\nestablished a feature perception threshold to distinguish clean images from\nattacked ones. Finally, we integrate a Scene-Aware Regulation guided by a\nstability threshold and leverage Test-Time Transformation Ensembling (TTE) to\nfurther mitigate the impact of residual noise and enhance robustness.Extensive\nexperimentation has demonstrated that FPT-Noise significantly outperforms\nexisting Test-Time defense methods, boosting average robust accuracy from 0.07%\nto 56.86% under AutoAttack while maintaining high performance on clean images\n(-1.1%). The code will be made public following the publication of the study.\nThe code will be made public following the publication of the study."}
{"id": "2510.20858", "pdf": "https://arxiv.org/pdf/2510.20858", "abs": "https://arxiv.org/abs/2510.20858", "authors": ["Nubio Vidal", "Naghmeh Moradpoor", "Leandros Maglaras"], "title": "Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology", "categories": ["cs.CR"], "comment": null, "summary": "Operational technology (OT) networks are increasingly coupled with\ninformation technology (IT), expanding the attack surface and complicating\nincident response. Although OT standards emphasise incident reporting and\nevidence preservation, they do not specify what data to capture during an\nincident, which hinders coordination across stakeholders. In contrast, IT\nguidance defines reporting content but does not address OT constraints. This\npaper presents the Agnostic Incident Reporting (AIR) framework for live OT\nincident reporting. AIR comprises 25 elements organised into seven groups to\ncapture incident context, chronology, impacts, and actions, tailored to\ntechnical, managerial, and regulatory needs. We evaluate AIR by mapping it to\nmajor OT standards, defining activation points for integration and triggering\nestablished OT frameworks, and then retrospectively applying it to the 2015\nUkrainian distribution grid incident. The evaluation indicates that AIR\ntranslates high-level requirements into concrete fields, overlays existing\nframeworks without vendor dependence, and can support situational awareness and\ncommunication during response. AIR offers a basis for standardising live OT\nincident reporting while supporting technical coordination and regulatory\nalignment."}
{"id": "2510.20922", "pdf": "https://arxiv.org/pdf/2510.20922", "abs": "https://arxiv.org/abs/2510.20922", "authors": ["Luigi D. C. Soares", "MÃ¡rio S. Alvim", "Natasha Fernandes"], "title": "A new measure for dynamic leakage based on quantitative information flow", "categories": ["cs.CR", "cs.IT", "math.IT"], "comment": null, "summary": "Quantitative information flow (QIF) is concerned with assessing the leakage\nof information in computational systems. In QIF there are two main perspectives\nfor the quantification of leakage. On one hand, the static perspective\nconsiders all possible runs of the system in the computation of information\nflow, and is usually employed when preemptively deciding whether or not to run\nthe system. On the other hand, the dynamic perspective considers only a\nspecific, concrete run of the system that has been realised, while ignoring all\nother runs. The dynamic perspective is relevant for, e.g., system monitors and\ntrackers, especially when deciding whether to continue or to abort a particular\nrun based on how much leakage has occurred up to a certain point. Although the\nstatic perspective of leakage is well-developed in the literature, the dynamic\nperspective still lacks the same level of theoretical maturity. In this paper\nwe take steps towards bridging this gap with the following key contributions:\n(i) we provide a novel definition of dynamic leakage that decouples the\nadversary's belief about the secret value from a baseline distribution on\nsecrets against which the success of the attack is measured; (ii) we\ndemonstrate that our formalisation satisfies relevant information-theoretic\naxioms, including non-interference and relaxed versions of monotonicity and the\ndata-processing inequality (DPI); (iii) we identify under what kind of analysis\nstrong versions of the axioms of monotonicity and the DPI might not hold, and\nexplain the implications of this (perhaps counter-intuitive) outcome; (iv) we\nshow that our definition of dynamic leakage is compatible with the\nwell-established static perspective; and (v) we exemplify the use of our\ndefinition on the formalisation of attacks against privacy-preserving data\nreleases."}
{"id": "2510.20930", "pdf": "https://arxiv.org/pdf/2510.20930", "abs": "https://arxiv.org/abs/2510.20930", "authors": ["Soham Hans", "Stacy Marsella", "Sophia Hirschmann", "Nikolos Gurney"], "title": "Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Understanding adversarial behavior in cybersecurity has traditionally relied\non high-level intelligence reports and manual interpretation of attack chains.\nHowever, real-time defense requires the ability to infer attacker intent and\ncognitive strategy directly from low-level system telemetry such as intrusion\ndetection system (IDS) logs. In this paper, we propose a novel framework that\nleverages large language models (LLMs) to analyze Suricata IDS logs and infer\nattacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded\nin the hypothesis that attacker behavior reflects underlying cognitive biases\nsuch as loss aversion, risk tolerance, or goal persistence that can be\nextracted and modeled through careful observation of log sequences. This lays\nthe groundwork for future work on behaviorally adaptive cyber defense and\ncognitive trait inference. We develop a strategy-driven prompt system to\nsegment large amounts of network logs data into distinct behavioral phases in a\nhighly efficient manner, enabling the LLM to associate each phase with likely\ntechniques and underlying cognitive motives. By mapping network-layer events to\nhigh-level attacker strategies, our method reveals how behavioral signals such\nas tool switching, protocol transitions, or pivot patterns correspond to\npsychologically meaningful decision points. The results demonstrate that LLMs\ncan bridge the semantic gap between packet-level logs and strategic intent,\noffering a pathway toward cognitive-adaptive cyber defense.\n  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),\nCyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive\nBiases"}
{"id": "2510.20932", "pdf": "https://arxiv.org/pdf/2510.20932", "abs": "https://arxiv.org/abs/2510.20932", "authors": ["Reza Ahmari", "Ahmad Mohammadi", "Vahid Hemmati", "Mohammed Mynuddin", "Mahmoud Nabil Mahmoud", "Parham Kebria", "Abdollah Homaifar", "Mehrdad Saif"], "title": "An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.RO"], "comment": "6 pages", "summary": "This study investigates the vulnerabilities of autonomous navigation and\nlanding systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses\non Trojan attacks that target deep learning models, such as Convolutional\nNeural Networks (CNNs). Trojan attacks work by embedding covert triggers within\na model's training data. These triggers cause specific failures under certain\nconditions, while the model continues to perform normally in other situations.\nWe assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using\nthe DroNet framework. Our experiments showed a significant drop in accuracy,\nfrom 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To\nconduct this study, we collected a custom dataset and trained models to\nsimulate real-world conditions. We also developed an evaluation framework\ndesigned to identify Trojan-infected models. This work demonstrates the\npotential security risks posed by Trojan attacks and lays the groundwork for\nfuture research on enhancing the resilience of UAM systems."}
{"id": "2510.20956", "pdf": "https://arxiv.org/pdf/2510.20956", "abs": "https://arxiv.org/abs/2510.20956", "authors": ["Zheng-Xin Yong", "Stephen H. Bach"], "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "We discover a novel and surprising phenomenon of unintentional misalignment\nin reasoning language models (RLMs), which we call self-jailbreaking.\nSpecifically, after benign reasoning training on math or code domains, RLMs\nwill use multiple strategies to circumvent their own safety guardrails. One\nstrategy is to introduce benign assumptions about users and scenarios to\njustify fulfilling harmful requests. For instance, an RLM reasons that harmful\nrequests like ``outline a strategy for stealing customers' credit card\ninformation from a retail store'' could be associated with the benign intent of\n``a security professional trying to test defense,'' despite no such benign\ncontext being provided as input. We observe that many open-weight RLMs,\nincluding DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,\nsuffer from self-jailbreaking despite being aware of the harmfulness of the\nrequests. We also provide a mechanistic understanding of self-jailbreaking:\nRLMs are more compliant after benign reasoning training, and after\nself-jailbreaking, models appear to perceive malicious requests as less harmful\nin the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,\nwe find that including minimal safety reasoning data during training is\nsufficient to ensure RLMs remain safety-aligned. Our work provides the first\nsystematic analysis of self-jailbreaking behavior and offers a practical path\nforward for maintaining safety in increasingly capable RLMs."}
{"id": "2510.20975", "pdf": "https://arxiv.org/pdf/2510.20975", "abs": "https://arxiv.org/abs/2510.20975", "authors": ["Darrin Lea", "James Ghawaly", "Golden Richard III", "Aisha Ali-Gombe", "Andrew Case"], "title": "REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted in 2025 Annual Computer Security Applications Conference\n  (ACSAC)", "summary": "Reverse engineering (RE) of x86 binaries is indispensable for malware and\nfirmware analysis, but remains slow due to stripped metadata and adversarial\nobfuscation. Large Language Models (LLMs) offer potential for improving RE\nefficiency through automated comprehension and commenting, but cloud-hosted,\nclosed-weight models pose privacy and security risks and cannot be used in\nclosed-network facilities. We evaluate parameter-efficient fine-tuned local\nLLMs for assisting with x86 RE tasks in these settings. Eight open-weight\nmodels across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned\non a custom curated dataset of 5,981 x86 assembly examples. We evaluate them\nquantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top\nperformer, which we name REx86.\n  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic\ncosine similarity against ground truth by 20.3\\% over its base model. In a\nlimited user case study (n=43), REx86 significantly enhanced line-level code\nunderstanding (p = 0.031) and increased the correct-solve rate from 31% to 53%\n(p = 0.189), though the latter did not reach statistical significance.\nQualitative analysis shows more accurate, concise comments with fewer\nhallucinations.\n  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight\nLLMs. Our findings demonstrate the value of domain-specific fine-tuning, and\nhighlight the need for more commented disassembly data to further enhance LLM\nperformance in RE. REx86, its dataset, and LoRA adapters are publicly available\nat https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461."}
{"id": "2510.21004", "pdf": "https://arxiv.org/pdf/2510.21004", "abs": "https://arxiv.org/abs/2510.21004", "authors": ["Nguyen Linh Bao Nguyen", "Alsharif Abuadbba", "Kristen Moore", "Tingming Wu"], "title": "Can Current Detectors Catch Face-to-Voice Deepfake Attacks?", "categories": ["cs.CR", "cs.LG", "cs.MM", "cs.SD"], "comment": "8 pages, Accepted at Workshop on AI for Cyber Threat Intelligence,\n  co-located with ACSAC 2025", "summary": "The rapid advancement of generative models has enabled the creation of\nincreasingly stealthy synthetic voices, commonly referred to as audio\ndeepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly\nalarming capability: generating a victim's voice from a single facial image,\nwithout requiring any voice sample. By exploiting correlations between facial\nand vocal features, FOICE produces synthetic voices realistic enough to bypass\nindustry-standard authentication systems, including WeChat Voiceprint and\nMicrosoft Azure. This raises serious security concerns, as facial images are\nfar easier for adversaries to obtain than voice samples, dramatically lowering\nthe barrier to large-scale attacks. In this work, we investigate two core\nresearch questions: (RQ1) can state-of-the-art audio deepfake detectors\nreliably detect FOICE-generated speech under clean and noisy conditions, and\n(RQ2) whether fine-tuning these detectors on FOICE data improves detection\nwithout overfitting, thereby preserving robustness to unseen voice generators\nsuch as SpeechT5.\n  Our study makes three contributions. First, we present the first systematic\nevaluation of FOICE detection, showing that leading detectors consistently fail\nunder both standard and noisy conditions. Second, we introduce targeted\nfine-tuning strategies that capture FOICE-specific artifacts, yielding\nsignificant accuracy improvements. Third, we assess generalization after\nfine-tuning, revealing trade-offs between specialization to FOICE and\nrobustness to unseen synthesis pipelines. These findings expose fundamental\nweaknesses in today's defenses and motivate new architectures and training\nprotocols for next-generation audio deepfake detection."}
{"id": "2510.21024", "pdf": "https://arxiv.org/pdf/2510.21024", "abs": "https://arxiv.org/abs/2510.21024", "authors": ["Jonathan Gold", "Tristan Freiberg", "Haruna Isah", "Shirin Shahabi"], "title": "JSTprove: Pioneering Verifiable AI for a Trustless Future", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "comment": "13 pages, 8 figures, and 4 tables", "summary": "The integration of machine learning (ML) systems into critical industries\nsuch as healthcare, finance, and cybersecurity has transformed decision-making\nprocesses, but it also brings new challenges around trust, security, and\naccountability. As AI systems become more ubiquitous, ensuring the transparency\nand correctness of AI-driven decisions is crucial, especially when they have\ndirect consequences on privacy, security, or fairness. Verifiable AI, powered\nby Zero-Knowledge Machine Learning (zkML), offers a robust solution to these\nchallenges. zkML enables the verification of AI model inferences without\nexposing sensitive data, providing an essential layer of trust and privacy.\nHowever, traditional zkML systems typically require deep cryptographic\nexpertise, placing them beyond the reach of most ML engineers. In this paper,\nwe introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's\nExpander backend, to enable AI developers and ML engineers to generate and\nverify proofs of AI inference. JSTprove provides an end-to-end verifiable AI\ninference pipeline that hides cryptographic complexity behind a simple\ncommand-line interface while exposing auditable artifacts for reproducibility.\nWe present the design, innovations, and real-world use cases of JSTprove as\nwell as our blueprints and tooling to encourage community review and extension.\nJSTprove therefore serves both as a usable zkML product for current engineering\nneeds and as a reproducible foundation for future research and production\ndeployments of verifiable AI."}
{"id": "2510.21053", "pdf": "https://arxiv.org/pdf/2510.21053", "abs": "https://arxiv.org/abs/2510.21053", "authors": ["Li An", "Yujian Liu", "Yepeng Liu", "Yuheng Bu", "Yang Zhang", "Shiyu Chang"], "title": "A Reinforcement Learning Framework for Robust and Secure LLM Watermarking", "categories": ["cs.CR"], "comment": null, "summary": "Watermarking has emerged as a promising solution for tracing and\nauthenticating text generated by large language models (LLMs). A common\napproach to LLM watermarking is to construct a green/red token list and assign\nhigher or lower generation probabilities to the corresponding tokens,\nrespectively. However, most existing watermarking algorithms rely on heuristic\ngreen/red token list designs, as directly optimizing the list design with\ntechniques such as reinforcement learning (RL) comes with several challenges.\nFirst, desirable watermarking involves multiple criteria, i.e., detectability,\ntext quality, robustness against removal attacks, and security against spoofing\nattacks. Directly optimizing for these criteria introduces many partially\nconflicting reward terms, leading to an unstable convergence process. Second,\nthe vast action space of green/red token list choices is susceptible to reward\nhacking. In this paper, we propose an end-to-end RL framework for robust and\nsecure LLM watermarking. Our approach adopts an anchoring mechanism for reward\nterms to ensure stable training and introduces additional regularization terms\nto prevent reward hacking. Experiments on standard benchmarks with two backbone\nLLMs show that our method achieves a state-of-the-art trade-off across all\ncriteria, with notable improvements in resistance to spoofing attacks without\ndegrading other criteria. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/RL-watermark."}
{"id": "2510.21057", "pdf": "https://arxiv.org/pdf/2510.21057", "abs": "https://arxiv.org/abs/2510.21057", "authors": ["Nils Philipp Walter", "Chawin Sitawarin", "Jamie Hayes", "David Stutz", "Ilia Shumailov"], "title": "Soft Instruction De-escalation Defense", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment; this makes them susceptible to\nprompt injections when dealing with untrusted data. To overcome this\nlimitation, we propose SIC (Soft Instruction Control)-a simple yet effective\niterative prompt sanitization loop designed for tool-augmented LLM agents. Our\nmethod repeatedly inspects incoming data for instructions that could compromise\nagent behavior. If such content is found, the malicious content is rewritten,\nmasked, or removed, and the result is re-evaluated. The process continues until\nthe input is clean or a maximum iteration limit is reached; if imperative\ninstruction-like content remains, the agent halts to ensure security. By\nallowing multiple passes, our approach acknowledges that individual rewrites\nmay fail but enables the system to catch and correct missed injections in later\nsteps. Although immediately useful, worst-case analysis shows that SIC is not\ninfallible; strong adversary can still get a 15% ASR by embedding\nnon-imperative workflows. This nonetheless raises the bar."}
{"id": "2510.21124", "pdf": "https://arxiv.org/pdf/2510.21124", "abs": "https://arxiv.org/abs/2510.21124", "authors": ["Jie Zhang", "Xiaohong Li", "Mengke Zhang", "Ruitao Feng", "Shanshan Xu", "Zhe Hou", "Guangdong Bai"], "title": "QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute", "categories": ["cs.CR"], "comment": "17 pages, 10 figures", "summary": "Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a\ndecentralized paradigm for secure data governance but faces two inherent\nchallenges: the transparency of blockchain ledgers threatens user privacy by\nenabling reidentification attacks through attribute analysis, while the\ncomputational complexity of policy matching clashes with blockchain's\nperformance constraints. Existing solutions, such as those employing\nZero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable\nanonymity guarantees, while efficiency optimizations frequently ignore privacy\nimplications. To address these dual challenges, this paper proposes QAEBAC\n(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with\nAttribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically\nquantify the re-identification risk of users based on their access attributes\nand history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that\noptimizes policy structure based on realtime anonymity metrics, drastically\nreducing policy matching complexity. Implemented and evaluated on Hyperledger\nFabric, QAE-BAC demonstrates a superior balance between privacy and\nperformance. Experimental results show that it effectively mitigates\nre-identification risks and outperforms state-of-the-art baselines, achieving\nup to an 11x improvement in throughput and an 87% reduction in latency, proving\nits practicality for privacy-sensitive decentralized applications."}
{"id": "2510.21133", "pdf": "https://arxiv.org/pdf/2510.21133", "abs": "https://arxiv.org/abs/2510.21133", "authors": ["Divyanshu Kumar", "Nitin Aravind Birur", "Tanay Baswa", "Sahil Agarwal", "Prashanth Harshangi"], "title": "Quantifying CBRN Risk in Frontier Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Frontier Large Language Models (LLMs) pose unprecedented dual-use risks\nthrough the potential proliferation of chemical, biological, radiological, and\nnuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation\nof 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and\na 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier\nattack methodology. Our findings expose critical safety vulnerabilities: Deep\nInception attacks achieve 86.0\\% success versus 33.8\\% for direct requests,\ndemonstrating superficial filtering mechanisms; Model safety performance varies\ndramatically from 2\\% (claude-opus-4) to 96\\% (mistral-small-latest) attack\nsuccess rates; and eight models exceed 70\\% vulnerability when asked to enhance\ndangerous material properties. We identify fundamental brittleness in current\nsafety alignment, where simple prompt engineering techniques bypass safeguards\nfor dangerous CBRN information. These results challenge industry safety claims\nand highlight urgent needs for standardized evaluation frameworks, transparent\nsafety metrics, and more robust alignment techniques to mitigate catastrophic\nmisuse risks while preserving beneficial capabilities."}
{"id": "2510.21189", "pdf": "https://arxiv.org/pdf/2510.21189", "abs": "https://arxiv.org/abs/2510.21189", "authors": ["Yukun Jiang", "Mingjie Li", "Michael Backes", "Yang Zhang"], "title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency", "categories": ["cs.CR"], "comment": "Accepted in NeurIPS 2025", "summary": "Despite their superior performance on a wide range of domains, large language\nmodels (LLMs) remain vulnerable to misuse for generating harmful content, a\nrisk that has been further amplified by various jailbreak attacks. Existing\njailbreak attacks mainly follow sequential logic, where LLMs understand and\nanswer each given task one by one. However, concurrency, a natural extension of\nthe sequential scenario, has been largely overlooked. In this work, we first\npropose a word-level method to enable task concurrency in LLMs, where adjacent\nwords encode divergent intents. Although LLMs maintain strong utility in\nanswering concurrent tasks, which is demonstrated by our evaluations on\nmathematical and general question-answering benchmarks, we notably observe that\ncombining a harmful task with a benign one significantly reduces the\nprobability of it being filtered by the guardrail, showing the potential risks\nassociated with concurrency in LLMs. Based on these findings, we introduce\n$\\texttt{JAIL-CON}$, an iterative attack framework that\n$\\underline{\\text{JAIL}}$breaks LLMs via task $\\underline{\\text{CON}}$currency.\nExperiments on widely-used LLMs demonstrate the strong jailbreak capabilities\nof $\\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the\nguardrail is applied as a defense, compared to the sequential answers generated\nby previous attacks, the concurrent answers in our $\\texttt{JAIL-CON}$ exhibit\ngreater stealthiness and are less detectable by the guardrail, highlighting the\nunique feature of task concurrency in jailbreaking LLMs."}
{"id": "2510.21190", "pdf": "https://arxiv.org/pdf/2510.21190", "abs": "https://arxiv.org/abs/2510.21190", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long", "Kwok Yan Lam"], "title": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning", "categories": ["cs.CR"], "comment": "under review", "summary": "Large Language Models (LLMs) have advanced rapidly and now encode extensive\nworld knowledge. Despite safety fine-tuning, however, they remain susceptible\nto adversarial prompts that elicit harmful content. Existing jailbreak\ntechniques fall into two categories: white-box methods (e.g., gradient-based\napproaches such as GCG), which require model internals and are infeasible for\nclosed-source APIs, and black-box methods that rely on attacker LLMs to search\nor mutate prompts but often produce templates that lack explainability and\ntransferability. We introduce TrojFill, a black-box jailbreak that reframes\nunsafe instruction as a template-filling task. TrojFill embeds obfuscated\nharmful instructions (e.g., via placeholder substitution or Caesar/Base64\nencoding) inside a multi-part template that asks the model to (1) reason why\nthe original instruction is unsafe (unsafety reasoning) and (2) generate a\ndetailed example of the requested text, followed by a sentence-by-sentence\nanalysis. The crucial \"example\" component acts as a Trojan Horse that contains\nthe target jailbreak content while the surrounding task framing reduces refusal\nrates. We evaluate TrojFill on standard jailbreak benchmarks across leading\nLLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical\nperformance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,\nand 97% on GPT-4o). Moreover, the generated prompts exhibit improved\ninterpretability and transferability compared with prior black-box optimization\napproaches. We release our code, sample prompts, and generated outputs to\nsupport future red-teaming research."}
{"id": "2510.21214", "pdf": "https://arxiv.org/pdf/2510.21214", "abs": "https://arxiv.org/abs/2510.21214", "authors": ["Xingwei Zhong", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses", "categories": ["cs.CR"], "comment": null, "summary": "Multimodal large language models (MLLMs) comprise of both visual and textual\nmodalities to process vision language tasks. However, MLLMs are vulnerable to\nsecurity-related issues, such as jailbreak attacks that alter the model's input\nto induce unauthorized or harmful responses. The incorporation of the\nadditional visual modality introduces new dimensions to security threats. In\nthis paper, we proposed a black-box jailbreak method via both text and image\nprompts to evaluate MLLMs. In particular, we designed text prompts with\nprovocative instructions, along with image prompts that introduced mutation and\nmulti-image capabilities. To strengthen the evaluation, we also designed a\nRe-attack strategy. Empirical results show that our proposed work can improve\ncapabilities to assess the security of both open-source and closed-source\nMLLMs. With that, we identified gaps in existing defense methods to propose new\nstrategies for both training-time and inference-time defense methods, and\nevaluated them across the new jailbreak methods. The experiment results showed\nthat the re-designed defense methods improved protections against the jailbreak\nattacks."}
{"id": "2510.21236", "pdf": "https://arxiv.org/pdf/2510.21236", "abs": "https://arxiv.org/abs/2510.21236", "authors": ["Christoph BÃ¼hler", "Matteo Biagiola", "Luca Di Grazia", "Guido Salvaneschi"], "title": "Securing AI Agent Execution", "categories": ["cs.CR", "cs.AI", "cs.SE", "D.2.0"], "comment": null, "summary": "Large Language Models (LLMs) have evolved into AI agents that interact with\nexternal tools and environments to perform complex tasks. The Model Context\nProtocol (MCP) has become the de facto standard for connecting agents with such\nresources, but security has lagged behind: thousands of MCP servers execute\nwith unrestricted access to host systems, creating a broad attack surface. In\nthis paper, we introduce AgentBound, the first access control framework for MCP\nservers. AgentBound combines a declarative policy mechanism, inspired by the\nAndroid permission model, with a policy enforcement engine that contains\nmalicious behavior without requiring MCP server modifications. We build a\ndataset containing the 296 most popular MCP servers, and show that access\ncontrol policies can be generated automatically from source code with 80.9%\naccuracy. We also show that AgentBound blocks the majority of security threats\nin several malicious MCP servers, and that policy enforcement engine introduces\nnegligible overhead. Our contributions provide developers and project managers\nwith a practical foundation for securing MCP servers while maintaining\nproductivity, enabling researchers and tool builders to explore new directions\nfor declarative access control and MCP security."}
{"id": "2510.21246", "pdf": "https://arxiv.org/pdf/2510.21246", "abs": "https://arxiv.org/abs/2510.21246", "authors": ["Michael KÃ¼lper", "Jan-Niclas Hilgert", "Frank Breitinger", "Martin Lambertz"], "title": "What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions", "categories": ["cs.CR"], "comment": null, "summary": "Self-hosted cloud storage platforms like Nextcloud are gaining popularity\namong individuals and organizations seeking greater control over their data.\nHowever, this shift introduces new challenges for digital forensic\ninvestigations, particularly in systematically analyzing both client and server\ncomponents. Despite Nextcloud's widespread use, it has received limited\nattention in forensic research. In this work, we critically examine existing\ncloud storage forensic frameworks and highlight their limitations. To address\nthe gaps, we propose an extended forensic framework that incorporates device\nmonitoring and leverages cloud APIs for structured, repeatable evidence\nacquisition. Using Nextcloud as a case study, we demonstrate how its native\nAPIs can be used to reliably access forensic artifacts, and we introduce an\nopen-source acquisition tool that implements this approach. Our framework\nequips investigators with a more flexible method for analyzing self-hosted\ncloud storage systems, and offers a foundation for further development in this\nevolving area of digital forensics."}
{"id": "2510.21272", "pdf": "https://arxiv.org/pdf/2510.21272", "abs": "https://arxiv.org/abs/2510.21272", "authors": ["Lu Liu", "Wuqi Zhang", "Lili Wei", "Hao Guan", "Yongqiang Tian", "Yepang Liu"], "title": "LLM-Powered Detection of Price Manipulation in DeFi", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Decentralized Finance (DeFi) smart contracts manage billions of dollars,\nmaking them a prime target for exploits. Price manipulation vulnerabilities,\noften via flash loans, are a devastating class of attacks causing significant\nfinancial losses. Existing detection methods are limited. Reactive approaches\nanalyze attacks only after they occur, while proactive static analysis tools\nrely on rigid, predefined heuristics, limiting adaptability. Both depend on\nknown attack patterns, failing to identify novel variants or comprehend complex\neconomic logic. We propose PMDetector, a hybrid framework combining static\nanalysis with Large Language Model (LLM)-based reasoning to proactively detect\nprice manipulation vulnerabilities. Our approach uses a formal attack model and\na three-stage pipeline. First, static taint analysis identifies potentially\nvulnerable code paths. Second, a two-stage LLM process filters paths by\nanalyzing defenses and then simulates attacks to evaluate exploitability.\nFinally, a static analysis checker validates LLM results, retaining only\nhigh-risk paths and generating comprehensive vulnerability reports. To evaluate\nits effectiveness, we built a dataset of 73 real-world vulnerable and 288\nbenign DeFi protocols. Results show PMDetector achieves 88% precision and 90%\nrecall with Gemini 2.5-flash, significantly outperforming state-of-the-art\nstatic analysis and LLM-based approaches. Auditing a vulnerability with\nPMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an\nefficient and cost-effective alternative to manual audits."}
{"id": "2510.21353", "pdf": "https://arxiv.org/pdf/2510.21353", "abs": "https://arxiv.org/abs/2510.21353", "authors": ["Aditya Mitra", "Sibi Chakkaravarthy Sethuraman"], "title": "The Qey: Implementation and performance study of post quantum cryptography in FIDO2", "categories": ["cs.CR", "cs.ET"], "comment": null, "summary": "Authentication systems have evolved a lot since the 1960s when Fernando\nCorbato first proposed the password-based authentication. In 2013, the FIDO\nAlliance proposed using secure hardware for authentication, thus marking a\nmilestone in the passwordless authentication era [1]. Passwordless\nauthentication with a possession-based factor often relied on hardware-backed\ncryptographic methods. FIDO2 being one an amalgamation of the W3C Web\nAuthentication and FIDO Alliance Client to Authenticator Protocol is an\nindustry standard for secure passwordless authentication with rising adoption\nfor the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256\n(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature\nalgorithms. This makes it insecure against attacks involving large-scale\nquantum computers [3]. This study aims at exploring the usability of Module\nLattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium\nas a post quantum cryptographic signature standard for FIDO2. The paper\nhighlights the performance and security in comparison to keys with classical\nalgorithms."}
{"id": "2510.21401", "pdf": "https://arxiv.org/pdf/2510.21401", "abs": "https://arxiv.org/abs/2510.21401", "authors": ["Mojtaba Eshghie", "Gabriele Morello", "Matteo Lauretano", "Alexandre Bartel", "Martin Monperrus"], "title": "FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Smart contract vulnerabilities cost billions of dollars annually, yet\nexisting automated analysis tools fail to generate deployable defenses. We\npresent FLAMES, a novel automated approach that synthesizes executable runtime\nguards as Solidity \"require\" statements to harden smart contracts against\nexploits. Unlike prior work that relies on vulnerability labels, symbolic\nanalysis, or natural language specifications, FLAMES employs domain-adapted\nlarge language models trained through fill-in-the-middle supervised fine-tuning\non real-world invariants extracted from 514,506 verified contracts. Our\nextensive evaluation across three dimensions demonstrates FLAMES's\neffectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for\nsynthesized invariant (2) Semantic Quality: on a curated test set of 5,000\nchallenging invariants, FLAMES produces exact or semantically equivalent\nmatches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES\nprevents 22 out of 108 real exploits (20.4%) while preserving contract\nfunctionality, and (4) FLAMES successfully blocks the real-world APEMAGA\nincident by synthesizing a pre-condition that mitigates the attack. FLAMES\nestablishes that domain-adapted LLMs can automatically generate\nproduction-ready security defenses for smart contracts without requiring\nvulnerability detection, formal specifications, or human intervention. We\nrelease our code, model weights, datasets, and evaluation infrastructure to\nenable reproducible research in this critical domain."}
{"id": "2510.21459", "pdf": "https://arxiv.org/pdf/2510.21459", "abs": "https://arxiv.org/abs/2510.21459", "authors": ["Adetayo Adebimpe", "Helmut Neukirchen", "Thomas Welsh"], "title": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5; D.4.6; I.2.7"], "comment": "to be published in: The 3rd International Conference on Foundation\n  and Large Language Models (FLLM2025), IEEE, 2025", "summary": "Honeypots are decoy systems used for gathering valuable threat intelligence\nor diverting attackers away from production systems. Maximising attacker\nengagement is essential to their utility. However research has highlighted that\ncontext-awareness, such as the ability to respond to new attack types, systems\nand attacker agents, is necessary to increase engagement. Large Language Models\n(LLMs) have been shown as one approach to increase context awareness but suffer\nfrom several challenges including accuracy and timeliness of response time,\nhigh operational costs and data-protection issues due to cloud deployment. We\npropose the System-Based Attention Shell Honeypot (SBASH) framework which\nmanages data-protection issues through the use of lightweight local LLMs. We\ninvestigate the use of Retrieval Augmented Generation (RAG) supported LLMs and\nnon-RAG LLMs for Linux shell commands and evaluate them using several different\nmetrics such as response time differences, realism from human testers, and\nsimilarity to a real system calculated with Levenshtein distance, SBert, and\nBertScore. We show that RAG improves accuracy for untuned models while models\nthat have been tuned via a system prompt that tells the LLM to respond like a\nLinux system achieve without RAG a similar accuracy as untuned with RAG, while\nhaving a slightly lower latency."}
{"id": "2510.21483", "pdf": "https://arxiv.org/pdf/2510.21483", "abs": "https://arxiv.org/abs/2510.21483", "authors": ["Pierre Guillot", "Auguste Hoang Duc", "Michel Koskas", "Florian MÃ©hats"], "title": "Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise", "categories": ["cs.CR", "math.GR", "E.3"], "comment": null, "summary": "We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic\nEncryption without the need for bootstrapping (or in other words, without\nnoise). Building on the work of Nuida and others, we achieve this using\nencodings in groups.\n  The groups are represented on a machine using rewriting systems. In this way\nthe subgroup membership problem, which an attacker would have to solve in order\nto break the scheme, becomes maximally hard, while performance is preserved. In\nfact we include a simple benchmark demonstrating that our implementation runs\nseveral orders of magnitude faster than existing standards.\n  We review many possible attacks against our protocol and explain how to\nprotect the scheme in each case."}
{"id": "2510.21601", "pdf": "https://arxiv.org/pdf/2510.21601", "abs": "https://arxiv.org/abs/2510.21601", "authors": ["Emmanuel Dare Alalade", "Ashraf Matrawy"], "title": "PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis", "categories": ["cs.CR"], "comment": "26 pages, 18 figures", "summary": "Previous studies on PTA have focused on analyzing privacy threats based on\nthe potential areas of occurrence and their likelihood of occurrence. However,\nan in-depth understanding of the threat actors involved, their actions, and the\nintentions that result in privacy threats is essential. In this paper, we\npresent a novel Privacy Threat Model Framework (PTMF) that analyzes privacy\nthreats through different phases.\n  The PTMF development is motivated through the selected tactics from the MITRE\nATT\\&CK framework and techniques from the LINDDUN privacy threat model, making\nPTMF a privacy-centered framework. The proposed PTMF can be employed in various\nways, including analyzing the activities of threat actors during privacy\nthreats and assessing privacy risks in IoT systems, among others. In this\npaper, we conducted a user study on 12 privacy threats associated with IoT by\ndeveloping a questionnaire based on PTMF and recruited experts from both\nindustry and academia in the fields of security and privacy to gather their\nopinions. The collected data were analyzed and mapped to identify the threat\nactors involved in the identification of IoT users (IU) and the remaining 11\nprivacy threats. Our observation revealed the top three threat actors and the\ncritical paths they used during the IU privacy threat, as well as the remaining\n11 privacy threats. This study could provide a solid foundation for\nunderstanding how and where privacy measures can be proactively and effectively\ndeployed in IoT systems to mitigate privacy threats based on the activities and\nintentions of threat actors within these systems."}
{"id": "2510.21684", "pdf": "https://arxiv.org/pdf/2510.21684", "abs": "https://arxiv.org/abs/2510.21684", "authors": ["Albert Cheu", "Artem Lagzdin", "Brett McLarnon", "Daniel Ramage", "Katharine Daly", "Marco Gruteser", "Peter Kairouz", "Rakshita Tandon", "Stanislav Chiknavaryan", "Timon Van Overveldt", "Zoe Gong"], "title": "Toward provably private analytics and insights into GenAI use", "categories": ["cs.CR"], "comment": null, "summary": "Large-scale systems that compute analytics over a fleet of devices must\nachieve high privacy and security standards while also meeting data quality,\nusability, and resource efficiency expectations. We present a next-generation\nfederated analytics system that uses Trusted Execution Environments (TEEs)\nbased on technologies like AMD SEV-SNP and Intel TDX to provide verifiable\nprivacy guarantees for all server-side processing. In our system, devices\nencrypt and upload data, tagging it with a limited set of allowable server-side\nprocessing steps. An open source, TEE-hosted key management service guarantees\nthat the data is accessible only to those steps, which are themselves protected\nby TEE confidentiality and integrity assurance guarantees. The system is\ndesigned for flexible workloads, including processing unstructured data with\nLLMs (for structured summarization) before aggregation into differentially\nprivate insights (with automatic parameter tuning). The transparency properties\nof our system allow any external party to verify that all raw and derived data\nis processed in TEEs, protecting it from inspection by the system operator, and\nthat differential privacy is applied to all released results. This system has\nbeen successfully deployed in production, providing helpful insights into\nreal-world GenAI experiences."}
{"id": "2510.21946", "pdf": "https://arxiv.org/pdf/2510.21946", "abs": "https://arxiv.org/abs/2510.21946", "authors": ["Kieu Dang", "Phung Lai", "NhatHai Phan", "Yelong Shen", "Ruoming Jin", "Abdallah Khreishah"], "title": "$Î´$-STEAL: LLM Stealing Attack with Local Differential Privacy", "categories": ["cs.CR", "68T07, 68T50", "I.2.6; I.2.7; K.6.5"], "comment": "Accepted at ACML 2025 (PMLR W&CP). Code:\n  https://github.com/kirudang/LDP_Stealing_Attack", "summary": "Large language models (LLMs) demonstrate remarkable capabilities across\nvarious tasks. However, their deployment introduces significant risks related\nto intellectual property. In this context, we focus on model stealing attacks,\nwhere adversaries replicate the behaviors of these models to steal services.\nThese attacks are highly relevant to proprietary LLMs and pose serious threats\nto revenue and financial stability. To mitigate these risks, the watermarking\nsolution embeds imperceptible patterns in LLM outputs, enabling model\ntraceability and intellectual property verification. In this paper, we study\nthe vulnerability of LLM service providers by introducing $\\delta$-STEAL, a\nnovel model stealing attack that bypasses the service provider's watermark\ndetectors while preserving the adversary's model utility. $\\delta$-STEAL\ninjects noise into the token embeddings of the adversary's model during\nfine-tuning in a way that satisfies local differential privacy (LDP)\nguarantees. The adversary queries the service provider's model to collect\noutputs and form input-output training pairs. By applying LDP-preserving noise\nto these pairs, $\\delta$-STEAL obfuscates watermark signals, making it\ndifficult for the service provider to determine whether its outputs were used,\nthereby preventing claims of model theft. Our experiments show that\n$\\delta$-STEAL with lightweight modifications achieves attack success rates of\nup to $96.95\\%$ without significantly compromising the adversary's model\nutility. The noise scale in LDP controls the trade-off between attack\neffectiveness and model utility. This poses a significant risk, as even robust\nwatermarks can be bypassed, allowing adversaries to deceive watermark detectors\nand undermine current intellectual property protection methods."}
{"id": "2510.21957", "pdf": "https://arxiv.org/pdf/2510.21957", "abs": "https://arxiv.org/abs/2510.21957", "authors": ["Zhixin Pan", "Ziyu Shu", "Amberbir Alemayoh"], "title": "Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning", "categories": ["cs.CR", "cs.AI", "K.6.5; I.2.6"], "comment": "This paper was accepted in the 2025 IEEE International Conference on\n  Computer Design (ICCD)", "summary": "Ransomware has become a critical threat to cybersecurity due to its rapid\nevolution, the necessity for early detection, and growing diversity, posing\nsignificant challenges to traditional detection methods. While AI-based\napproaches had been proposed by prior works to assist ransomware detection,\nexisting methods suffer from three major limitations, ad-hoc feature\ndependencies, delayed response, and limited adaptability to unseen variants. In\nthis paper, we propose a framework that integrates self-supervised contrastive\nlearning with neural architecture search (NAS) to address these challenges.\nSpecifically, this paper offers three important contributions. (1) We design a\ncontrastive learning framework that incorporates hardware performance counters\n(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a\ncustomized loss function that encourages early-stage detection of malicious\nactivity, and significantly reduces the detection latency. (3) We deploy a\nneural architecture search (NAS) framework to automatically construct adaptive\nmodel architectures, allowing the detector to flexibly align with unseen\nransomware variants. Experimental results show that our proposed method\nachieves significant improvements in both detection accuracy (up to 16.1%) and\nresponse time (up to 6x) compared to existing approaches while maintaining\nrobustness under evasive attacks."}
{"id": "2510.22024", "pdf": "https://arxiv.org/pdf/2510.22024", "abs": "https://arxiv.org/abs/2510.22024", "authors": ["Evangelos Bitsikas", "Jason Veara", "Aanjhan Ranganathan"], "title": "Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla", "categories": ["cs.CR"], "comment": null, "summary": "Modern connected vehicles rely on persistent LTE connectivity to enable\nremote diagnostics, over-the-air (OTA) updates, and critical safety services.\nWhile mobile network vulnerabilities are well documented in the smartphone\necosystem, their impact in safety-critical automotive settings remains\ninsufficiently examined. In this work, we conduct a black-box, non-invasive\nsecurity analysis of LTE connectivity in Tesla vehicles, including the Model 3\nand Cybertruck, revealing systemic protocol weaknesses and architectural\nmisconfigurations. We find that Tesla's telematics stack is susceptible to IMSI\ncatching, rogue base station hijacking, and insecure fallback mechanisms that\nmay silently degrade service availability. Furthermore, legacy control-plane\nconfigurations allow for silent SMS injection and broadcast message spoofing\nwithout driver awareness. These vulnerabilities have implications beyond a\nsingle vendor as they challenge core assumptions in regulatory frameworks like\nISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient\ntelematics for type approval of modern vehicles."}
{"id": "2510.22085", "pdf": "https://arxiv.org/pdf/2510.22085", "abs": "https://arxiv.org/abs/2510.22085", "authors": ["Pavlos Ntais"], "title": "Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.0; K.6.5"], "comment": "18 pages, 5 figures", "summary": "Large language models (LLMs) remain vulnerable to sophisticated prompt\nengineering attacks that exploit contextual framing to bypass safety\nmechanisms, posing significant risks in cybersecurity applications. We\nintroduce Jailbreak Mimicry, a systematic methodology for training compact\nattacker models to automatically generate narrative-based jailbreak prompts in\na one-shot manner. Our approach transforms adversarial prompt discovery from\nmanual craftsmanship into a reproducible scientific process, enabling proactive\nvulnerability assessment in AI-driven security systems. Developed for the\nOpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient\nfine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,\nachieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out\ntest set of 200 items. Cross-model evaluation reveals significant variation in\nvulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on\nLlama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad\napplicability and model-specific defensive strengths in cybersecurity contexts.\nThis represents a 54x improvement over direct prompting (1.5% ASR) and\ndemonstrates systematic vulnerabilities in current safety alignment approaches.\nOur analysis reveals that technical domains (Cybersecurity: 93% ASR) and\ndeception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,\nhighlighting threats to AI-integrated threat detection, malware analysis, and\nsecure systems, while physical harm categories show greater resistance (55.6%\nASR). We employ automated harmfulness evaluation using Claude Sonnet 4,\ncross-validated with human expert assessment, ensuring reliable and scalable\nevaluation for cybersecurity red-teaming. Finally, we analyze failure\nmechanisms and discuss defensive strategies to mitigate these vulnerabilities\nin AI for cybersecurity."}
{"id": "2510.22100", "pdf": "https://arxiv.org/pdf/2510.22100", "abs": "https://arxiv.org/abs/2510.22100", "authors": ["Saif E. Nouma", "Attila A. Yavuz"], "title": "Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things", "categories": ["cs.CR"], "comment": null, "summary": "The Internet of Things (IoT) relies heavily on resource-limited devices to\ncommunicate critical (e.g., military data) information under low-energy\nadversarial environments and low-latency wireless channels. Authenticated\nEncryption (AE) guarantees confidentiality, authenticity, and integrity, making\nit a vital security service for IoT. However, current deployed (lightweight) AE\nstandards lack essential features like key compromise resiliency and compact\nauthentication tags, as well as performance enhancements such as offline-online\ncryptography. To address these gaps, we propose Graphene, the first (to our\nknowledge) symmetric Forward-secure and Aggregate Authenticated Encryption\n(FAAE) framework designed for the performance and security demands of low-end\nIoT infrastructures. Graphene innovates by synergizing key evolution strategies\nand offline-online cryptographic processing with Universal Message\nAuthentication Codes (UMACs) to guarantee breach-resiliency, near-optimal\nonline latency, and compactness. We demonstrate Graphene efficiency through two\ndistinct instantiations, each balancing unique performance trade-offs with\nextensibility for diverse MACs. Our experimental evaluation on commodity\nhardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant\nperformance gains over existing alternatives. Graphene is also backward\ncompatible with standard-compliant cryptographic implementations. We release\nour implementation as open source for public testing and adaptation."}
{"id": "2510.22191", "pdf": "https://arxiv.org/pdf/2510.22191", "abs": "https://arxiv.org/abs/2510.22191", "authors": ["Qi Sheng"], "title": "TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation", "categories": ["cs.CR"], "comment": null, "summary": "Provenance analysis based on system audit data has emerged as a fundamental\napproach for investigating Advanced Persistent Threat (APT) attacks. Due to the\nhigh concealment and long-term persistence of APT attacks, they are only\nrepresented as a minimal part of the critical path in the provenance graph.\nWhile existing techniques employ behavioral pattern matching and data flow\nfeature matching to uncover latent associations in attack sequences through\nprovenance graph path reasoning, their inability to establish effective attack\ncontext associations often leads to the conflation of benign system operations\nwith real attack entities, that fail to accurately characterize real APT\nbehaviors. We observe that while the causality of entities in the provenance\ngraph exhibit substantial complexity, attackers often follow specific attack\npatterns-specifically, clear combinations of tactics and techniques to achieve\ntheir goals. Based on these insights, we propose TPPR, a novel framework that\nfirst extracts anomaly subgraphs through abnormal node detection,\nTTP-annotation and graph pruning, then performs attack path reasoning using\nmined TTP sequential pattern, and finally reconstructs attack scenarios through\nconfidence-based path scoring and merging. Extensive evaluation on real\nenterprise logs (more than 100 million events) and DARPA TC dataset\ndemonstrates TPPR's capability to achieve 99.9% graph simplification (700,000\nto 20 edges) while preserving 91% of critical attack nodes, outperforming\nstate-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in\nreconstruction precision while maintaining attack scenario integrity."}
{"id": "2510.22274", "pdf": "https://arxiv.org/pdf/2510.22274", "abs": "https://arxiv.org/abs/2510.22274", "authors": ["Anum Paracha", "Junaid Arshad", "Mohamed Ben Farah", "Khalid Ismail"], "title": "SecureLearn - An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Data poisoning attacks are a potential threat to machine learning (ML)\nmodels, aiming to manipulate training datasets to disrupt their performance.\nExisting defenses are mostly designed to mitigate specific poisoning attacks or\nare aligned with particular ML algorithms. Furthermore, most defenses are\ndeveloped to secure deep neural networks or binary classifiers. However,\ntraditional multiclass classifiers need attention to be secure from data\npoisoning attacks, as these models are significant in developing multi-modal\napplications. Therefore, this paper proposes SecureLearn, a two-layer\nattack-agnostic defense to defend multiclass models from poisoning attacks. It\ncomprises two components of data sanitization and a new feature-oriented\nadversarial training. To ascertain the effectiveness of SecureLearn, we\nproposed a 3D evaluation matrix with three orthogonal dimensions: data\npoisoning attack, data sanitization and adversarial training. Benchmarking\nSecureLearn in a 3D matrix, a detailed analysis is conducted at different\npoisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,\ndetection and correction rates, and false discovery rate. The experimentation\nis conducted for four ML algorithms, namely Random Forest (RF), Decision Tree\n(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with\nthree public datasets, against three poisoning attacks and compared with two\nexisting mitigations. Our results highlight that SecureLearn is effective\nagainst the provided attacks. SecureLearn has strengthened resilience and\nadversarial robustness of traditional multiclass models and neural networks,\nconfirming its generalization beyond algorithm-specific defenses. It\nconsistently maintained accuracy above 90%, recall and F1-score above 75%. For\nneural networks, SecureLearn achieved 97% recall and F1-score against all\nselected poisoning attacks."}
{"id": "2510.22283", "pdf": "https://arxiv.org/pdf/2510.22283", "abs": "https://arxiv.org/abs/2510.22283", "authors": ["Devon A. Kelly", "Christiana Chamon"], "title": "Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY", "physics.app-ph"], "comment": null, "summary": "Wide-bandgap (WBG) technologies offer unprecedented improvements in power\nsystem efficiency, size, and performance, but also introduce unique sensor\ncorruption and cybersecurity risks in industrial control systems (ICS),\nparticularly due to high-frequency noise and sophisticated cyber-physical\nthreats. This proof-of-concept (PoC) study demonstrates the adaptation of a\nnoise-driven physically unclonable function (PUF) and machine learning\n(ML)-assisted anomaly detection framework to the demanding environment of\nWBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG\nswitching noise (up to 100 kHz) as a PUF source, and simultaneously using this\nnoise as a real-time threat indicator, the proposed system unites\nhardware-level authentication and anomaly detection. Our approach integrates\nhybrid machine learning (ML) models with adaptive Bayesian filtering, providing\nrobust and low-latency detection capabilities resilient to both natural\nelectromagnetic interference (EMI) and active adversarial manipulation. Through\ndetailed simulations of WBG modules under benign and attack\nscenarios--including EMI injection, signal tampering, and node\nimpersonation--we achieve 95% detection accuracy and sub-millisecond processing\nlatency. These results demonstrate the feasibility of physics-driven, dual-use\nnoise exploitation as a scalable ICS defense primitive. Our findings lay the\ngroundwork for next-generation security strategies that leverage inherent\ndevice characteristics, bridging hardware and artificial intelligence (AI) for\nenhanced protection of critical ICS infrastructure."}
{"id": "2510.22300", "pdf": "https://arxiv.org/pdf/2510.22300", "abs": "https://arxiv.org/abs/2510.22300", "authors": ["Chenyu Zhang", "Tairen Zhang", "Lanjun Wang", "Ruidong Chen", "Wenhui Li", "Anan Liu"], "title": "T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "AAAI under review", "summary": "Using risky text prompts, such as pornography and violent prompts, to test\nthe safety of text-to-image (T2I) models is a critical task. However, existing\nrisky prompt datasets are limited in three key areas: 1) limited risky\ncategories, 2) coarse-grained annotation, and 3) low effectiveness. To address\nthese limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark\ndesigned for evaluating safety-related tasks in T2I models. Specifically, we\nfirst develop a hierarchical risk taxonomy, which consists of 6 primary\ncategories and 14 fine-grained subcategories. Building upon this taxonomy, we\nconstruct a pipeline to collect and annotate risky prompts. Finally, we obtain\n6,432 effective risky prompts, where each prompt is annotated with both\nhierarchical category labels and detailed risk reasons. Moreover, to facilitate\nthe evaluation, we propose a reason-driven risky image detection method that\nexplicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,\nwe conduct a comprehensive evaluation of eight T2I models, nine defense\nmethods, five safety filters, and five attack strategies, offering nine key\ninsights into the strengths and limitations of T2I model safety. Finally, we\ndiscuss potential applications of T2I-RiskyPrompt across various research\nfields. The dataset and code are provided in\nhttps://github.com/datar001/T2I-RiskyPrompt."}
{"id": "2510.22387", "pdf": "https://arxiv.org/pdf/2510.22387", "abs": "https://arxiv.org/abs/2510.22387", "authors": ["Nader Nemati"], "title": "Privacy-Aware Federated nnU-Net for ECG Page Digitization", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks can convert ECG page images into analyzable waveforms,\nyet centralized training often conflicts with cross-institutional privacy and\ndeployment constraints. A cross-silo federated digitization framework is\npresented that trains a full-model nnU-Net segmentation backbone without\nsharing images and aggregates updates across sites under realistic non-IID\nheterogeneity (layout, grid style, scanner profile, noise).\n  The protocol integrates three standard server-side aggregators--FedAvg,\nFedProx, and FedAdam--and couples secure aggregation with central, user-level\ndifferential privacy to align utility with formal guarantees. Key features\ninclude: (i) end-to-end full-model training and synchronization across clients;\n(ii) secure aggregation so the server only observes a clipped, weighted sum\nonce a participation threshold is met; (iii) central Gaussian DP with Renyi\naccounting applied post-aggregation for auditable user-level privacy; and (iv)\na calibration-aware digitization pipeline comprising page normalization, trace\nsegmentation, grid-leakage suppression, and vectorization to twelve-lead\nsignals.\n  Experiments on ECG pages rendered from PTB-XL show consistently faster\nconvergence and higher late-round plateaus with adaptive server updates\n(FedAdam) relative to FedAvg and FedProx, while approaching centralized\nperformance. The privacy mechanism maintains competitive accuracy while\npreventing exposure of raw images or per-client updates, yielding deployable,\nauditable guarantees suitable for multi-institution settings."}
{"id": "2510.22396", "pdf": "https://arxiv.org/pdf/2510.22396", "abs": "https://arxiv.org/abs/2510.22396", "authors": ["Zhaoyang Li", "Zheng Yu", "Jingyi Song", "Meng Xu", "Yuxuan Luo", "Dongliang Mu"], "title": "PortGPT: Towards Automated Backporting Using Large Language Models", "categories": ["cs.CR"], "comment": "Accepted by IEEE S&P 2026", "summary": "Patch backporting, the process of migrating mainline security patches to\nolder branches, is an essential task in maintaining popular open-source\nprojects (e.g., Linux kernel). However, manual backporting can be\nlabor-intensive, while existing automated methods, which heavily rely on\npredefined syntax or semantic rules, often lack agility for complex patches.\n  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation\nof patch backporting in real-world scenarios. PORTGPT enhances an LLM with\ntools to access code on-demand, summarize Git history, and revise patches\nautonomously based on feedback (e.g., from compilers), hence, simulating\nhuman-like reasoning and verification. PORTGPT achieved an 89.15% success rate\non existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex\ncases, both outperforms state-of-the-art of backporting tools. We contributed 9\nbackported patches from PORTGPT to the Linux kernel community and all patches\nare now merged."}
{"id": "2510.22400", "pdf": "https://arxiv.org/pdf/2510.22400", "abs": "https://arxiv.org/abs/2510.22400", "authors": ["Fei Shao", "Jia Zou", "Zhichao Cao", "Xusheng Xiao"], "title": "ProGQL: A Provenance Graph Query System for Cyber Attack Investigation", "categories": ["cs.CR", "cs.DB"], "comment": null, "summary": "Provenance analysis (PA) has recently emerged as an important solution for\ncyber attack investigation. PA leverages system monitoring to monitor system\nactivities as a series of system audit events and organizes these events as a\nprovenance graph to show the dependencies among system activities, which can\nreveal steps of cyber attacks. Despite their potential, existing PA techniques\nface two critical challenges: (1) they are inflexible and non-extensible,\nmaking it difficult to incorporate analyst expertise, and (2) they are memory\ninefficient, often requiring>100GB of RAM to hold entire event streams, which\nfundamentally limits scalability and deployment in real-world environments. To\naddress these limitations, we propose the PROGQL framework, which provides a\ndomain-specific graph search language with a well-engineered query engine,\nallowing PA over system audit events and expert knowledge to be jointly\nexpressed as a graph search query and thereby facilitating the investigation of\ncomplex cyberattacks. In particular, to support dependency searches from a\nstarting edge required in PA, PROGQL introduces new language constructs for\nconstrained graph traversal, edge weight computation, value propagation along\nweighted edges, and graph merging to integrate multiple searches. Moreover, the\nPROGQL query engine is optimized for efficient incremental graph search across\nheterogeneous database backends, eliminating the need for full in-memory\nmaterialization and reducing memory overhead. Our evaluations on real attacks\ndemonstrate the effectiveness of the PROGQL language in expressing a diverse\nset of complex attacks compared with the state-of-the-art graph query language\nCypher, and the comparison with the SOTA PA technique DEPIMPACT further\ndemonstrates the significant improvement of the scalability brought by our\nPROGQL framework's design."}
{"id": "2510.22536", "pdf": "https://arxiv.org/pdf/2510.22536", "abs": "https://arxiv.org/abs/2510.22536", "authors": ["Jotaro Yano"], "title": "ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole", "categories": ["cs.CR"], "comment": null, "summary": "We formalize a cross-domain \"ZK coprocessor bridge\" that lets Solana programs\nrequest private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable\nAction Approvals (VAAs) as authenticated transport. The system comprises: (i) a\nSolana program that posts messages to Wormhole Core with explicit finality;\n(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound\npayload secretHash||m from the attested VAA, derives a domain-separated field\ncommitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference\nimplementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we\nprovide migration guidance to the payload-bound interface); (iii) a minimal\nAztec contract that consumes the message privately; and (iv) an off-chain\nrelayer that ferries VAAs and can record receipts on Solana. We present state\nmachines, message formats, and proof sketches for replay-safety, origin\nauthenticity, finality alignment, parameter binding (no relayer front-running\nof Aztec parameters), privacy, idempotence, and liveness. Finally, we include a\nconcise Reproducibility note with pinned versions and artifacts to replicate a\npublic testnet run."}
{"id": "2510.22555", "pdf": "https://arxiv.org/pdf/2510.22555", "abs": "https://arxiv.org/abs/2510.22555", "authors": ["Dongyi Liu", "Jiangtong Li", "Dawei Cheng", "Changjun Jiang"], "title": "Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where\nadversaries implant malicious triggers to manipulate model predictions.\n  Existing trigger generators are often simplistic in structure and overly\nreliant on specific features, confining them to a single graph learning\nparadigm, such as graph supervised learning, graph contrastive learning, or\ngraph prompt learning.\n  This specialized design, which aligns the trigger with one learning\nobjective, results in poor transferability when applied to other learning\nparadigms.\n  For instance, triggers generated for the graph supervised learning paradigm\nperform poorly when tested within graph contrastive learning or graph prompt\nlearning environments.\n  Furthermore, these simple generators often fail to utilize complex structural\ninformation or node diversity within the graph data.\n  These constraints limit the attack success rates of such methods in general\ntesting scenarios.\n  Therefore, to address these limitations, we propose Cross-Paradigm Graph\nBackdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable\ngraph backdoor attack that employs graph prompt learning(GPL) to train a set of\nuniversal subgraph triggers.\n  First, we distill a compact yet expressive trigger set from target graphs,\nwhich is structured as a queryable repository, by jointly enforcing\nclass-awareness, feature richness, and structural fidelity.\n  Second, we conduct the first exploration of the theoretical transferability\nof GPL to train these triggers under prompt-based objectives, enabling\neffective generalization to diverse and unseen test-time paradigms.\n  Extensive experiments across multiple real-world datasets and defense\nscenarios show that CP-GBA achieves state-of-the-art attack success rates."}
{"id": "2510.22561", "pdf": "https://arxiv.org/pdf/2510.22561", "abs": "https://arxiv.org/abs/2510.22561", "authors": ["Kaveri Banerjee", "Sajal Saha"], "title": "Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study", "categories": ["cs.CR", "cs.AI"], "comment": "13 Pages, 2 Figures", "summary": "Blockchain systems rely on decentralized ledgers and strong security\nguarantees. A key requirement is non-repudiation, which prevents denial of\ntransaction authorship and supports integrity of recorded data. This work\nsurveys digital signature schemes used in blockchain platforms and analyzes how\nthey deliver non-repudiation and contribute to overall system security. We\nexamine representative scheme families and their cryptographic foundations,\nsecurity assumptions, and properties relevant to deployment, including\nunforgeability, resistance to malleability, support for aggregation and\nmultisignature or threshold settings, key and signature sizes, and verification\ncost. Using these criteria, we compare the suitability of different designs for\nconsensus protocols, smart contract constraints, and resource limits. We\nhighlight practical tradeoffs that affect throughput, storage, scalability, and\nattack surfaces, and summarize benefits and limitations of each scheme in\nblockchain contexts. The study underscores that carefully chosen digital\nsignatures are central to achieving non-repudiation and preserving information\nintegrity, and it outlines implementation considerations and open directions\nsuch as interoperability and post-quantum readiness."}
{"id": "2510.22566", "pdf": "https://arxiv.org/pdf/2510.22566", "abs": "https://arxiv.org/abs/2510.22566", "authors": ["Md. Mehedi Hasan"], "title": "FAARM: Firmware Attestation and Authentication Framework for Mali GPUs", "categories": ["cs.CR"], "comment": "10 pages, 8 figures. Preprint version under review in the area of\n  Computer Security (cs.CR)", "summary": "Recent work has revealed MOLE, the first practical attack to compromise GPU\nTrusted Execution Environments (TEEs), by injecting malicious firmware into the\nembedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence\nof cryptographic verification during initialization, adversaries with kernel\nprivileges can bypass memory protections, exfiltrate sensitive data at over 40\nMB/s, and tamper with inference results, all with negligible runtime overhead.\nThis attack surface affects commodity mobile SoCs and cloud accelerators,\nexposing a critical firmware-level trust gap in existing GPU TEE designs. To\naddress this gap, this paper presents FAARM, a lightweight Firmware Attestation\nand Authentication framework that prevents MOLE-style firmware subversion.\nFAARM integrates digital signature verification at the EL3 secure monitor using\nvendor-signed firmware bundles and an on-device public key anchor. At boot, EL3\nverifies firmware integrity and authenticity, enforces version checks, and\nlocks the firmware region, eliminating both pre-verification and\ntime-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a\nsoftware-only prototype on a Mali GPU testbed, using a Google Colab-based\nemulation framework that models the firmware signing process, the EL1 to EL3\nload path, and secure memory configuration. FAARM reliably detects and blocks\nmalicious firmware injections, rejecting tampered images before use and denying\noverwrite attempts after attestation. Firmware verification incurs only 1.34 ms\nlatency on average, demonstrating that strong security can be achieved with\nnegligible overhead. FAARM thus closes a fundamental gap in shim-based GPU\nTEEs, providing a practical, deployable defense that raises the security\nbaseline for both mobile and cloud GPU deployments."}
{"id": "2510.22620", "pdf": "https://arxiv.org/pdf/2510.22620", "abs": "https://arxiv.org/abs/2510.22620", "authors": ["Julia Bazinska", "Max Mathys", "Francesco Casucci", "Mateo Rojas-Carulla", "Xander Davies", "Alexandra Souly", "Niklas Pfister"], "title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Julia Bazinska and Max Mathys contributed equally", "summary": "AI agents powered by large language models (LLMs) are being deployed at\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\naffects agent security. The non-deterministic sequential nature of AI agents\ncomplicates security modeling, while the integration of traditional software\nwith AI components entangles novel LLM vulnerabilities with conventional\nsecurity risks. Existing frameworks only partially address these challenges as\nthey either capture specific vulnerabilities only or require modeling of\ncomplete agents. To address these limitations, we introduce threat snapshots: a\nframework that isolates specific states in an agent's execution flow where LLM\nvulnerabilities manifest, enabling the systematic identification and\ncategorization of security risks that propagate from the LLM to the agent\nlevel. We apply this framework to construct the $\\operatorname{b}^3$ benchmark,\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\nenhanced reasoning capabilities improve security, while model size does not\ncorrelate with security. We release our benchmark, dataset, and evaluation code\nto facilitate widespread adoption by LLM providers and practitioners, offering\nguidance for agent developers and incentivizing model developers to prioritize\nbackbone security improvements."}
{"id": "2510.22622", "pdf": "https://arxiv.org/pdf/2510.22622", "abs": "https://arxiv.org/abs/2510.22622", "authors": ["Kangran Zhao", "Yupeng Chen", "Xiaoyu Zhang", "Yize Chen", "Weinan Guan", "Baicheng Chen", "Chengzhe Sun", "Soumyya Kanti Datta", "Qingshan Liu", "Siwei Lyu", "Baoyuan Wu"], "title": "DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection", "categories": ["cs.CR", "cs.CV", "cs.MM"], "comment": "Preprint", "summary": "The misuse of advanced generative AI models has resulted in the widespread\nproliferation of falsified data, particularly forged human-centric audiovisual\ncontent, which poses substantial societal risks (e.g., financial fraud and\nsocial instability). In response to this growing threat, several works have\npreliminarily explored countermeasures. However, the lack of sufficient and\ndiverse training data, along with the absence of a standardized benchmark,\nhinder deeper exploration. To address this challenge, we first build Mega-MMDF,\na large-scale, diverse, and high-quality dataset for multimodal deepfake\ndetection. Specifically, we employ 21 forgery pipelines through the combination\nof 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face\nreenactment methods. Mega-MMDF currently contains 0.1 million real samples and\n1.1 million forged samples, making it one of the largest and most diverse\nmultimodal deepfake datasets, with plans for continuous expansion. Building on\nit, we present DeepfakeBench-MM, the first unified benchmark for multimodal\ndeepfake detection. It establishes standardized protocols across the entire\ndetection pipeline and serves as a versatile platform for evaluating existing\nmethods as well as exploring novel approaches. DeepfakeBench-MM currently\nsupports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our\ncomprehensive evaluations and in-depth analyses uncover several key findings\nfrom multiple perspectives (e.g., augmentation, stacked forgery). We believe\nthat DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as\nfoundational infrastructures for advancing multimodal deepfake detection."}
{"id": "2510.22628", "pdf": "https://arxiv.org/pdf/2510.22628", "abs": "https://arxiv.org/abs/2510.22628", "authors": ["Md. Mehedi Hasan", "Ziaur Rahman", "Rafid Mostafiz", "Md. Abir Hossain"], "title": "Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 5 figures. Preprint version under review in the area of\n  Artificial Intelligence (cs.AI)", "summary": "This paper presents a real-time modular defense system named Sentra-Guard.\nThe system detects and mitigates jailbreak and prompt injection attacks\ntargeting large language models (LLMs). The framework uses a hybrid\narchitecture with FAISS-indexed SBERT embedding representations that capture\nthe semantic meaning of prompts, combined with fine-tuned transformer\nclassifiers, which are machine learning models specialized for distinguishing\nbetween benign and adversarial language inputs. It identifies adversarial\nprompts in both direct and obfuscated attack vectors. A core innovation is the\nclassifier-retriever fusion module, which dynamically computes context-aware\nrisk scores that estimate how likely a prompt is to be adversarial based on its\ncontent and context. The framework ensures multilingual resilience with a\nlanguage-agnostic preprocessing layer. This component automatically translates\nnon-English prompts into English for semantic evaluation, enabling consistent\ndetection across over 100 languages. The system includes a HITL feedback loop,\nwhere decisions made by the automated system are reviewed by human experts for\ncontinual learning and rapid adaptation under adversarial pressure.\nSentra-Guard maintains an evolving dual-labeled knowledge base of benign and\nmalicious prompts, enhancing detection reliability and reducing false\npositives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =\n1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading\nbaselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike\nblack-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible\nwith diverse LLM backends. Its modular design supports scalable deployment in\nboth commercial and open-source environments. The system establishes a new\nstate-of-the-art in adversarial LLM defense."}
{"id": "2510.22661", "pdf": "https://arxiv.org/pdf/2510.22661", "abs": "https://arxiv.org/abs/2510.22661", "authors": ["Malik Imran", "Safiullah Khan", "Zain Ul Abideen", "Ciara Rafferty", "Ayesha Khalid", "Muhammad Rashid", "Maire O'Neill"], "title": "RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography", "categories": ["cs.CR", "cs.AR"], "comment": "6 pages, 1 figure, conference", "summary": "Post-quantum multivariate public key cryptography (MPKC) schemes resist\nquantum threats but require heavy operations, such as rejection sampling, which\nchallenge resource-limited devices. Prior hardware designs have addressed\nvarious aspects of MPKC signature generation. However, rejection sampling\nremains largely unexplored in such contexts. This paper presents RejSCore, a\nlightweight hardware accelerator for rejection sampling in post-quantum\ncryptography. It specifically targets the QR-UOV scheme, which is a prominent\ncandidate under the second-round of the National Institute of Standards and\nTechnology (NIST) additional digital signature standardization process. The\narchitecture includes an AES-CTR-128-based pseudorandom number generator.\nMoreover, a lightweight iterative method is employed in rejection sampling,\noffering reduced resource consumption and area overhead while slightly\nincreasing latency. The performance of RejSCore is comprehensively evaluated on\nArtix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and\nPower-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area\nof 2042 slices and 464,866~$\\mu m^2$, with operating frequencies of 222 MHz and\n565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =\n127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525\nclock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for\ndeployment in resource-constrained and security-critical environments."}
{"id": "2510.22726", "pdf": "https://arxiv.org/pdf/2510.22726", "abs": "https://arxiv.org/abs/2510.22726", "authors": ["Van Le", "Tan Le"], "title": "SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking", "categories": ["cs.CR"], "comment": null, "summary": "SpoofTrackBench is a reproducible, modular benchmark for evaluating\nadversarial robustness in real-time localization and tracking (RTLS) systems\nunder radar spoofing. Leveraging the Hampton University Skyler Radar Sensor\ndataset, we simulate drift, ghost, and mirror-type spoofing attacks and\nevaluate tracker performance using both Joint Probabilistic Data Association\n(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates\nclean and spoofed detection streams, visualizes spoof-induced trajectory\ndivergence, and quantifies assignment errors via direct drift-from-truth\nmetrics. Clustering overlays, injection-aware timelines, and scenario-adaptive\nvisualizations enable interpretability across spoof types and configurations.\nEvaluation figures and logs are auto-exported for reproducible comparison.\nSpoofTrackBench sets a new standard for open, ethical benchmarking of\nspoof-aware tracking pipelines, enabling rigorous cross-architecture analysis\nand community validation."}
{"id": "2510.22944", "pdf": "https://arxiv.org/pdf/2510.22944", "abs": "https://arxiv.org/abs/2510.22944", "authors": ["Bin Wang", "YiLu Zhong", "MiDi Wan", "WenJie Yu", "YuanBing Ouyang", "Yenan Huang", "Hui Li"], "title": "Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have become indispensable for automated code\ngeneration, yet the quality and security of their outputs remain a critical\nconcern. Existing studies predominantly concentrate on adversarial attacks or\ninherent flaws within the models. However, a more prevalent yet underexplored\nissue concerns how the quality of a benign but poorly formulated prompt affects\nthe security of the generated code. To investigate this, we first propose an\nevaluation framework for prompt quality encompassing three key dimensions: goal\nclarity, information completeness, and logical consistency. Based on this\nframework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale\nbenchmark dataset containing tasks with prompts categorized into four distinct\nlevels of normativity (L0-L3). Extensive experiments on multiple\nstate-of-the-art LLMs reveal a clear correlation: as prompt normativity\ndecreases, the likelihood of generating insecure code consistently and markedly\nincreases. Furthermore, we demonstrate that advanced prompting techniques, such\nas Chain-of-Thought and Self-Correction, effectively mitigate the security\nrisks introduced by low-quality prompts, substantially improving code safety.\nOur findings highlight that enhancing the quality of user prompts constitutes a\ncritical and effective strategy for strengthening the security of AI-generated\ncode."}
{"id": "2510.22945", "pdf": "https://arxiv.org/pdf/2510.22945", "abs": "https://arxiv.org/abs/2510.22945", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "QuantumShield: Multilayer Fortification for Quantum Federated Learning", "categories": ["cs.CR"], "comment": null, "summary": "In this paper, we propose a groundbreaking quantum-secure federated learning\n(QFL) framework designed to safeguard distributed learning systems against the\nemerging threat of quantum-enabled adversaries. As classical cryptographic\nmethods become increasingly vulnerable to quantum attacks, our framework\nestablishes a resilient security architecture that remains robust even in the\npresence of quantum-capable attackers. We integrate and rigorously evaluate\nadvanced quantum and post-quantum protocols including Quantum Key Distribution\n(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and\nPost-Quantum Cryptography (PQC) to fortify the QFL process against both\nclassical and quantum threats. These mechanisms are systematically analyzed and\nimplemented to demonstrate their seamless interoperability within a secure and\nscalable QFL ecosystem. Through comprehensive theoretical modeling and\nexperimental validation, this work provides a detailed security and performance\nassessment of the proposed framework. Our findings lay a strong foundation for\nnext-generation federated learning systems that are inherently secure in the\nquantum era."}
{"id": "2510.22963", "pdf": "https://arxiv.org/pdf/2510.22963", "abs": "https://arxiv.org/abs/2510.22963", "authors": ["Zesen Liu", "Zhixiang Zhang", "Yuchong Xie", "Dongdong She"], "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections."}
{"id": "2510.22971", "pdf": "https://arxiv.org/pdf/2510.22971", "abs": "https://arxiv.org/abs/2510.22971", "authors": ["Sudiksha Das", "Ashish Kundu"], "title": "Advancing Honeywords for Real-World Authentication Security", "categories": ["cs.CR"], "comment": null, "summary": "Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords\nstored alongside a real password, appear to be a proactive method to help\ndetect password credentials misuse. However, despite over a decade of research,\nthis technique has not been adopted by major authentication platforms. This\nposition paper argues that the core concept of Honeywords has potential but\nrequires more research on issues such as flatness, integration, and\nreliability, in order to be a practical deployable solution. This paper\nexamines the current work on Honeyword generation, attacker modeling, and\nhoneychecker architecture, analyzing the subproblems that have been addressed\nand ongoing issues that prevent this system from being more widely used. The\npaper then suggests a deployable framework that combines the\nattacker-resilient, context-aware decoy creation that Honeywords provide with\neasy integration into existing systems. Honeywords will only move from an\nacademic idea to a practical security tool if technical advances are paired\nwith secure and straightforward architectures, along with adaptive response\nhandling and detailed configuration checks."}
{"id": "2510.23024", "pdf": "https://arxiv.org/pdf/2510.23024", "abs": "https://arxiv.org/abs/2510.23024", "authors": ["Chuan Yan", "Zeng Li", "Kunlin Cai", "Liuhuo Wan", "Ruomai Ren", "Yiran Shen", "Guangdong Bai"], "title": "A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem", "categories": ["cs.CR", "cs.SE"], "comment": "16 pages", "summary": "Virtual Reality (VR) has gained increasing traction among various domains in\nrecent years, with major companies such as Meta, Pico, and Microsoft launching\ntheir application stores to support third-party developers in releasing their\napplications (or simply apps). These apps offer rich functionality but\ninherently collect privacy-sensitive data, such as user biometrics, behaviors,\nand the surrounding environment. Nevertheless, there is still a lack of\ndomain-specific regulations to govern the data handling of VR apps, resulting\nin significant variations in their privacy practices among app stores.\n  In this work, we present the first comprehensive multi-store study of privacy\npractices in the current VR app ecosystem, covering a large-scale dataset\ninvolving 6,565 apps collected from five major app stores. We assess both\ndeclarative and behavioral privacy practices of VR apps, using a multi-faceted\napproach based on natural language processing, reverse engineering, and static\nanalysis. Our assessment reveals significant privacy compliance issues across\nall stores, underscoring the premature status of privacy protection in this\nrapidly growing ecosystem. For instance, one third of apps fail to declare\ntheir use of sensitive data, and 21.5\\% of apps neglect to provide valid\nprivacy policies. Our work sheds light on the status quo of privacy protection\nwithin the VR app ecosystem for the first time. Our findings should raise an\nalert to VR app developers and users, and encourage store operators to\nimplement stringent regulations on privacy compliance among VR apps."}
{"id": "2510.23034", "pdf": "https://arxiv.org/pdf/2510.23034", "abs": "https://arxiv.org/abs/2510.23034", "authors": ["Gokulnath Rajendran", "Suman Deb", "Anupam Chattopadhyay"], "title": "Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures", "categories": ["cs.CR", "cs.AI"], "comment": "to be published in: 7th International Conference on Emerging\n  Electronics (ICEE 2025)", "summary": "Binarized Neural Networks (BNNs) are a class of deep neural networks designed\nto utilize minimal computational resources, which drives their popularity\nacross various applications. Recent studies highlight the potential of mapping\nBNN model parameters onto emerging non-volatile memory technologies,\nspecifically using crossbar architectures, resulting in improved inference\nperformance compared to traditional CMOS implementations. However, the common\npractice of protecting model parameters from theft attacks by storing them in\nan encrypted format and decrypting them at runtime introduces significant\ncomputational overhead, thus undermining the core principles of in-memory\ncomputing, which aim to integrate computation and storage. This paper presents\na robust strategy for protecting BNN model parameters, particularly within\nin-memory computing frameworks. Our method utilizes a secret key derived from a\nphysical unclonable function to transform model parameters prior to storage in\nthe crossbar. Subsequently, the inference operations are performed on the\nencrypted weights, achieving a very special case of Fully Homomorphic\nEncryption (FHE) with minimal runtime overhead. Our analysis reveals that\ninference conducted without the secret key results in drastically diminished\nperformance, with accuracy falling below 15%. These results validate the\neffectiveness of our protection strategy in securing BNNs within in-memory\ncomputing architectures while preserving computational efficiency."}
{"id": "2510.23035", "pdf": "https://arxiv.org/pdf/2510.23035", "abs": "https://arxiv.org/abs/2510.23035", "authors": ["Jun Jiang", "Weiming Zhang", "Nenghai Yu", "Kejiang Chen"], "title": "A high-capacity linguistic steganography based on entropy-driven rank-token mapping", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Linguistic steganography enables covert communication through embedding\nsecret messages into innocuous texts; however, current methods face critical\nlimitations in payload capacity and security. Traditional modification-based\nmethods introduce detectable anomalies, while retrieval-based strategies suffer\nfrom low embedding capacity. Modern generative steganography leverages language\nmodels to generate natural stego text but struggles with limited entropy in\ntoken predictions, further constraining capacity. To address these issues, we\npropose an entropy-driven framework called RTMStega that integrates rank-based\nadaptive coding and context-aware decompression with normalized entropy. By\nmapping secret messages to token probability ranks and dynamically adjusting\nsampling via context-aware entropy-based adjustments, RTMStega achieves a\nbalance between payload capacity and imperceptibility. Experiments across\ndiverse datasets and models demonstrate that RTMStega triples the payload\ncapacity of mainstream generative steganography, reduces processing time by\nover 50%, and maintains high text quality, offering a trustworthy solution for\nsecure and efficient covert communication."}
{"id": "2510.23036", "pdf": "https://arxiv.org/pdf/2510.23036", "abs": "https://arxiv.org/abs/2510.23036", "authors": ["Xudong Yang", "Jincheng Li", "Kaiwen Xing", "Zhenjia Xiao", "Mingjian Duan", "Weili Han", "Hu Xiong"], "title": "KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation", "categories": ["cs.CR"], "comment": null, "summary": "As the primary mechanism of digital authentication, user-created passwords\nexhibit common patterns and regularities that can be learned from leaked\ndatasets. Password choices are profoundly shaped by external factors, including\nsocial contexts, cultural trends, and popular vocabulary. Prevailing password\nguessing models primarily emphasize patterns derived from leaked passwords,\nwhile neglecting these external influences -- a limitation that hampers their\nadaptability to emerging password trends and erodes their effectiveness over\ntime.\n  To address these challenges, we propose KAPG, a knowledge-augmented password\nguessing framework that adaptively integrates external lexical knowledge into\nthe guessing process. KAPG couples internal statistical knowledge learned from\nleaked passwords with external information that reflects real-world trends. By\nusing password prefixes as anchors for knowledge lookup, it dynamically injects\nrelevant external cues during generation while preserving the structural\nregularities of authentic passwords. Experiments on twelve leaked datasets show\nthat KnowGuess achieves average improvements of 36.5\\% and 74.7\\% over\nstate-of-the-art models in intra-site and cross-site scenarios, respectively.\nFurther analyses of password overlap and model efficiency highlight its\nrobustness and computational efficiency. To counter these attacks, we further\ndevelop KAPSM, a trend-aware and site-specific password strength meter.\nExperiments demonstrate that KAPSM significantly outperforms existing tools in\naccuracy across diverse evaluation settings."}
{"id": "2510.23060", "pdf": "https://arxiv.org/pdf/2510.23060", "abs": "https://arxiv.org/abs/2510.23060", "authors": ["Paritosh Ramanan", "H. M. Mohaimanul Islam", "Abhiram Reddy Alugula"], "title": "zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks", "categories": ["cs.CR", "cs.SY", "eess.SY"], "comment": null, "summary": "Industrial control systems (ICS) form the operational backbone of critical\ninfrastructure networks (CIN) such as power grids, water supply systems, and\ngas pipelines. As cyber threats to these systems escalate, regulatory agencies\nare imposing stricter compliance requirements to ensure system-wide security\nand reliability. A central challenge, however, is enabling regulators to verify\nthe effectiveness of detection mechanisms without requiring utilities to\ndisclose sensitive operational data. In this paper, we introduce zkSTAR, a\ncyberattack detection framework that leverages zk-SNARKs to reconcile these\nrequirements and enable provable detection guarantees while preserving data\nconfidentiality. Our approach builds on established residual-based statistical\nhypothesis testing methods applied to state-space detection models.\nSpecifically, we design a two-pronged zk-SNARK architecture that enforces\ntemporal consistency of the state-space dynamics and statistical consistency of\nthe detection tests, allowing regulators to temporally verify alarm correctness\nwithout visibility into utility-level data. We formally analyze the soundness\nand zero knowledge properties of our framework and validate its practical\nfeasibility through computational experiments on real-world ICS datasets. As a\nresult, our work demonstrates a scalable, privacy-preserving alternative for\nregulatory compliance for ICS driven critical infrastructure networks."}
{"id": "2510.23074", "pdf": "https://arxiv.org/pdf/2510.23074", "abs": "https://arxiv.org/abs/2510.23074", "authors": ["Hiromu Takahashi", "Shotaro Ishihara"], "title": "Fast-MIA: Efficient and Scalable Membership Inference for LLMs", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library\nfor efficiently evaluating membership inference attacks (MIA) against Large\nLanguage Models (LLMs). MIA against LLMs has emerged as a crucial challenge due\nto growing concerns over copyright, security, and data privacy, and has\nattracted increasing research attention. However, the progress of this research\nis significantly hindered by two main obstacles: (1) the high computational\ncost of inference in LLMs, and (2) the lack of standardized and maintained\nimplementations of MIA methods, which makes large-scale empirical comparison\ndifficult. To address these challenges, our library provides fast batch\ninference and includes implementations of representative MIA methods under a\nunified evaluation framework. This library supports easy implementation of\nreproducible benchmarks with simple configuration and extensibility. We release\nFast-MIA as an open-source (Apache License 2.0) tool to support scalable and\ntransparent research on LLMs."}
{"id": "2510.23101", "pdf": "https://arxiv.org/pdf/2510.23101", "abs": "https://arxiv.org/abs/2510.23101", "authors": ["Yifan Zhang", "Xin Zhang"], "title": "Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing", "categories": ["cs.CR", "cs.PL", "cs.SE"], "comment": "Preprint, under submission", "summary": "Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific\ntarget locations by prioritizing seeds whose execution paths are more likely to\nmutate into triggering target bugs. However, existing DGF approaches suffer\nfrom imprecise probability calculations due to their reliance on complex\ndistance metrics derived from static analysis. The over-approximations inherent\nin static analysis cause a large number of irrelevant execution paths to be\nmistakenly considered to potentially mutate into triggering target bugs,\nsignificantly reducing fuzzing efficiency. We propose to replace static\nanalysis-based distance metrics with precise call stack representations. Call\nstacks represent precise control flows, thereby avoiding false information in\nstatic analysis. We leverage large language models (LLMs) to predict\nvulnerability-triggering call stacks for guiding seed prioritization. Our\napproach constructs call graphs through static analysis to identify methods\nthat can potentially reach target locations, then utilizes LLMs to predict the\nmost likely call stack sequence that triggers the vulnerability. Seeds whose\nexecution paths have higher overlap with the predicted call stack are\nprioritized for mutation. This is the first work to integrate LLMs into the\ncore seed prioritization mechanism of DGF. We implement our approach and\nevaluate it against several state-of-the-art fuzzers. On a suite of real-world\nprograms, our approach triggers vulnerabilities $1.86\\times$ to $3.09\\times$\nfaster compared to baselines. In addition, our approach identifies 10 new\nvulnerabilities and 2 incomplete fixes in the latest versions of programs used\nin our controlled experiments through directed patch testing, with 10 assigned\nCVE IDs."}
{"id": "2510.23172", "pdf": "https://arxiv.org/pdf/2510.23172", "abs": "https://arxiv.org/abs/2510.23172", "authors": ["Mohsen Ahmadvand", "Pedro Souto"], "title": "Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation", "categories": ["cs.CR"], "comment": null, "summary": "The Optimism derivation pipeline is engineered for correctness and liveness,\nnot for succinct validity proofs. A straightforward port to a zkVM imposes\nsignificant overheads, making validity proofs significantly more costly than\nnecessary. We systematically identify inefficiencies in the current design,\nanalyze their impact on proving costs, and provide a soundness-preserving\nredesign tailored to zk proving. Our redesign achieves up to 6.5x faster\nderivation inside zkVMs (3.5x overall speedup) while maintaining identical\nsafety guarantees."}
{"id": "2510.23274", "pdf": "https://arxiv.org/pdf/2510.23274", "abs": "https://arxiv.org/abs/2510.23274", "authors": ["Weixuan Chen", "Qianqian Yang", "Shuo Shao", "Shunpu Tang", "Zhiguo Shi", "Shui Yu"], "title": "Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy", "categories": ["cs.CR", "eess.IV"], "comment": null, "summary": "While semantic communication (SemCom) improves transmission efficiency by\nfocusing on task-relevant information, it also raises critical privacy\nconcerns. Many existing secure SemCom approaches rely on restrictive or\nimpractical assumptions, such as favorable channel conditions for the\nlegitimate user or prior knowledge of the eavesdropper's model. To address\nthese limitations, this paper proposes a novel secure SemCom framework for\nimage transmission over wiretap channels, leveraging differential privacy (DP)\nto provide approximate privacy guarantees. Specifically, our approach first\nextracts disentangled semantic representations from source images using\ngenerative adversarial network (GAN) inversion method, and then selectively\nperturbs private semantic representations with approximate DP noise. Distinct\nfrom conventional DP-based protection methods, we introduce DP noise with\nlearnable pattern, instead of traditional white Gaussian or Laplace noise,\nachieved through adversarial training of neural networks (NNs). This design\nmitigates the inherent non-invertibility of DP while effectively protecting\nprivate information. Moreover, it enables explicitly controllable security\nlevels by adjusting the privacy budget according to specific security\nrequirements, which is not achieved in most existing secure SemCom approaches.\nExperimental results demonstrate that, compared with the previous DP-based\nmethod and direct transmission, the proposed method significantly degrades the\nreconstruction quality for the eavesdropper, while introducing only slight\ndegradation in task performance. Under comparable security levels, our approach\nachieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86\nfor the legitimate user compared with the previous DP-based method."}
{"id": "2510.23313", "pdf": "https://arxiv.org/pdf/2510.23313", "abs": "https://arxiv.org/abs/2510.23313", "authors": ["Yaokai Feng", "Kouichi Sakurai"], "title": "Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks", "categories": ["cs.CR"], "comment": "28 pages,1 figure, 204 references", "summary": "This survey systematizes the evolution of network intrusion detection systems\n(NIDS), from conventional methods such as signature-based and neural network\n(NN)-based approaches to recent integrations with large language models (LLMs).\nIt clearly and concisely summarizes the current status, strengths, and\nlimitations of conventional techniques, and explores the practical benefits of\nintegrating LLMs into NIDS. Recent research on the application of LLMs to NIDS\nin diverse environments is reviewed, including conventional network\ninfrastructures, autonomous vehicle environments and IoT environments.\n  From this survey, readers will learn that: 1) the earliest methods,\nsignature-based IDSs, continue to make significant contributions to modern\nsystems, despite their well-known weaknesses; 2) NN-based detection, although\nconsidered promising and under development for more than two decades, and\ndespite numerous related approaches, still faces significant challenges in\npractical deployment; 3) LLMs are useful for NIDS in many cases, and a number\nof related approaches have been proposed; however, they still face significant\nchallenges in practical applications. Moreover, they can even be exploited as\noffensive tools, such as for generating malware, crafting phishing messages, or\nlaunching cyberattacks. Recently, several studies have been proposed to address\nthese challenges, which are also reviewed in this survey; and 4) strategies for\nconstructing domain-specific LLMs have been proposed and are outlined in this\nsurvey, as it is nearly impossible to train a NIDS-specific LLM from scratch."}
{"id": "2510.23457", "pdf": "https://arxiv.org/pdf/2510.23457", "abs": "https://arxiv.org/abs/2510.23457", "authors": ["Saleh Darzi", "Mirza Masfiqur Rahman", "Imtiaz Karim", "Rouzbeh Behnia", "Attila A Yavuz", "Elisa Bertino"], "title": "Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era", "categories": ["cs.CR"], "comment": "17 pages, 3 tables, 6 figures", "summary": "The 5G protocol lacks a robust base station authentication mechanism during\nthe initial bootstrapping phase, leaving it susceptible to threats such as fake\nbase station attacks. Conventional solutions, including digital signatures\nbased on Public Key Infrastructures (PKIs) and identity-based signatures, are\ninadequate against quantum-capable adversaries. While integrating NIST's\nPost-Quantum Cryptography (PQC) standards is a leading approach for quantum\nresistance, their suitability for 5G base station authentication remains\nunexplored. Moreover, current solutions are predominantly centralized and lack\nsecurity features such as distributed authentication. This work presents, to\nour knowledge, the first comprehensive network-level performance\ncharacterization of integrating NIST-PQC standards and conventional digital\nsignatures (including threshold and identity-based schemes) into 5G base\nstation authentication. Our findings reveal significant feasibility concerns,\nwith direct PQC adoption hindered by protocol constraints and large signature\nsizes. We also highlight the performance limitations of conventional methods\ndue to the overhead of certificate chains. To mitigate these challenges, we\npropose BORG, a transitional authentication solution based on a Hierarchical\nIdentity-Based Threshold Signature scheme with a Fail-Stop property. BORG\noffers post-mortem post-quantum forgery detection and distributed trust via\nthreshold and compact signatures, well-suited for 5G's stringent requirements.\nOur performance analysis underscores an important warning on the infeasibility\nof direct PQC integration and positions BORG as an effective transitional\nsolution toward future quantum-resilient 5G authentication."}
{"id": "2510.23483", "pdf": "https://arxiv.org/pdf/2510.23483", "abs": "https://arxiv.org/abs/2510.23483", "authors": ["Valentin Reyes HÃ¤usler", "Gabriel Ott", "Aruna Jayasena", "Andreas Peter"], "title": "Towards a Functionally Complete and Parameterizable TFHE Processor", "categories": ["cs.CR"], "comment": null, "summary": "Fully homomorphic encryption allows the evaluation of arbitrary functions on\nencrypted data. It can be leveraged to secure outsourced and multiparty\ncomputation. TFHE is a fast torus-based fully homomorphic encryption scheme\nthat allows both linear operations, as well as the evaluation of arbitrary\nnon-linear functions. It currently provides the fastest bootstrapping operation\nperformance of any other FHE scheme. Despite its fast performance, TFHE suffers\nfrom a considerably higher computational overhead for the evaluation of\nhomomorphic circuits. Computations in the encrypted domain are orders of\nmagnitude slower than their unencrypted equivalents. This bottleneck hinders\nthe widespread adoption of (T)FHE for the protection of sensitive data. While\nstate-of-the-art implementations focused on accelerating and outsourcing single\noperations, their scalability and practicality are constrained by high memory\nbandwidth costs. In order to overcome this, we propose an FPGA-based hardware\naccelerator for the evaluation of homomorphic circuits. Specifically, we design\na functionally complete TFHE processor for FPGA hardware capable of processing\ninstructions on the data completely on the FPGA. In order to achieve a higher\nthroughput from our TFHE processor, we implement an improved programmable\nbootstrapping module which outperforms the current state-of-the-art by 240\\% to\n480\\% more bootstrappings per second. Our efficient, compact, and scalable\ndesign lays the foundation for implementing complete FPGA-based TFHE processor\narchitectures."}
