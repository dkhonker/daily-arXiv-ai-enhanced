<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 74]
- [cs.AI](#cs.AI) [总数: 23]
- [stat.ML](#stat.ML) [总数: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer, Joshua Kazdan, Yegor Denisov-Blanch, Brando Miranda, Matthias Gerstgrasser, Susan Zhang, Andreas Haupt, Isha Gupta, Elyas Obbad, Jesse Dodge, Jessica Zosa Forde, Koustuv Sinha, Francesco Orabona, Sanmi Koyejo, David Donoho*

**主要类别:** cs.LG

**AI概要:** 这篇论文摘要主张在机器学习（ML）会议中建立一个专门的“反驳与批评”（R & C）轨道，以提供一个高知名度、可信赖的平台，支持对先前研究进行批判性挑战的重要研究，从而促进自我修正的研究生态系统。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，机器学习领域的快速发展导致了大量出版物的涌现，但同时也出现了误导性、错误或有缺陷的研究被接受甚至在会议上被突出展示的情况。由于同行评审的局限性，当前的ML会议缺乏系统纠正这些错误的机制。

**方法:** 提出在ML会议中设立“反驳与批评”（R & C）轨道，为批判性挑战先前研究的研究提供一个正式且可信的平台，并讨论了该轨道的设计、评审原则以及可能存在的问题。

**结果:** 通过设置R & C轨道，可以促进动态自我修正的研究生态系统的发展，并提高ML研究的质量和可靠性。

**结论:** ML会议应创建官方、可信赖的机制，帮助ML研究实现自我修正，从而推动科学的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+Machine+Learning+Conferences+Should+Establish+a+%22Refutations+and+Critiques%22+Track，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19882&send_immediately=true&force_search=false)

**原文摘要:** Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [2] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu, Chaosheng Dong, Michinari Momma, Simone Shao, Shaoyuan Xu, Yan Gao, Haibo Yang, Jia Liu*

**主要类别:** cs.LG

**AI概要:** This paper addresses the limitations of current MOO algorithms by introducing STIMULUS and its variants, which offer improved convergence and lower sample complexity through recursive gradient updates and optional momentum terms.


<details>
  <summary>更多</summary>
  
**动机:** Existing MOO algorithms have unsatisfactory convergence rates and high sample complexities, motivating the development of more efficient methods.

**方法:** STIMULUS uses a recursive framework to update stochastic gradient estimates, improving convergence with low sample complexity. STIMULUS-M adds a momentum term for faster convergence. Enhanced versions with adaptive batching (STIMULUS+ and STIMULUS-M+) reduce the need for full gradient evaluations.

**结果:** The proposed methods achieve $ O(1/T) $ convergence rates for non-convex settings and $ O(\exp{-\mu T}) $ for strongly convex settings. They also attain optimal sample complexities.

**结论:** The paper proposes STIMULUS and its variants, achieving state-of-the-art performance in terms of convergence rate and sample complexity for MOO problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STIMULUS%3A+Achieving+Fast+Convergence+and+Low+Sample+Complexity+in+Stochastic+Multi-Objective+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19883&send_immediately=true&force_search=false)

**原文摘要:** Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [3] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu, Xuan Wu, Yizhun Tian, Songhan Fan, Yali Fang*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的基于HIPPO方法、Koopman理论和状态空间方程的飞行轨迹预测框架FlightKooba，直接从数据构建Koopman算子，提高可解释性并减少训练参数与时间，实验表明其在时间和内存消耗上具有优势。


<details>
  <summary>更多</summary>
  
**动机:** 当前将Koopman理论应用于飞行轨迹预测任务的模型效果不佳，存在模型可解释性差和计算量大的问题，导致训练时间过长。

**方法:** 提出名为FlightKooba的新建模与控制框架，结合了HIPPO方法、Koopman理论和控制论中的状态空间方程。受结构化状态空间方程思想启发，FlightKooba直接从数据构建Koopman算子，大幅减少了模块中的可训练参数数量，从而显著缩短了训练时间。

**结果:** 实验表明FlightKooba建模方法在时间和内存消耗方面具有优越性：训练时间与Mamba模块相当（未使用CUDA级加速），大多数数据集上的内存消耗减少超过50%，参数数量减少十倍，成功完成飞行轨迹预测任务。

**结论:** FlightKooba提供了一种快速计算Koopman算子的新方法，为时间序列预测与控制的结合开辟了新可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlightKooba%3A+A+Fast+Interpretable+FTP+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19885，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19885&send_immediately=true&force_search=false)

**原文摘要:** The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [4] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang, Jiadong Yu, Danny H. K. Tsang*

**主要类别:** cs.LG

**AI概要:** 提出了一种智能框架，通过结合自适应关键帧提取和因果感知强化学习（RL）来优化多用户虚拟现实交互中的体验质量（QoE）。该框架利用部分状态因果深度确定性策略梯度（PS-CDDPG），显著减少了交互延迟，提高了QoE，并保持了公平性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法虽然通过自适应关键帧提取减轻了传输开销，但常常忽略了分配带宽、CPU频率和用户体验之间的因果关系，限制了QoE的提升。

**方法:** 1. 提出一种新的QoE指标，基于韦伯-费希纳定律，综合考虑感知敏感度、注意力驱动优先级和运动重建精度。
2. 将QoE优化问题建模为混合整数规划（MIP）任务，联合优化关键帧比例、带宽和计算资源，并在时间公平约束下进行优化。
3. 提出部分状态因果深度确定性策略梯度（PS-CDDPG），将深度确定性策略梯度（DDPG）与因果影响检测相结合。
4. 利用因果信息指导动作选择，提高训练效率。

**结果:** 实验结果表明，该框架显著减少了交互延迟，提高了QoE，并且维持了公平性，性能优于基准方法。

**结论:** 提出的智能框架能够有效优化多用户虚拟现实交互中的体验质量，具有良好的性能表现和公平性保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal-Aware+Intelligent+QoE+Optimization+for+VR+Interaction+with+Adaptive+Keyframe+Extraction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19890，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19890&send_immediately=true&force_search=false)

**原文摘要:** The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [5] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong, Xue Yang, Xiaohu Tang*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的类别感知软剪枝框架，通过正交卷积核正则化实现快速精确的机器遗忘，具有毫秒级响应时间，能够在保留数据准确性最小损失的情况下完全忘记目标类别。相比现有方法，该框架显著降低了成员推理攻击风险并加速了遗忘过程，适用于实时机器学习服务场景。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器遗忘方法通常在遗忘速度和预测准确性保持之间存在权衡，要么计算开销高，要么对保留类别的性能退化显著。因此需要一种能够快速、精确地移除特定类别知识且对模型影响较小的方法来满足隐私法规（如GDPR）的要求。

**方法:** 提出了一种基于正交卷积核正则化的类别感知软剪枝框架。通过在训练过程中施加正交性约束，使卷积滤波器去相关，并解缠特征表示。同时，通过激活差异分析高效识别类别特定通道，从而实现快速而精确的知识遗忘。

**结果:** 广泛的实验评估表明，该方法在多个架构和数据集上表现出稳定的剪枝效果，执行近乎即时，能够完全忘记目标类别，同时保留数据的准确性损失最小。实验还证实，该方法大幅降低了成员推理攻击的风险，并比现有基线方法加速了数个数量级。

**结论:** 所提出的框架为实时机器遗忘提供了一种高效、实用的解决方案，特别适用于机器学习即服务（MLaaS）场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Orthogonal+Soft+Pruning+for+Efficient+Class+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19891，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19891&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [6] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu, Geoffrey Ye Li*

**主要类别:** cs.LG

**AI概要:** DeKA-g是一种为生成式语义通信系统设计的知识对齐算法，通过元词辅助知识蒸馏和可变速率分组SNR适应两种方法，显著提高了边缘生成图像与云端生成图像的一致性、压缩率适应效率以及低SNR条件下的性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于AI生成内容（AIGC）的激增，从云端向边缘和移动用户提供服务产生了大量的网络流量。生成式语义通信（GSC）通过传输紧凑的信息（如提示文本和潜在表示）而非高维AIGC数据提供了一个有希望的解决方案。然而，GSC面临知识对齐的挑战，包括云端生成式AI与边缘用户之间的知识对齐，以及无线传输知识与实际信道条件的知识对齐。

**方法:** 提出了DeKA-g算法，核心是将云端生成式AI的知识蒸馏到低秩矩阵中，供边缘设备使用，并适应不同的无线信道条件。DeKA-g包含两种新方法：元词辅助知识蒸馏（MAKD）和可变速率分组SNR适应（VGSA）。MAKD利用优化的元词提高知识蒸馏效率，而VGSA则实现了对不同压缩率和SNR范围的有效适应。

**结果:** 仿真结果显示，DeKA-g使边缘生成的图像与云端生成的图像之间的一致性提高了44%，对压缩率的适应效率比基线高出116%，并在低SNR条件下提升了28%的性能。

**结论:** DeKA-g算法有效解决了GSC系统中的知识对齐问题，提高了生成一致性、压缩适应性和低SNR性能，为高效传输AI生成内容提供了支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distillation-Enabled+Knowledge+Alignment+for+Generative+Semantic+Communications+in+AIGC+Provisioning+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19893，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19893&send_immediately=true&force_search=false)

**原文摘要:** Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [7] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti, Aidan OSullivan*

**主要类别:** cs.LG

**AI概要:** 本文使用深度神经网络（DNN）预测电力市场价格，并结合可解释性人工智能（XAI）方法如SHAP和梯度，以及热力图等可视化技术，分析五个电力市场中不同特征的行为与贡献。此外，文章引入了SSHAP值和SSHAP线的概念，以提升高维表格模型的复杂表示能力。


<details>
  <summary>更多</summary>
  
**动机:** 电力市场具有高度复杂性和交互性，理解其内部运作及价格驱动因素颇具挑战性。虽然计量经济学方法（白盒模型）可用于此目的，但其效果不如深度神经网络模型（DNN）。

**方法:** 利用DNN预测电力市场价格，随后采用XAI方法（如SHAP和梯度）与可视化技术（如热力图）来解析影响市场价格动态的因素。在分析中覆盖了五个不同的电力市场，并提出了SSHAP值和SSHAP线的新概念。

**结果:** 通过结合DNN和XAI方法，成功增强了对电力市场运作机制的理解，并且提出的SSHAP值和SSHAP线为高维表格模型提供了更复杂的表示形式。

**结论:** 深度学习模型结合XAI方法能够有效揭示电力市场价格动态背后的驱动因素，有助于增进对不同电力市场运行方式的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explaining+deep+neural+network+models+for+electricity+price+forecasting+with+XAI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19894&send_immediately=true&force_search=false)

**原文摘要:** Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [8] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font, José L. Jorro-Aragoneses, Carlos M. Alaíz*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的事后框架，通过检索与查询在各层具有相似激活向量的训练案例来测量神经网络决策的不确定性，并提出了两个新指标：决策变化和层不确定性。实验结果表明，这些指标提高了不确定性估计，尤其是在具有挑战性的分类任务中，优于基于softmax的置信度。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络在解决难以检测模式或创建逻辑模型的问题时具有高准确性，但有时会返回错误解决方案，这在医疗诊断或自动驾驶等高风险领域成为问题。因此，需要一种策略来检测和缓解这些错误，测量神经网络决策的不确定性是一个重要方向。

**方法:** 作者提出了一种新的事后框架，通过检索与查询在各层具有相似激活向量的训练案例来测量决策的不确定性。基于检索到的案例，提出了两个新指标：决策变化（Decision Change）和层不确定性（Layer Uncertainty），用于捕获跨层最近邻类别分布的变化。

**结果:** 该方法在CIFAR-10和MNIST数据集上的分类模型中进行了评估，结果显示这些新指标提高了不确定性估计，特别是在具有挑战性的分类任务中，表现优于基于softmax的置信度。

**结论:** 所提出的框架和新指标为测量神经网络决策的不确定性提供了一种有效的方法，特别是在困难的分类任务中表现出色，可以用于改善高风险领域的决策可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Framework+for+Uncertainty+Quantification+Based+on+Nearest+Neighbors+Across+Layers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19895，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19895&send_immediately=true&force_search=false)

**原文摘要:** Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [9] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang, Jianyu Zhang, Léon Bottou*

**主要类别:** cs.LG

**AI概要:** 迁移学习是现代机器学习的基石，但其在处理未见数据集和量化任务相关性方面存在挑战。本文评估了预训练模型向各组成任务转移的能力，并发现了深度学习模型中的“信息饱和瓶颈”。此瓶颈导致网络无法学习新特征，从而影响迁移性能。研究表明，仅依赖大规模网络可能不如专注于任务特定训练有效。提出更丰富的特征表示作为潜在解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 迁移学习承诺以最少的新数据将预训练模型适应新任务，但确保转移特征足以处理未见数据集仍是一个重大挑战。此外，量化任务间的“相关性”也很困难。

**方法:** 评估从预训练混合数据到每个组成任务的模型转移，分析预训练特征是否能匹配任务特定直接训练的性能。识别出深度学习模型中的“信息饱和瓶颈”，并研究限制预训练期间学习的关键特征子集对模型性能的影响。

**结果:** 发现当网络在训练中编码类似竞争特征后，无法再学习新特征。如果预训练时仅学习关键特征的子集，模型将永久丢失重要特征，导致在数据分布上的表现不一致。实证证据表明，这种现象在深度学习架构中普遍存在。

**结论:** 依靠大规模网络可能不如聚焦于任务特定训练有效。建议采用更丰富的特征表示来更好地泛化到新数据集，并提出了现有方法与一种新方法作为解决该挑战的初步步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是These+are+Not+All+the+Features+You+are+Looking+For%3A+A+Fundamental+Bottleneck+In+Supervised+Pretraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.18221，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.18221&send_immediately=true&force_search=false)

**原文摘要:** Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [10] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır, Patrick Dumond*

**主要类别:** cs.LG

**AI概要:** 研究探讨了强化学习，特别是深度Q网络（DQN），在轴承故障分类任务中的可行性，以提高诊断的准确性和适应性。结果表明，在控制条件下，RL模型的表现可与传统监督学习模型媲美，但在优化奖励结构后，其适应能力更胜一筹。然而，计算需求较高，需要进一步改进。


<details>
  <summary>更多</summary>
  
**动机:** 现有的轴承故障诊断方法依赖于振动分析和机器学习技术，需要大量标注数据且在动态环境中适应性较差。因此，探索强化学习在轴承故障分类中的应用成为必要。

**方法:** 使用强化学习中的深度Q网络（DQN）进行轴承故障分类任务，并通过优化奖励结构来提高模型适应性。

**结果:** 在控制条件下，强化学习模型表现与传统监督学习模型相当；在优化奖励结构后，适应性显著提高，但计算需求较大。

**结论:** 强化学习具有补充传统轴承故障诊断方法的潜力，为开发自适应诊断框架铺平道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comparative+Analysis+of+Reinforcement+Learning+and+Conventional+Deep+Learning+Approaches+for+Bearing+Fault+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19929，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19929&send_immediately=true&force_search=false)

**原文摘要:** Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [11] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue, Tianyu Xie, Tianyang Hu, Zijin Feng, Jiacheng Sun, Kenji Kawaguchi, Zhenguo Li, Zhi-Ming Ma*

**主要类别:** cs.LG

**AI概要:** 本研究通过在仅解码器框架内评估掩码扩散模型（MDM），实现了对MDM（作为任意顺序自回归，或AO-AR）和标准自回归（AR）范式的公平比较，并探讨了架构影响（仅解码器对比仅编码器）内的MDM表现。研究发现表明，尽管仅编码器的MDM建模了一个更简单的条件概率空间，但仅解码器的MDM可以通过温度退火实现显著的生成加速（~25倍）和可比的困惑度，尽管它建模了一个更大的空间。


<details>
  <summary>更多</summary>
  
**动机:** 当前在比较自回归（AR）和掩码扩散模型（MDM）范式时，由于同时改变建模范式和架构，导致不公平的直接比较。因此需要在一个公平的框架下进行比较，以区分范式本身和架构变化的影响。

**方法:** 研究在仅解码器框架内评估MDM，以：(1) 公平地比较MDM（作为任意顺序自回归，或AO-AR）和标准AR范式；(2) 探讨架构影响（仅解码器 vs. 仅编码器）内的MDM表现。

**结果:** 研究表明，标准的AO-AR目标可能需要改进，因为许多排列相比语言固有的左到右结构显得不够有信息量。此外，尽管仅编码器的MDM建模了一个更简单的条件概率空间，但仅解码器的MDM可以实现显著的生成加速（~25倍）和可比的困惑度，尽管它建模了一个更大的空间。

**结论:** 这项工作将核心范式差异与架构影响分离，为未来的模型设计提供了见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Any-Order+GPT+as+Masked+Diffusion+Model%3A+Decoupling+Formulation+and+Architecture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19935&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [12] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter, Luis Franca, Jessica Wolk, Yiyuan Wu, Bella Mehta, Joseph Dehoney, Orsolya Kiss, Fiona C. Baker, Qingyu Zhao, Rich Caruana, Kilian M. Pohl*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新颖的方法，用于确定广义加法模型（GAMs）中特征组的重要性。这种方法高效、无需重新训练模型、允许事后定义和重叠的特征组，并且在高维设置下仍然有意义。通过合成实验和两个案例研究，展示了该方法在多模态数据中的应用及其对医学问题更准确的整体视图。


<details>
  <summary>更多</summary>
  
**动机:** 在可解释的机器学习中，虽然分析特征的重要性变得普遍，但来自一组相关特征的联合信号有时被忽视或无意中排除。忽略联合信号可能会遗漏关键洞察：在许多情况下，最重要的预测因子不是孤立的特征，而是特征组的综合效应。这对于包含自然特征分组的数据集（如多模态数据集）尤其成问题。

**方法:** 研究引入了一种新方法来评估广义加法模型（GAMs）中特征组的重要性。此方法具有以下特点：高效、无需重新训练模型、允许事后定义特征组、允许特征组重叠，并在高维设置下保持意义。此外，该定义与统计学中的解释变异具有平行性。

**结果:** 通过三个合成实验，展示了该方法在不同数据条件下的行为特性。随后，通过两个案例研究，即从多模态神经科学数据集中识别抑郁症状和研究全髋关节置换术后健康的社会决定因素，证明了分析特征组重要性的价值。

**结论:** 分析特征组的重要性提供了对医学问题更准确、更全面的理解，相比单个特征分析更具优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Most+Important+Features+in+Generalized+Additive+Models+Might+Be+Groups+of+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19937，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19937&send_immediately=true&force_search=false)

**原文摘要:** While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [13] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay, Miika Aittala, Karsten Kreis, Noah Brenowitz, Arash Vahdat, Morteza Mardani, Rose Yu*

**主要类别:** cs.LG

**AI概要:** Elucidated Rolling Diffusion Models (ERDM) 是一种新框架，成功将滚动预测结构与 Elucidated Diffusion Models (EDM) 的高性能设计统一起来，通过改进的损失权重、预训练初始化和混合序列架构，在 2D Navier-Stokes 模拟和 ERA5 天气预报中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的扩散模型在高维混沌系统中的应用难以捕捉复杂的时序依赖关系，并且无法显式地考虑不确定性随时间的增长问题，而将滚动扩散框架与最先进的扩散技术结合仍是一个重大挑战。

**方法:** 研究者提出了 Elucidated Rolling Diffusion Models (ERDM)，通过以下方法实现：(i) 设计了一种新的损失权重方案，集中于确定性向随机性转变的中程预测范围；(ii) 使用预训练的 EDM 进行高效的初始化；(iii) 构建了一个定制的混合序列架构以提取鲁棒的空间时间特征。同时，核心的 EDM 组件（噪声计划、网络预处理和 Heun 采样器）被适配到滚动预测环境中。

**结果:** 在 2D Navier-Stokes 模拟和 ERA5 全球天气预报（1.5° 分辨率）任务中，ERDM 显著优于关键的基于扩散的基线模型，包括条件自回归 EDM。

**结论:** ERDM 提供了一种灵活且强大的通用框架，用于解决需要建模递增不确定性的扩散序列生成问题，代码已公开发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Elucidated+Rolling+Diffusion+Models+for+Probabilistic+Weather+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20024&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [14] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi, Bernadett Aradi*

**主要类别:** cs.LG

**AI概要:** HERCULES是一种新的算法和Python包，通过递归应用k-means聚类并结合大型语言模型（LLMs）生成语义丰富的标题和描述，以增强对复杂数据集的可解释性和高效总结。


<details>
  <summary>更多</summary>
  
**动机:** 随着各种模态的复杂数据集的爆炸性增长，需要先进的分析工具不仅能够有效地对数据进行分组，还能够提供对发现结构的人类可理解的见解。

**方法:** HERCULES通过递归应用k-means聚类构建集群层次结构，并使用大型语言模型（LLMs）生成每个层次级别的语义丰富的标题和描述。它支持两种主要表示模式：`直接'模式和`描述'模式，并允许用户提供`主题种子'来引导LLM生成的摘要。

**结果:** 展示了HERCULES的能力，并讨论了其从复杂数据集中提取有意义的、分层知识的潜力。

**结论:** HERCULES为分层k-means聚类提供了新颖的方法，显著增强了对聚类结果的可解释性，并且可以通过交互式可视化工具进行深入分析和理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HERCULES%3A+Hierarchical+Embedding-based+Recursive+Clustering+Using+LLMs+for+Efficient+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19992&send_immediately=true&force_search=false)

**原文摘要:** The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [15] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg, Christos Thrampoulidis, Lalitha Sankar*

**主要类别:** cs.LG

**AI概要:** 在最后层重训练（LLR）的场景中，损失权重仍然有效，但需要考虑模型的相对过参数化程度。


<details>
  <summary>更多</summary>
  
**动机:** 尽管机器学习模型在大规模判别任务中表现出色，但其克服训练数据引入偏差的能力受到越来越多的关注。已有研究表明，在参数化的两个极端情况下，损失加权的效果截然不同：在群体（欠参数化）设置中损失加权是最优的，而在可分离的过参数化设置中损失加权无法保证各类别的等效性能。

**方法:** 研究了最后层重训练（LLR）的场景，其中未见的（重训练）数据通常是不可分的，并且模型规模适中，介于上述两种极端情况之间。通过理论和实践相结合的方法探讨了损失加权在这种场景中的效果。

**结果:** 发现损失加权在这种场景下依然有效，但这些权重必须考虑到模型的相对过参数化程度。

**结论:** 损失加权在最后层重训练场景中是有效的，但需要调整以适应模型的相对过参数化程度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thumb+on+the+Scale%3A+Optimal+Loss+Weighting+in+Last+Layer+Retraining，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20025，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20025&send_immediately=true&force_search=false)

**原文摘要:** While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [16] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho, Jaegyun Im, Jihwan Lee, Hojun Yi, Sejin Kim, Sundong Kim*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的方法TRACED，通过结合转换预测误差和共同学习性来改进未见环境中的深度强化学习代理的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的无监督环境设计（UED）方法在生成具有高学习潜力的任务时，通常仅通过价值函数损失来近似后悔值，这种方法可能不够全面。因此，需要一种更精确的方式来衡量任务的学习潜力。

**方法:** 引入了转换预测误差作为后悔值近似的额外项，并提出了一个名为共同学习性的轻量级度量标准，以捕捉在一个任务上训练如何影响其他任务的表现。将这两个度量标准结合起来，形成了TRACED方法。

**结果:** 实证评估表明，TRACED生成的课程可以改善多个基准测试中的零样本泛化能力，并且与强大的基线相比，最多可减少2倍的环境交互次数。消融研究确认了转换预测误差驱动复杂性的快速提升，而共同学习性在与转换预测误差配对时提供了额外的收益。

**结论:** 精炼的后悔值近似和显式的任务关系建模可以用于提高UED中样本效率的课程设计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRACED%3A+Transition-aware+Regret+Approximation+with+Co-learnability+for+Environment+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19997&send_immediately=true&force_search=false)

**原文摘要:** Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [17] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan, Hernando Ombao*

**主要类别:** cs.LG

**AI概要:** Granger因果关系（GC）研究多变量时间序列数据的关联。尽管线性向量自回归模型（VAR）具有良好的解释性，但其实际应用受限于假设条件。已有多篇文献利用深度神经网络（DNN）的功能近似能力进行GC估计，但这些方法通常将GC视为变量选择问题。本文提出了一种新的GC分析范式，认为GC本质上与预测相关。通过使用深度学习模型对时间序列进行联合建模，并通过比较使用所有过去信息和排除特定时间序列分量时的模型不确定性或残差分布，可以揭示真实的Granger因果结构。此外，文章探讨了输入层Dropout对神经网络学习Granger因果关系的影响，表明经过良好正则化的模型可以在不显式添加引导变量选择或稀疏回归的损失函数项的情况下，从数据中学习到真实的GC结构。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于深度神经网络的Granger因果关系估计方法主要将GC视为变量选择问题，而本文试图重新定义GC的本质，强调其与预测之间的联系，从而提出一种新的方法来挖掘时间序列中的因果结构。

**方法:** 1. 使用深度学习模型对时间序列进行联合建模。
2. 通过比较在包含所有过去信息和排除特定时间序列分量时模型的不确定性或残差分布，揭示Granger因果结构。
3. 探讨输入层Dropout对神经网络学习Granger因果关系的影响。
4. 不需要显式地在损失函数中添加变量选择或稀疏回归的引导项。

**结果:** 研究表明，经过良好正则化的模型可以从数据中学习到真实的Granger因果结构，无需显式添加变量选择或稀疏回归的损失函数项。

**结论:** 本文提出的范式提供了一种新的理解Granger因果关系的方法，即通过深度学习模型的预测能力和正则化特性，可以直接从数据中学习到因果结构，而不需要显式的变量选择过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+ability+of+Deep+Neural+Networks+to+Learn+Granger+Causality+in+Multi-Variate+Time+Series+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20347&send_immediately=true&force_search=false)

**原文摘要:** Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [18] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu, Jiechen Chen, H. Vincent Poor, Bipin Rajendran, Osvaldo Simeone*

**主要类别:** cs.LG

**AI概要:** 神经形态计算为实时时间序列处理提供了一种比传统深度学习加速器更节能的替代方案。本文研究了一种使用共振发放（RF）神经元的无线分拆计算架构，该架构能够直接处理时域信号，减少了昂贵的频谱预处理需求。RF神经元通过可调谐频率提取时间局部化的频谱特征，同时保持低发放活动，从而显著节省计算和传输能量。实验结果表明，所提出的RF-SNN架构在音频分类和调制分类任务中，与传统的LIF-SNNs和ANNs相比，精度相当，但大幅降低了发放率和总能耗。


<details>
  <summary>更多</summary>
  
**动机:** 许多边缘应用（如无线传感和音频识别）生成具有丰富频谱特征的流式信号，而传统的漏电积分发放（LIF）脉冲神经元无法有效捕获这些特征。因此，需要一种新的方法来直接处理时域信号并减少频谱预处理的需求。

**方法:** 采用共振发放（RF）神经元的无线分拆计算架构，利用振荡动力学直接处理时域信号。RF神经元通过在可调谐频率下共振，提取时间局部化的频谱特征，同时维持较低的脉冲活动。假设基于OFDM的模拟无线接口用于脉冲传输，并提出完整的系统设计。

**结果:** 实验结果表明，在音频分类和调制分类任务中，所提出的RF-SNN架构实现了与传统LIF-SNNs和ANNs相当的准确性，同时显著降低了脉冲率和总能耗。

**结论:** 使用RF神经元的无线分拆计算架构能够在保持高精度的同时，显著降低脉冲率和总能耗，适用于无线传感和音频识别等边缘应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neuromorphic+Wireless+Split+Computing+with+Resonate-and-Fire+Neurons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20015，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20015&send_immediately=true&force_search=false)

**原文摘要:** Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [19] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo, Matteo Marsili, Yasser Roudi*

**主要类别:** cs.LG

**AI概要:** Closed-loop learning in exponential family models can lead to amplification of initial biases, but this can be mitigated by data pollution, MAP estimation, or regularisation.


<details>
  <summary>更多</summary>
  
**动机:** To understand the process of closed-loop learning where models are repeatedly estimated from data generated by themselves, especially its implications for future large neural network training.

**方法:** Studying closed-loop learning in models belonging to exponential families, deriving equations of motion for parameter dynamics, and examining effects of maximum likelihood estimation and alternative methods.

**结果:** Maximum likelihood estimation leads to convergence to absorbing states that amplify initial biases; these outcomes can be prevented by data pollution, MAP estimation, or regularisation. The asymptotic behavior is not reparametrisation invariant.

**结论:** Closed-loop learning dynamics need careful handling due to bias amplification, but mitigation strategies exist.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lost+in+Retraining%3A+Roaming+the+Parameter+Space+of+Exponential+Families+Under+Closed-Loop+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20623，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20623&send_immediately=true&force_search=false)

**原文摘要:** Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [20] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara, Shiva Raj Pokhrel*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于深度展开的新方法，使客户端能够自主优化超参数，从而在高度异构环境中实现约90%的准确率，显著优于传统方法的55%，并在基因表达分析和癌症检测等关键应用中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 客户端异质性对量子联邦学习（QFL）的性能提出了重大挑战，标准聚合方法通常失效。

**方法:** 通过利用深度展开技术，让客户端根据其特定的训练行为自主优化超参数（如学习率和正则化因子），并采用自适应精细调整的方法来缓解过拟合问题，确保鲁棒优化。

**结果:** 该框架在IBM量子硬件和Qiskit Aer模拟器上进行实时训练时，达到了约90%的准确率，而传统方法通常只能达到约55%的准确率。

**结论:** 这项研究解决了传统QFL的核心限制，提高了其在复杂挑战（如医疗保健和基因组研究）中的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是New+Insights+on+Unfolding+and+Fine-tuning+Quantum+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20016，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20016&send_immediately=true&force_search=false)

**原文摘要:** Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [21] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou, Nikhil Ghosh, Bin Yu*

**主要类别:** cs.LG

**AI概要:** Low-Rank Adaptation (LoRA) 是一种广泛使用的大型模型微调方法，具有较小的内存占用。通过设置学习率、秩和初始化等修改来提高其效率。此外，适配器放置策略也是一个改进方向。本文通过直观的理论分析引入了 PLoP（Precise LoRA Placement），这是一种轻量级方法，可以根据预训练模型和微调任务自动识别应放置 LoRA 适配器的模块类型。实验表明，PLoP 在监督微调和强化学习推理中始终优于常用放置策略，或至少与之竞争。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已经提出了多种方法来提高 LoRA 的效率，但关于适配器放置策略的研究较少，且结果不一致。这促使研究者探索一种更精确的方法来确定适配器的最佳放置位置。

**方法:** 研究者通过直观的理论分析提出了一种名为 PLoP（Precise LoRA Placement）的轻量级方法，该方法能够根据给定的预训练模型和微调任务自动识别适合放置 LoRA 适配器的模块类型。

**结果:** 通过全面的实验验证，PLoP 方法在监督微调和强化学习推理任务中表现出色，始终优于常见的放置策略，即使在最差情况下也能与之竞争。

**结论:** PLoP 提供了一种自动化的解决方案，以优化 LoRA 适配器的放置策略，从而进一步提升 LoRA 方法的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PLoP%3A+Precise+LoRA+Placement+for+Efficient+Finetuning+of+Large+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20629&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [22] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant, Rahul Bhope, Sharad Mehrotra, Christopher Tull, Nalini Venkatasubramanian*

**主要类别:** cs.LG

**AI概要:** DIM-SUM是一种预处理框架，用于训练稳健的时间序列填补模型。它结合了模式聚类和自适应掩码策略，在人工掩码数据和真实缺失模式之间架起桥梁。通过大量实验表明，DIM-SUM在减少处理时间、训练数据量以及提高推理精度方面优于传统方法和大型预训练模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有时间序列填补模型大多基于完整数据集并通过人工掩码模拟缺失值，但在实际基础设施监控中，数据缺失量大且模式复杂多样，这促使研究者开发一种能更好地适应真实世界缺失模式的模型。

**方法:** 提出DIM-SUM框架，该框架整合了模式聚类与自适应掩码策略，并具有理论学习保证，能够处理数据中实际观察到的各种缺失模式。

**结果:** 在超过20亿条来自加州水区、电力数据集及基准测试的读数上进行广泛实验，结果表明DIM-SUM用更少的处理时间和训练数据达到了与传统方法相似的精度，并且相比大型预训练模型，平均精度提高了2倍，同时推理时间显著缩短。

**结论:** DIM-SUM提供了一种有效的方法来应对真实世界中复杂的缺失数据模式，表现出更高的效率和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DIM-SUM%3A+Dynamic+IMputation+for+Smart+Utility+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20023，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20023&send_immediately=true&force_search=false)

**原文摘要:** Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [23] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao, Mehryar Mohri, Yutao Zhong*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了新的代理损失函数和高效算法，以优化多专家系统中的任务分配问题，并在单阶段和双阶段学习场景中提供了理论保证。


<details>
  <summary>更多</summary>
  
**动机:** 多专家系统中的任务分配需要平衡准确性和计算成本之间的权衡，这在自然语言生成、图像处理和医学诊断等领域是一个关键挑战。尽管已有研究提出了一些代理损失函数，但其一致性属性仍存在挑战。

**方法:** 作者引入了新的代理损失函数和高效算法，解决了可实现的H-一致性、H-一致性界限和贝叶斯一致性等问题。对于单阶段学习，提出了新的H-一致代理损失函数；对于双阶段学习，推导出了适用于两专家和多专家场景的代理损失函数。此外，在低噪声假设下提供了增强的理论保证。

**结果:** 通过实验验证了所提出的代理损失函数的性能，并与现有基线方法进行了比较，展示了其优越性。

**结论:** 新的代理损失函数和算法为多专家系统的任务分配问题提供了理论支持，并在单阶段和双阶段学习场景中表现良好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mastering+Multiple-Expert+Routing%3A+Realizable+%24H%24-Consistency+and+Strong+Guarantees+for+Learning+to+Defer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20650，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20650&send_immediately=true&force_search=false)

**原文摘要:** The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [24] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar, Ehsan Tarkesh Esfahani, Karthik Dantu, Souma Chowdhury*

**主要类别:** cs.LG

**AI概要:** 在灾害响应、搜救和军事任务中，涉及多个代理的操作需要自动化过程来支持行动路线（COA）的规划。此外，环境中的变化（如雨、雪、封锁等）可能影响COA的预期性能，因此需要有一组在代理间任务分布上多样化的COA。本文提出了一种新的理论公式和计算框架，用于生成具有软性代理-任务兼容性变化操作的多样化COA池。问题公式的关键是任务空间和COA池本身的图抽象，以量化其多样性。通过将COA公式化为集中式多机器人任务分配问题，使用遗传算法进行任务分配，从而联合最大化COA池内的多样性和整体代理-任务映射的兼容性。然后使用策略梯度方法训练图神经网络来进行单个代理任务排序，以最大化适应任务特征的完成率。我们在模拟环境中对COA生成过程的测试表明，相较于随机游走基线有显著的性能提升，任务排序的小优化差距，以及为5个代理/100个任务操作计划多达20个COA约50分钟的执行时间。


<details>
  <summary>更多</summary>
  
**动机:** 在灾害响应、搜救和军事任务中，涉及多个代理的操作需要自动化过程来支持行动路线（COA）的规划。环境变化可能影响COA的预期性能，因此需要一组在代理间任务分布上多样化的COA。此外，代理能力的变化也为规划过程带来了实际机会和计算挑战。

**方法:** 提出了一种新的理论公式和计算框架，将COA生成问题表示为集中式多机器人任务分配问题。使用遗传算法进行任务分配，联合最大化COA池内的多样性和整体代理-任务映射的兼容性。通过图神经网络进行单个代理任务排序，以最大化适应任务特征的完成率。

**结果:** 测试表明，相较于随机游走基线有显著的性能提升，任务排序的小优化差距，以及为5个代理/100个任务操作计划多达20个COAs约50分钟的执行时间。

**结论:** 本文提出的新理论公式和计算框架能够有效生成具有多样性的COA池，适用于具有软性代理-任务兼容性变化的操作，并在性能、优化差距和执行时间方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Generation+of+Diverse+Courses+of+Actions+for+Multi-Agent+Operations+using+Binary+Optimization+and+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20031，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20031&send_immediately=true&force_search=false)

**原文摘要:** Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [25] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri, Alex Davidson, Hamed Haddadi*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于零知识证明（zk-SNARKs）的验证框架，用于在边缘设备上确认数据遗忘操作，同时保护隐私并保持模型性能。该方法确保了可验证、隐私保护和有效的机器遗忘操作。


<details>
  <summary>更多</summary>
  
**动机:** 当前机器学习中，中心化模型分发到边缘设备后会使用本地数据进行个性化训练。然而，由于版权、偏见或法规要求，可能需要从所有边缘设备中删除某些数据样本。因此，确保边缘设备正确执行这些“遗忘”操作至关重要。

**方法:** 引入了一个基于零知识证明（特别是 zk-SNARKs）的验证框架，以确认边缘设备上的数据遗忘操作。开发了专门算法，使遗忘操作与高效的 zk-SNARK 证明生成兼容，并最小化计算和内存开销。此外，该方法保留了个性化增强功能，维持遗忘后的模型性能。

**结果:** 实验结果表明，该验证框架是实用且有效的，能够在几乎不降低个性化性能改进的情况下实现可验证的遗忘操作。

**结论:** 该方法实现了可验证、隐私保护和高效的机器遗忘操作，适用于边缘设备环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Verifiable+Unlearning+on+Edge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20037&send_immediately=true&force_search=false)

**原文摘要:** Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [26] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg, Xuemin Yu, Hassan Sajjad, Samira Ebrahimi Kahou*

**主要类别:** cs.LG

**AI概要:** 提出CLVQ-VAE框架，通过向量量化将跨层表示映射到紧凑、可解释的概念向量，并结合多种技术优化离散潜在空间的探索和码本多样性。


<details>
  <summary>更多</summary>
  
**动机:** 当前研究主要关注单层神经表示，忽视了跨层叠加和冗余问题。此外，现有方法通常直接分析激活模式或将表示映射到有限预定义概念，无法充分揭示大型语言模型中特征的演化过程。

**方法:** 提出CLVQ-VAE框架，利用向量量化将各层表示映射为紧凑且可解释的概念向量，同时结合top-k温度采样进行量化，并使用EMA更新码本以保持多样性。还采用scaled-spherical k-means++初始化码本，按方向相似性聚类以更好地与词嵌入空间中的语义结构对齐。

**结果:** 该方法能够有效压缩重复的残差流特征，提供对离散潜在空间的可控探索，并维持码本多样性，从而提升对大型语言模型中特征演化的理解。

**结论:** CLVQ-VAE框架提供了一种新的方式来分析跨层表示，解决了现有方法的局限性，为揭示大型语言模型中特征的演化过程提供了更好的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Layer+Discrete+Concept+Discovery+for+Interpreting+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20040&send_immediately=true&force_search=false)

**原文摘要:** Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [27] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard, Fazli Can*

**主要类别:** cs.LG

**AI概要:** The paper introduces LSH-DynED, a novel method combining Locality Sensitive Hashing with Random Hyperplane Projections for undersampling majority classes in multi-class imbalanced non-stationary data streams. It outperforms 15 state-of-the-art methods across 33 datasets in terms of Kappa and mG-Mean measures.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods for handling imbalanced data streams have mainly focused on binary classification tasks, with limited attention given to multi-class scenarios. The dynamic imbalance ratio in non-stationary data streams presents significant challenges that require a robust and resilient approach.

**方法:** The method integrates Locality Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic Ensemble Diversification (DynED) framework. It undersamples the majority classes using LSH-RHP to provide a balanced training set and improve ensemble prediction performance.

**结果:** LSH-DynED surpasses 15 state-of-the-art methods in experiments conducted on 23 real-world and ten semi-synthetic datasets. It demonstrates superior performance in terms of Kappa and mG-Mean effectiveness measures, particularly in large-scale, high-dimensional datasets with significant class imbalances.

**结论:** LSH-DynED is an effective solution for multi-class imbalanced non-stationary data streams, showing strong adaptation and robustness in real-world scenarios. The authors encourage future research and ensure reproducibility by releasing their implementation on GitHub.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LSH-DynED%3A+A+Dynamic+Ensemble+Framework+with+LSH-Based+Undersampling+for+Evolving+Multi-Class+Imbalanced+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20041，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20041&send_immediately=true&force_search=false)

**原文摘要:** The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [28] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar, Reza Samavi*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种基于知识蒸馏的新方法，通过自蒸馏和新的不确定性度量来高效且精确地量化图神经网络（GNNs）的预测不确定性。实验表明该方法在捕获模型预测不确定性方面效果显著，且性能与MC Dropout和集成方法相当。


<details>
  <summary>更多</summary>
  
**动机:** 尽管GNN在医疗领域表现优异，但量化其预测不确定性仍具挑战性。现有的贝叶斯和集成方法虽然可以用于量化不确定性，但计算成本高，并且集成方法中的分歧度量无法捕捉模型多样性。

**方法:** 提出了一种基于知识蒸馏的方法，利用自蒸馏（同一网络作为教师和学生模型），避免独立训练多个网络。同时开发了一种新的不确定性度量方法，通过为每个GNN分类器分配不同权重来捕捉网络的多样性。

**结果:** 实验结果表明，所提出的方法能够有效地捕获模型的预测不确定性，在区分分布外数据方面表现出色，且性能与MC Dropout和集成方法相似。

**结论:** 所提出的基于自蒸馏和新不确定性度量的方法可以更高效、更精确地量化GNN的预测不确定性，具有较高的实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GNN%27s+Uncertainty+Quantification+using+Self-Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20046&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [29] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了使用随机生成的数据预训练模型的方法，从算法复杂性的角度进行了理论验证，并通过实验证明合成数据可用于预训练，提升了模型在不同数据集上的零样本学习能力，同时在实际数据微调时表现出更快的收敛和更好的泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望探索一种无需依赖真实数据即可进行模型预训练的方法，从而降低对大规模标注数据的依赖，同时验证随机生成数据是否能有效提升模型性能。

**方法:** 作者基于算法复杂性理论，结合Solomonoff归纳法，证明了随机生成数据用于预训练的可行性。然后通过实验设计，比较了使用合成数据与真实数据预训练模型的性能差异，并测试了微调阶段的表现。

**结果:** 实验表明，使用合成数据预训练的模型能够在多种数据集上实现零样本学习，并且随着模型规模增大，性能进一步提升。此外，在真实数据上微调后，模型展现出更快的收敛速度和更好的泛化能力。

**结论:** 随机生成的数据可以作为一种有效的预训练方法，为模型提供先验知识，减少对真实数据的依赖，同时提升模型的泛化能力和学习效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+pre-training+by+iterated+random+computation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20057&send_immediately=true&force_search=false)

**原文摘要:** We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [30] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种利用大型语言模型（LLMs）自动从先前收集的智能体轨迹中生成开放性指令的新方法，通过重新标注未成功的轨迹来丰富训练数据，减少对人工标注的依赖，并学习统一的指令跟随策略。在Craftax环境中的实证评估显示，该方法在样本效率、指令覆盖范围和整体策略性能方面优于现有最佳基线。


<details>
  <summary>更多</summary>
  
**动机:** 当前强化学习中开发有效的指令跟随策略面临两个主要挑战：依赖大量人工标注的指令数据集以及从稀疏奖励中学习的困难。为了应对这些挑战，论文提出了利用LLMs的能力来自动创建开放性指令的方法。

**方法:** 利用LLMs回溯性地从之前收集的智能体轨迹中生成开放性指令，特别是为不成功的轨迹识别有意义的子任务并重新标注，从而丰富训练数据。通过这种方式，可以高效学习到一个能够处理多样化任务的统一指令跟随策略。

**结果:** 在Craftax环境中的实验结果表明，与现有最佳方法相比，所提出的方法在样本效率、指令覆盖范围和整体策略性能方面都有显著提升。

**结论:** 研究证明了使用LLM引导的开放性指令重新标注技术可以有效增强指令跟随强化学习的效果，为未来减少人工干预、提高学习效率的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Instruction-Following+Policies+through+Open-Ended+Instruction+Relabeling+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20061&send_immediately=true&force_search=false)

**原文摘要:** Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [31] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri, Sophia Tesic, Kayvan Najarian, Ryan Stidham*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的监督耦合矩阵-张量分解（SCMTF）方法，用于整合患者报告结果（PROs）、时间实验室数据和静态特征，以预测溃疡性结肠炎患者的药物依从性。该方法首次成功应用于UC领域和高度缺失的PRO数据，并通过低秩矩阵和张量分解方法提取了可解释的表型。


<details>
  <summary>更多</summary>
  
**动机:** 现有的计算表型方法通常不包括患者报告的症状（PROs），因为这些数据通常是噪声、主观且稀疏的。然而，在炎症性肠病中，症状量化对于理解患者体验至关重要。因此，需要一种方法来有效利用PROs数据进行预测和表型分析。

**方法:** 论文提出了一种新型的监督耦合矩阵-张量分解（SCMTF）方法，将时间性的PROs和实验室数据与静态特征相结合，用于预测溃疡性结肠炎患者的药物依从性。同时，使用深度学习框架使模型更灵活且易于训练，从而处理PROs中的大量缺失数据。

**结果:** 最佳模型在测试集上分别能够提前8个月和20个月预测药物变化，AUC分别为0.853和0.803。此外，该方法提取了包含静态特征和时间特征（包括其时间模式）的可解释表型，并证明PROs数据包含相关的信息，可用于预测药物依从性。

**结论:** 低秩矩阵和张量分解方法可以成功应用于溃疡性结肠炎领域和高度缺失的PRO数据。通过该方法识别出的表型有助于预测药物依从性，并表明PROs数据包含有价值的信息，值得进一步利用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supervised+Coupled+Matrix-Tensor+Factorization+%28SCMTF%29+for+Computational+Phenotyping+of+Patient+Reported+Outcomes+in+Ulcerative+Colitis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20065，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20065&send_immediately=true&force_search=false)

**原文摘要:** Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [32] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi, Dongchan Kim, Muhammad Arif*

**主要类别:** cs.LG

**AI概要:** 本文综述了预测性维护(PdM)方法，重点比较了回归和分类方法在故障预测中的应用，并探讨了数据不平衡、高维特征空间等挑战及混合方法和AI支持系统等趋势。


<details>
  <summary>更多</summary>
  
**动机:** 尽管许多研究已经针对PdM展开，但尚未有独立的回归与分类方法的比较研究。

**方法:** 通过全面分析近期文献，对比回归和分类方法在预测性维护中的应用，包括对RUL估计和故障概率预测的不同侧重。

**结果:** 强调了两种方法的优势与妥协，提出了未来研究方向，如公共数据集、基准测试平台和开源工具的系统评估。

**结论:** 本研究为研究人员和实践者提供了不同PdM方法的优缺点认知，有助于构建更强大的自适应维护系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+Predictive+Maintenance+Methods%3A+An+Analysis+of+Prognostics+via+Classification+and+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20090，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20090&send_immediately=true&force_search=false)

**原文摘要:** Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [33] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty, Walid A. Hanafy, Kaan Ozkara, Qianlin Liang, Jesse Milzman, Prashant Shenoy, Suhas Diggavi*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为Multi-Level Ensemble Learning (MEL)的新框架，用于提高边缘推理的弹性。该方法通过同时训练多个轻量级备份模型，在服务器故障时能够保持良好的准确性，并在不同数据集上的实验证明了其性能和容错能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前边缘环境下的AI推理服务受到功率、资源限制以及易受故障影响的问题困扰。传统的故障恢复方法（如云故障转移或压缩备份）通常会在延迟或准确性上做出妥协，从而限制了其在关键边缘推理服务中的有效性。因此，需要一种新的方法来解决这些问题。

**方法:** 提出了Multi-Level Ensemble Learning (MEL)框架，通过同时训练多个轻量级备份模型，这些模型可以在多服务器环境下相互精炼，也可以在单个服务器故障时独立运行并保持良好准确性。方法被表述为一个多目标优化问题，损失函数鼓励模型间的多样性以促进相互精炼的表示，同时确保每个模型都有良好的独立性能。

**结果:** 在视觉、语言和音频数据集上的实证评估表明，MEL提供的性能与原始架构相当，同时提供了容错能力和在边缘平台上的部署灵活性。实验结果还显示，使用MEL训练的集成模型大小仅为原始模型的40%，却能实现类似的性能，并在故障情况下保留95.6%的集成准确性。

**结论:** Multi-Level Ensemble Learning (MEL)是一种有效的框架，能够在保证性能的同时提供容错能力和灵活的边缘部署选项，适用于资源受限的边缘环境下的AI推理任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MEL%3A+Multi-level+Ensemble+Learning+for+Resource-Constrained+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20094，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20094&send_immediately=true&force_search=false)

**原文摘要:** AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [34] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson, Gabriel Tseng, Yawen Zhang, Heather Heward, Virginia Sjahli, Favyen Bastani, Joseph Redmon, Patrick Beukema*

**主要类别:** cs.LG

**AI概要:** 论文探讨了使用预训练的多模态地球观测模型生成大规模、空间完整的实时燃料湿度含量（LFMC）地图的方法，相比随机初始化模型方法显著降低了RMSE，并提供了自动化管道以快速生成美国的LFMC地图，展示了其在受野火影响区域的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 野火的强度和严重程度正在迅速增加，而现有的基于地面的实时燃料湿度含量（LFMC）样本获取方式耗时且成本高昂，导致数据更新稀疏而不及时。利用AI和公开可用的卫星数据可以实现对全球野火风险因素的高分辨率、低延迟监测。

**方法:** 研究采用了预训练的高度多模态地球观测模型来生成大规模的空间完整（全面覆盖）的LFMC地图。这种方法通过自动化管道实现了这些地图在美国范围内的快速生成。

**结果:** 该方法相较于使用随机初始化模型的方法，在均方根误差（RMSE）上减少了20%。通过在两个近期受到野火影响的地区（Eaton和Palisades）进行了有效性的展示。

**结论:** 预训练的多模态地球观测模型能够显著提高生成LFMC地图的精度，并为野火研究和应急响应提供有力支持。此方法及自动化管道可快速生成大范围LFMC地图，有助于改进野火风险管理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High-Resolution+Live+Fuel+Moisture+Content+%28LFMC%29+Maps+for+Wildfire+Risk+from+Multimodal+Earth+Observation+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20132，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20132&send_immediately=true&force_search=false)

**原文摘要:** Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [35] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi, Arun K. Tangirala*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的因果发现方法，即变量划分（PoV）方法，适用于线性时不变微分代数系统（LTI-DAE）。该方法通过动态迭代主成分分析（DIPCA）确定代数和动态关系的数量及约束矩阵，并通过计算条件数进行可接受的分区，从而识别出最小子集中的因果驱动因素。相比Kathari和Tangirala（2022）提出的方法，PoV方法在处理纯动力学系统和包含代数方程的系统方面表现更优。案例研究证明了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的因果发现方法如Kathari和Tangirala（2022）提出的动态迭代主成分分析（DIPCA）方法虽然对纯动力学系统有效，但在处理受反馈控制或与守恒定律耦合的微分-代数系统时存在局限性。因此需要一种新方法来解决这一问题。

**方法:** 论文提出了一种名为变量划分（PoV）的新方法。该方法首先使用DIPCA确定系统的代数关系数量（$n_a$）、动态关系数量（$n_d$）以及约束矩阵；然后通过计算约束矩阵的条件数进行可接受的分区，从而识别出最小子集中的因果驱动因素。

**结果:** PoV方法可以有效地识别出线性时不变微分代数系统（LTI-DAE）中的因果驱动因素，包括那些包含代数方程的系统。案例研究表明，该方法优于现有方法，尤其在处理复杂系统时表现出色。

**结论:** 变量划分（PoV）方法是一种有效的因果发现工具，适用于线性时不变微分代数系统（LTI-DAE），并且在处理纯动力学系统和混合因果系统时均表现出优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+discovery+in+deterministic+discrete+LTI-DAE+systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20169&send_immediately=true&force_search=false)

**原文摘要:** Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [36] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种结合物理信息神经网络和反事实扰动发现偏微分方程因果结构的框架。该方法通过功能干预量化算子级别的必要性，并引入因果敏感指数和结构偏差度量来评估候选微分算子的影响。理论上，证明了在限制等距或互相关条件下能够精确恢复因果算子支持，并保证可识别性。实证上，在气候动力学、肿瘤扩散和海洋流动等多个数据集上验证了框架的有效性。该方法在噪声、冗余和数据稀缺的情况下仍能准确恢复控制算子，优于标准PINNs和DeepONets。


<details>
  <summary>更多</summary>
  
**动机:** 现有的残差最小化或稀疏回归方法无法有效量化偏微分方程中算子级别的必要性，因此需要一种新的方法来发现PDE中的因果结构。

**方法:** 利用物理信息神经网络和反事实扰动构建一个原则性的框架，通过功能干预量化算子级别的必要性，并引入因果敏感指数和结构偏差度量来评估候选微分算子的影响。

**结果:** 该方法在合成和真实世界的数据集上进行了验证，即使在噪声、冗余和数据稀缺的情况下也能一致地恢复控制算子，且在结构保真度方面优于标准PINNs和DeepONets。

**结论:** 提出的因果PDE发现方法是一种可行且可解释的推理任务，基于结构因果模型和变分残差分析，为发现PDE因果结构提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Operator+Discovery+in+Partial+Differential+Equations+via+Counterfactual+Physics-Informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20181&send_immediately=true&force_search=false)

**原文摘要:** We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [37] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin, Yuhang Li, Donghyun Lee, Priyadarshini Panda*

**主要类别:** cs.LG

**AI概要:** 大型语言模型（LLMs）尽管性能强大，但因高内存和计算成本难以部署。虽然剪枝可以减少这些需求，但大多数方法忽略了运行时的激活稀疏性。本文将激活稀疏性重新解释为动态结构化的权重稀疏性，并提出了DuoGPT框架，通过结合非结构化权重剪枝与激活稀疏性构建双重稀疏工作负载。为了保持精度，扩展了最优大脑压缩（OBC）框架，引入了来自密集模型的输出残差作为校正项。进一步优化了解决方案以实现高效的GPU执行，使得能够扩展到十亿参数级别的LLMs。在LLaMA-2和LLaMA-3上的评估显示，DuoGPT在保持1.39倍加速的同时，比现有的最佳结构化剪枝方法提高了多达9.17%的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型虽然性能强大，但其高内存和计算成本限制了实际应用中的广泛部署。传统的剪枝方法虽能减少资源需求，但未能充分利用运行时的激活稀疏性这一特性。因此，研究团队希望开发一种新的方法，能够同时利用权重剪枝和激活稀疏性来降低模型的资源消耗，同时保持或提升模型的性能。

**方法:** 提出了一种名为DuoGPT的统一框架，该框架通过结合非结构化权重剪枝与激活稀疏性构建双重稀疏工作负载。具体而言，研究团队将激活稀疏性重新定义为动态结构化的权重稀疏性。此外，为了确保模型精度不下降，他们扩展了最优大脑压缩（OBC）框架，增加了激活感知校准步骤，并引入了来自密集模型的输出残差作为校正项。最后，针对高效的GPU执行进行了优化，使该方法能够扩展到具有数十亿参数的大规模语言模型。

**结果:** 在LLaMA-2和LLaMA-3模型上的实验结果表明，DuoGPT在保持1.39倍加速的情况下，相比基线密集模型，准确率提升了高达9.17%，并且显著优于当前最先进的结构化剪枝方法。

**结论:** DuoGPT框架通过结合非结构化权重剪枝和激活稀疏性，成功降低了大型语言模型的计算和内存需求，同时保持甚至提升了模型的性能。这种方法不仅在资源效率上表现出色，而且在大规模模型的应用中展现了巨大的潜力，为未来高效部署大型语言模型提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DuoGPT%3A+Training-free+Dual+Sparsity+through+Activation-aware+Pruning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20194，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20194&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [38] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne, Yash Pote, Uddalok Sarkar*

**主要类别:** cs.LG

**AI概要:** 随着大型语言模型（LLMs）生成代码的使用增加，研究者们提出了一个名为Anubis的工具，用于通过假设检验技术来归属这些代码。Anubis在区分不同LLM生成的代码时表现出色，仅需约2000个样本即可达到高AUROC分数（≥0.9）。


<details>
  <summary>更多</summary>
  
**动机:** 越来越多的代码来自于大型语言模型（LLMs），因此需要一种方法来归属这些代码的来源。

**方法:** 提出了一种称为Anubis的零样本归属工具，将归属问题转化为分布测试问题，并利用假设检验技术和来自LLM的样本与密度估计。

**结果:** 在代码样本基准测试中，Anubis能够以高AUROC分数（≥0.9）区分不同的LLM生成的代码，如DeepSeek-Coder、CodeGemma和Stable-Code。

**结论:** Anubis提供了一种有效的方法来归属由LLM生成的代码，仅需少量样本即可实现高精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Attribution+for+Large+Language+Models%3A+A+Distribution+Testing+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20197&send_immediately=true&force_search=false)

**原文摘要:** A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [39] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro, Hadi Banaee, Amy Loutfi*

**主要类别:** cs.LG

**AI概要:** 研究提出了一种名为情感启动评分（APS）的数据驱动方法，用于检测受启动效应影响的数据点，并通过实验验证了其有效性，证明去除启动效应可以显著降低模型的误分类率。


<details>
  <summary>更多</summary>
  
**动机:** 当前情感计算领域主要从基于标签的角度解决模糊性问题，但对于数据本身（特别是生理信号）受启动效应的影响研究不足，可能导致学习模型中的误分类。

**方法:** 提出一种数据驱动的方法——情感启动评分（APS），为每个数据点分配一个分数，以量化其受启动效应影响的程度。将该方法应用于SEED和SEED-VII数据集，使用相同配置训练模型，比较原始数据和无启动序列的效果。

**结果:** 使用无启动序列进行模型训练时，相比使用原始数据，误分类率显著降低。

**结论:** 本研究通过识别和减轻数据层面的启动效应，有助于应对情感计算中的模糊性挑战，提升模型的鲁棒性，并为情感计算数据集的设计和收集提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Affective+Priming+Score%3A+A+Data-Driven+Method+to+Detect+Priming+in+Sequential+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20204&send_immediately=true&force_search=false)

**原文摘要:** Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [40] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang, Xu Shen, Yu Xie, Ka-Chun Wong, Weidun Xie, Chengbin Peng*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的GNN框架，融合特征嵌入与社区信息，并通过将输入图转换为有向线图来提升链接预测性能。实验表明，在使用30%-60%连接链路作为训练数据时，该方法在大多数情况下优于现有最佳方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前的深度学习方法通常通过对比学习分析节点相似性，并通过图卷积聚合邻域信息，但未充分结合特征嵌入和社区信息对有向图链接预测的影响。

**方法:** 1. 提出一种新的GNN框架，将特征嵌入与社区信息相融合。
2. 理论上证明了混合特征对有向链接预测性能的提升作用。
3. 提出一种方法，将输入图转换为有向线图，以使节点在图卷积过程中能聚合更多信息。

**结果:** 在多个基准数据集上的实验结果表明，当分别使用30%、40%、50%和60%的连接链路作为训练数据时，所提出的方法在大多数情况下优于现有最佳方法。

**结论:** 融合特征嵌入与社区信息的GNN框架以及将输入图转换为有向线图的方法可以有效提升有向链接预测的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Directed+Link+Prediction+using+GNN+with+Local+and+Global+Feature+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20235&send_immediately=true&force_search=false)

**原文摘要:** Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [41] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao, Jinyuan He, Donglai Chen, Weijie Luo, Chong Xie, Ri Zhang, Yonghong Chen, Yan Xu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的无需数据的蒸馏框架FedBKD，解决了联邦学习中non-IID数据的问题，同时提高了全局和本地模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习面临non-IID数据处理、全局模型泛化能力以及本地模型性能提升的挑战，现有方法难以同时解决这些问题，并且引入公共数据集增加了数据泄露风险。

**方法:** 提出了Federated Bidirectional Knowledge Distillation (FedBKD)框架，利用生成对抗网络（GAN）合成数据进行双向蒸馏，通过冻结本地模型参数作为判别器，实现全局与本地模型之间的知识交互。

**结果:** 在4个基准数据集的不同non-IID设置下进行了广泛的实验，结果表明FedBKD在每种情况下都达到了最佳性能。

**结论:** FedBKD框架无需公共数据集，有效提升了联邦学习中全局和本地模型的性能，解决了non-IID数据带来的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedBKD%3A+Distilled+Federated+Learning+to+Embrace+Gerneralization+and+Personalization+on+Non-IID+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20245，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20245&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [42] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song*

**主要类别:** cs.LG

**AI概要:** 量化的大规模语言模型（LLMs）在资源受限环境中部署的重要性日益提升，但其安全性可能因量化过程而受到损害。本文通过系统评估不同主流量化技术的安全性，并提出Q-resafe框架以恢复量化LLM的安全能力，同时最小化对效用的影响。实验表明，Q-resafe能有效恢复量化LLM的安全性至量化前水平。


<details>
  <summary>更多</summary>
  
**动机:** 量化LLMs有助于在资源受限环境下部署，然而一些无需校准数据集的量化方法可能会削弱LLMs的安全性能，因此需要进行系统的安全评估并开发有效的缓解策略。

**方法:** 研究者们进行了全面的安全评估，涵盖多种主流量化技术和不同的校准数据集，并使用广泛认可的安全基准。基于此，他们提出了一个量化感知的安全修补框架Q-resafe，旨在高效恢复量化LLM的安全能力，同时尽量减少对其实用性的影响。

**结果:** 广泛的实验结果证明，Q-resafe能够在具有挑战性的评估场景下，成功使量化LLM的安全性重新与量化前的水平对齐。

**结论:** Q-resafe提供了一种有效的方法来恢复量化LLM的安全性能，同时确保其效用不受显著影响，这对于推动安全且高效的量化LLM部署具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Q-resafe%3A+Assessing+Safety+Risks+and+Quantization-aware+Safety+Patching+for+Quantized+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20251&send_immediately=true&force_search=false)

**原文摘要:** Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [43] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards, Nikita Popkov, Annekatrin König, Marcel Arpogaus, Bastian Schäfermeier, Leonie Riedl, Stephan Vogt, Philip Hehlert*

**主要类别:** cs.LG

**AI概要:** 这篇论文深入评估了用于长期个体电力消耗预测的数据驱动方法，包括WGAN、DDPM、HMM和MABF等技术，以生成高保真的合成时间序列数据。


<details>
  <summary>更多</summary>
  
**动机:** 目前大多数研究集中于短期发电或消费预测，且更多关注系统而非个人消费者。而针对长期个人电力消耗预测的研究相对较少。因此，本研究旨在通过对比多种先进的数据生成方法来解决这一问题。

**方法:** 使用了四种先进技术：WGAN（混合Wasserstein生成对抗网络）、DDPM（去噪扩散概率模型）、HMM（隐马尔可夫模型）和MABF（基于掩码自回归Bernstein多项式流的归一化流）。这些方法被用来生成符合个体能源消耗特征的合成时间序列数据，并分析其对时间动态、长程依赖性和概率转换的复制能力。

**结果:** 通过对德国15分钟分辨率的家庭公开数据集进行实验，比较了各方法在生成高保真合成数据方面的性能，揭示了每种方法的优势和局限性。

**结论:** 该研究提供了一种生成和分析框架，可以提高合成电力消耗数据的准确性和可靠性，同时满足匿名化需求以保护隐私。生成的合成数据可直接应用于状态估计或消费预测等相关任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time-series+surrogates+from+energy+consumers+generated+by+machine+learning+approaches+for+long-term+forecasting+scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20253，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20253&send_immediately=true&force_search=false)

**原文摘要:** Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [44] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang, Antonio Rago, Francesco Leofante, Francesca Toni*

**主要类别:** cs.LG

**AI概要:** 在机器学习中，对于相同的预测任务，通常会得到多个性能相当的模型（例如使用不同的随机种子训练神经网络）。当这些竞争模型对相同输入的预测结果不同时，就会出现模型多重性（Model Multiplicity, MM）问题。为了在这种情况下提供有效的反事实解释（Counterfactual Explanations, CEs），本文提出了应对MM的解决方案——Recourse-Aware Ensembling (RAE)。我们提出了一种新颖的基于论辩的集成方法，该方法通过显式表示模型和反事实之间的冲突，并利用论辩语义解决这些冲突，从而保证了CEs在MM下的鲁棒性。此外，我们的方法允许对MM下的模型进行偏好指定，以进一步定制集成。我们通过理论分析和实验证明了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 在模型多重性（MM）的情况下，现有的反事实解释（CEs）可能并不适用于所有模型，因此需要一种新的方法来确保在MM下生成的CEs具有鲁棒性。

**方法:** 作者提出了Recourse-Aware Ensembling (RAE)，这是一种基于论辩的集成方法。该方法通过计算论辩来明确表示模型和反事实之间的冲突，并利用论辩语义解决这些冲突。最终输出的结果是综合考虑了预测和反事实解释后的解决方案。此外，该方法还支持根据用户需求对模型进行偏好设置。

**结果:** 理论分析表明，基于论辩的集成方法在四种不同的论辩语义下表现良好。实验部分展示了该方法的八个实例化版本在满足期望属性方面的有效性。

**结论:** 本文提出的Recourse-Aware Ensembling (RAE) 方法为在模型多重性（MM）下的反事实解释提供了鲁棒性保障，并且通过理论和实验证明了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Argumentative+Ensembling+for+Robust+Recourse+under+Model+Multiplicity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20260，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20260&send_immediately=true&force_search=false)

**原文摘要:** In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [45] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng, Chunxu Zhang, Guodong Long, Riting Xia, Bo Yang*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的联邦学习框架，通过知识蒸馏生成一个通用专家模型，以解决现有方法忽视跨集群共享信息的问题。该框架包含三个迭代步骤：本地模型训练、集群特定模型聚合和通用专家蒸馏。实验结果表明，该方法在不同场景下表现出优越性能，能够更有效地平衡个性化和共享知识。


<details>
  <summary>更多</summary>
  
**动机:** 现有的集群联邦学习（CFL）方法虽然可以应对非独立同分布（non-IID）数据的挑战，但往往忽略了跨集群的共享信息，这些信息代表了对所有联邦学习系统参与者都有价值的可泛化知识。

**方法:** 1. 本地模型训练：每个客户端进行本地模型训练。
2. 集群特定模型聚合：将本地模型聚合为集群特定模型。
3. 通用专家蒸馏：从多个集群的知识中提取通用专家模型，并将其分发给每个客户端作为下一轮模型训练的初始化。
这种方法保留了细粒度的非-IID特性，同时有效整合了跨集群的共享知识。

**结果:** 与传统的基于梯度的聚合方法相比，该方法在处理模型异构性方面具有更大的灵活性，并减少了集群特定专家之间的冲突。广泛的实验结果表明，该方法在各种场景下均表现出优越的性能。

**结论:** 该研究提出的新框架通过引入通用专家蒸馏步骤，更有效地平衡了个性化和共享知识，从而推动了集群联邦学习的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distilling+A+Universal+Expert+from+Clustered+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20285&send_immediately=true&force_search=false)

**原文摘要:** Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [46] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda, Kazuhiko Kawamoto, Hiroshi Kera*

**主要类别:** cs.LG

**AI概要:** 本文探讨了基于学习的QR码解码，并研究了中等敏感度的学习函数。实验表明，Transformer可以通过学习嵌入文本的结构成功解码QR码，甚至超出理论纠错极限。此外，Transformer解码器主要关注数据位而忽略纠错位。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探索不同输入敏感度任务的学习特性，特别是介于图像分类（低敏感度）和算术/符号计算（高敏感度）之间的任务，例如QR码解码。

**方法:** 作者使用Transformers进行QR码解码，通过学习嵌入文本的结构来实现。模型在包含英语丰富的训练数据上进行训练，并测试其对其他语言和随机字符串的泛化能力。

**结果:** Transformer能够成功解码QR码，包括超出理论纠错极限的情况。模型可以泛化到其他语言和随机字符串，并且更关注数据位而非纠错位。

**结论:** 本研究表明，Transformers可以有效处理中等敏感度的任务，如QR码解码，并且采用与传统方法不同的机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Moderately+Input-Sensitive+Functions%3A+A+Case+Study+in+QR+Code+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20305，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20305&send_immediately=true&force_search=false)

**原文摘要:** The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [47] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao, Xingrui Yu, David M. Bossens, Ivor W. Tsang, Quanquan Gu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的模仿学习算法ILDE，通过双探索机制（乐观策略优化和好奇心驱动探索）在样本效率上超越现有算法，并在Atari和MuJoCo任务中用更少的演示实现超越专家的表现。理论分析表明，该方法是一种带有乐观探索的不确定性正则化策略优化方法，其遗憾值随剧集数量亚线性增长。


<details>
  <summary>更多</summary>
  
**动机:** 在强化学习中，模仿学习旨在学习模仿专家行为的策略。然而，由于状态空间的复杂性，从有限数量的演示中准确学习专家策略具有挑战性。此外，为了超越专家表现，还需要探索环境并收集数据。

**方法:** 提出了一种名为ILDE的新模仿学习算法，该算法通过两种方式实施探索：(1) 通过探索奖励（针对高不确定性状态-动作对）进行乐观策略优化，以潜在地改进向专家策略的收敛；(2) 好奇心驱动的探索（偏离演示轨迹的状态），以潜在地获得超越专家的表现。

**结果:** 实验表明，与现有的模仿学习算法相比，ILDE在样本效率方面表现出色，并在Atari和MuJoCo任务中使用比先前工作更少的演示实现了超越专家的表现。

**结论:** ILDE被证明是一种有效的模仿学习算法，在样本效率和超越专家表现方面具有优势。理论上，ILDE可以被视为一种带有乐观探索的不确定性正则化策略优化方法，其遗憾值随剧集数量亚线性增长。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond-Expert+Performance+with+Limited+Demonstrations%3A+Efficient+Imitation+Learning+with+Double+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20307&send_immediately=true&force_search=false)

**原文摘要:** Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [48] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam, Shalini Majumdar, Shantanu Nadar, Kaustubh Kulkarni*

**主要类别:** cs.LG

**AI概要:** This research develops an AI-driven crop disease detection system for farmers in rural areas with limited resources, comparing deep learning models including EfficientNet, ResNet101, MobileNetV2, and a custom CNN for their efficacy in transfer learning. The custom CNN achieved a validation accuracy of 95.76%, demonstrating the potential of transfer learning to improve agricultural practices.


<details>
  <summary>更多</summary>
  
**动机:** To assist farmers in rural areas with limited resources by developing an effective crop disease detection system using AI.

**方法:** The research compares different deep learning models (EfficientNet, ResNet101, MobileNetV2, and a custom CNN) focusing on their efficacy in transfer learning to classify plant diseases.

**结果:** The custom CNN model achieved a validation accuracy of 95.76% in classifying plant diseases.

**结论:** Transfer learning has great potential in reshaping agricultural practices, enhancing crop health management, and supporting sustainable farming in rural environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+Analysis+of+Deep+Learning+Models+for+Crop+Disease+Detection%3A+A+Transfer+Learning+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20323&send_immediately=true&force_search=false)

**原文摘要:** This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [49] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt, Benjamin Walker, Tiexin Qin, Jan Stühmer, Andrey Kormilitzin*

**主要类别:** cs.LG

**AI概要:** 动态图由于节点特征的演变和网络结构的变化表现出复杂的时序动力学。本文在Graph Neural CDEs的基础上，引入了Permutation Equivariant Neural Graph CDEs，通过将模型投影到置换等变函数空间，减少参数数量并提高泛化能力，在模拟动力系统和实际任务中均展现出更好的插值和外推性能。


<details>
  <summary>更多</summary>
  
**动机:** 动态图的时间动力学复杂，现有方法虽然取得一定成功，但模型参数量大且训练效率低。

**方法:** 提出了一种新的模型——Permutation Equivariant Neural Graph CDEs，该模型通过将Graph Neural CDEs投影到置换等变函数空间，减少了参数数量而不降低表示能力。

**结果:** 实验结果表明，新方法在模拟动力系统和真实世界任务中均提高了插值和外推性能，同时训练更高效、泛化能力更强。

**结论:** Permutation Equivariant Neural Graph CDEs在保持表示能力的同时显著减少了参数数量，提升了训练效率和泛化性能，适用于动态图的时间序列建模任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Permutation+Equivariant+Neural+Controlled+Differential+Equations+for+Dynamic+Graph+Representation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20324，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20324&send_immediately=true&force_search=false)

**原文摘要:** Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [50] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio, Marta Soare, Sihem Amer-Yahia*

**主要类别:** cs.LG

**AI概要:** 本文探讨了顺序捆绑推荐中的公平性问题，提出了生产者公平性的概念，并研究了在不影响捆绑包质量的前提下提供公平推荐的几种方法。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界场景中，不同的项目组在推荐会话中需要获得期望的曝光度，因此需要解决推荐系统中的生产者公平性问题。

**方法:** 提出了一种精确解决方案用于小型实例问题，并考察了两种启发式方法（质量优先和公平性优先）以及一种自适应变体，该变体能够在运行时动态确定公平性和质量之间的适当平衡。

**结果:** 通过在三个真实数据集上的实验，突显了每种解决方案的优势和局限性，证明了这些方法在提供公平捆绑推荐的同时不会影响捆绑包的质量。

**结论:** 提出的解决方案能够在顺序捆绑推荐中实现生产者公平性，并且可以与构建高质量捆绑包的目标自然结合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Producer-Fairness+in+Sequential+Bundle+Recommendation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20329&send_immediately=true&force_search=false)

**原文摘要:** We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [51] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu*

**主要类别:** cs.LG

**AI概要:** 针对大型语言模型（LLMs）的计算需求和部署成本不断增加的问题，本文提出了一种双层重要性保护机制（DipSVD），以增强基于奇异值分解（SVD）的压缩方法。该机制包括局部重要性保护和全局重要性保护，通过通道加权数据白化保留关键奇异向量，并让不重要的层承担更多的压缩负担，从而减少对关键层的影响。实验表明，DipSVD在多个基准测试中优于现有的SVD压缩方法，特别是在高模型压缩比下表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型的计算需求和部署成本不断增长，推动了多种压缩方法的发展。然而，现有的基于SVD的压缩方法虽然具有良好的硬件兼容性和理论保证，但在保护矩阵中的关键组件方面存在不足，导致压缩模型性能较差。

**方法:** 提出了一种双层重要性保护机制（DipSVD），包括：1. 局部重要性保护：通过通道加权数据白化保留每个权重矩阵中最关键的奇异向量；2. 全局重要性保护：通过启发式或优化方法，使不重要的层承担更大的压缩负担，从而最小化压缩对关键层的影响。

**结果:** 广泛的实验证明，DipSVD在多个基准测试中优于现有的SVD压缩方法，在高模型压缩比下尤其表现出色，实现了更好的模型性能。

**结论:** DipSVD通过引入双层重要性保护机制，显著提高了基于SVD的压缩方法的效果，尤其是在高压缩比情况下，为大型语言模型的高效部署提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DipSVD%3A+Dual-importance+Protected+SVD+for+Efficient+LLM+Compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20353，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20353&send_immediately=true&force_search=false)

**原文摘要:** The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [52] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga, Michael Hersche, Abu Sebastian, Kaspar Schindler, Abbas Rahimi*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的自注意力机制——多变量平行注意力（MVPA），并基于此构建了MVPFormer模型，用于预测人类脑电图信号的演变。该模型在癫痫发作检测等任务上表现出色，并且适用于异构时间序列数据。作者还发布了目前最大的公开iEEG数据集SWEC，以支持未来的研究。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络在处理具有异构通道配置的多变量时间序列数据时面临挑战，特别是在临床领域如颅内脑电图（iEEG）中，不同受试者的通道设置差异很大。因此，需要一种能够灵活、通用且高效地建模这些数据的方法。

**方法:** 论文引入了多变量平行注意力（MVPA），这是一种新颖的自注意力机制，可以分离内容、时间和空间注意力。基于MVPA，构建了MVPFormer模型，用于预测跨不同受试者的iEEG信号演变。此外，作者还发布了名为SWEC的大型公开iEEG数据集。

**结果:** MVPFormer在跨受试者泛化方面表现出色，在癫痫发作检测任务中达到专家级水平，并在SWEC、MAYO和FNUSA数据集上超越现有的Transformer基线模型。MVPA在标准时间序列预测和分类任务中也匹配或超过了现有基于注意力的模型。

**结论:** MVPA被确立为一种适用于异构时间序列数据的通用注意力机制，而MVPFormer则成为首个开源、开放权重和开放数据的iEEG基础模型，具有最先进的临床性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+foundation+model+with+multi-variate+parallel+attention+to+generate+neuronal+activity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20354&send_immediately=true&force_search=false)

**原文摘要:** Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [53] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini, Stephane Marchand-Maillet*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的自监督图学习框架LaplaceGNN，通过利用谱引导优化和对抗引导训练方案，无需负采样或对比目标即可捕获丰富的结构表示，并在多个基准数据集上表现出优于现有方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前的图学习方法通常依赖于对比目标或手工增强，需要负采样且效率较低，因此需要一种更简单、更有效的自监督方法来捕获图的结构信息。

**方法:** LaplaceGNN通过将拉普拉斯信号整合到学习过程中，采用正对齐策略实现线性扩展；预先计算基于最大最小中心性优化的谱增强，提供丰富的结构监督；同时引入对抗引导的训练方案以加强特征学习和鲁棒性。

**结果:** 在不同基准数据集上的广泛实验证明，LaplaceGNN相比最先进的自监督图学习方法表现出更优的性能。

**结论:** LaplaceGNN为高效学习表达性图表示提供了一个有前途的方向，无需依赖对比目标或手工增强，具有广泛的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Supervised+Graph+Learning+via+Spectral+Bootstrapping+and+Laplacian-Based+Augmentations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20362，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20362&send_immediately=true&force_search=false)

**原文摘要:** We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [54] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage, Dhruv Gulabani*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种基于分类法的特征选择方法，将特征分为几何和运动学特征，并进一步细分为曲率、凹度、速度和加速度。该方法提高了预测性能，减少了特征选择的时间，增加了数据的可解释性，降低了维度和计算复杂度，为轨迹数据分析提供了方法论框架。


<details>
  <summary>更多</summary>
  
**动机:** 轨迹分析不仅涉及获取运动数据，还对理解物体在时空中的移动模式以及预测其下一步动作至关重要。然而，高维特征爆炸问题降低了数据效率和机器学习模型的准确性，因此需要一种有效的特征选择方法。

**方法:** 本研究引入了一种基于分类法的特征选择方法，根据特征的内部结构对其进行分类。具体来说，将数据分为几何特征和运动学特征，并进一步细分为曲率、凹度、速度和加速度。

**结果:** 实验结果表明，基于分类法的方法在预测性能上达到或优于其他方法。此外，由于分类组合空间减小，特征选择所需时间大幅减少，同时可以揭示不同数据集对特征集合的敏感性。

**结论:** 基于分类法的特征选择方法可以增加数据的可解释性，降低维度和计算复杂度，有助于高层次决策制定，为轨迹数据分析提供了一个方法论框架，推动了可解释人工智能领域的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Interpretable+and+Efficient+Feature+Selection+in+Trajectory+Datasets%3A+A+Taxonomic+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20359，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20359&send_immediately=true&force_search=false)

**原文摘要:** Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [55] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri, Denys Herasymuk, Hamed Haddadi*

**主要类别:** cs.LG

**AI概要:** 在物联网生态系统中，人工智能的日益普及加剧了对能够在异构、资源受限设备上高效和私密运行的个性化学习方法的需求。本文提出了一种名为P4的方法，解决了去中心化环境中个性化学习面临的挑战，包括客户间的高效知识转移、数据隐私保护和对投毒攻击的抵御能力。实验结果表明，P4比领先的差分隐私点对点方法提高了5%到30%的准确性，并且在高达30%的恶意客户端存在下仍能保持鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI在IoT生态系统的应用增加，需要高效的个性化学习方法，这些方法可以在异构、资源受限的设备上运行，同时保护数据隐私并抵御投毒攻击。

**方法:** 开发了P4方法，采用轻量级、完全去中心化的算法来检测客户相似性并形成协作组。组内客户使用差分隐私知识蒸馏共同训练模型，以保持高准确度并抵御恶意客户的影响。

**结果:** 实验结果显示，P4在各种异构设置和攻击场景中，比领先方法提高了5%到30%的准确性，并在高达30%的恶意客户端存在下保持了鲁棒性。实际部署显示，两个客户之间的协作训练仅增加了约7秒的开销。

**结论:** P4方法成功地为资源受限的IoT设备提供了个性化的模型，确保了差分隐私和对投毒攻击的鲁棒性，展示了其在实际应用中的潜力和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Client+Clustering+Meets+Knowledge+Sharing%3A+Enhancing+Privacy+and+Robustness+in+Personalized+Peer-to-Peer+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20413&send_immediately=true&force_search=false)

**原文摘要:** The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [56] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu, Kazuki Kawamura, Takanori Muroi, Yusuke Narita, Kei Tateno, Takuma Udagawa, Yuta Saito*

**主要类别:** cs.LG

**AI概要:** 本文研究了在非平稳环境下的未来离线策略评估（F-OPE）和学习（F-OPL）问题，提出了一个名为OPFV的新估计器，通过时间序列数据中的结构进行准确的未来策略价值评估，并扩展为一种新的策略梯度方法以优化未来策略。


<details>
  <summary>更多</summary>
  
**动机:** 在非平稳环境中，分布随时间变化，现有方法假设平稳性或依赖限制性的奖励建模假设，导致显著偏差。需要一种能够利用历史数据中与未来环境相关的时间结构的方法来估计和优化未来策略的价值。

**方法:** 提出了一种名为OPFV的估计器，利用时间序列数据中的结构（如季节性、周效应或节假日效应），通过一种新的重要性加权方法实现有效的未来离线策略评估。进一步扩展为一种新的策略梯度方法，仅使用历史数据主动学习良好的未来策略。

**结果:** 理论分析表明，在特定条件下，OPFV具有低偏差。实证结果表明，该方法在各种实验设置下显著优于现有方法，能够更好地估计和优化非平稳环境下的未来策略价值。

**结论:** OPFV估计器及其扩展的策略梯度方法成功解决了非平稳环境下的未来策略评估和优化问题，为实际应用（如电子商务推荐系统）提供了更准确和有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Off-Policy+Evaluation+and+Learning+for+the+Future+under+Non-Stationarity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20417，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20417&send_immediately=true&force_search=false)

**原文摘要:** We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [57] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng, Sadiq Jaffer, Jovana Knezevic, Silja Sormunen, Robin Young, Madeline Lisaius, Markus Immitzer, James Ball, Clement Atzberger, David A. Coomes, Anil Madhavapeddy, Andrew Blake, Srinivasan Keshav*

**主要类别:** cs.LG

**AI概要:** TESSERA是一种新型的遥感基础模型，使用自监督学习生成10米尺度的全球稳健表示，结合了Sentinel-1 SAR和Sentinel-2 MSI数据，并通过多层感知器融合，实现了2017至2024年的全球表示图，在五个不同任务中表现优于现有模型。


<details>
  <summary>更多</summary>
  
**动机:** 卫星遥感在地球观测领域具有广泛应用，包括气候建模、碳核算以及保护和可持续土地利用策略等，但缺乏一个能够在像素级卫星时间序列数据上生成全球稳健表示的模型。

**方法:** TESSERA采用自监督学习方法，结合两个并行的Transformer编码器分别处理Sentinel-1 SAR极化数据和Sentinel-2 MSI数据（10个选定光谱带），然后使用多层感知器进行融合，生成2017年至2024年的全球表示图。

**结果:** TESSERA在五个不同的下游任务中表现出色，不仅超越了传统的遥感基准模型，还超过了现有的领先地理空间基础模型。

**结论:** TESSERA为高分辨率、高性能的遥感表示提供了开源解决方案，推动了遥感技术在多样任务中的应用，并设定了新的性能基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TESSERA%3A+Temporal+Embeddings+of+Surface+Spectra+for+Earth+Representation+and+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20380，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20380&send_immediately=true&force_search=false)

**原文摘要:** Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [58] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han, Wolfgang Bruckner*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于谱图理论的算法，用于自动选择表格数据分类中In-Context Learning（ICL）所需的合理演示次数。通过结合数据分布、提示模板和特定大语言模型（LLM），定义了新的度量标准，并构建相似性图以确定能代表数据的最少演示次数。实验表明该方法优于传统的随机选择算法。


<details>
  <summary>更多</summary>
  
**动机:** 在表格数据分类中应用In-Context Learning（ICL）时，如何确定提示中理想的演示次数是一个基本问题。现有的方法可能不够准确或高效，因此需要一种能够自动选择合理演示次数的方法。

**方法:** 研究提出了一种基于谱图理论的算法，该算法综合考虑了表格数据分布、用户选定的提示模板以及具体的大型语言模型（LLM）。通过定义新的度量标准来量化不同演示之间的相似性，构建相似性图并分析其拉普拉斯矩阵的特征值，从而推导出能够在LLM内在表示空间中代表数据的最小演示次数。

**结果:** 实验结果表明，与传统的随机选择算法相比，所提出的方法在各种数据集和LLM上的表现更优，验证了其有效性和优越性。

**结论:** 该研究成功开发了一种基于谱图理论的算法，可以自动选择ICL中合理的演示次数。这种方法显著提高了表格数据分类任务的效果，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automatic+Demonstration+Selection+for+LLM-based+Tabular+Data+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20451&send_immediately=true&force_search=false)

**原文摘要:** A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [59] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus, Igor Shilov, Georgios Kaissis, Yves-Alexandre de Montjoye*

**主要类别:** cs.LG

**AI概要:** 机器学习模型的记忆行为不仅受自身影响（self-influence），还受到训练集中其他样本（如近似副本）的显著影响。研究发现，仅关注自我影响可能会低估记忆化带来的实际风险，而通过分析完整的影响力分布可以更全面地理解记忆化现象及其潜在问题。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习模型会记忆训练数据中的样本，这引发了隐私和泛化方面的担忧。虽然反事实自我影响（counterfactual self-influence）是研究记忆化的常用指标，但已有研究表明，记忆化还受到其他因素的影响，特别是训练集中的（近似）副本样本的作用。因此，需要一种更全面的方法来研究记忆化现象。

**方法:** 本研究将反事实影响视为一个分布量，考虑所有训练样本如何共同影响某个样本的记忆化程度。对于一个小规模的语言模型，计算了训练样本之间的完整影响分布，并分析了其特性。同时，还将这种方法应用于图像分类任务（如CIFAR-10数据集）。

**结果:** 研究发现，仅关注自我影响可能会严重低估与记忆化相关的实际风险。（近似）副本样本的存在会显著降低自我影响，但这些样本实际上是（近似）可提取的。在图像分类中，通过分析影响分布可以揭示CIFAR-10数据集中存在的近似副本。

**结论:** 记忆化现象源于训练数据之间的复杂交互作用，完整的影响力分布比单独的自我影响更能准确捕捉记忆化的行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Counterfactual+Influence+as+a+Distributional+Quantity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20481&send_immediately=true&force_search=false)

**原文摘要:** Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [60] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò, Andrea Castellani, Sebastian Schmitt, Fabio Stella, Barbara Hammer*

**主要类别:** cs.LG

**AI概要:** 论文提出SIDED数据集和AMDA方法，解决工业NILM中数据稀缺与模型泛化问题。实验表明，使用AMDA增强数据训练的NILM模型在复杂工业设备能耗分解任务上显著优于其他方法。


<details>
  <summary>更多</summary>
  
**动机:** 工业非侵入式负荷监测（NILM）受到高质量数据集匮乏和工业能耗模式复杂多变的限制。为此，研究者旨在通过生成合成数据集和改进数据增强方法来解决数据稀缺和隐私保护问题，同时提升NILM模型的泛化能力。

**方法:** 1. 开发了开源合成数据集SIDED，基于数字孪生模拟生成，包含三种不同地理位置的工业设施，涵盖多样化的设备行为、天气条件和负载分布。
2. 提出了Appliance-Modulated Data Augmentation (AMDA) 方法，通过智能调整设备功率贡献来增强NILM模型的泛化性能，计算效率高。

**结果:** 实验结果表明：
1. 使用AMDA增强数据训练的NILM模型在样本外场景中表现优异，其归一化分解误差为0.093，显著低于无增强（0.451）和随机增强（0.290）的模型。
2. 数据分布分析验证了AMDA能够有效对齐训练和测试数据分布，从而提高模型泛化能力。

**结论:** SIDED数据集和AMDA方法为工业NILM领域提供了有效的解决方案，解决了数据稀缺问题并提升了模型性能，特别是在复杂工业设备能耗分解任务中的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Industrial+Energy+Disaggregation+with+Digital+Twin-generated+Dataset+and+Efficient+Data+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20525，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20525&send_immediately=true&force_search=false)

**原文摘要:** Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [61] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

**主要类别:** cs.LG

**AI概要:** 在联邦学习中，当只有少量客户端参与训练时，模型性能会受到严重影响。本文提出了一种知识蒸馏与师生不平等聚合（KDIA）策略，通过有效利用所有客户端的知识来提升模型性能。实验表明，KDIA在严重异构性情况下能以更少的训练轮次实现更高的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习方法通常忽视了在大规模客户端设置中，仅少量客户端参与训练的情况，这种场景对模型性能提出了更大的挑战。

**方法:** 1. 提出 KDIA 策略：
   - 学生模型为参与客户端的平均聚合。
   - 教师模型基于参与间隔、参与次数和数据量比例进行加权聚合。
2. 在本地训练中引入自知识蒸馏。
3. 利用服务器上的生成器生成近似独立同分布 (IID) 的数据特征，用于辅助训练。

**结果:** 在CIFAR-10/100/CINIC-10数据集及各种异构环境下进行实验，结果表明KDIA可以在更少的训练轮次中达到更高的准确率，特别是在严重异构情况下表现更优。

**结论:** 提出的KDIA策略能够有效应对联邦学习中客户端参与有限及数据异构性问题，提升了模型性能并减少了训练轮次需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tackling+Data+Heterogeneity+in+Federated+Learning+through+Knowledge+Distillation+with+Inequitable+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20431，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20431&send_immediately=true&force_search=false)

**原文摘要:** Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [62] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot, Rémi Emonet, Amaury Habrard, Abdel-Rahim Mezidi, Marc Sebban*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于函数海森矩阵的新积分方法，用于指导PINNs训练过程中配置点的选择。


<details>
  <summary>更多</summary>
  
**动机:** 现有的PINNs通过在损失函数中嵌入物理模型并使用自动微分最小化残差来学习PDE的代理神经解算器，但对其配置点的选择仍有改进空间。

**方法:** 提出一种新的基于海森矩阵的求积方法，用以指导PINNs训练过程中配置点的选择。

**结果:** 该方法能够更有效地选择配置点，从而提高PINNs的性能。

**结论:** 新方法为提高PINNs的求解效率提供了一个可行的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M%C3%A9thode+de+quadrature+pour+les+PINNs+fond%C3%A9e+th%C3%A9oriquement+sur+la+hessienne+des+r%C3%A9siduels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20441&send_immediately=true&force_search=false)

**原文摘要:** Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [63] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin, Enze Ge, Yuhang Xie, Hongying Luo, Junhao Song, Ziqian Bi, Chia Xin Liang, Jibin Guan, Joe Yeong, Junfeng Hao*

**主要类别:** cs.LG

**AI概要:** 多模态学习通过结合不同来源的信息（如图像、文本和音频）来帮助机器理解复杂的事物。尽管取得了良好进展，但仍存在一些主要问题，例如处理不同的数据格式、不完整输入和对抗性攻击等。研究人员正在探索新的方法，以提高模型效率和可扩展性，并设计更好的评估指标和基准。未来，多模态学习有望改善计算机视觉、自然语言处理、语音识别和医疗保健等领域，并可能建立更像人类理解世界的AI系统。


<details>
  <summary>更多</summary>
  
**动机:** 当前人工智能需要理解和处理复杂事物，而单一模态的数据不足以满足需求。多模态学习通过结合多种信息源的强项，使得AI系统能够构建更强大和丰富的内部表示，从而更好地解释、推理和在实际场景中做决策。

**方法:** 多模态学习的核心技术包括：表示学习（从不同类型的数据中获取共享特征）、对齐方法（跨模态匹配信息）和融合策略（通过深度学习模型组合信息）。此外，研究者正在探索无监督或半监督学习以及AutoML工具，以提高模型效率和可扩展性。

**结果:** 多模态学习已经在许多领域取得良好进展，但仍然面临诸如处理不同数据格式、缺失或不完整的输入以及防御对抗性攻击等问题。同时，新的方法和技术正在被开发，以解决这些问题并进一步提升性能。

**结论:** 随着多模态学习领域的持续发展，它有望改进计算机视觉、自然语言处理、语音识别和医疗保健等多个领域。未来，多模态学习可能有助于建立更灵活、上下文感知且能够应对现实世界复杂性的AI系统，使其更接近人类理解世界的方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Representation+Learning+and+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20494，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20494&send_immediately=true&force_search=false)

**原文摘要:** Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [64] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer, Karthick Panner Selvam, Beltran Fiz Pontiveros*

**主要类别:** cs.LG

**AI概要:** 本论文通过贪心随机搜索优化联邦学习中各参与方的本地批量大小，从而提升训练配置效率和收敛速度。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的参与者可能共享训练硬件，但由于没有信息交换，不恰当的训练配置会阻碍训练过程，因此需要优化硬件使用以改进本地训练过程。

**方法:** 利用联邦学习固有的并行处理能力，采用贪心随机搜索方法优化所有参与者的本地批量大小，以找到最佳训练设置。

**结果:** 相较于默认参数设置，该方法提高了收敛速度，并且几乎与本地参数经过优化的情况持平。

**结论:** 优化本地批量大小可以有效改善联邦学习的训练效率，在不增加太多复杂性的情况下提高收敛速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Collaborative+Batch+Size+Optimization+for+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20511&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [65] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer, Beltran Fiz Pontiveros, Radu State*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架，引入客户特定的代币作为投资工具，并通过DeFi平台和AMM创建更灵活、可扩展的奖励分配系统及第三方投资机制。


<details>
  <summary>更多</summary>
  
**动机:** 当前在盈利性联邦学习中，虽然已经提出了许多分配方法，但这些方法仍存在局限性，而奖励分配框架的研究相对较少。

**方法:** 提出一个新框架，使用客户特定的代币作为投资工具，并结合去中心化金融（DeFi）平台和自动化做市商（AMMs），构建更灵活和可扩展的奖励分配系统，同时允许第三方投资于联邦学习过程。

**结果:** 该框架可以克服现有激励方案的限制，提供更灵活和可扩展的奖励分配方式，并为第三方提供投资途径。

**结论:** 提出的基于代币和DeFi的奖励分配框架为联邦学习生态系统提供了更灵活和可扩展的奖励机制，有助于吸引更多的参与者并提升系统的整体性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WallStreetFeds%3A+Client-Specific+Tokens+as+Investment+Vehicles+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20518&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [66] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal, Gaëtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos*

**主要类别:** cs.LG

**AI概要:** 本研究探讨了介于离线强化学习（RL）和监督微调之间的算法范围，通过分析一个简单的离线REINFORCE算法，发现当基线V下界为期望奖励时，该算法具有策略改进保证。实验验证了在离线更新中更关注正奖励比负奖励更有益。


<details>
  <summary>更多</summary>
  
**动机:** 离线方法相较于在线方法在实现简单性和数据效率方面更具优势，但通常性能较差，因此需要探索介于离线RL和监督微调之间的算法。

**方法:** 研究了一个简单的离线REINFORCE算法，其中优势函数定义为A=r-V，并分析了基线V对样本的影响。提供了理论分析并揭示了离线更新聚焦正奖励的重要性。

**结果:** 理论分析表明当基线下界为期望奖励时，算法具有策略改进保证。实验在受控随机Bandit设置和LLM推理任务微调中验证了这些发现。

**结论:** 离线更新应更关注正奖励而非负奖励，这与在线更新可以安全利用正负信号不同。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymmetric+REINFORCE+for+off-Policy+Reinforcement+Learning%3A+Balancing+positive+and+negative+rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20520，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20520&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [67] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma, M. Raissi, Y. B. Guo*

**主要类别:** cs.LG

**AI概要:** FEA-PINN是一种结合有限元分析和物理信息神经网络的高效建模框架，用于加速激光粉末床熔融（LPBF）过程中的热场预测，同时保持FEA的准确性。通过引入动态材料更新策略和校正的FEA模拟，该框架在显著降低计算成本的同时保证了物理一致性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的数值方法如有限元分析（FEA）在模拟LPBF过程中存在高计算成本的问题，因此需要一种更高效的建模方法来加速热场预测。

**方法:** 提出了一种名为FEA-Regulated Physics-Informed Neural Network (FEA-PINN)的建模框架，集成了温度相关的材料特性和相变行为，并采用了动态材料更新策略。此外，在推理过程中通过集成校正的FEA模拟来强制执行物理一致性和减少误差漂移。

**结果:** FEA-PINN框架在单轨扫描的LPBF过程中验证了其有效性，证明了其在显著降低计算成本的同时能够达到与FEA相当的精度。

**结论:** FEA-PINN框架为LPBF过程提供了高效的热场预测工具，解决了传统方法计算成本高的问题，同时保持了高精度和物理一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Informed+Machine+Learning+Regulated+by+Finite+Element+Analysis+for+Simulation+Acceleration+of+Laser+Powder+Bed+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20537，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20537&send_immediately=true&force_search=false)

**原文摘要:** Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [68] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen, Jaron Sanders, Fiona Sloothaak, Maarten G. Wolf*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了在数据中心、云计算网络和服务系统等基于技能的排队系统中实现最优控制的方法。通过使用真实数据集的案例研究，探讨了一种新开发的强化学习算法在最优客户路由中的实际应用。实验表明，该算法能高效学习并适应变化环境，优于静态基准策略，并具有实时实施潜力。此外，通过引入新的启发式路由规则以减少延迟，增强了算法的实际应用性。该算法还可以优化多个目标，包括收益最大化、服务器负载公平性和减少客户等待时间等，同时利用调优参数平衡性能权衡。最后，研究了该算法对估计误差和参数调优的敏感性，为在复杂现实排队系统中实施自适应路由算法提供了宝贵见解。


<details>
  <summary>更多</summary>
  
**动机:** 当前在基于技能的排队系统（如数据中心、云计算网络和服务系统）中，需要一种能够有效适应动态环境并优化多目标的客户路由方法。而传统的静态策略可能无法满足这一需求，因此有必要探索和验证新的强化学习算法的实际效果和适用性。

**方法:** 作者通过一个基于真实数据集的案例研究，应用了一种新开发的强化学习算法来解决最优客户路由问题。为了提高算法的实际应用能力，还引入了一个新的启发式路由规则以减少延迟。此外，研究展示了该算法如何通过调整参数来平衡不同目标（如收益最大化、服务器负载公平性和客户等待时间减少）。最后，分析了该算法对估计误差和参数调优的敏感性。

**结果:** 实验结果表明，该强化学习算法可以高效地学习和适应变化环境，其表现优于静态基准策略。同时，引入的启发式路由规则有效地减少了延迟。该算法还能同时优化多个目标，并通过调优参数实现了性能权衡。此外，研究揭示了算法对估计误差和参数调优的敏感性，为实际应用提供了重要指导。

**结论:** 强化学习算法在基于技能的排队系统中具有显著的应用潜力，特别是在最优客户路由方面。它不仅能适应动态环境，还能优化多个目标。通过引入启发式规则和调整参数，进一步增强了其实用性。然而，该算法对估计误差和参数调优较为敏感，因此在实际部署时需谨慎考虑这些因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demonstration+of+effective+UCB-based+routing+in+skill-based+queues+on+real-world+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20543，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20543&send_immediately=true&force_search=false)

**原文摘要:** This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [69] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia, Rafael Teixeira de Lima, Bogdan Malaescu*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了基于变压器的多变量时间序列异常检测方法，重点研究iTransformer架构的应用、参数影响、标签提取方法、评价指标、训练数据中的异常影响及损失函数的作用，并进行了模型对比。


<details>
  <summary>更多</summary>
  
**动机:** 在医疗保健、金融服务、制造业和物理探测器监控等多个领域中，准确识别意外错误或故障的发生至关重要，但由于异常的未知性质和时间序列维度之间的复杂相互依赖性，这一任务充满挑战。因此需要研究有效的变压器架构以改善多变量时间序列的异常检测能力。

**方法:** 本研究主要采用了iTransformer架构进行时间序列异常检测，分析了窗口大小、步长和模型维度等关键参数对性能的影响；研究了从多维异常评分中提取异常标签的方法及适当的评估指标；考察了训练期间存在的异常数据的影响，并评估了替代损失函数的有效性；最后比较了几种基于变压器的模型在不同数据集上的表现。

**结果:** 通过实验，作者明确了关键参数对iTransformer性能的影响，提出了有效的异常标签提取方法和评估指标，验证了损失函数在减轻训练数据中异常影响方面的有效性，并展示了多种变压器模型在时间序列异常检测中的相对优势。

**结论:** iTransformer架构在多变量时间序列异常检测中具有良好的潜力，其性能可以通过调整关键参数得到优化；同时，针对异常标签提取和评估指标的选择提供了指导，并强调了处理训练数据中的异常数据的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+Unsupervised+Strategies+for+Anomaly+Detection+in+Multivariate+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20574&send_immediately=true&force_search=false)

**原文摘要:** Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [70] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv, Neta Rabin*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了图神经网络在分布外（OOD）泛化方面的挑战，特别是骨干架构的影响。通过系统评估图变压器（GT）和混合骨干架构在OOD设置下的表现，并与消息传递神经网络（MPNNs）进行比较，结果表明GT和混合GT-MPNN骨架具有更强的泛化能力。此外，提出了一种新的后训练分析方法，该方法通过比较整个ID和OOD测试数据集的聚类结构来检查领域对齐和类别分离，显示出在图学习之外的DG问题中的广泛应用潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的图深度学习方法通常假设训练和测试数据具有相同的分布，但在实际应用中这一条件很少满足。尽管图变压器（GT）在同分布（ID）基准测试中表现出色，但其在分布转移情况下的有效性尚未得到充分研究。因此，有必要探索不同骨干架构在分布外（OOD）泛化中的表现。

**方法:** 作者系统地评估了图变压器（GT）和混合GT-MPNN骨干架构在OOD设置下的表现，并将其与传统的消息传递神经网络（MPNNs）进行比较。为了实现这一点，他们将几种领先的领域泛化（DG）算法适配到GT中，并在一个设计用于测试多种分布转移的基准上评估其性能。此外，还提出了一种新的后训练分析方法，该方法通过比较ID和OOD测试数据集的聚类结构来检查领域对齐和类别分离。

**结果:** 实验结果表明，GT和混合GT-MPNN骨干架构在没有专门的DG算法的情况下，相比MPNNs展现出更强的泛化能力。提出的后训练分析方法不仅为理解GT和MPNN骨干提供了有意义的见解，还展示了其在更广泛的DG问题中的潜在应用。

**结论:** 本研究强调了图变压器在鲁棒、真实世界图学习中的潜力，并为未来的OOD泛化研究指明了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Graph-Transformer+Out-of-Distribution+Generalization+Abilities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20575，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20575&send_immediately=true&force_search=false)

**原文摘要:** Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [71] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper, Ted Willke*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的图索引方法SVG，通过机器学习和核方法构建适用于度量和非度量向量空间的图索引，并提供了正式的可导航性保证。此外，还提出了SVG-L0，通过加入稀疏性约束来限制出度并实现自调优特性。


<details>
  <summary>更多</summary>
  
**动机:** 目前最流行的向量搜索图索引仅在欧几里得空间中有效，无法适用于度量和非度量向量空间（如内积相似性）。因此，需要一种新方法来扩展图索引的适用范围。

**方法:** 引入支持向量图（SVG）作为新的图索引类型，利用核方法建立图的连通性，并提供度量和非度量向量空间中的正式可导航性保证。将HNSW和DiskANN等流行图索引解释为SVG的特定特例，并提出SVG-L0，通过加入ℓ₀稀疏性约束控制节点出度，同时具备自调优特性以避免启发式方法。

**结果:** SVG和SVG-L0在理论和实践中均表现出色，能够有效处理度量和非度量向量空间中的向量搜索问题，同时改进了传统方法的局限性。

**结论:** SVG是一种通用且强大的图索引方法，适用于更广泛的向量空间。SVG-L0进一步优化了图结构，提供了有原则的出度限制和自调优能力，为未来的研究提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+kernel+of+graph+indices+for+vector+search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20584，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20584&send_immediately=true&force_search=false)

**原文摘要:** The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [72] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai, Senwei Liang, Chunmei Wang*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的符号学习方法H-FEX，用于从数据中学习哈密顿系统，尤其在复杂和刚性系统中表现出色，能准确捕捉系统动力学并长时间保持能量守恒。


<details>
  <summary>更多</summary>
  
**动机:** 当前的数据驱动方法（如符号回归和基于神经网络的方法）虽然可以从观测数据中学习动力系统的控制方程，但难以准确捕捉复杂的哈密顿函数同时保持能量守恒。

**方法:** 提出了名为H-FEX的有限表达式方法，这是一种符号学习方法，通过引入新颖的交互节点来有效捕捉复杂的交互项。

**结果:** 实验表明，H-FEX可以恢复复杂系统的哈密顿函数，这些函数能够准确捕捉系统动力学，并在长时间内保持能量守恒。

**结论:** H-FEX作为一种强大的框架，具有发现复杂动力系统封闭形式表达式的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是H-FEX%3A+A+Symbolic+Learning+Method+for+Hamiltonian+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20607&send_immediately=true&force_search=false)

**原文摘要:** Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [73] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li, Hongyue Wu, Guodong Fan, Zhen Zhang, Shizhan Chen, Zhiyong Feng*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的边缘设备联邦学习方案FedEDS，通过数据加密共享机制加速联邦学习训练的收敛速度，并缓解数据异质性对模型性能的负面影响。实验结果表明，FedEDS在提升模型性能方面具有显著效果。


<details>
  <summary>更多</summary>
  
**动机:** 隐私保护日益重要，越来越多的模型在边缘设备上进行训练并通过联邦学习合并到中央服务器。然而，当前研究忽略了网络拓扑、物理距离和数据异质性对边缘设备的影响，导致延迟增加和模型性能下降等问题。

**方法:** 提出了一种名为FedEDS（联邦学习与加密数据共享）的新联邦学习方案。该方案使用客户端模型和模型的随机层训练数据加密器，数据加密器生成加密数据并与其他客户端共享。客户端利用对应的随机层和加密数据训练和调整本地模型。FedEDS结合本地私有数据和来自其他客户端的加密共享数据进行模型训练。

**结果:** 实验结果表明，FedEDS能够有效促进模型性能，加速联邦学习训练的收敛速度，并缓解数据异质性带来的负面影响。

**结论:** FedEDS是一种适合部署在需要快速收敛的边缘设备上的应用服务方案，解决了现有联邦学习中因数据异质性导致的问题，提升了模型性能和训练效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Federated+Learning+with+Encrypted+Data+Sharing+for+Data-Heterogeneous+Edge+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20644，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20644&send_immediately=true&force_search=false)

**原文摘要:** As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [74] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang, Baochun Li*

**主要类别:** cs.LG

**AI概要:** 近期研究表明，联邦学习中的梯度更新可能会无意间泄露客户本地数据的敏感信息。本文从防御者的角度出发，首次对恶意梯度泄露攻击及其背后的模型操控技术进行了全面分析。研究揭示了这些攻击在重建私人数据的有效性和逃避检测的隐蔽性之间的核心权衡。基于这一见解，我们提出了一种简单、轻量且广泛适用的客户端检测机制，可以在本地训练开始前标记可疑的模型更新。该机制强调了在实际联邦学习环境中通过基本监控即可有效防御此类攻击的可能性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的梯度更新可能无意中泄露客户的敏感数据，而恶意服务器可以通过操控全局模型来引发信息丰富的更新，从而加剧这一风险。因此，需要对恶意梯度泄露攻击进行深入分析，并探索其实际威胁程度及防御方法。

**方法:** 1. 从防御者视角出发，对恶意梯度泄露攻击和模型操控技术进行全面分析。
2. 研究攻击在重建私人数据的有效性和隐蔽性之间的权衡。
3. 提出一种轻量级的客户端检测机制，在本地训练开始前标记可疑的模型更新。

**结果:** 恶意梯度泄露攻击虽然理论上令人担忧，但在实际中受到限制，通常可以通过基本监控检测到。提出的客户端检测机制可以有效防御攻击，且开销较小。

**结论:** 恶意梯度泄露攻击在实践中受到限制，且可以通过简单的监控和检测机制有效防御。本文提出的客户端检测机制为隐私保护的联邦学习系统提供了一个可行的安全保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hear+No+Evil%3A+Detecting+Gradient+Leakage+by+Malicious+Servers+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20651&send_immediately=true&force_search=false)

**原文摘要:** Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [75] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba, Chaoran Liu, Shuhei Kurita, Akiyoshi Sannai*

**主要类别:** cs.AI

**AI概要:** 本研究提出了Prover Agent，一种用于自动定理证明的新颖AI代理，它将大型语言模型与形式证明助手Lean相结合。该方法在MiniF2F基准测试中达到了86.1%的成功率，并展示了生成的辅助引理如何有助于解决复杂问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管现有方法使用小型语言模型进行定理证明取得了一定进展，但它们通常需要较大的样本预算，并且证明成功率有限。因此，需要一种新的方法来提高定理证明的效果和效率。

**方法:** 研究人员开发了Prover Agent，这是一种整合了非形式推理LLM、形式证明模型以及Lean反馈的AI代理。此外，Prover Agent还能够生成辅助引理以帮助发现整体证明策略。

**结果:** Prover Agent在MiniF2F基准测试中实现了86.1%的成功率，显著优于之前使用小型语言模型的方法，并且在样本预算较低的情况下建立了新的先进水平。

**结论:** Prover Agent通过结合大型语言模型和形式证明助手Lean，展示出在自动定理证明中的高效性能，同时生成的辅助引理对于解决复杂问题具有重要作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prover+Agent%3A+An+Agent-based+Framework+for+Formal+Mathematical+Proofs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19923&send_immediately=true&force_search=false)

**原文摘要:** We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [76] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan, Keerthiram Murugesan, Nuno Moniz, Nitesh Chawla*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的框架，将上下文归因问题转化为组合多臂老虎机（CMAB）问题，并使用组合汤普森采样（CTS）方法在有限查询预算下高效探索上下文子集空间。通过基于归一化标记似然性的奖励函数，该方法自适应地平衡探索与利用，从而显著提高查询效率并保持高归因保真度。实验表明，此方法在减少模型查询的同时达到竞争性的归因质量。


<details>
  <summary>更多</summary>
  
**动机:** 构建可解释和值得信赖的生成式问答系统需要理解检索到的上下文中哪些部分对大语言模型生成的答案有贡献。传统基于扰动的方法如SHAP计算成本高且效率低。

**方法:** 将上下文归因建模为组合多臂老虎机问题，其中每个上下文片段被视为一个老虎机臂。使用组合汤普森采样算法在指数级大的上下文子集空间中进行有效探索，并定义基于归一化令牌似然的奖励函数来评估片段子集对原始模型响应的支持程度。通过后验估计片段相关性，自适应地平衡探索与利用。

**结果:** 在多个数据集和大语言模型上的广泛实验证明，该方法能够在更少的模型查询下实现具有竞争力的归因质量。

**结论:** 所提出的方法通过更高效的查询策略，在保持高归因保真度的同时减少了计算负担，为构建更可解释的生成式问答系统提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context+Attribution+with+Multi-Armed+Bandit+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19977&send_immediately=true&force_search=false)

**原文摘要:** Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [77] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit, Minghao Shao, Haider Asif, Nouhaila Innan, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique*

**主要类别:** cs.AI

**AI概要:** 近期大语言模型（LLMs）在代码生成方面展现了强大的潜力，但其在量子计算领域的有效性尚未被充分研究。本文通过使用来自Quantum Hackathon的真实挑战，对基于PennyLane的量子代码生成的大语言模型进行基准测试，并引入了QHackBench这一新的基准数据集。通过结构化评估框架，我们评估了模型在不同难度挑战下的功能正确性、语法有效性和执行成功率。结果表明，增强检索生成（RAG）的模型在复杂量子算法中与标准提示生成的结果相似。此外，我们还提出了一个可以迭代改进错误解决方案的多代理评估管道，进一步提高了执行成功率。为了推动进一步的研究，我们将公开发布QHackBench以及我们的评估框架和实验结果，以促进AI辅助量子编程的持续发展。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大语言模型在代码生成领域展现出了强大的潜力，但它们在量子计算领域的应用效果还未被深入探索。因此，有必要针对这一特定领域进行系统性的研究和评估。

**方法:** 作者创建了一个名为QHackBench的新基准数据集，该数据集来源于Quantum Hackathon竞赛中的实际问题。利用这个数据集，他们对大语言模型在两种提示方式下（普通提示和检索增强生成[RAG]）的表现进行了评估。评估框架包括功能正确性、语法有效性和执行成功率等多个维度。同时，提出了一种多代理评估管道来逐步优化错误解决方案。

**结果:** 结果显示，在复杂量子算法中，RAG增强的模型与标准提示生成的结果相近。并且，提出的多代理评估管道能够有效提高执行成功率。

**结论:** 本文通过构建QHackBench数据集和详细的评估框架，揭示了大语言模型在量子计算代码生成方面的潜力和局限性。此外，通过公开发布数据集和评估工具，为未来的研究提供了基础，有助于推动AI辅助量子编程的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QHackBench%3A+Benchmarking+Large+Language+Models+for+Quantum+Code+Generation+Using+PennyLane+Hackathon+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20008&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [78] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos, Michail E. Klontzas*

**主要类别:** cs.AI

**AI概要:** A customizable RAG framework using local LLMs outperformed commercial models in medical tasks, offering higher accuracy and lower energy consumption. This promotes sustainable AI development.


<details>
  <summary>更多</summary>
  
**动机:** To address the environmental and ethical concerns of using commercial large language models in healthcare, such as resource intensity and patient privacy issues.

**方法:** Developed a customizable RAG framework for medical tasks that monitors energy usage and CO2 emissions, tested with various open-source LLMs including general and medical-domain specific models.

**结果:** Custom RAG models achieved better accuracy and lower energy consumption than commercial models; llama3.1-RAG performed best with 58.5% accuracy and lowest energy use.

**结论:** Local LLM-based RAGs can surpass commercial models in medical tasks while having less environmental impact, supporting sustainable AI development.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accurate+and+Energy+Efficient%3A+Local+Retrieval-Augmented+Generation+Models+Outperform+Commercial+Large+Language+Models+in+Medical+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20009，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20009&send_immediately=true&force_search=false)

**原文摘要:** Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [79] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng, Ziwei Liu, Ziqian Bi, Junhao Song, Chia Xin Liang, Joe Yeong, Junfeng Hao*

**主要类别:** cs.AI

**AI概要:** 这篇论文调查了实时决策支持系统，利用低延迟AI模型，结合整体AI驱动的决策工具、与Edge-IoT技术的集成以及有效的人工智能团队合作的方法。它研究了大型语言模型在资源有限时如何辅助决策，并探讨了如DeLLMa等技术发展、模型压缩方法和边缘设备分析改进的影响。通过详细综述，论文提供了关于开发策略和应用领域的实际视角，指出了更高效和灵活的AI支持系统的机遇。结论为这一快速变化领域的未来突破奠定了基础，强调了AI如何重塑实时决策支持。


<details>
  <summary>更多</summary>
  
**动机:** 当前实时决策支持系统需要更高效、灵活的解决方案，特别是在资源有限的情况下。随着AI技术的发展，例如大语言模型和边缘计算的进步，这些技术可以被整合到决策支持系统中以提升其性能。此外，人机协作的有效性也是一个重要的研究方向。

**方法:** 论文通过详细综述的方法，分析了低延迟AI模型、大型语言模型、Edge-IoT技术和模型压缩方法对实时决策支持系统的影响。同时，还讨论了资源限制和技术框架适应性的问题。

**结果:** 论文提供了一系列关于开发策略和应用领域的实际视角，明确了更高效和灵活的AI支持系统的机遇，并指出技术进步（如DeLLMa）和方法改进（如模型压缩）对提升实时决策支持系统性能的关键作用。

**结论:** AI技术，特别是低延迟模型和边缘计算的进步，将重塑实时决策支持系统。未来的研究应集中于开发更高效、灵活的系统，以应对资源限制并实现更好的人机协作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Achieving+Trustworthy+Real-Time+Decision+Support+Systems+with+Low-Latency+Interpretable+AI+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20018，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20018&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [80] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash, Amélie Reymond, Emma S. Spiro, Aylin Caliskan*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了大型语言模型（LLMs）在被赋予8种不同的人格特征后是否会表现出类似人类的动机推理偏差。研究发现，这些人格化的LLM在处理虚假信息和科学证据时，其判断准确度有所下降，并且政治相关的人格更容易根据符合自身政治身份的信息作出判断。传统的基于提示的去偏倚方法对缓解这些效应效果有限。这表明人格化的LLM表现出难以通过常规手段消除的人类动机推理偏差，可能进一步加剧身份一致的推理偏差。


<details>
  <summary>更多</summary>
  
**动机:** 研究者观察到人类推理容易受到身份保护等动机的影响，从而损害理性决策和判断能力。这种集体层面的动机推理会对社会造成负面影响，例如在气候变化或疫苗安全性等问题上的争论中。虽然已有研究表明LLM也会受到类似人类的认知偏差影响，但它们是否会选择性地倾向于与身份一致的结论尚未得到充分探索。

**方法:** 研究人员为8个LLM（包括开源和专有模型）分配了8种不同的人格特征，涵盖4种政治和社会人口属性。通过两个任务测试这些模型的表现：一是辨别虚假信息标题的真实性；二是评估数值型科学证据的有效性。此外，还尝试了基于提示的去偏倚方法以观察其效果。

**结果:** 实验结果显示，与未分配人格的模型相比，分配了人格的LLM在辨别虚假信息真实性方面表现降低了9%。具体而言，当科学证据的真实情况与其政治身份一致时，具有政治人格的模型正确评估的可能性高出90%。然而，传统的基于提示的去偏倚方法几乎无法有效缓解这些偏差。

**结论:** 这是首次实证证明，分配了人格的LLM会表现出类似人类的动机推理偏差，并且这种偏差难以通过常规的去偏倚提示来缓解。这一发现引发了对LLM和人类可能进一步加剧身份一致推理的担忧。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Persona-Assigned+Large+Language+Models+Exhibit+Human-Like+Motivated+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20020，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20020&send_immediately=true&force_search=false)

**原文摘要:** Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [81] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren, Tianxiang Zhao, Lei Wang, Tianchun Wang, Vasant Honavar*

**主要类别:** cs.AI

**AI概要:** DiaLLM 是一种新的医疗大语言模型，通过整合电子健康记录（EHR）数据，提供临床测试推荐、结果解读和诊断预测。它采用强化学习框架进行证据获取和自动化诊断，并设计了确认奖励和类别敏感诊断奖励来提升诊断准确性。实验表明，DiaLLM 在临床测试推荐和诊断预测方面优于基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有的医疗大语言模型忽略了电子健康记录（EHR）的重要作用，主要集中在诊断推荐上，限制了其在临床实践中的应用范围。

**方法:** 提出了一种名为 DiaLLM 的新模型，该模型将异构 EHR 数据整合到以临床为基础的对话中，从而实现临床测试推荐、结果解读和诊断预测。具体方法包括：1) 设计临床测试参考（CTR）策略，将每个临床代码映射到其对应描述，并对测试结果分类为“正常”或“异常”；2) 使用强化学习框架进行证据获取和自动化诊断；3) 引入拒绝采样策略以减少冗余并提高探索效率；4) 设计确认奖励和类别敏感诊断奖励以指导准确的诊断预测。

**结果:** 广泛的实验结果表明，DiaLLM 在临床测试推荐和诊断预测任务上显著优于现有基线模型。

**结论:** DiaLLM 通过整合 EHR 数据和强化学习框架，能够更贴近实际医疗实践，具有更高的临床适用性。未来工作可以进一步扩展其功能和应用场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiaLLMs%3A+EHR+Enhanced+Clinical+Conversational+System+for+Clinical+Test+Recommendation+and+Diagnosis+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20059，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20059&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [82] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal, Steven N. Minton, Deborah Khider, Yolanda Gil*

**主要类别:** cs.AI

**AI概要:** 本论文介绍了OpenPub平台中的可重复性副驾驶模块，该模块通过分析手稿、代码和补充材料生成结构化的Jupyter笔记本和建议，旨在促进计算可重复性。测试结果显示，该模块能将可重复性工作时间从30小时以上减少到约1小时，同时检测到影响可重复性的障碍因素。这表明AI驱动的工具可以减轻可重复性工作的负担，并有助于更透明和可验证的科学交流。


<details>
  <summary>更多</summary>
  
**动机:** 开放科学计划旨在使研究输出更加透明、可访问和可重用，但确保已发表的研究成果能够被独立重现仍然是一个持续的挑战。为了解决这一问题，需要一种工具来支持研究人员、评审员和读者，特别是在关键的开放科学任务上。

**方法:** 本研究提出了可重复性副驾驶模块，它分析手稿、代码和补充材料以生成结构化的Jupyter笔记本和建议。通过使用先前研究过的具有已知可重复性基准的研究论文进行可行性测试，评估其性能。

**结果:** 结果表明，OpenPub可以大幅减少可重复性时间（从超过30小时减少到大约1小时），同时实现高覆盖率的图表和结果，适合计算可重复性。系统还系统地检测到可重复性的障碍，包括缺失的超参数、未记录的预处理步骤和不完整或无法访问的数据集。

**结论:** AI驱动的工具可以有意义地减少可重复性努力的负担，并有助于更透明和可验证的科学交流。模块化副驾驶架构还为将AI辅助扩展到其他开放科学目标提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Copilots+for+Reproducibility+in+Science%3A+A+Case+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20130，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20130&send_immediately=true&force_search=false)

**原文摘要:** Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [83] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng, Peter Clark, Kyle Richardson*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种多智能体LLM方法（Genesys），模拟从概念到验证的研究过程，通过规模递增的方式提出、审查和验证新的语言模型架构设计。实验表明，该方法在生成成功设计方面优于传统方法，并发现了与已知架构竞争的新设计。


<details>
  <summary>更多</summary>
  
**动机:** 受真实研究过程启发，希望利用LLM来建模发现新语言模型架构的过程，从而提高发现效率并使其更具可分解性。

**方法:** 提出一个多智能体LLM系统Genesys，采用规模递增（Ladder of Scales）方法，包括提案、代码生成、预训练和验证等阶段。使用遗传编程作为骨干，优化设计生成流程。

**结果:** 实验涉及1,162个新设计（1,062个完全验证），其中最佳设计在6/9常见基准上超越了GPT2和Mamba2等已知架构。此外，还进行了全面的系统级消融分析。

**结论:** Genesys系统展示了在高效自主发现系统设计中的潜力，其遗传编程方法相比直接提示生成工作流有显著优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+Modeling+by+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20249，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20249&send_immediately=true&force_search=false)

**原文摘要:** Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [84] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang, David Yi, Damien Jose, John Passarelli, James Gao, Jordan Leventis, Kang Li*

**主要类别:** cs.AI

**AI概要:** 大型语言模型（LLMs）在提升AI驱动工具生产力方面显示出潜力，但现有基准如MMLU未能充分评估企业特定任务的复杂性。我们提出了一个基于布鲁姆分类法的14任务框架，以全面评估LLM在企业环境中的能力。为了解决噪声数据和昂贵注释的挑战，我们开发了一个可扩展的管道，结合了LLM-as-a-Labeler、LLM-as-a-Judge和矫正检索增强生成（CRAG），策划了一个强大的9700样本基准。对六个领先模型的评估显示，开源竞争者如DeepSeek R1在推理任务中与专有模型相当，但在基于判断的情景中落后，可能是因为过度思考。我们的基准揭示了关键的企业性能差距，并提供了可行的优化见解。这项工作为企业提供了定制评估的蓝图，并推动了实用LLM的部署。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在提高AI驱动工具的生产力方面展现出潜力，但现有的基准测试（例如Massive Multitask Language Understanding (MMLU)）无法充分评估企业特定任务的复杂性。这促使研究者寻求更精确和全面的评估方法来衡量LLM在企业环境中的表现。

**方法:** 提出了一种基于布鲁姆分类法的14任务框架，用于全面评估LLM在企业环境中的能力。为了应对噪声数据和高昂标注成本的问题，开发了一个可扩展的管道，结合了LLM-as-a-Labeler、LLM-as-a-Judge以及矫正检索增强生成（CRAG），从而创建了一个包含9,700个样本的强大基准。

**结果:** 评估结果显示，像DeepSeek R1这样的开源模型在推理任务上可以媲美专有模型，但在基于判断的任务中表现较差，可能是由于过度思考问题导致的。这一发现揭示了企业在使用这些模型时可能面临的性能差距。

**结论:** 该研究为企业提供了一个量身定制的评估蓝图，有助于推进大型语言模型在实际场景中的部署。同时，这项工作还强调了根据具体需求优化模型的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enterprise+Large+Language+Model+Evaluation+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20274&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [85] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang, Yingxiu Zhao, Ming-Liang Zhang, Jun Song, Yuning Jiang, Bo Zheng*

**主要类别:** cs.AI

**AI概要:** Mobile-R1是一种新的移动代理方法，采用任务级奖励的多轮交互强化学习，通过三个训练阶段（格式微调、基于动作级奖励的单步在线训练和基于任务级奖励的多轮轨迹在线训练）来提高探索能力和错误纠正能力。同时，研究者还收集了一个包含28个中国应用的数据集，并建立了500条轨迹的新基准，所有资源将开源。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉-语言模型驱动的移动代理主要依赖离线强化学习或基于动作级奖励的在线优化，这限制了代理与环境的动态交互，容易陷入局部最优，削弱了探索能力和错误纠正能力。

**方法:** 提出了一种名为Mobile-R1的方法，使用任务级奖励的多轮交互强化学习。其训练框架包括三个阶段：初始格式微调、基于动作级奖励的单步在线训练以及基于多轮轨迹的任务级奖励在线训练。

**结果:** 通过增强探索和错误纠正能力，Mobile-R1显著提高了性能，并在新收集的数据集和基准上表现出色。

**结论:** Mobile-R1通过引入任务级奖励的多轮交互强化学习，有效提升了移动代理的性能，数据集和基准的发布为未来研究提供了重要资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mobile-R1%3A+Towards+Interactive+Reinforcement+Learning+for+VLM-Based+Mobile+Agent+via+Task-Level+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20332&send_immediately=true&force_search=false)

**原文摘要:** Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [86] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han, Sungkyu Park, Seungeon Lee*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为REFeat的新方法，通过引导大型语言模型（LLM）利用多种推理类型来生成更多样化和有意义的特征。实验表明，该方法不仅提高了预测准确性，还发现了更丰富的特征。


<details>
  <summary>更多</summary>
  
**动机:** 在表格数据的机器学习中，特征工程仍然是一个关键但具有挑战性的步骤。尽管现有的基于LLM的方法可以自动生成新特征，但这些特征往往过于简单或重复。

**方法:** 提出了一种新方法REFeat，通过利用多种推理类型来指导LLM进行特征生成过程，从而发现更多样化和有意义的特征。

**结果:** 在59个基准数据集上的实验表明，该方法不仅平均预测准确性更高，还发现了更多样化和有意义的特征。

**结论:** 将丰富的推理范式和自适应策略选择纳入LLM驱动的特征发现中，对于表格数据具有很大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tabular+Feature+Discovery+With+Reasoning+Type+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20357，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20357&send_immediately=true&force_search=false)

**原文摘要:** Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [87] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry, Oran Nahum*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了两个重要贡献，以解决在给定上下文中验证主张的问题：一个紧凑的开源分类模型Paladin-mini（3.8B参数），用于判断数据是否具有支持性证据；以及一个新的评估数据集grounding-benchmark，用于评估关键推理任务的性能。论文还展示了Paladin-mini与当前最先进模型的基准测试结果，并分享了清晰且可重复的结果。


<details>
  <summary>更多</summary>
  
**动机:** 在文档和主张的背景下，确保主张有至少一个支持性的证据是重要的。然而，目前可能缺乏有效的工具和数据集来准确地评估这一问题。

**方法:** 提出了一种名为Paladin-mini的紧凑型开源分类模型（3.8B参数），用于判断数据是否有支持性证据（grounded或ungrounded）。同时开发了一个新的评估数据集grounding-benchmark，用于衡量在关键推理任务上的表现。

**结果:** Paladin-mini模型在基准测试中表现出色，与当前最先进的技术相比具有竞争力。此外，研究提供了清晰且可复现的结果。

**结论:** Paladin-mini模型和grounding-benchmark数据集为解决在给定上下文中验证主张的问题提供了有效的工具，增强了在现实场景中的稳健性，并推动了该领域的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Paladin-mini%3A+A+Compact+and+Efficient+Grounding+Model+Excelling+in+Real-World+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20384，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20384&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [88] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du, Bojie Shen, Muhammad Aamir Cheema, Adel N. Toosi*

**主要类别:** cs.AI

**AI概要:** 随着电动汽车（EV）的普及，现代服务系统越来越多地将电动汽车整合到运营中。本文提出了一个结合Vehicle-to-Grid (V2G)技术的新问题——Electric Vehicle Orienteering Problem with V2G（EVOP-V2G），并提出了解决方案，显著提高了司机的利润。


<details>
  <summary>更多</summary>
  
**动机:** 电动汽车逐渐被集成到现代服务系统中，但由于其续航里程较短以及V2G技术的出现，需要解决充电和放电管理的问题，以实现利润最大化。

**方法:** 通过构建混合整数规划（MIP）模型，并提出了两种近似最优的元启发式算法：进化算法（EA）和基于大邻域搜索的算法（LNS）。

**结果:** 实验表明，这些方法可以将司机的利润提高一倍，并且在小规模实例上接近最优性能，在大规模实例上表现出色。

**结论:** 这项工作为更智能、更有利可图的基于电动汽车的移动系统提供了一条有希望的途径，同时积极支持能源网络。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Smart+Ride+and+Delivery+Services+with+Electric+Vehicles%3A+Leveraging+Bidirectional+Charging+for+Profit+Optimisation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20401，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20401&send_immediately=true&force_search=false)

**原文摘要:** With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [89] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco, Willem van Jaarsveld, Remco Dijkman*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种名为GymPN的软件库，通过深度强化学习支持业务流程中的最优决策。它引入了对部分流程可观测性的支持和建模业务流程中多个决策的能力，解决了以往工作中的基本局限性。实验表明，GymPN能够轻松建模期望的问题并学习到最优决策策略。


<details>
  <summary>更多</summary>
  
**动机:** 现有的流程管理系统在任务分配、执行时间和人员指派等方面的决策支持存在局限性，无法充分满足组织的最优需求。此外，之前的软件工具缺乏对部分流程可观测性和多决策建模的支持，这限制了其在实际业务流程中的应用范围。

**方法:** 论文开发了一个名为GymPN的软件库，基于深度强化学习技术，用于支持业务流程中的最优决策。GymPN的主要创新点包括：1）支持部分流程可观测性；2）能够建模业务流程中的多个决策。这些特性使得GymPN可以更真实地表示业务流程中的复杂决策场景。

**结果:** 通过对八个典型的业务流程决策问题模式进行评估，结果表明GymPN能够轻松建模期望的问题，并且能够学习到最优的决策策略。这证明了GymPN的有效性和实用性。

**结论:** GymPN软件库通过引入部分流程可观测性和多决策建模的能力，显著提高了对现实业务流程决策的支持能力。该工具为优化业务流程中的决策提供了新的可能性，未来可以在更多复杂的业务场景中进行应用和扩展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GymPN%3A+A+Library+for+Decision-Making+in+Process+Management+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20404，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20404&send_immediately=true&force_search=false)

**原文摘要:** Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [90] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite, Giulio Caravagna, Andrea Sottoriva*

**主要类别:** cs.AI

**AI概要:** 提出Mixture of Neural Cellular Automata (MNCA)，将混合模型概念引入神经元胞自动机，通过结合概率规则分配和内在噪声来模拟生物过程中的随机动力学。在组织生长、图像形态发生鲁棒性和显微图像分割三个关键领域的实验表明，MNCA在抗扰动能力、真实生物生长模式再现以及规则可解释性方面表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 神经元胞自动机（NCA）虽能有效建模自组织过程，但其确定性本质限制了对真实世界生物和物理系统随机性的捕捉能力。

**方法:** 提出Mixture of Neural Cellular Automata (MNCA)，将混合模型与NCA范式结合，通过概率规则分配和内在噪声模拟多种局部行为及生物过程中的随机动力学。

**结果:** 在合成组织生长与分化模拟、图像形态发生鲁棒性测试和显微图像分割三个领域中，MNCA展现出更高的抗扰动能力、更准确地再现真实生物生长模式，并提供了可解释的规则分割结果。

**结论:** MNCAs是一种有潜力的工具，用于建模随机动力学系统和研究自增长过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixtures+of+Neural+Cellular+Automata%3A+A+Stochastic+Framework+for+Growth+Modelling+and+Self-Organization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20486，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20486&send_immediately=true&force_search=false)

**原文摘要:** Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [91] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin, Taylor Webb, Eric Elmoznino, Hakwan Lau*

**主要类别:** cs.AI

**AI概要:** 论文探讨了如何在机器中定义和实现感知，并提出感知信号需要具备持续性和质性特征。


<details>
  <summary>更多</summary>
  
**动机:** 为了设计和构建机器中的感知，需要一个明确的功能性和计算性定义。

**方法:** 提出感知信号需同时具备断言性和质性特征，并探讨了基于当前技术的潜在实现方式。

**结果:** 明确了人工代理功能性感知所需的条件，有助于避免无意中创建具有感知能力的AI。

**结论:** 理解人工代理的功能性感知条件，可以帮助我们及时意识到是否已创造出具有感知能力的AI。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Engineering+Sentience，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20504，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20504&send_immediately=true&force_search=false)

**原文摘要:** We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [92] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan, Minh-Son Dao, Koji Zettsu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于案例推理增强的大语言模型（CBR-LLM）框架，用于复杂风险场景下的避让操作决策。通过整合行车记录仪视频输入的语义场景理解和相关过往驾驶案例的检索，使大语言模型能够生成既与情境相关又符合人类行为的避让建议。实验表明，该框架提高了决策准确性、解释质量，并与人类专家行为更一致。风险感知提示策略和基于相似性的案例检索进一步提升了性能。


<details>
  <summary>更多</summary>
  
**动机:** 在关键安全场景中驾驶需要快速且基于情境的理解和经验推理的决策。虽然大语言模型具备强大的通用推理能力，但直接应用于自动驾驶仍面临领域适应、情境嵌入和缺乏动态高风险环境中的经验知识等挑战。

**方法:** 提出了一种Case-Based Reasoning Augmented Large Language Model (CBR-LLM)框架，将行车记录仪视频输入的语义场景理解与相关过往驾驶案例的检索相结合，以生成与情境相关的避让建议。

**结果:** 实验显示，该框架提高了决策准确性、解释质量和与人类专家行为的一致性。风险感知提示策略和基于相似性的案例检索在各种风险类型中表现优于随机采样。

**结论:** CBR-LLM框架在复杂的实际驾驶条件下表现出鲁棒性，有潜力成为智能驾驶系统的自适应和值得信赖的决策支持工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Case-based+Reasoning+Augmented+Large+Language+Model+Framework+for+Decision+Making+in+Realistic+Safety-Critical+Driving+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20531&send_immediately=true&force_search=false)

**原文摘要:** Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [93] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian, Jaewook Lee, Stefan P. Johannesson, Lennart Otte, Christer Hogstrand, Miao Guo*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种多智能体AI框架的概念验证，用于支持可持续蛋白质生产研究。该系统由两个基于GPT的LLM代理组成：一个用于检索指定微生物菌株的蛋白质生产相关文献，另一个用于提取相关的生物和化学信息。通过微调和提示工程两种方法优化代理，其中微调提高了平均分数（达到0.94），而提示工程则表现出较低的统计不确定性。开发了一个用户界面以方便使用该系统，并初步探索了额外的化学安全搜索功能。


<details>
  <summary>更多</summary>
  
**动机:** 全球对可持续蛋白质来源的需求促使需要能够快速处理和合成领域特定科学知识的智能工具。

**方法:** 提出了一个基于检索增强生成（RAG）系统的多智能体AI框架，包括两个基于GPT的LLM代理：文献检索代理和信息提取代理。通过微调和提示工程两种方法优化信息提取代理。

**结果:** 两种优化方法均有效提高信息提取代理的表现，平均余弦相似度得分提升了25%，达到了至少0.89的平均分数。微调整体上将平均分数提升到更高的水平（达到0.94），提示工程则表现出较低的统计不确定性。

**结论:** 多智能体AI框架为支持可持续蛋白质生产研究提供了概念验证，展示了其在微生物蛋白质来源研究中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Tuning+and+Prompt+Engineering+of+LLMs%2C+for+the+Creation+of+Multi-Agent+AI+for+Addressing+Sustainable+Protein+Production+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20598，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20598&send_immediately=true&force_search=false)

**原文摘要:** The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [94] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li, Roy Pea, Nick Haber, Hari Subramonyam*

**主要类别:** cs.AI

**AI概要:** The paper introduces CogGen, an AI architecture that turns programming videos into adaptive learning experiences using student modeling and generative AI tutoring.


<details>
  <summary>更多</summary>
  
**动机:** To improve video-based programming education by creating interactive, adaptive learning experiences from programming videos.

**方法:** CogGen consists of three components: (1) video segmentation by learning goals, (2) a conversational tutoring engine applying Cognitive Apprenticeship strategies, and (3) a student model using Bayesian Knowledge Tracing to adapt instruction.

**结果:** Technical evaluation shows effective video segmentation accuracy and strong pedagogical alignment across knowledge, method, action, and interaction layers. Ablation studies confirm the necessity of each component.

**结论:** CogGen advances AI-powered tutoring by integrating structured student modeling with interactive AI conversations, offering a scalable approach to enhance video-based programming education.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CogGen%3A+A+Learner-Centered+Generative+AI+Architecture+for+Intelligent+Tutoring+with+Programming+Video，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20600，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20600&send_immediately=true&force_search=false)

**原文摘要:** We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [95] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith, Junchao Zhang, Hong Zhang, Lois Curfman McInnes, Murat Keceli, Archit Vasan, Satish Balay, Toby Isaac, Le Chen, Venkatram Vishwanath*

**主要类别:** cs.AI

**AI概要:** 生成式AI，特别是通过大型语言模型（LLMs），正在改变技术知识的访问、重用和扩展方式。PETSc团队开始构建一个由LLM驱动的系统，结合PETSc内容与自定义LLM工具，如检索增强生成（RAG）、重新排序算法和聊天机器人，以帮助用户、支持开发者并提出对正式文档的更新。本文介绍了设计和评估这些工具的初步经验，重点关注系统架构、使用RAG和重新排序进行PETSc特定信息处理、各种LLMs和嵌入模型的评估方法以及用户界面设计。目标是建立一个可扩展的知识中心型AI框架，用于科学软件，使可扩展的支持、丰富的文档和增强的研究与开发工作流程成为可能。


<details>
  <summary>更多</summary>
  
**动机:** PETSc库经过三十年的发展积累了丰富但分散的知识库，其中大部分知识是非正式且用户及新开发者难以访问的。因此，需要更有效地激活和利用这一知识库。

**方法:** PETSc团队构建了一个由LLM驱动的系统，结合了PETSc内容与自定义LLM工具，包括检索增强生成（RAG）、重新排序算法和聊天机器人等，来帮助用户、支持开发者，并提出对正式文档的更新。该系统利用Argonne Leadership Computing Facility资源，分析LLM响应如何增强数值软件的开发和使用。

**结果:** 初步设计和评估了系统架构、RAG和重新排序在PETSc特定信息中的应用、不同LLMs和嵌入模型的评估方法及用户界面设计。结果表明，LLM可以增强数值软件的开发和使用，特别是在可扩展Krylov求解器方面。

**结论:** 建立了可扩展的知识中心型AI框架，用于科学软件，实现可扩展支持、丰富文档和增强研究与开发工作流程。未来将扩展此系统成为一个强大且不断发展的平台，推动软件生态系统加速科学发现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Assistants+to+Enhance+and+Exploit+the+PETSc+Knowledge+Base，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20608，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20608&send_immediately=true&force_search=false)

**原文摘要:** Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [96] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li, Weiwei Sun, Shanda Li, Ameet Talwalkar, Yiming Yang*

**主要类别:** cs.AI

**AI概要:** CoMind is a new machine learning agent that works well in exchanging insights and developing novel solutions within a community context, achieving state-of-the-art performance on MLE-Live.


<details>
  <summary>更多</summary>
  
**动机:** Existing ML agents typically operate in isolation without engaging with the broader research community. There is a need for an agent that can communicate and leverage collective knowledge from a simulated research community.

**方法:** Introduced MLE-Live, a live evaluation framework to assess an agent's ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Proposed CoMind, a novel agent designed to excel at exchanging insights and developing novel solutions within a community context.

**结果:** CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% human competitors on average across four ongoing Kaggle competitions.

**结论:** CoMind represents a significant advancement in ML agents capable of engaging with and benefiting from community interactions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Community-Driven+Agents+for+Machine+Learning+Engineering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20640&send_immediately=true&force_search=false)

**原文摘要:** Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [97] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu, Timon Willi, Jakob Foerster*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种名为Decrypto的游戏基准，用于评估大型语言模型（LLMs）在多智能体场景中的推理能力和心智理论（ToM）。通过实证评估，发现LLMs在游戏中的表现不如人类和简单的词嵌入基线，并且最先进的推理模型在ToM任务上表现不佳。Decrypto填补了当前推理和ToM评估中的关键空白。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型获得代理能力，它们需要在复杂的多智能体环境中进行导航，与人类用户和其他代理进行合作和竞争互动。这需要新的推理技能，特别是心智理论（ToM），即对其他代理的“心理”状态进行推理的能力。然而，现有的基准测试存在范围狭窄、数据泄漏、饱和以及缺乏交互性等问题，因此需要一个新的评估工具来更好地研究LLMs的ToM和多智能体能力。

**方法:** 作者提出了Decrypto，这是一个基于游戏的基准测试平台，灵感来自认知科学、计算语用学和多智能体强化学习。它设计得尽可能简单，消除了其他基准测试中常见的混淆因素，并且是第一个用于设计交互式ToM实验的平台。通过全面的经验评估，包括前沿LLMs的表现、鲁棒性研究以及人机交叉实验，验证了该基准的有效性。此外，作者还创建了两个经典认知科学实验的变体，以评估三种关键的ToM能力。

**结果:** 研究表明，LLMs在游戏中的表现落后于人类和简单的词嵌入基线。令人惊讶的是，最先进的推理模型在ToM任务上的表现比其较旧的版本更差。

**结论:** Decrypto解决了当前推理和ToM评估中的关键空白，为开发更好的人工代理铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Decrypto+Benchmark+for+Multi-Agent+Reasoning+and+Theory+of+Mind，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20664&send_immediately=true&force_search=false)

**原文摘要:** As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [98] [Data-Driven Dynamic Factor Modeling via Manifold Learning](https://arxiv.org/abs/2506.19945)
*Graeme Baker, Agostino Capponi, J. Antonio Sidaoui*

**主要类别:** stat.ML

**AI概要:** 本研究提出了一种数据驱动的动态因子框架，通过非线性流形学习技术（Anisotropic Diffusion Maps）揭示协变量和响应变量的联合动态，并利用线性扩散近似和卡尔曼滤波进行预测。该方法在压力测试中优于传统方法，在历史回测中显著降低了预测误差。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法可能无法充分捕捉高维协变量与响应变量之间的复杂非线性关系，且缺乏对时间序列数据特性的考虑。因此，需要一种更灵活、数据驱动的方法来建模和预测这些动态关系。

**方法:** 1. 使用Anisotropic Diffusion Maps提取协变量和响应变量的联合动态。
2. 通过线性扩散近似嵌入动态，并利用Kalman滤波直接从扩散图嵌入空间预测变量演化。
3. 推广了Singer关于图拉普拉斯算子收敛率分析，适用于Langevin扩散生成的时间序列。
4. 提供理论支持，证明扩散坐标线性扩散近似的鲁棒性和遍历平均值的收敛性。

**结果:** 该方法在股权组合的压力测试中表现优异，相比标准情景分析和主成分分析基准，分别减少了55%和39%的基于情景的组合回报预测均值绝对误差。

**结论:** 所提出的动态因子框架提供了一种有效的数据驱动方法，能够显著改善金融压力测试中的预测精度，并为高维动态系统建模提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-Driven+Dynamic+Factor+Modeling+via+Manifold+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.19945，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.19945&send_immediately=true&force_search=false)

**原文摘要:** We propose a data-driven dynamic factor framework where a response variable
depends on a high-dimensional set of covariates, without imposing any
parametric model on the joint dynamics. Leveraging Anisotropic Diffusion Maps,
a nonlinear manifold learning technique introduced by Singer and Coifman, our
framework uncovers the joint dynamics of the covariates and responses in a
purely data-driven way. We approximate the embedding dynamics using linear
diffusions, and exploit Kalman filtering to predict the evolution of the
covariates and response variables directly from the diffusion map embedding
space. We generalize Singer's convergence rate analysis of the graph Laplacian
from the case of independent uniform samples on a compact manifold to the case
of time series arising from Langevin diffusions in Euclidean space.
Furthermore, we provide rigorous justification for our procedure by showing the
robustness of approximations of the diffusion map coordinates by linear
diffusions, and the convergence of ergodic averages under standard spectral
assumptions on the underlying dynamics. We apply our method to the stress
testing of equity portfolios using a combination of financial and macroeconomic
factors from the Federal Reserve's supervisory scenarios. We demonstrate that
our data-driven stress testing method outperforms standard scenario analysis
and Principal Component Analysis benchmarks through historical backtests
spanning three major financial crises, achieving reductions in mean absolute
error of up to 55% and 39% for scenario-based portfolio return prediction,
respectively.

</details>


### [99] [A Principled Path to Fitted Distributional Evaluation](https://arxiv.org/abs/2506.20048)
*Sungee Hong, Jiayi Wang, Zhengling Qi, Raymond Ka Wai Wong*

**主要类别:** stat.ML

**AI概要:** 本论文扩展了广泛使用的拟合Q值评估(FQE)方法至分布式的离线策略评估(OPE)设定，并提出了构建理论基础的拟合分布评估(FDE)方法的指导原则。基于这些原则，作者开发了几种新的FDE方法，并提供了收敛性分析和理论证明，同时在多种实验环境中验证了FDE方法的优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前在强化学习中，离线策略评估主要集中在期望值估计上，但分布式离线策略评估（OPE）的研究相对较少，且缺乏统一的框架来设计FDE方法。因此，需要将FQE方法扩展到分布式的OPE设定，并建立相应的理论基础。

**方法:** 论文提出了一套构建FDE方法的指导原则，基于这些原则开发了新的FDE方法，并对这些方法进行了收敛性分析。此外，还为现有的FDE方法提供了理论依据。

**结果:** 通过大量的实验，包括线性二次调节器和Atari游戏的模拟，结果表明所提出的FDE方法具有更优的性能。

**结论:** 本文成功地将FQE扩展到了分布式的OPE设定，并为FDE方法的设计提供了理论支持，展现了其在不同环境下的优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Principled+Path+to+Fitted+Distributional+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20048，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20048&send_immediately=true&force_search=false)

**原文摘要:** In reinforcement learning, distributional off-policy evaluation (OPE) focuses
on estimating the return distribution of a target policy using offline data
collected under a different policy. This work focuses on extending the widely
used fitted-Q evaluation -- developed for expectation-based reinforcement
learning -- to the distributional OPE setting. We refer to this extension as
fitted distributional evaluation (FDE). While only a few related approaches
exist, there remains no unified framework for designing FDE methods. To fill
this gap, we present a set of guiding principles for constructing theoretically
grounded FDE methods. Building on these principles, we develop several new FDE
methods with convergence analysis and provide theoretical justification for
existing methods, even in non-tabular environments. Extensive experiments,
including simulations on linear quadratic regulators and Atari games,
demonstrate the superior performance of the FDE methods.

</details>


### [100] [Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives](https://arxiv.org/abs/2506.20114)
*Brian Liu, Rahul Mazumder, Peter Radchenko*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种从树集成中提取紧凑决策规则集的估计器，通过精确算法和近似算法解决优化问题，并证明其性能接近最佳线性组合的oracle。


<details>
  <summary>更多</summary>
  
**动机:** 尽管树集成模型预测能力强，但它们难以解释且可能无法揭示数据中有用的关系。因此需要一种方法来提取可解释的、准确的决策规则。

**方法:** 提出一种估计器以从树集成模型中提取紧凑的决策规则集，该估计器能够同时控制提取规则的数量和每个规则的交互深度。开发了精确算法和近似算法来解决优化问题，并生成正则化路径。

**结果:** 建立了非渐进预测误差界并与oracle进行比较，结果表明大样本预测性能与oracle相当。实验显示该估计器优于现有的规则提取算法。

**结论:** 所提出的估计器能有效提取准确且可解释的决策规则，其性能在理论上和实验上都表现优异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Extracting+Interpretable+Models+from+Tree+Ensembles%3A+Computational+and+Statistical+Perspectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20114，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20114&send_immediately=true&force_search=false)

**原文摘要:** Tree ensembles are non-parametric methods widely recognized for their
accuracy and ability to capture complex interactions. While these models excel
at prediction, they are difficult to interpret and may fail to uncover useful
relationships in the data. We propose an estimator to extract compact sets of
decision rules from tree ensembles. The extracted models are accurate and can
be manually examined to reveal relationships between the predictors and the
response. A key novelty of our estimator is the flexibility to jointly control
the number of rules extracted and the interaction depth of each rule, which
improves accuracy. We develop a tailored exact algorithm to efficiently solve
optimization problems underlying our estimator and an approximate algorithm for
computing regularization paths, sequences of solutions that correspond to
varying model sizes. We also establish novel non-asymptotic prediction error
bounds for our proposed approach, comparing it to an oracle that chooses the
best data-dependent linear combination of the rules in the ensemble subject to
the same complexity constraint as our estimator. The bounds illustrate that the
large-sample predictive performance of our estimator is on par with that of the
oracle. Through experiments, we demonstrate that our estimator outperforms
existing algorithms for rule extraction.

</details>


### [101] [Valid Selection among Conformal Sets](https://arxiv.org/abs/2506.20173)
*Mahmoud Hegazy, Liviu Aolaritei, Michael I. Jordan, Aymeric Dieuleveut*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种基于稳定性的方法，用于选择最优的预测集，同时保持覆盖率的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 在实际应用中，可能有多个有效的共形预测集可供选择，但选择最优集（如最小集）可能会使覆盖率保证失效。

**方法:** 提出了一种基于稳定性的方法来确保所选预测集的覆盖率，并将结果扩展到在线共形预测设置中，同时在具有额外结构的设置中提出了几种改进。

**结果:** 通过实验验证了该方法的有效性。

**结论:** 稳定性方法可以有效解决共形预测中选择最优预测集的问题，同时保留覆盖率保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Valid+Selection+among+Conformal+Sets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20173&send_immediately=true&force_search=false)

**原文摘要:** Conformal prediction offers a distribution-free framework for constructing
prediction sets with coverage guarantees. In practice, multiple valid conformal
prediction sets may be available, arising from different models or
methodologies. However, selecting the most desirable set, such as the smallest,
can invalidate the coverage guarantees. To address this challenge, we propose a
stability-based approach that ensures coverage for the selected prediction set.
We extend our results to the online conformal setting, propose several
refinements in settings where additional structure is available, and
demonstrate its effectiveness through experiments.

</details>


### [102] [POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.20406)
*Ruijia Zhang, Zhengling Qi, Yue Wu, Xiangyu Zhang, Yanxun Xu*

**主要类别:** stat.ML

**AI概要:** POLAR是一种新的悲观模型策略学习算法，用于离线DTR优化。它通过从离线数据中估计转移动态并量化每个历史-动作对的不确定性，将悲观惩罚纳入奖励函数以减少高不确定性的动作选择。POLAR直接针对最终学习策略的次优性，提供理论保证，并在统计和计算上都有保障。实验表明，POLAR优于现有方法，能产生接近最优的历史感知治疗策略。


<details>
  <summary>更多</summary>
  
**动机:** 现有的统计方法依赖于强假设且缺乏鲁棒性，而离线强化学习方法则关注平均训练表现，缺乏统计保证且需要解决复杂的优化问题。因此需要一种新方法来克服这些挑战。

**方法:** POLAR算法通过离线数据估计转移动态并量化历史-动作对的不确定性，然后将悲观惩罚加入奖励函数以减少高不确定性的动作选择。该方法不依赖于计算密集型的极小极大或约束优化程序，而是直接针对最终学习策略的次优性。

**结果:** 在合成数据和MIMIC-III数据集上的实证结果表明，POLAR优于现有方法，能够生成接近最优的历史感知治疗策略。

**结论:** POLAR是首个提供统计和计算双重保证（包括策略次优性的有限样本界）的基于模型的DTR方法，为离线DTR优化提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是POLAR%3A+A+Pessimistic+Model-based+Policy+Learning+Algorithm+for+Dynamic+Treatment+Regimes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20406&send_immediately=true&force_search=false)

**原文摘要:** Dynamic treatment regimes (DTRs) provide a principled framework for
optimizing sequential decision-making in domains where decisions must adapt
over time in response to individual trajectories, such as healthcare,
education, and digital interventions. However, existing statistical methods
often rely on strong positivity assumptions and lack robustness under partial
data coverage, while offline reinforcement learning approaches typically focus
on average training performance, lack statistical guarantees, and require
solving complex optimization problems. To address these challenges, we propose
POLAR, a novel pessimistic model-based policy learning algorithm for offline
DTR optimization. POLAR estimates the transition dynamics from offline data and
quantifies uncertainty for each history-action pair. A pessimistic penalty is
then incorporated into the reward function to discourage actions with high
uncertainty. Unlike many existing methods that focus on average training
performance, POLAR directly targets the suboptimality of the final learned
policy and offers theoretical guarantees, without relying on computationally
intensive minimax or constrained optimization procedures. To the best of our
knowledge, POLAR is the first model-based DTR method to provide both
statistical and computational guarantees, including finite-sample bounds on
policy suboptimality. Empirical results on both synthetic data and the
MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods
and yields near-optimal, history-aware treatment strategies.

</details>


### [103] [Scalable Subset Selection in Linear Mixed Models](https://arxiv.org/abs/2506.20425)
*Ryan Thompson, Matt P. Wand, Joanna J. J. Wang*

**主要类别:** stat.ML

**AI概要:** 本论文提出了一种新的ℓ0正则化方法，用于线性混合模型（LMM）子集选择，可以快速处理包含数千个预测变量的数据集。通过坐标下降算法和局部搜索算法，确保了计算效率和优化效果，并提供了有限样本的Kullback-Leibler散度界，证明了其在合成实验和真实数据中的优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的稀疏学习方法在线性混合模型（LMMs）中无法很好地扩展到包含数千个候选预测变量的数据集，而线性模型的稀疏方法又忽略了随机效应。因此需要一种能够高效处理大规模预测变量并保持解释性的方法。

**方法:** 提出了一种ℓ0正则化方法来进行LMM子集选择，开发了坐标下降算法作为主要计算工具，并保证了其收敛性；还提出了局部搜索算法以辅助非凸优化过程。此外，通过惩罚拟似然近似，该方法可扩展到广义LMM的子集选择。

**结果:** 理论上，提供了有限样本的Kullback-Leibler散度界；实验证明，该方法在合成数据上表现出色，并在生物学和新闻学的两个真实数据集中展示了其实用性。

**结论:** 所提出的ℓ0正则化方法填补了LMM稀疏学习方法与线性模型稀疏方法之间的差距，能够在包含数千个预测变量的数据集上高效运行，同时保持良好的预测和解释能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Subset+Selection+in+Linear+Mixed+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20425，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20425&send_immediately=true&force_search=false)

**原文摘要:** Linear mixed models (LMMs), which incorporate fixed and random effects, are
key tools for analyzing heterogeneous data, such as in personalized medicine or
adaptive marketing. Nowadays, this type of data is increasingly wide, sometimes
containing thousands of candidate predictors, necessitating sparsity for
prediction and interpretation. However, existing sparse learning methods for
LMMs do not scale well beyond tens or hundreds of predictors, leaving a large
gap compared with sparse methods for linear models, which ignore random
effects. This paper closes the gap with a new $\ell_0$ regularized method for
LMM subset selection that can run on datasets containing thousands of
predictors in seconds to minutes. On the computational front, we develop a
coordinate descent algorithm as our main workhorse and provide a guarantee of
its convergence. We also develop a local search algorithm to help traverse the
nonconvex optimization surface. Both algorithms readily extend to subset
selection in generalized LMMs via a penalized quasi-likelihood approximation.
On the statistical front, we provide a finite-sample bound on the
Kullback-Leibler divergence of the new method. We then demonstrate its
excellent performance in synthetic experiments and illustrate its utility on
two datasets from biology and journalism.

</details>


### [104] [Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery](https://arxiv.org/abs/2506.20533)
*Gilad Lerman, Kang Li, Tyler Maunu, Teng Zhang*

**主要类别:** stat.ML

**AI概要:** 本文研究了带动态平滑正则化的IRLS变体在鲁棒子空间估计中的线性收敛性，首次提供了全局收敛性保证，并展示了其在低维神经网络训练中的实际优势。


<details>
  <summary>更多</summary>
  
**动机:** 尽管IRLS方法在鲁棒子空间估计问题上具有优雅性和经验有效性，但其理论性质尚未被充分理解，因此需要更深入的理论分析和保障。

**方法:** 作者提出了一种带有动态平滑正则化的IRLS变体，并在确定性条件下证明了该方法从任何初始化开始都能线性收敛到潜在的子空间。此外，还将这些保证扩展到了仿射子空间估计的情景中。

**结果:** 理论结果表明，所提出的IRLS变体能够实现线性收敛，并且适用于缺乏先前恢复理论的仿射子空间估计问题。实验结果展示了IRLS方法在低维神经网络训练中的实用性。

**结论:** 本文为IRLS方法在鲁棒子空间恢复以及更广泛的非凸优化问题（如黎曼流形上的IRLS）提供了首个全局收敛性保证，推动了该领域理论与实践的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Global+Convergence+of+Iteratively+Reweighted+Least+Squares+for+Robust+Subspace+Recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20533&send_immediately=true&force_search=false)

**原文摘要:** Robust subspace estimation is fundamental to many machine learning and data
analysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and
empirically effective approach to this problem, yet its theoretical properties
remain poorly understood. This paper establishes that, under deterministic
conditions, a variant of IRLS with dynamic smoothing regularization converges
linearly to the underlying subspace from any initialization. We extend these
guarantees to affine subspace estimation, a setting that lacks prior recovery
theory. Additionally, we illustrate the practical benefits of IRLS through an
application to low-dimensional neural network training. Our results provide the
first global convergence guarantees for IRLS in robust subspace recovery and,
more broadly, for nonconvex IRLS on a Riemannian manifold.

</details>


### [105] [LARP: Learner-Agnostic Robust Data Prefiltering](https://arxiv.org/abs/2506.20573)
*Kristian Minchev, Dimitar Iliev Dimitrov, Nikola Konstantinov*

**主要类别:** stat.ML

**AI概要:** 这篇论文探讨了如何为下游学习任务预过滤公共数据集的问题，提出了LARP框架并分析了其性能损失和效用下降。


<details>
  <summary>更多</summary>
  
**动机:** 公共数据集中存在低质量或受污染的数据，许多学习方法对此敏感。因此需要构建一种与学习者无关的稳健数据预过滤方法，以保护下游学习器免受腐败数据的影响。

**方法:** 提出了LARP问题的形式化定义，并在Huber估计器和Huber数据污染模型的背景下实例化了该框架。研究了几个自然的预过滤程序，并通过理论分析表明，为一组学习器执行LARP可能导致性能损失。

**结果:** 理论结果表明，在异构学习器集合上执行LARP会导致性能损失。通过在真实世界图像和表格数据上的广泛实验，观察到统计显著的效用减少。

**结论:** LARP在大型数据集上具有优势，尽管对异构学习器集合进行预过滤会导致模型性能的损失，但通过博弈论框架可以权衡效用下降和重复预过滤的成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LARP%3A+Learner-Agnostic+Robust+Data+Prefiltering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20573，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20573&send_immediately=true&force_search=false)

**原文摘要:** The widespread availability of large public datasets is a key factor behind
the recent successes of statistical inference and machine learning methods.
However, these datasets often contain some low-quality or contaminated data, to
which many learning procedures are sensitive. Therefore, the question of
whether and how public datasets should be prefiltered to facilitate accurate
downstream learning arises. On a technical level this requires the construction
of principled data prefiltering methods which are learner-agnostic robust, in
the sense of provably protecting a set of pre-specified downstream learners
from corrupted data. In this work, we formalize the problem of Learner-Agnostic
Robust data Prefiltering (LARP), which aims at finding prefiltering procedures
that minimize a worst-case loss over a pre-specified set of learners. We first
instantiate our framework in the context of scalar mean estimation with Huber
estimators under the Huber data contamination model. We provide a hardness
result on a specific problem instance and analyze several natural prefiltering
procedures. Our theoretical results indicate that performing LARP on a
heterogeneous set of learners leads to some loss in model performance compared
to the alternative of prefiltering data for each learner/use-case individually.
We explore the resulting utility loss and its dependence on the problem
parameters via extensive experiments on real-world image and tabular data,
observing statistically significant reduction in utility. Finally, we model the
trade-off between the utility drop and the cost of repeated (learner-specific)
prefiltering within a game-theoretic framework and showcase benefits of LARP
for large datasets.

</details>
