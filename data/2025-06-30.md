<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 53]
- [cs.AI](#cs.AI) [总数: 17]
- [stat.ML](#stat.ML) [总数: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong, Zirun Guo, Yan Xia, Zehan Wang, Ziang Zhang, Tao Jin, Zhou Zhao*

**主要类别:** cs.LG

**AI概要:** Multimodal Large Language Models (MLLMs) struggle with complex reasoning despite their data integration capabilities. Reinforcement Learning (RL) can improve reasoning but poses challenges when applied to MLLMs such as performance degradation and overthinking. This paper investigates the impact of KL penalty and overthinking in RL training for MLLMs, proposing Asymmetric Policy Optimization (APO) to address these issues. APO includes Difficulty-Adaptive Divergence Shaping (DADS) for positive samples and Suboptimal Trajectory Complexity Regularization (STCR) for negative samples. The method was applied to Qwen2.5-VL-3B creating View-R1-3B, which showed a 7% gain over the base model and outperformed larger models on reasoning benchmarks without degrading general task performance.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to enhance the reasoning capabilities of MLLMs while avoiding common pitfalls like performance degradation on general tasks and overthinking, by investigating the effects of KL penalty and overthinking during RL training.

**方法:** The method involves using Asymmetric Policy Optimization (APO), which categorizes sampled responses into positive and negative groups. For positive samples, Difficulty-Adaptive Divergence Shaping (DADS) dynamically adjusts the KL divergence weight based on difficulty. For negative samples, Suboptimal Trajectory Complexity Regularization (STCR) penalizes overly long responses.

**结果:** View-R1-3B, derived from Qwen2.5-VL-3B, demonstrated an average 7% improvement over the base model and outperformed larger MLLMs on various reasoning benchmarks. Notably, it maintained consistent improvement on general tasks without degradation.

**结论:** The proposed DADS and STCR techniques effectively advance complex multimodal reasoning in MLLMs, showing strong generalization and applicability. The code will be released on GitHub.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是APO%3A+Enhancing+Reasoning+Ability+of+MLLMs+via+Asymmetric+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21655&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [2] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su, Jia Lin Hau, Gersi Doko, Kishan Panaganti, Marek Petrik*

**主要类别:** cs.LG

**AI概要:** 提出了一种Q学习算法，用于计算具有强收敛性和性能保证的总奖励ERM和EVaR目标的最优 stationary policy。在表格域上的数值结果表明，所提出的Q学习算法能够快速且可靠地收敛到最优的风险厌恶值函数。


<details>
  <summary>更多</summary>
  
**动机:** Risk-averse total-reward MDPs提供了一个有希望的框架来建模和解决未折扣的无限时间目标问题。然而现有的基于模型的算法在小规模问题中有效，但需要完全访问转移概率。

**方法:** 提出了一种Q-learning算法，用于计算总奖励ERM和EVaR目标的最优 stationary policy。该算法利用了ERM的动态一致性和可引出性。

**结果:** 在表格域上的数值结果表明，所提出的Q学习算法能够快速且可靠地收敛到最优的风险厌恶值函数。

**结论:** 所提出的Q学习算法具有强收敛性和性能保证，并能成功应用于总奖励ERM和EVaR目标的优化问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Risk-Averse+Total-Reward+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21683，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21683&send_immediately=true&force_search=false)

**原文摘要:** Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [3] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir, Jay Tenenbaum, Ariel Shamir*

**主要类别:** cs.LG

**AI概要:** 密度聚类方法在处理带噪声或任意数据分布时通常优于基于质心的聚类方法。本文揭示了密度聚类中聚类数与核心点邻域半径间的关键特性，并通过三元搜索算法提出更高效的半径选择策略，验证了其在高维大规模数据上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 密度聚类方法在处理带噪声或任意分布的数据方面表现优异，但参数调优（如邻域半径）在高维大规模数据上计算代价高昂，需要更高效的方法。

**方法:** 研究发现核心点邻域半径和聚类数之间的关系近似单峰性，并基于此特性结合三元搜索算法提出新策略以更高效地寻找合适的半径值。

**结果:** 通过在高维、大规模的NLP、音频和计算机视觉任务中的广泛实验，证明了该方法的有效性和鲁棒性。

**结论:** 本研究不仅为密度聚类提供了重要的参数控制改进，还加深了对密度聚类参数间关系的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unimodal+Strategies+in+Density-Based+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21695&send_immediately=true&force_search=false)

**原文摘要:** Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [4] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy, Wenzhao Zheng, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的方法，通过重连Transformer架构中的块来解决内部离散化ODE，同时在流匹配训练中使用时间和长度一致性项，从而实现动态控制质量-复杂度权衡。该方法减少了延迟和内存使用，并在图像生成实验中表现出高达3倍的延迟降低和最多3.5点FID分数的改进。


<details>
  <summary>更多</summary>
  
**动机:** 连续标准化流(CNFs)和扩散模型(DMs)可以生成高质量的数据点，但采样过程需要多次迭代求解常微分方程(ODE)，计算复杂度高。现有方法主要关注减少采样过程中的时间步数以提高效率，本文探索了另一种方向，即通过动态控制时间步长和神经网络长度来平衡质量和复杂度。

**方法:** 通过重连Transformer架构中的块来求解与长度相关的内部离散化ODE。在流匹配训练过程中引入时间和长度一致性项，使得采样可以在任意数量的时间步长和Transformer块上执行。这种方法在时间维度上与求解器无关，能够同时减少延迟和内存使用。

**结果:** 在CelebA-HQ和ImageNet上的图像生成实验表明，在最高效的采样模式下，延迟最多可减少3倍，高质量采样时FID分数最多可提高3.5点。

**结论:** 所提出的方法不仅显著提高了采样效率，还改善了生成数据的质量。通过开源代码和模型权重，确保实验结果完全可复现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%5Ctextrm%7BODE%7D_t+%5Cleft%28%5Ctextrm%7BODE%7D_l+%5Cright%29%24%3A+Shortcutting+the+Time+and+Length+in+Diffusion+and+Flow+Models+for+Faster+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21714，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21714&send_immediately=true&force_search=false)

**原文摘要:** Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [5] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri, Bryan Lewandowski, Cheng-Hsi Lin, Adrian N. Reyes, Grant C. Forbes, Arissa Wongpanich, Bangding Yang, Mohamed S. Abdelfattah, Sagi Perel, Xingyou Song*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种文本到文本回归方法，用于预测大型系统（如Google的Borg集群调度系统）的资源效率。该方法在无需特征工程的情况下，对复杂系统数据（如配置文件或系统日志）表现出色。实验表明，相比传统表格回归方法，新方法具有更高的秩相关性（接近完美0.99）和更低的均方误差（MSE低100倍）。此外，模型可通过少量样本适应新任务，并能捕捉复杂结果分布的密度。消融研究强调了使用编码器、增加序列长度以及模型内在不确定性量化的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 许多行业需要预测大型系统的度量结果，但传统表格回归方法在处理复杂系统数据（如配置文件或系统日志）时效果不佳，因为特征工程往往不可行。因此，需要一种通用且可扩展的替代方法来解决这一问题。

**方法:** 作者提出了文本到文本回归方法，利用一个6000万参数的编码器-解码器模型，从随机初始化开始训练。该方法适用于预测Google Borg集群调度系统的资源效率，无需进行特征工程。

**结果:** 在预测Borg系统资源效率的任务中，该模型实现了接近完美的0.99（平均值为0.9）秩相关性，均方误差（MSE）比表格方法低100倍。此外，模型仅需500个样本即可适应新任务，并能捕捉复杂结果分布的密度。

**结论:** 文本到文本回归方法为预测大型复杂系统提供了有效且通用的解决方案，其性能优于传统方法。同时，该方法具有良好的适应性和不确定性量化能力，为现实世界结果的通用模拟器奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performance+Prediction+for+Large+Systems+via+Text-to-Text+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21718，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21718&send_immediately=true&force_search=false)

**原文摘要:** In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [6] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou, Nanyu Luo, Feng Ji*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的框架，即联合项目反应理论（FedIRT），它能够在分布式环境下估计传统的IRT模型，同时保护隐私并减少通信成本，且不损失估计精度。作者通过数值实验和真实考试数据集验证了该方法的有效性，并提供了开源R包FedIRT以支持实际应用。


<details>
  <summary>更多</summary>
  
**动机:** 传统的IRT估计需要将所有个体的原始响应数据集中到一个地方，这可能会引发隐私问题。为了结合联邦学习的优势与现代心理测量学，提出了一种新的框架来解决这一问题。

**方法:** 提出了一种名为Federated Item Response Theory (IRT)的新框架，允许在分布式环境下进行IRT模型的估计，同时提供隐私保护和降低通信成本。该框架适用于两参数逻辑模型(2PL)和部分信用模型(PCM)。

**结果:** 数值实验表明，FedIRT在统计准确性上与使用流行R包的标准IRT估计相似，同时具有隐私保护和降低通信成本的优势。通过真实考试数据集验证了FedIRT在教育环境中的有效性。

**结论:** FedIRT扩展了IRT在分布式环境（如多校评估）中的适用性，同时保持了准确性和安全性。为支持实际应用，开发了一个开源R包FedIRT。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Item+Response+Theory+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21744，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21744&send_immediately=true&force_search=false)

**原文摘要:** Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [7] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter, Min Chi*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于梯度的神经可塑性适应方法，用于同时优化神经模糊网络（NFNs）的参数和结构，克服了传统方法中参数与结构优化分离的问题，并通过在线强化学习验证了其在视觉任务中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 神经模糊网络（NFNs）具有透明、符号化和通用函数逼近的优点，但其系统设计过程仍面临挑战。现有方法通常将参数和结构识别隔离，导致效率低下且架构脆弱。

**方法:** 提出一种与具体应用无关的基于梯度的神经可塑性适应方法，用于同时优化NFNs的参数和结构，避免了参数与结构优化的分离。

**结果:** 通过在线强化学习训练NFNs玩基于视觉的游戏DOOM，实证了同时优化NFNs参数和结构的有效性。

**结论:** 基于梯度的神经可塑性适应方法能够有效解决NFNs参数与结构优化问题，为NFNs在复杂任务中的应用提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gradient-Based+Neuroplastic+Adaptation+for+Concurrent+Optimization+of+Neuro-Fuzzy+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21771&send_immediately=true&force_search=false)

**原文摘要:** Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [8] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz, Elias Bareinboim*

**主要类别:** cs.LG

**AI概要:** LGES是一种改进版的GES算法，通过更精确的搜索策略提升了计算速度和准确性，并能利用干预数据及修正先验假设，实现在样本极限下恢复真实等价类。实验表明LGES在速度、准确性和对错误假设的鲁棒性上优于其他方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管经典的因果发现算法GES在理论上能够从观测数据中恢复描述数据的马尔可夫等价类，但在实际应用中存在计算成本高和有限样本精度不足的问题。因此，需要一种改进的方法来解决这些问题。

**方法:** 开发了Less Greedy Equivalence Search (LGES)，它通过修改贪婪步骤来避免变量之间因得分暗示某些条件独立而进行的边插入。此外，LGES可以使用先验假设引导搜索，并在数据矛盾时修正这些假设，同时还能利用干预数据进一步细化学习到的观测等价类。

**结果:** LGES相比GES实现了高达10倍的速度提升和显著的结构误差减少。实验还证明，LGES在速度、准确性和对错误假设的鲁棒性方面优于GES和其他基线方法。

**结论:** LGES不仅保留了GES的理论保证，还部分解决了其计算成本和有限样本精度问题。它能够在观测和干预数据（即使存在错误指定的先验假设）的样本极限下恢复真实的等价类。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Less+Greedy+Equivalence+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22331&send_immediately=true&force_search=false)

**原文摘要:** Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [9] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra, Dmitry Makarov, Aleksandr Panov*

**主要类别:** cs.LG

**AI概要:** M3PO是一种可扩展的基于模型的强化学习框架，结合了隐式世界模型和混合探索策略，提供了一种高效且稳健的替代现有基于模型的策略优化方法，并在多个基准测试中实现了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于模型的方法（如DreamerV3）依赖于忽视控制中心表示的像素级生成模型，而无模型方法（如PPO）则面临高样本复杂性和弱探索的问题。为了解决单任务设置中的样本效率低下和多任务领域中的泛化能力差的问题，提出了M3PO。

**方法:** M3PO整合了一个隐式世界模型，该模型被训练用于预测任务结果而不进行观察重建，以及一种结合了基于模型的规划和基于模型的不确定性驱动奖励的混合探索策略。通过使用基于模型和无模型价值估计之间的差异来引导探索，并通过信任区域优化器保持稳定的策略更新。

**结果:** M3PO提供了一种高效且稳健的替代现有基于模型的策略优化方法，在多个基准测试中实现了最先进的性能。

**结论:** M3PO解决了样本效率低下和多任务领域中的泛化能力差的问题，是一种有效的基于模型的强化学习框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M3PO%3A+Massively+Multi-Task+Model-Based+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21782，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21782&send_immediately=true&force_search=false)

**原文摘要:** We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [10] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini, Jong Youl Choi, Pei Zhang, Kshitij Mehta, Rylie Weaver, Ashwin M. Aji, Karl W. Schulz, Jorda Polo, Prasanna Balaprakash*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种多任务并行方法，通过GPU加速将每个解码头分配到计算资源上。该方法在开源HydraGNN架构中实现，并在三个超级计算机上进行了测试，表现出良好的扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 图神经网络的图基础模型为可持续、高效的原子建模提供了可能性。然而，在预训练过程中处理多源、多保真度数据仍存在挑战。现有研究采用多任务学习方法来应对这些挑战，但其在更大、更多样化数据集上的泛化能力和在超级计算机上的可扩展性仍有待验证。

**方法:** 研究者提出了一种多任务并行方法，使用GPU加速将每个解码头分布在计算资源上。此方法被实现在开源的HydraGNN架构中，并在超过2400万结构的五个数据集上进行训练。

**结果:** 实验结果表明，该方法在Perlmutter、Aurora和Frontier三个超级计算机上均展现出高效的扩展性，适用于高度异构的超级计算架构。

**结论:** 所提出的多任务并行方法能够有效提升图基础模型在大规模数据集上的泛化能力和在超级计算机上的可扩展性，为未来的原子建模提供了有力支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-task+parallelism+for+robust+pre-training+of+graph+foundation+models+on+multi-source%2C+multi-fidelity+atomistic+modeling+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21788，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21788&send_immediately=true&force_search=false)

**原文摘要:** Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [11] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang, Zhangyang Wang*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一个理论框架，解释了离散符号结构如何在连续神经网络训练动态中自然出现。通过将神经参数提升到测度空间并建模为Wasserstein梯度流，在几何约束（如群不变性）下，参数测度经历了两个现象：(1) 梯度流分解为一些势函数上的独立优化轨迹；(2) 自由度的逐步收缩。这些势函数编码与任务相关的代数约束，并作为测度空间上交换半环结构下的环同态。随着训练进行，网络从高维探索过渡到符合代数运算的组合表示，自由度降低。此外，论文建立了实现符号任务的数据缩放定律，将表示能力与促进符号解的群不变性联系起来。该框架为理解和设计结合连续学习与离散代数推理的神经符号系统奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望理解离散符号结构如何能从连续神经网络训练过程中自然产生，从而为神经符号系统的理论基础提供支持。这有助于更好地整合连续学习和离散代数推理，推动神经网络在符号任务中的应用。

**方法:** 研究者通过将神经网络参数映射到测度空间，并将训练过程建模为Wasserstein梯度流来分析其动态特性。在几何约束（如群不变性）下，研究者发现参数测度经历了两种现象：梯度流的分解和自由度的收缩。进一步地，研究者建立了数据缩放定律以连接表示能力和群不变性。

**结果:** 研究表明，在特定几何约束下，神经网络训练动态可以导致离散符号结构的自然出现。网络从高维探索逐渐过渡到具有较低自由度的组合表示，这些表示符合代数运算规则。并且，表示能力与群不变性之间存在明确关系，能够促进符号任务的实现。

**结论:** 本研究提出了一种理论框架，为理解离散符号结构在连续神经网络训练中的自然形成提供了新视角。这一框架不仅有助于设计更有效的神经符号系统，还为结合连续学习和离散代数推理提供了原则性的指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+Neural+Network+Can+Discover+Symbolic+Structures+with+Gradient-based+Training%3A+An+Algebraic+and+Geometric+Foundation+for+Neurosymbolic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21797，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21797&send_immediately=true&force_search=false)

**原文摘要:** We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [12] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal, Sunav Choudhary, Yuriy Brun, Hui Guan*

**主要类别:** cs.LG

**AI概要:** 在低资源环境下，前向模式自动微分(FmAD)和零阶(ZO)优化被提出作为反向传播(BP)的内存高效替代方案。然而，由于缺乏与高效的BP变体（如激活检查点）的比较以及统一的理论分析，其实际优势尚不明确。本文通过理论和实证研究对比了BP、FmAD和ZO方法。结果表明，尽管FmAD和ZO可以减少内存使用，但它们在精度、收敛速度和计算成本上显著逊色于带有检查点的BP，且随着模型增大或扰动预算受限，这种劣势更加明显。实验显示，在相似内存消耗下，带有检查点的BP比FmAD和ZO变体表现更优，包括那些采用方差缩减技术的变体，最高可提升31.1%的精度、加速34.8%的收敛，并减少3.8倍的计算量。这突显了FmAD和ZO的根本局限性，同时再次确认了带有检查点的BP是在内存受限情况下训练模型的最佳策略。


<details>
  <summary>更多</summary>
  
**动机:** 为了明确FmAD和ZO方法在低资源环境下的实际优势，弥补其与高效的BP变体（如激活检查点）之间缺乏比较以及统一理论分析的空白。

**方法:** 对BP、FmAD和ZO方法进行全面的理论和实证比较，包括分析内存使用、精度、收敛速度和计算成本等方面的影响，并通过大型语言模型和视觉-语言模型进行实验验证。

**结果:** 理论分析和实验证明，FmAD和ZO虽然能减少内存使用，但在精度、收敛速度和计算成本上远不如带有检查点的BP，且这种差距随着模型规模增大或扰动预算受限而加剧。实验结果展示了带有检查点的BP在多个指标上的优越性。

**结论:** FmAD和ZO存在根本性的局限性，在内存受限的情况下，带有检查点的BP仍然是训练模型最有效的策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Cost+of+Avoiding+Backpropagation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21833，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21833&send_immediately=true&force_search=false)

**原文摘要:** Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [13] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

**主要类别:** cs.LG

**AI概要:** 本论文探讨了在随机系统中使用Koopman算子理论处理部分观测的效果，强调了区分状态空间和函数空间的重要性，并展示了延迟嵌入技术对部分观测的益处。数值实验揭示了加性噪声幅度精度的幂律行为，还讨论了幂律行为指数与部分观测影响的关系。


<details>
  <summary>更多</summary>
  
**动机:** 由于有时难以对完整的一组可观测量进行全面观测，因此需要进行部分观测。尽管确定性系统有Mori-Zwanzig形式主义作为理论框架，但随机系统中的部分观测仍需进一步探讨，尤其是结合Koopman算子理论的情况。

**方法:** 利用Koopman算子理论研究随机系统中的部分观测效果，通过延迟嵌入技术来处理部分观测数据，并通过数值实验分析加性噪声幅度精度的幂律行为。

**结果:** 发现延迟嵌入技术在随机系统中对部分观测有益；数值实验显示加性噪声幅度精度呈现幂律行为；幂律行为的指数与部分观测的影响存在关系。

**结论:** 在随机系统中，部分观测可通过Koopman算子理论有效处理，延迟嵌入技术有助于提高观测效果，幂律行为可作为评估部分观测影响的一个指标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Koopman+operator-based+discussion+on+partial+observation+in+stochastic+systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21844，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21844&send_immediately=true&force_search=false)

**原文摘要:** It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [14] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan, Xin Yang, Yanhua Li, Wei Wei, Tianrui Li, Bo An, Jiye Liang*

**主要类别:** cs.LG

**AI概要:** 本文综述了持续强化学习（CRL），包括其核心概念、挑战和方法论。文章提出了一个新的CRL方法分类，并指出了CRL的独特挑战及未来方向。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习（RL）的成功依赖于大量的训练数据和计算资源，且在跨任务泛化能力上有限，这限制了其在动态和真实世界环境中的应用。因此，引入连续学习（CL）以克服这些限制，使CRL成为一个有前景的研究方向。

**方法:** 1. 对现有的CRL工作进行详细回顾，分析其指标、任务、基准和场景设置。
2. 提出了一种新的CRL方法分类法，从知识存储和/或转移的角度将其分为四种类型。
3. 阐述CRL的核心概念、挑战和方法论。

**结果:** 通过详细的审查和分析，提出了一种新的CRL方法分类法，并明确了CRL所面临的独特挑战。

**结论:** CRL为解决传统RL的局限性提供了一个新的研究方向。尽管面临诸多挑战，但该领域具有广阔的发展前景和实际应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+Continual+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21872，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21872&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [15] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer, Michael Burke, Mehrtash Harandi*

**主要类别:** cs.LG

**AI概要:** 本论文探讨了连续强化学习如何将强化学习（RL）智能体转化为动态连续学习者，强调了机器人领域中的最新进展，并为新手提供了评估环境的简要概述。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习任务的多样性和动态特性要求智能体能够进行顺序和持续的学习，即连续强化学习。此需求驱动了对连续学习在强化学习中应用的研究。

**方法:** 本文通过回顾连续强化学习的基础方面、关键概念、主要挑战和新方法来分析这一领域。特别关注机器人领域的最新进展及常用评估环境。

**结果:** 读者可以理解连续强化学习的基本原理及其在机器人领域的应用，同时了解当前面临的挑战和评估手段。

**结论:** 尽管连续强化学习取得了进展，但仍存在局限性。未来的研究方向包括改进知识保留与获取的方法，以及开发更有效的评估标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancements+and+Challenges+in+Continual+Reinforcement+Learning%3A+A+Comprehensive+Review，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21899&send_immediately=true&force_search=false)

**原文摘要:** The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [16] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun, Jianhua Pei, Ping Wang*

**主要类别:** cs.LG

**AI概要:** 论文提出TOAST框架，通过任务平衡、参数高效微调和扩散模型提升多任务优化在动态无线环境中的表现。


<details>
  <summary>更多</summary>
  
**动机:** 6G网络的发展要求从以比特为中心的传输转向强调任务相关信息的语义感知通信。

**方法:** 1. 将自适应任务平衡建模为马尔可夫决策过程，使用深度强化学习动态调整图像重建保真度与语义分类准确率之间的权衡。2. 在基于Swin Transformer的联合源信道编码架构中集成模块特定的低秩适应(LoRA)机制，减少适应开销并维持全性能。3. 引入阐明性扩散模型，在潜在空间恢复受信道噪声影响的特征。

**结果:** 在多个数据集上的广泛实验表明，TOAST相比基线方法在低信噪比条件下显著提高了分类准确性和重建质量，并在所有测试场景中保持稳健性能。

**结论:** TOAST框架为动态无线环境中多任务优化的核心挑战提供了一种有效的解决方案，展示了优越性能和广泛应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TOAST%3A+Task-Oriented+Adaptive+Semantic+Transmission+over+Dynamic+Wireless+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21900，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21900&send_immediately=true&force_search=false)

**原文摘要:** The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [17] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou, Mohamed Bennai*

**主要类别:** cs.LG

**AI概要:** 提出了一种混合量子-经典模型HQCM-EBTC，用于自动化脑肿瘤分类。该模型在包含正常、脑膜瘤、胶质瘤和垂体瘤的7,576个扫描数据集上训练，整合了5-qubit深度2的量子层与5个并行电路，通过AdamW优化器和组合损失函数进行优化。HQCM-EBTC在准确率（96.48%）、精确度和F1分数上显著优于经典的基线模型（86.72%），特别是在胶质瘤检测方面表现优异。t-SNE投影显示量子空间中特征可分离性增强，混淆矩阵表明误分类减少。注意力图分析（Jaccard指数）确认了高置信阈值下的更准确和聚焦的肿瘤定位。这些结果展示了量子增强模型在医学成像中的潜力，提高了临床脑肿瘤评估的诊断准确性和可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 当前脑肿瘤分类方法可能面临准确性不足或可解释性较差的问题，尤其是在复杂类型的肿瘤如胶质瘤的检测上。因此，探索一种结合量子计算优势的新型模型，以提升分类性能和诊断解释性成为研究的动机。

**方法:** 提出了一个名为HQCM-EBTC的混合量子-经典模型，该模型将5-qubit、深度为2的量子层与5个并行量子电路相结合，并使用AdamW优化器和融合交叉熵与注意力一致性的复合损失函数进行优化。模型在包含正常、脑膜瘤、胶质瘤和垂体瘤的7,576个MRI扫描数据集上进行了训练。

**结果:** HQCM-EBTC模型在测试中达到了96.48%的准确率，明显优于经典基线模型的86.72%。同时，模型在所有类别中均表现出更高的精确度和F1分数，尤其在胶质瘤检测方面效果显著。此外，t-SNE投影显示量子空间中的特征可分离性增强，混淆矩阵表明误分类情况减少，注意力图分析（Jaccard指数）验证了模型在高置信阈值下具有更准确和聚焦的肿瘤定位能力。

**结论:** 研究表明，量子增强模型在医学影像领域具有巨大潜力，能够提高脑肿瘤诊断的准确性和可解释性，为临床脑肿瘤评估提供了新的技术手段。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HQCM-EBTC%3A+A+Hybrid+Quantum-Classical+Model+for+Explainable+Brain+Tumor+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21937，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21937&send_immediately=true&force_search=false)

**原文摘要:** We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [18] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou, Mohamed Bennai*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种名为GuiderNet的元学习框架，通过几何元调节来改善参数化量子电路（PQCs）的训练性能，显著降低了训练损失并提高了测试准确率。


<details>
  <summary>更多</summary>
  
**动机:** 变分量子算法（VQAs）在近期量子优势方面具有潜力，但面临着梯度消失和优化景观条件不佳的问题。

**方法:** 引入了GuiderNet，一种元学习框架，利用数据依赖的参数移位来最小化Fubini-Study度量张量的对数条件数，引导PQC参数进入几何有利区域，并嵌入混合量子-经典管道中以指导初始化和自适应调制。

**结果:** 在Kaggle糖尿病分类任务中，GuiderNet将累积训练损失减少了5倍以上，将测试准确率从75.3%提高到98.6%，并将少数类F1分数从0.67提高到0.95。同时，它抑制了梯度爆炸并稳定了参数更新，使优化更平滑、更稳健。

**结论:** 几何元调节可以缓解梯度消失和不良条件问题，为增强量子机器学习中的可训练性和泛化能力提供了一种可扩展的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GuiderNet%3A+A+Meta-Learning+Framework+for+Optimizing+Quantum+Circuit+Geometry+and+Mitigating+Barren+Plateaus，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21940，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21940&send_immediately=true&force_search=false)

**原文摘要:** Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [19] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan, Haotian Wang, Xuhui Yu, Jiageng Chen, Xinyu Fan, Zuyuan He*

**主要类别:** cs.LG

**AI概要:** DAS应用中，数据获取困难和噪声强是重要问题。本文提出了一种基于物理信息的神经网络范式，无需真实世界事件数据进行训练，通过生成网络生成DAS事件数据，并使用去背景网络消除背景噪声。该方法在公共数据集和输送带故障监测中的表现优于或媲美基于真实世界数据训练的数据驱动网络，同时展示了跨场地的良好泛化能力。实验结果表明，在没有测试现场故障数据的情况下，该方法仍能达到91.8%的故障诊断准确率。


<details>
  <summary>更多</summary>
  
**动机:** 分布式声学传感（DAS）在多个领域受到广泛关注，但实际场景中可用事件数据有限，且背景噪声强烈，这限制了AI模型的应用效果。因此，需要一种不依赖真实世界数据的训练方法来解决数据获取和噪声问题。

**方法:** 提出了一种基于物理信息的DAS神经网络范式：
1. 通过对目标事件、实际环境和DAS系统的物理建模，推导出物理函数以训练生成网络，生成DAS事件数据。
2. 使用生成的DAS事件数据训练去背景网络，以消除DAS数据中的背景噪声。
3. 在事件识别和输送带故障监测应用中验证该范式的有效性。

**结果:** - 在公共DAS时空数据集上的事件识别应用中，表现与基于真实世界数据训练的数据驱动网络相当或更好。
- 在基于DAS时频数据的输送带故障监测应用中，达到了91.8%的故障诊断准确率。
- 展示了在不同场地间的良好泛化能力，即使在没有测试现场故障数据的情况下，也能取得高精度的结果。

**结论:** 所提出的基于物理信息的DAS神经网络范式为解决实际DAS应用中的数据获取困难和强噪声问题提供了有前景的解决方案，并有望探索更多DAS的潜在应用领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-informed+network+paradigm+with+data+generation+and+background+noise+removal+for+diverse+distributed+acoustic+sensing+applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21952，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21952&send_immediately=true&force_search=false)

**原文摘要:** Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [20] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang, Ali Ebrahimpour-Boroojeny, Hari Sundaram*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种轻量级的输出重加权遗忘方法RWFT，该方法可以在不完全重新训练的情况下从已训练的分类器中删除整个类别。通过引入新的基于总变差距离的度量标准，证明了该方法在防止成员推断攻击方面比现有技术更有效，并且与完全重新训练的结果相当。


<details>
  <summary>更多</summary>
  
**动机:** 从训练好的模型中遗忘特定类别对于执行用户删除权利和减轻有害或有偏差的预测至关重要。然而，完全重新训练成本高昂，现有的遗忘方法无法在预测未学习类别的样本时复制重新训练模型的行为。

**方法:** 1. 设计了一个成员推断攻击变体MIA-NN，用于揭示现有遗忘方法的失败。
2. 提出了一种简单的概率质量重新分布方法，以对遗忘类别的样本进行预测，该方法对MIA-NN具有鲁棒性。
3. 引入了一种新的基于总变差距离的度量标准，用于量化残余泄漏并防止未来方法受到新攻击的影响。

**结果:** 通过与最先进的基线方法进行广泛的实验比较，结果表明：
- 在先前工作中使用的评估指标中，该方法的表现与完全重新训练相当。
- 在新提出的TV-based度量标准中，性能优于现有最佳方法（分别提高了2.79%和111.45%）。

**结论:** RWFT方法提供了一种轻量级解决方案，能够在不完全重新训练的情况下有效地从分类器中删除类别，同时保持预测性能和安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Necessity+of+Output+Distribution+Reweighting+for+Effective+Class+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.20893，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.20893&send_immediately=true&force_search=false)

**原文摘要:** In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [21] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang, Yongxiang Tang, Yanxiang Zeng, Pengjia Yuan, Yanhua Cheng, Teng Sha, Xialong Liu, Peng Jiang*

**主要类别:** cs.LG

**AI概要:** R* Decision Transformer (R* DT) 是一种改进的决策变压器模型，专为在线广告中的自动竞价任务设计。它通过三个步骤（R DT、R^ DT 和 R* DT）克服传统DT模型的缺陷，并通过模拟生成高质量轨迹来优化训练数据集。实验表明，R* DT 在处理混合质量轨迹时表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 在线广告中，广告商参与广告位拍卖并使用需求方平台提供的自动出价工具。为了提高自动出价系统的自动化程度，需要解决传统决策变压器（DT）模型的局限性，例如需要预设回报值（RTG）以及受混合质量轨迹的限制。

**方法:** 提出了一种三步法：1) R DT - 类似于传统DT，存储基于状态和RTG的动作；2) R^ DT - 预测给定状态下RTG的最大值以推导次优策略；3) R* DT - 生成轨迹并通过模拟选择高奖励轨迹以增强训练数据集。

**结果:** 实验结果表明，R* DT 能够改善训练数据集中轨迹的RTG，并逐步引导次优策略向最优策略发展。

**结论:** R* DT 在公开的竞价数据集上的全面测试验证了其有效性，尤其在处理混合质量轨迹方面表现优异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Return-to-Go+Guided+Decision+Transformer+for+Auto-Bidding+in+Advertisement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21956&send_immediately=true&force_search=false)

**原文摘要:** In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [22] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan, John Lambert, Hong Jeon, Sakshum Kulshrestha, Yijing Bai, Jing Luo, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang*

**主要类别:** cs.LG

**AI概要:** 提出SceneDiffuser++，一种端到端生成式世界模型，可在城市规模上进行点A到点B的交通模拟，整合了场景生成、代理行为建模、遮挡推理、动态场景生成和环境模拟等需求。通过增强版Waymo Open Motion Dataset评估其模拟质量，展示其在长时间模拟条件下的优越现实主义。


<details>
  <summary>更多</summary>
  
**动机:** 目前的城市交通模拟需要整合多种技术，如场景生成、代理行为建模、遮挡推理、动态场景生成和环境模拟等，但某些关键技术（如动态场景生成和环境模拟）尚未得到充分研究。因此，需要一个统一的解决方案来实现无缝的城市级交通模拟。

**方法:** 开发了SceneDiffuser++，这是第一个基于单一损失函数训练的端到端生成式世界模型，能够满足城市规模的点A到点B交通模拟的所有需求，包括场景生成、代理行为建模、遮挡推理、动态场景生成和环境模拟等。

**结果:** 实验表明，SceneDiffuser++能够在城市规模上实现高质量的交通模拟，并在长时间模拟条件下展现出优越的现实主义。使用增强版Waymo Open Motion Dataset进行了评估，证明了模型的有效性。

**结论:** SceneDiffuser++为城市级交通模拟提供了一个全面的解决方案，展示了其在长时间模拟中的优越性能，推动了CitySim愿景的实现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SceneDiffuser%2B%2B%3A+City-Scale+Traffic+Simulation+via+a+Generative+World+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21976，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21976&send_immediately=true&force_search=false)

**原文摘要:** The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [23] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo, Javier Díaz-Rozo, Concha Bielza, Pedro Larrañaga*

**主要类别:** cs.LG

**AI概要:** This paper introduces a new type of probabilistic semiparametric model that uses data binning to reduce computational cost in kernel density estimation. Two new conditional probability distributions are developed which address the curse of dimensionality through sparse tensors and parent node restrictions. Experiments show that the new binned semiparametric Bayesian networks achieve similar results to non-binned models but with higher efficiency.


<details>
  <summary>更多</summary>
  
**动机:** To create a more computationally efficient model for kernel density estimation in nonparametric distributions without sacrificing accuracy.

**方法:** Development of two new conditional probability distributions for binned semiparametric Bayesian networks: sparse binned kernel density estimation and Fourier kernel density estimation. These methods use sparse tensors and limit parent nodes in conditional probability calculations to combat the curse of dimensionality.

**结果:** The binned semiparametric Bayesian networks perform structural learning and log-likelihood estimations with no statistically significant differences compared to non-binned semiparametric Bayesian networks, but with significantly higher speed.

**结论:** The new binned semiparametric Bayesian networks are a reliable and more efficient alternative to traditional non-binned models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Binned+semiparametric+Bayesian+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21997&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [24] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi, Riccardo Taormina, Elvin Isufi*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种用于图时间序列的图感知状态空间模型，该模型通过结合图结构和时间模式来捕捉数据特征。它利用噪声在图边上的传播特性，并通过深度学习架构进行参数学习和状态跟踪，适用于预测和插补等下游任务。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列分析在城市供水网络、经济学和神经科学等领域具有重要意义。然而，现有的方法通常依赖于计算成本高昂的模型，难以同时捕捉图结构和时间模式的数据特征。因此，需要一种新的模型来解决这些问题。

**方法:** 作者提出了一个图感知状态空间模型，其中隐含状态和观测方程都是由图诱导的参数化模型，具有少量需要学习的参数。具体而言，状态方程遵循由图边上噪声驱动的随机偏微分方程，考虑了边不确定性并增加了自由度。观测模型是状态的采样和图滤波版本，捕捉多跳邻居的影响。通过最大似然估计进行初步推断，随后使用基于Kalman神经网络的深度学习架构进行端到端的学习和状态跟踪。

**结果:** 该模型能够从部分观测数据中学习状态和观测模型的参数，适用于预测和插补等下游任务。相比传统的最大似然方法，深度学习架构在表达能力和可扩展性方面表现更优。

**结论:** 所提出的图感知状态空间模型可以有效捕捉图时间序列中的图-时间模式，通过深度学习方法进一步提升了模型的表达能力和可扩展性，为相关领域的时间序列分析提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GKNet%3A+Graph+Kalman+Filtering+and+Model+Inference+via+Model-based+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22004，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22004&send_immediately=true&force_search=false)

**原文摘要:** Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [25] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini, Joakim Bergdahl, Konrad Tollmar, Andrew D. Bagdanov, Linus Gisslén*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为TROFI的新方法，可以在没有预定义奖励函数的情况下进行离线强化学习。通过从人类偏好中学习奖励函数，并将其应用于数据集标注，从而实现有效的策略学习。实验表明，TROFI在D4RL基准和3D游戏环境中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 在离线强化学习中，通常需要使用奖励函数标注的数据集进行训练。然而，在实际应用中（如视频游戏开发），奖励函数可能不可用。为了解决这个问题，本文提出了TROFI方法。

**方法:** TROFI首先从人类偏好中学习奖励函数，然后利用该奖励函数对原始数据集进行标注，使其适用于策略训练。与其它方法不同，TROFI不需要最优轨迹。

**结果:** 通过在D4RL基准上的实验，证明了TROFI始终优于基线方法，并且其性能与使用真实奖励函数学习策略的性能相当。此外，还在3D游戏环境中验证了该方法的有效性。

**结论:** 研究强调了奖励函数在离线强化学习中的重要性：为了确保价值函数与实际未来折扣奖励的一致性，必须具备一个设计良好且易于学习的奖励函数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TROFI%3A+Trajectory-Ranked+Offline+Inverse+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22008&send_immediately=true&force_search=false)

**原文摘要:** In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [26] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang, Yu Zhao, Xuhui Sui, Baohang Zhou, Xiangrui Cai, Li Shen, Xiaojie Yuan, Dacheng Tao*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Federated Multimodal Knowledge Graph Completion (FedMKGC)的任务，并设计了一个框架MMFeD3-HidE来解决多模态知识图谱的联邦学习问题，通过实验验证了其有效性、语义一致性和收敛稳健性。


<details>
  <summary>更多</summary>
  
**动机:** 随着多模态知识私有化需求的增加，不同机构的多模态知识图谱通常是分散的，缺乏有效的协作系统以同时提供更强的推理能力和传输安全保障。

**方法:** 提出了一个名为MMFeD3-HidE的框架，其中包含两个主要部分：(1) Hyper-modal Imputation Diffusion Embedding模型（HidE），用于从不完整的实体嵌入中恢复完整的多模态分布；(2) Multimodal FeDerated Dual Distillation (MMFeD3)，通过logit和特征蒸馏在客户端和服务器之间相互传递知识。此外，还提出了一种FedMKGC基准，包括通用的FedMKGC主干网络MMFedE、具有异构多模态信息的数据集以及构建的三组基线。

**结果:** 在提出的基准上进行的实验验证了MMFeD3-HidE的有效性、语义一致性和收敛稳健性。

**结论:** 提出的FedMKGC任务及MMFeD3-HidE框架能够有效应对多模态不确定性不可用性和多模态客户端异构性的挑战，在联邦学习环境下提高了缺失链接预测的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyper-modal+Imputation+Diffusion+Embedding+with+Dual-Distillation+for+Federated+Multimodal+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22036&send_immediately=true&force_search=false)

**原文摘要:** With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [27] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han, Yu Liu, Qiwen Deng, Jian Jiang, Yinbo Sun, Zhe Yu, Binfeng Wang, Xingyu Lu, Lintao Ma, Han-Jia Ye, De-Chuan Zhan*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Unified Covariate Adaptation（UniCA）的新框架，用于弥合时间序列基础模型（TSFMs）与通用协变量感知预测之间的差距。该框架首先执行协变量同质化，将异构协变量转换为高级同质序列表示，然后通过统一的基于注意力的融合机制进行融合。实验表明，UniCA在单模态和多模态协变量感知预测基准上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 当前的时间序列基础模型（TSFMs）主要针对实值序列设计，在处理涉及多样且通常异构协变量（如分类变量、多模态数据等）的通用预测任务时能力有限。这促使研究者寻找一种方法，使TSFMs能够更好地适应这些复杂场景。

**方法:** 提出了一种名为Unified Covariate Adaptation（UniCA）的框架，该框架包括两个关键步骤：1) 协变量同质化，将异构协变量转化为高级同质序列表示；2) 通过统一的基于注意力的融合机制对这些表示进行融合。此方法适用于同质和异质协变量，并能在引入额外协变量信息的同时保持TSFMs的泛化能力。

**结果:** 广泛的实验表明，UniCA在多个单模态和多模态协变量感知预测基准上表现优异，证明了其在实际预测场景中的潜力。

**结论:** Unified Covariate Adaptation（UniCA）提供了一种有效的解决方案，增强了TSFMs在处理包含多样化和异构协变量的通用预测任务方面的能力，展现了其在现实世界预测场景中的广泛应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UniCA%3A+Adapting+Time+Series+Foundation+Model+to+General+Covariate-Aware+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22039&send_immediately=true&force_search=false)

**原文摘要:** Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [28] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, Shizhe Diao, Shiwei Liu, Yu Li, Yin Lu, Can Yang*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Gradient-Preserving Activation Scaling（GPAS）的新技术，可以有效解决Pre-LayerNorm Transformer架构中激活方差指数增长的问题，提升模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Pre-LayerNorm Transformer架构虽然在预训练期间稳定且可扩展到大型模型尺寸，但存在激活方差跨层指数增长的问题，导致残差路径占据主导地位并限制了深层的学习能力。

**方法:** 提出了Gradient-Preserving Activation Scaling（GPAS），通过缩放中间激活同时保持其梯度不变来解决问题，从而保留激活中的信息并避免与梯度下缩相关的梯度消失问题。

**结果:** 广泛的实验表明，GPAS在71M至1B不同模型尺寸上均实现了性能的持续提升，并且不仅增强了Pre-LN Transformers，还对Sandwich-LN和DeepNorm等替代架构表现出改进潜力。

**结论:** GPAS作为一种简单技术，能够与现有方法结合使用，展现出其多功能性和在各种场景中改善训练动态的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GPAS%3A+Accelerating+Convergence+of+LLM+Pretraining+via+Gradient-Preserving+Activation+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22049&send_immediately=true&force_search=false)

**原文摘要:** Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [29] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种结合长短期记忆网络（LSTM）和极限梯度提升（XGBoost）的混合深度学习与机器学习模型，用于加密货币价格预测。通过对比分析显示，该混合模型在多种加密货币的历史数据集上表现优于单独模型和传统预测方法。


<details>
  <summary>更多</summary>
  
**动机:** 加密货币市场的波动性和复杂动态为准确的价格预测带来了独特的挑战。

**方法:** 提出了一种混合模型，整合了LSTM和XGBoost。LSTM部分捕捉历史价格数据中的时间依赖性，而XGBoost通过建模非线性关系（如情绪分数和宏观经济指标等辅助特征）来增强预测能力。

**结果:** 使用平均绝对百分比误差（MAPE）和最小-最大归一化均方根误差（MinMax RMSE）进行比较分析，结果表明LSTM+XGBoost混合模型始终优于单独模型和传统预测方法。

**结论:** 研究表明，混合架构在金融预测中具有潜力，并提供了关于不同加密货币和市场环境下模型适应性的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是crypto+price+prediction+using+lstm%2Bxgboost，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22055，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22055&send_immediately=true&force_search=false)

**原文摘要:** The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [30] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

**主要类别:** cs.LG

**AI概要:** 本论文探讨了Transformer架构与图神经网络（GNNs）之间的联系，表明Transformer可以被视为在完全连接的图上进行消息传递的GNN，尽管其实现方式更高效。


<details>
  <summary>更多</summary>
  
**动机:** 为了建立Transformer架构与图神经网络（GNNs）之间的联系，并理解两者在表示学习上的相似性和差异性。

**方法:** 通过分析Transformer作为消息传递GNN在全连接图上的操作机制，其中自注意力机制捕捉令牌间的相对重要性，位置编码提供序列顺序或结构的提示。

**结果:** 发现Transformer是一种表达能力强的集合处理网络，能够学习输入元素间的关系，而不受预先设定的图结构限制。此外，由于其实现依赖于密集矩阵运算，在现代硬件上比稀疏消息传递更高效。

**结论:** Transformer可以被视为一种在当前硬件条件下非常高效的图神经网络，因此在某种程度上是'赢得硬件彩票'的GNN。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformers+are+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22084&send_immediately=true&force_search=false)

**原文摘要:** We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [31] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin, Attila Lischka, Jiaming Wu, Morteza Haghir Chehreghani, Balázs Kulcsár*

**主要类别:** cs.LG

**AI概要:** 本论文提出了两种神经网络方法来解决多目标多图路由问题，一个直接在多图上操作，另一个先将多图修剪为简单图再进行操作。实验表明这两种方法在多个问题上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 尽管多图设置在实际中非常重要，但基于学习的路由方法大多忽略了该场景，特别是在多目标情况下。

**方法:** 第一种方法直接在多图上通过自回归选择边构建路径；第二种方法先将多图修剪成简单图，然后再构建路径。

**结果:** 两种模型在包括旅行商问题和容量约束车辆路径问题等多个问题上表现出强大的性能。

**结论:** 提出的两种神经网络方法为多目标多图路由问题提供了有效的解决方案，并在多种问题上展示了良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Solve+Multi-Objective+Routing+Problems+on+Multigraphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22095&send_immediately=true&force_search=false)

**原文摘要:** Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [32] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang*

**主要类别:** cs.LG

**AI概要:** EFRame，一个探索-过滤-回放框架，通过增强GRPO在探索、样本过滤和经验回放三个方面的能力，不仅提高了训练的稳健性和效率，还使模型能够实现更深的推理能力，并允许对训练样本进行更细致的分类。


<details>
  <summary>更多</summary>
  
**动机:** 尽管GRPO降低了强化学习的计算成本，但在复杂推理任务上仍受到有限探索、低样本效率和不稳定性的限制。

**方法:** EFRame通过执行额外的rollouts来探索高质量轨迹，在线过滤以消除低质量样本，以及利用经验回放重复利用稀有但有用样本。

**结果:** 实验表明，EFRame提高了训练的稳健性和效率，使模型能获得更深的推理能力，并且实现了对训练样本更细粒度的分类。

**结论:** EFRame为大型语言模型提供了更完整的稳定学习周期，从探索到收敛都有结构化的转变。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EFRame%3A+Deeper+Reasoning+via+Exploration-Filtering-Replay+Reinforcement+Learning+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22200，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22200&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [33] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai, Farnaz Farid, Yueyang Kuan, Xintian Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于深度学习的模型，用于简化重金属评估过程，并通过转移学习解决了水沉积物领域的数据稀缺问题。该模型在澳大利亚新南威尔士州六个主要港口的数据评估中表现出显著较低的MAE和MAPE。


<details>
  <summary>更多</summary>
  
**动机:** 检测土壤和海港中的重金属污染对于区域环境监测至关重要，但传统的PLI评估涉及繁琐的过程和数据分析。

**方法:** 提出了一种基于深度学习的模型，利用转移学习来解决水沉积物领域中的数据稀缺问题，从而实现准确的定量评估方法以预测PLI。

**结果:** 模型在澳大利亚新南威尔士州六个港口的数据上进行评估，结果显示出大约0.5的MAE和0.03的MAPE，性能比其他基线模型高出两个数量级。

**结论:** 所提出的模型提供了一种创新、易用且成本效益高的方法来预测水质，有助于海洋生物保护、水产养殖和工业污染监测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transfer+Learning+for+Assessing+Heavy+Metal+Pollution+in+Seaports+Sediments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22096，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22096&send_immediately=true&force_search=false)

**原文摘要:** Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [34] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak, Michał Krutul, Jan Małaśnicki, Maciej Pióro, Jakub Krajewski, Sebastian Jaszczur, Marek Cygan, Kamil Adamczewski, Jan Ludziejewski*

**主要类别:** cs.LG

**AI概要:** 大型语言模型虽然通过增加规模提升了性能，但推理时间和计算需求也随之增加。为解决此问题，本文提出了Projected Compression方法，通过投影模块减少模型权重，最终形成降维的Transformer模型，且无需额外计算开销。实验表明该方法优于硬剪枝和再训练方法，特别是在高质量模型上。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型为了提升性能而不断增大，但这导致了更长的推理时间和更高的计算需求，因此需要一种有效的模型压缩方法来降低模型大小和计算成本。

**方法:** 提出了一种名为Projected Compression的新模型压缩技术，首先训练附加的可训练投影权重，保留对所有原始模型参数的访问；然后将这些投影合并到一个低维度的产品矩阵中，从而生成一个缩小尺寸的标准Transformer模型。此方法与基础模型的每令牌计算步骤在FLOPs上相匹配，不需额外计算开销。

**结果:** 实验结果表明，Projected Compression在高质量模型上的表现优于硬剪枝和再训练方法，并且随着令牌数量的增加，性能差距也呈良好扩展趋势。

**结论:** Projected Compression是一种有效的模型压缩技术，能够在不增加计算开销的情况下显著减小模型尺寸并保持或提高性能，特别是在高质量模型上效果明显。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Projected+Compression%3A+Trainable+Projection+for+Efficient+Transformer+Compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22255&send_immediately=true&force_search=false)

**原文摘要:** Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [35] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda, Gaurav Kumar Yadav*

**主要类别:** cs.LG

**AI概要:** This paper addresses the problem of class imbalance in predicting structural damage grades after earthquakes using SMOTE and various machine learning models.


<details>
  <summary>更多</summary>
  
**动机:** Earthquake-induced structural and infrastructural damage assessment is crucial for post-disaster response. Accurate estimation of damage grades influences lives, properties, and relief fund allocation.

**方法:** The research employs synthetic minority oversampling technique (SMOTE) to handle class imbalance. It explores multi-class classification using machine learning, deep learning models, and ensembling methods, along with comprehensive feature manipulation experiments and diverse training approaches.

**结果:** Performance determinants are elucidated through feature manipulation and different training techniques. Key factors contributing to seismic vulnerability are identified, and model performance is evaluated using the confusion matrix.

**结论:** This study enhances understanding of earthquake damage prediction effectiveness by addressing class imbalance and identifying key factors that contribute to seismic vulnerability.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Earthquake+Damage+Grades+Prediction+using+An+Ensemble+Approach+Integrating+Advanced+Machine+and+Deep+Learning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22129&send_immediately=true&force_search=false)

**原文摘要:** In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [36] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu, Longlong Lin, Yunfeng Yu, Xi Ou, Youan Zhang, Zhiqiu Ye, Tao Jia*

**主要类别:** cs.LG

**AI概要:** 提出了一种双通道GNN框架CoATA，通过共同增强拓扑结构和属性来应对现实图中的噪声和不完整性问题。实验表明CoATA在七个基准数据集上优于11种最先进方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的GNN方法在处理真实世界图的噪声和不完整性时效果不佳，且单维增强方法忽略了拓扑结构和节点属性之间的深层次相互作用。

**方法:** CoATA框架首先传播结构信号以丰富和去噪节点属性；然后将增强的属性空间投影到节点-属性二分图中进行进一步优化或重建；最后引入对比学习，利用原型对齐和一致性约束实现增强图与原图之间的相互校正。

**结果:** 在七个基准数据集上的广泛实验证明，CoATA显著优于11种最先进的基线方法。

**结论:** CoATA能够有效捕捉拓扑结构和属性之间的协同关系，解决了现有方法的不足。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoATA%3A+Effective+Co-Augmentation+of+Topology+and+Attribute+for+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22299，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22299&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [37] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng, Dawei Shi, Yang Shi, Long Wang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于再生核希尔伯特空间的控制律学习参数化方法，并设计了数据驱动的主动学习控制方法。通过Thompson sampling框架探索潜在的最优控制律，理论分析表明该方法以指数速率学习控制律与闭环性能指标之间的关系，并推导了控制后悔值的上界。数值实验验证了该方法在未知非线性系统中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的Thompson sampling方法依赖于有限的参数表示，限制了其在更通用空间中的应用，而这些空间在控制系统设计中更为常见。因此需要一种新的方法来克服这一限制。

**方法:** 1. 提出基于再生核希尔伯特空间的控制律学习参数化方法。
2. 设计数据驱动的主动学习控制方法，将控制律视为函数空间中的元素。
3. 构建Thompson sampling框架以探索潜在的最优控制律，并提供学习过程的收敛性保证。
4. 理论分析方法的学习速率和控制后悔值的上界。
5. 通过数值实验验证方法的有效性。

**结果:** 理论分析表明，所提出的方法能够以指数速率学习控制律与闭环性能指标之间的关系，并推导出了控制后悔值的上界。数值实验结果验证了该方法在控制未知非线性系统方面的有效性。

**结论:** 本研究提出的方法克服了传统Thompson sampling方法的限制，适用于更通用的空间。通过理论分析和实验验证，证明了该方法在学习控制律与性能指标关系方面具有高效性和有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thompson+Sampling-Based+Learning+and+Control+for+Unknown+Dynamic+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22186，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22186&send_immediately=true&force_search=false)

**原文摘要:** Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [38] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep, Samuel Genheden, Ola Engkvist, Jens Sjölund*

**主要类别:** cs.LG

**AI概要:** 大型语言模型（LLMs）和代理系统在药物发现与设计中提供了令人兴奋的机会。本文研究了基于LLM的代理系统的模块化特性，即LLM等组件是否可互换。通过对比不同LLM及工具调用型与代码生成型代理的表现，结果表明Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o表现更优。尽管代码生成型代理通常优于工具调用型代理，但其效果依赖于具体问题和模型。此外，提示词替换的影响也因问题和模型而异，这表明即使在此特定领域内，也不能简单地更换语言模型而不考虑提示词的重新设计。


<details>
  <summary>更多</summary>
  
**动机:** 探索基于LLM的代理系统在药物发现中的模块化特性，特别是LLM组件的可互换性，以推动稳定且可扩展解决方案的发展。

**方法:** 比较不同LLM在药物发现任务中的表现，并分析工具调用型与代码生成型代理的效果差异，同时评估系统提示词替换对性能的影响。

**结果:** Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o表现最佳；代码生成型代理通常优于工具调用型代理，但效果取决于具体问题和模型；提示词替换的影响也具有问题和模型相关性。

**结论:** 需要进一步研究代理系统的模块化特性，以开发适用于实际问题的稳定且可扩展的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Modularity+of+Agentic+Systems+for+Drug+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22189&send_immediately=true&force_search=false)

**原文摘要:** Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [39] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan, Zhiyuan Zhao, Fengwei Tian, Dung Nguyen, Payel Bhattacharjee, Ravi Tandon, B. Aditya Prakash, Anil Vullikanti*

**主要类别:** cs.LG

**AI概要:** 将深度学习和流行病模型结合的框架，利用含差分隐私的数据进行流行病预测和传播模型学习。


<details>
  <summary>更多</summary>
  
**动机:** 多元数据集在流行病学和公共健康分析中有很大价值，但其中一些数据集敏感，需要隐私保护。差分隐私（DP）因强保障而成为标准。

**方法:** 开发了一个整合深度学习和流行病模型的框架，同时进行流行病预测和传播模型的学习，并结合多个数据集（包括具有DP保障的数据集）。

**结果:** 使用具有差分隐私的合成金融数据集展示了该框架的价值，即使在DP保障下，该数据集对预测和学习流行病模型仍有显著贡献。

**结论:** 所提出的框架可以有效地结合多种数据源，在保护隐私的同时提升流行病预测和模型学习的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Framework+for+Multi-source+Privacy+Preserving+Epidemic+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22342&send_immediately=true&force_search=false)

**原文摘要:** It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [40] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao, Aaron Hurst, Panagiotis Karras, Daniel E. Lucani*

**主要类别:** cs.LG

**AI概要:** 尽管机器学习特别是深度学习发展迅速，但其对大量标注数据、计算和存储的需求仍然很高。本文提出了dreaMLearning框架，该框架允许在不解压的情况下从压缩数据中学习，基于熵驱动的无损压缩方法EntroGeDe。实验表明，dreaMLearning可以加速训练、减少内存使用和存储需求，同时对模型性能影响较小。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习需要大量的标注数据以避免过拟合，并且对计算和存储资源的需求巨大，这推动了研究如何用更少资源获得良好性能的新架构。

**方法:** 引入了dreaMLearning框架，它基于Entropy-based Generalized Deduplication (EntroGeDe) 方法，能够在不解压的情况下直接从压缩数据中进行学习。此框架适用于多种数据类型、任务和模型架构。

**结果:** 实验结果表明，与传统方法相比，dreaMLearning可以将训练速度提高至8.8倍，减少10倍的内存使用，并降低42%的存储需求，同时对模型性能的影响极小。

**结论:** dreaMLearning为各种机器学习应用提供了新的可能性，包括分布式学习、联邦学习以及边缘设备上的tinyML，提高了学习的效率和可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是dreaMLearning%3A+Data+Compression+Assisted+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22190&send_immediately=true&force_search=false)

**原文摘要:** Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [41] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha, Zhuojun Tian, Chaouki Ben Issaid, Mehdi Bennis*

**主要类别:** cs.LG

**AI概要:** In large-scale communication systems, conventional federated learning (FL) algorithms are limited in their applicability to real-world scenarios with diverse modalities. This paper proposes Sheaf-DMFL and Sheaf-DMFL-Att, which leverage sheaf theory and attention mechanisms respectively, to enhance collaboration among devices with diverse modalities.


<details>
  <summary>更多</summary>
  
**动机:** Conventional FL algorithms typically consider unimodal datasets, require identical model architectures, and fail to leverage the rich information embedded in multimodal data.

**方法:** Sheaf-DMFL is a decentralized multimodal learning framework leveraging sheaf theory. Each client has local feature encoders for different modalities, whose outputs are concatenated before passing through a task-specific layer. Encoders for the same modality are trained collaboratively across clients while capturing intrinsic correlations among clients' task-specific layers using a sheaf-based structure. Sheaf-DMFL-Att is an enhanced algorithm that tailors the attention mechanism within each client to capture correlations among different modalities.

**结果:** Extensive simulations on real-world link blockage prediction and mmWave beamforming scenarios demonstrate the superiority of the proposed algorithms in heterogeneous wireless communication systems.

**结论:** Sheaf-DMFL and Sheaf-DMFL-Att provide theoretical guarantees and enhance collaboration among devices with diverse modalities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sheaf-Based+Decentralized+Multimodal+Learning+for+Next-Generation+Wireless+Communication+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22374&send_immediately=true&force_search=false)

**原文摘要:** In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [42] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška, Gustav Šír*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了关系深度学习（RDL）模型在不同关系型数据库（RDBs）上的表现，并通过一个名为REDELEX的框架对70多个RDB进行了全面评估。研究表明，RDL模型通常优于传统方法，同时揭示了影响性能的关键因素，如模型复杂性、数据库大小及其结构特性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管关系深度学习（RDL）作为新范式出现，但尚缺乏对RDL模型性能与底层关系型数据库（RDBs）特性的系统分析。因此，需要深入研究这些模型在不同数据库上的表现及影响因素。

**方法:** 研究者开发了一个名为REDELEX的综合探索框架，用于评估不同复杂度的RDL模型在超过70个关系型数据库上的表现。该框架还对比了经典方法的关键代表，并分析了模型复杂性、数据库大小和结构属性等因素对性能的影响。

**结果:** 实验结果表明，RDL模型通常优于传统方法。此外，研究揭示了模型复杂性、数据库规模及其结构特性是影响性能的主要因素。

**结论:** 通过REDELEX框架的评估，确认了RDL模型在预测任务中的优越性，并为未来的研究提供了关于影响性能的关键因素的重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是REDELEX%3A+A+Framework+for+Relational+Deep+Learning+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22199，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22199&send_immediately=true&force_search=false)

**原文摘要:** Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [43] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang, Jian Wang, Rubing Chen, Xiao-Yong Wei, Qing Li*

**主要类别:** cs.LG

**AI概要:** 提出了一种概率框架和算法OptScale，用于优化推理时扩展的样本数量，减少计算开销同时保持高性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的推理时扩展技术依赖于启发式策略，缺乏理论基础。

**方法:** 通过假设平行样本独立同分布，提出概率框架并推导出达到目标性能所需的样本数的理论下限。基于此开发了OptScale算法，动态决定最优样本数。

**结果:** 在数学推理基准测试中，OptScale显著减少了采样开销，且性能优于或与现有最佳方法持平。

**结论:** 该研究为推理时扩展提供了理论基础和实用解决方案，推动了LLM在复杂推理任务中的高效部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probabilistic+Optimality+for+Inference-time+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22376，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22376&send_immediately=true&force_search=false)

**原文摘要:** Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [44] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik, Tianyu He, Andrey Gromov*

**主要类别:** cs.LG

**AI概要:** 本研究提出并训练了分布式神经架构（DNA），在视觉和语言领域实现了与密集模型基线相当的性能，并展示了计算效率/参数共享可以从数据中学习。此外，研究还分析了训练后的DNA中的连接性和计算模式，发现路径分布符合幂律，部分路径表现出自发的专业化，模型能够以可解释的方式分配计算和激活参数。


<details>
  <summary>更多</summary>
  
**动机:** 为了探索一种更灵活、更高效的神经网络架构，能够根据每个token或patch的内容和上下文自适应地调整计算和通信模式，同时保持与密集模型基线相当的性能。

**方法:** 初始化一个包含（transformer、MLP、attention等）模块和路由器的原型架构，任何token或patch可以以任意顺序通过任意系列的模块。在训练过程中端到端学习DNA模块的计算和通信模式，并可根据优化目标进一步添加如计算/内存效率或负载平衡等要求。

**结果:** (i) 训练后的DNA在两个领域中与密集基线具有竞争力；(ii) 计算效率/参数共享可以从数据中学习；(iii) 模型中的路径分布符合幂律；(iv) 部分路径表现出自发的专业化；(v) 模型能够以可解释的方式分配计算和激活参数。

**结论:** 分布式神经架构（DNA）提供了一种新的方法来设计灵活、高效的神经网络，其计算和通信模式可以根据数据内容和上下文自适应调整，同时展现出自发的专业化和可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Distributed+Neural+Architectures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22389&send_immediately=true&force_search=false)

**原文摘要:** We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [45] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga, Koji Tabata, Yuta Mizuno, Tamiki Komatsuzaki*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的随机多臂老虎机优化问题设定，通过均值-方差标准同时解决最大化期望回报和最小化不确定性的问题。提出了一个统一的元算法框架，能够在固定置信度和固定预算条件下运行，并通过自适应置信区间设计实现。理论分析和实验证明了该方法的有效性和优越性。


<details>
  <summary>更多</summary>
  
**动机:** 在不确定环境下，决策需要同时考虑最大化期望回报和最小化风险。传统方法通常只关注期望回报，缺乏对风险的考量。因此，需要一种新方法来平衡回报与风险。

**方法:** 引入了一个基于均值-方差标准的新问题设定，目标是找到在期望回报和风险之间达到最佳平衡的Pareto最优臂集合。提出了一种统一的元算法框架，适用于固定置信度和固定预算两种场景，通过自适应置信区间设计实现高效探索。

**结果:** 理论上证明了该方法在两种场景下的正确性。实验结果表明，该方法在准确性和样本效率上优于现有方法。

**结论:** 所提出的方法为不确定环境下的风险感知决策提供了有效工具，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Risk-Averse+Best+Arm+Set+Identification+with+Fixed+Budget+and+Fixed+Confidence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22253，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22253&send_immediately=true&force_search=false)

**原文摘要:** Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [46] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh, Alex Bui*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的框架，利用多视角对比学习将时间模式、基于导数的动力学和频域特征结合起来。通过独立编码器和分层融合机制学习可跨领域转移且保持时间一致性的特征不变表示。该方法在多种医疗数据集上显著优于现有技术。


<details>
  <summary>更多</summary>
  
**动机:** 适应不同领域的医学时间序列对机器学习模型来说是一个挑战，因为存在复杂的时间依赖性和动态分布偏移。当前方法通常关注孤立的特征表示，限制了其捕捉必要的时间动力学的能力。

**方法:** 提出了一个新框架，采用多视角对比学习来整合时间模式、基于导数的动力学和频域特征，并使用独立编码器和分层融合机制学习可跨领域转移且保持时间一致性的特征不变表示。

**结果:** 在包括EEG、ECG和EMG在内的多个医疗数据集上的广泛实验表明，该方法在迁移学习任务中显著优于现有技术。

**结论:** 本框架提高了机器学习模型的鲁棒性和泛化能力，为在多样化医疗环境中部署可靠的AI系统提供了实际途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-View+Contrastive+Learning+for+Robust+Domain+Adaptation+in+Medical+Time+Series+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22393，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22393&send_immediately=true&force_search=false)

**原文摘要:** Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [47] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia, Nikos Papadis, Murali Kodialam, TV Lakshman, Sayak Chakrabarty*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的聚类联邦学习算法CLoVE，通过模型损失的客户端嵌入来识别和分离不同集群的客户端，并优化集群特定模型。相比现有算法，CLoVE更简单、适用范围更广且无需近似最优模型初始化。实验表明CLoVE在几轮训练中就能实现高精度的聚类恢复，并在监督和非监督任务中达到最先进的模型精度。


<details>
  <summary>更多</summary>
  
**动机:** 在聚类联邦学习中，根据数据分布将客户端自然分组为集群是一项挑战，因为客户端分配未知。需要一种能够有效识别这些集群并优化集群特定模型的方法。

**方法:** CLoVE利用从模型损失得出的客户端嵌入，基于同一集群中的客户端具有相似损失值，而不同集群中的客户端表现出不同的损失模式这一见解，迭代地识别和分离来自不同集群的客户端，并通过联邦聚合优化集群特定模型。

**结果:** 理论分析表明，CLoVE能够在一轮中以高概率准确恢复集群，并在线性设置下快速收敛到最优模型。实验结果表明，CLoVE在多种数据集和非独立同分布（non-IID）设置下，仅需几轮训练即可实现高精度的聚类恢复，并在监督和非监督个性化联邦学习任务中达到最先进的模型精度。

**结论:** CLoVE是一种简单且鲁棒的聚类联邦学习算法，适用于监督和非监督设置，无需近似最优模型初始化，在实际应用中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLoVE%3A+Personalized+Federated+Learning+through+Clustering+of+Loss+Vector+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22427&send_immediately=true&force_search=false)

**原文摘要:** We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


### [48] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng, Changhao Wang, Guanwen Zhang, Yi Xu, Wei Zhou, Xiangyang Ji*

**主要类别:** cs.LG

**AI概要:** Low-rank tensor decompositions (TDs) are useful for multiway data analysis, but traditional methods rely on predefined assumptions that may not hold in practical scenarios. This paper proposes a score-based model to learn the compatibility between tensors and shared factors without requiring these assumptions. The method uses a neural network to optimize an energy function via score matching and integrates block coordinate descent (BCD) with smooth regularization for tensor completion and denoising.


<details>
  <summary>更多</summary>
  
**动机:** 传统的低秩张量分解方法依赖于预定义的结构假设，如CP或Tucker分解，这些假设在实际场景中往往不适用。此外，基于固定收缩规则的优化过程复杂且容易导致精度损失。

**方法:** 提出了一种基于得分的模型，无需预定义的结构或分布假设，通过设计神经网络学习能量函数，并利用得分匹配进行优化，以捕捉张量条目和共享因子的联合对数概率梯度。同时，将块坐标下降（BCD）算法与平滑正则化相结合，实现张量补全和去噪。

**结果:** 实验结果表明，该方法在各种类型的张量上表现出显著的性能提升，包括稀疏张量、连续时间张量以及视觉数据。

**结论:** 所提出的基于得分的模型能够有效学习张量与共享因子之间的兼容性，无需预定义假设，并且在张量补全和去噪任务中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Score-Based+Model+for+Low-Rank+Tensor+Recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22295，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22295&send_immediately=true&force_search=false)

**原文摘要:** Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [49] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo, Shinnosuke Matsuo, Shota Harada, Kiyohito Tanaka, Ryoma Bise*

**主要类别:** cs.LG

**AI概要:** 提出了一种弱监督领域适应方法，利用目标领域的类别比例信息进行伪标签分配，提升模型在不同数据分布下的表现。


<details>
  <summary>更多</summary>
  
**动机:** 领域偏移是机器学习中的重要挑战，尤其是在医疗应用中，由于数据收集实践、设备和程序的差异，导致不同机构的数据分布不同。当在源域数据上训练的模型应用于目标域时，性能可能会下降。现有的领域适应方法在源域和目标域类别比例不同时效果不佳。

**方法:** 提出一种弱监督领域适应方法，通过目标领域的类别比例信息为目标域的未标记数据分配伪标签（称为比例约束伪标签）。该方法不需要额外的注释，并且在类别比例不同的情况下也能有效工作。

**结果:** 在两个内窥镜数据集上的实验表明，该方法优于半监督领域适应技术，即使目标域中有5%的数据被标记。此外，使用噪声比例标签的实验结果表明该方法具有鲁棒性。

**结论:** 所提出的方法在领域适应方面表现出色，特别是在类别比例不同的情况下，并且对噪声比例标签具有鲁棒性，适用于实际应用场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weakly-Supervised+Domain+Adaptation+with+Proportion-Constrained+Pseudo-Labeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22301，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22301&send_immediately=true&force_search=false)

**原文摘要:** Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [50] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan, Aristotelis Siozopoulos, Maks Ovsjanikov*

**主要类别:** cs.LG

**AI概要:** Conditional Flow Matching (CFM)结合Koopman算子理论提出了一种无解码器架构，通过矩阵指数实现一步采样，显著加速了传统CFM，并在MNIST、F-MNIST和TFD等数据集上验证了其高效性。此外，该方法提供了一个结构良好的Koopman生成器，可分析生成行为的谱特性、特征值和特征函数。


<details>
  <summary>更多</summary>
  
**动机:** 虽然CFM提供了一个无需模拟的框架来训练连续时间生成模型，但其采样依赖于数值求解非线性ODE，计算成本高且难以解释。现有加速方法虽提升了采样速度，但未能揭示生成过程的底层结构。

**方法:** 将Koopman算子理论与CFM结合，提出一种无解码器的Koopman-CFM架构。该架构学习一个嵌入空间，在其中生成动力学变为线性，从而可通过矩阵指数实现闭式一步采样。

**结果:** 相比传统CFM，该方法在受控2D数据集和真实世界基准（如MNIST、F-MNIST和TFD）上实现了显著的速度提升。同时，提供了具有谱特性的Koopman生成器，可用于分析生成行为。

**结论:** Koopman增强的流匹配方法不仅提高了采样效率，还提供了可解释的分析工具，为快速且可解释的生成建模迈出了潜在的一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unfolding+Generative+Flows+with+Koopman+Operators%3A+Fast+and+Interpretable+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22304&send_immediately=true&force_search=false)

**原文摘要:** Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [51] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li, Haozhe Lei, Mingsheng Yin, Yaqi Hu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架PiPRL，通过符号化方法将物理信息归纳偏差注入到强化学习代理中，显著提高了训练效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前在强化学习中融入物理信息归纳偏差的方法需要大量的人工劳动和领域专业知识，限制了其广泛应用。

**方法:** 开发了物理信息程序引导的强化学习（PiPRL）框架，采用分层模块化的神经-符号集成方法，其中元符号程序接收来自神经感知模块的语义特征，这些特征构成了编码物理先验并指导低级神经控制器强化学习过程的符号编程基础。

**结果:** 广泛的实验表明，与纯符号或神经策略相比，PiPRL持续表现出更优的性能，并且借助程序导向的归纳偏差减少了超过26%的训练时间。

**结论:** PiPRL框架展示了如何有效利用物理先验知识提高强化学习在物理控制任务中的样本效率和泛化能力，同时降低了对领域专业知识的依赖。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+with+Physics-Informed+Symbolic+Program+Priors+for+Zero-Shot+Wireless+Indoor+Navigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22365&send_immediately=true&force_search=false)

**原文摘要:** When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [52] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的价值激励的参与者-评论家方法（VAC），通过乐观原则和原始-对偶优化视角，结合探索和开发。该方法在特定条件下有接近最优的后悔保证，并可扩展至更通用的函数逼近设置。


<details>
  <summary>更多</summary>
  
**动机:** 受到通过乐观正则化进行探索的最新发展的启发

**方法:** 本文提出了一种新的基于价值激励的参与者-评论家（VAC）方法，该方法从原始-对偶优化的角度解释了乐观原则。VAC方法优化了一个单一的、易于优化的目标，将探索和开发集成在一起——它促进了与收集的数据转换一致并导致更高价值函数的状态-动作和策略估计。

**结果:** 理论上，在线性马尔可夫决策过程（MDPs）中，所提出的VAC方法在有限范围和无限范围设置下都具有接近最优的后悔保证，这可以在适当的假设下扩展到一般函数逼近设置。

**结论:** 本文提供了一种新的在线强化学习方法，即价值激励的参与者-评论家方法，它在理论上有接近最优的表现保证，并且可以应用于更一般的函数逼近场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploration+from+a+Primal-Dual+Lens%3A+Value-Incentivized+Actor-Critic+Methods+for+Sample-Efficient+Online+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22401，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22401&send_immediately=true&force_search=false)

**原文摘要:** Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [53] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash, Ethan Chan, Nathan P. Lawrence, Karthik Pattabiraman*

**主要类别:** cs.LG

**AI概要:** ARMOR是一种新的强化学习控制方法，能够增强无人机在传感器受到攻击时的安全性和鲁棒性。通过两阶段训练框架，ARMOR无需特权信息即可实现实时部署，并且减少了对抗训练的需求，提高了对未知攻击的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习方法无法有效应对物理攻击（如GPS欺骗）对无人机传感器的影响，这些攻击可能导致状态估计被破坏并引发不安全行为。因此需要一种新的方法来提高无人机在对抗环境中的安全性与稳定性。

**方法:** ARMOR采用无模型的强化学习控制器，通过两阶段训练框架学习无人机物理状态的鲁棒潜在表示。第一阶段使用具有特权攻击信息的教师编码器生成对抗感知的潜在状态用于策略训练；第二阶段通过监督学习训练学生编码器以仅使用历史传感器数据逼近教师的潜在状态，从而实现实际部署。

**结果:** 实验表明，ARMOR优于传统方法，在确保无人机安全性的同时，提高了对未见过攻击的泛化能力，并通过消除迭代对抗训练需求降低了训练成本。

**结论:** ARMOR提供了一种有效的解决方案，使无人机能够在对抗性传感器操控下保持稳健运行，同时减少训练复杂度和提高泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ARMOR%3A+Robust+Reinforcement+Learning-based+Control+for+UAVs+under+Physical+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22423，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22423&send_immediately=true&force_search=false)

**原文摘要:** Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian, Shijie Zhang, Kevin Zhang, Xiaowei Chi, Yulin Luo, Junyu Lu, Chunkai Fan, Qiang Zhou, Yiming Zhao, Ning Liu Siyu Lin, Zhiyuan Qin, Xiaozhu Ju, Shanghang Zhang, Jian Tang*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种名为SEEA-R1的强化微调框架，通过Tree-GRPO和MGRM方法解决稀疏奖励问题和跨任务奖励估计问题，在ALFWorld基准测试中超越现有方法，包括GPT-4o，展示了其作为可自我演化的具身智能体的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前强化微调技术在提升大型语言模型推理能力方面表现出色，但在多模态互动中的自我演化智能体现仍待探索，主要受限于多步推理任务中缺乏有效的学习信号以及对人工设计奖励函数的依赖。

**方法:** 提出SEEA-R1框架，包含Tree-GRPO（将蒙特卡洛树搜索整合到组相对策略优化中以改善多步推理）和MGRM（用于跨任务和场景的奖励估计，支持自主适应和奖励驱动的自我演化）。

**结果:** 在ALFWorld基准测试中，文本和多模态任务分别达到85.07%和36.19%的分数，超过包括GPT-4o在内的先前模型；即使没有环境奖励，也获得了80.3%的分数，超越所有开源基线模型。

**结论:** SEEA-R1框架有效提升了具身智能体的自我演化能力，并在多个评测指标上展现了优越性，为未来可扩展的具身智能研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SEEA-R1%3A+Tree-Structured+Reinforcement+Fine-Tuning+for+Self-Evolving+Embodied+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21669，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21669&send_immediately=true&force_search=false)

**原文摘要:** Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [55] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, Yasin Abbasi Yadkori*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的递归架构——分层推理模型（HRM），它通过两个相互依赖的递归模块执行顺序推理任务，仅用2700万参数和1000个训练样本，在复杂推理任务上表现出色，甚至超过更大规模的模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型主要使用Chain-of-Thought技术，但该技术存在任务分解脆弱、数据需求大和高延迟的问题。受人类大脑分层和多时间尺度处理的启发，需要一种更稳定和高效的推理模型。

**方法:** 设计了分层推理模型（HRM），包含两个相互依赖的递归模块：高层模块负责慢速抽象规划，低层模块处理快速详细计算。HRM在单次前向传递中完成顺序推理任务，无需中间过程的显式监督。

**结果:** HRM仅用2700万参数和1000个训练样本，在复杂推理任务如数独和迷宫寻路中几乎达到完美表现，并且在ARC基准测试中超越具有更长上下文窗口的大规模模型。

**结论:** HRM展示了其作为通向通用计算和通用推理系统的重要进步的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Reasoning+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21734，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21734&send_immediately=true&force_search=false)

**原文摘要:** Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [56] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang, Jiyao Liu, Yulong Xiao, Junzhi Ning, Lihao Liu, Junjun He, Botian Shi, Kaicheng Yu*

**主要类别:** cs.AI

**AI概要:** 提出THE-Tree框架，通过构建领域特定的科技演化树，解决了科学创意生成中关于新颖性和事实准确性的评估问题。该框架使用独特的验证过程确保每一步都有根据，并在多个实验中表现出色，显著提高了科学预测和重要论文评估的性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）虽然加速了科学创意的生成，但对这些创意进行严谨的新颖性和事实准确性评估却成为瓶颈。现有的验证方法存在不足：LLMs可能产生幻觉且缺乏领域知识，而传统引用网络缺乏明确的因果关系，叙述性综述则无结构可言。

**方法:** 引入THE-Tree（技术历史演化树），一种计算框架，从科学文献中构建领域特定的演化树。使用搜索算法探索演化路径，在节点扩展时采用“Think-Verbalize-Cite-Verify”过程，LLM提出潜在进展并引用支持文献，通过自然语言推理机制验证每个提议的演化链接。

**结果:** 构建并验证了88个THE-Tree，涵盖多个领域，发布了一个基准数据集。实验表明：1）在图补全任务中，相比传统引用网络，THE-Tree使多个模型的hit@1提高8%到14%；2）预测未来科学发展时，hit@1指标提高近10%；3）与其他方法结合时，评估重要科学论文的性能几乎提升100%。

**结论:** THE-Tree提供了一种有效的方法来解决科学创意评估中的关键挑战，即缺乏结构化、可验证和因果关联的科学演化历史数据，其应用可显著提升相关任务的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是THE-Tree%3A+Can+Tracing+Historical+Evolution+Enhance+Scientific+Verification+and+Reasoning%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21763&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [57] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu, Xishun Liao, Haoxuan Ma, Jonathan Liu, Rohan Jadhav, Jiaqi Ma*

**主要类别:** cs.AI

**AI概要:** MobiVerse是一个混合框架，结合了轻量级领域特定生成器和大型语言模型（LLMs），用于生成基础活动链并根据上下文进行修改。该框架在洛杉矶Westwood的案例研究中成功模拟了约53,000个代理的日程安排，并能应对环境反馈如道路封闭、大型聚会事件和拥堵。实验结果表明，MobiVerse保持计算效率的同时增强了行为现实主义，为移动系统规划和操作提供了一个可定制平台。


<details>
  <summary>更多</summary>
  
**动机:** 理解和建模人类移动模式对于有效的交通规划和城市发展至关重要。然而，当前的移动研究存在一个关键差距，即缺乏允许大规模算法开发、政策实施和全面评估的仿真平台。传统的方法要么需要大量数据收集和手动校准，要么在适应动态条件方面存在困难，或者在大规模仿真时面临计算限制。

**方法:** 提出了MobiVerse，这是一个混合框架，利用轻量级领域特定生成器生成基础活动链，并通过LLMs进行上下文感知修改。该框架具有模块化设计，可以在交通系统和代理级别测试各种移动算法。

**结果:** 实验表明，MobiVerse使代理能够对环境反馈做出反应，包括道路封闭、大型集会事件和拥堵等。其模块化设计促进了不同移动算法的测试。结果表明，该方法在保持计算效率的同时增强了行为现实主义。

**结论:** MobiVerse通过提供一个可定制的平台，弥补了移动仿真中的空白，适用于移动系统规划和操作，并带有基准算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MobiVerse%3A+Scaling+Urban+Mobility+Simulation+with+Hybrid+Lightweight+Domain-Specific+Generator+and+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21784&send_immediately=true&force_search=false)

**原文摘要:** Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [58] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie, Narimasa Watanabe*

**主要类别:** cs.AI

**AI概要:** 构建了一个城市模拟器CitySim，利用大型语言模型的人类智能来生成更真实的城市行为模拟。通过赋予代理信念、长期目标和空间记忆，CitySim在微观和宏观层面都与真实人类行为更加一致，并可扩展用于理解城市现象。


<details>
  <summary>更多</summary>
  
**动机:** 现有的城市行为建模工作通常依赖于刚性、手工设计的规则，限制了对复杂意图、计划和适应性行为的模拟能力。

**方法:** 开发了一个名为CitySim的城市模拟器，使用递归价值驱动的方法生成现实的日程安排，平衡强制活动、个人习惯和情境因素。同时，赋予代理信念、长期目标和空间记忆以实现长期、类似生活的模拟。

**结果:** CitySim在微观和宏观层面上都表现出比以前的工作更接近真实人类的行为。实验结果展示了CitySim在估计人群密度、预测地点受欢迎程度和评估幸福感等实际场景中的应用潜力。

**结论:** CitySim作为一个可扩展且灵活的测试平台，为理解和预测城市现象提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CitySim%3A+Modeling+Urban+Behaviors+and+City+Dynamics+with+Large-Scale+LLM-Driven+Agent+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21805，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21805&send_immediately=true&force_search=false)

**原文摘要:** Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [59] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen, Sang T. Truong, Natalie Dullerud, Sanmi Koyejo, Carlos Guestrin*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种名为Active-MoSH的交互式局部-全局框架，用于高风险决策中的多目标优化问题。它通过整合软硬边界与概率偏好学习来逐步细化偏好结构，并利用多目标敏感性分析以建立决策者的信任。实验表明，该框架在提高收敛性和增强决策者信心方面具有显著优势。


<details>
  <summary>更多</summary>
  
**动机:** 在高风险决策中，需要在多个竞争目标之间进行权衡，而每个评估都可能耗费大量资源。现有的方法难以系统地迭代改进多维偏好结构，且决策者必须信任最终决定，确保没有错过更优方案。因此，需要一种能够有效引导用户并逐步完善偏好的框架。

**方法:** Active-MoSH框架包含两个主要部分：1) 局部组件结合软硬边界与概率偏好学习，通过分布建模动态调整Pareto子集；2) 全局组件T-MoSH使用多目标敏感性分析识别潜在被忽略的高价值点。此外，采用主动采样策略优化探索与开发之间的平衡，同时减少认知负担。

**结果:** 通过多样化的合成和真实世界应用，证明了Active-MoSH在性能上的提升。用户研究进一步验证了该框架在改善收敛性、增强决策者信任以及提供明确偏好表达方面的有效性。

**结论:** Active-MoSH为高风险决策中的多目标优化提供了一种有效的解决方案，其局部和全局组件协同工作，能够在降低认知负担的同时，帮助决策者找到更优解并增强对其决策的信任。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interactive+Multi-Objective+Probabilistic+Preference+Learning+with+Soft+and+Hard+Bounds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21887&send_immediately=true&force_search=false)

**原文摘要:** High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [60] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige, Amine Boumaza, Bruno Scherrer*

**主要类别:** cs.AI

**AI概要:** 传统确定性博弈求解算法通常基于简化模型分析，该模型假设叶节点值独立采样。然而这种模型忽略了游戏的结构性复杂度。本文提出了一种新的概率模型，通过引入祖先依赖关系来生成具有可调难度的游戏树，并推导了几个经典算法（如AlphaBeta和Scout）在该模型下的平均复杂度递归公式。研究发现，在深层有限树上，不同算法之间存在显著差异，例如AlphaBeta相较于Scout有更大的常数因子，导致实际运行速度较慢。


<details>
  <summary>更多</summary>
  
**动机:** 现有的博弈求解算法分析基于叶节点独立采样的简化模型，这忽略了真实游戏中结构复杂性，使得分析结果无法反映实际情况。因此需要一种更接近现实、同时保持一定分析可行性的新模型。

**方法:** 提出了一种新的概率模型，通过固定层次条件分布逐步构建游戏树，并强制执行祖先依赖关系以反映真实游戏的结构性复杂度。在此基础上，推导了几种经典算法（如AlphaBeta和Scout）在新模型下的平均复杂度递归公式。

**结果:** 在新模型下，虽然所有算法在渐近情况下似乎都收敛到相同的分支因子，但在深层有限树上，不同算法的表现存在显著差异。例如，AlphaBeta相比Scout具有更大的常数因子，导致实际运行速度变慢。

**结论:** 新模型为经典博弈求解算法提供了更现实、更具挑战性且可分析的框架，有助于深入理解这些算法的实际表现及差异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AlphaBeta+is+not+as+good+as+you+think%3A+a+new+probabilistic+model+to+better+analyze+deterministic+game-solving+algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21996&send_immediately=true&force_search=false)

**原文摘要:** Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [61] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda, Kazumi Kasaura, Yuta Oriike, Masaya Taniguchi, Akiyoshi Sannai, Sho Sonoda*

**主要类别:** cs.AI

**AI概要:** 本研究提出了LeanConjecturer，一个结合规则基础和大语言模型（LLMs）的混合系统，用于自动生成大学水平的数学猜想。通过迭代生成与评估，该系统从40个Mathlib种子文件中生成了12,289个猜想，其中3,776个被确认为语法有效且非平凡。这些生成的猜想不仅可用于强化学习训练，还成功验证了拓扑学中的几个非平凡定理，显示出超越现有结果简单变体的数学发现潜力。


<details>
  <summary>更多</summary>
  
**动机:** 在形式化定理证明中，数据稀缺是一个主要挑战，这限制了自动定理生成和验证系统的进步。此外，现有的方法通常依赖于已有数据集进行训练，难以生成新的、有意义的数学猜想。因此，需要一种创新的方法来解决这一问题，促进更广泛的数学发现。

**方法:** 提出了一种名为LeanConjecturer的管道，结合规则基础的上下文提取和基于大语言模型的定理陈述生成技术。具体步骤包括：1) 使用规则基础方法从现有数学库（如Mathlib）中提取背景信息；2) 利用LLMs生成新的数学猜想；3) 通过迭代生成与评估过程筛选出有效的、非平凡的猜想。最后，通过Group Relative Policy Optimization (GRPO) 方法，将生成的猜想用于强化学习训练，以增强定理证明能力。

**结果:** LeanConjecturer从40个Mathlib种子文件中生成了12,289个猜想，其中3,776个被确认为语法有效且非平凡。这些猜想不仅可用于强化学习训练，还成功验证了拓扑学中的几个非平凡定理，例如关于半开、alpha-开和预开集合的性质。平均每个种子文件生成了103.25个新猜想，提供了一个可扩展的解决方案，用于创建定理证明系统的训练数据。

**结论:** LeanConjecturer展示了一个成功的案例，即如何利用大语言模型和规则基础方法相结合的方式，自动生成高质量的数学猜想。这种方法不仅解决了形式化定理证明中的数据稀缺问题，还展示了其在数学发现方面的潜力，特别是对于那些超越现有结果简单变体的领域。未来的研究可以进一步优化生成和验证过程，探索更多数学领域的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LeanConjecturer%3A+Automatic+Generation+of+Mathematical+Conjectures+for+Theorem+Proving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22005&send_immediately=true&force_search=false)

**原文摘要:** We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [62] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang, Ziyan Jiang, Rui Meng, Yifei Leng, Zhenbang Xiao, Zora Zhiruo Wang, Yanyi Shang, Dehan Kong*

**主要类别:** cs.AI

**AI概要:** 本研究提出了多模态轨迹检索（Multimodal Trajectory Retrieval），通过构建统一的代理轨迹数据集（UATD）和GAE-Bench基准，以及提出GAE-Retriever框架，显著提升了轨迹数据的检索召回率，推动了AI代理在GUI环境中的能力发展。


<details>
  <summary>更多</summary>
  
**动机:** 轨迹数据能够捕捉人类行为和环境状态，在增强AI代理能力（特别是在GUI环境中）方面具有巨大潜力。然而，如何对轨迹级数据进行建模仍然是一个尚未系统解决的重要挑战。

**方法:** 1. 构建了一个名为Unified Agent Trajectory Dataset (UATD)的数据集，包含来自多样化真实场景的注释演示和状态。
2. 提出了GAE-Bench基准，包含大量基于轨迹的检索对。
3. 设计了GAE-Retriever框架，采用视觉-语言模型，并通过令牌选择和GradCache机制优化对比学习。

**结果:** 在多个数据集上的综合评估表明，GAE-Retriever在检索召回率上持续优于强大的基线方法，证明了其在推进多模态轨迹检索方面的有效性。

**结论:** 多模态轨迹检索为连接通用检索和以代理为中心的轨迹建模提供了桥梁。所提出的GAE-Retriever框架展示了卓越的效果，为未来的研究奠定了坚实的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Retrieval+for+Multimodal+Trajectory+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22056，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22056&send_immediately=true&force_search=false)

**原文摘要:** Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [63] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao, Runqing Guo, Yangyang Qin, Miangbing Meng, Jipeng Cao, Yilun Lin, Yisheng Lv, Fei-Yue Wang*

**主要类别:** cs.AI

**AI概要:** 本论文提出了Query as Test（QaT）的概念，通过统一的数据表示和逻辑查询来测试自动驾驶系统，并引入了Extensible Scenarios Notations（ESN）框架和Validation-Driven Development（VDD）概念，以提升测试的灵活性、解释性和安全性。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能在交通领域的深入应用，智能座舱、自动驾驶和智能路网三个关键领域的发展迅速，但其数据生态系统却日益碎片化且不兼容。现有的测试方法依赖于数据堆叠，无法覆盖所有边缘情况，缺乏灵活性。

**方法:** 论文提出了一种名为“Query as Test”（QaT）的新概念，将功能验证和安全合规性检查转化为对统一数据表示的逻辑查询；设计了一种新型声明式数据框架“Extensible Scenarios Notations”（ESN），基于Answer Set Programming（ASP），能够统一表示来自座舱、车辆和道路的异构多模态数据；还介绍了“Validation-Driven Development”（VDD）理念，倡导以逻辑验证引导开发而非传统的定量测试。

**结果:** 该方法不仅实现了数据的深度语义融合，还带来了三大核心优势：支持复杂灵活的语义查询、提供决策过程的自然可解释性、以及通过逻辑规则实现按需数据抽象从而保障隐私保护。此外，QaT范式显著增强了测试的表达能力和形式严谨性。

**结论:** QaT、ESN和VDD共同为自动驾驶系统的功能验证和安全性检查提供了更高效、更灵活的解决方案，有助于加速迭代和开发过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Query+as+Test%3A+An+Intelligent+Driving+Test+and+Data+Storage+Method+for+Integrated+Cockpit-Vehicle-Road+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22068，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22068&send_immediately=true&force_search=false)

**原文摘要:** With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [64] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François, Ludovic Péran, Ayah Bdeir, Nouha Dziri, Will Hawkins, Yacine Jernite, Sayash Kapoor, Juliet Shen, Heidy Khlaaf, Kevin Klyman, Nik Marda, Marie Pellat, Deb Raji, Divya Siddarth, Aviya Skowron, Joseph Spisak, Madhulika Srikumar, Victor Storchan, Audrey Tang, Jen Weedon*

**主要类别:** cs.AI

**AI概要:** 本文探讨了AI系统的安全性问题，并提出了五个优先研究方向，以推动开放、多元和负责任的AI安全学科的发展。


<details>
  <summary>更多</summary>
  
**动机:** 随着开源权重和开源基础模型的迅速兴起，确保AI系统安全的需求愈发迫切，同时这也带来了新的机遇。

**方法:** 通过参与式、解决方案导向的过程，工作小组在安全与开源AI的交叉点上制定了研究议程，绘制了现有和需要的技术干预及开源工具图谱，以及内容安全过滤器生态系统的映射。

**结果:** 发现开放性（透明权重、互操作工具、公共治理）可以增强安全性，但仍然存在多模态和多语言基准不足、针对提示注入和组合攻击的防御有限等问题。

**结论:** 文章提出了五个优先研究方向：强调多方参与、未来验证的内容过滤器、全生态系统的安全基础设施、严格的代理保护措施和扩展的危害分类法。这些建议为2025年法国AI行动峰会提供了信息支持，并为开放、多元和负责任的AI安全学科奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Different+Approach+to+AI+Safety%3A+Proceedings+from+the+Columbia+Convening+on+Openness+in+Artificial+Intelligence+and+AI+Safety，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22183&send_immediately=true&force_search=false)

**原文摘要:** The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [65] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine, Emile van Krieken, Luciano Serafini*

**主要类别:** cs.AI

**AI概要:** 本论文探讨了秩瓶颈对知识图谱补全(KGC)模型的影响，并提出了一种基于混合输出层的方法KGE-MoS以解决该问题。实验表明，KGE-MoS在低参数成本下提高了KGC模型的性能和概率拟合。


<details>
  <summary>更多</summary>
  
**动机:** 许多知识图谱补全模型尽管使用了强大的编码器，但依赖于简单的向量-矩阵乘法来评分查询与候选对象实体。当实体数量大于模型的嵌入维度时（这种情况在实际应用中经常出现），线性输出层会出现秩瓶颈，限制模型表达能力。

**方法:** 研究者从理论和实证两方面分析了秩瓶颈如何影响KGC模型，并受语言建模文献启发，提出了KGE-MoS，一种基于混合的输出层方法，用于打破KGC模型中的秩瓶颈。

**结果:** 在四个数据集上的实验表明，KGE-MoS提高了KGC模型的性能和概率拟合，且参数成本较低。

**结论:** 秩瓶颈会损害KGC模型的排名准确性和分数分布保真度，而KGE-MoS能够有效缓解这一问题，提升模型表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+Rank+Bottlenecks+in+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22271&send_immediately=true&force_search=false)

**原文摘要:** Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [66] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

**主要类别:** cs.AI

**AI概要:** 这篇论文探讨了在人机协作中扩展AI自主性的必要性，提出了'智能不服从'的概念，并通过不同自治层级的实例分析其表现形式，最后提出了研究不服从行为作为人工代理核心能力的初步界限和考虑因素。


<details>
  <summary>更多</summary>
  
**动机:** 尽管人工智能在许多任务上取得了超人的表现，但大多数合作AI系统仍然严格遵循人类指令，这种无条件服从可能在某些情况下是反生产力或不安全的。因此，需要重新思考AI在团队中的角色和自主性。

**方法:** 论文引入了一个AI自主性等级尺度，使用代表性的例子来说明在人机协作环境中将AI自主性作为一个独立研究重点的重要性。然后探讨了智能不服从在不同自主性水平上的表现形式。

**结果:** 论文展示了智能不服从在不同自主性水平上的具体表现，并强调了将其视为人工代理核心能力进行研究的必要性。

**结论:** 作者提出应将不服从行为的研究设为一个独立领域，明确了初步的研究边界和注意事项，以促进更有效、更安全的人机协作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Artificial+Intelligent+Disobedience%3A+Rethinking+the+Agency+of+Our+Artificial+Teammates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22276&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [67] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst, Dominik Dürrschnabel, Johannes Hirth, Gerd Stumme*

**主要类别:** cs.AI

**AI概要:** 提出了一种基于正式概念分析（FCA）的方法FAT-CAT，用于增强有意义的主题聚合和可视化。该方法在案例研究中表现出比现有主题建模技术更有意义和可解释的见解。


<details>
  <summary>更多</summary>
  
**动机:** 数据量的快速增长使得传统的手动检查变得不可行，需要采用计算方法进行高效的数据探索。现有的主题建模方法难以提供可解释的表示，限制了对数据结构和内容的深入洞察。

**方法:** 提出FAT-CAT方法，基于正式概念分析（FCA），能够处理多样化的主题和文件类型，并通过构建概念格提供主题分布的结构化、分层表示。

**结果:** 在ETYNTKE数据集上的案例研究中，FAT-CAT相较于其他表示方法提供了更有意义和可解释的数据集组成见解。

**结论:** FCA-based aggregation在主题建模方面可以提供更可解释的表示，有助于更深入地理解数据结构和内容。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conceptual+Topic+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22309&send_immediately=true&force_search=false)

**原文摘要:** The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [68] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hervé Jégou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Théo Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Jitendra Malik*

**主要类别:** cs.AI

**AI概要:** 本论文探讨了以视觉、虚拟或实体形式呈现的AI代理的研究，这些代理能够与用户及其环境进行交互。提出世界模型的发展是具身AI代理推理和规划的核心，并且还提出了学习用户的心理世界模型以实现更好的人机协作。


<details>
  <summary>更多</summary>
  
**动机:** 当前的AI代理在与环境和用户交互方面存在局限性，特别是缺乏对环境的理解和预测能力以及对用户意图和社会背景的理解。因此，研究具身AI代理及其世界模型的发展变得重要。

**方法:** 研究包括虚拟化身、可穿戴设备和机器人等具身AI代理的设计和开发，使其能够在周围环境中感知、学习和行动。通过发展世界模型来增强这些代理的理解和预测能力，包括物理世界的多模态感知、基于推理的动作和控制规划，以及创建对物理世界的全面理解。此外，还提出学习用户的心理世界模型以促进人机协作。

**结果:** 研究表明，具身AI代理通过发展世界模型可以更好地理解和预测其环境，理解用户意图和社会背景，从而提高执行复杂任务的自主能力。同时，学习用户的心理世界模型有助于实现更高效的人机协作。

**结论:** 具身AI代理的世界模型发展对其推理、规划和执行复杂任务的能力至关重要。未来的研究方向可能包括进一步优化世界模型的学习方法，以及深化对用户心理世界模型的理解，以推动AI代理在实际应用中的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embodied+AI+Agents%3A+Modeling+the+World，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22355&send_immediately=true&force_search=false)

**原文摘要:** This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [69] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri, Nikolaos S. Tachos, Charalampos N. Kalantzopoulos, Stelios Sfakianakis, Haridimos Kondylakis, Dimitrios I. Zaridis, Sara Colantonio, Daniele Regge, Nikolaos Papanikolaou, The ProCAncer-I consortium, Konstantinos Marias, Dimitrios I. Fotiadis, Manolis Tsiknakis*

**主要类别:** cs.AI

**AI概要:** The paper introduces AI Model Passport, a standardized documentation framework for AI models in healthcare that enhances transparency, reproducibility, and regulatory readiness. Implemented as AIPassport, it automates metadata collection and verification, demonstrated through a lesion segmentation case.


<details>
  <summary>更多</summary>
  
**动机:** Current frameworks for documenting AI models lack scalability, comparability, machine interpretability, and the ability to provide verifiable identities, limiting reproducibility and trust.

**方法:** Development of the AI Model Passport framework which captures essential metadata across the AI model lifecycle and its implementation via AIPassport, an MLOps tool.

**结果:** AIPassport successfully automates metadata collection, ensures versioning, decouples results from scripts, and integrates with various environments. Demonstrated effectiveness in a lesion segmentation use case.

**结论:** AI Model Passport sets a new standard for trustworthy and accountable AI-driven healthcare solutions, promoting transparency and regulatory compliance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Model+Passport%3A+Data+and+System+Traceability+Framework+for+Transparent+AI+in+Health，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22358，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22358&send_immediately=true&force_search=false)

**原文摘要:** The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [70] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao, Despoina Magka, Minqi Jiang, Xian Li, Roberta Raileanu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Kelvin Niu, Shagun Sodhani, Michael Shvartsman, Andrei Lupu, Alisia Lupidi, Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Thomas Foster, Lucia Cipolina-Kun, Abhishek Charnalia, Derek Dunfield, Alexander H. Miller, Oisin Mac Aodha, Jakob Foerster, Yoram Bachrach*

**主要类别:** cs.AI

**AI概要:** 本研究通过引入自动化LLM速通基准测试，评估AI代理在科学领域重现已有成果的能力。尽管结合最先进框架的推理LLM在有详细提示的情况下仍难以重现已知创新，但该基准为提升LLM训练提供了简单且现实的衡量标准。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型（LLMs）的快速发展，其在推动科学研究方面具有巨大潜力。其中关键能力之一是能够重现现有工作。为了评估AI代理在活跃研究领域中重现结果的能力，需要设计一个合适的基准测试。

**方法:** 研究引入了自动化LLM速通基准测试（Automated LLM Speedrunning Benchmark），基于NanoGPT速通竞赛，涉及19个速通任务。每个任务提供先前记录的训练脚本以及不同形式的提示（如伪代码或类似论文的描述）。这些任务涵盖了从高级算法改进到硬件优化的多样化代码级变更。

**结果:** 研究表明，即使给出详细提示，最近的推理LLMs与最先进的框架结合时，仍然难以在基准测试中重新实现已知的创新。

**结论:** 该基准测试提供了一个简单且非饱和的度量方法，用于评估LLMs自动化科学重现的能力，这是自主研究代理所需的基本技能之一。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Automated+LLM+Speedrunning+Benchmark%3A+Reproducing+NanoGPT+Improvements，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22419，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22419&send_immediately=true&force_search=false)

**原文摘要:** Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [71] [Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19](https://arxiv.org/abs/2506.21739)
*Felipe Rogério Pimentel, Rafael Gustavo Alves*

**主要类别:** stat.ML

**AI概要:** 通过修改Chen等人的算法，使用不同的FIR滤波器阶数和正则化参数值，来跟踪和预测巴西米纳斯吉拉斯州在疫情初期的COVID-19感染和康复人数，并与实际数据进行比较，结果显示修改后的算法在某些模拟中具有更好的逼近误差。


<details>
  <summary>更多</summary>
  
**动机:** 在没有疫苗的情况下，隔离是避免感染的唯一方法，因此需要一种方法来跟踪和预测感染和康复人数。Chen等人提出了一种基于FIR线性系统滤波的方法，该研究在此基础上进行了改进以提高预测精度。

**方法:** 采用时间依赖离散SIR模型，通过机器学习中的岭回归优化问题估计FIR滤波器系数（即岭系数）。对Chen等人的算法进行了小修改，重新设置了FIR滤波器阶数和正则化参数值，用于预测巴西米纳斯吉拉斯州的COVID-19感染和康复人数。

**结果:** 修改后的算法在某些模拟中表现出比Chen等人原算法更好的逼近误差，预测结果更接近实际数据。

**结论:** 通过调整FIR滤波器阶数和正则化参数，可以改善基于FIR滤波器的疫情预测模型的性能，提高感染和康复人数预测的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modification+of+a+Numerical+Method+Using+FIR+Filters+in+a+Time-dependent+SIR+Model+for+COVID-19，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21739，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21739&send_immediately=true&force_search=false)

**原文摘要:** Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use
the Finite Impulse Response (FIR) linear system filtering method to track and
predict the number of people infected and recovered from COVID-19, in a
pandemic context in which there was still no vaccine and the only way to avoid
contagion was isolation. To estimate the coefficients of these FIR filters,
Chen et al. used machine learning methods through a classical optimization
problem with regularization (ridge regression). These estimated coefficients
are called ridge coefficients. The epidemic mathematical model adopted by these
researchers to formulate the FIR filters is the time-dependent discrete SIR. In
this paper, we propose a small modification to the algorithm of Chen et al. to
obtain the ridge coefficients. We then used this modified algorithm to track
and predict the number of people infected and recovered from COVID-19 in the
state of Minas Gerais/Brazil, within a prediction window, during the initial
period of the pandemic. We also compare the predicted data with the respective
real data to check how good the approximation is. In the modified algorithm, we
set values for the FIR filter orders and for the regularization parameters,
both different from the respective values defined by Chen et al. in their
algorithm. In this context, the numerical results obtained by the modified
algorithm in some simulations present better approximation errors compared to
the respective approximation errors presented by the algorithm of Chen et al.

</details>


### [72] [Critically-Damped Higher-Order Langevin Dynamics](https://arxiv.org/abs/2506.21741)
*Benjamin Sterling, Chad Gueli, Mónica F. Bugallo*

**主要类别:** stat.ML

**AI概要:** 生成式AI方法中的去噪扩散概率模型尚待充分探索。关键阻尼虽已在CLD和TOLD++中成功引入，但尚未应用于任意阶的动力学中。本研究通过引入系统分析中的关键阻尼概念，对最近的顶级扩散方法——高阶朗之万动力学（HOLD）进行了推广。


<details>
  <summary>更多</summary>
  
**动机:** 将关键阻尼的概念引入到任意阶的动力学中，以期改进现有扩散模型的性能。

**方法:** 通过对高阶朗之万动力学（HOLD）进行推广，并结合系统分析中的关键阻尼概念，构建新的生成模型。

**结果:** 实现了对HOLD的推广，可能提升生成模型的效果和效率。

**结论:** 引入关键阻尼概念的推广方法为扩散模型的研究提供了新的方向和可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Critically-Damped+Higher-Order+Langevin+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21741，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21741&send_immediately=true&force_search=false)

**原文摘要:** Denoising Diffusion Probabilistic Models represent an entirely new class of
generative AI methods that have yet to be fully explored. Critical damping has
been successfully introduced in Critically-Damped Langevin Dynamics (CLD) and
Critically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been
applied to dynamics of arbitrary order. The proposed line of work generalizes
Higher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion
method, by introducing the concept of critical damping from systems analysis.

</details>


### [73] [TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics](https://arxiv.org/abs/2506.21757)
*Tianrong Chen, Huangjie Zheng, David Berthelot, Jiatao Gu, Josh Susskind, Shuangfei Zhai*

**主要类别:** stat.ML

**AI概要:** 扩散模型虽擅长生成高保真图像，但采样效率低。本文提出一种新的采样方法，比现有最佳求解器快186%，且无需训练，基于ODE求解器，使用更高维初始噪声减少函数评估次数，并可通过超参数控制细节水平。此外，该方法利用动量动力学，揭示了动量扩散模型与传统扩散模型的等价性，并在多个预训练扩散模型上展示了优异性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管扩散模型在生成高保真图像方面表现出色，但其采样效率低下，为了解决这一问题并提高采样速度，作者引入了一种新的采样方法。

**方法:** 提出一种无需训练的采样方法，采用ODE求解器，使用更高维初始噪声以减少函数评估次数，从而生成更详细样本。此外，通过一个超参数可以无额外计算成本地控制样本细节水平。方法还利用了动量动力学，并建立了动量扩散模型与传统扩散模型之间的基本等价性。

**结果:** 在ImageNet512上，该方法比当前最先进的求解器快186%（针对比较FID）。在包括EDM、EDM2和Stable-Diffusion 3在内的代表性预训练扩散模型上表现出色，覆盖像素和潜在空间模型以及类别和文本条件设置。

**结论:** 所提出的新采样方法显著提高了扩散模型的采样效率，同时保持甚至提升了生成样本的质量。此外，方法揭示了动量扩散模型与传统扩散模型的等价性，为进一步研究提供了新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TADA%3A+Improved+Diffusion+Sampling+with+Training-free+Augmented+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21757，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21757&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images but typically suffer from inefficient sampling. Many
solver designs and noise scheduling strategies have been proposed to
dramatically improve sampling speeds. In this paper, we introduce a new
sampling method that is up to $186\%$ faster than the current state of the art
solver for comparative FID on ImageNet512. This new sampling method is
training-free and uses an ordinary differential equation (ODE) solver. The key
to our method resides in using higher-dimensional initial noise, allowing to
produce more detailed samples with less function evaluations from existing
pretrained diffusion models. In addition, by design our solver allows to
control the level of detail through a simple hyper-parameter at no extra
computational cost. We present how our approach leverages momentum dynamics by
establishing a fundamental equivalence between momentum diffusion models and
conventional diffusion models with respect to their training paradigms.
Moreover, we observe the use of higher-dimensional noise naturally exhibits
characteristics similar to stochastic differential equations (SDEs). Finally,
we demonstrate strong performances on a set of representative pretrained
diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover
models in both pixel and latent spaces, as well as class and text conditional
settings. The code is available at https://github.com/apple/ml-tada.

</details>


### [74] [Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction](https://arxiv.org/abs/2506.21802)
*Johan Hallberg Szabadváry, Tuwe Löfström, Ulf Johansson, Cecilia Sönströd, Ernst Ahlberg, Lars Carlsson*

**主要类别:** stat.ML

**AI概要:** 本论文研究了在二分类问题中，通过符合性预测（Conformal Prediction, CP）方法实现带拒绝选项的机器学习模型。该方法能够提供分布无关的有效性保证，并通过控制拒绝率与错误率之间的权衡来优化预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 在实际应用中，机器学习模型即使可能出错也会做出预测，这导致了预测可信度的问题。为解决这一问题，引入了带拒绝选项的机器学习方法，即当预测可能不准确时选择不进行预测。

**方法:** 论文将符合性预测（CP）方法形式化地应用于带拒绝选项的二分类问题中。通过仅接受单标签预测结果，将CP转化为具有拒绝选项的二分类器。此外，论文还提供了理论上的错误率保证和有限样本估计。

**结果:** 数值实验展示了不同符合性预测设置下的错误率表现，包括完整的符合性预测和离线批量归纳式符合性预测。误差-拒绝曲线进一步说明了错误率和拒绝率之间的权衡关系。

**结论:** 带拒绝选项的机器学习方法结合符合性预测能够在二分类问题中提供有效的错误率控制，并允许用户根据实际需求设置可接受的错误率或拒绝率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Classification+with+Reject+Option%3A+Distribution-free+Error+Guarantees+via+Conformal+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21802&send_immediately=true&force_search=false)

**原文摘要:** Machine learning (ML) models always make a prediction, even when they are
likely to be wrong. This causes problems in practical applications, as we do
not know if we should trust a prediction. ML with reject option addresses this
issue by abstaining from making a prediction if it is likely to be incorrect.
In this work, we formalise the approach to ML with reject option in binary
classification, deriving theoretical guarantees on the resulting error rate.
This is achieved through conformal prediction (CP), which produce prediction
sets with distribution-free validity guarantees. In binary classification, CP
can output prediction sets containing exactly one, two or no labels. By
accepting only the singleton predictions, we turn CP into a binary classifier
with reject option.
  Here, CP is formally put in the framework of predicting with reject option.
We state and prove the resulting error rate, and give finite sample estimates.
Numerical examples provide illustrations of derived error rate through several
different conformal prediction settings, ranging from full conformal prediction
to offline batch inductive conformal prediction. The former has a direct link
to sharp validity guarantees, whereas the latter is more fuzzy in terms of
validity guarantees but can be used in practice. Error-reject curves illustrate
the trade-off between error rate and reject rate, and can serve to aid a user
to set an acceptable error rate or reject rate in practice.

</details>


### [75] [Thompson Sampling in Function Spaces via Neural Operators](https://arxiv.org/abs/2506.21894)
*Rafael Oliveira, Xuesong Wang, Kian Ming A. Chai, Edwin V. Bonilla*

**主要类别:** stat.ML

**AI概要:** 提出了一种扩展的汤普森采样方法，用于函数空间上的优化问题。该算法采用神经算子代理进行样本-优化策略，并提供理论收敛性保证，在涉及偏微分方程的功能优化任务中表现出更高的采样效率和竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 在函数空间优化问题中，目标函数是未知算子输出的已知泛函，而算子查询成本高但泛函评估廉价。为解决此类问题，需要一种高效的方法来减少昂贵的算子查询次数，同时保持优化性能。

**方法:** 该方法扩展了汤普森采样，使用神经算子代理进行样本-优化策略。将训练好的神经算子视为高斯过程的近似样本，避免了显式的不确定性量化。通过结合廉价的泛函评估和代价高昂的算子查询，实现高效的优化。

**结果:** 在涉及偏微分方程和其他非线性算子驱动现象的功能优化任务中，该方法相较于现有基线方法展现出更高的采样效率和具有竞争力的性能。

**结论:** 提出的扩展汤普森采样方法在函数空间优化问题中表现良好，提供了理论收敛性保证，并在实际任务中验证了其高效性和竞争力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thompson+Sampling+in+Function+Spaces+via+Neural+Operators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21894，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21894&send_immediately=true&force_search=false)

**原文摘要:** We propose an extension of Thompson sampling to optimization problems over
function spaces where the objective is a known functional of an unknown
operator's output. We assume that functional evaluations are inexpensive, while
queries to the operator (such as running a high-fidelity simulator) are costly.
Our algorithm employs a sample-then-optimize approach using neural operator
surrogates. This strategy avoids explicit uncertainty quantification by
treating trained neural operators as approximate samples from a Gaussian
process. We provide novel theoretical convergence guarantees, based on Gaussian
processes in the infinite-dimensional setting, under minimal assumptions. We
benchmark our method against existing baselines on functional optimization
tasks involving partial differential equations and other nonlinear
operator-driven phenomena, demonstrating improved sample efficiency and
competitive performance.

</details>


### [76] [Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport](https://arxiv.org/abs/2506.22204)
*Gurjeet Sangra Singh, Maciej Falkiewicz, Alexandros Kalousis*

**主要类别:** stat.ML

**AI概要:** 这篇论文提出了一种结合深度灰色建模与最优传输（OT）方法的混合生成模型，以增强不完整的物理模型。该方法在解决非配对问题方面表现出色，并通过基于物理的归纳偏差准确学习系统动力学，同时保持模型的可解释性。实验结果证明了该方法在生成任务和模型透明度方面的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 许多现实世界系统只能用近似描述，其微分方程中存在缺失或未知项，导致物理模型分布与真实数据生成过程（DGP）不同。作者希望通过有限且非配对的数据，结合理论驱动和数据驱动的方法来完善已知物理模型，描述DGP中的偏移分布。

**方法:** 提出了一种新型混合生成模型，将深度灰色建模与最优传输（OT）方法相结合。该方法在数据空间中实现OT映射，同时尽量减少源分布失真，确保正确使用物理参数。利用物理基础的归纳偏差，准确学习系统动力学并保持可解释性。

**结果:** 实验结果表明，所提出的方法在生成任务和模型透明度方面表现有效，能够提供关于学习到的物理动力学的详细见解。解决了非配对问题，并准确捕捉了系统动力学。

**结论:** 结合深度灰色建模与最优传输方法可以有效增强不完整的物理模型，同时保持高可解释性。此方法适用于解决非配对数据问题，并能准确学习复杂系统的动力学行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Generative+Modeling+for+Incomplete+Physics%3A+Deep+Grey-Box+Meets+Optimal+Transport，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22204&send_immediately=true&force_search=false)

**原文摘要:** Physics phenomena are often described by ordinary and/or partial differential
equations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,
many real-world systems are described only approximately with missing or
unknown terms in the equations. This makes the distribution of the physics
model differ from the true data-generating process (DGP). Using limited and
unpaired data between DGP observations and the imperfect model simulations, we
investigate this particular setting by completing the known-physics model,
combining theory-driven models and data-driven to describe the shifted
distribution involved in the DGP. We present a novel hybrid generative model
approach combining deep grey-box modelling with Optimal Transport (OT) methods
to enhance incomplete physics models. Our method implements OT maps in data
space while maintaining minimal source distribution distortion, demonstrating
superior performance in resolving the unpaired problem and ensuring correct
usage of physics parameters. Unlike black-box alternatives, our approach
leverages physics-based inductive biases to accurately learn system dynamics
while preserving interpretability through its domain knowledge foundation.
Experimental results validate our method's effectiveness in both generation
tasks and model transparency, offering detailed insights into learned physics
dynamics.

</details>


### [77] [Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings](https://arxiv.org/abs/2506.22228)
*Rong Ma, Xi Li, Jingyuan Hu, Bin Yu*

**主要类别:** stat.ML

**AI概要:** This paper addresses the challenge of extracting smooth, low-dimensional representations from noisy single-cell data by introducing NESS, an approach that improves NE representations for robust inference of biological structures. It evaluates popular NE algorithms and demonstrates NESS's effectiveness across multiple datasets.


<details>
  <summary>更多</summary>
  
**动机:** Single-cell sequencing enables detailed investigations of cell-state transitions but it's challenging to extract smooth, low-dimensional representations from noisy, high-dimensional data. Existing NE algorithms introduce distortions and existing evaluation methods focus on separating discrete cell types rather than capturing continuous transitions.

**方法:** The authors systematically evaluate popular NE algorithms through empirical analysis, simulation, and theory. They then introduce NESS, a machine learning approach to improve NE representations by leveraging algorithmic stability for robust inference of smooth biological structures.

**结果:** NESS offers useful concepts, quantitative stability metrics, and efficient computational workflows to uncover developmental trajectories and cell-state transitions in single-cell data. When applied to six single-cell datasets, NESS consistently provides useful biological insights.

**结论:** NESS consistently yields useful biological insights across diverse single-cell datasets, such as identification of transitional and stable cell states and quantification of transcriptional dynamics during development.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncovering+smooth+structures+in+single-cell+data+with+PCS-guided+neighbor+embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22228，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22228&send_immediately=true&force_search=false)

**原文摘要:** Single-cell sequencing is revolutionizing biology by enabling detailed
investigations of cell-state transitions. Many biological processes unfold
along continuous trajectories, yet it remains challenging to extract smooth,
low-dimensional representations from inherently noisy, high-dimensional
single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,
are widely used to embed high-dimensional single-cell data into low dimensions.
But they often introduce undesirable distortions, resulting in misleading
interpretations. Existing evaluation methods for NE algorithms primarily focus
on separating discrete cell types rather than capturing continuous cell-state
transitions, while dynamic modeling approaches rely on strong assumptions about
cellular processes and specialized data. To address these challenges, we build
on the Predictability-Computability-Stability (PCS) framework for reliable and
reproducible data-driven discoveries. First, we systematically evaluate popular
NE algorithms through empirical analysis, simulation, and theory, and reveal
their key shortcomings, such as artifacts and instability. We then introduce
NESS, a principled and interpretable machine learning approach to improve NE
representations by leveraging algorithmic stability and to enable robust
inference of smooth biological structures. NESS offers useful concepts,
quantitative stability metrics, and efficient computational workflows to
uncover developmental trajectories and cell-state transitions in single-cell
data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent
stem cell differentiation, organoid development, and multiple tissue-specific
lineage trajectories. Across these diverse contexts, NESS consistently yields
useful biological insights, such as identification of transitional and stable
cell states and quantification of transcriptional dynamics during development.

</details>


### [78] [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
*Xiang Li, Garrett Wen, Weiqing He, Jiayuan Wu, Qi Long, Weijie J. Su*

**主要类别:** stat.ML

**AI概要:** 这篇论文探讨了在混合来源文本中估计水印比例的问题，提出了一种基于关键统计量的估计方法，并证明了该方法在特定条件下可识别且达到最优下界。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究多关注于检测整个文本是否包含水印，但在许多现实场景中，文本可能由人工撰写内容和带水印的内容混合而成，因此需要一种方法来准确估计混合文本中的水印比例。

**方法:** 将问题建模为基于关键统计量的混合模型参数估计问题。对于使用连续关键统计量的水印方法，在温和条件下证明了比例参数的可识别性，并提出了有效的估计器。同时推导了任何基于关键统计量的可测量估计器的极小极大下界。

**结果:** 通过在合成数据和开源模型生成的混合来源文本上的评估，证明所提出的估计器能够持续达到高估计精度。

**结论:** 在特定水印方案下，比例参数是可识别的，并且所提出的估计器达到了理论最优下界，适用于实际应用中的混合来源文本分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Estimation+of+Watermark+Proportions+in+Hybrid+AI-Human+Texts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22343，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22343&send_immediately=true&force_search=false)

**原文摘要:** Text watermarks in large language models (LLMs) are an increasingly important
tool for detecting synthetic text and distinguishing human-written content from
LLM-generated text. While most existing studies focus on determining whether
entire texts are watermarked, many real-world scenarios involve mixed-source
texts, which blend human-written and watermarked content. In this paper, we
address the problem of optimally estimating the watermark proportion in
mixed-source texts. We cast this problem as estimating the proportion parameter
in a mixture model based on \emph{pivotal statistics}. First, we show that this
parameter is not even identifiable in certain watermarking schemes, let alone
consistently estimable. In stark contrast, for watermarking methods that employ
continuous pivotal statistics for detection, we demonstrate that the proportion
parameter is identifiable under mild conditions. We propose efficient
estimators for this class of methods, which include several popular unbiased
watermarks as examples, and derive minimax lower bounds for any measurable
estimator based on pivotal statistics, showing that our estimators achieve
these lower bounds. Through evaluations on both synthetic data and mixed-source
text generated by open-source models, we demonstrate that our proposed
estimators consistently achieve high estimation accuracy.

</details>


### [79] [Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks](https://arxiv.org/abs/2506.22429)
*David Holzmüller, Max Schölpple*

**主要类别:** stat.ML

**AI概要:** 这篇论文扩展了对神经网络中常用的激活函数（如SELU、ELU、LeakyReLU等）的理论理解，揭示了它们在不同网络深度下生成的RKHS特性以及NNGP样本路径的平滑性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习理论近年来取得了一些进展，但大多数研究局限于ReLU激活函数，对于其他常见激活函数的性质了解不足，特别是关于神经切线核（NTK）和神经网络高斯过程核（NNGP）。

**方法:** 作者对典型的激活函数（其唯一非光滑点在零处）提供了更广泛的再生希尔伯特空间（RKHS）特征分析，涵盖了包括缺失偏差、两层网络或多项式激活等特殊情况。

**结果:** 研究表明，一大类非无限光滑的激活函数在不同的网络深度下生成等价的RKHS，而多项式激活函数则生成非等价的RKHS，并且推导出了NNGP样本路径的平滑性结果。

**结论:** 这些发现加深了我们对非ReLU激活函数的理解，特别是在无限宽神经网络初始化时的平滑性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+ReLU%3A+How+Activations+Affect+Neural+Kernels+and+Random+Wide+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.22429，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.22429&send_immediately=true&force_search=false)

**原文摘要:** While the theory of deep learning has made some progress in recent years,
much of it is limited to the ReLU activation function. In particular, while the
neural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)
have given theoreticians tractable limiting cases of fully connected neural
networks, their properties for most activation functions except for powers of
the ReLU function are poorly understood. Our main contribution is to provide a
more general characterization of the RKHS of these kernels for typical
activation functions whose only non-smoothness is at zero, such as SELU, ELU,
or LeakyReLU. Our analysis also covers a broad set of special cases such as
missing biases, two-layer networks, or polynomial activations. Our results show
that a broad class of not infinitely smooth activations generate equivalent
RKHSs at different network depths, while polynomial activations generate
non-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP
sample paths, characterizing the smoothness of infinitely wide neural networks
at initialization.

</details>
