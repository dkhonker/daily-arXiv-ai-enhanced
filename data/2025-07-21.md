<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 53]
- [cs.AI](#cs.AI) [总数: 20]
- [cs.CR](#cs.CR) [总数: 11]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

**主要类别:** cs.LG

**AI概要:** 本文从物理角度出发，基于现代芯片和量子系统建立了转换器架构的物理模型，填补了对转换器工作原理理论理解的空白。


<details>
  <summary>更多</summary>
  
**动机:** 尽管转换器在自然语言处理方面取得了显著进展，但对其工作原理的理论理解仍存在差距。

**方法:** 作者从现代芯片的物理视角出发，在希尔伯特空间的福克空间中实现基于转换器架构的大规模语言模型作为开放量子系统，构建物理模型。

**结果:** 这些物理模型能够解释转换器架构在大规模语言模型中的运作机制。

**结论:** 该研究为转换器架构提供了新的物理层面的理解，有助于进一步优化和发展转换器技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physical+models+realizing+the+transformer+architecture+of+large+language+models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13354&send_immediately=true&force_search=false)

**原文摘要:** The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [2] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi, Tian Huey Teh, Pushkar Mishra, Roma Patel, Ding Wang, Mark Díaz, Alicia Parrish, Aida Mostafazadeh Davani, Zoe Ashwood, Michela Paganini, Vinodkumar Prabhakaran, Verena Rieser, Lora Aroyo*

**主要类别:** cs.LG

**AI概要:** 当前的文本到图像模型常常无法考虑到多样的人类体验，导致系统错位。本文提出了多元化的对齐概念，并为此提供了三个核心贡献：引入了DIVE数据集、确认了人口统计学作为这一领域中多样化观点的重要代理，以及讨论了构建对齐T2I模型的影响。


<details>
  <summary>更多</summary>
  
**动机:** 作者认为当前的文本到图像模型未能充分考虑到不同的人类价值观，因此希望能够创建一个更加多元化和包容性的AI系统。

**方法:** 作者提出了三项主要的方法：1. 引入了DIVE数据集；2. 通过实验证明了人口统计学在该领域内是多样化观点的重要代理；3. 讨论了构建对齐T2I模型的影响。

**结果:** 研究结果表明，不同的背景会导致对于安全性和危害的不同理解，这些差异对于传统的评估方式是一个挑战。此外，还提出了一些构建更公平和对齐的T2I系统的工具和策略。

**结论:** 本研究为更公平和对齐的T2I系统提供了基础工具，包括高效的数据收集策略，LLM判断能力，以及模型向多样化视角的可引导性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Whose+View+of+Safety%3F+A+Deep+DIVE+Dataset+for+Pluralistic+Alignment+of+Text-to-Image+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13383&send_immediately=true&force_search=false)

**原文摘要:** Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [3] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa, Jarek Duda*

**主要类别:** cs.LG

**AI概要:** 本文介绍了将Copula理论中的CDF归一化方法应用于机器学习领域，特别是Kolmogorov-Arnold Networks (KANs)，以改善预测效果。


<details>
  <summary>更多</summary>
  
**动机:** 作者观察到在机器学习中，数据归一化是重要的预处理步骤，而来自金融领域的Copula理论所使用的CDF归一化方法在机器学习领域几乎不为人知。因此，作者希望展示CDF归一化在机器学习中的优势。

**方法:** 通过将传统的归一化方法替换为CDF（累积分布函数）归一化，并应用到Kolmogorov-Arnold Networks (KANs) 中，特别是在Legendre-KAN模型上进行测试。此外，在HCR解释框架下，这种神经元的权重被理解为混合矩，提供局部联合分布模型，允许传播概率分布并改变传播方向。

**结果:** 使用CDF归一化改进了Legendre-KAN的预测性能。同时，它还提供了能够表示概率分布传播的方法，并且可以改变传播的方向。

**结论:** CDF归一化方法在机器学习特别是KANs中显示出了其独特的优势，可以简化模型表示，减少过拟合的风险，并提高了预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+KAN+with+CDF+normalization+to+quantiles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13393，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13393&send_immediately=true&force_search=false)

**原文摘要:** Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [4] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri, Zehui Hua, Francisco de Assis Boldt, Patrick Dumond*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的数据加载策略——选择性嵌入，它通过模仿认知心理学中的信息处理方式，交替使用来自多个来源的短数据段来减少模型过拟合，增强泛化能力并提高计算效率。在六个时间域数据集上验证了该方法的有效性，显示出高分类准确性和显著减少的训练时间，适用于需要鲁棒性和适应性的多源数据复杂系统。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习算法对输入数据敏感，在非平稳条件下和不同领域之间的性能往往下降，特别是在使用时域数据时。传统的单通道或多源并行数据加载策略要么限制泛化能力，要么增加计算成本。

**方法:** 选择性嵌入是一种新的数据加载策略，它在一个输入通道内交替使用来自多个来源的短数据段，灵感来源于认知心理学，旨在模拟人类的信息处理过程以减少模型过拟合，增强泛化能力，并提高计算效率。

**结果:** 在六个时间域数据集上的验证表明，该方法在各种深度学习架构中都能保持较高的分类准确性，同时显著减少了训练时间。

**结论:** 选择性嵌入为具有多个数据源的复杂系统提供了一个可扩展且资源高效的解决方案，特别适用于医疗保健、重型机械、海洋、铁路和农业等领域，这些领域对鲁棒性和适应性有严格要求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Selective+Embedding+for+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13399&send_immediately=true&force_search=false)

**原文摘要:** Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [5] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin, Igor Hromov, Stanislav Chumakov, Mile Mitrovic, Dmitry Simakov, Nikolay O. Nikitin, Andrey V. Savchenko*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为LightAutoDS-Tab的多AutoML代理系统，用于处理表格数据任务。它结合了基于LLM的代码生成和几种AutoML工具，提高了管道设计的灵活性和鲁棒性，并在多个数据科学任务上超越了最先进的开源解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 尽管AutoML在处理复杂任务方面取得了进展，但其效率仍然受到对特定底层工具的依赖的限制。为了提高处理表格数据任务的灵活性和鲁棒性，同时克服这种依赖性。

**方法:** 引入了LightAutoDS-Tab，一个用于表格数据任务的多AutoML代理系统，该系统将基于LLM的代码生成与多种AutoML工具相结合。

**结果:** 在来自Kaggle的多个数据科学任务中，性能超过了最先进的开源解决方案。

**结论:** LightAutoDS-Tab提高了管道设计的灵活性和鲁棒性，在处理表格数据的任务中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LightAutoDS-Tab%3A+Multi-AutoML+Agentic+System+for+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13413&send_immediately=true&force_search=false)

**原文摘要:** AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [6] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk, Roland Assam*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的生成流模型——规范流模型（Gauge Flow Models），其在流常微分方程中引入了可学习的规范场，实验表明该模型相较于传统流模型性能更好，并且可能在更广泛的生成任务中提供增强的性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高生成流模型的性能并探索在更广泛的生成任务中的应用潜力。

**方法:** 在流常微分方程（Flow ODE）中引入一个可学习的规范场（Gauge Field），形成一种新型的生成流模型——规范流模型（Gauge Flow Models）。

**结果:** 通过使用高斯混合模型进行流匹配实验，证明规范流模型比传统流模型具有更好的性能。未发表的研究还暗示了它在更多生成任务上潜在的性能提升。

**结论:** 规范流模型作为一种新类别的生成流模型，在性能方面优于传统的流模型，并可能对更广泛的生成任务产生积极影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gauge+Flow+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13414，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13414&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [7] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi, Bernardo P. Ferreira, Miguel A. Bessa*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种考虑历史依赖多保真数据的数据驱动学习方法，能够量化认知不确定性并将其与数据噪声分离。该方法具有层次性，并适应不同的学习场景。通过应用到不同的数据驱动的本构建模场景中，证明了其多功能性和通用性。


<details>
  <summary>更多</summary>
  
**动机:** 动机在于提高数据驱动学习方法在处理复杂、不确定性的工程和科学问题中的适用性和准确性，特别是在设计和分析不确定性的情况下。

**方法:** 该方法是分层的，可以适应从最简单的单保真确定性神经网络训练到提出的多保真方差估计贝叶斯递归神经网络。它能准确预测响应，量化模型误差，同时发现噪声分布（如果存在）。

**结果:** 该方法在不同的数据驱动本构建模场景中得到了验证，包括有无噪声的多种保真度。结果表明，该方法可以准确预测响应，量化模型误差，并发现噪声分布（如果存在）。

**结论:** 所提出的方法为未来的实际应用提供了机会，特别是在涉及不确定性设计和分析的最具挑战性的案例中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Single-+to+multi-fidelity+history-dependent+learning+with+uncertainty+quantification+and+disentanglement%3A+application+to+data-driven+constitutive+modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13416，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13416&send_immediately=true&force_search=false)

**原文摘要:** Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [8] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga, Thomas Guyet, Violaine Antoine*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的算法Soft-ECM，可以对复杂数据（如混合数据和时间序列）进行聚类。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于信任函数的聚类算法无法处理非欧几里得空间中的复杂数据，例如混合数据或时间序列数据。

**方法:** 作者重新定义了Evidential C-Means问题，并提出了一个新算法Soft-ECM，该算法仅需要半度量即可一致地定位不精确簇的质心。

**结果:** 实验表明，Soft-ECM在数值数据上的结果可与传统的模糊聚类方法相媲美，并且能够处理混合数据，在结合使用DTW等半度量时具有优势。

**结论:** Soft-ECM是一种有效的复杂数据聚类方法，特别是在处理混合数据和时间序列数据方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Soft-ECM%3A+An+extension+of+Evidential+C-Means+for+complex+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13417，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13417&send_immediately=true&force_search=false)

**原文摘要:** Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [9] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek, Dawid Płudowski, Piotr Wilczyński, Przemysław Biecek, Krzysztof Kotowski, Ramez Shendy, Jakub Nalepa, Artur Janicki, Evridiki Ntagiou*

**主要类别:** cs.LG

**AI概要:** 本文介绍了Kaggle上关于区分大型语言模型正常输出和恶意修改输出的竞赛，此竞赛与欧洲航天局资助的“太空领域AI应用的保障”项目有关。


<details>
  <summary>更多</summary>
  
**动机:** 竞赛基于项目中确定的两个现实的AI安全威胁：数据投毒和大型语言模型中的过度依赖。

**方法:** 参与者需要开发新技术或调整已有技术来解决这个问题。

**结果:** 摘要中未提及具体结果。

**结论:** 摘要中未提及具体结论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fake+or+Real%3A+The+Impostor+Hunt+in+Texts+for+Space+Operations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13508&send_immediately=true&force_search=false)

**原文摘要:** The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [10] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson, Dewi Gould, Richard Everson, George De Ath, Nick Pepper*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于图神经网络（GNN）的框架，以更准确地预测空中交通管制员的任务需求。


<details>
  <summary>更多</summary>
  
**动机:** 现有复杂性度量方法通常无法捕捉到超出简单飞机数量之外的细微操作驱动因素，因此在日益拥挤的空域中，实时评估短期空中交通管制员（ATCO）任务需求是一项重大挑战。

**方法:** 作者引入了一个可解释的图神经网络框架，该模型基于静态交通场景中的互动，预测即将发布的许可数量，即ATCO向飞机发出的指令。通过系统地消融飞机并测量其对模型预测的影响，推导出每个飞机的任务需求评分。

**结果:** 该框架显著优于基于ATCO的启发式算法，并且比现有的基准更能可靠地估计情景复杂度。

**结论:** 所开发的工具可以将任务需求归因于特定的飞机，为分析和理解复杂性的驱动因素提供了一种新方法，可用于管制员培训和空域重新设计等方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Air+Traffic+Controller+Task+Demand+via+Graph+Neural+Networks%3A+An+Interpretable+Approach+to+Airspace+Complexity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13423，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13423&send_immediately=true&force_search=false)

**原文摘要:** Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [11] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi, Buyao Lyu, Thomas Lisko, Rajesh Rajamani, Robert A. McGovern, Yogatheesan Varatharajah*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的跨模态自监督预训练方法，用于从未标记的大规模IMU-视频数据中学习表示，并展示了在HAR任务中对OOD IMU数据集的改进泛化性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于IMU数据的机器学习方法在HAR任务中大多依赖于特定应用的标签，缺乏对不同环境或人群收集的数据的泛化能力。

**方法:** 提出了一个新的跨模态自监督预训练方法，使用大规模未标记的IMU-视频数据进行学习。

**结果:** 该方法在零样本和少样本评估中，优于当前最先进的IMU-视频预训练方法和仅IMU预训练方法。

**结论:** 研究表明，在高度动态的数据模式（如IMU信号）中，跨模态预训练可能是学习泛化数据表示的一种有用工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Out-of-distribution+Human+Activity+Recognition+via+IMU-Video+Cross-modal+Representation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13482，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13482&send_immediately=true&force_search=false)

**原文摘要:** Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [12] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker, Ali Mesbah*

**主要类别:** cs.LG

**AI概要:** 本文探讨了基于模型的智能体作为控制策略近似的替代方案，以解决无模型强化学习中的样本效率低、学习不安全和可解释性有限的问题。


<details>
  <summary>更多</summary>
  
**动机:** 动机在于神经网络在无模型强化学习中放大了样本效率低下、不安全学习和可解释性差的问题。为了解决这些问题，提出了基于模型的智能体。

**方法:** 文中介绍了基于模型的智能体的主要学习方法：贝叶斯优化、策略搜索强化学习和离线策略，并讨论了它们各自的优势。

**结果:** 该工作概述了基于模型的智能体的学习优势和挑战，展示了这些智能体在安全策略学习方面的潜力。

**结论:** 结论是结合无模型和基于模型的方法可以实现样本高效的、安全且可解释的决策智能体的学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-free+Reinforcement+Learning+for+Model-based+Control%3A+Towards+Safe%2C+Interpretable+and+Sample-efficient+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13491，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13491&send_immediately=true&force_search=false)

**原文摘要:** Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [13] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang, Hidenori Tanaka, Wei Hu*

**主要类别:** cs.LG

**AI概要:** 论文提出了一个统一的双收敛框架，解释了大语言模型在情境学习中隐藏表示如何在上下文和层间收敛，导致对平滑表示的隐式偏好，并验证了其对高频噪声的内在鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在解释大语言模型在情境学习中的机制，特别是隐藏表示如何从输入序列中获取新的行为而无需参数更新。

**方法:** 引入了一个双收敛框架，其中隐藏表示在上下文和层间收敛，导致对平滑表示的隐式偏好。通过分析和实证方法证明了这一过程。

**结果:** 该理论解释了几个开放的经验观察，包括为什么学习到的表示具有全局结构但局部扭曲的几何形状，以及为什么它们的总能量衰减但不消失。还预测并验证了ICL对高频噪声的内在鲁棒性。

**结论:** 这些结果为理解情境学习的潜在机制提供了新的见解，并提供了一个理论基础，可能扩展到更一般的数据分布和设置。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provable+Low-Frequency+Bias+of+In-Context+Learning+of+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13540，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13540&send_immediately=true&force_search=false)

**原文摘要:** In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [14] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili, Carlos J. Fernandez-Candel, Matías Pérez Paredes*

**主要类别:** cs.LG

**AI概要:** 传统超声心动图参数在心脏功能障碍早期检测中存在局限性。本文介绍了一种新型的基于AI的超声心动图参数——声学指数，该参数结合了扩展动态模式分解和混合神经网络，从标准超声视图中量化心脏功能障碍。在736名患者的前瞻性队列研究中，声学指数在独立测试集中达到了0.89的曲线下面积（AUC），显示出良好的敏感性和特异性。它是一种具有物理信息、可解释的人工智能生物标志物，有望成为早期检测、分诊和纵向监测的可扩展工具。


<details>
  <summary>更多</summary>
  
**动机:** 传统的心脏功能评估参数如射血分数（EF）和整体纵向应变（GLS）在早期检测心脏功能障碍方面存在局限性，需要一种新的、可重复的、可解释的、与操作者无关的参数来捕捉细微且全面的心脏功能变化。

**方法:** 声学指数是通过结合扩展动态模式分解（EDMD）和混合神经网络开发的，这些方法利用Koopman算子理论并结合临床元数据。从超声心动图序列中提取时空动力学以识别连贯的运动模式，这些模式通过注意力机制加权，并使用流形学习与临床数据融合，最终生成一个从0到1的连续评分。

**结果:** 在包含各种心脏病理和正常对照的736名患者前瞻性队列中，声学指数在独立测试集中实现了0.89的曲线下面积（AUC）。五折交叉验证确认了模型的稳健性，敏感性和特异性均超过0.8。阈值分析显示敏感性和特异性之间的稳定权衡。

**结论:** 声学指数作为心脏功能的一种物理信息、可解释的人工智能生物标志物，展示了其作为早期检测、分诊和纵向监测的可扩展工具的潜力。未来的研究方向包括外部验证、纵向研究以及适应疾病特定分类器。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Acoustic+Index%3A+A+Novel+AI-Driven+Parameter+for+Cardiac+Disease+Risk+Stratification+Using+Echocardiography，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13542&send_immediately=true&force_search=false)

**原文摘要:** Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [15] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang, Steven Klee, Alexis Roos*

**主要类别:** cs.LG

**AI概要:** 本文提出两种量化时间序列预测性的度量方法：频谱可预测性评分和最大Lyapunov指数，并在M5预测竞赛数据集上验证了它们的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 传统模型评估指标无法在任何预测尝试之前评估数据的固有预测特性，因此需要新的度量来填补这一空白。

**方法:** 使用频谱可预测性评分评估时间序列中频率成分的强度和规律性；用最大Lyapunov指数量化生成数据系统的混沌和稳定性。

**结果:** 这两种度量能够正确反映时间序列的固有预测能力，并与不同模型的实际预测性能有很强的相关性。

**结论:** 通过理解时间序列的固有预测性，从业者可以将规划重点放在更具有预测性的产品和供应链层级上，同时为预测性有限的产品设定适当的期望或寻求替代策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+Series+Forecastability+Measures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13556&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [16] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur, Mike Doan, Barak Pearlmutter, Sergey Plis*

**主要类别:** cs.LG

**AI概要:** 通过引入SELF-Transformer，可以在不增加参数数量的情况下，在编码器风格的基准测试中获得高达20%的准确率提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Transformers在单次、固定深度传递中的表达能力有限。虽然自回归运行可以提高其表达能力，但这种方式需要将内部状态解码为标记然后再重新编码，类似于“大声思考”。为了增强编码器Transformers的表达能力并避免标记级别的自回归，我们引入了SELF-Transformer。

**方法:** SELF-Transformer是一种编码层，它迭代地优化自身的注意力权重到一个固定点。不是一次性生成对齐矩阵来重新混合输入序列，而是迭代地在内部更新该矩阵，并根据输入难度调整测试时的计算量。

**结果:** 这种适应性方法在编码器风格的基准测试中实现了高达20%的准确率提升，而无需增加参数数量。这表明，在测试时进行输入适应性对齐提供了巨大的好处，只需要适度增加计算预算。

**结论:** Self-Transformers在保留纯编码器架构简单性的前提下，恢复了迭代推理的大部分表达能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Change+of+Thought%3A+Adaptive+Test-Time+Computation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13569，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13569&send_immediately=true&force_search=false)

**原文摘要:** Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [17] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou, Erik Hornberger, Pengsheng Guo, Xiyou Zhou, Saiwen Wang, Xin Wang, Yifei He, Xuankai Chang, Rene Rauch, Louis D'hauwe, John Peebles, Alec Doane, Kohen Chia, Jenna Thibodeau, Zi-Yi Dou, Yuanyang Zhang, Ruoming Pang, Reed Li, Zhifeng Chen, Jeremy Warner, Zhaoyang Xu, Sophy Lee, David Mizrahi, Ramsey Tantawi, Chris Chaney, Kelsey Peterson, Jun Qin, Alex Dombrowski, Mira Chiang, Aiswarya Raghavan, Gerard Casamayor, Qibin Chen, Aonan Zhang, Nathalie Tran, Jianyu Wang, Hang Su, Thomas Voice, Alessandro Pappalardo, Brycen Wershing, Prasanth Yadla, Rui Li, Priyal Chhatrapati, Ismael Fernandez, Yusuf Goren, Xin Zheng, Forrest Huang, Tao Lei, Eray Yildiz, Alper Kokmen, Gokul Santhanam, Areeba Kamal, Kaan Elgin, Dian Ang Yap, Jeremy Liu, Peter Gray, Howard Xing, Kieran Liu, Matteo Ronchi, Moritz Schwarzer-Becker, Yun Zhu, Mandana Saebi, Jeremy Snow, David Griffiths, Guillaume Tartavel, Erin Feldman, Simon Lehnerer, Fernando Bermúdez-Medina, Hans Han, Joe Zhou, Xiaoyi Ren, Sujeeth Reddy, Zirui Wang, Tom Gunter, Albert Antony, Yuanzhi Li, John Dennison, Tony Sun, Yena Han, Yi Qin, Sam Davarnia, Jeffrey Bigham, Wayne Shan, Hannah Gillis Coleman, Guillaume Klein, Peng Liu, Muyang Yu, Jack Cackler, Yuan Gao, Crystal Xiao, Binazir Karimzadeh, Zhengdong Zhang, Felix Bai, Albin Madappally Jose, Feng Nan, Nazir Kamaldin, Dong Yin, Hans Hao, Yanchao Sun, Yi Hua, Charles Maalouf, Alex Guillen Garcia, Guoli Yin, Lezhi Li, Mohana Prasad Sathya Moorthy, Hongbin Gao, Jay Tang, Joanna Arreaza-Taylor, Faye Lao, Carina Peng, Josh Shaffer, Dan Masi, Sushma Rao, Tommi Vehvilainen, Senyu Tong, Dongcai Shen, Yang Zhao, Chris Bartels, Peter Fu, Qingqing Cao, Christopher Neubauer, Ethan Li, Mingfei Gao, Rebecca Callahan, Richard Wei, Patrick Dong, Alex Braunstein, Sachin Ravi, Adolfo Lopez Mendez, Kaiwei Huang, Kun Duan, Haoshuo Huang, Rui Qian, Stefano Ligas, Jordan Huffaker, Dongxu Li, Bailin Wang, Nanzhu Wang, Anuva Agarwal, Tait Madsen, Josh Newnham, Abhishek Sharma, Zhile Ren, Deepak Gopinath, Erik Daxberger, Saptarshi Guha, Oron Levy, Jing Lu, Nan Dun, Marc Kirchner, Yinfei Yang, Manjot Bilkhu, Dave Nelson, Anthony Spalvieri-Kruse, Juan Lao Tebar, Yang Xu, Phani Mutyala, Gabriel Jacoby-Cooper, Yingbo Wang, Karla Vega, Vishaal Mahtani, Darren Botten, Eric Wang, Hanli Li, Matthias Paulik, Haoran Yan, Navid Shiee, Yihao Qian, Bugu Wu, Qi Zhu, Ob Adaranijo, Bhuwan Dhingra, Zhe Gan, Nicholas Seidl, Grace Duanmu, Rong Situ, Yiping Ma, Yin Xia, David Riazati, Vasileios Saveris, Anh Nguyen, Michael, Lee, Patrick Sonnenberg, Chinguun Erdenebileg, Yanghao Li, Vivian Ma, James Chou, Isha Garg, Mark Lee, Keen You, Yuhong Li, Ransen Niu, Nandhitha Raghuram, Pulkit Agrawal, Henry Mason, Sumeet Singh, Keyu He, Hong-You Chen, Lucas Guibert, Shiyu Li, Varsha Paidi, Narendran Raghavan, Mingze Xu, Yuli Yang, Sergiu Sima, Irina Belousova, Sprite Chu, Afshin Dehghan, Philipp Dufter, David Haldimann, Zhen Yang, Margit Bowler, Chang Liu, Ying-Chang Cheng, Vivek Rathod, Syd Evans, Wilson Tsao, Dustin Withers, Haitian Sun, Biyao Wang, Peter Grasch, Walker Cheng, Yihao Feng, Vivek Kumar, Frank Chu, Victoria MönchJuan Haladjian, Doug Kang, Jiarui Lu, Ciro Sannino, Max Lam, Floris Weers, Bowen Pan, Kenneth Jung, Dhaval Doshi, Fangping Shi, Olli Saarikivi, Alp Aygar, Josh Elman, Cheng Leong, Eshan Verma, Matthew Lei, Jeff Nichols, Jiulong Shan, Donald Zhang, Lawrence Zhou, Stephen Murphy, Xianzhi Du, Chang Lan, Ankur Jain, Elmira Amirloo, Marcin Eichner, Naomy Sabo, Anupama Mann Anupama, David Qiu, Zhao Meng, Michael FitzMaurice, Peng Zhang, Simon Yeung, Chen Chen, Marco Zuliani, Andrew Hansen, Yang Lu, Brent Ramerth, Ziyi Zhong, Parsa Mazaheri, Matthew Hopkins, Mengyu Li, Simon Wang, David Chen, Farzin Rasteh, Chong Wang, Josh Gardner, Asaf Liberman, Haoxuan You, Andrew Walkingshaw, Xingyu Zhou, Jinhao Lei, Yan Meng, Quentin Keunebroek, Sam Wiseman, Anders Boesen Lindbo Larsen, Yi Zhang, Zaid Ahmed, Haiming Gang, Aaron Franklin, Kelvin Zou, Guillaume Seguin, Jonathan Janke, Rachel Burger, Co Giang, Cheng Shen, Jen Liu, Sanskruti Shah, Xiang Kong, Yiran Fei, TJ Collins, Chen Zhang, Zhiyun Lu, Michael Booker, Qin Ba, Yasutaka Tanaka, Andres Romero Mier Y Teran, Federico Scozzafava, Regan Poston, Jane Li, Eduardo Jimenez, Bas Straathof, Karanjeet Singh, Lindsay Hislop, Rajat Arora, Deepa Seshadri, Boyue Li, Colorado Reed, Zhen Li, TJ Lu, Yi Wang, Kaelen Haag, Nicholas Lusskin, Raunak Sinha, Rahul Nair, Eldon Schoop, Mary Beth Kery, Mehrdad Farajtbar, Brenda Yang, George Horrell, Shiwen Zhao, Dhruti Shah, Cha Chen, Bowen Zhang, Chang Gao, Devi Krishna, Jennifer Mallalieu, Javier Movellan, Di Feng, Emily Zhang, Sam Xu, Junting Pan, Dominik Moritz, Suma Jayaram, Kevin Smith, Dongseong Hwang, Daniel Parilla, Jiaming Hu, You-Cyuan Jhang, Emad Soroush, Fred Hohman, Nan Du, Emma Wang, Sam Dodge, Pragnya Sridhar, Joris Pelemans, Wei Fang, Nina Wenzel, Joseph Yitan Cheng, Hadas Kotek, Chung-Cheng Chiu, Meng Cao, Haijing Fu, Ruixuan Hou, Ke Ye, Diane Zhu, Nikhil Bhendawade, Joseph Astrauskas, Jian Liu, Sai Aitharaju, Wentao Wu, Artsiom Peshko, Hyunjik Kim, Nilesh Shahdadpuri, Andy De Wang, Qi Shan, Piotr Maj, Raul Rea Menacho, Justin Lazarow, Eric Liang Yang, Arsalan Farooq, Donghan Yu, David Güera, Minsik Cho, Kavya Nerella, Yongqiang Wang, Tao Jia, John Park, Jeff Lai, Haotian Zhang, Futang Peng, Daniele Molinari, Aparna Rajamani, Tyler Johnson, Lauren Gardiner, Chao Jia, Violet Yao, Wojciech Kryscinski, Xiujun Li, Shang-Chen Wu*

**主要类别:** cs.LG

**AI概要:** 苹果公司推出了两款多语言、多模态的基础语言模型，分别针对设备端和服务器端进行了优化，并在大规模多语言和多模态数据集上训练。这些模型支持多种语言和图像理解，并且在公共基准测试和人类评估中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 随着智能设备和服务的需求增长，需要更强大、高效并且保护用户隐私的语言模型来提升用户体验。

**方法:** 通过架构创新如KV-cache共享和2-bit量化感知训练优化的3B参数设备端模型；以及基于Parallel-Track Mixture-of-Experts (PT-MoE) transformer的可扩展服务器模型。两者均使用负责任的网络爬虫、许可语料库和高质量合成数据进行大规模训练，并经过监督微调和强化学习进一步优化。

**结果:** 这两个模型在多语言支持、图像理解和工具调用方面都有显著表现，并且在公共基准测试和人类评估中匹配或超越了同等规模的开源基线模型。

**结论:** 苹果的新模型不仅增强了其设备和服务中的智能功能，还通过Swift-centric框架简化了开发者的集成过程，同时坚持了负责任的人工智能方法和用户隐私保护。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Apple+Intelligence+Foundation+Language+Models%3A+Tech+Report+2025，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13575，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13575&send_immediately=true&force_search=false)

**原文摘要:** We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [18] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam, Yanming Wan, Mickel Liu, Jianxun Lian, Natasha Jaques*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架PLUS，该框架使用强化学习生成每个用户的文本摘要，并同时更新奖励模型，以实现个性化预测。与先前的技术相比，PLUS在处理新用户和多样化话题方面表现得更加稳健，并且可以迁移到更强大的模型如GPT-4中进行零样本个性化。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型（LLM）AI助手的日常应用扩展，个性化响应变得越来越重要。然而，目前的方法无法考虑到不同用户之间的差异性，因为它们用单一的奖励模型来模拟整个用户群体。

**方法:** 作者提出了一个名为PLUS的新框架，它通过总结每个用户的偏好、特征和过去的对话来生成文本摘要。这些摘要用于调节奖励模型，使其能够做出个性化的预测。用户摘要模型通过强化学习进行训练，同时更新奖励模型，创建在线协同适应循环。

**结果:** PLUS生成的摘要捕捉到了用户偏好的有意义方面，在不同的多用户数据集上，该方法对新用户和多样化的对话主题具有鲁棒性。此外，所生成的用户文本摘要可以迁移到更强大的模型如GPT-4中进行零样本个性化。

**结论:** PLUS生成的用户摘要是简洁和可移植的，易于用户理解和修改，从而提高了LLM对齐过程中的透明度和用户控制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Pluralistic+User+Preferences+through+Reinforcement+Learning+Fine-tuned+Summaries，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13579&send_immediately=true&force_search=false)

**原文摘要:** As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [19] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi, Shuhei Goda, Yuta Saito*

**主要类别:** cs.LG

**AI概要:** 本文提出了针对匹配市场的新型OPE评估器，通过结合直接方法、逆倾向评分和双重稳健估计器，并引入中间标签以提高偏差-方差控制。理论分析和实验证明了新方法在离线评估和学习任务中的优越性。


<details>
  <summary>更多</summary>
  
**动机:** A/B测试是评估推荐系统中匹配市场的新策略的黄金标准，但频繁更新策略时成本高且不切实际。离线日志数据的离策略评估（OPE）虽然重要，但在匹配平台的大规模和双向用户交互中，传统OPE方法由于方差问题和奖励稀疏性变得不可靠。

**方法:** 作者提出了一种新的OPE评估器——DiPS和DPR，这些评估器融合了直接方法、逆倾向评分和双重稳健估计器的元素，并利用了诸如初步参与信号等中间标签来改进偏差-方差控制。

**结果:** 通过理论推导，作者证明了所提评估器的偏差和方差特性，并展示了它们相较于传统方法的优势。此外，通过合成数据和真实工作匹配平台的A/B测试日志进行的实验进一步验证了新方法在不同配置下的优越性。

**结论:** 所提出的OPE评估器能够有效解决匹配市场中离线评估面临的挑战，并可无缝扩展到离线策略学习方法，从而改善推荐策略以实现更多的匹配。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Off-Policy+Evaluation+and+Learning+for+Matching+Markets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13608，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13608&send_immediately=true&force_search=false)

**原文摘要:** Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [20] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li, Yuefei Wang, Xu Xiang, Xue Li, Binyu Zhao, Heyang Gao, Qinyu Zhao, Xi Yu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的深度聚类框架，称为Tri-GFN，结合了GCN、AE和Graph Transformer模块。通过三学习机制和特征融合策略，该模型在处理大规模复杂图数据集方面表现出色，尤其在Reuters数据集上提高了14.14%的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于GCN的模型在处理大规模和复杂的图数据时遇到了过度平滑和过度压缩的问题，而Graph Transformer架构虽然缓解了一些问题，但在处理异构图数据时性能仍然受限。

**方法:** 提出了一个名为Tri-GFN的新框架，它包括GCN、Autoencoder (AE) 和Graph Transformer，并通过三通道增强模块将这些组件精心融合。利用三学习机制和特征融合增强策略来加强全局和局部信息的一致性和差异性。

**结果:** 该框架在ACM数据集上的准确率提高了约0.87%，在Reuters数据集上提高了14.14%，在USPS数据集上提高了7.58%。

**结论:** 由于其在Reuters数据集上的卓越表现，Tri-GFN可以应用于自动新闻分类、主题检索等相关领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tri-Learn+Graph+Fusion+Network+for+Attributed+Graph+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13620&send_immediately=true&force_search=false)

**原文摘要:** In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [21] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey, Kamel Abbad, Garth V. Crosby, Lyes Khoukhi*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的客户端跳过算法FedSkipTwin，通过服务器端的数字孪生预测客户端更新的重要性与不确定性，以减少联邦学习中的通信开销，并在UCI-HAR和MNIST数据集上证明了其有效性和优越性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习（FL）中通信开销是主要瓶颈，特别是在涉及带宽受限的移动和物联网设备的应用中。为了节省带宽并提高效率，需要一种能够智能地决定何时进行通信的方法。

**方法:** FedSkipTwin使用轻量级的服务器端数字孪生（简单LSTM模型），根据客户端的历史梯度范数序列预测下一次更新的重要性和不确定性。只有当预测值超过预定义阈值时，才会请求通信；否则，客户端将跳过本轮次，从而节省带宽。

**结果:** 实验结果表明，在非独立同分布的数据条件下，FedSkipTwin可以减少12-15.5%的总通信量，并且最终模型准确率比标准FedAvg算法提高了0.5个百分点。

**结论:** 基于预测指导的跳过策略被证明是在带宽受限的边缘环境中实现资源感知型联邦学习的一种实用且有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedSkipTwin%3A+Digital-Twin-Guided+Client+Skipping+for+Communication-Efficient+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13624&send_immediately=true&force_search=false)

**原文摘要:** Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [22] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng, Hengkai Tan, Xinyi Mao, Guodong Liu, Shuhe Huang, Chendong Xiang, Hang Su, Jun Zhu*

**主要类别:** cs.LG

**AI概要:** 本文提出VIDAR框架，通过大规模视频预训练和新的逆动力学模型解决双臂机器人操作中的数据稀缺和实体异构性问题。


<details>
  <summary>更多</summary>
  
**动机:** 双臂机器人操作在解决复杂任务中具有基础性作用，但数据稀缺和实体异质性限制了其进一步发展。

**方法:** VIDAR框架包括两个阶段：利用75万个多视图视频进行大规模扩散模型预训练；使用遮罩逆动力学模型进行动作预测，该模型可以提取与动作相关的信息而不依赖像素级标签。

**结果:** 实验表明，VIDAR仅需20分钟的人类演示即可适应未见过的机器人平台和背景，并且在新任务上表现出色，超越了现有方法。

**结论:** 研究结果强调了视频基础模型与遮罩动作预测相结合，在多种实际环境中实现可扩展和通用的机器人操作的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalist+Bimanual+Manipulation+via+Foundation+Video+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.12898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.12898&send_immediately=true&force_search=false)

**原文摘要:** Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [23] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh, Daniele Santoni, Debaleena Nawn, Eleonora Ottaviani, Giovanni Felici*

**主要类别:** cs.LG

**AI概要:** 这篇论文回顾了基于Transformer的模型在蛋白质序列分析和设计中的最新进展，讨论了其优势和劣势，并指出了未来的研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 鉴于基于Transformer的语言模型在自然语言处理领域的巨大成功及其在其他领域（如生物信息学）的应用，作者希望探讨这些模型在蛋白质序列分析和设计中的应用情况。

**方法:** 作者对大量关于基于Transformer的模型应用于蛋白质序列分析和设计的文献进行了讨论和分析。

**结果:** 作者提供了这些应用的全面见解，涵盖了基因本体、功能和结构蛋白质识别、从头生成蛋白质和蛋白质结合等方面。

**结论:** 作者强调了现有研究中的不足，并探索了未来发展的潜在途径，认为该综述将帮助研究人员了解该领域的最新进展，并指导他们的未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensive+Review+of+Transformer-based+language+models+for+Protein+Sequence+Analysis+and+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13646，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13646&send_immediately=true&force_search=false)

**原文摘要:** The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [24] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang, Zihan Su, Ying Zhang, Chang Chuan Goh, Yuxiang Lin, Anthony Graham Bellotti, Boon Giin Lee*

**主要类别:** cs.LG

**AI概要:** 本文通过引入GRU-KAN和LSTM-KAN两种创新架构，将Kolmogorov-Arnold Networks与GRU和LSTM网络相结合，以解决时间序列异常检测中的关键挑战，即在贷款违约事件发生前三个月以上增强预测能力。实验结果表明，该模型在提前3个月和8个月的预测准确率分别超过92%和88%，显著优于现有基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前的时间序列异常检测方法存在早期预测准确性不足的问题，并且依赖于特定年份和时间段内的训练和测试数据，这限制了它们在处理跨时间数据时的实际应用。

**方法:** 研究提出了两种新的模型架构，分别是GRU-KAN和LSTM-KAN，这两种架构结合了Kolmogorov-Arnold Networks (KAN) 和两种循环神经网络（Gated Recurrent Units, GRU 和 Long Short-Term Memory, LSTM）。

**结果:** 实验评估显示，所提出的模型在不同的特征窗口长度、样本量和早期预测间隔中，都显著优于基准模型，在提前3个月和8个月的预测准确率分别为92%以上和88%以上。

**结论:** 新提出的GRU-KAN和LSTM-KAN模型提高了贷款违约预测的准确性，特别是在较长时间的早期预测方面，为金融机构提供了更有效的风险预警工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kolmogorov-Arnold+Networks-based+GRU+and+LSTM+for+Loan+Default+Early+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13685，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13685&send_immediately=true&force_search=false)

**原文摘要:** This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [25] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský, Gustav Šír, Vyacheslav Kungurtsev, Georgios Korpas*

**主要类别:** cs.LG

**AI概要:** 物理启发的图神经网络（PI-GNNs）在组合优化问题上的表现随着问题图密度增加而下降，本文分析了其原因并提出了改进方法，实验表明新方法在高密度设置下显著提升了PI-GNNs的性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们注意到物理启发的图神经网络（PI-GNNs）在解决由特定图结构和损失编码的组合优化问题时表现出色，但不清楚其在不同密度的组合问题图上的适应性。

**方法:** 通过分析PI-GNNs在训练动态中的相变现象，揭示了密集问题导致的退化解是性能下降的原因，并基于模糊逻辑和二值化神经网络的见解提出了新的策略来弥补模型输出与问题解之间的差异。

**结果:** 实验结果显示，所提出的方法在处理更密集的问题图时，能够显著改善PI-GNNs的表现。

**结论:** 该研究表明，对于物理启发的图神经网络来说，在面对更高密度的组合优化问题时，采用改进后的策略可以有效提升其性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Binarizing+Physics-Inspired+GNNs+for+Combinatorial+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13703&send_immediately=true&force_search=false)

**原文摘要:** Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [26] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong, Austin Tripp, Layla Hosseini-Gerami, Brooks Paige*

**主要类别:** cs.LG

**AI概要:** 多目标贝叶斯优化（MOBO）在分子设计中优于单一目标方法，尤其是在数据有限和权衡复杂的情况下。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探讨多目标贝叶斯优化（MOBO）在分子设计中的优势，并与单一目标的替代方案进行比较，特别是在低数据环境下的表现。

**方法:** 通过使用Expected Hypervolume Improvement (EHVI)的简单Pareto-based MOBO策略与使用Expected Improvement (EI)的固定权重单一目标基线进行对比，所有实验均在严格控制的设置下进行，包括相同的高斯过程代理和分子表示。

**结果:** 在三个分子优化任务中，EHVI在Pareto前沿覆盖、收敛速度和化学多样性方面始终优于单一目标EI。结果表明，即使在强确定性实例中，单一目标方法在低数据环境下也可能表现不佳。

**结论:** 该研究表明，在从头分子优化中，特别是当评估预算有限且权衡非平凡时，Pareto感知获取具有实际优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Optimization+for+Molecules+Should+Be+Pareto-Aware，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13704&send_immediately=true&force_search=false)

**原文摘要:** Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [27] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang, Yu Liu, Daniel Biggs, Haoru Wang, Jiandong Yu, Ping Huang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种自适应空间标记化（AST）方法，通过将非结构化网格映射到结构化网格并使用注意力机制来实现可扩展的、准确的变形体交互模拟。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于图神经网络的学习方法在处理大规模变形体交互时面临计算复杂度问题，无法有效扩展。

**方法:** 该方法首先将仿真空间划分为单元格网格，并将非结构化网格映射到此结构化网格上，以自然地分组相邻网格节点。然后应用交叉注意力模块将稀疏单元格映射为紧凑的固定长度嵌入。再利用自注意力模块预测这些嵌入的下一个状态。

**结果:** 实验表明，该方法在建模变形体交互方面显著优于现有最先进方法，尤其是在包含超过100,000个节点的大规模网格上依然保持高效。

**结论:** AST方法实现了高效且准确的变形体交互模拟，解决了现有方法在大规模仿真中的计算限制问题，并提供了一个新的大规模数据集以支持未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Deformable+Body+Interactions+With+Adaptive+Spatial+Tokenization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13707，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13707&send_immediately=true&force_search=false)

**原文摘要:** Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [28] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola, Andrea Bernardini, Giancarlo Crocetti, Andrea Ladogana, Mario Lezoche, Maurizio Mancini, Daniele Pannone, Amedeo Ranaldi*

**主要类别:** cs.LG

**AI概要:** 研究比较了传统机器学习和深度学习模型在帕金森病分类上的表现，发现CNN-LSTM模型效果最佳，同时也肯定了XGBoost等传统分类器的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 帕金森病（PD）是一种影响运动和认知功能的神经退行性疾病，早期诊断对于有效的临床干预至关重要。为了寻找一种非侵入性和成本效益高的检测方法，本研究旨在通过公开的奇数任务数据集，系统性地评估传统机器学习和深度学习模型在PD分类中的性能。

**方法:** 研究采用了一致的七步预处理管道，并应用了主题内交叉验证和评价标准，以确保模型间的可比性。

**结果:** 结果表明，基线深度学习架构，特别是CNN-LSTM模型，相较于其他深度学习架构实现了最佳性能，强调了捕捉长程时间依赖的重要性；而一些传统分类器如XGBoost也展示了强大的预测准确性和校准的决策边界。

**结论:** 通过严格比较这些基准，该研究为未来开发和评估更复杂或专业化的架构提供了坚实的基础框架。建立可靠的基准结果集对于将新方法引入时的情境化改进是必要的，这保证了基于EEG的神经诊断领域中的科学严谨性和可重复性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+of+EEG+Analysis+Techniques+for+Parkinson%27s+Disease+Diagnosis%3A+A+Comparison+between+Traditional+ML+Methods+and+Foundation+DL+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13716&send_immediately=true&force_search=false)

**原文摘要:** Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [29] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola, Muhammad Yasir Bilal, Emad Emam, Cristina Lakasz, Daniele Pannone, Amedeo Ranaldi*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于EEG信号的深度学习方法，用于分类欺骗性和真实性行为。使用双向门控循环单元神经网络进行二元分类，达到了97%的测试准确率，展示了在实时应用和未来探索中的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 欺骗检测在安全、心理学和法医学等领域是一个重大挑战。为了应对这一挑战，研究人员提出了使用脑电图（EEG）信号来区分欺骗性和真实性行为的方法。

**方法:** 该研究采用了来自Bag-of-Lies数据集的EEG信号，这是一个为自然、随意的欺骗场景设计的多模态语料库。采用双向门控循环单元（Bi-GRU）神经网络训练模型以执行EEG样本的二元分类任务。

**结果:** 该模型实现了97%的测试准确率，并且在两个类别上都具有高精度、召回率和F1分数。

**结论:** 研究表明，使用双向时间建模对于基于EEG的欺骗检测是有效的，这表明了其在实时应用中的潜力以及对未来高级神经架构探索的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bi-GRU+Based+Deception+Detection+using+EEG+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13718，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13718&send_immediately=true&force_search=false)

**原文摘要:** Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [30] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang, Tianxiang Zhao, Yu Sun, Liping Sun, Jichuan Kang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的混合特征融合框架，以构建自主货船的故障模式图结构数据集。使用改进的布谷鸟搜索算法（HN-CSA），文献检索效率显著提高。通过多种编码方式处理子系统/组件特征、故障模式/原因和语义关联。验证结果显示GATE-GNN模型分类准确率为0.735，Shore-based Meteorological Service System的F1分数为0.93。


<details>
  <summary>更多</summary>
  
**动机:** 自主货船（ACS）中的组件故障会引发级联反应，并且在紧急决策中存在不确定性。为了应对这些挑战，需要一种有效的方法来分析故障模式并支持智能决策。

**方法:** 提出了一个分层特征融合框架，采用Word2Vec编码子系统/组件特征，BERT-KPCA处理故障模式/原因，Sentence-BERT量化故障影响与紧急决策之间的语义关联。此外，还引入了改进的布谷鸟搜索算法（HN-CSA）以提高文献检索效率。

**结果:** GATE-GNN模型实现了0.735的分类准确率，特征具有较高的区分度（轮廓系数为0.641）。Shore-based Meteorological Service System的F1得分为0.93，显示出高预测准确性。

**结论:** 该研究不仅为自主货船的故障分析提供了坚实的基础，还为故障诊断、风险评估和智能决策系统提供了可靠的支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph-Structured+Data+Analysis+of+Component+Failure+in+Autonomous+Cargo+Ships+Based+on+Feature+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13721&send_immediately=true&force_search=false)

**原文摘要:** To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [31] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich, Lukas Rauch, Bernhard Sick, Christoph Scholz*

**主要类别:** cs.LG

**AI概要:** 对抗训练，特别是基于输出空间攻击的训练，能提升音频分类中模型在干净测试数据上的性能和对抗鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 探索对抗训练对音频分类中显著的数据分布变化下的泛化能力的影响。

**方法:** 使用两种模型架构（ConvNeXt和AudioProtoPNet），针对鸟鸣分类基准，评估两种对抗训练策略：输出空间攻击和嵌入空间攻击。

**结果:** 对抗训练提高了模型在干净测试数据上的平均性能10.5%，并增强了模型的对抗鲁棒性。

**结论:** 对抗训练不仅有助于提高音频分类模型的泛化性能，而且可以增强其对抗强分布变化和对抗攻击的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Training+Improves+Generalization+Under+Distribution+Shifts+in+Bioacoustics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13727，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13727&send_immediately=true&force_search=false)

**原文摘要:** Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [32] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst, Tim Langer, Chen Liu, Mehmet Alici, Hector A. Gonzalez, Christian Mayr*

**主要类别:** cs.LG

**AI概要:** 该论文介绍了一个多层DNN调度框架，作为OctopuScheduler的扩展，提供从PyTorch模型到SpiNNaker2芯片推理的端到端流程。通过量化和降低步骤组成的前端，所提出的框架能够在神经形态平台SpiNNaker2上实现基于边缘的大规模和复杂DNN（达到变压器规模）的执行。


<details>
  <summary>更多</summary>
  
**动机:** 随着深度神经网络(DNN)的规模和复杂性不断增大，传统的计算平台难以满足其在资源受限环境下的实时处理需求。因此，研究者们致力于开发新的方法和技术，使DNN能够在如SpiNNaker2这样的神经形态平台上高效运行。

**方法:** 研究人员提出了一个多层DNN调度框架，作为OctopuScheduler的扩展。此框架包括一个由量化和降低步骤组成的前端，并提供从PyTorch模型到SpiNNaker2芯片推理的端到端流程。

**结果:** 该框架能够在 SpiNNaker2 神经形态平台上实现大规模和复杂 DNN 的边缘执行，最高可达变压器规模。

**结论:** 该框架为在 SpiNNaker2 芯片上进行大型和复杂 DNN 的推理提供了一种有效的方法，从而推进了神经形态计算在实际应用中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+End-to-End+DNN+Inference+Framework+for+the+SpiNNaker2+Neuromorphic+MPSoC，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13736，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13736&send_immediately=true&force_search=false)

**原文摘要:** This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [33] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang, Zezhong Ding, Xike Xie*

**主要类别:** cs.LG

**AI概要:** SamGoG是一种基于采样的图神经网络框架，旨在解决类别和图大小不平衡的问题。它通过构建多个图的图并顺序训练来提高模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的图通常存在类别不平衡和图大小不平衡的问题，这会导致学习过程中的偏差和模型性能的下降。现有的方法要么只解决一种不平衡问题，要么计算成本过高。

**方法:** SamGoG采用了一种基于重要性采样的机制来构建多个图的图，并顺序地在这些图上进行训练。这种采样机制结合了可学习的成对相似性和自适应的节点度来增强边同质性。

**结果:** 实验结果表明，SamGoG在基准数据集上实现了最先进的性能，准确率提高了15.66%，训练速度加快了6.7倍。

**结论:** SamGoG有效地缓解了类别和图大小不平衡的问题，可以与各种下游GNN无缝集成，从而高效地适应图分类任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SamGoG%3A+A+Sampling-Based+Graph-of-Graphs+Framework+for+Imbalanced+Graph+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13741，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13741&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [34] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad, Natalia Grabar*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种基于变换器的模型，通过一系列优化技术（如动态量化、使用不同执行提供者的搜索）实现生物医学本体对齐任务的加速和资源消耗减少。在不损失性能的情况下，实现了20倍的推理加速和70%的内存节省。


<details>
  <summary>更多</summary>
  
**动机:** 开发更高效的AI模型部署方法以应对大型模型带来的计算需求挑战，特别是在边缘设备或资源受限环境中面临的能源消耗、内存使用和延迟问题。

**方法:** 采用监督的最先进变换器模型和余弦相似度进行语义相似性分析，结合微软Olive、ONNX Runtime后端、Intel Neural Compressor和IPEX等工具进行模型优化。

**结果:** 在DEFT 2020评估活动中两个任务上达到了新的最先进水平，同时保持性能指标不变，实现了平均20倍的推理加速和大约70%的内存使用减少。

**结论:** 提出的系统方法不仅提高了模型的效率，还在资源受限环境下维持了高性能表现，为未来高效模型优化技术的发展趋势做出了贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Search-Optimized+Quantization+in+Biomedical+Ontology+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13742&send_immediately=true&force_search=false)

**原文摘要:** In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [35] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin, Junjie Wang, Wenkai Xiang, Duanhua Cao, Dan Teng, Zhehuan Fan, Jiacheng Xiong, Xia Sheng, Chuanlong Zeng, Mingyue Zheng, Qian Shi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于参数插值流的模型（PIF）并应用于分子生成，特别是基于结构的药物设计。它在多个指标上优于基线模型，证明了参数空间生成建模范式的有效性，并为模型设计提供了新视角。


<details>
  <summary>更多</summary>
  
**动机:** 尽管贝叶斯流网络（BFNs）在化学任务中表现出色，但其基于贝叶斯推理的策略限制了更灵活的分布转换路径的设计，并难以适应不同的数据分布和任务需求。此外，尚未探索更简单、更高效的参数空间模型的潜力。

**方法:** 作者提出了一个带有详细理论基础、训练和推理程序的新型Parameter Interpolation Flow模型（PIF）。随后，他们开发了MolPIF用于基于结构的药物设计。

**结果:** MolPIF在多个评估指标上都优于基线模型。

**结论:** 这项工作验证了分子参数空间生成建模范式的有效性，并为未来的模型设计提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MolPIF%3A+A+Parameter+Interpolation+Flow+Model+for+Molecule+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13762，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13762&send_immediately=true&force_search=false)

**原文摘要:** Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [36] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng, Shoujia Zhang, Jianhua Yin, Li Jin, Liqiang Nie*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于邻域分布特性的双中心图聚类方法，利用邻域分布作为监督信号，增强了表示学习的效果，并引入了邻域分布中心和特征中心共同构建双目标分布，以进行双中心优化。实验表明该方法性能优越。


<details>
  <summary>更多</summary>
  
**动机:** 由于图聚类的无监督特性，它在揭示复杂数据结构方面存在重大挑战。现有的目标导向聚类方法依赖于伪标签作为监督信号，这是不可靠的，并且只使用特征来构造单目标分布，这导致指导不完整和不太可靠。

**方法:** 我们提出了一个基于邻域分布特性的双中心图聚类（DCGC）方法，包括带有邻域分布的表示学习和双中心优化。具体来说，我们利用邻域分布作为监督信号，在对比学习中挖掘难负样本，同时引入邻域分布中心与特征中心一起构建双目标分布。

**结果:** 广泛的实验和分析证明了我们提出的方法的优越性能和有效性。

**结论:** 我们的工作提出了一种新的双中心图聚类方法，通过利用邻域分布作为监督信号，提高了表示学习的有效性，并通过双中心优化提高了聚类性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dual-Center+Graph+Clustering+with+Neighbor+Distribution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13765&send_immediately=true&force_search=false)

**原文摘要:** Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [37] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer, Denis Kramer, Oliver Niggemann*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于贝叶斯神经网络方法的微调方法，可以在保持预设精度的同时自动微调模型，并能检测到诸如过渡态等稀有事件并以更高的频率采样。


<details>
  <summary>更多</summary>
  
**动机:** 由于从头计算原子间力的计算复杂性，创建原子间机器学习力场变得非常活跃。然而，生成足够规模和样本多样性的训练数据集本身带有计算负担，这使得该方法在建模稀有事件或具有大配置空间的系统时可能不实用。

**方法:** 作者引入了一种基于贝叶斯神经网络方法的微调方法，以及一个后续的即时工作流程，该工作流程可以自动微调模型，同时保持预设的准确性，并能够检测到诸如过渡态等稀有事件并以更高的频率进行采样。

**结果:** 这种新的微调方法克服了基础模型在微调过程中无法评估不确定性的问题，实现了自动化的模型微调，提高了对稀有事件的检测和采样率。

**结论:** 这种方法为解决基础模型微调过程中的不确定性评估问题提供了一个有效的解决方案，特别是对于需要检测和采样稀有事件的应用场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On-the-Fly+Fine-Tuning+of+Foundational+Neural+Network+Potentials%3A+A+Bayesian+Neural+Network+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13805，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13805&send_immediately=true&force_search=false)

**原文摘要:** Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [38] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand, Suman Banerjee, Dildar Ali*

**主要类别:** cs.LG

**AI概要:** 本文研究了奖励函数为次模的强化学习问题，提出了一种剪枝次模图方法，并证明在可行的计算时间内能提供近似最优解。实验表明该方法比基线方法获得了更多的奖励。


<details>
  <summary>更多</summary>
  
**动机:** 传统的RL设定中，奖励函数被认为是可加的。然而，在许多实际问题中，奖励函数遵循边际效益递减规律，即次模函数。因此，有必要研究奖励函数为次模的RL问题，以找到使这种奖励函数最大化的策略。

**方法:** 作者提出了一种基于剪枝次模图的方法，这种方法能在可行的计算时间内提供一个可以被证明的近似解。

**结果:** 通过使用基准智能体-环境设置进行实验，结果表明，与基线方法相比，所提出的策略能够获得更多的奖励。

**结论:** 这项研究表明，对于具有次模奖励函数的RL问题，所提出的方法可以在合理的时间内提供性能更优的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Submodular+Policy+Optimization+via+Pruned+Submodularity+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13834&send_immediately=true&force_search=false)

**原文摘要:** In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [39] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat, Massinissa Hamidi, Pierre Bartet, Blaise Hanczar*

**主要类别:** cs.LG

**AI概要:** 本文研究了自监督学习方法在基因表达数据表型预测中的应用，证明了其能有效捕捉复杂信息并提高预测准确性，减少了对标注数据的依赖。


<details>
  <summary>更多</summary>
  
**动机:** 传统的机器学习和深度学习方法在基因表达数据分析中需要大量的标注数据，这既昂贵又耗时。为了解决这个问题，本研究探索了自监督学习方法的应用，以从无标签数据中直接提取信息。

**方法:** 选择了三种基于不同原理的自监督学习方法，并使用多个公开的基因表达数据集进行评估。这些方法旨在利用数据的内在结构生成定性表示，用于下游预测任务。

**结果:** 实验结果显示所选自监督学习方法能够有效地捕捉复杂信息，改善表型预测准确度，并且优于传统监督模型。

**结论:** 该研究首次将自监督学习应用于批量RNA-Seq数据，展示了其减少对注释数据依赖的优势，并为未来的研究方向提供了建议。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-supervised+learning+on+gene+expression+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13912，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13912&send_immediately=true&force_search=false)

**原文摘要:** Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [40] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu, Christian Gumbsch, Martin V. Butz, Charley M Wu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的因果过程框架及其实现模型，通过将注意力机制与强化学习结合，以从视觉观察中推断可解释的因果过程，并展示了其在因果表示学习和代理性能上的优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的神经模型在处理因果关系时大多假设静态因果图，忽略了因果交互的动态性质。为了改进这一点，需要一种能够代表动态因果结构假设的新理论。

**方法:** 作者引入了因果过程框架作为新理论，并提出了因果过程模型作为该框架的实现。此方法重新定义了Transformer网络中的注意力机制，在RL环境中通过建立因果图假设进行因果推理。

**结果:** 该方法在一个RL环境中表现优异，不仅在因果表示学习和代理性能上超过了当前替代方案，而且唯一地恢复了动态因果过程的图形。

**结论:** 因果过程框架及其模型提供了一种新的方法来理解动态因果关系，并且在实际应用中显示出了强大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reframing+attention+as+a+reinforcement+learning+problem+for+causal+discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13920&send_immediately=true&force_search=false)

**原文摘要:** Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [41] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang, Bruna Jacobson*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为MoDyGAN的新颖管道，结合了分子动力学模拟和生成对抗网络来探索蛋白质的构象空间。通过将3D蛋白质结构转换为2D矩阵，使得可以使用先进的基于图像的GAN架构。实验表明，MoDyGAN可以生成可信的新构象，并且在潜在空间内的插值与受控分子动力学（SMD）模拟获得的轨迹高度一致。


<details>
  <summary>更多</summary>
  
**动机:** 广泛探索蛋白质的构象景观是计算生物学中的一个主要挑战，因为动态物理基础模拟涉及极高的计算成本。为了克服这一难题，研究人员希望找到一种高效且低成本的方法来探索蛋白质的构象空间。

**方法:** 该方法包括一个生成器，它将高斯分布映射到由分子动力学（MD）衍生的蛋白质轨迹中；一个改进模块，它结合集成学习和双重判别器以进一步提高生成构象的合理性；以及一种创新的表示技术，可逆地将3D蛋白质结构转变为2D矩阵。

**结果:** 通过对三种刚性蛋白质的研究，证明了MoDyGAN可以生成可信的新构象。此外，使用十肽丙氨酸作为案例研究，显示在潜在空间内的插值与受控分子动力学（SMD）模拟获得的轨迹紧密对齐。

**结论:** 本研究表明，将蛋白质表示为类似图像的数据为应用先进的深度学习技术于生物分子模拟开辟了新的可能性，从而有效地采样构象状态。此外，所提出的框架具有扩展到其他复杂3D结构的强大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoDyGAN%3A+Combining+Molecular+Dynamics+With+GANs+to+Investigate+Protein+Conformational+Space，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13950&send_immediately=true&force_search=false)

**原文摘要:** Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [42] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei, Anwar Said, Waseem Abbas, Xenofon Koutsoukos*

**主要类别:** cs.LG

**AI概要:** 本文提出了两种将平均可控性整合到图神经网络中的新方法，以提升异常检测的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图基机器学习模型虽然可以结合属性和关系数据来揭示复杂模式，但异常数据的稀缺性使得模型学习面临挑战。为了用有限的信息提高模型的学习效果，本文假设节点的影响力（通过平均可控性量化）能够显著改善异常检测的性能。

**方法:** 提出的方法包括：1) 使用平均可控性作为边权重；2) 将其编码为one-hot边属性向量。

**结果:** 通过对真实世界和合成网络进行严格评估，并与六个最先进基线对比，所提出的方法在识别异常方面表现出更好的性能。

**结论:** 这项工作强调了在处理稀疏和不平衡数据集的异常检测时，整合平均可控性作为额外度量指标的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Anomaly+Detection+with+Graph+Neural+Networks+using+Controllability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13954&send_immediately=true&force_search=false)

**原文摘要:** Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [43] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp, Gustav Ryberg Smidt, Hendrik Hameeuw, Katrien De Graef*

**主要类别:** cs.LG

**AI概要:** 本文介绍了用于楔形文字分类的机器学习技术的训练和评估。


<details>
  <summary>更多</summary>
  
**动机:** 楔形文字存在大量变异性，这使得在一个数据集上训练的机器学习模型不太可能在另一个数据集上成功运行。作者希望通过研究这些差异对性能的影响，来影响未来的数据获取标准，并为未来的楔形文字分类任务提供坚实的基础。

**方法:** 该研究使用了ResNet50机器学习模型，该模型已在来自三个美索不达米亚城市的粘土板上的手写古巴比伦（约公元前2000-1600年）文献文本上进行了训练和测试。

**结果:** 对于至少有20个实例的符号，ResNet50模型达到了87.1%的top-1得分和96.5%的top-5得分。

**结论:** 这些自动分类结果是首次应用于古巴比伦文本的结果，因此目前没有可比较的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Signs+of+the+Past%2C+Patterns+of+the+Present%3A+On+the+Automatic+Classification+of+Old+Babylonian+Cuneiform+Signs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13959，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13959&send_immediately=true&force_search=false)

**原文摘要:** The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [44] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia, Anindya Ghosh, Srikanth Ramaswamy*

**主要类别:** cs.LG

**AI概要:** 该研究提出了一种不确定性调制增益动力学机制，以解决持续学习中的稳定性差距问题。


<details>
  <summary>更多</summary>
  
**动机:** 近期的持续学习研究中发现，在掌握新任务时，已掌握任务的表现会出现短暂下降，即所谓的“稳定性差距”。这种现象违背了持续学习的目标，揭示了在缓解遗忘方面的缺乏稳健性。因此，有必要调查可以调和可塑性和稳定性的机制。

**方法:** 研究人员提出了不确定性调制增益动力学，这是一种自适应机制，它近似于双时间尺度优化器，并动态平衡知识整合，尽量减少对先前巩固信息的干扰。

**结果:** 通过在联合训练下的MNIST和CIFAR基准测试的领域增量和类别增量变体上评估该机制，结果表明不确定性调制增益动力学有效减弱了稳定性差距。

**结论:** 分析表明，增益调制复制了去甲肾上腺素能功能在皮层电路中的作用，为减少稳定性差距和提高持续学习任务的性能提供了机制上的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Noradrenergic-inspired+gain+modulation+attenuates+the+stability+gap+in+joint+training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14056，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14056&send_immediately=true&force_search=false)

**原文摘要:** Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [45] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel, Thomas A. W. Bolton, Mikkel Schöttner, Anjali Tarun, Sebastien Tourbier, Yasser Alemàn-Gòmez, Jonas Richiardi, Patric Hagmann*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的深度和谐化框架，用于在多站点研究中调整结构连接组数据，无需元数据或旅行主体，并通过图卷积自编码器实现最佳的拓扑结构保留和个体性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的神经影像样本量小限制了可靠的生物标志物的发展；大型多地点研究存在由于扫描仪异质性导致的获取偏差。现有方法要么依赖详细元数据，要么忽视了SCs的图拓扑。

**方法:** 提出了一个基于图卷积自编码器的现场条件深度和谐化框架，在不同的获取地点间调整SCs，无需元数据或旅行主体。并在三种深度架构中进行了基准测试：全连接自编码器、卷积自编码器和图卷积自编码器。

**结果:** 非图模型在边权重预测和边存在检测方面表现出色，而图自编码器在保持拓扑结构和个体性方面表现更优，分别由图度量和指纹识别准确性反映。LR基线在数值性能上最高，但实际应用受限于元数据的可用性。

**结论:** 模型架构对SC和谐化性能起着关键作用，图基方法特别适合于大规模多地点SC研究中的结构意识领域泛化的SC和谐化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Structural+Connectome+Harmonization+Using+Deep+Learning%3A+The+Strength+of+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13992&send_immediately=true&force_search=false)

**原文摘要:** Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [46] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav, Vivek Vijay*

**主要类别:** cs.LG

**AI概要:** Kolmogorov Arnold Networks (KANs)在处理类别不平衡分类问题上虽然对原始不平衡数据表现良好，但存在计算成本高且与传统不平衡策略冲突的问题。MLPs结合不平衡技术以较低资源成本实现了与KANs相当的效果。未来研究需要针对KANs开发专门的架构修改、优化计算效率等。


<details>
  <summary>更多</summary>
  
**动机:** 评估KANs在类别不平衡分类中的性能，并探讨其相对于传统神经网络（如MLPs）的优势和局限性。

**方法:** 通过十个基准数据集进行实证评估，比较了KANs和MLPs在处理原始不平衡数据时的表现，并测试了它们与重采样和焦点损失等不平衡策略的兼容性。

**结果:** KANs在处理原始不平衡数据方面优于MLPs，但在应用重采样或焦点损失后性能显著下降；同时，KANs的计算成本较高，而MLPs结合不平衡技术能以较低的资源成本达到相似效果。

**结论:** KANs对于原始不平衡数据是一种专业解决方案，但由于其计算成本高和与标准重采样技术不兼容的问题，实际应用受到限制。未来的研究方向包括为KANs开发特定的架构修改、优化计算效率以及解决其与数据增强之间的理论冲突。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kolmogorov+Arnold+Networks+%28KANs%29+for+Imbalanced+Data+--+An+Empirical+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14121，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14121&send_immediately=true&force_search=false)

**原文摘要:** Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


### [47] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav, Aryeh Kontorovich*

**主要类别:** cs.LG

**AI概要:** 提出ParallelTime架构，结合并行注意力和Mamba机制，并引入动态权重分配器来优化长短依赖关系的处理，实现了更优的时间序列预测。


<details>
  <summary>更多</summary>
  
**动机:** 当前的时间序列预测方法对长短期依赖赋予了相同的权重，这并不是最优的做法。

**方法:** 提出ParallelTime Weighter机制，根据输入和模型知识为每个token计算长短期依赖的相互依存权重。同时引入ParallelTime架构，该架构包含ParallelTime Weighter机制。

**结果:** 在多个基准测试中达到最先进水平，表现出更强的鲁棒性、更低的FLOPs、更少的参数需求、更有效的扩展到更长的预测范围，并显著优于现有方法。

**结论:** 这项工作为时间序列预测中的并行注意力-Mamba的发展指明了有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ParallelTime%3A+Dynamically+Weighting+the+Balance+of+Short-+and+Long-Term+Temporal+Dependencies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13998&send_immediately=true&force_search=false)

**原文摘要:** Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [48] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen, Meng Zhao, Mostafa Reisi Gahrooei, Xubo Yue*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的因果表征学习框架CaRTeD，它结合了时间因果表征学习与不规则张量分解，能够处理高维、长度变化的数据，并且在理论和实验上都展示了优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间因果表征学习工具主要适用于低维时间序列数据，而许多实际应用中遇到的数据是高维的、长度不一且形式为不规则张量。为了分析此类数据，需要一种能提取有意义聚类的方法。

**方法:** 作者引入了一个新的因果公式来表示潜在的聚类集，并提出了一个联合学习框架CaRTeD，该框架将时间因果表征学习与不规则张量分解相结合。此外，还提供了一种更灵活的正则化设计以增强张量分解的效果。

**结果:** 理论分析表明算法可以收敛到稳定点，并填补了最先进的不规则张量分解理论保证上的空白。实验证明，所提出的方法优于现有技术，并增强了因果表征的可解释性。

**结论:** CaRTeD框架不仅提供了一种有效的方法来分析复杂、高维、长度不一的时间序列数据，而且在理论上证明了其有效性，在实践中也展示了更好的性能和更强的可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Temporal+Causal+Representation+Learning+with+Tensor+Decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14126，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14126&send_immediately=true&force_search=false)

**原文摘要:** Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


### [49] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout, Audrey Durand*

**主要类别:** cs.LG

**AI概要:** 本文解释了基于对偶公式在马尔科夫决策过程中寻找静态CVaR最优策略的动态规划方法失败的原因，并证明了通过CVaR分解在对偶CVaR上搜索单一、均匀最优策略是根本受限的。


<details>
  <summary>更多</summary>
  
**动机:** 之前的研究表明，在马尔可夫决策过程中使用基于对偶公式的动态规划方法来找到静态CVaR最优策略可能会失败，但原因尚不清楚。本研究旨在探讨这种失败的根本原因。

**方法:** 作者将评估给定策略的静态CVaR问题框架为两个不同的最小化问题，并提出了“风险分配一致性约束”的概念。当这些约束条件的交集为空时，就会出现以前观察到的评估错误。

**结果:** 作者证明了在优化对偶CVaR DP时出现的问题可以通过返回的策略具有非零CVaR评估差距来解释，并且在某些MDP中，没有任何单一策略可以在所有初始风险水平上都是最优的。

**结论:** 基于对偶CVaR分解搜索单一、均匀最优策略的方法存在根本限制，这为未来研究提供了新的视角和方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Fundamental+Limitations+of+Dual+Static+CVaR+Decompositions+in+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14005&send_immediately=true&force_search=false)

**原文摘要:** Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [50] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang, Zhenyuan Yuan, Minghui Zhu*

**主要类别:** cs.LG

**AI概要:** 本文研究了高斯过程回归的拜占庭弹性联邦在线学习，并提出了一种能够抵抗拜占庭故障的算法，通过实验验证了该算法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 随着分布式机器学习的发展，如何在存在拜占庭故障的情况下保证模型的准确性变得至关重要。本研究旨在解决这一问题，特别是在高斯过程回归领域。

**方法:** 我们开发了一种拜占庭弹性的联邦GPR算法，该算法允许云端和一组代理共同学习一个潜在函数，并改进学习性能。每个基于代理的本地GPR向云端发送可能被破坏的本地预测，而基于云端的聚合GPR则通过拜占庭弹性的专家产品聚合规则计算全局模型。然后，云端将当前全局模型广播给所有代理，代理融合GPR通过融合接收到的全局模型和基于代理的本地GPR来优化本地预测。

**结果:** 通过在玩具示例和两个中等规模的真实数据集上的实验表明，所提出的算法可以显著提高学习精度。

**结论:** 我们提出的拜占庭弹性联邦GPR算法能够在存在拜占庭故障的情况下有效提高学习性能，为分布式机器学习提供了一种新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Byzantine-resilient+federated+online+learning+for+Gaussian+process+regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14021，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14021&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [51] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo, Tao Zhou, Ming Du, Martin V. Holt, Andrej Singer, Mathew J. Cherukara*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为DONUT的物理感知神经网络，它可以在无需标记数据集或预训练的情况下，实时预测晶体晶格应变和方向。实验表明，与传统方法相比，DONUT的数据特征提取效率提高了200多倍。


<details>
  <summary>更多</summary>
  
**动机:** X射线散射技术对于研究材料的纳米级基本结构属性至关重要。然而，实时分析仍然是一个重要的瓶颈，通常受到人工制品和计算需求的阻碍。在扫描X射线纳米衍射显微镜中，这种挑战由于发散光束与样品局部结构的卷积而变得更加复杂。

**方法:** 作者引入了DONUT（通过无监督训练对纳米束进行光学衍射），这是一种用于快速自动分析纳米束衍射数据的物理感知神经网络。通过将其架构中直接结合可区分的几何衍射模型，DONUT学习实时预测晶体晶格应变和方向。

**结果:** 实验结果表明，DONUT能够在超过200倍于传统拟合方法的效率下准确提取数据中的所有特征。

**结论:** DONUT克服了X射线科学中监督机器学习的基本限制，为扫描X射线纳米衍射显微镜提供了一种新的、更有效的实时分析工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DONUT%3A+Physics-aware+Machine+Learning+for+Real-time+X-ray+Nanodiffraction+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14038&send_immediately=true&force_search=false)

**原文摘要:** Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [52] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu, Yao Luan, Qing-Shan Jia*

**主要类别:** cs.LG

**AI概要:** 论文介绍了一种基于偏好的多目标强化学习方法（Pb-MORL），通过将偏好整合进MORL框架，以更灵活直观地指导决策，避免了复杂的奖励设计，并在多个任务中超越了使用真实奖励函数的Oracle方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的多目标强化学习方法依赖于预定义的奖励函数，这难以平衡冲突的目标且可能过于简化问题。为了提供更灵活和直观的决策指导，本文引入了基于偏好的多目标强化学习方法。

**方法:** 该方法通过构建一个与给定偏好一致的多目标奖励模型来指导策略优化，并理论证明了优化此奖励模型等同于训练帕累托最优策略。

**结果:** 在多个基准多目标任务、多能源管理任务和多车道高速公路自动驾驶任务中，该方法的表现超过了使用真实奖励函数的Oracle方法。

**结论:** 该研究展示了基于偏好的多目标强化学习方法在复杂实际系统中的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preference-based+Multi-Objective+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14066，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14066&send_immediately=true&force_search=false)

**原文摘要:** Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [53] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的DPMT框架，以改善人类与AI在实时协作中的互动，特别是通过多尺度ToM模块来建模人类的心理特征。实验表明该框架增强了人机协作效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型代理在没有直接沟通的情况下，难以准确模拟复杂的人类心理特征，如领域意图，特别是在动态场景中适应不同和未见过的人类行为时。

**方法:** 提出了一个基于认知科学双重过程理论的新型DPMT框架，其中包含一个多尺度的ToM模块，用于通过心理特征推理来强化对人类伙伴的建模。

**结果:** 实验结果证明了DPMT显著提升了人机协作的效果，并且消融研究进一步验证了多尺度ToM对慢系统的重要贡献。

**结论:** DPMT框架能够有效解决当前大型语言模型代理存在的问题，提高了AI在理解和适应人类行为方面的能力，为更高效的人机协作提供了可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DPMT%3A+Dual+Process+Multi-scale+Theory+of+Mind+Framework+for+Real-time+Human-AI+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14088，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14088&send_immediately=true&force_search=false)

**原文摘要:** Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb, Abdolazim Rezaei, Raj Atulkumar Patel, Mehdi Sookhak*

**主要类别:** cs.AI

**AI概要:** GraphTrafficGPT是一种新的基于图的架构，通过并行执行任务和动态资源分配来提高大型语言模型在交通管理中的效率。实验表明，它比TrafficGPT减少了50.2%的token消耗和19.0%的平均响应延迟，并提高了多查询处理的效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于链的系统（如TrafficGPT）受到顺序任务执行、高token使用率和扩展性差的影响，无法有效应对复杂的真实世界场景。

**方法:** GraphTrafficGPT将任务及其依赖关系表示为有向图中的节点和边，实现了高效的并行执行和动态资源分配。此外，引入了Brain Agent分解用户查询，构建优化的依赖图，并协调专门代理网络进行数据检索、分析、可视化和模拟。

**结果:** 实验结果表明，与TrafficGPT相比，GraphTrafficGPT减少了50.2%的token消耗和19.0%的平均响应延迟，并支持同时多查询执行，效率提高了23.0%。

**结论:** GraphTrafficGPT提供了一种改进的方法，用于解决现有LLM驱动的交通应用中遇到的问题，特别是通过减少token使用和支持并发多查询处理来提升效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GraphTrafficGPT%3A+Enhancing+Traffic+Management+Through+Graph-Based+AI+Agent+Coordination，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13511&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [55] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li, Melanie Sclar, Hunter Lang, Ansong Ni, Jacqueline He, Puxin Xu, Andrew Cohen, Chan Young Park, Yulia Tsvetkov, Asli Celikyilmaz*

**主要类别:** cs.AI

**AI概要:** PrefPalette 是一个框架，它将偏好分解为属性维度，并根据不同的社交社区的价值观调整其偏好预测。通过多属性决策制定原则的两种方式，PrefPalette 在 Reddit 的 45 个社交社区中评估时，比 GPT-4o 高出 46.6% 的平均预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前的偏好模型通常将人类判断视为黑箱。研究者希望理解用户偏好的背后原因，而不仅仅是他们喜欢什么。

**方法:** PrefPalette 框架通过生成合成训练数据以隔离个别属性效果（例如正式性、幽默、文化价值观），并基于注意力机制学习不同社交社区如何动态地对这些属性进行加权，从而实现多属性决策制定。

**结果:** 在来自在线平台 Reddit 的 45 个社交社区上进行评估时，PrefPalette 的平均预测准确性比 GPT-4o 高出 46.6%。此外，PrefPalette 还揭示了直观的、特定于社区的特征：学术社区重视冗长和刺激；冲突导向的社区重视讽刺和直接性；支持性的社区强调共情。

**结论:** 通过建模由属性介导的人类判断结构，PrefPalette 不仅提供了优越的偏好建模，还提供了透明、可解释的见解，是迈向更值得信赖、更具价值意识的个性化应用的第一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PrefPalette%3A+Personalized+Preference+Modeling+with+Latent+Attributes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13541，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13541&send_immediately=true&force_search=false)

**原文摘要:** Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [56] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán, Cristina Puente*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种利用大型语言模型开发专家系统的新方法，通过限制领域和使用结构化的基于提示的提取方法，生成可以用Prolog表示并由人类专家验证和纠正的知识。这种方法确保了开发的专家系统的可解释性、可扩展性和可靠性，并展示了对事实的严格遵守和语义连贯性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在知识型系统如开放域问答中取得了成功，但它们存在一些缺点，例如产生幻觉或自信地生成错误或无法验证的事实。

**方法:** 通过限制领域和采用结构化的基于提示的提取方法，将知识以Prolog的形式符号化表示，可以由人类专家进行验证和纠正。

**结果:** 通过与Claude Sonnet 3.7和GPT-4.1的定量和定性实验，表明生成的知识库对事实的严格遵守和语义连贯性。

**结论:** 提出了一种透明的混合解决方案，结合了大型语言模型的记忆能力和符号系统的精确性，为敏感领域的可靠AI应用奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GOFAI+meets+Generative+AI%3A+Development+of+Expert+Systems+by+means+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13550，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13550&send_immediately=true&force_search=false)

**原文摘要:** The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [57] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

**主要类别:** cs.AI

**AI概要:** 本文探讨了关系学习的重要性及当前未得到应有重视的原因，并提出提升其地位的建议。


<details>
  <summary>更多</summary>
  
**动机:** 作者观察到尽管现实世界的数据大多以关系型格式存在，但AI研究和应用却多集中于像素、文本等感知或描述层面的数据建模，因此希望强调关系学习的价值并促进其发展。

**方法:** 通过分析当前机器学习研究与实际商业数据形式之间的差异，解释关系学习领域的现状。

**结果:** 指出关系学习未能广泛流行的原因在于其适用范围有限以及对关系的限制较多。

**结论:** 为了使关系学习获得应有的重要性，需要解决现有方法在处理复杂关系数据时的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+Isn%27t+Relational+Learning+Taking+Over+the+World%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13558，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13558&send_immediately=true&force_search=false)

**原文摘要:** AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [58] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang, Xi Wang, Mo Hu, Zhenyu Zhang*

**主要类别:** cs.AI

**AI概要:** 论文介绍了一种新的系统BifrostRAG，该系统通过结合实体网络图和文档导航图来改善法规文本的多跳问题回答。它在多跳问题数据集上实现了高精度、召回率和F1分数，显著优于现有的向量或图基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 信息检索和问答系统对于自动化建设合规性检查至关重要，但法规文本的语言和结构复杂性给传统检索增强生成（RAG）系统带来了挑战。

**方法:** BifrostRAG是一个双图RAG集成系统，它明确地建模了语言关系（通过实体网络图）和文档结构（通过文档导航图）。该架构支持一种混合检索机制，将图遍历与基于向量的语义搜索相结合。

**结果:** 评估显示BifrostRAG达到了92.8%的精确度，85.5%的召回率，以及87.3%的F1得分，这些结果明显优于当前领先的向量或图RAG基线方法。

**结论:** BifrostRAG被确立为强大的LLM驱动的合规性检查知识引擎。其双图混合检索机制为跨知识密集型工程领域的复杂技术文档提供了可转移的设计蓝图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BifrostRAG%3A+Bridging+Dual+Knowledge+Graphs+for+Multi-Hop+Question+Answering+in+Construction+Safety，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13625，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13625&send_immediately=true&force_search=false)

**原文摘要:** Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [59] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek, Johan Jeuring, Rogier Bos*

**主要类别:** cs.AI

**AI概要:** 本文研究了基于最终答案的自动化错误诊断在智能辅导系统中的潜力，特别是当学生将多个步骤合并为一步时。通过自动完成中间输入并诊断解决方案，可以缓解组合爆炸的问题。实验结果表明，该方法能诊断29.4%的步骤，并且与教师诊断的一致性达到97%。


<details>
  <summary>更多</summary>
  
**动机:** 许多智能辅导系统可以支持学生解决分步任务，但当学生将多个步骤合并为一个步骤时，可能产生大量的路径连接连续输入，导致组合爆炸，使得错误诊断困难。因此，需要一种新的方法来解决这个问题。

**方法:** 本研究设计了一项服务，该服务提供基于最终答案的错误规则诊断，当学生将多个步骤合并为一步时，自动根据任务解决方案完成中间输入并进行诊断。

**结果:** 实验结果表明，该方法能够诊断出29.4%的学生步骤，其中与教师诊断的一致性达到了97%。

**结论:** 这些结果可以为进一步探索这种方法提供基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Buggy+rule+diagnosis+for+combined+steps+through+final+answer+evaluation+in+stepwise+tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13651&send_immediately=true&force_search=false)

**原文摘要:** Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [60] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek, Johan Jeuring, Rogier Bos*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种融合模型跟踪和基于约束的建模两种范式的方法，以诊断学生在解题过程中的输入，并通过实际数据集验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 模型跟踪和基于约束的建模是诊断学生在分步任务中输入的两种方法。然而，这两种方法各自有其局限性：模型跟踪支持识别学生连续的问题解决步骤，而基于约束的建模则在学生将多个步骤合并为一个步骤时仍能进行诊断。因此，作者希望结合这两种方法的优势，提高对学生解题过程的诊断能力。

**方法:** 作者提出了将模型跟踪和基于约束的建模相结合的方法，具体来说，就是把约束定义为学生的输入与策略的某一步骤之间的共同属性，从而即使当学生偏离策略或合并多个步骤时也能提供诊断。为了证明概念的可行性，研究者们使用现有的包含学生解二次方程步骤的数据集（n=2136）生成了诊断结果。

**结果:** 研究结果显示，系统生成的诊断与教师对随机样本（包括70个偏差实例和70个策略应用实例）的编码完全吻合。

**结论:** 这种结合模型跟踪和基于约束的建模的新方法能够有效地诊断学生的解题过程，特别是在多步骤策略的应用上，且其准确性得到了验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Combining+model+tracing+and+constraint-based+modeling+for+multistep+strategy+diagnoses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13652&send_immediately=true&force_search=false)

**原文摘要:** Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [61] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian, Xiaoyuan Ren, Zihao Wang, Onat Gungor, Xiaofan Yu, Tajana Rosing*

**主要类别:** cs.AI

**AI概要:** DailyLLM 是一个创新的日志生成和总结系统，利用大型语言模型（LLM）全面整合位置、运动、环境和生理四个维度的上下文活动信息，仅使用智能手机和智能手表上的常见传感器。该系统通过轻量级框架和高效特征提取，实现了高精度和快速推理速度的日志生成，相比现有方法提高了17%的BERTScore精度，并且推理速度快了近10倍。


<details>
  <summary>更多</summary>
  
**动机:** 现有的活动日志生成方法在准确性、效率和语义丰富性方面存在显著局限。为了改善这些问题，研究者希望利用大型语言模型的强大语义理解和生成能力，结合常见的移动设备传感器数据，提供更精确高效的日志生成解决方案。

**方法:** DailyLLM 引入了一种轻量级的 LLM 框架，将结构化提示与高效特征提取相结合，从而实现高水平的活动理解。它整合了位置、运动、环境和生理四方面的上下文活动信息，仅使用智能手机和智能手表上常见的传感器。此外，该系统采用了一个1.5B参数的LLM模型，确保了高性能的同时保持了较低的计算需求。

**结果:** 实验表明 DailyLLM 在多个方面优于最先进的日志生成方法：它不仅在 BERTScore 精度上提升了17%，而且推理速度也快了近10倍。这些结果证明了 DailyLLM 在个人电脑和 Raspberry Pi 上的有效部署可能性。

**结论:** DailyLLM 作为首个全面整合多维上下文活动信息的日志生成和总结系统，成功地解决了现有方法中存在的准确性和效率问题。它的高性能和低资源消耗使得其可以在各种计算平台上广泛部署，为用户行为分析和健康监测提供了新的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DailyLLM%3A+Context-Aware+Activity+Log+Generation+Using+Multi-Modal+Sensors+and+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13737&send_immediately=true&force_search=false)

**原文摘要:** Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [62] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed, Carlota Quintana, Eduardo Mena, Jorge Bobed, Fernando Bobillo*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一款新的本体论查看工具OntView，它提供直观的本体论概念及其定义的可视化表示，并且能够展示通用概念包含（GCI），提供了多种简化本体论视图的方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的本体编辑器和查看器大多未能以有意义且不过于复杂的方式图形化表示本体结构，限制了用户理解大型本体框架中的依赖关系和属性的能力。

**方法:** 通过使用描述逻辑推理机并遵循“所见即所得”的范式，OntView展示了实际推断出的知识，并能可视化通用概念包含（GCI）。为了防止信息过载，OntView还提供三种方法来简化本体论视图：1）创建重要性评估的概念摘要；2）聚焦于两个给定类之间的TBox元素；3）动态隐藏/显示不同的分支。

**结果:** OntView成功地为用户提供了一个友好的界面，用于直观地理解和浏览复杂的本体论结构，同时避免了信息过载的问题。

**结论:** OntView作为一个开源工具发布，旨在解决当前本体论可视化工具存在的不足，提供更有效的本体论结构图形表示方式，帮助用户更好地理解大型本体框架中的依赖关系和属性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OntView%3A+What+you+See+is+What+you+Meant，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13759&send_immediately=true&force_search=false)

**原文摘要:** In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [63] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini, Remo Pareschi, Marco Pedroni, Giovanni Battista Raggi*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种结合启发式提取、语义激活和组合合成的混合架构，通过语义相互依赖将多个启发式融合成连贯且与背景相关的叙述。


<details>
  <summary>更多</summary>
  
**动机:** 传统的决策引擎通常选择最佳规则，而该研究旨在融合冲突的启发式策略，以形成连贯且对背景敏感的叙述。

**方法:** 该方法从古典军事理论到现代企业战略中汲取灵感，通过语义相互依赖过程激活并组合多个启发式策略，受到量子认知研究的启发。它不选择最佳规则，而是通过语义交互建模和修辞框架将冲突的启发式策略融合在一起。

**结果:** 该框架通过Meta vs. FTC案例研究进行演示，并通过语义度量进行了初步验证。讨论了局限性和扩展（例如动态干扰调谐）。

**结论:** 这种新的混合架构为智能体增强的战略推理提供了一个不同于传统决策引擎的新视角，但需要进一步研究其局限性及改进方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Extraction+to+Synthesis%3A+Entangled+Heuristics+for+Agent-Augmented+Strategic+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13768&send_immediately=true&force_search=false)

**原文摘要:** We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [64] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li, Yuming Xu, Yiming Li, Hanmo Liu, Darian Li, Chen Jason Zhang, Lei Chen, Qing Li*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种轻量级框架EAGLE，它结合了短期时间临近性和长期全局结构模式，通过时间感知模块和结构感知模块，并采用自适应加权机制平衡两者贡献。EAGLE在效率和效果上均优于现有T-GNN模型，速度提升了50倍以上。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间图神经网络（T-GNNs）虽然成功地利用复杂的架构来建模时间和结构依赖关系，但往往由于计算开销高而面临可扩展性和效率挑战。因此，需要一种更高效的方法来进行动态图的时间链接预测。

**方法:** EAGLE包含一个时间感知模块，该模块聚合来自节点最近邻居的信息以反映其即时偏好；以及一个结构感知模块，利用时间个性化PageRank捕捉全局重要节点的影响。此外，EAGLE使用自适应加权机制根据数据特征动态调整两者的贡献，并避免复杂的多跳消息传递或内存密集型机制。

**结果:** 在七个真实世界的时间图上的广泛实验表明，EAGLE在效率和效果方面始终优于最先进的T-GNNs，实现了超过50倍的速度提升。

**结论:** EAGLE提供了一种更高效、更有效的解决方案来进行动态图中的时间链接预测，解决了现有T-GNNs的可扩展性和效率问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Speed+meets+Accuracy%3A+an+Efficient+and+Effective+Graph+Model+for+Temporal+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13825&send_immediately=true&force_search=false)

**原文摘要:** Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [65] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte, Christian Medeiros Adriano, Sona Ghahremani, Holger Giese*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种因果知识迁移框架，使多智能体在非平稳环境中能够学习并共享紧凑的因果表示。当环境变化时，该框架允许智能体通过查询包含碰撞信息的查找模型来即时应用从其他智能体转移来的恢复动作，而无需重新训练。研究发现，具有不同目标的智能体能缩小随机探索和完全再训练策略之间约一半的差距，并且因果知识迁移的效果取决于环境复杂性和智能体异质性目标之间的相互作用。


<details>
  <summary>更多</summary>
  
**动机:** 传统MARL中的知识迁移方法难以泛化，智能体经常需要昂贵的再训练以适应变化。为了解决这一问题，提高智能体在非平稳环境中的适应能力，作者引入了因果知识迁移框架。

**方法:** 作者将每次碰撞建模为一个因果干预，其形式为一系列恢复动作（宏），这些动作的效果对应于如何绕过障碍物并增加实现目标的机会。恢复动作宏可以从第二个智能体在线转移，并通过使用包含局部上下文信息（如碰撞）的查找模型查询后立即应用，无需再训练。

**结果:** 实验结果表明：1. 具有异质目标的智能体能够在适应新环境时弥合随机探索与完全再训练策略间大约一半的差距；2. 因果知识迁移的影响依赖于环境复杂性与智能体异质性目标间的互动。

**结论:** 因果知识迁移框架为解决多智能体强化学习中跨智能体的知识迁移难题提供了一种新的解决方案，尤其是在非平稳环境下。它不仅减少了再训练的成本，还展示了根据环境复杂性和智能体目标差异调整迁移效果的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Knowledge+Transfer+for+Multi-Agent+Reinforcement+Learning+in+Dynamic+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13846，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13846&send_immediately=true&force_search=false)

**原文摘要:** [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [66] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński, Mikołaj Hołysz, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种模型无关的潜在空间创意框架，通过导航想法的连续嵌入空间实现可控和可扩展的创造力。该框架无需手工制作规则，易于适应不同领域、输入格式和创意任务。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型（LLMs）在生成既新颖又相关的想法方面存在困难，往往只能复制训练中看到的模式，缺乏创造性的发散能力。尽管有领域特定的启发式方法和结构化提示管道，但这些解决方案脆弱且难以推广。

**方法:** 作者提出了一个模型无关的潜在空间创意框架，通过导航想法的连续嵌入空间来实现可控和可扩展的创造力。这个框架不需要手工编写的规则，并且可以轻松适应不同的领域、输入格式和创意任务。

**结果:** 文中介绍了该方法的早期原型，展示了其作为人类-AI协作的一般用途共同创意者的潜力。

**结论:** 此框架可能成为人类与AI合作进行创意工作的通用辅助工具，显示出巨大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+as+Innovators%3A+A+Framework+to+Leverage+Latent+Space+Exploration+for+Novelty+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13874，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13874&send_immediately=true&force_search=false)

**原文摘要:** Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [67] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin, Haowen Xiao, Jielei Chu, Fengmao Lv, Yuxiao Li, Tianrui Li*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种新的视觉-语言因果干预框架ADPC，用于辅助诊断阿尔茨海默病。该框架利用大型语言模型总结临床数据，并结合MRI、fMRI图像和文本数据对认知正常、轻度认知障碍和阿尔茨海默病进行分类。通过因果干预消除混淆因素，实验证明其性能优越，展示了多模态学习与因果推理结合在神经疾病诊断中的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 早期识别和干预轻度认知障碍（MCI）可以有效减缓痴呆的发展，但目前诊断阿尔茨海默病（AD）仍面临巨大挑战，主要由于多模态数据的选择偏差和变量之间的复杂关系造成的混淆因素。

**方法:** ADPC框架采用大型语言模型（LLM）以严格模板总结临床数据，维持结构化文本输出，即使数据集不完整或分布不均。它使用MRI、fMRI图像和由LLM生成的文本数据来将参与者分为认知正常（CN）、MCI和AD三类。通过因果干预方法消除混淆因素。

**结果:** 实验结果显示，该方法在区分CN/MCI/AD案例方面表现出色，在大多数评估指标上达到了最先进（SOTA）水平。

**结论:** 研究表明，将因果推理与多模态学习相结合在神经疾病诊断中具有潜在的应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-modal+Causal+Intervention+for+Alzheimer%27s+Disease+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13956&send_immediately=true&force_search=false)

**原文摘要:** Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [68] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar, Martín Diéguez, François Olivier, Torsten Schaub, Igor Stéphan*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的基于时间与约束的逻辑扩展，增强了Answer Set Programming处理复杂动态系统的能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于逻辑的方法如Answer Set Programming (ASP) 在处理具有精细时间和数值分辨率的动态系统时面临重大挑战。为了解决这一问题，需要一种新的方法来增强ASP的时间和数值推理能力。

**方法:** 作者引入了Here-and-There逻辑及其非单调平衡扩展的一种新颖的时间和约束基础扩展，结合线性时间Here-and-There逻辑和带约束的Here-and-There逻辑，以实现对数字约束的直接集成和操作。

**结果:** 该表达系统通过两种基础ASP扩展的协同组合实现了对复杂动态系统的高效处理，提供了强大的非单调时间推理能力和直接处理数值约束的能力。

**结论:** 这项工作建立了在ASP范式内处理高分辨率复杂动态系统的基础逻辑框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Constraint+Temporal+Answer+Set+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13958，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13958&send_immediately=true&force_search=false)

**原文摘要:** Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [69] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen, Erika Barcelos, Roger French, Yinghui Wu*

**主要类别:** cs.AI

**AI概要:** 提出名为KROMA的新OM框架，利用大型语言模型和检索增强生成管道来丰富语义互操作性任务的上下文，通过一系列优化技术提高性能和效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的本体匹配系统通常依赖于手工规则或适应性有限的专门模型，因此需要一个更灵活且高效的解决方案。

**方法:** KROMA结合了大型语言模型和检索增强生成（RAG）流程，并引入基于双相似性的概念匹配和轻量级本体细化步骤，以减少候选概念并降低与LLMs交互的通信开销。

**结果:** 实验表明，KROMA在多个基准数据集上的表现优于传统OM系统和最新的LLM方法，同时保持了可比的通信开销。

**结论:** 研究证明了所提出的优化技术（目标知识检索、提示词丰富和本体细化）对于大规模本体匹配的可行性和优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KROMA%3A+Ontology+Matching+with+Knowledge+Retrieval+and+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14032&send_immediately=true&force_search=false)

**原文摘要:** Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [70] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau, Baiying Lu, Yanjun Cui*

**主要类别:** cs.AI

**AI概要:** 论文介绍了Glucose-ML，一个包含来自4个国家的2500多人的超过300,000天连续葡萄糖监测数据的集合，旨在促进透明、可重复和强大的AI解决方案的发展，并为算法开发者提供数据选择指导。


<details>
  <summary>更多</summary>
  
**动机:** 开发强大AI解决方案以管理糖尿病的一个主要障碍是获取大型高质量的数据集。为了加速AI解决方案的发展，需要创建公开可用的数据集来支持研究和创新。

**方法:** 作者引入了Glucose-ML数据集集合，它包含了10个公共糖尿病数据集，并通过案例研究对这些数据集进行了比较分析，以引导算法开发者进行数据选择并提供短期血糖预测基准。

**结果:** 研究表明，使用不同的数据集开发/评估时，同一算法可以产生显著不同的预测结果。

**结论:** 研究结果用于提出在糖尿病或其他更广泛的健康领域中开发稳健AI解决方案的建议。提供了每个纵向糖尿病数据集的直接链接以及开放代码。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Glucose-ML%3A+A+collection+of+longitudinal+diabetes+datasets+for+development+of+robust+AI+solutions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14077&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [71] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer, Neel Macwan, Atharva Jitendra Hude, Heejin Jeong, Shenghan Guo*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的基于生成式AI的人体运动模拟方法（G-AI-HMS），该方法在大多数测试任务中提高了动作的保真度和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的人体运动模拟方法通常存在运动保真度低的问题，这影响了其在工业任务中的应用效果。

**方法:** 通过整合文本到文本和文本到运动模型，G-AI-HMS将任务描述转换为与MotionGPT训练词汇一致的动作感知语言，并利用计算机视觉验证增强型动作的真实性。

**结果:** 在涉及八个任务的案例研究中，AI增强的动作在六个任务的空间准确性、四个任务的姿势归一化后对齐以及七个任务的整体时间相似性方面表现出较低的误差。统计分析显示AI增强显著降低了关节误差和时间错位。

**结论:** G-AI-HMS成功解决了现有HMS技术中存在的两个关键问题，即任务描述到动作感知语言的转换和AI增强动作的真实感验证，从而提高了物理任务模拟的质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+AI-Driven+High-Fidelity+Human+Motion+Simulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14097，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14097&send_immediately=true&force_search=false)

**原文摘要:** Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [72] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji, Callie C. Liao, Duoduo Liao*

**主要类别:** cs.AI

**AI概要:** 这项研究探讨了大语言模型（LLMs）在解释非破坏性评估（NDE）轮廓图中的应用，以提高桥梁状况分析的效率和准确性。通过评估九个LLM模型对五种不同NDE轮廓图的解析能力，发现四个模型能提供更优质的图像描述，并由其中两个模型生成更有效的总结。这表明LLMs有潜力改善桥梁维护决策的速度和质量。


<details>
  <summary>更多</summary>
  
**动机:** 桥梁的维护和安全对于交通部门至关重要，而非破坏性评估（NDE）技术是评估结构完整性的关键手段。然而，解读NDE数据既耗时又需要专业知识，可能会延误决策。因此，利用最新的大型语言模型（LLMs）来自动化并改进这种分析的需求应运而生。

**方法:** 本研究引入了一种全面评估LLMs解释NDE轮廓图的能力的方法，建立了将LLMs整合到桥梁检查工作流程中的框架。具体来说，研究中探索了几种LLM，并设计了专门的提示语以提升图像描述的质量。这些模型被用来解释通过评估桥梁状况的技术所获得的五种不同的NDE轮廓图。

**结果:** 研究表明，在评估的九个模型中有四个能够提供更好的图像描述，涵盖与桥梁状况相关的广泛话题。特别是ChatGPT-4和Claude 3.5 Sonnet这两个模型生成了更为有效的总结。

**结论:** 研究结果表明，LLMs有可能显著提高效率和准确性，为桥梁维护和安全评估提供更快的决策支持。该试点研究提出了一种创新的方法，即并行使用LLMs进行图像字幕和总结，从而增强基础设施管理和安全性评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Interpretation+of+Non-Destructive+Evaluation+Contour+Maps+Using+Large+Language+Models+for+Bridge+Condition+Assessment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14107&send_immediately=true&force_search=false)

**原文摘要:** Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [73] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为CUDA-L1的自动化强化学习框架，用于优化CUDA性能。它在NVIDIA A100上训练，在KernelBench上的250个CUDA内核中平均加速了17.7倍，最高可达449倍。并且该模型具有良好的GPU架构移植性。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型的快速发展，对GPU计算资源的需求呈指数级增长，迫切需要自动化的CUDA优化策略。然而，目前最先进的模型在提高CUDA速度方面的成功率较低。

**方法:** 引入了CUDA-L1，这是一种基于强化学习的自动化CUDA优化框架。它通过加速比奖励信号将最初表现不佳的LLM转换为有效的CUDA优化器，无需人类专业知识或领域知识。

**结果:** CUDA-L1在各种GPU架构上实现了显著的性能提升，并且能够发现多样化的CUDA优化技术，识别非明显的性能瓶颈。

**结论:** 强化学习可以通过简单的加速比奖励机制将一个初始性能较差的LLM转变为高效的CUDA优化器。这为自动优化CUDA操作开辟了新的可能性，有望大幅提升GPU效率并缓解GPU计算资源的压力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CUDA-L1%3A+Improving+CUDA+Optimization+via+Contrastive+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14111，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14111&send_immediately=true&force_search=false)

**原文摘要:** The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [74] [A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security](https://arxiv.org/abs/2507.13367)
*Mehrab Hosain, Rajiv Kapoor*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的信息隐藏策略，结合自适应像素值差分（APVD）和伪随机像素选择方法，解决了未使用块的问题，提高了数据隐藏的安全性、容量和图像质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的APVD方法遇到了“未使用块”的问题，这可能降低安全性、嵌入容量和视觉质量。为了克服这些挑战，需要一种改进的方法。

**方法:** 研究引入了伪随机像素选择与APVD相结合的新方法，以解决未使用块的问题，并提高数据隐藏的性能。

**结果:** 实验结果表明，新方法在安全性和数据隐藏容量方面优于现有技术，并且在PSNR、UIQ和SSIM等关键图像质量指标上表现出色。

**结论:** 该方法能够处理各种彩色和灰度的封面和秘密图像，在不损害图像美学质量的前提下确保数据传输的安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Novel+APVD+Steganography+Technique+Incorporating+Pseudorandom+Pixel+Selection+for+Robust+Image+Security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13367，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13367&send_immediately=true&force_search=false)

**原文摘要:** Steganography is the process of embedding secret information discreetly
within a carrier, ensuring secure exchange of confidential data. The Adaptive
Pixel Value Differencing (APVD) steganography method, while effective,
encounters certain challenges like the "unused blocks" issue. This problem can
cause a decrease in security, compromise the embedding capacity, and lead to
lower visual quality. This research presents a novel steganographic strategy
that integrates APVD with pseudorandom pixel selection to effectively mitigate
these issues. The results indicate that the new method outperforms existing
techniques in aspects of security, data hiding capacity, and the preservation
of image quality. Empirical results reveal that the combination of APVD with
pseudorandom pixel selection significantly enhances key image quality metrics
such as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),
and Structural Similarity Index (SSIM), surpassing other contemporary methods
in performance. The newly proposed method is versatile, able to handle a
variety of cover and secret images in both color and grayscale, thereby
ensuring secure data transmission without compromising the aesthetic quality of
the image.

</details>


### [75] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp, Jason D. Hiser, Anh Nguyen-Tuong, Jack W. Davidson*

**主要类别:** cs.CR

**AI概要:** 本文提出PHASE，一个能通过分析网络连接日志来区分人类和非人类活动的机器学习框架，并使用SHAP分析揭示真实用户的时间和行为特征。


<details>
  <summary>更多</summary>
  
**动机:** 网络安全模拟环境需要真实的人类行为才能有效，但目前缺乏量化方法来评估合成用户角色的行为保真度。

**方法:** PHASE完全被动运行，依赖标准网络监控，无需用户端仪器或可见监控迹象。它利用Zeek网络设备收集所有用于机器学习的网络活动，避免引入不必要的网络流量或人工痕迹。该研究还提出了一种新的标签方法，利用本地DNS记录对网络流量进行分类，从而实现机器学习分析。

**结果:** PHASE能够以超过90%的准确率区分人类与非人类活动，并成功识别出合成用户角色中的明显非人类模式，提出了改进方案，提高了合成活动的人类相似性。

**结论:** 通过PHASE框架和改进后的配置，可以生成更真实有效的合成用户角色，提高网络安全模拟环境的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PHASE%3A+Passive+Human+Activity+Simulation+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13505&send_immediately=true&force_search=false)

**原文摘要:** Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


### [76] [FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning](https://arxiv.org/abs/2507.13591)
*Sahar Ghoflsaz Ghinani, Elaheh Sadredini*

**主要类别:** cs.CR

**AI概要:** FuSeFL是一个完全安全且可扩展的联邦学习方案，适用于跨筒仓环境。它通过轻量级的安全多方计算和安全聚合来保护数据、模型和更新的保密性，并有效抵御推理威胁，同时大幅降低通信延迟和服务器内存使用，提高了准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习方法虽然采用了同态加密、差分隐私或安全多方计算等技术来缓解推理攻击，但这些方法通常伴随着高计算、通信或内存开销，并且许多方法忽略了全局模型本身的机密性。这限制了安全联邦学习的实际应用，特别是在涉及大数据集和严格合规要求的跨筒仓部署中。

**方法:** FuSeFL采用去中心化的方式在客户端对之间进行训练，利用轻量级的安全多方计算（MPC），并限制服务器的作用仅限于安全聚合。这种设计消除了服务器瓶颈，避免了数据卸载，并在整个训练过程中保持了数据、模型和更新的完全保密性。

**结果:** FuSeFL能够防御推理威胁，实现高达95%更低的通信延迟和50%更低的服务器内存使用，并且相比以前的安全联邦学习解决方案提高了准确性。

**结论:** FuSeFL展示了强大的安全性和效率，特别适合于跨筒仓的联邦学习部署，能够在保证数据和模型安全的同时，提供高效的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FuSeFL%3A+Fully+Secure+and+Scalable+Cross-Silo+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13591，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13591&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative model training without
centralizing client data, making it attractive for privacy-sensitive domains.
While existing approaches employ cryptographic techniques such as homomorphic
encryption, differential privacy, or secure multiparty computation to mitigate
inference attacks-including model inversion, membership inference, and gradient
leakage-they often suffer from high computational, communication, or memory
overheads. Moreover, many methods overlook the confidentiality of the global
model itself, which may be proprietary and sensitive. These challenges limit
the practicality of secure FL, especially in cross-silo deployments involving
large datasets and strict compliance requirements.
  We present FuSeFL, a fully secure and scalable FL scheme designed for
cross-silo settings. FuSeFL decentralizes training across client pairs using
lightweight secure multiparty computation (MPC), while confining the server's
role to secure aggregation. This design eliminates server bottlenecks, avoids
data offloading, and preserves full confidentiality of data, model, and updates
throughout training. FuSeFL defends against inference threats, achieves up to
95% lower communication latency and 50% lower server memory usage, and improves
accuracy over prior secure FL solutions, demonstrating strong security and
efficiency at scale.

</details>


### [77] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla, Ismail Shaheen, Dan DeGenaro, Rupayan Mallick, Bogdan Raita, Sarah Adel Bargal*

**主要类别:** cs.CR

**AI概要:** 提出GIFT技术，以保护扩散模型免受恶意微调的影响，同时保持生成安全内容的能力。实验表明，该方法在不影响安全内容性能的情况下，显著削弱了模型重新学习有害概念的能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的安全机制如安全检查器容易被绕过，概念擦除方法在对抗性微调下失败。因此需要一种新的方法来保护模型免受恶意微调的影响，同时保持其生成安全内容的能力。

**方法:** GIFT将免疫问题表述为一个双层优化问题：上层目标通过表示噪声和最大化来降低模型表示有害概念的能力；下层目标则保留安全数据上的性能。

**结果:** GIFT实现了对恶意微调的稳健抵抗，同时保持了安全生成质量。实验结果表明，该方法显著削弱了模型重新学习有害概念的能力，同时保持了安全内容的性能。

**结论:** GIFT提供了一个有希望的方向，可以创建本质上更安全的生成模型，这些模型能够抵抗对抗性微调攻击。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GIFT%3A+Gradient-aware+Immunization+of+diffusion+models+against+malicious+Fine-Tuning+with+safe+concepts+retention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13598，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13598&send_immediately=true&force_search=false)

**原文摘要:** We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


### [78] [Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques](https://arxiv.org/abs/2507.13629)
*Niveen O. Jaffal, Mohammed Alkhanafseh, David Mohaisen*

**主要类别:** cs.CR

**AI概要:** 本文综述了大型语言模型在网络安全中的应用，包括其整合入关键安全领域和自身的脆弱性及缓解策略，提供了实用见解和战略建议。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）正在通过智能化、适应性和自动化的方法改变网络安全领域，解决物联网、区块链和硬件安全等领域的挑战。

**方法:** 该研究通过综合最近的发展并确定关键限制，提供了一个关于LLM在网络安全应用方面的全面概述，集中在两个核心区域：1）将LLM融入关键网络安全领域；2）LLM本身的脆弱性以及缓解策略。

**结果:** 这项工作提供了实际的见解和战略性建议，以利用LLM建立安全、可扩展且面向未来的网络防御系统。

**结论:** 尽管存在一定的局限性，LLM为提升网络安全提供了新的途径和策略，对于构建智能的、自适应的自动化安全解决方案至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+in+Cybersecurity%3A+Applications%2C+Vulnerabilities%2C+and+Defense+Techniques，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13629&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are transforming cybersecurity by enabling
intelligent, adaptive, and automated approaches to threat detection,
vulnerability assessment, and incident response. With their advanced language
understanding and contextual reasoning, LLMs surpass traditional methods in
tackling challenges across domains such as IoT, blockchain, and hardware
security. This survey provides a comprehensive overview of LLM applications in
cybersecurity, focusing on two core areas: (1) the integration of LLMs into key
cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along
with mitigation strategies. By synthesizing recent advancements and identifying
key limitations, this work offers practical insights and strategic
recommendations for leveraging LLMs to build secure, scalable, and future-ready
cyber defense systems.

</details>


### [79] [TopicAttack: An Indirect Prompt Injection Attack via Topic Transition](https://arxiv.org/abs/2507.13686)
*Yulin Chen, Haoran Li, Yuexin Li, Yue Liu, Yangqiu Song, Bryan Hooi*

**主要类别:** cs.CR

**AI概要:** 提出了一种新的攻击方法TopicAttack，通过生成逐步转移话题的提示，实现更平滑的间接提示注入攻击，实验表明其攻击成功率超过90%，并且在各种防御方法下仍然有效。


<details>
  <summary>更多</summary>
  
**动机:** 现有的间接提示注入攻击方法由于突然的指令注入方式，其有效性受到了限制。为了克服这些限制并提高攻击的成功率和可信度，需要一种新的攻击方法。

**方法:** 提出的方法名为TopicAttack，它促使大型语言模型（LLM）生成一个虚构的对话过渡提示，该提示能够逐渐将话题转移到注入的指令上，从而使注入过程更加平滑。

**结果:** 通过广泛的实验验证，TopicAttack实现了最先进的性能，在大多数情况下攻击成功率（ASR）超过90%，即使面对不同的防御措施也表现良好。分析注意力分数发现，更高的注入到原始注意力比率导致更大的成功概率。

**结论:** TopicAttack相比现有方法，可以实现更高比例的注入到原始注意力比率，从而提高了攻击的成功率和可信度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TopicAttack%3A+An+Indirect+Prompt+Injection+Attack+via+Topic+Transition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13686，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13686&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown remarkable performance across a range
of NLP tasks. However, their strong instruction-following capabilities and
inability to distinguish instructions from data content make them vulnerable to
indirect prompt injection attacks. In such attacks, instructions with malicious
purposes are injected into external data sources, such as web documents. When
LLMs retrieve this injected data through tools, such as a search engine and
execute the injected instructions, they provide misled responses. Recent attack
methods have demonstrated potential, but their abrupt instruction injection
often undermines their effectiveness. Motivated by the limitations of existing
attack methods, we propose TopicAttack, which prompts the LLM to generate a
fabricated conversational transition prompt that gradually shifts the topic
toward the injected instruction, making the injection smoother and enhancing
the plausibility and success of the attack. Through comprehensive experiments,
TopicAttack achieves state-of-the-art performance, with an attack success rate
(ASR) over 90\% in most cases, even when various defense methods are applied.
We further analyze its effectiveness by examining attention scores. We find
that a higher injected-to-original attention ratio leads to a greater success
probability, and our method achieves a much higher ratio than the baseline
methods.

</details>


### [80] [Quantum Blockchain Survey: Foundations, Trends, and Gaps](https://arxiv.org/abs/2507.13720)
*Saurav Ghosh*

**主要类别:** cs.CR

**AI概要:** 这篇论文综述了量子计算对经典区块链系统的威胁，以及应对这一挑战的两大研究方向：后量子区块链和量子区块链。它分析了这两种方案的加密基础、架构设计和实施挑战，并提供了技术提案的比较概述，强调了安全、可扩展性和部署之间的权衡，指出了硬件、共识和网络设计中开放的研究问题。


<details>
  <summary>更多</summary>
  
**动机:** 随着量子计算的发展，它对传统区块链系统构成了基本风险，特别是削弱了常用的加密原语。为了应对这种威胁，研究人员探索了两种主要方向：一是采用量子抗性算法的后量子区块链，二是利用量子特性如纠缠和量子密钥分发的量子区块链。

**方法:** 该论文通过文献综述的方法，详细分析了上述两个研究领域的关键进展，包括它们的加密基础、架构设计及实现挑战。此外，文章还对不同技术方案进行了对比分析，探讨了在安全、可扩展性和部署方面的折衷。

**结果:** 结果是提供了一个结构化且全面的参考文献，有助于推动量子时代下安全区块链系统的发展。文中也明确了当前存在的开放研究问题，涉及硬件、共识机制和网络设计等多个方面。

**结论:** 结论指出，尽管存在许多技术和实施上的挑战，但针对量子计算威胁的区块链研究正在取得进展。未来的研究需要解决安全、可扩展性和部署的平衡问题，并克服硬件、共识和网络设计中的难题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum+Blockchain+Survey%3A+Foundations%2C+Trends%2C+and+Gaps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13720&send_immediately=true&force_search=false)

**原文摘要:** Quantum computing poses fundamental risks to classical blockchain systems by
undermining widely used cryptographic primitives. In response, two major
research directions have emerged: post-quantum blockchains, which integrate
quantum-resistant algorithms, and quantum blockchains, which leverage quantum
properties such as entanglement and quantum key distribution. This survey
reviews key developments in both areas, analyzing their cryptographic
foundations, architectural designs, and implementation challenges. This work
provides a comparative overview of technical proposals, highlight trade-offs in
security, scalability, and deployment, and identify open research problems
across hardware, consensus, and network design. The goal is to offer a
structured and comprehensive reference for advancing secure blockchain systems
in the quantum era.

</details>


### [81] [Developers Insight On Manifest v3 Privacy and Security Webextensions](https://arxiv.org/abs/2507.13926)
*Libor Polčák, Giorgio Maone, Michael McMahon, Martin Bednář*

**主要类别:** cs.CR

**AI概要:** 本文研究了Manifest v3对Web扩展带来的挑战和机遇，发现其影响因项目而异，部分关键API的缺失成为主要问题。


<details>
  <summary>更多</summary>
  
**动机:** 研究浏览器向Manifest v3过渡的影响，评估新API集对Web扩展功能性的挑战和机遇。

**方法:** 通过深入结构化的定性研究，分析Manifest v3的影响。

**结果:** 大多数项目表达了对用户利益有限、关键API移除或需要寻找替代方案的担忧；不同类型的Web扩展受影响程度不同，一些可以无缝迁移，而另一些则需移除功能或拒绝更新。

**结论:** Manifest v3的转换对Web扩展有不同程度的影响，某些关键API的缺乏是主要问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Developers+Insight+On+Manifest+v3+Privacy+and+Security+Webextensions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13926，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13926&send_immediately=true&force_search=false)

**原文摘要:** Webextensions can improve web browser privacy, security, and user experience.
The APIs offered by the browser to webextensions affect possible functionality.
Currently, Chrome transitions to a modified set of APIs called Manifest v3.
This paper studies the challenges and opportunities of Manifest v3 with an
in-depth structured qualitative research. Even though some projects observed
positive effects, a majority expresses concerns over limited benefits to users,
removal of crucial APIs, or the need to find workarounds. Our findings indicate
that the transition affects different types of webextensions differently; some
can migrate without losing functionality, while other projects remove
functionality or decline to update. The respondents identified several critical
missing APIs, including reliable APIs to inject content scripts, APIs for
storing confidential content, and others.

</details>


### [82] [Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology](https://arxiv.org/abs/2507.13932)
*Feng Yu, Ryan Laird*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种名为chain table的数据库内设计，以保证数据完整性，无需使用区块链系统。


<details>
  <summary>更多</summary>
  
**动机:** 实施完整的区块链系统以保护数据库在技术上可能很繁琐。为了简化流程并确保数据完整性，需要一种新的方法。

**方法:** 引入了一个名为chain table的数据库内设计，并提出了一套数据写入原则。

**结果:** 证明了chain table加上数据写入原则可以保证灵活的数据完整性，称为表级数据完整性（TDI）。

**结论:** chain table提供了一种简洁的设计，在没有重大技术障碍或存储开销的情况下保护数据完整性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chain+Table%3A+Protecting+Table-Level+Data+Integrity+by+Digital+Ledger+Technology，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.13932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.13932&send_immediately=true&force_search=false)

**原文摘要:** The rise of blockchain and Digital Ledger Technology (DLT) has gained wide
traction. Instead of relying on a traditional centralized data authority, a
blockchain system consists of digitally entangled block data shared across a
distributed network. The specially designed chain data structure and its
consensus mechanism protect blockchain data from being tampered by unauthorized
adversaries. However, implementing a full-fledged blockchain system to protect
a database can be technically cumbersome. In this work, we introduce an
in-database design, named chain table, to protect data integrity without the
need for a blockchain system. It features a succinct design without significant
technology barriers or storage overhead. To realize rigorous data security, we
also propose a set of data writing principles for the chain table. We prove
that the chain table, together with the data writing principles, will guarantee
flexible data integrity, named table-level data integrity (TDI).

</details>


### [83] [The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems](https://arxiv.org/abs/2507.14007)
*Serhan W. Bahar*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的威胁建模框架——CryptoNeo Threat Modelling Framework (CNTMF)，旨在解决区块链、加密货币和Web3技术集成到数字银行和金融科技运营中所带来的风险。


<details>
  <summary>更多</summary>
  
**动机:** 随着区块链、加密货币和Web3技术快速融入数字银行和金融科技操作，创建了一个融合传统金融系统与去中心化元素的集成环境。这种环境带来了诸如预言机操控和跨链漏洞等风险，需要一种专门的威胁模型来应对这些挑战。

**方法:** CNTMF是建立在STRIDE、OWASP Top 10、NIST框架、LINDDUN和PASTA等已确立的方法论之上的扩展，同时加入了混合层分析、CRYPTOQ助记符（针对加密货币特定风险）和AI增强反馈循环等定制组件。该框架通过资产映射、风险配置文件、优先级排序、缓解措施和迭代反馈阶段，支持数据驱动的风险缓解。

**结果:** 基于2025年发生的2025起事件的实际数据，CNTMF有助于减少损失，这些损失在2025年上半年因344起安全事件而达到了约24.7亿美元。

**结论:** CNTMF为对抗不断演变的风险（如国家赞助的攻击）提供了支持，确保了安全性和适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+CryptoNeo+Threat+Modelling+Framework+%28CNTMF%29%3A+Securing+Neobanks+and+Fintech+in+Integrated+Blockchain+Ecosystems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14007，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14007&send_immediately=true&force_search=false)

**原文摘要:** The rapid integration of blockchain, cryptocurrency, and Web3 technologies
into digital banks and fintech operations has created an integrated environment
blending traditional financial systems with decentralised elements. This paper
introduces the CryptoNeo Threat Modelling Framework (CNTMF), a proposed
framework designed to address the risks in these ecosystems, such as oracle
manipulation and cross-chain exploits. CNTMF represents a proposed extension of
established methodologies like STRIDE, OWASP Top 10, NIST frameworks, LINDDUN,
and PASTA, while incorporating tailored components including Hybrid Layer
Analysis, the CRYPTOQ mnemonic for cryptocurrency-specific risks, and an
AI-Augmented Feedback Loop. Drawing on real-world data from 2025 incidents,
CNTMF supports data-driven mitigation to reduce losses, which totalled
approximately $2.47 billion in the first half of 2025 across 344 security
events (CertiK via GlobeNewswire, 2025; Infosecurity Magazine, 2025). Its
phases guide asset mapping, risk profiling, prioritisation, mitigation, and
iterative feedback. This supports security against evolving risks like
state-sponsored attacks.

</details>


### [84] [An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting](https://arxiv.org/abs/2507.14109)
*Xinyu Cao, Bimal Adhikari, Shangqing Zhao, Jingxian Wu, Yanjun Pan*

**主要类别:** cs.CR

**AI概要:** 本文通过实验分析了基于深度学习的RF指纹识别系统的安全风险，发现其存在可被利用的错误分类行为和后门攻击，并指出原始信号训练模型容易将RF指纹与环境特征混淆，从而增加了攻击途径。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究主要集中在提高系统在无线环境变化中的鲁棒性，而忽略了DL方法在RF指纹识别中的安全漏洞。

**方法:** 作者进行了对抗驱动的实验分析，观察DL模型在域转移下的误分类行为，并通过大量真实世界实验验证这些行为可作为后门攻击手段。此外，还探讨了原始接收信号训练模型导致的问题。

**结果:** 发现了DL模型在域转移下的一致性误分类行为，这种行为可以被用作有效的后门，允许外部攻击者入侵系统。同时，基于原始信号训练的模型会将RF指纹与环境特征混淆，形成额外的攻击向量。

**结论:** 该研究揭示了DL-based RF指纹识别系统中未被充分重视的安全风险，强调需要新的防御策略来应对这些挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Adversarial-Driven+Experimental+Study+on+Deep+Learning+for+RF+Fingerprinting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.14109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.14109&send_immediately=true&force_search=false)

**原文摘要:** Radio frequency (RF) fingerprinting, which extracts unique hardware
imperfections of radio devices, has emerged as a promising physical-layer
device identification mechanism in zero trust architectures and beyond 5G
networks. In particular, deep learning (DL) methods have demonstrated
state-of-the-art performance in this domain. However, existing approaches have
primarily focused on enhancing system robustness against temporal and spatial
variations in wireless environments, while the security vulnerabilities of
these DL-based approaches have often been overlooked. In this work, we
systematically investigate the security risks of DL-based RF fingerprinting
systems through an adversarial-driven experimental analysis. We observe a
consistent misclassification behavior for DL models under domain shifts, where
a device is frequently misclassified as another specific one. Our analysis
based on extensive real-world experiments demonstrates that this behavior can
be exploited as an effective backdoor to enable external attackers to intrude
into the system. Furthermore, we show that training DL models on raw received
signals causes the models to entangle RF fingerprints with environmental and
signal-pattern features, creating additional attack vectors that cannot be
mitigated solely through post-processing security methods such as confidence
thresholds.

</details>
