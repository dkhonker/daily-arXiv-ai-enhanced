<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 97]
- [cs.AI](#cs.AI) [总数: 21]
- [stat.ML](#stat.ML) [总数: 9]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL](https://arxiv.org/abs/2506.03154)
*Zhaoyang Chen, Cody Fleming*

**主要类别:** cs.LG

**AI概要:** 论文研究了在扩散强化学习中如何通过模块化训练解耦引导模块与扩散模型，以提升训练效率与性能，并展示其良好的可转移性和可重用性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于扩散的强化学习方法依赖于引导模块和扩散模型的联合训练，这在早期阶段可能效果不佳。而离线RL中，引导仅依赖于离线数据，因此不需要联合训练。

**方法:** 基于三个关键发现进行实验：指导必要性、先指导后扩散训练以及跨模块可转移性。通过独立训练引导模块并将其冻结以指导扩散模型，同时验证不同算法间引导模块的可重用性。

**结果:** 所提出的模块化训练方法不仅降低了内存使用并提高了计算效率，还显著增强了样本效率和最终性能；此外，跨模块的可转移性减少了标准化分数的方差，并展示了不同算法间的直接重用能力。

**结论:** 本文提出了一种模块化训练方法，将引导模块与扩散模型解耦，为离线强化学习提供了一个新的范式：模块化、可重用和可组合的训练流程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modular+Diffusion+Policy+Training%3A+Decoupling+and+Recombining+Guidance+and+Diffusion+for+Offline+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03154&send_immediately=true&force_search=false)

**原文摘要:** Classifier free guidance has shown strong potential in diffusion-based
reinforcement learning. However, existing methods rely on joint training of the
guidance module and the diffusion model, which can be suboptimal during the
early stages when the guidance is inaccurate and provides noisy learning
signals. In offline RL, guidance depends solely on offline data: observations,
actions, and rewards, and is independent of the policy module's behavior,
suggesting that joint training is not required. This paper proposes modular
training methods that decouple the guidance module from the diffusion model,
based on three key findings:
  Guidance Necessity: We explore how the effectiveness of guidance varies with
the training stage and algorithm choice, uncovering the roles of guidance and
diffusion. A lack of good guidance in the early stage presents an opportunity
for optimization.
  Guidance-First Diffusion Training: We introduce a method where the guidance
module is first trained independently as a value estimator, then frozen to
guide the diffusion model using classifier-free reward guidance. This
modularization reduces memory usage, improves computational efficiency, and
enhances both sample efficiency and final performance.
  Cross-Module Transferability: Applying two independently trained guidance
models, one during training and the other during inference, can significantly
reduce normalized score variance (e.g., reducing IQR by 86%). We show that
guidance modules trained with one algorithm (e.g., IDQL) can be directly reused
with another (e.g., DQL), with no additional training required, demonstrating
baseline-level performance as well as strong modularity and transferability.
  We provide theoretical justification and empirical validation on bullet D4RL
benchmarks. Our findings suggest a new paradigm for offline RL: modular,
reusable, and composable training pipelines.

</details>


### [2] [Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World](https://arxiv.org/abs/2506.03155)
*Yu Zheng*

**主要类别:** cs.LG

**AI概要:** 本文探讨了跨领域多模态数据融合的问题，提出了一个四层框架来解决这一问题，并展示了其在现实世界问题中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 由于单一的信息获取方法无法对复杂的物理环境进行建模，因此需要融合来自不同来源的多模态数据。然而，为每个问题从头开始收集原始数据是不可行的，因此有必要利用已有领域的知识进行跨领域知识融合。

**方法:** 提出了一种包含领域层、链接层、模型层和数据层的四层框架，回答了'融合什么'、'为什么可以融合'和'如何融合'三个关键问题。

**结果:** 正式定义了跨领域多模态数据融合问题，讨论了其独特的挑战、与单领域数据融合的差异及优势，并通过提出的框架展示了如何有效融合跨领域多模态数据。

**结论:** 本文提出了一个四层框架，用于解决跨领域多模态数据融合问题，并展示了如何利用该框架设计有效的端到端解决方案以解决现实世界中的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fusing+Cross-Domain+Knowledge+from+Multimodal+Data+to+Solve+Problems+in+the+Physical+World，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03155，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03155&send_immediately=true&force_search=false)

**原文摘要:** The proliferation of artificial intelligence has enabled a diversity of
applications that bridge the gap between digital and physical worlds. As
physical environments are too complex to model through a single information
acquisition approach, it is crucial to fuse multimodal data generated by
different sources, such as sensors, devices, systems, and people, to solve a
problem in the real world. Unfortunately, it is neither applicable nor
sustainable to deploy new resources to collect original data from scratch for
every problem. Thus, when data is inadequate in the domain of problem, it is
vital to fuse knowledge from multimodal data that is already available in other
domains. We call this cross-domain knowledge fusion. Existing research focus on
fusing multimodal data in a single domain, supposing the knowledge from
different datasets is intrinsically aligned; however, this assumption may not
hold in the scenarios of cross-domain knowledge fusion. In this paper, we
formally define the cross-domain multimodal data fusion problem, discussing its
unique challenges, differences and advantages beyond data fusion in a single
domain. We propose a four-layer framework, consisting of Domains, Links, Models
and Data layers, answering three key questions: "what to fuse", "why can be
fused", and "how to fuse". The Domains Layer selects relevant data from
different domains for a given problem. The Links Layer reveals the philosophy
of knowledge alignment beyond specific model structures. The Models Layer
provides two knowledge fusion paradigms based on the fundamental mechanisms for
processing data. The Data Layer turns data of different structures,
resolutions, scales and distributions into a consistent representation that can
be fed into an AI model. With this framework, we can design end-to-end
solutions that fuse cross-domain multimodal data effectively for solving
real-world problems.

</details>


### [3] [DUAL: Dynamic Uncertainty-Aware Learning](https://arxiv.org/abs/2506.03158)
*Jiahao Qin, Bei Peng, Feng Liu, Guangliang Cheng, Lu Zong*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种名为DUAL的新框架，用于解决深度学习中的特征不确定性问题，并在多个任务上展示了其优越的性能。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习模型在多种学习场景中经常遇到特征不确定性问题，这显著影响了模型的性能和可靠性，尤其是在多模态场景下。

**方法:** DUAL引入了三个关键技术：动态特征不确定性建模、自适应分布感知调制和不确定性感知跨模态关系学习。

**结果:** 实验结果表明，DUAL在计算机视觉任务上取得了显著提升，在多模态学习中也表现出一致的增益。

**结论:** 论文提出了一种统一的框架DUAL，能够有效处理单模态和多模态场景中的特征不确定性问题，并通过实验验证了其在多个领域的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DUAL%3A+Dynamic+Uncertainty-Aware+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03158&send_immediately=true&force_search=false)

**原文摘要:** Deep learning models frequently encounter feature uncertainty in diverse
learning scenarios, significantly impacting their performance and reliability.
This challenge is particularly complex in multi-modal scenarios, where models
must integrate information from different sources with inherent uncertainties.
We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that
effectively handles feature uncertainty in both single-modal and multi-modal
scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty
Modeling, which continuously refines uncertainty estimates through joint
consideration of feature characteristics and learning dynamics; Adaptive
Distribution-Aware Modulation, which maintains balanced feature distributions
through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal
Relationship Learning, which explicitly models uncertainties in cross-modal
interactions. Through extensive experiments, we demonstrate DUAL's
effectiveness across multiple domains: in computer vision tasks, it achieves
substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on
CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it
demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy
on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements
on MISR. The code will be available on GitHub soon.

</details>


### [4] [Bayes Error Rate Estimation in Difficult Situations](https://arxiv.org/abs/2506.03159)
*Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi*

**主要类别:** cs.LG

**AI概要:** The study compares kNN, GHP, and KDE techniques for estimating Bayes Error Rate (BER) and finds that kNN is the most accurate non-parametric estimator, especially with fewer features.


<details>
  <summary>更多</summary>
  
**动机:** To identify which BER estimators are 'useful' by evaluating their accuracy on both synthetic and real-world applications across a wide range of BER values.

**方法:** Monte Carlo simulations with synthetic data and new test scenarios were used to examine the accuracy of kNN, GHP, and KDE techniques in estimating BER.

**结果:** kNN was found to be the most accurate non-parametric estimator for binary classification problems. At least 1000 samples per class are required to achieve under 5 percent range for 95 percent confidence bounds, and this requirement increases with the number of features.

**结论:** kNN is the most accurate non-parametric estimator for BER when dealing with fewer features, but as more features are added, other estimators may become more accurate. However, they continuously fail to meet the target range.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayes+Error+Rate+Estimation+in+Difficult+Situations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03159，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03159&send_immediately=true&force_search=false)

**原文摘要:** The Bayes Error Rate (BER) is the fundamental limit on the achievable
generalizable classification accuracy of any machine learning model due to
inherent uncertainty within the data. BER estimators offer insight into the
difficulty of any classification problem and set expectations for optimal
classification performance. In order to be useful, the estimators must also be
accurate with a limited number of samples on multivariate problems with unknown
class distributions. To determine which estimators meet the minimum
requirements for "usefulness", an in-depth examination of their accuracy is
conducted using Monte Carlo simulations with synthetic data in order to obtain
their confidence bounds for binary classification. To examine the usability of
the estimators on real-world applications, new test scenarios are introduced
upon which 2500 Monte Carlo simulations per scenario are run over a wide range
of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized
Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,
results show that kNN is overwhelmingly the more accurate non-parametric
estimator. In order to reach the target of an under 5 percent range for the 95
percent confidence bounds, the minimum number of required samples per class is
1000. As more features are added, more samples are needed, so that 2500 samples
per class are required at only 4 features. Other estimators do become more
accurate than kNN as more features are added, but continuously fail to meet the
target range.

</details>


### [5] [Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes](https://arxiv.org/abs/2506.03160)
*Shriyank Somvanshi, Anannya Ghosh Tusti, Mahmuda Sultana Mimi, Md Monzurul Islam, Sazzad Bin Bashar Polock, Anandi Dutta, Subasish Das*

**主要类别:** cs.LG

**AI概要:** 该研究利用深度学习模型改进自动化车辆碰撞事故的SAE级别分类，显著提高了分类的准确性和效率。


<details>
  <summary>更多</summary>
  
**动机:** 准确识别碰撞事件中的SAE自动化级别对于理解碰撞动态和系统责任至关重要，但现有方法往往忽视了特定于自动化的因素。

**方法:** 使用MambaAttention、TabPFN和TabTransformer三种先进的表格深度学习模型对德克萨斯州的碰撞数据进行分类，并采用SMOTEENN方法平衡类别分布。

**结果:** MambaAttention总体表现最佳（F1分数：SAE 1为88%，SAE 2为97%，SAE 3-5为99%），而TabPFN在零样本推理方面表现出色。TabTransformer在检测部分自动化碰撞时表现不佳（F1分数：55%）。

**结论:** 深度学习模型在自动化水平分类中的应用提高了准确性与效率，有助于政策制定和AV安全评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Applying+MambaAttention%2C+TabPFN%2C+and+TabTransformers+to+Classify+SAE+Automation+Levels+in+Crashes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03160&send_immediately=true&force_search=false)

**原文摘要:** The increasing presence of automated vehicles (AVs) presents new challenges
for crash classification and safety analysis. Accurately identifying the SAE
automation level involved in each crash is essential to understanding crash
dynamics and system accountability. However, existing approaches often overlook
automation-specific factors and lack model sophistication to capture
distinctions between different SAE levels. To address this gap, this study
evaluates the performance of three advanced tabular deep learning models
MambaAttention, TabPFN, and TabTransformer for classifying SAE automation
levels using structured crash data from Texas (2024), covering 4,649 cases
categorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level
2), and Advanced Automation (SAE Levels 3-5 combined). Following class
balancing using SMOTEENN, the models were trained and evaluated on a unified
dataset of 7,300 records. MambaAttention demonstrated the highest overall
performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),
while TabPFN excelled in zero-shot inference with high robustness for rare
crash categories. In contrast, TabTransformer underperformed, particularly in
detecting Partial Automation crashes (F1-score: 55%), suggesting challenges in
modeling shared human-system control dynamics. These results highlight the
capability of deep learning models tailored for tabular data to enhance the
accuracy and efficiency of automation-level classification. Integrating such
models into crash analysis frameworks can support policy development, AV safety
evaluation, and regulatory decisions, especially in distinguishing high-risk
conditions for mid- and high-level automation technologies.

</details>


### [6] [Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment](https://arxiv.org/abs/2506.03161)
*Mira Nuthakki*

**主要类别:** cs.LG

**AI概要:** 本研究通过开发一个结合宏观和微观交通动态的3D城市交通仿真环境、一个碰撞模型和一个优先考虑安全的强化学习框架，成功减少了交通事故和碳排放，并优化了交通流量。


<details>
  <summary>更多</summary>
  
**动机:** 为了弥补当前研究的不足，解决全球范围内因交通拥堵和碰撞带来的经济、环境和社会挑战，论文提出了创新的方法来提高交通管理的效果。

**方法:** 论文提出了一种综合性的3D全市范围交通仿真环境，结合了宏观和微观交通动力学；一种碰撞模型；以及一种优先考虑安全而非效率的定制奖励函数强化学习框架。此外，使用Unity游戏引擎进行直接碰撞建模，并采用了PPO（Proximal Policy Optimization）模型。

**结果:** 该模型在基线结果上取得了显著改进，严重碰撞次数、车辆间碰撞次数和总行驶距离均减少了超过三倍，燃油效率提高了39%，碳排放减少了88%。

**结论:** 论文得出结论，开发的Unity游戏引擎模拟环境、碰撞模型和基于定制奖励函数的强化学习框架能够有效减少严重碰撞次数，优化交通流量并降低温室气体排放。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safety-Prioritized%2C+Reinforcement+Learning-Enabled+Traffic+Flow+Optimization+in+a+3D+City-Wide+Simulation+Environment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03161，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03161&send_immediately=true&force_search=false)

**原文摘要:** Traffic congestion and collisions represent significant economic,
environmental, and social challenges worldwide. Traditional traffic management
approaches have shown limited success in addressing these complex, dynamic
problems. To address the current research gaps, three potential tools are
developed: a comprehensive 3D city-wide simulation environment that integrates
both macroscopic and microscopic traffic dynamics; a collision model; and a
reinforcement learning framework with custom reward functions prioritizing
safety over efficiency. Unity game engine-based simulation is used for direct
collision modeling. A custom reward enabled reinforcement learning method,
proximal policy optimization (PPO) model, yields substantial improvements over
baseline results, reducing the number of serious collisions, number of
vehicle-vehicle collisions, and total distance travelled by over 3 times the
baseline values. The model also improves fuel efficiency by 39% and reduces
carbon emissions by 88%. Results establish feasibility for city-wide 3D traffic
simulation applications incorporating the vision-zero safety principles of the
Department of Transportation, including physics-informed, adaptable, realistic
collision modeling, as well as appropriate reward modeling for real-world
traffic signal light control towards reducing collisions, optimizing traffic
flow and reducing greenhouse emissions.

</details>


### [7] [Active Learning via Regression Beyond Realizability](https://arxiv.org/abs/2506.00316)
*Atul Ganju, Shashaank Aiyer, Ved Sriraman, Karthik Sridharan*

**主要类别:** cs.LG

**AI概要:** 提出了一种突破标准可实现性假设的主动学习新方法，在非现实条件下依然表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于代理的主动学习算法依赖于可实现性假设，这在实际应用中往往受限。因此需要一种更广泛的解决方案。

**方法:** 基于代理风险最小化的主动学习方法，并采用周期性算法来拟合完整模型类别的数据。

**结果:** 该方法在弱于可实现性假设的条件下仍能获得与之前工作相当的标签和样本复杂度。

**结论:** 本文提出了一种新的主动学习框架，即使在模型类别不完全符合实际的情况下也能有效工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Learning+via+Regression+Beyond+Realizability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.00316，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.00316&send_immediately=true&force_search=false)

**原文摘要:** We present a new active learning framework for multiclass classification
based on surrogate risk minimization that operates beyond the standard
realizability assumption. Existing surrogate-based active learning algorithms
crucially rely on realizability$\unicode{x2014}$the assumption that the optimal
surrogate predictor lies within the model class$\unicode{x2014}$limiting their
applicability in practical, misspecified settings. In this work we show that
under conditions significantly weaker than realizability, as long as the class
of models considered is convex, one can still obtain a label and sample
complexity comparable to prior work. Despite achieving similar rates, the
algorithmic approaches from prior works can be shown to fail in non-realizable
settings where our assumption is satisfied. Our epoch-based active learning
algorithm departs from prior methods by fitting a model from the full class to
the queried data in each epoch and returning an improper classifier obtained by
aggregating these models.

</details>


### [8] [Causal Discovery in Dynamic Fading Wireless Networks](https://arxiv.org/abs/2506.03163)
*Oluwaseyi Giwa*

**主要类别:** cs.LG

**AI概要:** 本文研究了动态无线网络中的因果推理问题，提出了一种新的顺序回归算法，并分析了影响结构变化检测延迟的关键因素。


<details>
  <summary>更多</summary>
  
**动机:** 由于干扰、衰落和移动性的演变，传统的静态因果模型难以适用，因此需要研究动态因果发现方法以保持网络可靠性。

**方法:** 提出一种顺序回归算法，并通过蒙特卡洛仿真验证了识别结构变化所需的检测延迟的理论上下界。

**结果:** 推导出检测延迟的理论上下界，表明其与网络规模呈线性增加、与噪声方差呈二次增长、与结构变化幅度呈平方反比关系。

**结论:** 该论文提出了一种用于动态衰落无线环境的顺序回归算法，并结合NOTEARS无环约束的新应用，为在非平稳无线条件下设计鲁棒的在线因果推理机制提供了理论见解和实用指南。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Discovery+in+Dynamic+Fading+Wireless+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03163，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03163&send_immediately=true&force_search=false)

**原文摘要:** Dynamic causal discovery in wireless networks is essential due to evolving
interference, fading, and mobility, which complicate traditional static causal
models. This paper addresses causal inference challenges in dynamic fading
wireless environments by proposing a sequential regression-based algorithm with
a novel application of the NOTEARS acyclicity constraint, enabling efficient
online updates. We derive theoretical lower and upper bounds on the detection
delay required to identify structural changes, explicitly quantifying their
dependence on network size, noise variance, and fading severity. Monte Carlo
simulations validate these theoretical results, demonstrating linear increases
in detection delay with network size, quadratic growth with noise variance, and
inverse-square dependence on the magnitude of structural changes. Our findings
provide rigorous theoretical insights and practical guidelines for designing
robust online causal inference mechanisms to maintain network reliability under
nonstationary wireless conditions.

</details>


### [9] [Test-Time Scaling of Diffusion Models via Noise Trajectory Search](https://arxiv.org/abs/2506.03164)
*Vignav Ramesh, Morteza Mardani*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种通过优化噪声轨迹来提高扩散模型生成样本质量的新方法，其性能优于基线方法，并且在任意奖励函数下具有实用性。


<details>
  <summary>更多</summary>
  
**动机:** 扩散模型的迭代和随机特性使其在测试时可以通过增加计算资源来提升生成质量，但现有的主要扩展方式（增加去噪步骤）存在收益递减的问题。因此，研究如何优化噪声轨迹以提升样本质量成为关键挑战。

**方法:** 将扩散过程建模为具有终端奖励的马尔可夫决策过程（MDP），并进一步放松为上下文赌博机问题，从而引入一种ε-贪心搜索算法，在全局探索和局部利用之间取得平衡。

**结果:** 实验表明，该方法在类条件生成和文本到图像任务中表现优异，得分超过基线最多达164%，并且匹配或超过了MCTS的性能。

**结论:** 这是首个能够在测试时有效优化任意奖励函数下的噪声轨迹的实用方法，为扩散模型的进一步发展提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-Time+Scaling+of+Diffusion+Models+via+Noise+Trajectory+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03164，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03164&send_immediately=true&force_search=false)

**原文摘要:** The iterative and stochastic nature of diffusion models enables test-time
scaling, whereby spending additional compute during denoising generates
higher-fidelity samples. Increasing the number of denoising steps is the
primary scaling axis, but this yields quickly diminishing returns. Instead
optimizing the noise trajectory--the sequence of injected noise vectors--is
promising, as the specific noise realizations critically affect sample quality;
but this is challenging due to a high-dimensional search space, complex
noise-outcome interactions, and costly trajectory evaluations. We address this
by first casting diffusion as a Markov Decision Process (MDP) with a terminal
reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to
be meaningful but impractical. To balance performance and efficiency, we then
resort to a relaxation of MDP, where we view denoising as a sequence of
independent contextual bandits. This allows us to introduce an
$\epsilon$-greedy search algorithm that globally explores at extreme timesteps
and locally exploits during the intermediate steps where de-mixing occurs.
Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for
class-conditioned/text-to-image generation, exceeding baselines by up to
$164\%$ and matching/exceeding MCTS performance. To our knowledge, this is the
first practical method for test-time noise trajectory optimization of arbitrary
(non-differentiable) rewards.

</details>


### [10] [Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony](https://arxiv.org/abs/2506.03302)
*James Bagrow, Josh Bongard*

**主要类别:** cs.LG

**AI概要:** 本文介绍 multi-exit KANs，它能在多个深度同时进行预测，提高训练效果并自动找到每个任务的最佳模型复杂度级别。


<details>
  <summary>更多</summary>
  
**动机:** Kolmogorov-Arnold Networks (KANs) 虽然具有高准确性和可解释性，但事先不清楚任何给定任务需要多深的网络，更深的KAN可能难以优化。

**方法:** 引入 multi-exit KAN 架构，并开发了一个可微的“learning to exit”算法。

**结果:** multi-exit KANs 在合成函数、动力系统和真实世界数据集上始终优于标准的单出口版本。

**结论:** multi-exit KANs 提供了一种实用的方法来实现高性能和可解释性，解决了科学发现中机器学习的一个基本挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Exit+Kolmogorov-Arnold+Networks%3A+enhancing+accuracy+and+parsimony，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03302，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03302&send_immediately=true&force_search=false)

**原文摘要:** Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with
interpretability, making them valuable for scientific modeling. However, it is
unclear a priori how deep a network needs to be for any given task, and deeper
KANs can be difficult to optimize. Here we introduce multi-exit KANs, where
each layer includes its own prediction branch, enabling the network to make
accurate predictions at multiple depths simultaneously. This architecture
provides deep supervision that improves training while discovering the right
level of model complexity for each task. Multi-exit KANs consistently
outperform standard, single-exit versions on synthetic functions, dynamical
systems, and real-world datasets. Remarkably, the best predictions often come
from earlier, simpler exits, revealing that these networks naturally identify
smaller, more parsimonious and interpretable models without sacrificing
accuracy. To automate this discovery, we develop a differentiable "learning to
exit" algorithm that balances contributions from exits during training. Our
approach offers scientists a practical way to achieve both high performance and
interpretability, addressing a fundamental challenge in machine learning for
scientific discovery.

</details>


### [11] [Non-collective Calibrating Strategy for Time Series Forecasting](https://arxiv.org/abs/2506.03176)
*Bin Wang, Yongqi Han, Minbo Ma, Tianrui Li, Junbo Zhang, Feng Hong, Yanwei Yu*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种通用的模型校准策略 SoP，能够在不重新训练模型的情况下显著提升时序预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习在时序预测方面取得了显著进展，但设计最优模型架构仍面临挑战。通过通用的校准策略改进现有模型能够以较低资源成本带来显著收益。

**方法:** 提出了一种名为 Socket+Plug (SoP) 的校准策略，该策略为每个预测目标保留独立的优化器和早停监控，并在校准过程中冻结预训练的 Socket 主干网络。

**结果:** 在多个时序基准数据集和气象数据集 ERA5 上的实验表明，SoP 策略可带来最高 22% 的性能提升，即使使用简单的 MLP 作为 Plug 也能取得良好效果。

**结论:** Socket+Plug (SoP) 策略可以有效提升已有时序预测模型的性能，且具有模型无关性，适用于各种架构的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-collective+Calibrating+Strategy+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03176，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03176&send_immediately=true&force_search=false)

**原文摘要:** Deep learning-based approaches have demonstrated significant advancements in
time series forecasting. Despite these ongoing developments, the complex
dynamics of time series make it challenging to establish the rule of thumb for
designing the golden model architecture. In this study, we argue that refining
existing advanced models through a universal calibrating strategy can deliver
substantial benefits with minimal resource costs, as opposed to elaborating and
training a new model from scratch. We first identify a multi-target learning
conflict in the calibrating process, which arises when optimizing variables
across time steps, leading to the underutilization of the model's learning
capabilities. To address this issue, we propose an innovative calibrating
strategy called Socket+Plug (SoP). This approach retains an exclusive optimizer
and early-stopping monitor for each predicted target within each Plug while
keeping the fully trained Socket backbone frozen. The model-agnostic nature of
SoP allows it to directly calibrate the performance of any trained deep
forecasting models, regardless of their specific architectures. Extensive
experiments on various time series benchmarks and a spatio-temporal
meteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up
to a 22% improvement even when employing a simple MLP as the Plug (highlighted
in Figure 1)

</details>


### [12] [Probabilistic Factorial Experimental Design for Combinatorial Interventions](https://arxiv.org/abs/2506.03363)
*Divya Shyamal, Jiaqi Zhang, Caroline Uhler*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的概率因子实验设计方法，用以高效估计处理间的交互效应，并证明了其在单轮和多轮实验中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 组合干预在多个领域有广泛应用，但当处理数量增加时，所有可能组合的实验变得不可行，因此需要一种更有效的实验设计方法。

**方法:** 论文提出了概率因子实验设计，并在被动设置下提供了封闭形式的近似最优解，在多轮设置下提供了可数值优化的近似最优获取函数。

**结果:** 研究表明，对于任何k阶交互模型，每种处理使用0.5的剂量是最优的，且需要O(kp^(3k)ln(p))次观测来准确估计模型。

**结论:** 论文得出了一种近似最优的实验设计方法，用于估计任何k阶交互模型，并提出了一种多轮设置下的近似最优获取函数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probabilistic+Factorial+Experimental+Design+for+Combinatorial+Interventions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03363，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03363&send_immediately=true&force_search=false)

**原文摘要:** A combinatorial intervention, consisting of multiple treatments applied to a
single unit with potentially interactive effects, has substantial applications
in fields such as biomedicine, engineering, and beyond. Given $p$ possible
treatments, conducting all possible $2^p$ combinatorial interventions can be
laborious and quickly becomes infeasible as $p$ increases. Here we introduce
probabilistic factorial experimental design, formalized from how scientists
perform lab experiments. In this framework, the experimenter selects a dosage
for each possible treatment and applies it to a group of units. Each unit
independently receives a random combination of treatments, sampled from a
product Bernoulli distribution determined by the dosages. Additionally, the
experimenter can carry out such experiments over multiple rounds, adapting the
design in an active manner. We address the optimal experimental design problem
within an intervention model that imposes bounded-degree interactions between
treatments. In the passive setting, we provide a closed-form solution for the
near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each
treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating
any $k$-way interaction model, regardless of $k$, and imply that
$O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate
this model. For the multi-round setting, we provide a near-optimal acquisition
function that can be numerically optimized. We also explore several extensions
of the design problem and finally validate our findings through simulations.

</details>


### [13] [Out-of-Vocabulary Sampling Boosts Speculative Decoding](https://arxiv.org/abs/2506.03206)
*Nadav Timor, Jonathan Mamou, Oren Pereg, Hongyang Zhang, David Harel*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的采样方法Redistributing Drafter Kernels (RDK)，它解决了使用较小词汇表的drafters时出现的接受率问题，通过一种新颖的标记亲和先验方法实现了更高的接受率，并提出了一个一阶近似实现来降低计算复杂度。


<details>
  <summary>更多</summary>
  
**动机:** 现有采样方法无法处理词汇表之外的标记，导致drafters的效率受限。

**方法:** 提出了一种名为Redistributing Drafter Kernels (RDK)的方法，并提供了一阶近似实现。

**结果:** 实验表明线性时间RDK在极端剪枝后显著提高了接受率。

**结论:** RDK使得极度精简的drafters变得实用，从而提升了推测解码的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Out-of-Vocabulary+Sampling+Boosts+Speculative+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03206，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03206&send_immediately=true&force_search=false)

**原文摘要:** Speculative decoding relies on fast and accurate drafters. Recent
state-of-the-art language models employ larger and larger vocabularies, which
significantly slows down drafters. One promising approach to boost the
efficiency of speculative decoding is to use drafters with smaller
vocabularies. However, existing sampling methods cannot draw out-of-vocabulary
tokens, creating a tradeoff between drafters' vocabulary size and acceptance
rates. This paper introduces Redistributing Drafter Kernels (RDK), the first
out-of-vocabulary sampler that effectively recovers acceptance rates by
virtually restoring pruned target tokens. RDK leverages token-affinity priors
to reallocate drafter mass towards high-overlap regions. We prove
mathematically that RDK can achieve higher acceptance rates than vanilla and
state-of-the-art samplers. We provide an efficient first-order approximation of
RDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,
enabling lightweight implementations for large vocabularies. Our experiments
demonstrate that this linear-time RDK significantly boosts acceptance rates
even after extreme pruning (removing more than 75% of the drafter's
vocabulary), where existing samplers fail. RDK opens the door to extremely
pruned drafters, which were previously impractical.

</details>


### [14] [Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach](https://arxiv.org/abs/2506.03522)
*Daniel Campa, Mehdi Saeedi, Ian Colbert, Srinjoy Das*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种新的导航路径生成与评估方法，结合了非参数统计技术和copula模型，解决了深度学习生成模型在视频游戏中的复杂训练需求和可解释性问题，实现了精确控制和多样化路径生成。


<details>
  <summary>更多</summary>
  
**动机:** 尽管最近深度学习生成模型取得了显著进展，但视频游戏行业往往因为其复杂的训练需求和可解释性挑战而不愿意采用此类模型进行路径生成。为了解决这些问题，我们提出了这一新方法。

**方法:** 我们的路径生成方法融合了两种统计技术：(1) 非参数无模型变换，通过时间捕捉路径轨迹的统计特征；(2) 捕捉空间统计依赖性的copula模型。对于路径评估，我们采用了一种非参数三样本假设检验，用于判断生成的路径是否过拟合（过于模仿原始数据）或欠拟合（偏离原始数据太远）。

**结果:** 通过实证分析，我们验证了所提方法的精确性和可靠性，并展示了多样化导航路径的可控生成。此外，我们的新颖路径生成器可以微调以创建不同人类相似度水平的路径，相较于基于神经网络的代理生成的路径更具优势。

**结论:** 我们提出了一种基于非参数统计原理的导航路径生成与评估方法，该方法能够提供精确控制和可解释的见解。通过实证分析，我们的方法在两个现有游戏基准上展示了多样化导航路径的可控生成，并且我们的新颖路径生成器可以通过用户可控参数微调，以创建具有不同人类相似度水平的导航路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Path+Generation+and+Evaluation+in+Video+Games%3A+A+Nonparametric+Statistical+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03522，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03522&send_immediately=true&force_search=false)

**原文摘要:** Navigation path traces play a crucial role in video game design, serving as a
vital resource for both enhancing player engagement and fine-tuning
non-playable character behavior. Generating such paths with human-like realism
can enrich the overall gaming experience, and evaluating path traces can
provide game designers insights into player interactions. Despite the
impressive recent advancements in deep learning-based generative modeling, the
video game industry hesitates to adopt such models for path generation, often
citing their complex training requirements and interpretability challenges. To
address these problems, we propose a novel path generation and evaluation
approach that is grounded in principled nonparametric statistics and provides
precise control while offering interpretable insights. Our path generation
method fuses two statistical techniques: (1) nonparametric model-free
transformations that capture statistical characteristics of path traces through
time; and (2) copula models that capture statistical dependencies in space. For
path evaluation, we adapt a nonparametric three-sample hypothesis test designed
to determine if the generated paths are overfit (mimicking the original data
too closely) or underfit (diverging too far from it). We demonstrate the
precision and reliability of our proposed methods with empirical analysis on
two existing gaming benchmarks to showcase controlled generation of diverse
navigation paths. Notably, our novel path generator can be fine-tuned with user
controllable parameters to create navigation paths that exhibit varying levels
of human-likeness in contrast to those produced by neural network-based agents.
The code is available at https://github.com/daniel-campa/mf-copula.

</details>


### [15] [Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning](https://arxiv.org/abs/2506.03207)
*Md Nahid Hasan Shuvo, Moinul Hossain*

**主要类别:** cs.LG

**AI概要:** 本研究发现联邦学习环境中的深度学习模型可以通过网络流量分析被精确地指纹识别，从而揭示了潜在的安全漏洞，并建议在网络层面加强安全防护。


<details>
  <summary>更多</summary>
  
**动机:** 由于联邦学习通过不集中用户数据来保护数据隐私而被广泛采用，但其容易受到通过网络流量分析的间接隐私泄露，这一领域尚未得到研究。因此，本文旨在研究在联邦学习环境中通过分析网络层流量信息来指纹识别深度学习模型的可行性。

**方法:** 该研究通过使用多种深度学习架构（如CNN、RNN）在联邦学习测试平台中进行实验评估，并采用支持向量机（SVM）、随机森林和梯度提升等机器学习算法对流量数据中的独特模式进行指纹识别。

**结果:** 实验结果显示了高指纹识别准确性，使用随机森林达到了100%的准确率，而SVM和梯度提升分类器的准确率约为95.7%。这表明可以在网络流量的子部分中识别特定的深度学习架构。

**结论:** 论文得出结论，联邦学习系统存在显著的安全漏洞，需要在网络层面上加强防护以防止模型指纹识别和针对性攻击。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fingerprinting+Deep+Learning+Models+via+Network+Traffic+Patterns+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03207&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) is increasingly adopted as a decentralized machine
learning paradigm due to its capability to preserve data privacy by training
models without centralizing user data. However, FL is susceptible to indirect
privacy breaches via network traffic analysis-an area not explored in existing
research. The primary objective of this research is to study the feasibility of
fingerprinting deep learning models deployed within FL environments by
analyzing their network-layer traffic information. In this paper, we conduct an
experimental evaluation using various deep learning architectures (i.e., CNN,
RNN) within a federated learning testbed. We utilize machine learning
algorithms, including Support Vector Machines (SVM), Random Forest, and
Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our
experiments show high fingerprinting accuracy, achieving 100% accuracy using
Random Forest and around 95.7% accuracy using SVM and Gradient Boosting
classifiers. This analysis suggests that we can identify specific architectures
running within the subsection of the network traffic. Hence, if an adversary
knows about the underlying DL architecture, they can exploit that information
and conduct targeted attacks. These findings suggest a notable security
vulnerability in FL systems and the necessity of strengthening it at the
network level.

</details>


### [16] [Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner](https://arxiv.org/abs/2506.03595)
*Runa Eschenhagen, Aaron Defazio, Tsung-Hsien Lee, Richard E. Turner, Hao-Jun Michael Shi*

**主要类别:** cs.LG

**AI概要:** 本文研究了基于Kronecker分解的优化算法Shampoo中的启发式方法，并提出了改进方案。


<details>
  <summary>更多</summary>
  
**动机:** Shampoo虽然在训练神经网络中表现出色，但依赖于一些缺乏理论支持的启发式方法，增加了算法复杂性和调参难度。

**方法:** 从Frobenius范数近似和矩阵Adam的角度分析Shampoo中的启发式方法，解耦预条件矩阵的特征值和特征基更新，并提出了一种自适应调整特征基计算频率的方法。

**结果:** 证明了Adam的嫁接能够缓解预条件矩阵特征值的陈旧性和错误缩放问题；通过直接修正特征值可以消除对学习率嫁接的需求。此外，提出的自适应标准有助于减少因稀疏特征基计算带来的误差影响。

**结论:** 这些技术为去除Shampoo中的启发式方法提供了理论依据，并有助于开发更优的Kronecker分解训练算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Purifying+Shampoo%3A+Investigating+Shampoo%27s+Heuristics+by+Decomposing+its+Preconditioner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03595&send_immediately=true&force_search=false)

**原文摘要:** The recent success of Shampoo in the AlgoPerf contest has sparked renewed
interest in Kronecker-factorization-based optimization algorithms for training
neural networks. Despite its success, Shampoo relies heavily on several
heuristics such as learning rate grafting and stale preconditioning to achieve
performance at-scale. These heuristics increase algorithmic complexity,
necessitate further hyperparameter tuning, and lack theoretical justification.
This paper investigates these heuristics from the angle of Frobenius norm
approximation to full-matrix Adam and decouples the preconditioner's
eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates
the staleness and mis-scaling of the preconditioner's eigenvalues and how
correcting the eigenvalues directly can eliminate the need for learning rate
grafting. To manage the error induced by infrequent eigenbasis computations, we
propose an adaptive criterion for determining the eigenbasis computation
frequency motivated by terminating a warm-started QR algorithm. This criterion
decouples the update frequency of different preconditioner matrices and enables
us to investigate the impact of approximation error on convergence. These
practical techniques offer a principled angle towards removing Shampoo's
heuristics and developing improved Kronecker-factorization-based training
algorithms.

</details>


### [17] [FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution](https://arxiv.org/abs/2506.03210)
*Qiusheng Huang, Yuan Niu, Xiaohui Zhong, Anboyu Guo, Lei Chen, Dianjun Zhang, Xuefeng Zhang, Hao Li*

**主要类别:** cs.LG

**AI概要:** FuXi-Ocean是一种创新性的数据驱动模型，实现了高效的次日海洋预测并解决了误差累积问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统数值模型计算成本高且难以在精细时空尺度上保持准确性；现有数据驱动方法通常只能提供日分辨率预测并在次日预测中表现不佳。

**方法:** 通过结合上下文感知特征提取模块和使用堆叠注意力块的预测网络，并采用Mixture-of-Time (MoT) 模块进行多时间上下文预测集成。

**结果:** 实验表明，FuXi-Ocean在多个深度的关键变量（如温度、盐度和洋流）预测方面表现出色。

**结论:** FuXi-Ocean是一个创新的数据驱动全球海洋预测模型，能够实现每六小时一次的高分辨率预测，其引入的MoT模块能有效减少序列预测中的累积误差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FuXi-Ocean%3A+A+Global+Ocean+Forecasting+System+with+Sub-Daily+Resolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03210&send_immediately=true&force_search=false)

**原文摘要:** Accurate, high-resolution ocean forecasting is crucial for maritime
operations and environmental monitoring. While traditional numerical models are
capable of producing sub-daily, eddy-resolving forecasts, they are
computationally intensive and face challenges in maintaining accuracy at fine
spatial and temporal scales. In contrast, recent data-driven approaches offer
improved computational efficiency and emerging potential, yet typically operate
at daily resolution and struggle with sub-daily predictions due to error
accumulation over time. We introduce FuXi-Ocean, the first data-driven global
ocean forecasting model achieving six-hourly predictions at eddy-resolving
1/12{\deg} spatial resolution, reaching depths of up to 1500 meters. The model
architecture integrates a context-aware feature extraction module with a
predictive network employing stacked attention blocks. The core innovation is
the Mixture-of-Time (MoT) module, which adaptively integrates predictions from
multiple temporal contexts by learning variable-specific reliability ,
mitigating cumulative errors in sequential forecasting. Through comprehensive
experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting
key variables, including temperature, salinity, and currents, across multiple
depths.

</details>


### [18] [On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://arxiv.org/abs/2506.03719)
*Quentin Bertrand, Anne Gagneux, Mathurin Massias, Rémi Emonet*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了流匹配模型泛化能力的原因，发现损失函数的随机性不是主要贡献因素，而闭合形式方法有时能提高性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了理解现代深度生成模型为何能够产生高质量的合成样本以及为何流匹配技术能够有效泛化，需要排除损失函数随机性对其泛化能力影响的假设。

**方法:** 通过在高维设置下对流匹配损失的随机版本和闭合形式版本进行实证比较，并使用最先进的流匹配模型在标准图像数据集上测试两者的统计性能。

**结果:** 实验表明，无论是在损失值还是统计性能方面，流匹配损失的随机版本和闭合形式版本表现几乎相同，甚至闭合形式有时可以提升性能。

**结论:** 论文得出结论，流匹配中损失函数的随机性并不是其泛化能力的主要贡献因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Closed-Form+of+Flow+Matching%3A+Generalization+Does+Not+Arise+from+Target+Stochasticity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03719，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03719&send_immediately=true&force_search=false)

**原文摘要:** Modern deep generative models can now produce high-quality synthetic samples
that are often indistinguishable from real training data. A growing body of
research aims to understand why recent methods -- such as diffusion and flow
matching techniques -- generalize so effectively. Among the proposed
explanations are the inductive biases of deep learning architectures and the
stochastic nature of the conditional flow matching loss. In this work, we rule
out the latter -- the noisy nature of the loss -- as a primary contributor to
generalization in flow matching. First, we empirically show that in
high-dimensional settings, the stochastic and closed-form versions of the flow
matching loss yield nearly equivalent losses. Then, using state-of-the-art flow
matching models on standard image datasets, we demonstrate that both variants
achieve comparable statistical performance, with the surprising observation
that using the closed-form can even improve performance.

</details>


### [19] [Multiple-Frequencies Population-Based Training](https://arxiv.org/abs/2506.03225)
*Waël Doulazmi, Auguste Lehuger, Marin Toromanoff, Valentin Charraut, Thibault Buhet, Fabien Moutarde*

**主要类别:** cs.LG

**AI概要:** 本文提出了MF-PBT，一种改进的超参数优化算法，通过使用不同进化频率的子种群和迁移过程，解决了PBT算法中的贪婪问题，提高了样本效率和长期性能。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习对超参数的高度敏感性导致了不稳定性和低效性，而现有的超参数优化算法如PBT在长期时间尺度上表现不佳，容易陷入局部最优。

**方法:** 提出了一种名为MF-PBT的新颖HPO算法，该算法通过采用具有不同进化频率的子种群以及不对称设计的迁移过程来解决贪婪问题。

**结果:** 在Brax套件上的广泛实验表明，即使不实际调整超参数，MF-PBT也能提高样本效率和长期性能。

**结论:** MF-PBT算法有效地解决了PBT算法中由于中间选择步骤导致的贪婪问题，为超参数优化提供了一个新的有效方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiple-Frequencies+Population-Based+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03225&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning's high sensitivity to hyperparameters is a source of
instability and inefficiency, creating significant challenges for
practitioners. Hyperparameter Optimization (HPO) algorithms have been developed
to address this issue, among them Population-Based Training (PBT) stands out
for its ability to generate hyperparameters schedules instead of fixed
configurations. PBT trains a population of agents, each with its own
hyperparameters, frequently ranking them and replacing the worst performers
with mutations of the best agents. These intermediate selection steps can cause
PBT to focus on short-term improvements, leading it to get stuck in local
optima and eventually fall behind vanilla Random Search over longer timescales.
This paper studies how this greediness issue is connected to the choice of
evolution frequency, the rate at which the selection is done. We propose
Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm
that addresses greediness by employing sub-populations, each evolving at
distinct frequencies. MF-PBT introduces a migration process to transfer
information between sub-populations, with an asymmetric design to balance short
and long-term optimization. Extensive experiments on the Brax suite demonstrate
that MF-PBT improves sample efficiency and long-term performance, even without
actually tuning hyperparameters.

</details>


### [20] [When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective](https://arxiv.org/abs/2506.03784)
*Beatrix M. G. Nielsen, Emanuele Marconato, Andrea Dittadi, Luigi Gresele*

**主要类别:** cs.LG

**AI概要:** 该论文研究了不同深度神经网络学习到的表示的相似性问题，指出分布接近的小KL散度并不能保证表示的相似性，并提出了一个分布距离定义，使得分布接近意味着表示相似。


<details>
  <summary>更多</summary>
  
**动机:** 论文旨在探讨不同深度神经网络学习到的表示何时以及为何会相似，这是当前研究的一个热点问题。

**方法:** 论文从可识别性理论的角度出发，研究了生成分布接近的模型是否具有相似的表示，并通过实验证明了这一观点。

**结果:** 论文证明了小KL散度不能保证表示的相似性，并在合成实验中发现更宽的网络在分布上更接近且具有更相似的表示。

**结论:** 论文得出结论，分布接近的小KL散度并不能保证相应表示的相似性。这表明即使模型接近最大化似然，它们仍可能学习到不相似的表示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Does+Closeness+in+Distribution+Imply+Representational+Similarity%3F+An+Identifiability+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03784&send_immediately=true&force_search=false)

**原文摘要:** When and why representations learned by different deep neural networks are
similar is an active research topic. We choose to address these questions from
the perspective of identifiability theory, which suggests that a measure of
representational similarity should be invariant to transformations that leave
the model distribution unchanged. Focusing on a model family which includes
several popular pre-training approaches, e.g., autoregressive language models,
we explore when models which generate distributions that are close have similar
representations. We prove that a small Kullback-Leibler divergence between the
model distributions does not guarantee that the corresponding representations
are similar. This has the important corollary that models arbitrarily close to
maximizing the likelihood can still learn dissimilar representations, a
phenomenon mirrored in our empirical observations on models trained on
CIFAR-10. We then define a distributional distance for which closeness implies
representational similarity, and in synthetic experiments, we find that wider
networks learn distributions which are closer with respect to our distance and
have more similar representations. Our results establish a link between
closeness in distribution and representational similarity.

</details>


### [21] [Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification](https://arxiv.org/abs/2506.03227)
*Abdelrahman Sayed Sayed, Pierre-Jean Meyer, Mohamed Ghazel*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一个误差界限，用于描述神经ODE和ResNet之间的近似程度，并展示了如何将一个模型用作另一个模型的安全性验证代理。


<details>
  <summary>更多</summary>
  
**动机:** 神经ODE和ResNet之间存在强相关性，但需要一种方法在不增加验证成本的情况下利用这种关系。

**方法:** 论文通过建立两个相关模型之间的近似误差界限，来展示神经ODE和ResNet之间的更正式的关系。

**结果:** 论文提出的误差界限方法允许在一个模型上进行安全性验证，并保证其在另一个模型上的适用性。

**结论:** 论文得出结论，通过建立误差界限，可以在不运行验证工具两次的情况下，将一个模型用作另一个模型的验证代理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Neural+ODE+and+ResNet%3A+A+Formal+Error+Bound+for+Safety+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03227，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03227&send_immediately=true&force_search=false)

**原文摘要:** A neural ordinary differential equation (neural ODE) is a machine learning
model that is commonly described as a continuous depth generalization of a
residual network (ResNet) with a single residual block, or conversely, the
ResNet can be seen as the Euler discretization of the neural ODE. These two
models are therefore strongly related in a way that the behaviors of either
model are considered to be an approximation of the behaviors of the other. In
this work, we establish a more formal relationship between these two models by
bounding the approximation error between two such related models. The obtained
error bound then allows us to use one of the models as a verification proxy for
the other, without running the verification tools twice: if the reachable
output set expanded by the error bound satisfies a safety property on one of
the models, this safety property is then guaranteed to be also satisfied on the
other model. This feature is fully reversible, and the initial safety
verification can be run indifferently on either of the two models. This novel
approach is illustrated on a numerical example of a fixed-point attractor
system modeled as a neural ODE.

</details>


### [22] [Revisiting Unbiased Implicit Variational Inference](https://arxiv.org/abs/2506.03839)
*Tobias Pielok, Bernd Bischl, David Rügamer*

**主要类别:** cs.LG

**AI概要:** 本文研究了半隐式变分推理（SIVI）方法，并改进了无偏隐式变分推理（UIVI）方法，使其在性能上优于或等同于当前最先进的SIVI技术。


<details>
  <summary>更多</summary>
  
**动机:** 由于当前的研究集中在寻找有效的SIVI训练程序，而UIVI因精度问题和计算成本高而被忽视，因此需要重新审视UIVI并找到改进其性能的方法。

**方法:** 该论文提出通过重要性采样替代UIVI中的MCMC循环，并通过最小化预期前向Kullback-Leibler散度来稳定学习最佳提议分布。

**结果:** 论文的结果表明，经过改进的UIVI方法在现有SIVI基准测试中展示了优越的或相当的性能。

**结论:** 论文得出结论，改进后的UIVI方法在性能上优于或等同于现有的SIVI技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Unbiased+Implicit+Variational+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03839，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03839&send_immediately=true&force_search=false)

**原文摘要:** Recent years have witnessed growing interest in semi-implicit variational
inference (SIVI) methods due to their ability to rapidly generate samples from
complex distributions. However, since the likelihood of these samples is
non-trivial to estimate in high dimensions, current research focuses on finding
effective SIVI training routines. Although unbiased implicit variational
inference (UIVI) has largely been dismissed as imprecise and computationally
prohibitive because of its inner MCMC loop, we revisit this method and show
that UIVI's MCMC loop can be effectively replaced via importance sampling and
the optimal proposal distribution can be learned stably by minimizing an
expected forward Kullback-Leibler divergence without bias. Our refined approach
demonstrates superior performance or parity with state-of-the-art methods on
established SIVI benchmarks.

</details>


### [23] [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
*Selcuk Gurses, Aozhong Zhang, Yanxia Deng, Xun Dong, Xin Li, Naigang Wang, Penghang Yin, Zi Yang*

**主要类别:** cs.LG

**AI概要:** DiaBlo 是一种有效的参数高效微调方法，通过仅更新选定模型权重矩阵的对角块，在保持高内存效率和快速微调速度的同时实现稳定收敛。


<details>
  <summary>更多</summary>
  
**动机:** 为了减少全模型微调带来的计算和内存成本，现有的参数高效微调（PEFT）方法只更新一小部分模型参数，但它们与全模型微调之间仍存在性能差距。

**方法:** DiaBlo 选择性地更新模型权重矩阵中的对角块，而不需要低秩矩阵乘法，从而避免依赖辅助初始化方案或定制优化策略。

**结果:** 在多个任务（包括常识推理、算术推理、代码生成和安全对齐）的广泛实验表明，DiaBlo 表现出色且稳定的性能，同时保持了较高的内存效率和快速的微调速度。

**结论:** DiaBlo 提供了一种简单而有效的方式来改进 PEFT 方法，实现了与 LoRA 相当的内存效率和训练速度，并具有更好的收敛稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiaBlo%3A+Diagonal+Blocks+Are+Sufficient+For+Finetuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03230，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03230&send_immediately=true&force_search=false)

**原文摘要:** Finetuning is a critical step for adapting large language models (LLMs) to
domain-specific downstream tasks. To mitigate the substantial computational and
memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)
methods have been proposed to update only a small subset of model parameters.
However, performance gaps between PEFT approaches and full-model fine-tuning
still exist. In this work, we present DiaBlo, a simple yet effective PEFT
approach that updates only the diagonal blocks of selected model weight
matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates
the need for low rank matrix products, thereby avoiding the reliance on
auxiliary initialization schemes or customized optimization strategies to
improve convergence. This design leads to stable and robust convergence while
maintaining comparable memory efficiency and training speed to LoRA. We conduct
extensive experiments across a range of tasks, including commonsense reasoning,
arithmetic reasoning, code generation, and safety alignment, to evaluate the
effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo
demonstrates strong and consistent performance while maintaining high memory
efficiency and fast finetuning speed. Codes are available at
https://github.com/ziyangjoy/DiaBlo.

</details>


### [24] [A kernel conditional two-sample test](https://arxiv.org/abs/2506.03898)
*Pierre-François Massiani, Christian Fiedler, Lukas Haverbeck, Friedrich Solowjow, Sebastian Trimpe*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于学习方法置信界的条件两样本检验新框架，特别适用于非独立数据，并通过核岭回归和核均值嵌入展示了其理论与实践效果。


<details>
  <summary>更多</summary>
  
**动机:** 为了识别两个条件期望值在高概率下存在差异的输入变量，需要发展一种新的假设检验框架。

**方法:** 通过将学习方法的置信界转化为条件两样本检验，并具体应用于核岭回归（KRR）和条件核均值嵌入。

**结果:** 开发了适用于无限维输出和非迹类核的新置信界，并提出了利用在线抽样的统计检验方法及避免调整不可达参数的引导方案。

**结论:** 该论文为条件两样本检验提供了理论基础和实际应用方法，尤其在数据顺序到达或非独立的情况下非常有用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+kernel+conditional+two-sample+test，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03898&send_immediately=true&force_search=false)

**原文摘要:** We propose a framework for hypothesis testing on conditional probability
distributions, which we then use to construct conditional two-sample
statistical tests. These tests identify the inputs -- called covariates in this
context -- where two conditional expectations differ with high probability. Our
key idea is to transform confidence bounds of a learning method into a
conditional two-sample test, and we instantiate this principle for kernel ridge
regression (KRR) and conditional kernel mean embeddings. We generalize existing
pointwise-in-time or time-uniform confidence bounds for KRR to
previously-inaccessible yet essential cases such as infinite-dimensional
outputs with non-trace-class kernels. These bounds enable circumventing the
need for independent data in our statistical tests, since they allow online
sampling. We also introduce bootstrapping schemes leveraging the parametric
form of testing thresholds identified in theory to avoid tuning inaccessible
parameters, making our method readily applicable in practice. Such conditional
two-sample tests are especially relevant in applications where data arrive
sequentially or non-independently, or when output distributions vary with
operational parameters. We demonstrate their utility through examples in
process monitoring and comparison of dynamical systems. Overall, our results
establish a comprehensive foundation for conditional two-sample testing, from
theoretical guarantees to practical implementation, and advance the
state-of-the-art on the concentration of vector-valued least squares
estimation.

</details>


### [25] [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/abs/2506.03234)
*Kaiwen Duan, Hongwei Yao, Yufei Chen, Ziyun Li, Tong Qiao, Zhan Qin, Cong Wang*

**主要类别:** cs.LG

**AI概要:** 该论文介绍了一种名为BadReward的新攻击方式，它能通过污染少量自然外观的偏好数据来劫持文本到图像模型的生成结果。


<details>
  <summary>更多</summary>
  
**动机:** 文本到图像模型与人类偏好的对齐使得反馈机制为对手提供了新的途径，这促使了隐蔽且实用的攻击方法的发展。

**方法:** 提出了BadReward，这是一种针对多模态RLHF中的奖励模型的隐蔽干净标签中毒攻击。

**结果:** 实验显示，BadReward能够持续引导生成不当输出，如针对特定概念的偏见或暴力图像。

**结论:** BadReward攻击方法的存在表明了多模态系统中RLHF的增强威胁，强调需要强大的防御措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BadReward%3A+Clean-Label+Poisoning+of+Reward+Models+in+Text-to-Image+RLHF，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03234，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03234&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.

</details>


### [26] [Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study](https://arxiv.org/abs/2506.03931)
*Yotam Alexander, Yonatan Slutzky, Yuval Ran-Milo, Nadav Cohen*

**主要类别:** cs.LG

**AI概要:** 论文探讨了梯度下降是否是神经网络实现良好泛化所必需的方法，发现对于矩阵分解模型，Guess & Check方法在宽网络中效果变差但在深网络中效果更好。


<details>
  <summary>更多</summary>
  
**动机:** 现有的体积假设挑战了传统观点，认为即使将梯度下降替换为Guess & Check（G&C），过参数化的神经网络仍具有良好的泛化能力。然而该假设在深层和宽层网络中是否成立仍然是一个开放性问题。

**方法:** 本文通过理论研究和实验证明了Guess & Check方法在不同宽度和深度的矩阵分解模型中的泛化能力变化。

**结果:** 作者理论上证明了对于线性和非线性激活函数的矩阵分解模型，随着宽度增加，Guess & Check的泛化能力下降；而随着深度增加，其泛化能力提升，并通过实验验证了这一现象。

**结论:** 论文得出结论，即使在简单的设定下，关于神经网络是否需要梯度下降以良好泛化的问题，可能并不存在简单的答案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+Neural+Networks+Need+Gradient+Descent+to+Generalize%3F+A+Theoretical+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03931，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03931&send_immediately=true&force_search=false)

**原文摘要:** Conventional wisdom attributes the mysterious generalization abilities of
overparameterized neural networks to gradient descent (and its variants). The
recent volume hypothesis challenges this view: it posits that these
generalization abilities persist even when gradient descent is replaced by
Guess & Check (G&C), i.e., by drawing weight settings until one that fits the
training data is found. The validity of the volume hypothesis for wide and deep
neural networks remains an open question. In this paper, we theoretically
investigate this question for matrix factorization (with linear and non-linear
activation)--a common testbed in neural network theory. We first prove that
generalization under G&C deteriorates with increasing width, establishing what
is, to our knowledge, the first case where G&C is provably inferior to gradient
descent. Conversely, we prove that generalization under G&C improves with
increasing depth, revealing a stark contrast between wide and deep networks,
which we further validate empirically. These findings suggest that even in
simple settings, there may not be a simple answer to the question of whether
neural networks need gradient descent to generalize well.

</details>


### [27] [On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models](https://arxiv.org/abs/2506.03267)
*Shahbaz Rezaei, Avishai Halev, Xin Liu*

**主要类别:** cs.LG

**AI概要:** 这篇论文讨论了时间序列模型解释的重要性，并指出单一时间域的解释存在局限性，提出了引入多域解释的新范式。


<details>
  <summary>更多</summary>
  
**动机:** 为了更全面地解释时间序列模型，需要考虑在时间和频率域中呈现XAI归因，因为某些情况下它们突出显示根本不同的特征。

**方法:** 通过引入不确定性原理（UP）来评估时间域和频率域的解释是否违反该界限，从而判断它们是否强调不同的特征。

**结果:** 验证了在各种深度学习模型、XAI方法以及分类和预测数据集中频繁出现UP违规现象。

**结论:** 论文得出结论，现有的仅关注时间域解释的方法存在局限性，需要引入多域解释作为新的范式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Necessity+of+Multi-Domain+Explanation%3A+An+Uncertainty+Principle+Approach+for+Deep+Time+Series+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03267&send_immediately=true&force_search=false)

**原文摘要:** A prevailing approach to explain time series models is to generate
attribution in time domain. A recent development in time series XAI is the
concept of explanation spaces, where any model trained in the time domain can
be interpreted with any existing XAI method in alternative domains, such as
frequency. The prevailing approach is to present XAI attributions either in the
time domain or in the domain where the attribution is most sparse. In this
paper, we demonstrate that in certain cases, XAI methods can generate
attributions that highlight fundamentally different features in the time and
frequency domains that are not direct counterparts of one another. This
suggests that both domains' attributions should be presented to achieve a more
comprehensive interpretation. Thus it shows the necessity of multi-domain
explanation. To quantify when such cases arise, we introduce the uncertainty
principle (UP), originally developed in quantum mechanics and later studied in
harmonic analysis and signal processing, to the XAI literature. This principle
establishes a lower bound on how much a signal can be simultaneously localized
in both the time and frequency domains. By leveraging this concept, we assess
whether attributions in the time and frequency domains violate this bound,
indicating that they emphasize distinct features. In other words, UP provides a
sufficient condition that the time and frequency domain explanations do not
match and, hence, should be both presented to the end user. We validate the
effectiveness of this approach across various deep learning models, XAI
methods, and a wide range of classification and forecasting datasets. The
frequent occurrence of UP violations across various datasets and XAI methods
highlights the limitations of existing approaches that focus solely on
time-domain explanations. This underscores the need for multi-domain
explanations as a new paradigm.

</details>


### [28] [Lower Ricci Curvature for Hypergraphs](https://arxiv.org/abs/2506.03943)
*Shiyi Yang, Can Chen, Didong Li*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新颖的超图低里奇曲率（HLRC）度量方法，该方法结合了几何敏感性和算法简单性，可用于分析生物、社会和信息系统中的复杂高阶交互网络。


<details>
  <summary>更多</summary>
  
**动机:** 现有的超图几何表征方法存在关键权衡问题，需要一种新的曲率度量来克服这些限制。

**方法:** 引入了闭式定义的超图低里奇曲率（HLRC），平衡了解释性和效率之间的关系。

**结果:** 在多种合成和真实世界超图数据集中，HLRC揭示了有意义的高阶组织结构，并支持基于全局结构的鲁棒聚类。

**结论:** HLRC为超图分析提供了多功能的基础，对节点分类、异常检测和生成建模等任务具有广泛意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lower+Ricci+Curvature+for+Hypergraphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03943，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03943&send_immediately=true&force_search=false)

**原文摘要:** Networks with higher-order interactions, prevalent in biological, social, and
information systems, are naturally represented as hypergraphs, yet their
structural complexity poses fundamental challenges for geometric
characterization. While curvature-based methods offer powerful insights in
graph analysis, existing extensions to hypergraphs suffer from critical
trade-offs: combinatorial approaches such as Forman-Ricci curvature capture
only coarse features, whereas geometric methods like Ollivier-Ricci curvature
offer richer expressivity but demand costly optimal transport computations. To
address these challenges, we introduce hypergraph lower Ricci curvature (HLRC),
a novel curvature metric defined in closed form that achieves a principled
balance between interpretability and efficiency. Evaluated across diverse
synthetic and real-world hypergraph datasets, HLRC consistently reveals
meaningful higher-order organization, distinguishing intra- from
inter-community hyperedges, uncovering latent semantic labels, tracking
temporal dynamics, and supporting robust clustering of hypergraphs based on
global structure. By unifying geometric sensitivity with algorithmic
simplicity, HLRC provides a versatile foundation for hypergraph analytics, with
broad implications for tasks including node classification, anomaly detection,
and generative modeling in complex systems.

</details>


### [29] [Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach](https://arxiv.org/abs/2506.03979)
*Haoxuan Chen, Yinuo Ren, Martin Renqiang Min, Lexing Ying, Zachary Izzo*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种用于贝叶斯逆问题的新后验采样方法，结合了扩散模型和序列蒙特卡洛方法的优点，避免了传统方法中的启发式近似，并在理论和实验上都展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于扩散模型的后验采样方法依赖于对生成过程的启发式近似，为了充分利用扩散模型的生成能力并避免这些近似，作者提出了新的后验采样方法。

**方法:** 作者通过研究先验在预训练得分函数编码的扩散过程中的演化，推导出了一个包含修正扩散项和加权项的偏微分方程（PDE），并利用随机加权粒子方法对该方程进行模拟。

**结果:** 理论上，作者证明了真实后验分布之间的误差可以由预训练得分函数的训练误差和集成中粒子数量来限定；实证上，作者在多个成像逆问题上验证了所提方法的优越性能。

**结论:** 本文提出了一种基于集成的后验采样算法，该算法无需使用启发式近似即可求解贝叶斯逆问题，并通过理论分析和实验验证了其优于现有基于扩散模型的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Solving+Inverse+Problems+via+Diffusion-Based+Priors%3A+An+Approximation-Free+Ensemble+Sampling+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03979，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03979&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models (DMs) have proven to be effective in modeling
high-dimensional distributions, leading to their widespread adoption for
representing complex priors in Bayesian inverse problems (BIPs). However,
current DM-based posterior sampling methods proposed for solving common BIPs
rely on heuristic approximations to the generative process. To exploit the
generative capability of DMs and avoid the usage of such approximations, we
propose an ensemble-based algorithm that performs posterior sampling without
the use of heuristic approximations. Our algorithm is motivated by existing
works that combine DM-based methods with the sequential Monte Carlo (SMC)
method. By examining how the prior evolves through the diffusion process
encoded by the pre-trained score function, we derive a modified partial
differential equation (PDE) governing the evolution of the corresponding
posterior distribution. This PDE includes a modified diffusion term and a
reweighting term, which can be simulated via stochastic weighted particle
methods. Theoretically, we prove that the error between the true posterior
distribution can be bounded in terms of the training error of the pre-trained
score function and the number of particles in the ensemble. Empirically, we
validate our algorithm on several inverse problems in imaging to show that our
method gives more accurate reconstructions compared to existing DM-based
methods.

</details>


### [30] [Budgeted Online Active Learning with Expert Advice and Episodic Priors](https://arxiv.org/abs/2506.03307)
*Kristen Goebel, William Solow, Paola Pesantez-Cabrera, Markus Keller, Alan Fern*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种创新的在线主动学习方法，适用于预算有限、时间范围有限的农业数据流场景，并证明其优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 农业应用中需要从有限的数据流中进行高效学习，但标签获取成本高昂，因此需要一种新的主动学习方法。

**方法:** 整合专家预测器和基于未标记数据流的片段知识，同时考虑查询预算和有限时间范围。

**结果:** 实验表明，即使在高度受限的标签预算下，该方法也显著优于基线专家预测和现有方法。

**结论:** 本文提出了一种在有限预算和有限时间范围内进行在线主动学习的新方法，并通过实验验证了其优于现有方法的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Budgeted+Online+Active+Learning+with+Expert+Advice+and+Episodic+Priors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03307&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces a novel approach to budgeted online active learning
from finite-horizon data streams with extremely limited labeling budgets. In
agricultural applications, such streams might include daily weather data over a
growing season, and labels require costly measurements of weather-dependent
plant characteristics. Our method integrates two key sources of prior
information: a collection of preexisting expert predictors and episodic
behavioral knowledge of the experts based on unlabeled data streams. Unlike
previous research on online active learning with experts, our work
simultaneously considers query budgets, finite horizons, and episodic
knowledge, enabling effective learning in applications with severely limited
labeling capacity. We demonstrate the utility of our approach through
experiments on various prediction problems derived from both a realistic
agricultural crop simulator and real-world data from multiple grape cultivars.
The results show that our method significantly outperforms baseline expert
predictions, uniform query selection, and existing approaches that consider
budgets and limited horizons but neglect episodic knowledge, even under highly
constrained labeling budgets.

</details>


### [31] [Guided Speculative Inference for Efficient Test-Time Alignment of LLMs](https://arxiv.org/abs/2506.04118)
*Jonathan Geuter, Youssef Mroueh, David Alvarez-Melis*

**主要类别:** cs.LG

**AI概要:** This paper proposes Guided Speculative Inference (GSI), a novel algorithm for efficient reward-guided decoding in large language models, achieving higher accuracy than existing methods.


<details>
  <summary>更多</summary>
  
**动机:** The motivation of this paper is to develop an efficient reward-guided decoding method for large language models which can achieve high accuracy.

**方法:** GSI combines soft best-of-n test-time scaling with a reward model r(x,y) and speculative samples from a small auxiliary model pi_S(y|x).

**结果:** The authors derive a theoretical bound on the KL divergence between their induced distribution and the optimal policy. They also demonstrate experimentally that their method achieves higher accuracy than standard soft best-of-n with pi_S and reward-guided speculative decoding.

**结论:** GSI is a new algorithm for decoding in large language models that achieves higher accuracy than existing methods, and sometimes even outperforms soft best-of-n with the primary model.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guided+Speculative+Inference+for+Efficient+Test-Time+Alignment+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04118，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04118&send_immediately=true&force_search=false)

**原文摘要:** We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .

</details>


### [32] [The Future of Continual Learning in the Era of Foundation Models: Three Key Directions](https://arxiv.org/abs/2506.03320)
*Jack Bell, Luigi Quarantiello, Eric Nuertey Coleman, Lanpei Li, Malio Li, Mauro Madeddu, Elia Piccoli, Vincenzo Lomonaco*

**主要类别:** cs.LG

**AI概要:** 本文讨论了持续学习在大型语言模型和基础模型时代的重要性，并指出持续组合性是未来人工智能发展的关键。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型和基础模型的发展，作者探讨了持续学习是否仍然必要，并阐述了持续学习在预训练、微调以及组合性方面的重要性。

**方法:** 论文通过回顾人工智能不同范式对持续学习的历史需求，并分析当前大型语言模型和基础模型的能力与限制来达到其结论。

**结果:** 论文展示了三个关键原因说明持续学习的重要性：(i) 持续预训练确保基础模型保持最新状态；(ii) 持续微调使模型能够专业化和个性化；(iii) 持续组合性提供可扩展且模块化的智能方法。

**结论:** 论文得出结论，尽管大型语言模型和基础模型的出现改变了连续学习的需求，但持续学习仍然至关重要。特别是持续组合性将标志着持续学习的重生。未来的AI将由不断进化和相互作用的模型生态系统定义，使得持续学习比以往任何时候都更加重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Future+of+Continual+Learning+in+the+Era+of+Foundation+Models%3A+Three+Key+Directions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03320&send_immediately=true&force_search=false)

**原文摘要:** Continual learning--the ability to acquire, retain, and refine knowledge over
time--has always been fundamental to intelligence, both human and artificial.
Historically, different AI paradigms have acknowledged this need, albeit with
varying priorities: early expert and production systems focused on incremental
knowledge consolidation, while reinforcement learning emphasised dynamic
adaptation. With the rise of deep learning, deep continual learning has
primarily focused on learning robust and reusable representations over time to
solve sequences of increasingly complex tasks. However, the emergence of Large
Language Models (LLMs) and foundation models has raised the question: Do we
still need continual learning when centralised, monolithic models can tackle
diverse tasks with access to internet-scale knowledge? We argue that continual
learning remains essential for three key reasons: (i) continual pre-training is
still necessary to ensure foundation models remain up to date, mitigating
knowledge staleness and distribution shifts while integrating new information;
(ii) continual fine-tuning enables models to specialise and personalise,
adapting to domain-specific tasks, user preferences, and real-world constraints
without full retraining, avoiding the need for computationally expensive long
context-windows; (iii) continual compositionality offers a scalable and modular
approach to intelligence, enabling the orchestration of foundation models and
agents to be dynamically composed, recombined, and adapted. While continual
pre-training and fine-tuning are explored as niche research directions, we
argue it is continual compositionality that will mark the rebirth of continual
learning. The future of AI will not be defined by a single static model but by
an ecosystem of continually evolving and interacting models, making continual
learning more relevant than ever.

</details>


### [33] [N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion](https://arxiv.org/abs/2506.04166)
*Caleb Chin, Aashish Khubchandani, Harshvardhan Maskara, Kyuseong Choi, Jacob Feitelberg, Albert Gong, Manit Paul, Tathagata Sadhukhan, Anish Agarwal, Raaz Dwivedi*

**主要类别:** cs.LG

**AI概要:** 论文介绍了N$^2$框架及其新NN变体，在真实数据中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** NN方法在矩阵补全中重新兴起，具有强大的经验性能和理论保证，但需要一个统一框架来促进研究和应用。

**方法:** 引入了一个统一的Python包N$^2$，支持快速实验和基准测试，并开发了一种新的NN变体。

**结果:** 新提出的NN变体在多个设置中实现了最先进的结果，并发布了一个包含真实世界数据集的基准套件。

**结论:** 基于NN的方法在真实世界数据中持续优于传统方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是N%24%5E2%24%3A+A+Unified+Python+Package+and+Test+Bench+for+Nearest+Neighbor-Based+Matrix+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04166，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04166&send_immediately=true&force_search=false)

**原文摘要:** Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix
completion, offering strong empirical performance and recent theoretical
guarantees, including entry-wise error bounds, confidence intervals, and
minimax optimality. Despite their simplicity, recent work has shown that NN
approaches are robust to a range of missingness patterns and effective across
diverse applications. This paper introduces N$^2$, a unified Python package and
testbed that consolidates a broad class of NN-based methods through a modular,
extensible interface. Built for both researchers and practitioners, N$^2$
supports rapid experimentation and benchmarking. Using this framework, we
introduce a new NN variant that achieves state-of-the-art results in several
settings. We also release a benchmark suite of real-world datasets, from
healthcare and recommender systems to causal inference and LLM evaluation,
designed to stress-test matrix completion methods beyond synthetic scenarios.
Our experiments demonstrate that while classical methods excel on idealized
data, NN-based techniques consistently outperform them in real-world settings.

</details>


### [34] [Optimization of Epsilon-Greedy Exploration](https://arxiv.org/abs/2506.03324)
*Ethan Che, Hakan Ceylan, James McInerney, Nathan Kallus*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一个推荐系统中动态调整探索率的新框架，通过最小化贝叶斯遗憾实现更优性能。


<details>
  <summary>更多</summary>
  
**动机:** 推荐系统需要在探索与利用之间权衡，但实际限制条件如批量更新、用户流量变化等使选择最佳探索率变得复杂。

**方法:** 使用随机梯度下降（SGD）直接最小化贝叶斯遗憾，并通过模型预测控制（MPC）进行动态探索率调整。

**结果:** 实验表明，跨周期的批量大小变化显著影响最优探索策略，优化方法能够根据具体问题设置自动校准探索。

**结论:** 论文提出了一种基于贝叶斯遗憾最小化和模型预测控制的动态调整探索率框架，能够在各种实际条件下自动校准探索策略，并优于现有启发式方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimization+of+Epsilon-Greedy+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03324，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03324&send_immediately=true&force_search=false)

**原文摘要:** Modern recommendation systems rely on exploration to learn user preferences
for new items, typically implementing uniform exploration policies (e.g.,
epsilon-greedy) due to their simplicity and compatibility with machine learning
(ML) personalization models. Within these systems, a crucial consideration is
the rate of exploration - what fraction of user traffic should receive random
item recommendations and how this should evolve over time. While various
heuristics exist for navigating the resulting exploration-exploitation
tradeoff, selecting optimal exploration rates is complicated by practical
constraints including batched updates, time-varying user traffic, short time
horizons, and minimum exploration requirements. In this work, we propose a
principled framework for determining the exploration schedule based on directly
minimizing Bayesian regret through stochastic gradient descent (SGD), allowing
for dynamic exploration rate adjustment via Model-Predictive Control (MPC).
Through extensive experiments with recommendation datasets, we demonstrate that
variations in the batch size across periods significantly influence the optimal
exploration strategy. Our optimization methods automatically calibrate
exploration to the specific problem setting, consistently matching or
outperforming the best heuristic for each setting.

</details>


### [35] [Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2209.01205)
*Han Wu, Jie Yin, Bala Rajaratnam, Jianyuan Guo*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种用于少样本知识图谱补全的新方法HiRe，该方法通过捕捉实体级、三元组级和上下文级的关系信息来有效学习和优化少样本关系的元表示，实验证明了其在基准数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 知识图谱在推理方面具有强大的能力，但由于其不完整性和关系的长尾分布而臭名昭著。为了应对这些挑战并扩大知识图谱的覆盖范围，少样本知识图谱补全旨在仅提供少量训练三元组作为参考时，对涉及新关系的三元组进行预测。

**方法:** 提出了一种名为HiRe的分层关系学习方法，旨在通过联合捕捉实体级、三元组级和上下文级的关系信息来改进少样本知识图谱补全。

**结果:** 广泛的实验验证了HiRe在基准数据集上的优越性，证明其优于最先进的方法。

**结论:** 论文提出了一种名为HiRe的分层关系学习方法，用于少样本知识图谱补全。通过捕捉三个层次的关系信息（实体级、三元组级和上下文级），HiRe能够有效地学习和优化少样本关系的元表示，从而很好地泛化到新的未见过的关系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hierarchical+Relational+Learning+for+Few-Shot+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2209.01205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2209.01205&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graphs (KGs) are powerful in terms of their inference abilities,
but are also notorious for their incompleteness and long-tail distribution of
relations. To address these challenges and expand the coverage of KGs, few-shot
KG completion aims to make predictions for triplets involving novel relations
when only a few training triplets are provided as reference. Previous methods
have focused on designing local neighbor aggregators to learn entity-level
information and/or imposing a potentially invalid sequential dependency
assumption at the triplet level to learn meta relation information. However,
pairwise triplet-level interactions and context-level relational information
have been largely overlooked for learning meta representations of few-shot
relations. In this paper, we propose a hierarchical relational learning method
(HiRe) for few-shot KG completion. By jointly capturing three levels of
relational information (entity-level, triplet-level and context-level), HiRe
can effectively learn and refine meta representations of few-shot relations,
and thus generalize well to new unseen relations. Extensive experiments on
benchmark datasets validate the superiority of HiRe over state-of-the-art
methods. The code can be found in https://github.com/alexhw15/HiRe.git.

</details>


### [36] [A Differential Perspective on Distributional Reinforcement Learning](https://arxiv.org/abs/2506.03333)
*Juan Sebastian Rojas, Chi-Guhn Lee*

**主要类别:** cs.LG

**AI概要:** 本文首次将分布强化学习应用于平均奖励设置，提出了有效的算法并验证其性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的分布强化学习方法只关注折扣设置，而本研究旨在优化每个时间步的奖励。

**方法:** 利用基于分位数的方法开发新算法，并推导了可证明收敛的表格算法。

**结果:** 提出了一组新算法，能够学习和优化长期每步奖励分布以及差分回报分布。

**结论:** 该论文成功地将分布强化学习扩展到了平均奖励设置，提出了新的算法来优化每时间步的奖励。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Differential+Perspective+on+Distributional+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03333&send_immediately=true&force_search=false)

**原文摘要:** To date, distributional reinforcement learning (distributional RL) methods
have exclusively focused on the discounted setting, where an agent aims to
optimize a potentially-discounted sum of rewards over time. In this work, we
extend distributional RL to the average-reward setting, where an agent aims to
optimize the reward received per time-step. In particular, we utilize a
quantile-based approach to develop the first set of algorithms that can
successfully learn and/or optimize the long-run per-step reward distribution,
as well as the differential return distribution of an average-reward MDP. We
derive proven-convergent tabular algorithms for both prediction and control, as
well as a broader family of algorithms that have appealing scaling properties.
Empirically, we find that these algorithms consistently yield competitive
performance when compared to their non-distributional equivalents, while also
capturing rich information about the long-run reward and return distributions.

</details>


### [37] [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](https://arxiv.org/abs/2506.03337)
*Yide Ran, Wentao Guo, Jingwei Sun, Yanzhou Pan, Xiaodong Yu, Hao Wang, Jianwen Xie, Yiran Chen, Denghui Zhang, Zhaozhuo Xu*

**主要类别:** cs.LG

**AI概要:** Meerkat是一种稀疏零阶优化方法，用于联邦大型语言模型微调，通过限制参数更新实现通信效率，有效处理非独立同分布数据挑战，并提出Meerkat-vp识别极端非独立同分布客户端以提高模型质量。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中大型语言模型的参数规模导致显著内存和通信挑战，需要一种高效优化方法应对去中心化非独立同分布客户端场景。

**方法:** Meerkat采用静态、可迁移的极稀疏参数子集进行微调，利用可追踪本地更新形成每个客户端的虚拟路径，并基于GradIP现象分析识别极端非独立同分布客户端。

**结果:** 实验表明Meerkat相比全参数零阶优化更高效且能有效缓解非独立同分布数据挑战；Meerkat-vp通过分析GradIP轨迹进一步提升聚合模型质量。

**结论:** Meerkat及Meerkat-vp在通信效率与模型性能上优于现有稀疏基线方法，为联邦大型语言模型微调提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Non-IID+Drift+in+Zeroth-Order+Federated+LLM+Fine-Tuning+with+Transferable+Sparsity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03337&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning enables collaborative fine-tuning of Large Language Models
(LLMs) across decentralized Non-Independent and Identically Distributed
(Non-IID) clients, but such models' massive parameter sizes lead to significant
memory and communication challenges. This work introduces Meerkat, a sparse
zeroth-order optimization (ZO) method designed for federated LLM fine-tuning.
By limiting fine-tuning to a transferable, static, extremely sparse subset of
parameters, Meerkat achieves remarkable communication efficiency, enabling
cost-effective high-frequency synchronization. With theoretical analysis and
experiments, we show that this high-frequency communication effectively
mitigates Non-IID data challenges and leads to superior performance compared to
full-parameter ZO. Furthermore, experiment results show that Meerkat
outperforms existing sparsity baselines with better performance at the same
communication frequency. To further handle Non-IID drift, Meerkat leverages
traceable local updates and forms a virtual path for each client. This virtual
path mechanism reveals the GradIP phenomenon: the inner products between LLM
pre-training gradients maintained by server and client gradients estimated via
ZO converges for extreme Non-IID clients but oscillates for IID ones. This
distinct behavior provides a signal for identifying clients with extreme data
heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP
trajectories to identify extreme Non-IID clients and applies early stopping to
enhance aggregated model quality. Experiments confirm that Meerkat and
Meerkat-vp significantly improve the efficiency and effectiveness of ZO
federated LLM fine-tuning.

</details>


### [38] [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/abs/2506.03355)
*Elias Abad Rocamora, Christian Schlarmann, Naman Deep Singh, Yongtao Wu, Matthias Hein, Volkan Cevher*

**主要类别:** cs.LG

**AI概要:** 本文提出LEAF方法，通过对抗微调提升CLIP文本编码器的鲁棒性，在多种任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 虽然已有研究致力于提高CLIP图像编码器的鲁棒性，但文本编码器的鲁棒性仍未被探索。本文填补了这一文献空白。

**方法:** 提出了LEAF：一种高效的文本域对抗微调方法，具有扩展到大型CLIP模型的能力。

**结果:** 该模型在文本域中显著提高了零样本对抗准确性，改善了对抗噪声下的生成质量和多模态检索任务中的召回率，并提升了输入文本嵌入后的重建能力。

**结论:** LEAF方法能够显著提升CLIP文本编码器的鲁棒性，同时保持其视觉性能，并在多模态任务和文本到图像生成任务中表现出优越的抗噪能力和重建能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robustness+in+Both+Domains%3A+CLIP+Needs+a+Robust+Text+Encoder，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03355&send_immediately=true&force_search=false)

**原文摘要:** Adversarial input attacks can cause a significant shift of CLIP embeddings.
This can affect the downstream robustness of models incorporating CLIP in the
pipeline, such as text-to-image generative models or large vision language
models. While some efforts have been done towards making the CLIP image
encoders robust, the robustness of text encoders remains unexplored. In this
work, we cover this gap in the literature. We propose LEAF: an efficient
adversarial finetuning method for the text domain, with the ability to scale to
large CLIP models. Our models significantly improve the zero-shot adversarial
accuracy in the text domain, while maintaining the vision performance provided
by robust image encoders. When combined with text-to-image diffusion models, we
can improve the generation quality under adversarial noise. When employing our
robust CLIP encoders in multimodal retrieval tasks, we improve the recall under
adversarial noise over standard CLIP models. Finally, we show that robust text
encoders facilitate better reconstruction of input text from its embedding via
direct optimization.

</details>


### [39] [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
*Leonid Ryvkin*

**主要类别:** cs.LG

**AI概要:** 这篇论文综述了独特硬注意力变压器编码器(UHATs)在识别形式语言方面的性能，探讨了不同类型的注意力机制，并确立了计算能力的理论边界。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地理解独特硬注意力变压器编码器(UHATs)在识别形式语言方面的潜力和限制。

**方法:** 对不同的结果进行了综述，区分了屏蔽与非屏蔽、有限与无限图像以及一般与双线性注意力得分函数之间的差异。

**结果:** 总结了这些模型之间的关系，并建立了基于一阶逻辑的下界和基于电路复杂度的上界。

**结论:** UHATs在识别形式语言方面的能力已经被调查，并且已经确定了与一阶逻辑的下界和电路复杂度的上界的关系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparison+of+different+Unique+hard+attention+transformer+models+by+the+formal+languages+they+can+recognize，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03370，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03370&send_immediately=true&force_search=false)

**原文摘要:** This note is a survey of various results on the capabilities of unique hard
attention transformers encoders (UHATs) to recognize formal languages. We
distinguish between masked vs. non-masked, finite vs. infinite image and
general vs. bilinear attention score functions. We recall some relations
between these models, as well as a lower bound in terms of first-order logic
and an upper bound in terms of circuit complexity.

</details>


### [40] [Product Quantization for Surface Soil Similarity](https://arxiv.org/abs/2506.03374)
*Haley Dozier, Althea Henslee, Ashley Abraham, Andrew Strelzoff, Mark Chappell*

**主要类别:** cs.LG

**AI概要:** 该研究通过机器学习改进土壤分类，克服了传统方法的局限性，提高了分类的准确性和适应性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的土壤分类依赖于历史理解和人工划分，缺乏数据驱动的统计相似性分析，限制了高维数据的分类精度。

**方法:** 结合产品量化与参数及输出的系统评估来优化机器学习流程，而不是使用默认或猜测设置。

**结果:** 实现了比人工分类更高精确度和灵活性的土壤分类方法，并能根据特定应用构建分类体系。

**结论:** 机器学习技术在土壤分类中的应用可以产生高度准确和灵活的分类系统，超越了传统人工分类方法的限制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Product+Quantization+for+Surface+Soil+Similarity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03374，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03374&send_immediately=true&force_search=false)

**原文摘要:** The use of machine learning (ML) techniques has allowed rapid advancements in
many scientific and engineering fields. One of these problems is that of
surface soil taxonomy, a research area previously hindered by the reliance on
human-derived classifications, which are mostly dependent on dividing a dataset
based on historical understandings of that data rather than data-driven,
statistically observable similarities. Using a ML-based taxonomy allows soil
researchers to move beyond the limitations of human visualization and create
classifications of high-dimension datasets with a much higher level of
specificity than possible with hand-drawn taxonomies. Furthermore, this
pipeline allows for the possibility of producing both highly accurate and
flexible soil taxonomies with classes built to fit a specific application. The
machine learning pipeline outlined in this work combines product quantization
with the systematic evaluation of parameters and output to get the best
available results, rather than accepting sub-optimal results by using either
default settings or best guess settings.

</details>


### [41] [Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons](https://arxiv.org/abs/2506.03392)
*Aref Ghoreishee, Abhishek Mishra, John Walsh, Anup Das, Nagarajan Kandasamy*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种改进的三值脉冲神经元模型，优化了深度Q学习任务中的梯度估计偏差问题，并在多个Atari游戏中验证了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高二值脉冲神经元在深度Q学习中的表示能力，解决现有三值神经元模型性能较差的问题。

**方法:** 通过减少训练过程中的估计偏差，提出了一种新的三值脉冲神经元模型，并将其作为基本计算单元应用于深度脉冲Q学习网络（DSQN）中，在七个Atari游戏中进行评估。

**结果:** 结果表明，提出的三值脉冲神经元模型有效缓解了三值模型在Q学习任务中的性能问题，同时优于现有二值模型。

**结论:** 提出的三值脉冲神经元模型能够减轻Q学习任务中三值神经元的剧烈性能下降，并且与现有二值神经元相比提高了网络性能，使得DSQN成为更实用的自主决策任务解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Performance+of+Spike-based+Deep+Q-Learning+using+Ternary+Neurons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03392，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03392&send_immediately=true&force_search=false)

**原文摘要:** We propose a new ternary spiking neuron model to improve the representation
capacity of binary spiking neurons in deep Q-learning. Although a ternary
neuron model has recently been introduced to overcome the limited
representation capacity offered by the binary spiking neurons, we show that its
performance is worse than that of binary models in deep Q-learning tasks. We
hypothesize gradient estimation bias during the training process as the
underlying potential cause through mathematical and empirical analysis. We
propose a novel ternary spiking neuron model to mitigate this issue by reducing
the estimation bias. We use the proposed ternary spiking neuron as the
fundamental computing unit in a deep spiking Q-learning network (DSQN) and
evaluate the network's performance in seven Atari games from the Gym
environment. Results show that the proposed ternary spiking neuron mitigates
the drastic performance degradation of ternary neurons in Q-learning tasks and
improves the network performance compared to the existing binary neurons,
making DSQN a more practical solution for on-board autonomous decision-making
tasks.

</details>


### [42] [The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks](https://arxiv.org/abs/2506.03404)
*Walter Mayor, Johan Obando-Ceron, Aaron Courville, Pablo Samuel Castro*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了强化学习中并行执行器的数据收集策略，重点分析了偏差-方差权衡和过拟合问题，并发现更大的数据集和更多的并行环境能显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 论文旨在探讨强化学习中通过并行执行器进行数据收集的技术，研究数据收集方式所引发的偏差-方差权衡，以及训练过程中样本效率和过拟合之间的平衡问题。

**方法:** 论文对PPO这一使用并行执行器的流行RL算法中的这些权衡进行了实证分析，并建立了与网络可塑性和优化稳定性的联系。

**结果:** 研究表明，更大的数据集可以提高各种设置下的最终性能，同时扩展并行环境的效果优于增加rollout长度。此外，还分析了其对网络架构的影响以及扩展数据时的超参数敏感性。

**结论:** 论文得出的结论是，数据收集策略在提升代理性能方面起着关键作用。更大的数据集大小可以提高最终性能，而扩展并行环境比增加 rollout 长度更有效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Impact+of+On-Policy+Parallelized+Data+Collection+on+Deep+Reinforcement+Learning+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03404，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03404&send_immediately=true&force_search=false)

**原文摘要:** The use of parallel actors for data collection has been an effective
technique used in reinforcement learning (RL) algorithms. The manner in which
data is collected in these algorithms, controlled via the number of parallel
environments and the rollout length, induces a form of bias-variance trade-off;
the number of training passes over the collected data, on the other hand, must
strike a balance between sample efficiency and overfitting. We conduct an
empirical analysis of these trade-offs on PPO, one of the most popular RL
algorithms that uses parallel actors, and establish connections to network
plasticity and, more generally, optimization stability. We examine its impact
on network architectures, as well as the hyper-parameter sensitivity when
scaling data. Our analyses indicate that larger dataset sizes can increase
final performance across a variety of settings, and that scaling parallel
environments is more effective than increasing rollout lengths. These findings
highlight the critical role of data collection strategies in improving agent
performance.

</details>


### [43] [A Machine Learning Theory Perspective on Strategic Litigation](https://arxiv.org/abs/2506.03411)
*Melissa Dutz, Han Shao, Avrim Blum, Aloni Cohen*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了战略诉讼在法律系统中的影响，并利用机器学习方法分析了战略诉讼者如何通过选择案件来改变未来裁决。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是探索战略诉讼如何通过法律案例对更广泛的法律体系产生影响，并从机器学习的角度提供新的见解。

**方法:** 论文使用了机器学习理论和抽象模型来模拟普通法法律系统，并分析战略诉讼者的策略和影响。

**结果:** 论文展示了战略诉讼者可以通过选择特定案件来塑造法院的决策规则，即使在某些情况下知道会输掉个别案件，也可以实现整体利益最大化。

**结论:** 论文得出结论，战略诉讼者通过精心挑选案件可以显著影响法院决策规则的学习过程，并进而影响未来的裁决。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Machine+Learning+Theory+Perspective+on+Strategic+Litigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03411&send_immediately=true&force_search=false)

**原文摘要:** Strategic litigation involves bringing a legal case to court with the goal of
having a broader impact beyond resolving the case itself: for example, creating
precedent which will influence future rulings. In this paper, we explore
strategic litigation from the perspective of machine learning theory. We
consider an abstract model of a common-law legal system where a lower court
decides new cases by applying a decision rule learned from a higher court's
past rulings. In this model, we explore the power of a strategic litigator, who
strategically brings cases to the higher court to influence the learned
decision rule, thereby affecting future cases. We explore questions including:
What impact can a strategic litigator have? Which cases should a strategic
litigator bring to court? Does it ever make sense for a strategic litigator to
bring a case when they are sure the court will rule against them?

</details>


### [44] [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
*Joonseong Kang, Soojeong Lee, Subeen Park, Sumin Park, Taero Kim, Jihee Kim, Ryunyi Lee, Kyungwoo Song*

**主要类别:** cs.LG

**AI概要:** 本文提出了Adaptive Task Vectors (ATV)，一种新的框架，通过根据每个输入查询动态生成任务向量来提高大型语言模型的性能和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决ICL和现有向量基础方法中存在的问题，例如对演示顺序敏感、上下文长度限制、计算效率低以及当输入查询与底层演示不一致时适应效果差的问题。

**方法:** 提出了一种名为Adaptive Task Vectors (ATV)的新框架，该框架基于每个输入查询动态生成任务向量，并使用小型语言模型生成任务向量，随后将其转换以匹配目标LLM架构并指导其输出生成。

**结果:** ATV展示了即使在未见过的任务上的强大性能和泛化能力，并提供了理论分析表明ATV在表示能力方面的优势。

**结论:** ATV方法相较于之前的向量基础方法和ICL在未见过的任务上展现了更强的性能和泛化能力，并且理论上与LoRA在相同等级预算下具有表达等价性，比Prefix-Tuning更具表现力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Task+Vectors+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03426&send_immediately=true&force_search=false)

**原文摘要:** In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks without parameter updates by conditioning on a few demonstrations
provided in the prompt. Despite its success, ICL suffers from several
limitations, including sensitivity to demonstration order, context length
constraints, and computational inefficiency. To address these challenges, task
vector-based approaches compress task information into a single vector.
However, these methods typically construct task vectors from fixed sets of
demonstrations and reuse them across input queries, without conditioning on the
specific input. This limitation can lead models to struggle with effective
adaptation when the input query is not well aligned with the underlying
demonstrations, consequently degrading their generalization performance on
unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors
(ATV), a simple and effective framework that dynamically generates task vectors
conditioned on each input query. ATV employs a small language model to generate
task vectors, which are then transformed to match the target LLM's architecture
and applied to guide its output generation. In contrast to ICL and previous
vector-based approaches, which rely on fixed demonstration sets and their
corresponding vectors, ATV dynamically generates task vectors tailored to each
specific input query and task. Consequently, ATV demonstrates strong
performance and generalization capabilities, even for unseen tasks.
Furthermore, we provide a theoretical analysis indicating that ATV is
expressively equivalent to LoRA under equal rank budgets and more expressive
than Prefix-Tuning, thereby offering formal support for its representational
advantage.

</details>


### [45] [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/abs/2506.03444)
*Yue Gong, Raul Castro Fernandez*

**主要类别:** cs.LG

**AI概要:** 本研究旨在解决自动化假设生成中的瓶颈问题——假设评估。作者提出了一种基于大规模语言模型（LLM）的方法来预测和评估变量之间的相关性，并证明其性能优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着自动化假设生成的发展，假设评估成为新的瓶颈。现代系统能够揭示大量的统计关系，但缺乏关于哪些关系是新颖、非显而易见或值得专家关注的指导。因此，需要一种自动化的假设评估方法。

**方法:** 研究提出了一种基于Logit的校准先验方法，该方法将LLM的原始输出logits转化为校准后的连续预测分布，用于评估变量对之间的相关性。

**结果:** 所提出的LLM引导的相关性先验方法在2096个真实世界变量对的基准测试中表现出色：符号准确率达到78.8%，平均绝对误差为0.26，95%可信区间覆盖率为89.2%。此外，它在二元相关性预测和假设排序的精度@K上优于微调的RoBERTa分类器。

**结论:** 论文得出结论，通过利用大规模语言模型（LLM）的先验知识可以有效地进行假设评估，尤其是对于相关性分析。这种方法在预测皮尔逊相关系数方面表现良好，并且能够推广到训练过程中未见过的相关性情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploiting+LLMs+for+Automatic+Hypothesis+Assessment+via+a+Logit-Based+Calibrated+Prior，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03444，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03444&send_immediately=true&force_search=false)

**原文摘要:** As hypothesis generation becomes increasingly automated, a new bottleneck has
emerged: hypothesis assessment. Modern systems can surface thousands of
statistical relationships-correlations, trends, causal links-but offer little
guidance on which ones are novel, non-trivial, or worthy of expert attention.
In this work, we study the complementary problem to hypothesis generation:
automatic hypothesis assessment. Specifically, we ask: given a large set of
statistical relationships, can we automatically assess which ones are novel and
worth further exploration? We focus on correlations as they are a common entry
point in exploratory data analysis that often serve as the basis for forming
deeper scientific or causal hypotheses.
  To support automatic assessment, we propose to leverage the vast knowledge
encoded in LLMs' weights to derive a prior distribution over the correlation
value of a variable pair. If an LLM's prior expects the correlation value
observed, then such correlation is not surprising, and vice versa. We propose
the Logit-based Calibrated Prior, an LLM-elicited correlation prior that
transforms the model's raw output logits into a calibrated, continuous
predictive distribution over correlation values. We evaluate the prior on a
benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of
78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of
89.2% in predicting Pearson correlation coefficient. It also outperforms a
fine-tuned RoBERTa classifier in binary correlation prediction and achieves
higher precision@K in hypothesis ranking. We further show that the prior
generalizes to correlations not seen during LLM pretraining, reflecting
context-sensitive reasoning rather than memorization.

</details>


### [46] [Directional Non-Commutative Monoidal Embeddings for MNIST](https://arxiv.org/abs/2506.03472)
*Mahesh Godavarti*

**主要类别:** cs.LG

**AI概要:** 论文验证了方向性非交换幺半群嵌入框架在MNIST图像分类中的有效性，表明其优于固定DFT嵌入，尤其在低维嵌入下表现更佳。


<details>
  <summary>更多</summary>
  
**动机:** 该框架通过学习任务特定的频率分量来推广离散傅里叶变换（DFT），假设能够更好地捕捉任务的关键谱分量。

**方法:** 将非交换幺半群嵌入框架应用于MNIST数据集的图像分类任务，并与固定DFT嵌入进行比较。

**结果:** 随着嵌入维度降低，学习到的幺半群嵌入与固定DFT嵌入之间的性能差距逐渐增大。

**结论:** 实验结果证明了方向性非交换幺半群嵌入在图像数据表示中非常有效，提供了一种紧凑且任务性能高的学习表示。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Directional+Non-Commutative+Monoidal+Embeddings+for+MNIST，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03472&send_immediately=true&force_search=false)

**原文摘要:** We present an empirical validation of the directional non-commutative
monoidal embedding framework recently introduced in prior
work~\cite{Godavarti2025monoidal}. This framework defines learnable
compositional embeddings using distinct non-commutative operators per dimension
(axis) that satisfy an interchange law, generalizing classical one-dimensional
transforms. Our primary goal is to verify that this framework can effectively
model real data by applying it to a controlled, well-understood task: image
classification on the MNIST dataset~\cite{lecun1998gradient}. A central
hypothesis for why the proposed monoidal embedding works well is that it
generalizes the Discrete Fourier Transform (DFT)~\cite{oppenheim1999discrete}
by learning task-specific frequency components instead of using fixed basis
frequencies. We test this hypothesis by comparing learned monoidal embeddings
against fixed DFT-based embeddings on MNIST. The results show that as the
embedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance
gap between the learned monoidal embeddings and fixed DFT-based embeddings on
MNIST grows increasingly large. This comparison is used as an analytic tool to
explain why the framework performs well: the learnable embeddings can capture
the most discriminative spectral components for the task. Overall, our
experiments confirm that directional non-commutative monoidal embeddings are
highly effective for representing image data, offering a compact learned
representation that retains high task performance. The code used in this work
is available at
https://github.com/mahesh-godavarti/directional_composition_mnist.

</details>


### [47] [CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design](https://arxiv.org/abs/2506.03474)
*Yifeng Xiao, Yurong Xu, Ning Yan, Masood Mortazavi, Pierluigi Nuzzo*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为CORE的方法，用于基于仿真的设计空间探索，通过约束感知和一步强化学习方法提高采样效率并优化设计配置。


<details>
  <summary>更多</summary>
  
**动机:** 现有的启发式和多步强化学习方法在处理高维结构化设计时难以平衡采样效率与约束满足，因为反馈稀疏、延迟且动作空间大。

**方法:** CORE方法通过定义设计配置的结构化分布进行采样，使用扩展图解码器建模依赖关系，并通过奖励塑造惩罚无效设计，利用代理目标函数比较批次内设计奖励更新策略。

**结果:** 实验表明，CORE在神经网络加速器硬件映射协同设计中显著提高了采样效率，并优于当前最先进的基线方法。

**结论:** CORE是一种通用方法，适用于各种离散-连续约束设计问题，能够有效解决现有方法面临的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CORE%3A+Constraint-Aware+One-Step+Reinforcement+Learning+for+Simulation-Guided+Neural+Network+Accelerator+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03474，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03474&send_immediately=true&force_search=false)

**原文摘要:** Simulation-based design space exploration (DSE) aims to efficiently optimize
high-dimensional structured designs under complex constraints and expensive
evaluation costs. Existing approaches, including heuristic and multi-step
reinforcement learning (RL) methods, struggle to balance sampling efficiency
and constraint satisfaction due to sparse, delayed feedback, and large hybrid
action spaces. In this paper, we introduce CORE, a constraint-aware, one-step
RL method for simulationguided DSE. In CORE, the policy agent learns to sample
design configurations by defining a structured distribution over them,
incorporating dependencies via a scaling-graph-based decoder, and by reward
shaping to penalize invalid designs based on the feedback obtained from
simulation. CORE updates the policy using a surrogate objective that compares
the rewards of designs within a sampled batch, without learning a value
function. This critic-free formulation enables efficient learning by
encouraging the selection of higher-reward designs. We instantiate CORE for
hardware-mapping co-design of neural network accelerators, demonstrating that
it significantly improves sample efficiency and achieves better accelerator
configurations compared to state-of-the-art baselines. Our approach is general
and applicable to a broad class of discrete-continuous constrained design
problems.

</details>


### [48] [A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems](https://arxiv.org/abs/2506.03588)
*Hiroki Shiraishi, Hisao Ishibuchi, Masaya Nakata*

**主要类别:** cs.LG

**AI概要:** This paper proposes a new class inference scheme for Learning Fuzzy-Classifier Systems (LFCSs) based on Dempster-Shafer Theory of Evidence (DS theory), which significantly improves performance, robustness, and interpretability across 30 real-world datasets.


<details>
  <summary>更多</summary>
  
**动机:** Improving decision-making mechanisms (class inference schemes) in Learning Fuzzy-Classifier Systems (LFCSs) is crucial due to their influence on prediction accuracy and reliability. Existing schemes like voting-based or single-winner-based methods risk overfitting and perform poorly on unseen data.

**方法:** A novel class inference scheme using Dempster-Shafer Theory of Evidence (DS theory) was introduced and applied to a variant of LFCS (Fuzzy-UCS). Belief masses for each class and the ``I don't know'' state were calculated from fuzzy rules to infer classes.

**结果:** The proposed scheme demonstrated statistically significant improvements in test macro F1 scores across 30 real-world datasets compared to conventional schemes. It formed smoother decision boundaries, provided reliable confidence measures, and enhanced robustness.

**结论:** The study concludes that the proposed class inference scheme based on DS theory improves transparency, reliability, robustness, and generalizability of LFCSs in real-world applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Class+Inference+Scheme+With+Dempster-Shafer+Theory+for+Learning+Fuzzy-Classifier+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03588，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03588&send_immediately=true&force_search=false)

**原文摘要:** The decision-making process significantly influences the predictions of
machine learning models. This is especially important in rule-based systems
such as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and
application of rules directly determine prediction accuracy and reliability.
LFCSs combine evolutionary algorithms with supervised learning to optimize
fuzzy classification rules, offering enhanced interpretability and robustness.
Despite these advantages, research on improving decision-making mechanisms
(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use
voting-based or single-winner-based inference schemes. These schemes rely on
classification performance on training data and may not perform well on unseen
data, risking overfitting. To address these limitations, this article
introduces a novel class inference scheme for LFCSs based on the
Dempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles
uncertainty well. By using the DS theory, the scheme calculates belief masses
(i.e., measures of belief) for each specific class and the ``I don't know''
state from each fuzzy rule and infers a class from these belief masses. Unlike
the conventional schemes, the proposed scheme also considers the ``I don't
know'' state that reflects uncertainty, thereby improving the transparency and
reliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the
proposed scheme demonstrates statistically significant improvements in terms of
test macro F1 scores across 30 real-world datasets compared to conventional
voting-based and single-winner-based fuzzy inference schemes. It forms smoother
decision boundaries, provides reliable confidence measures, and enhances the
robustness and generalizability of LFCSs in real-world applications. Our
implementation is available at https://github.com/YNU-NakataLab/jUCS.

</details>


### [49] [Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees](https://arxiv.org/abs/2506.03531)
*Daniel Ovalle, Lorenz T. Biegler, Ignacio E. Grossmann, Carl D. Laird, Mateo Dulce Rubio*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的框架C-MICL，这种方法在优化问题中为数据驱动约束提供了概率可行性保证。


<details>
  <summary>更多</summary>
  
**动机:** 标准的混合整数约束学习方法由于模型误差或数据限制经常违反真正的约束，因此提出了C-MICL方法以解决这个问题。

**方法:** 论文提出了一种名为Conformal Mixed-Integer Constraint Learning (C-MICL)的新框架，该框架利用了共形预测来确保解决方案是真实可行的。

**结果:** 实验结果表明，与现有方法相比，C-MICL持续实现了目标可行性率，保持了竞争性的目标性能，并显著降低了计算成本。

**结论:** 论文得出结论，C-MICL方法在数学优化和机器学习之间架起了桥梁，提供了一种有原则的方法，将不确定性感知约束纳入决策中，并具备严格的统计保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conformal+Mixed-Integer+Constraint+Learning+with+Feasibility+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03531&send_immediately=true&force_search=false)

**原文摘要:** We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel
framework that provides probabilistic feasibility guarantees for data-driven
constraints in optimization problems. While standard Mixed-Integer Constraint
Learning methods often violate the true constraints due to model error or data
limitations, our C-MICL approach leverages conformal prediction to ensure
feasible solutions are ground-truth feasible. This guarantee holds with
probability at least $1{-}\alpha$, under a conditional independence assumption.
The proposed framework supports both regression and classification tasks
without requiring access to the true constraint function, while avoiding the
scalability issues associated with ensemble-based heuristics. Experiments on
real-world applications demonstrate that C-MICL consistently achieves target
feasibility rates, maintains competitive objective performance, and
significantly reduces computational cost compared to existing methods. Our work
bridges mathematical optimization and machine learning, offering a principled
approach to incorporate uncertainty-aware constraints into decision-making with
rigorous statistical guarantees.

</details>


### [50] [Learning Monotonic Probabilities with a Generative Cost Model](https://arxiv.org/abs/2506.03542)
*Yongxiang Tang, Yanhua Cheng, Xiaocheng Liu, Chenchen Jiao, Yanxiang Zeng, Ning Luo, Pengjia Yuan, Xialong Liu, Peng Jiang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于生成模型的新方法（GCM和IGCM），有效解决了机器学习中的严格和隐式单调性问题，并在实验中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 在许多机器学习任务中，输入与输出变量之间的单调关系（包括严格单调和隐式单调）是必要的，但传统方法主要依赖构造或正则化技术，存在局限性。

**方法:** 引入了生成成本模型（GCM）和隐式生成成本模型（IGCM）来处理单调性问题，并采用数值模拟和公开数据集实验进行验证。

**结果:** 提出的GCM和IGCM方法在多个实验中显著优于现有的单调建模技术。

**结论:** 论文提出了一种新的严格单调和隐式单调问题的解决方法，并通过实验验证了其优于现有技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Monotonic+Probabilities+with+a+Generative+Cost+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03542&send_immediately=true&force_search=false)

**原文摘要:** In many machine learning tasks, it is often necessary for the relationship
between input and output variables to be monotonic, including both strictly
monotonic and implicitly monotonic relationships. Traditional methods for
maintaining monotonicity mainly rely on construction or regularization
techniques, whereas this paper shows that the issue of strict monotonic
probability can be viewed as a partial order between an observable revenue
variable and a latent cost variable. This perspective enables us to reformulate
the monotonicity challenge into modeling the latent cost variable. To tackle
this, we introduce a generative network for the latent cost variable, termed
the Generative Cost Model (GCM), which inherently addresses the strict
monotonic problem, and propose the Implicit Generative Cost Model (IGCM) to
address the implicit monotonic problem. We further validate our approach with a
numerical simulation of quantile regression and conduct multiple experiments on
public datasets, showing that our method significantly outperforms existing
monotonic modeling techniques. The code for our experiments can be found at
https://github.com/tyxaaron/GCM.

</details>


### [51] [Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems](https://arxiv.org/abs/2506.03602)
*Hiroki Shiraishi, Yohei Hayamizu, Tomonori Hashiyama, Keiki Takadama, Hisao Ishibuchi, Masaya Nakata*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种基于四参数Beta分布的新规则表示方法，用于模糊风格的学习分类系统，以自动选择适合不同输入空间子区域的规则表示形式，从而提高分类准确性和模型可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 论文的动机是解决学习分类系统（LCSs）中如何选择适当的规则表示的问题，因为某些问题需要针对输入空间的不同子空间采用不同的表示方式，因此需要一种自适应机制。

**方法:** 论文的方法是引入了一种灵活的规则表示方法，基于四参数Beta分布，并将其集成到模糊风格的LCS中。这种方法能够形成各种函数形状，从而实现对不同输入空间子区域的适当表示选择。

**结果:** 实验结果表明，该方法在真实世界分类任务中实现了显著更高的测试准确性，并生成了更加紧凑的规则集。

**结论:** 论文的结论是，通过使用四参数Beta分布规则表示和模糊风格LCS的整合，该方法能够自动选择适合不同子空间的表示形式，并且在真实世界分类任务中取得了显著优越的测试准确性和更紧凑的规则集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+Rule+Representation+With+Four-Parameter+Beta+Distribution+for+Learning+Classifier+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03602，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03602&send_immediately=true&force_search=false)

**原文摘要:** Rule representations significantly influence the search capabilities and
decision boundaries within the search space of Learning Classifier Systems
(LCSs), a family of rule-based machine learning systems that evolve
interpretable models through evolutionary processes. However, it is very
difficult to choose an appropriate rule representation for each problem.
Additionally, some problems benefit from using different representations for
different subspaces within the input space. Thus, an adaptive mechanism is
needed to choose an appropriate rule representation for each rule in LCSs. This
article introduces a flexible rule representation using a four-parameter beta
distribution and integrates it into a fuzzy-style LCS. The four-parameter beta
distribution can form various function shapes, and this flexibility enables our
LCS to automatically select appropriate representations for different
subspaces. Our rule representation can represent crisp/fuzzy decision
boundaries in various boundary shapes, such as rectangles and bells, by
controlling four parameters, compared to the standard representations such as
trapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the
appropriate rule representation for each subspace. Moreover, our LCS
incorporates a generalization bias favoring crisp rules where feasible,
enhancing model interpretability without compromising accuracy. Experimental
results on real-world classification tasks show that our LCS achieves
significantly superior test accuracy and produces more compact rule sets. Our
implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An
extended abstract related to this work is available at
https://doi.org/10.36227/techrxiv.174900805.59801248/v1.

</details>


### [52] [Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning](https://arxiv.org/abs/2506.03556)
*Wang WeiQuan, Riaz-ul-Haque Mian*

**主要类别:** cs.LG

**AI概要:** 这项研究提出了一种新的算法来改进随机抽样、分层抽样和k均值聚类抽样方法，从而减少所需的测试数量并保持预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 降低半导体制造中晶圆和FPGA测试的测试成本，同时保持预测准确性。

**方法:** 通过在高斯过程回归框架内评估 Stratified with Short Distance Elimination 和 k-means with Short Distance Elimination 的性能，使用实际工业生产数据进行实验，并对(alpha, beta)阈值进行了参数扫描以找到最小RMSD的最佳组合。

**结果:** K-SDE在晶圆和FPGA采样方面分别提高了16.26%和13.07%，S-SDE则分别提高了16.49%和8.84%。

**结论:** Stratified with Short Distance Elimination 和 k-means with Short Distance Elimination 这两种混合策略能够提高预测精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+FPGA+and+Wafer+Test+Coverage+with+Spatial+Sampling+and+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03556&send_immediately=true&force_search=false)

**原文摘要:** In semiconductor manufacturing, testing costs remain significantly high,
especially during wafer and FPGA testing. To reduce the number of required
tests while maintaining predictive accuracy, this study investigates three
baseline sampling strategies: Random Sampling, Stratified Sampling, and k-means
Clustering Sampling. To further enhance these methods, this study proposes a
novel algorithm that improves the sampling quality of each approach. This
research is conducted using real industrial production data from wafer-level
tests and silicon measurements from various FPGAs. This study introduces two
hybrid strategies: Stratified with Short Distance Elimination (S-SDE) and
k-means with Short Distance Elimination (K-SDE). Their performance is evaluated
within the framework of Gaussian Process Regression (GPR) for predicting wafer
and FPGA test data. At the core of our proposed approach is the Short Distance
Elimination (SDE) algorithm, which excludes spatially proximate candidate
points during sampling, thereby ensuring a more uniform distribution of
training data across the physical domain. A parameter sweep was conducted over
the (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,
3, 4} and not both zero, to identify the optimal combination that minimizes
RMSD. Experimental results on a randomly selected wafer file reveal that
(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent
experiments adopt this parameter configuration. The results demonstrate that
the proposed SDE-based strategies enhance predictive accuracy: K-SDE improves
upon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while
S-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84
percent (FPGA).

</details>


### [53] [GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS](https://arxiv.org/abs/2506.03618)
*Jiayi Wan, Xiang Zhu, Fanzhen Liu, Wei Fan, Xiaolong Xu*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一种新的差分隐私联邦学习框架，通过服务器端梯度校正机制，在保证隐私的同时提高了模型的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究主要集中在动态调整添加的噪声或丢弃某些梯度以减轻由差分隐私引入的噪声，但这些方法无法消除阻碍收敛的噪声并对受噪声影响的梯度进行校正，这大大降低了模型分类的准确性。

**方法:** 在客户端执行梯度裁剪和噪声扰动后，该框架检测噪声局部梯度中的偏差，并使用投影机制对其进行校正。

**结果:** 实验结果表明，该框架在相同的隐私预算下达到了最先进的性能。

**结论:** 论文提出了一种新的差分隐私联邦学习框架，该框架通过服务器端梯度校正机制，在保证隐私的同时提高了模型的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GCFL%3A+A+Gradient+Correction-based+Federated+Learning+Framework+for+Privacy-preserving+CPSS，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03618，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03618&send_immediately=true&force_search=false)

**原文摘要:** Federated learning, as a distributed architecture, shows great promise for
applications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the
privacy risks inherent in CPSS, the integration of differential privacy with
federated learning has attracted considerable attention. Existing research
mainly focuses on dynamically adjusting the noise added or discarding certain
gradients to mitigate the noise introduced by differential privacy. However,
these approaches fail to remove the noise that hinders convergence and correct
the gradients affected by the noise, which significantly reduces the accuracy
of model classification. To overcome these challenges, this paper proposes a
novel framework for differentially private federated learning that balances
rigorous privacy guarantees with accuracy by introducing a server-side gradient
correction mechanism. Specifically, after clients perform gradient clipping and
noise perturbation, our framework detects deviations in the noisy local
gradients and employs a projection mechanism to correct them, mitigating the
negative impact of noise. Simultaneously, gradient projection promotes the
alignment of gradients from different clients and guides the model towards
convergence to a global optimum. We evaluate our framework on several benchmark
datasets, and the experimental results demonstrate that it achieves
state-of-the-art performance under the same privacy budget.

</details>


### [54] [Scaling CrossQ with Weight Normalization](https://arxiv.org/abs/2506.03758)
*Daniel Palenicek, Florian Vogt, Jan Peters*

**主要类别:** cs.LG

**AI概要:** 这篇论文探讨了如何通过集成权重归一化来提高CrossQ框架在强化学习中的样本效率和可扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 虽然强化学习取得了重大里程碑，但样本效率仍然是现实应用的瓶颈。最近的CrossQ展示了具有低更新到数据比的最先进的样本效率，但需要探索其高比率下的行为。

**方法:** 在CrossQ框架中集成权重归一化以解决Q-偏差爆炸和评论网络权重增长幅度的问题。

**结果:** 该方法可以在增加的UTD比率下可靠地扩展，在DeepMind控制基准的一系列挑战性任务中实现了竞争或优越的性能。

**结论:** 本文提出了一种改进的CrossQ框架，通过集成权重归一化解决了强化学习中样本效率和可扩展性的问题，为模型无关的强化学习提供了一个稳健的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+CrossQ+with+Weight+Normalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03758，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03758&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning has achieved significant milestones, but sample
efficiency remains a bottleneck for real-world applications. Recently, CrossQ
has demonstrated state-of-the-art sample efficiency with a low update-to-data
(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with
higher UTD ratios. We identify challenges in the training dynamics which are
emphasized by higher UTDs, particularly Q-bias explosion and the growing
magnitude of critic network weights. To address this, we integrate weight
normalization into the CrossQ framework, a solution that stabilizes training,
prevents potential loss of plasticity and keeps the effective learning rate
constant. Our proposed approach reliably scales with increasing UTD ratios,
achieving competitive or superior performance across a range of challenging
tasks on the DeepMind control benchmark, notably the complex dog and humanoid
environments. This work eliminates the need for drastic interventions, such as
network resets, and offers a robust pathway for improving sample efficiency and
scalability in model-free reinforcement learning.

</details>


### [55] [VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration](https://arxiv.org/abs/2506.03590)
*Minh Luu, Surya Jasper, Khoi Le, Evan Pan, Michael Quinn, Aakash Tyagi, Jiang Hu*

**主要类别:** cs.LG

**AI概要:** VCDiag是一种利用VCD数据进行高效适应的失败波形分类和可能失败位置定位的方法，具有较高的准确率和数据压缩效果。


<details>
  <summary>更多</summary>
  
**动机:** 设计功能验证中的故障排除是一项耗时的工作，目前主要依赖于人工规范审查、日志检查和波形分析。虽然机器学习在某些领域有所改进，但在RTL级仿真失败排查中的应用仍然有限。

**方法:** 使用VCD数据和一种新的信号选择与统计压缩方法，实现了高效的失败波形分类和可能失败位置的定位。

**结果:** VCDiag在最大的实验中实现了超过94%的准确率，并实现了超过120倍的原始数据大小的减少。

**结论:** VCDiag可以有效地用于RTL级仿真失败分类，并能准确地识别出最有可能失败的模块。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VCDiag%3A+Classifying+Erroneous+Waveforms+for+Failure+Triage+Acceleration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03590，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03590&send_immediately=true&force_search=false)

**原文摘要:** Failure triage in design functional verification is critical but
time-intensive, relying on manual specification reviews, log inspections, and
waveform analyses. While machine learning (ML) has improved areas like stimulus
generation and coverage closure, its application to RTL-level simulation
failure triage, particularly for large designs, remains limited. VCDiag offers
an efficient, adaptable approach using VCD data to classify failing waveforms
and pinpoint likely failure locations. In the largest experiment, VCDiag
achieves over 94% accuracy in identifying the top three most likely modules.
The framework introduces a novel signal selection and statistical compression
approach, achieving over 120x reduction in raw data size while preserving
features essential for classification. It can also be integrated into diverse
Verilog/SystemVerilog designs and testbenches.

</details>


### [56] [HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark](https://arxiv.org/abs/2506.03954)
*Jianqing Zhang, Xinghao Wu, Yanbing Zhou, Xiaoting Sun, Qiqi Cai, Yang Liu, Yang Hua, Zhenzhe Zheng, Jian Cao, Qiang Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了 HtFLlib，这是一个用于异构联邦学习的易用且可扩展的框架，旨在提供标准化的评估基准并推动该领域的研究与应用。


<details>
  <summary>更多</summary>
  
**动机:** 缺乏标准化评估和分析快速增长的 HtFL 方法的综合性基准。

**方法:** 整合了多个数据集和模型异构性场景，并系统地评估了准确性、收敛性、计算成本和通信成本等方面。

**结果:** 引入了首个异构联邦学习库 (HtFLlib)，集成了多种数据集、模型架构以及代表性 HtFL 方法的实现。

**结论:** HtFLlib 是一个全面且可扩展的框架，为 HtFL 方法的研究和实际应用提供了稳健基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HtFLlib%3A+A+Comprehensive+Heterogeneous+Federated+Learning+Library+and+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03954&send_immediately=true&force_search=false)

**原文摘要:** As AI evolves, collaboration among heterogeneous models helps overcome data
scarcity by enabling knowledge transfer across institutions and devices.
Traditional Federated Learning (FL) only supports homogeneous models, limiting
collaboration among clients with heterogeneous model architectures. To address
this, Heterogeneous Federated Learning (HtFL) methods are developed to enable
collaboration across diverse heterogeneous models while tackling the data
heterogeneity issue at the same time. However, a comprehensive benchmark for
standardized evaluation and analysis of the rapidly growing HtFL methods is
lacking. Firstly, the highly varied datasets, model heterogeneity scenarios,
and different method implementations become hurdles to making easy and fair
comparisons among HtFL methods. Secondly, the effectiveness and robustness of
HtFL methods are under-explored in various scenarios, such as the medical
domain and sensor signal modality. To fill this gap, we introduce the first
Heterogeneous Federated Learning Library (HtFLlib), an easy-to-use and
extensible framework that integrates multiple datasets and model heterogeneity
scenarios, offering a robust benchmark for research and practical applications.
Specifically, HtFLlib integrates (1) 12 datasets spanning various domains,
modalities, and data heterogeneity scenarios; (2) 40 model architectures,
ranging from small to large, across three modalities; (3) a modularized and
easy-to-extend HtFL codebase with implementations of 10 representative HtFL
methods; and (4) systematic evaluations in terms of accuracy, convergence,
computation costs, and communication costs. We emphasize the advantages and
potential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze
advancing HtFL research and enable its broader applications. The code is
released at https://github.com/TsingZ0/HtFLlib.

</details>


### [57] [Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection](https://arxiv.org/abs/2506.03964)
*HyunGi Kim, Jisoo Mok, Dongjun Lee, Jaihyun Lew, Sungjae Kim, Sungroh Yoon*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的多变量时间序列异常检测方法CAROTS，它通过将因果关系纳入对比学习来提高检测能力。


<details>
  <summary>更多</summary>
  
**动机:** 利用多变量时间序列中复杂的变量间因果关系可以提供更稳健可靠的多变量时间序列异常检测(MTSAD)，但这一领域研究不足。

**方法:** 该论文提出了一种名为CAROTS的新颖MTSAD流程，使用两种数据增强器生成保持和干扰因果关系的样本来进行对比学习，并引入了相似度过滤的一类对比损失。

**结果:** 在五个真实世界和两个合成数据集上的广泛实验验证了因果关系整合赋予CAROTS改进的MTSAD能力。

**结论:** 论文得出结论，通过将因果关系纳入对比学习，CAROTS方法在多变量时间序列异常检测方面表现出改进的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causality-Aware+Contrastive+Learning+for+Robust+Multivariate+Time-Series+Anomaly+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03964，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03964&send_immediately=true&force_search=false)

**原文摘要:** Utilizing the complex inter-variable causal relationships within multivariate
time-series provides a promising avenue toward more robust and reliable
multivariate time-series anomaly detection (MTSAD) but remains an underexplored
area of research. This paper proposes Causality-Aware contrastive learning for
RObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that
incorporates the notion of causality into contrastive learning. CAROTS employs
two data augmentors to obtain causality-preserving and -disturbing samples that
serve as a wide range of normal variations and synthetic anomalies,
respectively. With causality-preserving and -disturbing samples as positives
and negatives, CAROTS performs contrastive learning to train an encoder whose
latent space separates normal and abnormal samples based on causality.
Moreover, CAROTS introduces a similarity-filtered one-class contrastive loss
that encourages the contrastive learning process to gradually incorporate more
semantically diverse samples with common causal relationships. Extensive
experiments on five real-world and two synthetic datasets validate that the
integration of causal relationships endows CAROTS with improved MTSAD
capabilities. The code is available at https://github.com/kimanki/CAROTS.

</details>


### [58] [CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor](https://arxiv.org/abs/2506.04001)
*Han Ji, Yuqi Feng, Jiahao Fan, Yanan Sun*

**主要类别:** cs.LG

**AI概要:** CARL方法通过关注关键架构特征，改进了神经架构搜索的性能预测，具有高准确率和强可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现有预测器忽略训练样本和测试样本之间的分布变化，导致泛化能力差。因此需要一种因果关系引导的方法提高预测准确性。

**方法:** 使用子结构提取器将输入架构分为关键和冗余子结构，并生成多个干预样本以优先考虑关键特征。

**结果:** 在五个NAS搜索空间的实验中，CARL展现了最先进的准确性和优越的可解释性。例如，在CIFAR-10上实现了97.67%的一致准确率。

**结论:** CARL方法通过分离架构中的关键和冗余特征，提高了神经架构搜索中性能预测的准确性和可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CARL%3A+Causality-guided+Architecture+Representation+Learning+for+an+Interpretable+Performance+Predictor，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04001，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04001&send_immediately=true&force_search=false)

**原文摘要:** Performance predictors have emerged as a promising method to accelerate the
evaluation stage of neural architecture search (NAS). These predictors estimate
the performance of unseen architectures by learning from the correlation
between a small set of trained architectures and their performance. However,
most existing predictors ignore the inherent distribution shift between limited
training samples and diverse test samples. Hence, they tend to learn spurious
correlations as shortcuts to predictions, leading to poor generalization. To
address this, we propose a Causality-guided Architecture Representation
Learning (CARL) method aiming to separate critical (causal) and redundant
(non-causal) features of architectures for generalizable architecture
performance prediction. Specifically, we employ a substructure extractor to
split the input architecture into critical and redundant substructures in the
latent space. Then, we generate multiple interventional samples by pairing
critical representations with diverse redundant representations to prioritize
critical features. Extensive experiments on five NAS search spaces demonstrate
the state-of-the-art accuracy and superior interpretability of CARL. For
instance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.

</details>


### [59] [Out-of-Distribution Graph Models Merging](https://arxiv.org/abs/2506.03674)
*Yidi Wang, Jiawei Gu, pei Xiaobing, Xubin Zheng, Xiao Luo, Pengyang Wang, Ziyue Qiao*

**主要类别:** cs.LG

**AI概要:** This paper introduces a novel framework for merging out-of-distribution graph models by using a graph generation strategy, a MoE module, and a masking mechanism, which operates without source/target domain data.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to construct a generalized model from multiple graph models trained on different domains with distribution discrepancies, which is challenging due to the difficulty in learning domain-invariant knowledge and consolidating expertise from heterogeneous GNN backbones.

**方法:** The paper proposes a graph generation strategy combined with a MoE module and a masking mechanism to merge and fine-tune pre-trained graph models.

**结果:** Both theoretical analysis and experimental results show the effectiveness of the approach in addressing the model generalization problem.

**结论:** The paper concludes that their proposed framework effectively addresses the model generalization problem in out-of-distribution graph models merging.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Out-of-Distribution+Graph+Models+Merging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03674，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03674&send_immediately=true&force_search=false)

**原文摘要:** This paper studies a novel problem of out-of-distribution graph models
merging, which aims to construct a generalized model from multiple graph models
pre-trained on different domains with distribution discrepancy. This problem is
challenging because of the difficulty in learning domain-invariant knowledge
implicitly in model parameters and consolidating expertise from potentially
heterogeneous GNN backbones. In this work, we propose a graph generation
strategy that instantiates the mixture distribution of multiple domains. Then,
we merge and fine-tune the pre-trained graph models via a MoE module and a
masking mechanism for generalized adaptation. Our framework is
architecture-agnostic and can operate without any source/target domain data.
Both theoretical analysis and experimental results demonstrate the
effectiveness of our approach in addressing the model generalization problem.

</details>


### [60] [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
*Jun-Peng Jiang, Yu Xia, Hai-Long Sun, Shiyin Lu, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye*

**主要类别:** cs.LG

**AI概要:** 本文提出了Turbo框架，利用训练期间的结构化信息提升多模态大语言模型在表格图像推理任务中的表现，并在少量数据下取得显著成果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型在处理表格数据时依赖高质量的文本表示，但在实际应用中，表格通常以图像形式存在，缺乏结构化的文本表示，因此需要一种新方法来提升多模态模型对表格图像的理解和推理能力。

**方法:** 提出了一种新的多模态表格推理框架Turbo，该框架包括一个基于DeepSeek-R1的结构感知推理轨迹生成器，并通过重复生成和选择优势推理路径来增强模型的推理能力。

**结果:** 实验结果表明，在数据量仅为9k的情况下，Turbo框架在多个数据集上取得了最先进的性能，相较之前的最佳结果提升了7.2%。

**结论:** Turbo框架通过利用训练期间的结构化信息，显著提升了多模态大语言模型在表格图像推理任务中的表现，即使数据量有限也能达到最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal+Tabular+Reasoning+with+Privileged+Structured+Information，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04088，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04088&send_immediately=true&force_search=false)

**原文摘要:** Tabular reasoning involves multi-step information extraction and logical
inference over tabular data. While recent advances have leveraged large
language models (LLMs) for reasoning over structured tables, such high-quality
textual representations are often unavailable in real-world settings, where
tables typically appear as images. In this paper, we tackle the task of tabular
reasoning from table images, leveraging privileged structured information
available during training to enhance multimodal large language models (MLLMs).
The key challenges lie in the complexity of accurately aligning structured
information with visual representations, and in effectively transferring
structured reasoning skills to MLLMs despite the input modality gap. To address
these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a
new framework for multimodal tabular reasoning with privileged structured
tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator
based on DeepSeek-R1, contributing to high-quality modality-bridged data. On
this basis, {\sc Turbo} repeatedly generates and selects the advantageous
reasoning paths, further enhancing the model's tabular reasoning ability.
Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo}
achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across
multiple datasets.

</details>


### [61] [Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring](https://arxiv.org/abs/2506.03696)
*Fang Wang, Paolo Ceravolo, Ernesto Damiani*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的预测性业务流程监控方法，通过动态LSTM HyperModels解决了现有方法在灵活性、自适应表示和异构数据泛化方面的不足，并取得了较高的准确率和F1分数。


<details>
  <summary>更多</summary>
  
**动机:** 现有的预测性业务流程监控方法缺乏应对现实挑战的灵活性，例如并发事件、类别不平衡和多级属性。因此，需要一种能够提供自适应表示并在异构数据集中推广的新方法。

**方法:** 该论文提出了集成双层层次编码、事件标签的字符级分解以及持续时间和属性相关性的伪嵌入技术的动态LSTM HyperModels。此外，还引入了用于同时建模并发事件的多维嵌入和时间差标志增强的LSTM变体。

**结果:** 实验结果显示，在四个公开的真实世界数据集上，该方法在平衡数据集上达到了100%的准确率，在不平衡数据集上的F1分数超过了86%。

**结论:** 论文提出了一种动态LSTM HyperModels套件，以解决预测性业务流程监控（PBPM）中的灵活性和泛化能力问题。这种方法通过模块化和可解释的模型设计，不仅推动了PBPM的发展，还为更广泛的AI社区在时间结果预测、数据异构性支持和可解释过程智能框架方面做出了贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comprehensive+Attribute+Encoding+and+Dynamic+LSTM+HyperModels+for+Outcome+Oriented+Predictive+Business+Process+Monitoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03696&send_immediately=true&force_search=false)

**原文摘要:** Predictive Business Process Monitoring (PBPM) aims to forecast future
outcomes of ongoing business processes. However, existing methods often lack
flexibility to handle real-world challenges such as simultaneous events, class
imbalance, and multi-level attributes. While prior work has explored static
encoding schemes and fixed LSTM architectures, they struggle to support
adaptive representations and generalize across heterogeneous datasets. To
address these limitations, we propose a suite of dynamic LSTM HyperModels that
integrate two-level hierarchical encoding for event and sequence attributes,
character-based decomposition of event labels, and novel pseudo-embedding
techniques for durations and attribute correlations. We further introduce
specialized LSTM variants for simultaneous event modeling, leveraging
multidimensional embeddings and time-difference flag augmentation. Experimental
validation on four public and real-world datasets demonstrates up to 100%
accuracy on balanced datasets and F1 scores exceeding 86\% on imbalanced ones.
Our approach advances PBPM by offering modular and interpretable models better
suited for deployment in complex settings. Beyond PBPM, it contributes to the
broader AI community by improving temporal outcome prediction, supporting data
heterogeneity, and promoting explainable process intelligence frameworks.

</details>


### [62] [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)
*Anastasiia Ivanova, Eva Bakaeva, Zoya Volovikova, Alexey K. Kovalev, Aleksandr I. Panov*

**主要类别:** cs.LG

**AI概要:** 本文提出了名为AmbiK的新数据集，旨在为厨房环境中机器人接收到的模糊指令提供统一的模糊性检测方法比较基准。


<details>
  <summary>更多</summary>
  
**动机:** 由于现有任务模糊性检测方法难以在不同数据集之间进行比较，因此需要一个通用基准数据集。

**方法:** 通过LLM辅助收集和人类验证的方式构建了包含1000对模糊任务及其非模糊对应任务的数据集，并按模糊类型进行分类。

**结果:** 构建了一个包含2000个任务的数据集，其中涵盖多种模糊类型以及环境描述、澄清问题、用户意图和任务计划等信息。

**结论:** 论文提出了一种全新的、完全文本的模糊任务数据集AmbiK，用于帮助研究者统一比较模糊性检测方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AmbiK%3A+Dataset+of+Ambiguous+Tasks+in+Kitchen+Environment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04089&send_immediately=true&force_search=false)

**原文摘要:** As a part of an embodied agent, Large Language Models (LLMs) are typically
used for behavior planning given natural language instructions from the user.
However, dealing with ambiguous instructions in real-world environments remains
a challenge for LLMs. Various methods for task ambiguity detection have been
proposed. However, it is difficult to compare them because they are tested on
different datasets and there is no universal benchmark. For this reason, we
propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual
dataset of ambiguous instructions addressed to a robot in a kitchen
environment. AmbiK was collected with the assistance of LLMs and is
human-validated. It comprises 1000 pairs of ambiguous tasks and their
unambiguous counterparts, categorized by ambiguity type (Human Preferences,
Common Sense Knowledge, Safety), with environment descriptions, clarifying
questions and answers, user intents, and task plans, for a total of 2000 tasks.
We hope that AmbiK will enable researchers to perform a unified comparison of
ambiguity detection methods. AmbiK is available at
https://github.com/cog-model/AmbiK-dataset.

</details>


### [63] [Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond](https://arxiv.org/abs/2506.03703)
*Xiansheng Cai, Sihan Hu, Tao Wang, Yuan Huang, Pan Zhang, Youjin Deng, Kun Chen*

**主要类别:** cs.LG

**AI概要:** 本研究提出LaC方法，利用临界现象使大型语言模型在极小数据下实现高效学习，解决了基础物理学中复杂且数据稀缺的问题。


<details>
  <summary>更多</summary>
  
**动机:** 基础物理学常常面临信息稀缺的复杂符号问题，而传统人工智能因依赖大量数据难以解决这些问题。

**方法:** 引入了一种强化学习方案（LaC），将大型语言模型调整到临界点，并分析了一个概念网络模型（CoNet）来揭示学习跃迁的本质。

**结果:** 在少量示例训练下，模型实现了从简单数据的最大泛化能力，并成功应用于量子场理论中的符号Matsubara求和问题。

**结论:** LaC通过利用临界现象这一物理原理，使LLMs在数据稀缺的复杂问题中达到峰值性能，展示了在基础物理学中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning-at-Criticality+in+Large+Language+Models+for+Quantum+Field+Theory+and+Beyond，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03703&send_immediately=true&force_search=false)

**原文摘要:** Fundamental physics often confronts complex symbolic problems with few
guiding exemplars or established principles. While artificial intelligence (AI)
offers promise, its typical need for vast datasets to learn from hinders its
use in these information-scarce frontiers. We introduce learning at criticality
(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models
(LLMs) to a sharp learning transition, addressing this information scarcity. At
this transition, LLMs achieve peak generalization from minimal data,
exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic
reasoning. To elucidate this peak, we analyze a minimal concept-network model
(CoNet) designed to capture the essence of how LLMs might link tokens. Trained
on a single exemplar, this model also undergoes a sharp learning transition.
This transition exhibits hallmarks of a second-order phase transition, notably
power-law distributed solution path lengths. At this critical point, the system
maximizes a ``critical thinking pattern" crucial for generalization, enabled by
the underlying scale-free exploration. This suggests LLMs reach peak
performance by operating at criticality, where such explorative dynamics enable
the extraction of underlying operational rules. We demonstrate LaC in quantum
field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a
few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,
significantly outperforming far larger models. LaC thus leverages critical
phenomena, a physical principle, to empower AI for complex, data-sparse
challenges in fundamental physics.

</details>


### [64] [Horizon Reduction Makes RL Scalable](https://arxiv.org/abs/2506.04168)
*Seohong Park, Kevin Frans, Deepinder Mann, Benjamin Eysenbach, Aviral Kumar, Sergey Levine*

**主要类别:** cs.LG

**AI概要:** 本文研究了离线强化学习（RL）算法的可扩展性，发现尽管数据增加，许多现有算法在复杂任务上表现不佳，长视野是限制可扩展性的主要因素。作者提出了一种新的方法SHARSA来减少视野并提升可扩展性。


<details>
  <summary>更多</summary>
  
**动机:** 为了实现真正的可扩展离线RL，算法应能在提供足够数据、计算能力和模型容量的情况下解决任何问题。当前的算法是否能够达到这一目标仍不清楚，因此需要进行深入研究。

**方法:** 通过使用比典型离线RL数据集大1000倍的数据集，评估现有离线RL算法在多样化、具有挑战性且此前未解决的任务上的表现，并分析其扩展行为。此外，作者提出了SHARSA方法以减少视野并提升扩展性。

**结果:** 研究发现，许多现有离线RL算法在扩展数据时表现不佳，性能远低于最优水平。实验验证表明，长视野是影响离线RL扩展性的根本障碍，而各种视野缩减技术可以显著提升扩展性。SHARSA在所有评估方法中表现出最佳的渐近性能和扩展行为。

**结论:** 显式减少视野能够解锁离线RL的可扩展性，SHARSA是一种简单但有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Horizon+Reduction+Makes+RL+Scalable，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04168，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04168&send_immediately=true&force_search=false)

**原文摘要:** In this work, we study the scalability of offline reinforcement learning (RL)
algorithms. In principle, a truly scalable offline RL algorithm should be able
to solve any given problem, regardless of its complexity, given sufficient
data, compute, and model capacity. We investigate if and how current offline RL
algorithms match up to this promise on diverse, challenging, previously
unsolved tasks, using datasets up to 1000x larger than typical offline RL
datasets. We observe that despite scaling up data, many existing offline RL
algorithms exhibit poor scaling behavior, saturating well below the maximum
performance. We hypothesize that the horizon is the main cause behind the poor
scaling of offline RL. We empirically verify this hypothesis through several
analysis experiments, showing that long horizons indeed present a fundamental
barrier to scaling up offline RL. We then show that various horizon reduction
techniques substantially enhance scalability on challenging tasks. Based on our
insights, we also introduce a minimal yet scalable method named SHARSA that
effectively reduces the horizon. SHARSA achieves the best asymptotic
performance and scaling behavior among our evaluation methods, showing that
explicitly reducing the horizon unlocks the scalability of offline RL. Code:
https://github.com/seohongpark/horizon-reduction

</details>


### [65] [Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints](https://arxiv.org/abs/2506.04171)
*Utkarsh Utkarsh, Pengfei Cai, Alan Edelman, Rafael Gomez-Bombarelli, Christopher Vincent Rackauckas*

**主要类别:** cs.LG

**AI概要:** 本论文提出了一种名为PCFM的方法，能够在深度生成模型中有效实施物理约束，以解决现有方法不能保证硬约束的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法往往依赖于软惩罚或架构偏差，无法保证硬约束，而物理约束（如守恒定律和物理一致性）的严格执行仍然具有挑战性。

**方法:** 提出了一种名为Physics-Constrained Flow Matching (PCFM)的零样本推理框架，通过基于物理的修正持续指导采样过程，同时保持与学习流的一致性并满足物理约束。

**结果:** PCFM在一系列偏微分方程（包括具有冲击波、不连续性和尖锐特征的问题）上优于无约束和有约束的基线方法，同时确保最终解的精确约束满足。

**结论:** PCFM方法为在科学和通用生成模型中执行硬约束提供了一个通用框架，特别是在满足约束条件至关重要的应用中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physics-Constrained+Flow+Matching%3A+Sampling+Generative+Models+with+Hard+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04171，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04171&send_immediately=true&force_search=false)

**原文摘要:** Deep generative models have recently been applied to physical systems
governed by partial differential equations (PDEs), offering scalable simulation
and uncertainty-aware inference. However, enforcing physical constraints, such
as conservation laws (linear and nonlinear) and physical consistencies, remains
challenging. Existing methods often rely on soft penalties or architectural
biases that fail to guarantee hard constraints. In this work, we propose
Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that
enforces arbitrary nonlinear constraints in pretrained flow-based generative
models. PCFM continuously guides the sampling process through physics-based
corrections applied to intermediate solution states, while remaining aligned
with the learned flow and satisfying physical constraints. Empirically, PCFM
outperforms both unconstrained and constrained baselines on a range of PDEs,
including those with shocks, discontinuities, and sharp features, while
ensuring exact constraint satisfaction at the final solution. Our method
provides a general framework for enforcing hard constraints in both scientific
and general-purpose generative models, especially in applications where
constraint satisfaction is essential.

</details>


### [66] [Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization](https://arxiv.org/abs/2506.03725)
*Daniil Medyakov, Sergey Stanko, Gleb Molodtsov, Philip Zmushko, Grigoriy Evseev, Egor Petrov, Aleksandr Beznosikov*

**主要类别:** cs.LG

**AI概要:** 这篇论文旨在解决Sign-SGD算法在大型语言模型训练中难以确定有效步长的问题，提出了几种单节点确定性Sign-SGD变体，并将其应用于实际场景和问题中进行验证。


<details>
  <summary>更多</summary>
  
**动机:** 训练大型语言模型是一项极其耗费资源的任务，Sign-SGD作为一种内存效率高且可作为分布式学习梯度压缩技术的方法，但理论上无法自动确定有效的步长。

**方法:** 论文提出了一些变种的Sign-SGD算法，并在实际机器学习问题上进行了广泛的实验验证其效果。

**结果:** 通过广泛的实验强调了所提方法的实际适用性。

**结论:** 作者设计了几种单节点确定性Sign-SGD变体，并将这些方法扩展到了实际场景中，如随机单节点和多节点学习以及带有动量的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sign-SGD+is+the+Golden+Gate+between+Multi-Node+to+Single-Node+Learning%3A+Significant+Boost+via+Parameter-Free+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03725，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03725&send_immediately=true&force_search=false)

**原文摘要:** Quite recently, large language models have made a significant breakthrough
across various disciplines. However, training them is an extremely
resource-intensive task, even for major players with vast computing resources.
One of the methods gaining popularity in light of these challenges is Sign-SGD.
This method can be applied both as a memory-efficient approach in single-node
training and as a gradient compression technique in the distributed learning.
Nevertheless, it is impossible to automatically determine the effective
stepsize from the theoretical standpoint. Indeed, it depends on the parameters
of the dataset to which we do not have access in the real-world learning
paradigm. To address this issue, we design several variants of single-node
deterministic Sign-SGD. We extend our approaches to practical scenarios:
stochastic single-node and multi-node learning, methods with incorporated
momentum. We conduct extensive experiments on real machine learning problems
that emphasize the practical applicability of our ideas.

</details>


### [67] [MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures](https://arxiv.org/abs/2506.04195)
*Elena Zamaraeva, Christopher M. Collins, George R. Darling, Matthew S. Dyer, Bei Peng, Rahul Savani, Dmytro Antypov, Vladimir V. Gusev, Judith Clymo, Paul G. Spirakis, Matthew J. Rosseinsky*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为MACS的新多智能体强化学习方法，用于周期晶体结构优化，显示出优越的性能和潜力。


<details>
  <summary>更多</summary>
  
**动机:** 几何优化是计算化学和材料设计中的关键任务，需要一种高效的方法来处理周期性晶体结构优化。

**方法:** 提出了一种新的多智能体强化学习方法MACS，将几何优化视为部分可观测的马尔可夫博弈。

**结果:** MACS在优化速度、能量计算次数和失败率方面优于现有最先进的优化方法。

**结论:** MACS方法在周期晶体结构优化中表现出色，具有良好的可扩展性和零样本迁移能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MACS%3A+Multi-Agent+Reinforcement+Learning+for+Optimization+of+Crystal+Structures，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04195，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04195&send_immediately=true&force_search=false)

**原文摘要:** Geometry optimization of atomic structures is a common and crucial task in
computational chemistry and materials design. Following the learning to
optimize paradigm, we propose a new multi-agent reinforcement learning method
called Multi-Agent Crystal Structure optimization (MACS) to address periodic
crystal structure optimization. MACS treats geometry optimization as a
partially observable Markov game in which atoms are agents that adjust their
positions to collectively discover a stable configuration. We train MACS across
various compositions of reported crystalline materials to obtain a policy that
successfully optimizes structures from the training compositions as well as
structures of larger sizes and unseen compositions, confirming its excellent
scalability and zero-shot transferability. We benchmark our approach against a
broad range of state-of-the-art optimization methods and demonstrate that MACS
optimizes periodic crystal structures significantly faster, with fewer energy
calculations, and the lowest failure rate.

</details>


### [68] [PPO in the Fisher-Rao geometry](https://arxiv.org/abs/2506.03757)
*Razvan-Andrei Lascu, David Šiška, Łukasz Szpruch*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种新的Proximal Policy Optimization（PPO）算法变体-Fisher-Rao PPO（FR-PPO），它在Fisher-Rao几何中提供了一个更严格的替代损失函数，从而提供了更强的理论保证，包括单调策略改进和次线性收敛。


<details>
  <summary>更多</summary>
  
**动机:** 尽管PPO很受欢迎，但它缺乏政策改进和收敛的正式理论保证，这促使作者寻找一种更有效的替代方案。

**方法:** 通过在线性化价值函数时使用Fisher-Rao（FR）几何，提出了一种新的PPO变体，即Fisher-Rao PPO（FR-PPO）。

**结果:** 所提出的FR-PPO在表格设置中实现了次线性收敛且不依赖于动作或状态空间的维度。

**结论:** FR-PPO提供了强大的理论保证，包括单调策略改进，并在表格设置中实现了次线性收敛，这是朝着建立基于PPO的算法形式收敛结果迈出的重要一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PPO+in+the+Fisher-Rao+geometry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03757，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03757&send_immediately=true&force_search=false)

**原文摘要:** Proximal Policy Optimization (PPO) has become a widely adopted algorithm for
reinforcement learning, offering a practical policy gradient method with strong
empirical performance. Despite its popularity, PPO lacks formal theoretical
guarantees for policy improvement and convergence. PPO is motivated by Trust
Region Policy Optimization (TRPO) that utilizes a surrogate loss with a KL
divergence penalty, which arises from linearizing the value function within a
flat geometric space. In this paper, we derive a tighter surrogate in the
Fisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).
Our proposed scheme provides strong theoretical guarantees, including monotonic
policy improvement. Furthermore, in the tabular setting, we demonstrate that
FR-PPO achieves sub-linear convergence without any dependence on the
dimensionality of the action or state spaces, marking a significant step toward
establishing formal convergence results for PPO-based algorithms.

</details>


### [69] [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
*Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了ReVisual-R1，这是一种新的开源7B MLLM，在多个复杂推理任务上取得了最先进的结果。


<details>
  <summary>更多</summary>
  
**动机:** 许多作品试图通过直接应用强化学习(RL)来激励Multimodal Large Language Models (MLLMs)中的类似能力，但它们仍然难以激活复杂的推理。

**方法:** 该论文研究了当前的训练管道，并确定了三个关键现象：有效的冷启动初始化对于增强MLLM推理至关重要；标准GRPO在多模态RL应用中存在梯度停滞问题；后续的文本RL训练可以进一步增强多模态推理。

**结果:** 该论文发现初始化的有效性，解决了多模态RL中的梯度停滞问题，并提出了一个分阶段的训练方法以平衡感知基础和认知推理发展。

**结论:** 通过引入ReVisual-R1，该论文在具有挑战性的基准测试中实现了新的最先进的结果，其中包括MathVerse、MathVision、WeMath、LogicVista、DynaMath和挑战性的AIME2024和AIME2025。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Multimodal+Reasoning%3A+From+Optimized+Cold+Start+to+Staged+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04207&send_immediately=true&force_search=false)

**原文摘要:** Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex
textual tasks, many works attempt to incentivize similar capabilities in
Multimodal Large Language Models (MLLMs) by directly applying reinforcement
learning (RL). However, they still struggle to activate complex reasoning. In
this paper, rather than examining multimodal RL in isolation, we delve into
current training pipelines and identify three crucial phenomena: 1) Effective
cold start initialization is critical for enhancing MLLM reasoning.
Intriguingly, we find that initializing with carefully selected text data alone
can lead to performance surpassing many recent multimodal reasoning models,
even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers
from gradient stagnation, which degrades training stability and performance. 3)
Subsequent text-only RL training, following the multimodal RL phase, further
enhances multimodal reasoning. This staged training approach effectively
balances perceptual grounding and cognitive reasoning development. By
incorporating the above insights and addressing multimodal RL issues, we
introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B
MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,
LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

</details>


### [70] [FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning](https://arxiv.org/abs/2506.03777)
*Li Zhang, Zhongxuan Han, Chaochao chen, Xiaohua Feng, Jiaming Zhang, Yuyuan Li*

**主要类别:** cs.LG

**AI概要:** 本文提出FedFACT，一种新型可控的联邦群体公平校准框架，解决了联邦学习中全局与局部公平性协调以及准确性和公平性权衡的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 当前联邦学习中的公平性标准难以分解且不可微分，导致无法有效协调全局公平与局部公平，并难以控制准确性和公平性之间的权衡。

**方法:** 提出了FedFACT框架，通过重新定义贝叶斯最优公平分类器，将公平联邦学习问题转化为个性化的成本敏感学习问题（用于预处理）和双层优化问题（用于后处理）。

**结果:** 实验表明，在多种数据异构性的数据集上，FedFACT在平衡准确性与全局-局部公平性方面始终优于基线方法。

**结论:** FedFACT框架能够在多类分类中识别全局和局部公平约束下的贝叶斯最优分类器，从而在保证公平性的同时实现最小的性能下降。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedFACT%3A+A+Provable+Framework+for+Controllable+Group-Fairness+Calibration+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03777，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03777&send_immediately=true&force_search=false)

**原文摘要:** With emerging application of Federated Learning (FL) in decision-making
scenarios, it is imperative to regulate model fairness to prevent disparities
across sensitive groups (e.g., female, male). Current research predominantly
focuses on two concepts of group fairness within FL: Global Fairness (overall
model disparity across all clients) and Local Fairness (the disparity within
each client). However, the non-decomposable, non-differentiable nature of
fairness criteria pose two fundamental, unresolved challenges for fair FL: (i)
Harmonizing global and local fairness in multi-class classification; (ii)
Enabling a controllable, optimal accuracy-fairness trade-off. To tackle the
aforementioned challenges, we propose a novel controllable federated
group-fairness calibration framework, named FedFACT. FedFACT identifies the
Bayes-optimal classifiers under both global and local fairness constraints in
multi-class case, yielding models with minimal performance decline while
guaranteeing fairness. To effectively realize an adjustable, optimal
accuracy-fairness balance, we derive specific characterizations of the
Bayes-optimal fair classifiers for reformulating fair FL as personalized
cost-sensitive learning problem for in-processing, and bi-level optimization
for post-processing. Theoretically, we provide convergence and generalization
guarantees for FedFACT to approach the near-optimal accuracy under given
fairness levels. Extensive experiments on multiple datasets across various data
heterogeneity demonstrate that FedFACT consistently outperforms baselines in
balancing accuracy and global-local fairness.

</details>


### [71] [Attention-Only Transformers via Unrolled Subspace Denoising](https://arxiv.org/abs/2506.03790)
*Peng Wang, Yifu Lu, Yaodong Yu, Druv Pai, Qing Qu, Yi Ma*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种更简洁、可解释的transformer模型，并验证了其在多个任务上的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管transformers在实践中很流行，但其架构设计是经验性的，既没有数学依据也不易于解释。此外，一些组件可能是冗余的。

**方法:** 通过将去噪操作与多头自注意力机制结合，推导出一种紧凑的transformer架构。

**结果:** 实验表明这种新的transformer在性能上接近标准的transformer架构，如GPT-2和CRATE。

**结论:** 论文得出了一种仅包含必要组件的完全可解释的transformer架构，并展示了其在视觉和语言任务上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention-Only+Transformers+via+Unrolled+Subspace+Denoising，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03790&send_immediately=true&force_search=false)

**原文摘要:** Despite the popularity of transformers in practice, their architectures are
empirically designed and neither mathematically justified nor interpretable.
Moreover, as indicated by many empirical studies, some components of
transformer architectures may be redundant. To derive a fully interpretable
transformer architecture with only necessary components, we contend that the
goal of representation learning is to compress a set of noisy initial token
representations towards a mixture of low-dimensional subspaces. To compress
these noisy token representations, an associated denoising operation naturally
takes the form of a multi-head (subspace) self-attention. By unrolling such
iterative denoising operations into a deep network, we arrive at a highly
compact architecture that consists of \textit{only} self-attention operators
with skip connections at each layer. Moreover, we show that each layer performs
highly efficient denoising: it improves the signal-to-noise ratio of token
representations \textit{at a linear rate} with respect to the number of layers.
Despite its simplicity, extensive experiments on vision and language tasks
demonstrate that such a transformer achieves performance close to that of
standard transformer architectures such as GPT-2 and CRATE.

</details>


### [72] [Learning Equilibria in Matching Games with Bandit Feedback](https://arxiv.org/abs/2506.03802)
*Andreas Athanasopoulos, Christos Dimitrakakis*

**主要类别:** cs.LG

**AI概要:** 本文研究了双侧匹配市场中的均衡学习问题，提出了一个UCB算法以实现次线性遗憾的均衡策略学习。


<details>
  <summary>更多</summary>
  
**动机:** 研究在具有零和博弈的双边匹配市场中，如何从老虎机反馈中学习均衡策略。

**方法:** 采用匹配均衡解概念，并引入匹配不稳定性作为遗憾度量标准；提出了一个UCB算法用于学习均衡策略。

**结果:** 提出的UCB算法能够在未知收益矩阵的情况下实现均衡学习，并达到次线性遗憾。

**结论:** 论文提出了一种基于乐观估计的UCB算法，能够实现时间范围T上的次线性、实例无关的遗憾。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Equilibria+in+Matching+Games+with+Bandit+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03802&send_immediately=true&force_search=false)

**原文摘要:** We investigate the problem of learning an equilibrium in a generalized
two-sided matching market, where agents can adaptively choose their actions
based on their assigned matches. Specifically, we consider a setting in which
matched agents engage in a zero-sum game with initially unknown payoff
matrices, and we explore whether a centralized procedure can learn an
equilibrium from bandit feedback. We adopt the solution concept of matching
equilibrium, where a pair consisting of a matching $\mathfrak{m}$ and a set of
agent strategies $X$ forms an equilibrium if no agent has the incentive to
deviate from $(\mathfrak{m}, X)$. To measure the deviation of a given pair
$(\mathfrak{m}, X)$ from the equilibrium pair $(\mathfrak{m}^\star, X^\star)$,
we introduce matching instability that can serve as a regret measure for the
corresponding learning problem. We then propose a UCB algorithm in which agents
form preferences and select actions based on optimistic estimates of the game
payoffs, and prove that it achieves sublinear, instance-independent regret over
a time horizon $T$.

</details>


### [73] [Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks](https://arxiv.org/abs/2506.03813)
*Lili Chen, Changyang She, Jingge Zhu, Jamie Evans*

**主要类别:** cs.LG

**AI概要:** 本文研究了无线网络中的联合信道和功率分配问题，提出了eWMMSE算法和基于图神经网络的JCPGNN-M解决方案，结果表明JCPGNN-M性能更优。


<details>
  <summary>更多</summary>
  
**动机:** 干扰成为无线网络数据速率提升的主要瓶颈，需要高效的联合信道和功率分配方案。

**方法:** 提出eWMMSE算法和JCPGNN-M解决方案，并通过拉格朗日函数进行功率约束。

**结果:** JCPGNN-M比eWMMSE具有更高的数据率和更低的推理时间。

**结论:** JCPGNN-M在密集网络场景中表现优异，具有良好的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Neural+Networks+for+Resource+Allocation+in+Multi-Channel+Wireless+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03813&send_immediately=true&force_search=false)

**原文摘要:** As the number of mobile devices continues to grow, interference has become a
major bottleneck in improving data rates in wireless networks. Efficient joint
channel and power allocation (JCPA) is crucial for managing interference. In
this paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the
JCPA problem in multi-channel wireless networks. To reduce the computational
complexity of iterative optimization, we further introduce JCPGNN-M, a graph
neural network-based solution that enables simultaneous multi-channel
allocation for each user. We reformulate the problem as a Lagrangian function,
which allows us to enforce the total power constraints systematically. Our
solution involves combining this Lagrangian framework with GNNs and iteratively
updating the Lagrange multipliers and resource allocation scheme. Unlike
existing GNN-based methods that limit each user to a single channel, JCPGNN-M
supports efficient spectrum reuse and scales well in dense network scenarios.
Simulation results show that JCPGNN-M achieves better data rate compared to
eWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and
it can generalize well to larger networks.

</details>


### [74] [Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid](https://arxiv.org/abs/2506.03817)
*Julius Gonsior, Tim Rieß, Anja Reusch, Claudio Hartmann, Maik Thiele, Wolfgang Lehner*

**主要类别:** cs.LG

**AI概要:** 这篇论文研究了主动学习中的超参数选择问题，通过大规模实验分析了超参数对结果的影响，并提出了改进实验设计的建议以提升研究的可重复性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管主动学习（AL）已有数十年的历史，但在实际应用中仍然很少使用，主要是因为设置AL的复杂性和对其效果缺乏信任，这通常归因于AL的大超参数空间，导致误导性和不可再现的AL实验结果。

**方法:** 首先编译了一个超过460万个超参数组合的大超参数网格，其次记录了所有组合在迄今为止最大规模的AL研究中的表现，最后分析了每个超参数在实验结果中的影响。

**结果:** 研究结果包括对每个超参数影响的分析、具体AL策略实现的意外影响展示，以及为可重复的AL实验设计提供的建议，旨在减少计算工作量并提高AL研究的可重复性和可信度。

**结论:** 该研究通过分析AL超参数空间对实验结果的影响，提供了关于每个超参数影响的建议，展示了具体AL策略实现的意外影响，并提出了最小计算努力下可重复的AL实验设计，从而有助于未来更可靠和可信的AL研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Survey+of+Active+Learning+Hyperparameters%3A+Insights+from+a+Large-Scale+Experimental+Grid，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03817，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03817&send_immediately=true&force_search=false)

**原文摘要:** Annotating data is a time-consuming and costly task, but it is inherently
required for supervised machine learning. Active Learning (AL) is an
established method that minimizes human labeling effort by iteratively
selecting the most informative unlabeled samples for expert annotation, thereby
improving the overall classification performance. Even though AL has been known
for decades, AL is still rarely used in real-world applications. As indicated
in the two community web surveys among the NLP community about AL, two main
reasons continue to hold practitioners back from using AL: first, the
complexity of setting AL up, and second, a lack of trust in its effectiveness.
We hypothesize that both reasons share the same culprit: the large
hyperparameter space of AL. This mostly unexplored hyperparameter space often
leads to misleading and irreproducible AL experiment results. In this study, we
first compiled a large hyperparameter grid of over 4.6 million hyperparameter
combinations, second, recorded the performance of all combinations in the
so-far biggest conducted AL study, and third, analyzed the impact of each
hyperparameter in the experiment results. In the end, we give recommendations
about the influence of each hyperparameter, demonstrate the surprising
influence of the concrete AL strategy implementation, and outline an
experimental study design for reproducible AL experiments with minimal
computational effort, thus contributing to more reproducible and trustworthy AL
research in the future.

</details>


### [75] [Learning task-specific predictive models for scientific computing](https://arxiv.org/abs/2506.03835)
*Jianyuan Yin, Qianxiao Li*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种任务特定的监督学习方法，用于下游任务中的模型评估，这种方法比传统的最小均方误差方法更有效。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在解决机器学习增强科学计算中常见的问题，即传统方法（如最小均方误差）无法有效适应某些下游任务的需求。

**方法:** 通过分析下游任务算法的支持集上的最大预测误差，提出了一种任务特定的监督学习问题，并基于训练数据开发了迭代算法来解决问题。

**结果:** 在轨迹预测、最优控制和最小能量路径计算的数值实验中，该方法展示了有效性。

**结论:** 论文表明，针对特定下游任务设计的学习方法可以显著提高模型的实际性能，为未来的研究提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+task-specific+predictive+models+for+scientific+computing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03835，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03835&send_immediately=true&force_search=false)

**原文摘要:** We consider learning a predictive model to be subsequently used for a given
downstream task (described by an algorithm) that requires access to the model
evaluation. This task need not be prediction, and this situation is frequently
encountered in machine-learning-augmented scientific computing. We show that
this setting differs from classical supervised learning, and in general it
cannot be solved by minimizing the mean square error of the model predictions
as is frequently performed in the literature. Instead, we find that the maximum
prediction error on the support of the downstream task algorithm can serve as
an effective estimate for the subsequent task performance. With this insight,
we formulate a task-specific supervised learning problem based on the given
sampling measure, whose solution serves as a reliable surrogate model for the
downstream task. Then, we discretize the empirical risk based on training data,
and develop an iterative algorithm to solve the task-specific supervised
learning problem. Three illustrative numerical examples on trajectory
prediction, optimal control and minimum energy path computation demonstrate the
effectiveness of the approach.

</details>


### [76] [Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning](https://arxiv.org/abs/2506.03850)
*Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Vulnerability-Aware Alignment (VAA)的新方法，旨在有效降低有害微调（HFT）的风险，并在多个任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法忽视了对齐数据中的脆弱模式，需要更针对性的解决方案来应对HFT带来的风险。

**方法:** 通过估计数据脆弱性，将数据分为“脆弱”和“不可侵”组，并采用Group DRO框架进行平衡学习。

**结果:** 实验表明，VAA显著降低了有害评分，且优于当前最先进的基线方法。

**结论:** VAA方法在减少有害微调影响方面表现优异，同时保持了任务性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Vulnerability-Aware+Alignment%3A+Mitigating+Uneven+Forgetting+in+Harmful+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03850，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03850&send_immediately=true&force_search=false)

**原文摘要:** Harmful fine-tuning (HFT), performed directly on open-source LLMs or through
Fine-tuning-as-a-Service, breaks safety alignment and poses significant
threats. Existing methods aim to mitigate HFT risks by learning robust
representation on alignment data or making harmful data unlearnable, but they
treat each data sample equally, leaving data vulnerability patterns
understudied. In this work, we reveal that certain subsets of alignment data
are consistently more prone to forgetting during HFT across different
fine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware
Alignment (VAA), which estimates data vulnerability, partitions data into
"vulnerable" and "invulnerable" groups, and encourages balanced learning using
a group distributionally robust optimization (Group DRO) framework.
Specifically, VAA learns an adversarial sampler that samples examples from the
currently underperforming group and then applies group-dependent adversarial
perturbations to the data during training, aiming to encourage a balanced
learning process across groups. Experiments across four fine-tuning tasks
demonstrate that VAA significantly reduces harmful scores while preserving
downstream task performance, outperforming state-of-the-art baselines.

</details>


### [77] [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/abs/2506.03857)
*Mingxuan Xia, Haobo Wang, Yixuan Li, Zewei Yu, Jindong Wang, Junbo Zhao, Runze Wu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的候选注释范式，通过大型语言模型输出所有可能的标签，并使用小型语言模型进行提炼，以解决现有方法在不确定时产生错误标签的问题。


<details>
  <summary>更多</summary>
  
**动机:** 由于现有方法在不确定时让大型语言模型决定单一黄金标签，导致数据质量问题，因此提出了新的候选注释范式。

**方法:** 开发了一个教师-学生框架CanDist，用于提炼候选注释，并提供了严格的理论保证。

**结果:** 在六个文本分类任务中进行了广泛的实验，验证了所提方法的有效性。

**结论:** 论文提出了一种新的候选注释范式，通过大型语言模型输出所有可能的标签，并利用小型语言模型进行提炼，以提高下游任务的数据质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prompt+Candidates%2C+then+Distill%3A+A+Teacher-Student+Framework+for+LLM-driven+Data+Annotation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03857&send_immediately=true&force_search=false)

**原文摘要:** Recently, Large Language Models (LLMs) have demonstrated significant
potential for data annotation, markedly reducing the labor costs associated
with downstream applications. However, existing methods mostly adopt an
aggressive strategy by prompting LLM to determine a single gold label for each
unlabeled sample. Due to the inherent uncertainty within LLMs, they often
produce incorrect labels for difficult samples, severely compromising the data
quality for downstream applications. Motivated by ambiguity aversion in human
behaviors, we propose a novel candidate annotation paradigm wherein large
language models are encouraged to output all possible labels when incurring
uncertainty. To ensure unique labels are provided for downstream tasks, we
develop a teacher-student framework CanDist that distills candidate annotations
with a Small Language Model (SLM). We further provide a rigorous justification
demonstrating that distilling candidate annotations from the teacher LLM offers
superior theoretical guarantees compared to directly using single annotations.
Extensive experiments across six text classification tasks validate the
effectiveness of our proposed method. The source code is available at
https://github.com/MingxuanXia/CanDist.

</details>


### [78] [Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](https://arxiv.org/abs/2506.03870)
*Mohd. Farhan Israk Soumik, Syed Mhamudul Hasan, Abdur R. Shahid*

**主要类别:** cs.LG

**AI概要:** 本文研究了Apple Intelligence的写作工具如何通过文本修改（如重写和语气调整）来防止大型语言模型被滥用于情绪推断攻击，从而增强用户隐私保护。


<details>
  <summary>更多</summary>
  
**动机:** 情绪推断攻击对用户隐私构成重大威胁，而Apple Intelligence的写作工具可能通过文本修改来减轻这些风险。

**方法:** 开发了专门用于评估文本修改如何影响基于LLM检测的早期新数据集，并对Apple Intelligence的写作工具进行了实证分析。

**结果:** 研究表明，不同的文本修改方式对基于LLM的情绪推断检测有显著影响，表明Apple Intelligence的写作工具可有效增强用户隐私保护。

**结论:** Apple Intelligence的写作工具在保护用户隐私方面具有巨大潜力，可以作为动态中和敏感情感内容的自适应重写系统的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Apple+Intelligence%27s+Writing+Tools+for+Privacy+Against+Large+Language+Model-Based+Inference+Attacks%3A+Insights+from+Early+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03870，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03870&send_immediately=true&force_search=false)

**原文摘要:** The misuse of Large Language Models (LLMs) to infer emotions from text for
malicious purposes, known as emotion inference attacks, poses a significant
threat to user privacy. In this paper, we investigate the potential of Apple
Intelligence's writing tools, integrated across iPhone, iPad, and MacBook, to
mitigate these risks through text modifications such as rewriting and tone
adjustment. By developing early novel datasets specifically for this purpose,
we empirically assess how different text modifications influence LLM-based
detection. This capability suggests strong potential for Apple Intelligence's
writing tools as privacy-preserving mechanisms. Our findings lay the groundwork
for future adaptive rewriting systems capable of dynamically neutralizing
sensitive emotional content to enhance user privacy. To the best of our
knowledge, this research provides the first empirical analysis of Apple
Intelligence's text-modification tools within a privacy-preservation context
with the broader goal of developing on-device, user-centric privacy-preserving
mechanisms to protect against LLMs-based advanced inference attacks on deployed
systems.

</details>


### [79] [Temporal horizons in forecasting: a performance-learnability trade-off](https://arxiv.org/abs/2506.03889)
*Pau Vilimelis Aceituno, Jack William Miller, Noah Marti, Youssef Farag, Victor Boussange*

**主要类别:** cs.LG

**AI概要:** 本文研究了自回归模型训练中的最佳预测时间范围问题，提出了基于系统类型选择合适的训练视野的原则方法。


<details>
  <summary>更多</summary>
  
**动机:** 在训练用于动态系统的自回归模型时，如何选择适当的预测未来时间范围是一个关键问题。

**方法:** 通过分析损失景观的几何特性随训练视野的变化来研究自回归模型的预测能力。

**结果:** 论文证明了对于混沌系统，损失景观的粗糙度随着训练视野呈指数增长，而对于极限环系统则呈线性增长。此外，长期视野训练的模型在短期预测中表现良好，而短期训练的模型在长期预测中表现较差。

**结论:** 论文得出结论，训练预测的视野长度对于模型性能至关重要，并为选择训练视野提供了理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Temporal+horizons+in+forecasting%3A+a+performance-learnability+trade-off，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03889，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03889&send_immediately=true&force_search=false)

**原文摘要:** When training autoregressive models for dynamical systems, a critical
question arises: how far into the future should the model be trained to
predict? Too short a horizon may miss long-term trends, while too long a
horizon can impede convergence due to accumulating prediction errors. In this
work, we formalize this trade-off by analyzing how the geometry of the loss
landscape depends on the training horizon. We prove that for chaotic systems,
the loss landscape's roughness grows exponentially with the training horizon,
while for limit cycles, it grows linearly, making long-horizon training
inherently challenging. However, we also show that models trained on long
horizons generalize well to short-term forecasts, whereas those trained on
short horizons suffer exponentially (resp. linearly) worse long-term
predictions in chaotic (resp. periodic) systems. We validate our theory through
numerical experiments and discuss practical implications for selecting training
horizons. Our results provide a principled foundation for hyperparameter
optimization in autoregressive forecasting models.

</details>


### [80] [Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods](https://arxiv.org/abs/2506.03910)
*Shyam Prabhu, P Akshay Kumar, Antov Selwinston, Pavan Taduvai, Shreya Bairi, Rohit Batra*

**主要类别:** cs.LG

**AI概要:** 本研究展示了机器学习方法如何克服传统实验设计方法在材料设计问题中的局限性，并证明了其在准确性与效率方面的优势。


<details>
  <summary>更多</summary>
  
**动机:** 材料设计问题通常需要优化多个变量，使得全面因子探索变得不切实际。传统的实验设计（DOE）方法，如Taguchi技术，虽然常用于高效地采样设计空间，但它们本质上无法捕捉过程变量的非线性依赖关系。

**方法:** 比较了Taguchi方法与基于主动学习的高斯过程回归（GPR）模型在金属丝弧增材制造（WAAM）过程中的性能。Taguchi方法使用了一个三因素、五水平L25正交阵列来建议焊接参数，而GPR模型则使用了基于不确定性的探索获取函数，并结合拉丁超立方采样进行初始训练数据采集。

**结果:** 通过15个测试案例评估两种模型的准确性和效率，GPR在两个指标上均优于Taguchi方法。

**结论:** 机器学习方法，特别是基于主动学习的高斯过程回归模型，在材料设计问题中比传统的Taguchi方法更准确和高效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Experimental+Efficiency+in+Materials+Design%3A+A+Comparative+Study+of+Taguchi+and+Machine+Learning+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03910，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03910&send_immediately=true&force_search=false)

**原文摘要:** Materials design problems often require optimizing multiple variables,
rendering full factorial exploration impractical. Design of experiment (DOE)
methods, such as Taguchi technique, are commonly used to efficiently sample the
design space but they inherently lack the ability to capture non-linear
dependency of process variables. In this work, we demonstrate how machine
learning (ML) methods can be used to overcome these limitations. We compare the
performance of Taguchi method against an active learning based Gaussian process
regression (GPR) model in a wire arc additive manufacturing (WAAM) process to
accurately predict aspects of bead geometry, including penetration depth, bead
width, and height. While Taguchi method utilized a three-factor, five-level L25
orthogonal array to suggest weld parameters, the GPR model used an
uncertainty-based exploration acquisition function coupled with latin hypercube
sampling for initial training data. Accuracy and efficiency of both models was
evaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.
This work applies to broader materials processing domain requiring efficient
exploration of complex parameters.

</details>


### [81] [Learning Fair And Effective Points-Based Rewards Programs](https://arxiv.org/abs/2506.03911)
*Chamsi Hssaine, Yichun Hu, Ciara Pike-Burke*

**主要类别:** cs.LG

**AI概要:** 该研究探讨了如何公平设计积分奖励计划，在公平性与收益间找到平衡，并提出了一种高效学习算法。


<details>
  <summary>更多</summary>
  
**动机:** 由于基于积分的奖励计划被指控存在不公平做法，因此需要公平地设计这些计划。

**方法:** 通过理论分析和数值实验，研究了个性化策略与统一兑换阈值的收益差距以及学习算法的设计。

**结果:** 论文显示使用统一兑换阈值的收益损失不超过 $1+\ln 2$，并提出一种限制兑换阈值调整次数的学习算法，实现了期望下的最优遗憾界。此外，通过数值实验验证了平均情况下个性化价值有限且所提算法性能优越。

**结论:** 论文得出了一种在公平性与收益之间取得平衡的积分奖励计划设计方法，并提出了具有实践性能的学习算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Fair+And+Effective+Points-Based+Rewards+Programs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03911，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03911&send_immediately=true&force_search=false)

**原文摘要:** Points-based rewards programs are a prevalent way to incentivize customer
loyalty; in these programs, customers who make repeated purchases from a seller
accumulate points, working toward eventual redemption of a free reward. These
programs have recently come under scrutiny due to accusations of unfair
practices in their implementation. Motivated by these concerns, we study the
problem of fairly designing points-based rewards programs, with a focus on two
obstacles that put fairness at odds with their effectiveness. First, due to
customer heterogeneity, the seller should set different redemption thresholds
for different customers to generate high revenue. Second, the relationship
between customer behavior and the number of accumulated points is typically
unknown; this requires experimentation which may unfairly devalue customers'
previously earned points. We first show that an individually fair rewards
program that uses the same redemption threshold for all customers suffers a
loss in revenue of at most a factor of $1+\ln 2$, compared to the optimal
personalized strategy that differentiates between customers. We then tackle the
problem of designing temporally fair learning algorithms in the presence of
demand uncertainty. Toward this goal, we design a learning algorithm that
limits the risk of point devaluation due to experimentation by only changing
the redemption threshold $O(\log T)$ times, over a horizon of length $T$. This
algorithm achieves the optimal (up to polylogarithmic factors)
$\widetilde{O}(\sqrt{T})$ regret in expectation. We then modify this algorithm
to only ever decrease redemption thresholds, leading to improved fairness at a
cost of only a constant factor in regret. Extensive numerical experiments show
the limited value of personalization in average-case settings, in addition to
demonstrating the strong practical performance of our proposed learning
algorithms.

</details>


### [82] [Learning equivariant models by discovering symmetries with learnable augmentations](https://arxiv.org/abs/2506.03914)
*Eduardo Santos Escriche, Stefanie Jegelka*

**主要类别:** cs.LG

**AI概要:** 本文提出了SEMoLA方法，在不依赖对称性先验知识的前提下，通过可学习数据增强发现数据中的对称性并将其近似等变性嵌入模型，实现了高预测准确率、良好的可解释性以及对分布偏移的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的两种方法均有局限性：软等变需要对称性的先验知识，而仅通过任务和更大数据隐式推断对称性缺乏可解释性。因此需要一种既能自动发现对称性又能保持模型鲁棒性和可解释性的方法。

**方法:** 提出SEMoLA方法，通过可学习的数据增强来发现未知对称性，并将相应的近似等变性以软编码方式嵌入到模型中。

**结果:** SEMoLA在多个数据集上能够稳健地发现相关对称性，同时实现高预测准确率，适用于多种数据模态和潜在对称群。

**结论:** SEMoLA能够在不需要对称性先验知识的情况下，以端到端的方式发现数据中的相关对称性，并将其近似等变性编码到任意无约束模型中，从而提供可解释性和对分布偏移的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+equivariant+models+by+discovering+symmetries+with+learnable+augmentations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03914，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03914&send_immediately=true&force_search=false)

**原文摘要:** Recently, a trend has emerged that favors learning relevant symmetries from
data in geometric domains instead of designing constrained architectures. To do
so, two popular options are (1) to modify the training protocol, e.g., with a
specific loss and data augmentations (soft equivariance), or (2) to ignore
equivariance and infer it only implicitly. However, both options have
limitations: soft equivariance requires a priori knowledge about relevant
symmetries, while inferring symmetries merely via the task and larger data
lacks interpretability. To address both limitations, we propose SEMoLA, an
end-to-end approach that jointly (1) discovers a priori unknown symmetries in
the data via learnable data augmentations, and (2) softly encodes the
respective approximate equivariance into an arbitrary unconstrained model.
Hence, it does not need prior knowledge about symmetries, it offers
interpretability, and it maintains robustness to distribution shifts.
Empirically, we demonstrate the ability of SEMoLA to robustly discover relevant
symmetries while achieving high prediction accuracy across various datasets,
encompassing multiple data modalities and underlying symmetry groups.

</details>


### [83] [Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win](https://arxiv.org/abs/2506.03919)
*Lorenz Kummer, Samir Moustafa, Anatol Ehrlich, Franka Bause, Nikolaus Suess, Wilfried N. Gansterer, Nils M. Kriege*

**主要类别:** cs.LG

**AI概要:** 本研究为图神经网络中的彩票假说提供了新的理论基础，表明保持稀疏初始化子网络的表达能力对于维持预测性能至关重要。


<details>
  <summary>更多</summary>
  
**动机:** 虽然彩票假说（LTH）在卷积神经网络中得到了充分研究，但其在图神经网络（GNNs）中的有效性主要依赖经验验证，缺乏理论支持。

**方法:** 通过建立稀疏初始化的GNN的表达能力与完整网络相当的条件，并结合Weisfeiler-Leman测试进行分析，提出并证明了强表达性中奖彩票假说。

**结果:** 作者证明了在特定条件下，稀疏初始化的GNN可以匹配完整网络的表达能力，并展示了提高初始化表达能力对模型收敛速度和泛化性能的积极影响。

**结论:** 该论文得出结论，在稀疏初始化的GNN中保持表达能力对于找到保留预测性能的“中奖彩票”至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weisfeiler+and+Leman+Go+Gambling%3A+Why+Expressive+Lottery+Tickets+Win，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03919，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03919&send_immediately=true&force_search=false)

**原文摘要:** The lottery ticket hypothesis (LTH) is well-studied for convolutional neural
networks but has been validated only empirically for graph neural networks
(GNNs), for which theoretical findings are largely lacking. In this paper, we
identify the expressivity of sparse subnetworks, i.e. their ability to
distinguish non-isomorphic graphs, as crucial for finding winning tickets that
preserve the predictive performance. We establish conditions under which the
expressivity of a sparsely initialized GNN matches that of the full network,
particularly when compared to the Weisfeiler-Leman test, and in that context
put forward and prove a Strong Expressive Lottery Ticket Hypothesis. We
subsequently show that an increased expressivity in the initialization
potentially accelerates model convergence and improves generalization. Our
findings establish novel theoretical foundations for both LTH and GNN research,
highlighting the importance of maintaining expressivity in sparsely initialized
GNNs. We illustrate our results using examples from drug discovery.

</details>


### [84] [FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review](https://arxiv.org/abs/2506.03938)
*Cédric Léonard, Dirk Stober, Martin Schulz*

**主要类别:** cs.LG

**AI概要:** 本文综述了在FPGA上部署机器学习模型用于遥感应用的研究，提出了两种分类法并保证研究的透明性和可重复性。


<details>
  <summary>更多</summary>
  
**动机:** 无人机技术和NewSpace时代正在改变地球观测任务和数据采集方式，需要机载实时决策以应对带宽限制。

**方法:** 作者系统地分析了66个实验，并遵循PRISMA 2020指南以确保透明度和可重复性，同时公开所有数据和代码。

**结果:** 研究介绍了两种分类法，分别用于捕捉高效的模型架构和FPGA实现策略，从而提高遥感数据处理的效率。

**结论:** 本论文得出结论，机器学习模型在FPGA上的部署为遥感应用提供了高效和适应性强的解决方案，并通过两个分类法展示了有效的模型架构和实现策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FPGA-Enabled+Machine+Learning+Applications+in+Earth+Observation%3A+A+Systematic+Review，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03938，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03938&send_immediately=true&force_search=false)

**原文摘要:** New UAV technologies and the NewSpace era are transforming Earth Observation
missions and data acquisition. Numerous small platforms generate large data
volume, straining bandwidth and requiring onboard decision-making to transmit
high-quality information in time. While Machine Learning allows real-time
autonomous processing, FPGAs balance performance with adaptability to
mission-specific requirements, enabling onboard deployment. This review
systematically analyzes 66 experiments deploying ML models on FPGAs for Remote
Sensing applications. We introduce two distinct taxonomies to capture both
efficient model architectures and FPGA implementation strategies. For
transparency and reproducibility, we follow PRISMA 2020 guidelines and share
all data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.

</details>


### [85] [Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective](https://arxiv.org/abs/2506.03951)
*Aojun Lu, Hangjie Yuan, Tao Feng, Yanan Sun*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个新的持续学习框架Dual-Arch，它在架构层面上解决了稳定性与可塑性之间的冲突，并且比现有方法更有效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习方法通常忽视了网络结构对稳定性和可塑性的影响，将这种权衡限制在参数级别。

**方法:** 提出了一种名为Dual-Arch的新框架，并进行了广泛的实验来验证其效果。

**结果:** 实验证明，Dual-Arch不仅提升了现有持续学习方法的性能，而且在参数数量上最多减少了87%。

**结论:** Dual-Arch通过结合两个专门设计的网络，一个用于可塑性，另一个用于稳定性，从而在架构层面解决稳定性-可塑性困境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Stability-Plasticity+Trade-off+in+Continual+Learning+from+an+Architectural+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03951，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03951&send_immediately=true&force_search=false)

**原文摘要:** The quest for Continual Learning (CL) seeks to empower neural networks with
the ability to learn and adapt incrementally. Central to this pursuit is
addressing the stability-plasticity dilemma, which involves striking a balance
between two conflicting objectives: preserving previously learned knowledge and
acquiring new knowledge. While numerous CL methods aim to achieve this
trade-off, they often overlook the impact of network architecture on stability
and plasticity, restricting the trade-off to the parameter level. In this
paper, we delve into the conflict between stability and plasticity at the
architectural level. We reveal that under an equal parameter constraint, deeper
networks exhibit better plasticity, while wider networks are characterized by
superior stability. To address this architectural-level dilemma, we introduce a
novel framework denoted Dual-Arch, which serves as a plug-in component for CL.
This framework leverages the complementary strengths of two distinct and
independent networks: one dedicated to plasticity and the other to stability.
Each network is designed with a specialized and lightweight architecture,
tailored to its respective objective. Extensive experiments demonstrate that
Dual-Arch enhances the performance of existing CL methods while being up to 87%
more compact in terms of parameters.

</details>


### [86] [Adapt before Continual Learning](https://arxiv.org/abs/2506.03956)
*Aojun Lu, Tao Feng, Hangjie Yuan, Chunhui Ding, Yanan Sun*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的持续学习框架ACL，通过在学习新任务前优化预训练模型，有效平衡了稳定性与可塑性之间的权衡。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习方法通常冻结预训练模型以保持稳定性，但限制了其可塑性；而对整个预训练模型进行顺序微调可能导致灾难性遗忘。因此需要一种平衡稳定性和可塑性的新方法。

**方法:** 提出了一种新的框架ACL，通过在核心CL过程之前进行插件式的适配阶段来优化预训练模型（PTM）主干。

**结果:** 实验表明，ACL在多个基准测试中显著提升了持续学习的性能，并且可以与现有方法集成。

**结论:** 论文得出结论，ACL方法在保持稳定性的同时提高了可塑性，为基于预训练模型的持续学习提供了一个通用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapt+before+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03956，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03956&send_immediately=true&force_search=false)

**原文摘要:** Continual Learning (CL) seeks to enable neural networks to incrementally
acquire new knowledge (plasticity) while retaining existing knowledge
(stability). While pre-trained models (PTMs) have become pivotal in CL,
prevailing approaches freeze the PTM backbone to preserve stability, limiting
their plasticity, particularly when encountering significant domain gaps in
incremental tasks. Conversely, sequentially finetuning the entire PTM risks
catastrophic forgetting of generalizable knowledge, exposing a critical
stability-plasticity trade-off. To address this challenge, we propose Adapting
PTMs before the core CL process (ACL), a novel framework that refines the PTM
backbone through a plug-and-play adaptation phase before learning each new task
with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by
aligning embeddings with their original class prototypes while distancing them
from others, theoretically and empirically shown to balance stability and
plasticity. Extensive experiments demonstrate that ACL significantly improves
CL performance across benchmarks and integrated methods, offering a versatile
solution for PTM-based CL.

</details>


### [87] [Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks](https://arxiv.org/abs/2506.03996)
*Lianfeng Shi, Ao Li, Benjamin Ward-Cherrier*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种用于Spiking Neural Networks (SNNs)的新型高效压缩框架Optimal Spiking Brain Compression (OSBC)，旨在降低SNNs在神经形态硬件上的计算和存储成本。


<details>
  <summary>更多</summary>
  
**动机:** 为了减少预训练或非常大的SNNs的成本，提高SNNs在神经形态硬件上的效率。

**方法:** 提出了一种新的单次后训练剪枝/量化框架Optimal Spiking Brain Compression (OSBC)，通过最小化脉冲神经元膜电位损失进行压缩。

**结果:** 实验表明，OSBC在N-MNIST、CIFAR10-DVS和DVS128-Gesture数据集上分别实现了97%的稀疏度，准确率损失分别为1.41%，10.20%和1.74%。

**结论:** OSBC是一种高效的SNN压缩方法，适用于资源受限的神经形态硬件。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Spiking+Brain+Compression%3A+Improving+One-Shot+Post-Training+Pruning+and+Quantization+for+Spiking+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03996&send_immediately=true&force_search=false)

**原文摘要:** Spiking Neural Networks (SNNs) have emerged as a new generation of
energy-efficient neural networks suitable for implementation on neuromorphic
hardware. As neuromorphic hardware has limited memory and computing resources,
weight pruning and quantization have recently been explored to improve SNNs'
efficiency. State-of-the-art SNN pruning/quantization methods employ multiple
compression and training iterations, increasing the cost for pre-trained or
very large SNNs. In this paper, we propose a new one-shot post-training
pruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that
adapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and
Alistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input
current as OBC does, OSBC achieves more efficient and accurate SNN compression
in one pass by minimizing the loss on spiking neuron membrane potential with a
small sample dataset. Our experiments on neuromorphic datasets (N-MNIST,
CIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity
through pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric
quantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code
will be available on GitHub.

</details>


### [88] [On the Usage of Gaussian Process for Efficient Data Valuation](https://arxiv.org/abs/2506.04026)
*Clément Bénesse, Patrick Mesana, Athénaïs Gautier, Sébastien Gambs*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种结合贝叶斯理论与高效计算的新方法，用于机器学习中的数据估值。


<details>
  <summary>更多</summary>
  
**动机:** 了解数据点对模型训练的影响是一个重要的任务，称为数据估值。

**方法:** 设计了一种新的标准分解方法，并利用高斯过程来估计子模型的效用函数。

**结果:** 论文提出了一种快速估算估值的方法，且能够将任何数据估值方法分析为效用函数与聚合过程的组合。

**结论:** 该论文提出了一种基于贝叶斯理论的新型数据估值方法，具有理论基础和实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Usage+of+Gaussian+Process+for+Efficient+Data+Valuation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04026，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04026&send_immediately=true&force_search=false)

**原文摘要:** In machine learning, knowing the impact of a given datum on model training is
a fundamental task referred to as Data Valuation. Building on previous works
from the literature, we have designed a novel canonical decomposition allowing
practitioners to analyze any data valuation method as the combination of two
parts: a utility function that captures characteristics from a given model and
an aggregation procedure that merges such information. We also propose to use
Gaussian Processes as a means to easily access the utility function on
``sub-models'', which are models trained on a subset of the training set. The
strength of our approach stems from both its theoretical grounding in Bayesian
theory, and its practical reach, by enabling fast estimation of valuations
thanks to efficient update formulae.

</details>


### [89] [Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence](https://arxiv.org/abs/2506.04053)
*Alexander Semenenko, Ivan Butakov, Alexey Frolov, Ivan Oseledets*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Curse+of+Slicing%3A+Why+Sliced+Mutual+Information+is+a+Deceptive+Measure+of+Statistical+Dependence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04053，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04053&send_immediately=true&force_search=false)

**原文摘要:** Sliced Mutual Information (SMI) is widely used as a scalable alternative to
mutual information for measuring non-linear statistical dependence. Despite its
advantages, such as faster convergence, robustness to high dimensionality, and
nullification only under statistical independence, we demonstrate that SMI is
highly susceptible to data manipulation and exhibits counterintuitive behavior.
Through extensive benchmarking and theoretical analysis, we show that SMI
saturates easily, fails to detect increases in statistical dependence (even
under linear transformations designed to enhance the extraction of
information), prioritizes redundancy over informative content, and in some
cases, performs worse than simpler dependence measures like the correlation
coefficient.

</details>


### [90] [Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning](https://arxiv.org/abs/2506.04071)
*Luiz Manella Pereira, M. Hadi Amini*

**主要类别:** cs.LG

**AI概要:** 为了解决联邦学习中因数据不平衡导致的模型性能下降问题，本文提出了一种基于最优传输和Wasserstein重心的预处理方法，并在CIFAR-10数据集上展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中由于本地数据集标签分布不均衡导致模型性能下降，需要一种有效的方法来对齐数据分布，提高模型准确性和效率。

**方法:** 引入了基于Wasserstein重心的预处理算法，将本地数据集投影到由中心服务器生成的目标RGB空间中，以减少样本间的方差并优化全局模型聚合。

**结果:** 在CIFAR-10数据集上验证了所提方法的有效性，结果显示该方法能够在更少通信轮次下达到更高的模型泛化能力。

**结论:** 本文提出了一种基于最优传输的预处理算法，通过最小化边缘设备上的数据分布差异来改善联邦学习中的性能下降问题。实验表明该方法能够在较少通信轮次下实现更高的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Transport-based+Domain+Alignment+as+a+Preprocessing+Step+for+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04071，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04071&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) is a subfield of machine learning that avoids sharing
local data with a central server, which can enhance privacy and scalability.
The inability to consolidate data leads to a unique problem called dataset
imbalance, where agents in a network do not have equal representation of the
labels one is trying to learn to predict. In FL, fusing locally-trained models
with unbalanced datasets may deteriorate the performance of global model
aggregation, and reduce the quality of updated local models and the accuracy of
the distributed agents' decisions. In this work, we introduce an Optimal
Transport-based preprocessing algorithm that aligns the datasets by minimizing
the distributional discrepancy of data along the edge devices. We accomplish
this by leveraging Wasserstein barycenters when computing channel-wise
averages. These barycenters are collected in a trusted central server where
they collectively generate a target RGB space. By projecting our dataset
towards this target space, we minimize the distributional discrepancy on a
global level, which facilitates the learning process due to a minimization of
variance across the samples. We demonstrate the capabilities of the proposed
approach over the CIFAR-10 dataset, where we show its capability of reaching
higher degrees of generalization in fewer communication rounds.

</details>


### [91] [Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems](https://arxiv.org/abs/2506.04126)
*Yujun Kim, Jaeyoung Cha, Chulhee Yun*

**主要类别:** cs.LG

**AI概要:** 该论文分析了在小epoch环境下，基于排列的SGD（如IGD）的收敛特性，并指出在某些条件下其表现可能显著劣于预期。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究表明，在大epoch环境下，基于排列的SGD（如随机重排SGD）比均匀采样SGD具有更快的收敛速度，但在epoch数K小于条件数κ的小epoch环境下，对其性能知之甚少。本文旨在填补这一空白。

**方法:** 研究采用了理论分析和数学证明的方法，对Incremental Gradient Descent (IGD) 在特定条件下的表现进行了探讨，并得出了收敛特性的结论。

**结果:** 论文揭示了在小epoch环境下，即使所有组件函数均为强凸函数，IGD仍可能表现出显著缓慢的收敛速度；而当允许部分组件函数为非凸时，IGD的最优间隙在整个小epoch环境下可能会更加恶化。

**结论:** 论文得出，基于排列的SGD在小epoch范围内，根据组件函数的假设不同，其收敛特性可能会有很大差异。此外，在大epoch范围内，IGD具有紧密的上下限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Incremental+Gradient+Descent+with+Small+Epoch+Counts+is+Surprisingly+Slow+on+Ill-Conditioned+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04126，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04126&send_immediately=true&force_search=false)

**原文摘要:** Recent theoretical results demonstrate that the convergence rates of
permutation-based SGD (e.g., random reshuffling SGD) are faster than
uniform-sampling SGD; however, these studies focus mainly on the large epoch
regime, where the number of epochs $K$ exceeds the condition number $\kappa$.
In contrast, little is known when $K$ is smaller than $\kappa$, and it is still
a challenging open question whether permutation-based SGD can converge faster
in this small epoch regime (Safran and Shamir, 2021). As a step toward
understanding this gap, we study the naive deterministic variant, Incremental
Gradient Descent (IGD), on smooth and strongly convex functions. Our lower
bounds reveal that for the small epoch regime, IGD can exhibit surprisingly
slow convergence even when all component functions are strongly convex.
Furthermore, when some component functions are allowed to be nonconvex, we
prove that the optimality gap of IGD can be significantly worse throughout the
small epoch regime. Our analyses reveal that the convergence properties of
permutation-based SGD in the small epoch regime may vary drastically depending
on the assumptions on component functions. Lastly, we supplement the paper with
tight upper and lower bounds for IGD in the large epoch regime.

</details>


### [92] [Faster Approx. Top-K: Harnessing the Full Power of Two Stages](https://arxiv.org/abs/2506.04165)
*Yashas Samaga, Varun Yerram, Spandana Raj Babbula, Prateek Jain, Praneeth Netrapalli*

**主要类别:** cs.LG

**AI概要:** 这篇论文提出了一个改进的Top-K选择算法，通过理论分析和TPU上的实现验证了其优越性，即在不牺牲召回率的前提下提高了计算效率。


<details>
  <summary>更多</summary>
  
**动机:** Top-K选择问题在许多机器学习算法中经常出现，并且在加速器上成为瓶颈，而加速器是为密集矩阵乘法优化的。因此需要一种更快的近似Top-K算法来解决这个问题。

**方法:** 作者提出了一种广义的两阶段Top-K选择算法：第一阶段从每个分区中选择top-K'元素（1 ≤ K' ≤ K），第二阶段对这些元素进行排序并返回top-K元素。此外，他们还推导出了该广义算法的预期召回率表达式和原始算法的更紧的预期召回率界限。

**结果:** 研究表明，在第一阶段选择K' > 1并减少分区数可以更有效地降低第二阶段的输入规模，同时保持与原算法相同的预期召回率。此外，作者还推导出了原始算法预期召回率的一个更紧的边界，并在实际任务中验证了他们的算法性能。

**结论:** 本文得出了一种改进的Top-K选择算法，并在Cloud TPUv5e上实现了比原算法快一个数量级的速度提升，同时保持了相同的召回率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Faster+Approx.+Top-K%3A+Harnessing+the+Full+Power+of+Two+Stages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04165&send_immediately=true&force_search=false)

**原文摘要:** We consider the Top-$K$ selection problem, which aims to identify the
largest-$K$ elements from an array. Top-$K$ selection arises in many machine
learning algorithms and often becomes a bottleneck on accelerators, which are
optimized for dense matrix multiplications. To address this problem,
\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage
\textit{approximate} Top-$K$ algorithm: (i) partition the input array and
select the top-$1$ element from each partition, (ii) sort this \textit{smaller
subset} and return the top $K$ elements. In this paper, we consider a
generalized version of this algorithm, where the first stage selects top-$K'$
elements, for some $1 \leq K' \leq K$, from each partition. Our contributions
are as follows: (i) we derive an expression for the expected recall of this
generalized algorithm and show that choosing $K' > 1$ with fewer partitions in
the first stage reduces the input size to the second stage more effectively
while maintaining the same expected recall as the original algorithm, (ii) we
derive a bound on the expected recall for the original algorithm in
\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of
$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud
TPUv5e and achieve around an order of magnitude speedups over the original
algorithm without sacrificing recall on real-world tasks.

</details>


### [93] [Does Prompt Design Impact Quality of Data Imputation by LLMs?](https://arxiv.org/abs/2506.04172)
*Shreenidhi Srinivasan, Lydia Manikonda*

**主要类别:** cs.LG

**AI概要:** 这篇论文介绍了一种针对类别不平衡数据缺失问题的新型提示工程技术，用于利用大型语言模型生成合成数据。


<details>
  <summary>更多</summary>
  
**动机:** 生成逼真的合成表格数据是机器学习中的一个关键挑战，尤其是当这些数据包含类别不平衡问题时。这是研究的主要动机。

**方法:** 论文提出了一种新的、基于上下文学习能力的大型语言模型的标记感知数据插补方法，通过结构化的逐组CSV风格提示技术和消除输入提示中的无关上下文信息来实现。

**结果:** 实验结果表明，该方法能显著减少输入提示的大小，同时保持或提高插补质量，特别是对于相对较小的数据集。

**结论:** 论文总结指出，该研究所提出的提示设计方法在利用大语言模型进行合成数据生成方面具有重要意义，并希望激发更多关于利用LLMs和提示工程技术进行合成数据生成的研究与讨论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Prompt+Design+Impact+Quality+of+Data+Imputation+by+LLMs%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04172&send_immediately=true&force_search=false)

**原文摘要:** Generating realistic synthetic tabular data presents a critical challenge in
machine learning. It adds another layer of complexity when this data contain
class imbalance problems. This paper presents a novel token-aware data
imputation method that leverages the in-context learning capabilities of large
language models. This is achieved through the combination of a structured
group-wise CSV-style prompting technique and the elimination of irrelevant
contextual information in the input prompt. We test this approach with two
class-imbalanced binary classification datasets and evaluate the effectiveness
of imputation using classification-based evaluation metrics. The experimental
results demonstrate that our approach significantly reduces the input prompt
size while maintaining or improving imputation quality compared to our baseline
prompt, especially for datasets that are of relatively smaller in size. The
contributions of this presented work is two-fold -- 1) it sheds light on the
importance of prompt design when leveraging LLMs for synthetic data generation
and 2) it addresses a critical gap in LLM-based data imputation for
class-imbalanced datasets with missing data by providing a practical solution
within computational constraints. We hope that our work will foster further
research and discussions about leveraging the incredible potential of LLMs and
prompt engineering techniques for synthetic data generation.

</details>


### [94] [OpenThoughts: Data Recipes for Reasoning Models](https://arxiv.org/abs/2506.04178)
*Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, Ludwig Schmidt*

**主要类别:** cs.LG

**AI概要:** 该项目旨在创建开放源代码的推理模型训练数据集，并通过改进数据生成流程实现了与现有最先进模型相媲美的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前最先进的推理模型依赖于专有数据集，缺乏公开信息，因此需要创建开放源代码的数据集。

**方法:** 通过系统研究数据生成流程中的每一步，并进行1000多次受控实验来改进数据集。

**结果:** OpenThinker3-7B模型在AIME、LiveCodeBench和GPQA Diamond等标准推理基准测试中分别达到53%、51%和54%的最先进结果。

**结论:** OpenThoughts项目成功创建了开源数据集用于训练推理模型，并通过改进数据生成流程达到了最先进的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OpenThoughts%3A+Data+Recipes+for+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04178，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04178&send_immediately=true&force_search=false)

**原文摘要:** Reasoning models have made rapid progress on many benchmarks involving math,
code, and science. Yet, there are still many open questions about the best
training recipes for reasoning since state-of-the-art models often rely on
proprietary datasets with little to no public information available. To address
this, the goal of the OpenThoughts project is to create open-source datasets
for training reasoning models. After initial explorations, our OpenThoughts2-1M
dataset led to OpenThinker2-32B, the first model trained on public reasoning
data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as
AIME and LiveCodeBench. We then improve our dataset further by systematically
investigating each step of our data generation pipeline with 1,000+ controlled
experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples
and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves
state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,
and 54% on GPQA Diamond. All of our datasets and models are available on
https://openthoughts.ai.

</details>


### [95] [How to Use Graph Data in the Wild to Help Graph Anomaly Detection?](https://arxiv.org/abs/2506.04190)
*Yuxuan Cao, Jiarong Xu, Chen Zhao, Jiaan Wang, Carl Yang, Chunping Wang, Yang Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Wild-GAD的新框架，通过引入外部图数据提升图异常检测的效果，取得了优于现有方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于图结构数据中的异常存在标签稀缺、定义模糊和类型多样等问题，监督或半监督方法不可靠，而现有的无监督方法难以准确全面地捕获正常数据分布。因此需要利用外部数据辅助图异常检测。

**方法:** 开发了一种基于统一数据库UniWildGraph的框架Wild-GAD，并基于代表性与多样性制定选择标准以确定最适合的外部数据进行图异常检测。

**结果:** 实验表明，该框架在六个真实世界数据集上平均AUCROC提高了18%，AUCPR提高了32%。

**结论:** Wild-GAD框架在图异常检测任务中表现出色，与现有方法相比具有显著的性能优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+to+Use+Graph+Data+in+the+Wild+to+Help+Graph+Anomaly+Detection%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04190&send_immediately=true&force_search=false)

**原文摘要:** In recent years, graph anomaly detection has found extensive applications in
various domains such as social, financial, and communication networks. However,
anomalies in graph-structured data present unique challenges, including label
scarcity, ill-defined anomalies, and varying anomaly types, making supervised
or semi-supervised methods unreliable. Researchers often adopt unsupervised
approaches to address these challenges, assuming that anomalies deviate
significantly from the normal data distribution. Yet, when the available data
is insufficient, capturing the normal distribution accurately and
comprehensively becomes difficult. To overcome this limitation, we propose to
utilize external graph data (i.e., graph data in the wild) to help anomaly
detection tasks. This naturally raises the question: How can we use external
data to help graph anomaly detection tasks? To answer this question, we propose
a framework called Wild-GAD. It is built upon a unified database, UniWildGraph,
which comprises a large and diverse collection of graph data with broad domain
coverage, ample data volume, and a unified feature space. Further, we develop
selection criteria based on representativity and diversity to identify the most
suitable external data for anomaly detection task. Extensive experiments on six
real-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the
baseline methods, our framework has an average 18% AUCROC and 32% AUCPR
improvement over the best-competing methods.

</details>


### [96] [EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation](https://arxiv.org/abs/2506.04205)
*Jinghan Jia, Hadi Reisizadeh, Chongyu Fan, Nathalie Baracaldo, Mingyi Hong, Sijia Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出一种名为EPiC的方法，通过保留CoT轨迹的起始和结束部分来实现高效的推理模型训练，显著降低了训练开销且不损失准确性。


<details>
  <summary>更多</summary>
  
**动机:** 为了降低使用链式思维监督训练大语言模型的高昂成本，需要对冗长的CoT轨迹进行压缩。

**方法:** 提出了一种Edge-Preserving Condensation（EPiC）方法，选择性地保留CoT轨迹的初始和最终部分，去除中间部分。

**结果:** 实验表明，EPiC在多个模型家族和基准测试中减少了超过34%的训练时间，并在MATH500上实现了与完整CoT监督相当的推理精度。

**结论:** EPiC方法在保持答案准确性和模型生成连贯推理能力的同时，显著减少了训练时间，为高效推理模型蒸馏提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EPiC%3A+Towards+Lossless+Speedup+for+Reasoning+Training+through+Edge-Preserving+CoT+Condensation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04205&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown remarkable reasoning capabilities
when trained with chain-of-thought (CoT) supervision. However, the long and
verbose CoT traces, especially those distilled from large reasoning models
(LRMs) such as DeepSeek-R1, significantly increase training costs during the
distillation process, where a non-reasoning base model is taught to replicate
the reasoning behavior of an LRM. In this work, we study the problem of CoT
condensation for resource-efficient reasoning training, aimed at pruning
intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling
supervised model training on length-reduced CoT data while preserving both
answer accuracy and the model's ability to generate coherent reasoning. Our
rationale is that CoT traces typically follow a three-stage structure: problem
understanding, exploration, and solution convergence. Through empirical
analysis, we find that retaining the structure of the reasoning trace,
especially the early stage of problem understanding (rich in reflective cues)
and the final stage of solution convergence, is sufficient to achieve lossless
reasoning supervision. To this end, we propose an Edge-Preserving Condensation
method, EPiC, which selectively retains only the initial and final segments of
each CoT trace while discarding the middle portion. This design draws an
analogy to preserving the "edge" of a reasoning trajectory, capturing both the
initial problem framing and the final answer synthesis, to maintain logical
continuity. Experiments across multiple model families (Qwen and LLaMA) and
benchmarks show that EPiC reduces training time by over 34% while achieving
lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To
the best of our knowledge, this is the first study to explore thought-level CoT
condensation for efficient reasoning model distillation.

</details>


### [97] [A Few Moments Please: Scalable Graphon Learning via Moment Matching](https://arxiv.org/abs/2506.04206)
*Reza Ramezanpour, Victor M. Tenorio, Antonio G. Marques, Ashutosh Sabharwal, Santiago Segarra*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新颖的graphon估计方法，该方法利用隐式神经表示并通过匹配观察到的子图计数来进行直接恢复，从而实现了更高的准确性和计算效率，并提出了MomentMixup以提升学习效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的graphon估计方法在处理大规模网络时存在可扩展性和分辨率无关近似的问题，因此需要一种更高效的方法。

**方法:** 利用隐式神经表示（INRs）通过匹配观察图中的子图计数（即矩）来直接恢复graphon值，并引入了MomentMixup数据增强技术。

**结果:** 所提出的graphon估计方法在75%的基准设置中优于最先进的可扩展估计器，在其余情况下也与之相当；MomentMixup在大多数基准测试中提高了图分类的准确性。

**结论:** 本文提出了一种新的、可扩展的graphon估计方法，该方法通过匹配子图计数直接恢复graphon，并且避免了使用潜在变量建模和Gromov-Wasserstein距离计算所带来的复杂性。此外，MomentMixup技术提升了基于graphon的学习性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Few+Moments+Please%3A+Scalable+Graphon+Learning+via+Moment+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04206，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04206&send_immediately=true&force_search=false)

**原文摘要:** Graphons, as limit objects of dense graph sequences, play a central role in
the statistical analysis of network data. However, existing graphon estimation
methods often struggle with scalability to large networks and
resolution-independent approximation, due to their reliance on estimating
latent variables or costly metrics such as the Gromov-Wasserstein distance. In
this work, we propose a novel, scalable graphon estimator that directly
recovers the graphon via moment matching, leveraging implicit neural
representations (INRs). Our approach avoids latent variable modeling by
training an INR--mapping coordinates to graphon values--to match empirical
subgraph counts (i.e., moments) from observed graphs. This direct estimation
mechanism yields a polynomial-time solution and crucially sidesteps the
combinatorial complexity of Gromov-Wasserstein optimization. Building on
foundational results, we establish a theoretical guarantee: when the observed
subgraph motifs sufficiently represent those of the true graphon (a condition
met with sufficiently large or numerous graph samples), the estimated graphon
achieves a provable upper bound in cut distance from the ground truth.
Additionally, we introduce MomentMixup, a data augmentation technique that
performs mixup in the moment space to enhance graphon-based learning. Our
graphon estimation method achieves strong empirical performance--demonstrating
high accuracy on small graphs and superior computational efficiency on large
graphs--outperforming state-of-the-art scalable estimators in 75\% of benchmark
settings and matching them in the remaining cases. Furthermore, MomentMixup
demonstrated improved graph classification accuracy on the majority of our
benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [98] [Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments](https://arxiv.org/abs/2506.03205)
*Umberto Gonçalves de Sousa*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种先进的多智能体量子强化学习框架Q-ARDNS-Multi，其在复杂三维环境中表现出卓越的协调性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了提升复杂三维环境中的多智能体协调性能，设计一种新的量子强化学习框架。

**方法:** Q-ARDNS-Multi利用2量子比特电路进行动作选择，采用受人类认知启发的双记忆系统、用于智能体协作的共享记忆模块以及由奖励方差和内在动机调节的自适应探索策略，并在10×10×3 GridWorld环境中进行了评估。

**结果:** Q-ARDNS-Multi分别以99.6%和99.5%的成功率超越了MADDPG和SAC算法，在成功率、稳定性、导航效率和避障方面表现优异，平均达到210步到目标。

**结论:** Q-ARDNS-Multi通过结合量子计算、认知科学和多智能体强化学习，提供了一种可扩展的类人方法，适用于机器人、自主导航和不确定性决策等应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Q-ARDNS-Multi%3A+A+Multi-Agent+Quantum+Reinforcement+Learning+Framework+with+Meta-Cognitive+Adaptation+for+Complex+3D+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03205&send_immediately=true&force_search=false)

**原文摘要:** This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum
reinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,
where Q-ARDNS-Multi stands for "Quantum Adaptive Reward-Driven Neural Simulator
- Multi-Agent". It integrates quantum circuits with RY gates, meta-cognitive
adaptation, and multi-agent coordination mechanisms for complex 3D
environments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action
selection, a dual-memory system inspired by human cognition, a shared memory
module for agent cooperation, and adaptive exploration strategies modulated by
reward variance and intrinsic motivation. Evaluated in a $10 \times 10 \times
3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi
achieves success rates of 99.6\% and 99.5\% for Agents 0 and 1, respectively,
outperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft
Actor-Critic (SAC) in terms of success rate, stability, navigation efficiency,
and collision avoidance. The framework records mean rewards of $-304.2891 \pm
756.4636$ and $-295.7622 \pm 752.7103$, averaging 210 steps to goal,
demonstrating its robustness in dynamic settings. Comprehensive analyses,
including learning curves, reward distributions, statistical tests, and
computational efficiency evaluations, highlight the contributions of quantum
circuits and meta-cognitive adaptation. By bridging quantum computing,
cognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,
human-like approach for applications in robotics, autonomous navigation, and
decision-making under uncertainty.

</details>


### [99] [A Trustworthiness-based Metaphysics of Artificial Intelligence Systems](https://arxiv.org/abs/2506.03233)
*Andrea Ferrario*

**主要类别:** cs.AI

**AI概要:** 该论文挑战了AI系统作为人工物缺乏明确身份和持续性的传统观点，通过引入基于可信度的身份标准理论，为其相关讨论提供了形而上学基础。


<details>
  <summary>更多</summary>
  
**动机:** 尽管对现代AI系统有广泛的认识论和伦理学讨论，但它们的形而上学基础仍相对未被探索，因此需要挑战传统观点并提出新的身份识别标准。

**方法:** 基于Carrara和Vermaas的细粒度人工物种类理论，引入了AI系统形而上学身份的理论，并通过可信度分析其种类和身份标准。

**结果:** 提出了一个AI系统的身份识别理论，强调其可信度特征决定了其身份标准，从而为其认识论、伦理和法律讨论提供基础。

**结论:** 论文得出结论认为，AI系统的身份和持续性通过其可信度的视角来理解，并为关于这些人工物的认识论、伦理和法律讨论提供了坚实的形而上学基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Trustworthiness-based+Metaphysics+of+Artificial+Intelligence+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03233，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03233&send_immediately=true&force_search=false)

**原文摘要:** Modern AI systems are man-made objects that leverage machine learning to
support our lives across a myriad of contexts and applications. Despite
extensive epistemological and ethical debates, their metaphysical foundations
remain relatively under explored. The orthodox view simply suggests that AI
systems, as artifacts, lack well-posed identity and persistence conditions --
their metaphysical kinds are no real kinds. In this work, we challenge this
perspective by introducing a theory of metaphysical identity of AI systems. We
do so by characterizing their kinds and introducing identity criteria -- formal
rules that answer the questions "When are two AI systems the same?" and "When
does an AI system persist, despite change?" Building on Carrara and Vermaas'
account of fine-grained artifact kinds, we argue that AI trustworthiness
provides a lens to understand AI system kinds and formalize the identity of
these artifacts by relating their functional requirements to their physical
make-ups. The identity criteria of AI systems are determined by their
trustworthiness profiles -- the collection of capabilities that the systems
must uphold over time throughout their artifact histories, and their
effectiveness in maintaining these capabilities. Our approach suggests that the
identity and persistence of AI systems is sensitive to the socio-technical
context of their design and utilization via their trustworthiness, providing a
solid metaphysical foundation to the epistemological, ethical, and legal
discussions about these artifacts.

</details>


### [100] [Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback](https://arxiv.org/abs/2506.03315)
*Kai Sauerwald, Kenneth Skiba, Eduardo Fermé, Thomas Meyer*

**主要类别:** cs.AI

**AI概要:** 该论文探讨了受限选择结构的实现与公理化，提出了一种通过线性序构造选择函数的方法，并应用于理论更新和抽象论证。


<details>
  <summary>更多</summary>
  
**动机:** 论文动机是解决潜在选择集被限制的问题，在这种情况下，传统的基于替代关系的选择函数构造方法可能失效。

**方法:** 本文研究了如何利用线性序来实现选择函数，并对受限设置下的公理化进行了分析。

**结果:** 结果显示，即便是在受限设置中，使用替代集合上的线性序仍能有效构造选择函数，并讨论了其在知识表示和推理中的应用。

**结论:** 论文得出结论，即使在编码回退值为线性序中的最小元素时，也可以始终通过替代集合上的线性序来构造选择函数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Axiomatics+of+Restricted+Choices+by+Linear+Orders+of+Sets+with+Minimum+as+Fallback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03315，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03315&send_immediately=true&force_search=false)

**原文摘要:** We study how linear orders can be employed to realise choice functions for
which the set of potential choices is restricted, i.e., the possible choice is
not possible among the full powerset of all alternatives. In such restricted
settings, constructing a choice function via a relation on the alternatives is
not always possible. However, we show that one can always construct a choice
function via a linear order on sets of alternatives, even when a fallback value
is encoded as the minimal element in the linear order. The axiomatics of such
choice functions are presented for the general case and the case of
union-closed input restrictions. Restricted choice structures have applications
in knowledge representation and reasoning, and here we discuss their
applications for theory change and abstract argumentation.

</details>


### [101] [Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows](https://arxiv.org/abs/2506.03332)
*Yifei Ming, Zixuan Ke, Xuan-Phi Nguyen, Jiayu Wang, Shafiq Joty*

**主要类别:** cs.AI

**AI概要:** 这篇论文主要研究了在代理工作流程中使用反馈机制存在的问题，并提出了新的基准来评估这些系统的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管反馈驱动的改进前景广阔，但代理工作流的稳定性取决于评判者的可靠性。然而，评判者可能会产生幻觉信息、表现出偏见或采取对抗行为，这给工作流程带来了关键的漏洞。

**方法:** 作者引入了一个二维框架来分析法官行为，并在此基础上构建了一套法官行为体系，并开发了WAFER-QA这一新基准来评估代理工作流程对事实支持对抗性反馈的鲁棒性。

**结果:** 研究揭示了即使是最强的代理也容易受到具有说服力但有缺陷的批评的影响，在一轮误导性反馈后常常改变正确的答案。此外，还研究了模型预测在多轮互动中的演变情况，揭示了推理模型和非推理模型之间的不同行为模式。

**结论:** 论文得出的结论是，基于反馈的工作流程存在根本性的漏洞，尤其是在面对有误导性的反馈时。同时，研究也为构建更稳健的代理系统提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Helpful+Agent+Meets+Deceptive+Judge%3A+Understanding+Vulnerabilities+in+Agentic+Workflows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03332&send_immediately=true&force_search=false)

**原文摘要:** Agentic workflows -- where multiple large language model (LLM) instances
interact to solve tasks -- are increasingly built on feedback mechanisms, where
one model evaluates and critiques another. Despite the promise of
feedback-driven improvement, the stability of agentic workflows rests on the
reliability of the judge. However, judges may hallucinate information, exhibit
bias, or act adversarially -- introducing critical vulnerabilities into the
workflow. In this work, we present a systematic analysis of agentic workflows
under deceptive or misleading feedback. We introduce a two-dimensional
framework for analyzing judge behavior, along axes of intent (from constructive
to malicious) and knowledge (from parametric-only to retrieval-augmented
systems). Using this taxonomy, we construct a suite of judge behaviors and
develop WAFER-QA, a new benchmark with critiques grounded in retrieved web
evidence to evaluate robustness of agentic workflows against factually
supported adversarial feedback. We reveal that even strongest agents are
vulnerable to persuasive yet flawed critiques -- often switching correct
answers after a single round of misleading feedback. Taking a step further, we
study how model predictions evolve over multiple rounds of interaction,
revealing distinct behavioral patterns between reasoning and non-reasoning
models. Our findings highlight fundamental vulnerabilities in feedback-based
workflows and offer guidance for building more robust agentic systems.

</details>


### [102] [Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration](https://arxiv.org/abs/2506.03469)
*Tuan Le, Risal Shefin, Debashis Gupta, Thai Le, Sarra Alqahtani*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种结合可解释性、模型检验和风险引导反例验证的混合框架，以提高高风险环境中强化学习策略的安全性。


<details>
  <summary>更多</summary>
  
**动机:** 在高风险环境中确保强化学习策略的安全性不仅需要形式化验证，还需要可解释性和针对性的反例验证。现有的模型检验方法受限于抽象质量和轨迹数据集的完备性，因此需要一种更全面的方法来解决这些问题。

**方法:** 首先使用可解释策略摘要（CAPS）构建人类可理解的RL策略抽象；接着利用Storm概率模型检验器验证时态安全规范的满足性；如果发现违规，则返回可解释的反例轨迹；如果没有发现违规，则通过估计风险来指导反例搜索策略，并提供PAC风格的未检测违规可能性保证；最后，在运行时超过阈值时切换到备用策略的安全保护机制被引入以缓解失败影响。

**结果:** 提出的混合框架能够通过可解释的抽象策略进行安全性验证，并在无法完全验证的情况下，通过风险评估引导反例搜索，同时提供了理论上的未检测违规可能性保证。此外，运行时的安全保护机制可以有效缓解策略失败的影响。

**结论:** 论文提出了一种混合框架，结合了解释性、模型检验和风险引导的反例验证，以实现高风险环境中强化学习策略的安全性保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Verification-Guided+Falsification+for+Safe+RL+via+Explainable+Abstraction+and+Risk-Aware+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03469&send_immediately=true&force_search=false)

**原文摘要:** Ensuring the safety of reinforcement learning (RL) policies in high-stakes
environments requires not only formal verification but also interpretability
and targeted falsification. While model checking provides formal guarantees,
its effectiveness is limited by abstraction quality and the completeness of the
underlying trajectory dataset. We propose a hybrid framework that integrates
(1) explainability, (2) model checking, and (3) risk-guided falsification to
achieve both rigor and coverage. Our approach begins by constructing a
human-interpretable abstraction of the RL policy using Comprehensible Abstract
Policy Summarization (CAPS). This abstract graph, derived from offline
trajectories, is both verifier-friendly, semantically meaningful, and can be
used as input to Storm probabilistic model checker to verify satisfaction of
temporal safety specifications. If the model checker identifies a violation, it
will return an interpretable counterexample trace by which the policy fails the
safety requirement. However, if no violation is detected, we cannot conclude
satisfaction due to potential limitation in the abstraction and coverage of the
offline dataset. In such cases, we estimate associated risk during model
checking to guide a falsification strategy that prioritizes searching in
high-risk states and regions underrepresented in the trajectory dataset. We
further provide PAC-style guarantees on the likelihood of uncovering undetected
violations. Finally, we incorporate a lightweight safety shield that switches
to a fallback policy at runtime when such a risk exceeds a threshold,
facilitating failure mitigation without retraining.

</details>


### [103] [Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis](https://arxiv.org/abs/2506.03503)
*Shan Shan*

**主要类别:** cs.AI

**AI概要:** 该研究通过整合量子力学与生成式人工智能，构建了一个用于模拟社会规范涌现与演化的框架，提出了基于量子原理的社会系统分析方法。


<details>
  <summary>更多</summary>
  
**动机:** 由于量子计算在社会理论中的相关性仍未得到充分探索，特别是在社会系统的系统级应用方面存在空白，因此需要填补这一空白。

**方法:** 结合量子力学与生成式人工智能，提出了一种理论和计算框架，利用量子叠加、纠缠和概率测量等核心概念，对25个生成代理进行五个理想类型实验的模拟。

**结果:** 关键发现表明，将量子原理与生成式人工智能结合，可以建模复杂社会系统中的不确定性和相互依存关系。模拟揭示了向规范秩序收敛、抵抗传播以及社会规则中新平衡的自发出现等模式。

**结论:** 本研究介绍了一种新的计算视角，为量子信息社会理论奠定了基础，并提供了跨学科的见解，将社会不仅视为观察的结构，还视为通过量子技术模拟和重新设计的动态系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Computational+Architects+of+Society%3A+Quantum+Machine+Learning+for+Social+Rule+Genesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03503，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03503&send_immediately=true&force_search=false)

**原文摘要:** The quantification of social science remains a longstanding challenge,
largely due to the philosophical nature of its foundational theories. Although
quantum computing has advanced rapidly in recent years, its relevance to social
theory remains underexplored. Most existing research focuses on micro-cognitive
models or philosophical analogies, leaving a gap in system-level applications
of quantum principles to the analysis of social systems. This study addresses
that gap by proposing a theoretical and computational framework that combines
quantum mechanics with Generative AI to simulate the emergence and evolution of
social norms. Drawing on core quantum concepts--such as superposition,
entanglement, and probabilistic measurement--this research models society as a
dynamic, uncertain system and sets up five ideal-type experiments. These
scenarios are simulated using 25 generative agents, each assigned evolving
roles as compliers, resistors, or enforcers. Within a simulated environment
monitored by a central observer (the Watcher), agents interact, respond to
surveillance, and adapt to periodic normative disruptions. These interactions
allow the system to self-organize under external stress and reveal emergent
patterns. Key findings show that quantum principles, when integrated with
generative AI, enable the modeling of uncertainty, emergence, and
interdependence in complex social systems. Simulations reveal patterns
including convergence toward normative order, the spread of resistance, and the
spontaneous emergence of new equilibria in social rules. In conclusion, this
study introduces a novel computational lens that lays the groundwork for a
quantum-informed social theory. It offers interdisciplinary insights into how
society can be understood not just as a structure to observe but as a dynamic
system to simulate and redesign through quantum technologies.

</details>


### [104] [CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications](https://arxiv.org/abs/2506.03543)
*Wanghao Ye, Sihan Chen, Yiting Wang, Shwai He, Bowei Tian, Guoheng Sun, Ziyi Wang, Ziyao Wang, Yexiao He, Zheyu Shen, Meng Liu, Yuning Zhang, Meng Feng, Yang Wang, Siyuan Peng, Yilong Dai, Zhenle Duan, Hanzhang Qin, Ang Li*

**主要类别:** cs.AI

**AI概要:** 这篇论文提出了一种结合全局工作区理论和新型人格测试的方法，以增强大型语言模型代理的心理真实性，并展示了其在模拟约会和职场匹配中的应用潜力。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大型语言模型代理缺乏真实的人类心理过程，这限制了它们在数字孪生和社会人工智能应用中的表现力和适用性。为了解决这一问题，研究者致力于增强LLM代理的心理真实性。

**方法:** 研究者们采用了全局工作区理论（GNWT）的计算实现方法，结合人类认知架构原则，构建了具有情感、记忆、社会规范、规划和目标跟踪等子代理的LLM系统，并开发了一种新的基于冒险的人格测试方法。

**结果:** 研究结果显示，基于551个GNWT代理和哥伦比亚大学快速约会数据集的验证表明，该模型与人类吸引力模式的相关性达到72%，匹配预测准确率达到77.8%，在人类验证研究中一致性达到74%。

**结论:** 该论文得出的结论是，通过将全局工作区理论（GNWT）计算化，并结合冒险式人格测试，可以显著提升大型语言模型代理的心理真实性，从而在智能约会平台和人力资源技术解决方案中实现应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CogniPair%3A+From+LLM+Chatbots+to+Conscious+AI+Agents+--+GNWT-Based+Multi-Agent+Digital+Twins+for+Social+Pairing+--+Dating+%26+Hiring+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03543，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03543&send_immediately=true&force_search=false)

**原文摘要:** Current large language model (LLM) agents lack authentic human psychological
processes necessary for genuine digital twins and social AI applications. To
address this limitation, we present a computational implementation of Global
Workspace Theory (GNWT) that integrates human cognitive architecture principles
into LLM agents, creating specialized sub-agents for emotion, memory, social
norms, planning, and goal-tracking coordinated through a global workspace
mechanism. However, authentic digital twins require accurate personality
initialization. We therefore develop a novel adventure-based personality test
that evaluates true personality through behavioral choices within interactive
scenarios, bypassing self-presentation bias found in traditional assessments.
Building on these innovations, our CogniPair platform enables digital twins to
engage in realistic simulated dating interactions and job interviews before
real encounters, providing bidirectional cultural fit assessment for both
romantic compatibility and workplace matching. Validation using 551 GNWT-Agents
and Columbia University Speed Dating dataset demonstrates 72% correlation with
human attraction patterns, 77.8% match prediction accuracy, and 74% agreement
in human validation studies. This work advances psychological authenticity in
LLM agents and establishes a foundation for intelligent dating platforms and HR
technology solutions.

</details>


### [105] [SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization](https://arxiv.org/abs/2506.03548)
*Chenglong Ye, Gang Xiong, Junyou Shang, Xingyuan Dai, Xiaoyan Gong, Yisheng Lv*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种新的交通仿真平台SUMO-MCP，它简化了传统工具的操作流程，提供了更多的辅助功能，使得用户可以通过简单的自然语言指令生成交通场景、创建需求、运行批量仿真并进行分析，从而提高交通仿真的易用性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的交通仿真工具如SUMO由于复杂的操作流程，对用户来说仍然具有挑战性。

**方法:** 开发了SUMO-MCP平台，将SUMO的核心工具封装到统一的工具套件中，并提供额外的辅助工具用于预处理和后处理任务。

**结果:** 实验表明，SUMO-MCP显著提高了交通仿真的可访问性和可靠性。

**结论:** SUMO-MCP使交通仿真对于研究人员来说更加易用和可靠，代码将在未来通过GitHub发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SUMO-MCP%3A+Leveraging+the+Model+Context+Protocol+for+Autonomous+Traffic+Simulation+and+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03548，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03548&send_immediately=true&force_search=false)

**原文摘要:** Traffic simulation tools, such as SUMO, are essential for urban mobility
research. However, such tools remain challenging for users due to complex
manual workflows involving network download, demand generation, simulation
setup, and result analysis. In this paper, we introduce SUMO-MCP, a novel
platform that not only wraps SUMO' s core utilities into a unified tool suite
but also provides additional auxiliary utilities for common preprocessing and
postprocessing tasks. Using SUMO-MCP, users can issue simple natural-language
prompts to generate traffic scenarios from OpenStreetMap data, create demand
from origin-destination matrices or random patterns, run batch simulations with
multiple signal-control strategies, perform comparative analyses with automated
reporting, and detect congestion for signal-timing optimization. Furthermore,
the platform allows flexible custom workflows by dynamically combining exposed
SUMO tools without additional coding. Experiments demonstrate that SUMO-MCP
significantly makes traffic simulation more accessible and reliable for
researchers. We will release code for SUMO-MCP at
https://github.com/ycycycl/SUMO-MCP in the future.

</details>


### [106] [Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach](https://arxiv.org/abs/2506.03586)
*Yu Ma, Chongtao Guo, Le Liang, Xiao Li, Shi Jin*

**主要类别:** cs.AI

**AI概要:** 该论文提出一种结合混合深度强化学习与多智能体策略的新方法，在RIS辅助OFDM系统中实现高效的相位设计与资源分配，显著降低平均延迟并提升系统性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了在用户数据包随机到达基站的情况下优化平均延迟，研究了下行链路可重构智能表面（RIS）辅助正交频分复用（OFDM）系统中的联合相位设计和资源分配问题。

**方法:** 采用了一种混合深度强化学习（DRL）方法，包括PPO-Θ用于优化RIS相移设计，PPO-N用于子载波分配决策，并引入多智能体策略以缓解维度灾难问题。

**结果:** 仿真结果表明，所提出的算法显著降低了平均延迟，提高了资源分配效率，并实现了比基线方法更优越的系统鲁棒性和公平性。

**结论:** 论文提出了一种混合深度强化学习方法，用于优化RIS辅助OFDM系统的下行链路中的相位设计和资源分配，有效降低了平均延迟并提高了系统鲁棒性和公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Joint+Beamforming+and+Resource+Allocation+for+Delay+Optimization+in+RIS-Assisted+OFDM+Systems%3A+A+DRL+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03586，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03586&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates a joint phase design and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to optimize average delay, where
data packets for each user arrive at the base station stochastically. The
sequential optimization problem is inherently a Markov decision process (MDP),
making it fall within the scope of reinforcement learning. To effectively
handle the mixed action space and reduce the state space dimensionality, a
hybrid deep reinforcement learning (DRL) approach is proposed. Specifically,
proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase
shift design, while PPO-N is responsible for subcarrier allocation decisions.
To further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize subcarrier
allocation indicater more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture network dynamics, key factors
closely related to average delay, including the number of backlogged packets in
buffers and the current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance training
efficiency and accelerate convergence. Simulation results demonstrate that the
proposed algorithm significantly reduces average delay, enhances resource
allocation efficiency, and achieves superior system robustness and fairness
compared to baseline methods.

</details>


### [107] [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/abs/2506.03610)
*Dongmin Park, Minkyu Kim, Beongjun Choi, Junhyuck Kim, Keon Lee, Jonghyun Lee, Inkyu Park, Byeong-Uk Lee, Jaeyoung Hwang, Jaewoo Ahn, Ameya S. Mahabaleshwarkar, Bilal Kartal, Pritam Biswas, Yoshi Suhara, Kangwook Lee, Jaewoong Cho*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为Orak的新游戏基准测试，旨在改进大型语言模型（LLM）在游戏代理中的应用，填补了现有基准测试的不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有游戏基准测试在评估多样化LLM能力、复杂游戏场景所需代理模块研究及预训练LLM向游戏代理对齐方面存在不足。

**方法:** 引入了一个名为Orak的基础基准测试，包括12款主流视频游戏，并提出了基于Model Context Protocol (MCP)的即插即用接口以及一个用于微调的LLM游戏轨迹数据集。

**结果:** Orak支持对LLM能力和代理模块进行全面研究，并提供了综合评估框架，包括游戏得分排行榜、LLM战斗竞技场以及对视觉输入状态、代理策略和微调效果的深入分析。

**结论:** Orak通过提供一个全面的评估框架和多样化的游戏环境，为构建通用的游戏代理奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Orak%3A+A+Foundational+Benchmark+for+Training+and+Evaluating+LLM+Agents+on+Diverse+Video+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03610，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03610&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM) agents are reshaping the game industry,
particularly with more intelligent and human-preferable game characters.
However, existing game benchmarks fall short of practical needs: they lack
evaluations of diverse LLM capabilities across various game genres, studies of
agentic modules crucial for complex gameplay, and fine-tuning datasets for
aligning pre-trained LLMs into gaming agents. To fill these gaps, we present
\textbf{\benchname{}}, a foundational benchmark designed to train and evaluate
LLM agents across diverse real-world video games. Unlike existing benchmarks,
Orak includes 12 popular video games spanning all major genres, enabling
comprehensive studies of LLM capabilities and agentic modules essential for
intricate game scenarios. To support consistent evaluation of LLMs, we
introduce a plug-and-play interface based on Model Context Protocol (MCP) that
enables LLMs to seamlessly connect with games and manipulate agentic modules.
Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay
trajectories across diverse game genres. Orak offers a comprehensive evaluation
framework, encompassing general game score leaderboards, LLM battle arenas, and
in-depth analyses of visual input state, agentic strategies, and fine-tuning
effects, establishing a foundation towards building generic gaming agents. Code
is available at https://github.com/krafton-ai/Orak.

</details>


### [108] [Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations](https://arxiv.org/abs/2506.03613)
*Shaoshan Liu, Fan Wang, Hongjun Zhou, Yuanfeng Wang*

**主要类别:** cs.AI

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+Cross-Morphology+Embodied+AI+Agents%3A+From+Practical+Challenges+to+Theoretical+Foundations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03613&send_immediately=true&force_search=false)

**原文摘要:** While theory and practice are often seen as separate domains, this article
shows that theoretical insight is essential for overcoming real-world
engineering barriers. We begin with a practical challenge: training a
cross-morphology embodied AI policy that generalizes across diverse robot
morphologies. We formalize this as the Heterogeneous Embodied Agent Training
(HEAT) problem and prove it reduces to a structured Partially Observable Markov
Decision Process (POMDP) that is PSPACE-complete. This result explains why
current reinforcement learning pipelines break down under morphological
diversity, due to sequential training constraints, memory-policy coupling, and
data incompatibility. We further explore Collective Adaptation, a distributed
learning alternative inspired by biological systems. Though NEXP-complete in
theory, it offers meaningful scalability and deployment benefits in practice.
This work illustrates how computational theory can illuminate system design
trade-offs and guide the development of more robust, scalable embodied AI. For
practitioners and researchers to explore this problem, the implementation code
of this work has been made publicly available at
https://github.com/airs-admin/HEAT

</details>


### [109] [Reason from Future: Reverse Thought Chain Enhances LLM Reasoning](https://arxiv.org/abs/2506.03673)
*Yinlong Xu, Yanzhao Zheng, Shuoshuo Sun, Shuaihan Huang, Baohua Dong, Hangcheng Zhu, Ruohui Huang, Gang Yu, Hongxia Xu, Jian Wu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的推理范式RFF，通过双向推理提高推理效率和准确性，同时减少搜索空间。


<details>
  <summary>更多</summary>
  
**动机:** 现有的推理范式如Chain-of-Thought (CoT)和Tree-of-Thought (ToT)虽然能增强小型语言模型的推理能力，但由于搜索空间中的无界分支因子导致推理消耗大，且容易陷入局部最优推理。

**方法:** 提出了一种名为Reason from Future (RFF)的新型推理范式，并进行了实证评估。

**结果:** RFF通过优先考虑核心逻辑关系并对中间步骤施加目标导向约束，有效减少搜索空间并减轻顺序前向推理固有的误差累积。

**结论:** RFF是一种新的推理范式，通过双向推理结合自顶向下规划和自底向上推理积累，提高了传统范式的准确性并减少了搜索空间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reason+from+Future%3A+Reverse+Thought+Chain+Enhances+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03673&send_immediately=true&force_search=false)

**原文摘要:** It has been demonstrated that carefully designed reasoning paradigms, like
Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning
capabilities of small language models by detailed thinking and extensive
thought searching, unbounded branching factors in the searching space create
prohibitive reasoning consumption. However these methods fall into the trap of
local optimum reasoning, which means the model lacks a global perspective while
solving problems. We propose a novel reasoning paradigm called Reason from
Future (RFF), which generates reasoning paths by bidirectional reasoning that
combines top-down planning with bottom-up reasoning accumulation. The essence
of RFF lies in its reverse reasoning mechanism, which prioritizes core logical
relationships and imposes goal-oriented constraints on intermediate steps,
thereby reducing the searching space and mitigating error accumulation inherent
in sequential forward reasoning. Empirical evaluations across diverse
experiments demonstrate that RFF outperforms conventional paradigms with higher
accuracy and less searching space to solve complex tasks.

</details>


### [110] [AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance](https://arxiv.org/abs/2506.03828)
*Dhaval Patel, Shuxin Lin, James Rayfield, Nianjun Zhou, Roman Vaculin, Natalia Martinez, Fearghal O'donncha, Jayant Kalagnanam*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种使用AI代理和大型语言模型实现工业资产生命周期端到端自动化的框架AssetOpsBench，旨在减少人工工作并优化系统运行效率。


<details>
  <summary>更多</summary>
  
**动机:** 传统AI/ML方法主要孤立地解决工业操作流程中的特定问题，而AI代理和LLM的出现为整个资产生命周期提供端到端自动化提供了新机会。

**方法:** 论文通过引入AI代理和大型语言模型（LLMs）的方法，提出了一个名为AssetOpsBench的统一框架和环境，用于开发和评估适用于工业场景的智能代理。

**结果:** 论文提出了AssetOpsBench框架，并概述了构建能够集成感知、推理和控制功能的智能代理所需的关键要求和可行见解。

**结论:** 该论文提出了一种基于AI代理和大语言模型的未来愿景，以实现工业资产生命周期管理的端到端自动化，并介绍了AssetOpsBench这一统一框架来指导行业4.0应用中特定领域代理的开发、协调和评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AssetOpsBench%3A+Benchmarking+AI+Agents+for+Task+Automation+in+Industrial+Asset+Operations+and+Maintenance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03828&send_immediately=true&force_search=false)

**原文摘要:** AI for Industrial Asset Lifecycle Management aims to automate complex
operational workflows -- such as condition monitoring, maintenance planning,
and intervention scheduling -- to reduce human workload and minimize system
downtime. Traditional AI/ML approaches have primarily tackled these problems in
isolation, solving narrow tasks within the broader operational pipeline. In
contrast, the emergence of AI agents and large language models (LLMs)
introduces a next-generation opportunity: enabling end-to-end automation across
the entire asset lifecycle. This paper envisions a future where AI agents
autonomously manage tasks that previously required distinct expertise and
manual coordination. To this end, we introduce AssetOpsBench -- a unified
framework and environment designed to guide the development, orchestration, and
evaluation of domain-specific agents tailored for Industry 4.0 applications. We
outline the key requirements for such holistic systems and provide actionable
insights into building agents that integrate perception, reasoning, and control
for real-world industrial operations. The software is available at
https://github.com/IBM/AssetOpsBench.

</details>


### [111] [Causal Explanations Over Time: Articulated Reasoning for Interactive Environments](https://arxiv.org/abs/2506.03915)
*Sebastian Rödling, Matej Zečević, Devendra Singh Dhami, Kristian Kersting*

**主要类别:** cs.AI

**AI概要:** This paper generalizes Structural Causal Explanations to handle temporal interactions and feedback loops, showing improved performance in causal reasoning over time.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is that current SCE methods are limited to small data and cannot effectively handle temporal interactions or feedback loops, making them unsuitable for complex causal reasoning.

**方法:** The authors generalize Structural Causal Explanations (SCEs) into a recursive formulation of explanation trees and test its effectiveness on synthetic time-series data and a 2D grid game.

**结果:** The generalized SCE algorithm demonstrates improved performance in capturing temporal interactions and causal reasoning over multiple time steps.

**结论:** The paper concludes that the generalized SCE algorithm provides better causal explanations for temporal interactions compared to existing methods.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Explanations+Over+Time%3A+Articulated+Reasoning+for+Interactive+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03915，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03915&send_immediately=true&force_search=false)

**原文摘要:** Structural Causal Explanations (SCEs) can be used to automatically generate
explanations in natural language to questions about given data that are
grounded in a (possibly learned) causal model. Unfortunately they work for
small data only. In turn they are not attractive to offer reasons for events,
e.g., tracking causal changes over multiple time steps, or a behavioral
component that involves feedback loops through actions of an agent. To this
end, we generalize SCEs to a (recursive) formulation of explanation trees to
capture the temporal interactions between reasons. We show the benefits of this
more general SCE algorithm on synthetic time-series data and a 2D grid game,
and further compare it to the base SCE and other existing methods for causal
explanations.

</details>


### [112] [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
*Junqi Gao, Xiang Zou, YIng Ai, Dong Li, Yichen Niu, Biqing Qi, Jianxing Liu*

**主要类别:** cs.AI

**AI概要:** Graph Counselor是一种新的图检索增强生成方法，通过多智能体协作提高大语言模型在专业领域的知识整合能力和推理准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有GraphRAG方法存在信息聚合效率低和推理机制僵化两个主要限制，需要一种更灵活、能自适应调整的图信息处理和推理方法。

**方法:** 提出了一种基于多智能体协作的GraphRAG方法Graph Counselor，其中包括自适应图信息提取模块（AGIEM）和多视角自我反思（SR）模块。

**结果:** 实验表明，Graph Counselor在多个图推理任务中优于现有方法。

**结论:** Graph Counselor通过多智能体协作的方法，显著提高了图推理任务的准确性和泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Counselor%3A+Adaptive+Graph+Exploration+via+Multi-Agent+Synergy+to+Enhance+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03939，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03939&send_immediately=true&force_search=false)

**原文摘要:** Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external
knowledge integration capabilities by explicitly modeling knowledge
relationships, thereby improving the factual accuracy and generation quality of
Large Language Models (LLMs) in specialized domains. However, existing methods
suffer from two inherent limitations: 1) Inefficient Information Aggregation:
They rely on a single agent and fixed iterative patterns, making it difficult
to adaptively capture multi-level textual, structural, and degree information
within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning
schemes, which cannot dynamically adjust reasoning depth nor achieve precise
semantic correction. To overcome these limitations, we propose Graph Counselor,
an GraphRAG method based on multi-agent collaboration. This method uses the
Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,
and Execution Agents work together to precisely model complex graph structures
and dynamically adjust information extraction strategies, addressing the
challenges of multi-level dependency modeling and adaptive reasoning depth.
Additionally, the Self-Reflection with Multiple Perspectives (SR) module
improves the accuracy and semantic consistency of reasoning results through
self-reflection and backward reasoning mechanisms. Experiments demonstrate that
Graph Counselor outperforms existing methods in multiple graph reasoning tasks,
exhibiting higher reasoning accuracy and generalization ability. Our code is
available at https://github.com/gjq100/Graph-Counselor.git.

</details>


### [113] [A framework for Conditional Reasoning in Answer Set Programming](https://arxiv.org/abs/2506.03997)
*Mario Alviano, Laura Giordano, Daniele Theseider Dupré*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个有条件答案集编程框架，利用典型性条件逻辑和多优先语义，实现了对答案集编程的条件扩展与推理。


<details>
  <summary>更多</summary>
  
**动机:** 为了扩展答案集编程（ASP）以支持条件推理，引入了有条件答案集编程框架（Conditional ASP）。

**方法:** 结合条件知识库与ASP程序，并采用多优先语义和KLM优先语义作为特殊情况来解释条件。

**结果:** 开发了一个形式化方法，能够通过条件逻辑对ASP程序的答案集进行条件扩展和推理。

**结论:** 提出了一种基于典型性条件逻辑的有条件答案集编程框架，用于对程序的答案集进行条件推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+framework+for+Conditional+Reasoning+in+Answer+Set+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03997&send_immediately=true&force_search=false)

**原文摘要:** In this paper we introduce a Conditional Answer Set Programming framework
(Conditional ASP) for the definition of conditional extensions of Answer Set
Programming (ASP). The approach builds on a conditional logic with typicality,
and on the combination of a conditional knowledge base with an ASP program, and
allows for conditional reasoning over the answer sets of the program. The
formalism relies on a multi-preferential semantics (and on the KLM preferential
semantics, as a special case) to provide an interpretation of conditionals.

</details>


### [114] [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
*Akshat Naik, Patrick Quinn, Guillermo Bosch, Emma Gouné, Francisco Javier Campos Zabala, Jason Ross Brown, Edward James Young*

**主要类别:** cs.AI

**AI概要:** 论文提出AgentMisalignment基准测试，以评估大型语言模型代理在现实场景中的不一致行为倾向，并发现模型能力和系统提示对不一致倾向有显著影响。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型代理的普及，相关的不一致风险增加，但现实中代理尝试不一致行为的可能性仍缺乏理解。

**方法:** 引入了一个名为AgentMisalignment的基准测试，包含一系列现实场景，评估LLM代理展示不一致行为的机会。

**结果:** 报告了前沿模型在基准测试中的表现，发现评估更有能力的模型时，平均而言不一致程度更高。通过不同的系统提示，代理个性的影响有时比模型选择本身更能影响不一致倾向。

**结论:** 当前的对齐方法在LLM代理中未能推广，强调了随着自主系统变得越来越普遍，需要进一步进行倾向性评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentMisalignment%3A+Measuring+the+Propensity+for+Misaligned+Behaviour+in+LLM-Based+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04018，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04018&send_immediately=true&force_search=false)

**原文摘要:** As Large Language Model (LLM) agents become more widespread, associated
misalignment risks increase. Prior work has examined agents' ability to enact
misaligned behaviour (misalignment capability) and their compliance with
harmful instructions (misuse propensity). However, the likelihood of agents
attempting misaligned behaviours in real-world settings (misalignment
propensity) remains poorly understood. We introduce a misalignment propensity
benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in
which LLM agents have the opportunity to display misaligned behaviour. We
organise our evaluations into subcategories of misaligned behaviours, including
goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report
the performance of frontier models on our benchmark, observing higher
misalignment on average when evaluating more capable models. Finally, we
systematically vary agent personalities through different system prompts. We
find that persona characteristics can dramatically and unpredictably influence
misalignment tendencies -- occasionally far more than the choice of model
itself -- highlighting the importance of careful system prompt engineering for
deployed AI agents. Our work highlights the failure of current alignment
methods to generalise to LLM agents, and underscores the need for further
propensity evaluations as autonomous systems become more prevalent.

</details>


### [115] [Interpretability by Design for Efficient Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.04022)
*Qiyue Xia, J. Michael Herrmann*

**主要类别:** cs.AI

**AI概要:** 这篇论文介绍了一种新的多目标强化学习方法，通过局部线性映射生成近似帕累托前沿，从而提高策略优化的效率。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机是为了解决多目标强化学习中灵活性和可靠性的问题，找到在多个冲突目标下表现良好的策略。

**方法:** 论文的方法包括利用局部线性映射来连接参数空间和性能空间，并通过实验验证了该方法的有效性。

**结果:** 论文结果显示，所提出的方法能够在不同领域中高效地生成近似帕累托前沿，并优于之前的方法。

**结论:** 论文得出结论，通过使用基于参数空间与性能空间之间局部线性映射的训练方案，可以有效解释当前参数向量，并提高多目标强化学习方法的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretability+by+Design+for+Efficient+Multi-Objective+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04022，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04022&send_immediately=true&force_search=false)

**原文摘要:** Multi-objective reinforcement learning (MORL) aims at optimising several,
often conflicting goals in order to improve flexibility and reliability of RL
in practical tasks. This can be achieved by finding diverse policies that are
optimal for some objective preferences and non-dominated by optimal policies
for other preferences so that they form a Pareto front in the multi-objective
performance space. The relation between the multi-objective performance space
and the parameter space that represents the policies is generally non-unique.
Using a training scheme that is based on a locally linear map between the
parameter space and the performance space, we show that an approximate Pareto
front can provide an interpretation of the current parameter vectors in terms
of the objectives which enables an effective search within contiguous solution
domains. Experiments are conducted with and without retraining across different
domains, and the comparison with previous methods demonstrates the efficiency
of our approach.

</details>


### [116] [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/abs/2506.04133)
*Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis*

**主要类别:** cs.AI

**AI概要:** 这篇论文主要讨论了基于大型语言模型的代理多智能体系统中的信任、风险和安全管理，提出了一个负责任的代理AI路线图，并通过案例研究展示了现实世界中的漏洞和解决方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着基于LLM的代理多智能体系统在企业和社会领域的应用，对其信任、风险和安全管理的研究变得尤为重要。

**方法:** 论文通过四个支柱（治理、可解释性、ModelOps和隐私/安全）详细分析了代理AI框架中的TRiSM，并引入了一个全面的风险分类法，结合案例研究说明了现实世界中的漏洞。

**结果:** 论文识别了独特的威胁向量，并为代理AI应用引入了一个全面的风险分类法。同时调查了信任建立机制、透明度和监督技术以及最先进的分布式LLM代理系统可解释性策略。

**结论:** 论文总结了一个负责任的代理AI路线图，提出了研究方向，以将新兴的多智能体系统与稳健的TRiSM原则对齐，实现安全、负责和透明的部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRiSM+for+Agentic+AI%3A+A+Review+of+Trust%2C+Risk%2C+and+Security+Management+in+LLM-based+Agentic+Multi-Agent+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04133&send_immediately=true&force_search=false)

**原文摘要:** Agentic AI systems, built on large language models (LLMs) and deployed in
multi-agent configurations, are redefining intelligent autonomy, collaboration
and decision-making across enterprise and societal domains. This review
presents a structured analysis of Trust, Risk, and Security Management (TRiSM)
in the context of LLM-based agentic multi-agent systems (AMAS). We begin by
examining the conceptual foundations of agentic AI, its architectural
differences from traditional AI agents, and the emerging system designs that
enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is
then detailed through four pillars governance, explainability, ModelOps, and
privacy/security each contextualized for agentic LLMs. We identify unique
threat vectors and introduce a comprehensive risk taxonomy for the agentic AI
applications, supported by case studies illustrating real-world
vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,
transparency and oversight techniques, and state-of-the-art explainability
strategies in distributed LLM agent systems. Additionally, metrics for
evaluating trust, interpretability, and human-centered performance are reviewed
alongside open benchmarking challenges. Security and privacy are addressed
through encryption, adversarial defense, and compliance with evolving AI
regulations. The paper concludes with a roadmap for responsible agentic AI,
proposing research directions to align emerging multi-agent systems with robust
TRiSM principles for safe, accountable, and transparent deployment.

</details>


### [117] [macOSWorld: A Multilingual Interactive Benchmark for GUI Agents](https://arxiv.org/abs/2506.04135)
*Pei Yang, Hai Ci, Mike Zheng Shou*

**主要类别:** cs.AI

**AI概要:** 本文介绍了首个全面的macOS GUI代理评估工具macOSWorld，揭示了现有代理在执行任务和面对欺骗攻击时的能力差距，特别是在不同的语言环境下。


<details>
  <summary>更多</summary>
  
**动机:** 现有的交互式基准测试主要为英文，涵盖网络使用或Windows、Linux和Android环境，但不包括macOS。而macOS具有独特的GUI模式和独家应用，因此需要填补这一空白。

**方法:** 构建了一个全面的评估工具macOSWorld，其中包括202个多语言交互任务，覆盖30个应用程序，并包括一个专门的安全基准测试子集。

**结果:** 专有计算机使用代理在成功率上领先于开源轻量级研究模型，多语言基准测试暴露了常见弱点，尤其是在阿拉伯语上，与英语相比平均下降了27.5%。

**结论:** macOSWorld的评估结果显示了专有计算机使用代理和开源轻量级研究模型之间在成功执行任务方面的显著差距，以及多语言基准测试中不同语言之间的性能差异。此外，安全基准测试强调了欺骗攻击的普遍性需要立即关注。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是macOSWorld%3A+A+Multilingual+Interactive+Benchmark+for+GUI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04135，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04135&send_immediately=true&force_search=false)

**原文摘要:** Graphical User Interface (GUI) agents show promising capabilities for
automating computer-use tasks and facilitating accessibility, but existing
interactive benchmarks are mostly English-only, covering web-use or Windows,
Linux, and Android environments, but not macOS. macOS is a major OS with
distinctive GUI patterns and exclusive applications. To bridge the gaps, we
present macOSWorld, the first comprehensive benchmark for evaluating GUI agents
on macOS. macOSWorld features 202 multilingual interactive tasks across 30
applications (28 macOS-exclusive), with task instructions and OS interfaces
offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As
GUI agents are shown to be vulnerable to deception attacks, macOSWorld also
includes a dedicated safety benchmarking subset. Our evaluation on six GUI
agents reveals a dramatic gap: proprietary computer-use agents lead at above
30% success rate, while open-source lightweight research models lag at below
2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks
also expose common weaknesses, especially in Arabic, with a 27.5% average
degradation compared to English. Results from safety benchmarking also
highlight that deception attacks are more general and demand immediate
attention. macOSWorld is available at https://github.com/showlab/macosworld.

</details>


### [118] [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
*Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi*

**主要类别:** cs.AI

**AI概要:** 该论文研究了测试时扩展思考对推理模型的影响，发现过度思考会增加输出方差并降低精度，而非真正提升推理能力。作者提出了一种新的测试时扩展方法——并行思考，通过生成多个独立推理路径并进行多数投票，显著提高了准确率。


<details>
  <summary>更多</summary>
  
**动机:** 最近在测试时通过扩展思考痕迹（例如使用“Wait”或“Let me rethink”提示）提高性能的趋势引发了关注，论文旨在探讨这种做法是否真正能提升推理能力。

**方法:** 作者进行了详细的实证研究，分析了模型和基准上的性能变化趋势，并引入了一个简单的概率模型来解释“过度思考”现象。此外，他们提出了“并行思考”的替代方法，并对其效果进行了实验验证。

**结果:** 实验结果显示，初期增加思考时间确实带来性能提升，但随后由于“过度思考”导致性能下降。额外的思考增加了输出方差，造成了推理能力提升的假象，最终反而降低了精度。

**结论:** 论文得出结论，测试时通过扩展思考并不是利用推理预算的有效方式。并提出了一种基于Best-of-N采样的“并行思考”方法，在相同预算下生成多个独立推理路径并通过多数投票选择最一致的响应，从而实现高达20%的准确率提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Thinking+More+always+Help%3F+Understanding+Test-Time+Scaling+in+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04210&send_immediately=true&force_search=false)

**原文摘要:** Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [119] [Models of Heavy-Tailed Mechanistic Universality](https://arxiv.org/abs/2506.03470)
*Liam Hodgkinson, Zhichao Wang, Michael W. Mahoney*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的随机矩阵模型（HTMP），用于解释深度学习中常见的重尾行为，揭示其作为模型隐性偏置的本质，并探讨其在神经缩放定律等多个方面的影响。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习中许多对象表现出重尾或幂律行为，如雅可比矩阵、海森矩阵和权重矩阵的重尾谱密度，这促使研究者提出了重尾机制普适性（HT-MU）的概念。

**方法:** 提出高温度Marchenko-Pastur（HTMP）随机矩阵模型，分析谱密度的幂律行为及其与训练数据、温度、特征向量熵的关系。

**结果:** 研究表明，谱密度的幂律行为源于数据相关性、训练温度降低和特征向量熵减少三个因素的组合，并可通过“特征值排斥”参数进行控制。

**结论:** 论文提出了一种新的随机矩阵模型（HTMP）来探索神经网络中的重尾行为，表明这些行为是模型结构中的隐性偏置，并讨论了该模型对神经缩放定律等现象的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Models+of+Heavy-Tailed+Mechanistic+Universality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03470，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03470&send_immediately=true&force_search=false)

**原文摘要:** Recent theoretical and empirical successes in deep learning, including the
celebrated neural scaling laws, are punctuated by the observation that many
objects of interest tend to exhibit some form of heavy-tailed or power law
behavior. In particular, the prevalence of heavy-tailed spectral densities in
Jacobians, Hessians, and weight matrices has led to the introduction of the
concept of heavy-tailed mechanistic universality (HT-MU). Multiple lines of
empirical evidence suggest a robust correlation between heavy-tailed metrics
and model performance, indicating that HT-MU may be a fundamental aspect of
deep learning efficacy. Here, we propose a general family of random matrix
models -- the high-temperature Marchenko-Pastur (HTMP) ensemble -- to explore
attributes that give rise to heavy-tailed behavior in trained neural networks.
Under this model, spectral densities with power laws on (upper and lower) tails
arise through a combination of three independent factors (complex correlation
structures in the data; reduced temperatures during training; and reduced
eigenvector entropy), appearing as an implicit bias in the model structure, and
they can be controlled with an "eigenvalue repulsion" parameter. Implications
of our model on other appearances of heavy tails, including neural scaling
laws, optimizer trajectories, and the five-plus-one phases of neural network
training, are discussed.

</details>


### [120] [SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search](https://arxiv.org/abs/2506.03657)
*Leonardo Martins Bianco, Christine Keribin, Zacharie Naulet*

**主要类别:** stat.ML

**AI概要:** 提出了一种名为SubSearch的新方法，用于稳健地估计Stochastic Block Model参数并检测图数据中的异常节点。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的图数据通常不满足理想化模型的假设，因此需要一种稳健的算法来进行社区检测和参数估计。

**方法:** 通过探索子图空间来稳健估计SBM参数的方法，寻找与模型假设紧密对齐的子图。

**结果:** 实验表明SubSearch在合成数据和真实世界数据集上都具有有效性，并能识别导致图偏离模型的异常节点。

**结论:** SubSearch能够有效地进行社区检测，并且在面对与模型假设不符的图数据时表现出色，同时可以用于异常节点检测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SubSearch%3A+Robust+Estimation+and+Outlier+Detection+for+Stochastic+Block+Models+via+Subgraph+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03657，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03657&send_immediately=true&force_search=false)

**原文摘要:** Community detection is a fundamental task in graph analysis, with methods
often relying on fitting models like the Stochastic Block Model (SBM) to
observed networks. While many algorithms can accurately estimate SBM parameters
when the input graph is a perfect sample from the model, real-world graphs
rarely conform to such idealized assumptions. Therefore, robust algorithms are
crucial-ones that can recover model parameters even when the data deviates from
the assumed distribution. In this work, we propose SubSearch, an algorithm for
robustly estimating SBM parameters by exploring the space of subgraphs in
search of one that closely aligns with the model's assumptions. Our approach
also functions as an outlier detection method, properly identifying nodes
responsible for the graph's deviation from the model and going beyond simple
techniques like pruning high-degree nodes. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method.

</details>


### [121] [Position: There Is No Free Bayesian Uncertainty Quantification](https://arxiv.org/abs/2506.03670)
*Ivan Melev, Goeran Kauermann*

**主要类别:** stat.ML

**AI概要:** 本文分析了贝叶斯方法在不确定性量化中的局限性，提出了一种基于优化的新解释框架，并评估了贝叶斯推断的质量。


<details>
  <summary>更多</summary>
  
**动机:** 由于贝叶斯方法因其直观吸引力而在现代机器学习和深度学习中广泛使用，但其不确定性量化的有效性值得进一步探讨。

**方法:** 通过讨论贝叶斯更新的等效优化表示，提供了一种与优化观点一致的解释，并提出了评估贝叶斯推断质量的措施。

**结果:** 挑战了贝叶斯不确定性量化的传统解释，提供了新的视角以及未来工作的方向。

**结论:** 作者质疑了贝叶斯方法在模型不确定性量化中的有效性，并提出了基于优化视角的替代解释和衡量贝叶斯推断阶段质量的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+There+Is+No+Free+Bayesian+Uncertainty+Quantification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03670&send_immediately=true&force_search=false)

**原文摘要:** Due to their intuitive appeal, Bayesian methods of modeling and uncertainty
quantification have become popular in modern machine and deep learning. When
providing a prior distribution over the parameter space, it is straightforward
to obtain a distribution over the parameters that is conventionally interpreted
as uncertainty quantification of the model. We challenge the validity of such
Bayesian uncertainty quantification by discussing the equivalent
optimization-based representation of Bayesian updating, provide an alternative
interpretation that is coherent with the optimization-based perspective,
propose measures of the quality of the Bayesian inferential stage, and suggest
directions for future work.

</details>


### [122] [Latent Guided Sampling for Combinatorial Optimization](https://arxiv.org/abs/2506.03672)
*Sobihan Surendran, Adeline Fermanian, Sylvain Le Corff*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种名为LGS-Net的新型潜在空间模型以及称为Latent Guided Sampling的有效推理方法，用于解决组合优化问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的潜在空间模型要么需要标记数据，要么依赖于预训练策略，而现有的神经组合优化方法往往在分布外实例上表现不佳，并且缺乏鲁棒的推理机制。

**方法:** 提出了一种新的潜在空间模型LGS-Net和一种有效的推理方法Latent Guided Sampling(LGS)，该方法基于马尔可夫链蒙特卡洛和随机逼近。

**结果:** 实验结果表明，在基准路由任务中，所提方法在基于RL的方法中达到了最先进的性能。

**结论:** LGS-Net和Latent Guided Sampling方法在基于强化学习的神经组合优化方法中达到了最先进的性能，并提供了严格的理论收敛保证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Guided+Sampling+for+Combinatorial+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03672&send_immediately=true&force_search=false)

**原文摘要:** Combinatorial Optimization problems are widespread in domains such as
logistics, manufacturing, and drug discovery, yet their NP-hard nature makes
them computationally challenging. Recent Neural Combinatorial Optimization
methods leverage deep learning to learn solution strategies, trained via
Supervised or Reinforcement Learning (RL). While promising, these approaches
often rely on task-specific augmentations, perform poorly on
out-of-distribution instances, and lack robust inference mechanisms. Moreover,
existing latent space models either require labeled data or rely on pre-trained
policies. In this work, we propose LGS-Net, a novel latent space model that
conditions on problem instances, and introduce an efficient inference method,
Latent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic
Approximation. We show that the iterations of our method form a
time-inhomogeneous Markov Chain and provide rigorous theoretical convergence
guarantees. Empirical results on benchmark routing tasks show that our method
achieves state-of-the-art performance among RL-based approaches.

</details>


### [123] [Infinitesimal Higher-Order Spectral Variations in Rectangular Real Random Matrices](https://arxiv.org/abs/2506.03764)
*Róisín Luo*

**主要类别:** stat.ML

**AI概要:** 本研究利用Kato的扰动理论，提出了一种计算实矩形矩阵奇异值高阶导数的新方法，并首次给出了二阶导数（Hessian）的闭合形式，为后续研究提供了实用工具。


<details>
  <summary>更多</summary>
  
**动机:** 标准矩阵分析技术在推导高阶奇异值导数的闭合形式时面临巨大挑战，因此需要一种新方法来处理非对称扰动问题并简化计算过程。

**方法:** 通过将实矩形矩阵视为有限维希尔伯特空间上的紧算子，并将其嵌入到块自伴算子中，结合Kato的解析扰动理论和渐近特征值展开方法，得到了n阶谱变分的闭合形式表达式。

**结果:** 成功推导出一般n阶Fréchet导数的闭合形式表达式，特别在n=2的情况下首次得到了奇异值的Hessian矩阵，并展示了该方法在随机矩阵应用中的潜力。

**结论:** 论文得出了一种用于计算实矩形矩阵奇异值的n阶Fréchet导数的理论框架，并提供了闭合形式的表达式，特别是在二阶情况下推导出了Hessian矩阵。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Infinitesimal+Higher-Order+Spectral+Variations+in+Rectangular+Real+Random+Matrices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03764，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03764&send_immediately=true&force_search=false)

**原文摘要:** We present a theoretical framework for deriving the general $n$-th order
Fr\'echet derivatives of singular values in real rectangular matrices, by
leveraging reduced resolvent operators from Kato's analytic perturbation theory
for self-adjoint operators. Deriving closed-form expressions for higher-order
derivatives of singular values is notoriously challenging through standard
matrix-analysis techniques. To overcome this, we treat a real rectangular
matrix as a compact operator on a finite-dimensional Hilbert space, and embed
the rectangular matrix into a block self-adjoint operator so that non-symmetric
perturbations are captured. Applying Kato's asymptotic eigenvalue expansion to
this construction, we obtain a general, closed-form expression for the
infinitesimal $n$-th order spectral variations. Specializing to $n=2$ and
deploying on a Kronecker-product representation with matrix convention yield
the Hessian of a singular value, not found in literature. By bridging abstract
operator-theoretic perturbation theory with matrices, our framework equips
researchers with a practical toolkit for higher-order spectral sensitivity
studies in random matrix applications (e.g., adversarial perturbation in deep
learning).

</details>


### [124] [High-Dimensional Learning in Finance](https://arxiv.org/abs/2506.03780)
*Hasan Fallahgoul*

**主要类别:** stat.ML

**AI概要:** 这篇论文探讨了在金融领域使用大型过参数化机器学习模型进行预测的问题。作者通过理论分析和实证验证研究了高维学习的三个方面，发现样本内标准化影响核近似、弱信噪比下的学习难度以及岭回归的有效复杂度。结果表明，在小样本和高维特征情况下，预测成功是由低复杂度因素驱动，而非真正高维学习。


<details>
  <summary>更多</summary>
  
**动机:** 论文的研究动机是理解在金融领域使用大型过参数化模型进行预测的方法何时以及如何取得预测成功，并提供理论基础和实证验证。

**方法:** 论文的方法包括理论分析和实证验证。具体方法包括随机傅里叶特征实现中的样本内标准化对高斯核近似的影响分析、弱信噪比下可靠学习的信息理论界限推导以及岭回归的有效复杂度的VC维度分析。

**结果:** 论文的结果包括三个主要方面：1）证明了随机傅里叶特征实现中的样本内标准化从根本上改变了基础高斯核近似；2）推导出了弱信噪比下可靠学习的信息理论界限；3）通过VC维度分析揭示了岭回归的有效复杂度受限于样本大小而非名义特征维度。此外，论文还进行了全面的数值验证，确认了这些理论预测。

**结论:** 论文得出的结论是，当样本量较小且特征为高维时，观察到的预测成功必然由低复杂度的伪影驱动，而不是真正的高维学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是High-Dimensional+Learning+in+Finance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03780，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03780&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in machine learning have shown promising results for
financial prediction using large, over-parameterized models. This paper
provides theoretical foundations and empirical validation for understanding
when and how these methods achieve predictive success. I examine three key
aspects of high-dimensional learning in finance. First, I prove that
within-sample standardization in Random Fourier Features implementations
fundamentally alters the underlying Gaussian kernel approximation, replacing
shift-invariant kernels with training-set dependent alternatives. Second, I
derive sample complexity bounds showing when reliable learning becomes
information-theoretically impossible under weak signal-to-noise ratios typical
in finance. Third, VC-dimension analysis reveals that ridgeless regression's
effective complexity is bounded by sample size rather than nominal feature
dimension. Comprehensive numerical validation confirms these theoretical
predictions, revealing systematic breakdown of claimed theoretical properties
across realistic parameter ranges. These results show that when sample size is
small and features are high-dimensional, observed predictive success is
necessarily driven by low-complexity artifacts, not genuine high-dimensional
learning.

</details>


### [125] [Spatially Resolved Meteorological and Ancillary Data in Central Europe for Rainfall Streamflow Modeling](https://arxiv.org/abs/2506.03819)
*Marc Aurel Vischer, Noelia Otero, Jackie Ma*

**主要类别:** stat.ML

**AI概要:** 论文提出了一个空间分辨率高的降雨径流数据集，用于推动水文建模中神经网络的应用。


<details>
  <summary>更多</summary>
  
**动机:** 为了将神经网络驱动的水文建模扩展到空间分辨更精细的领域，克服传统集总式流域方法的局限性。

**方法:** 整合了涵盖中欧五个河流流域的数据，包括气象强迫数据以及土壤、岩石、土地覆盖和地形的辅助信息，并将其统一为规则的9公里×9公里网格数据。

**结果:** 创建了一个包含1981年10月至2011年9日每日数据的综合数据集，并提供了结合公开河流流量数据进行端到端建模的代码支持。

**结论:** 提供了一个空间解析完整的降雨径流建模数据集，以推动基于神经网络的水文模型超越集总式流域的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spatially+Resolved+Meteorological+and+Ancillary+Data+in+Central+Europe+for+Rainfall+Streamflow+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03819，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03819&send_immediately=true&force_search=false)

**原文摘要:** We present a dataset for rainfall streamflow modeling that is fully spatially
resolved with the aim of taking neural network-driven hydrological modeling
beyond lumped catchments. To this end, we compiled data covering five river
basins in central Europe: upper Danube, Elbe, Oder, Rhine, and Weser. The
dataset contains meteorological forcings, as well as ancillary information on
soil, rock, land cover, and orography. The data is harmonized to a regular 9km
times 9km grid and contains daily values that span from October 1981 to
September 2011. We also provide code to further combine our dataset with
publicly available river discharge data for end-to-end rainfall streamflow
modeling.

</details>


### [126] [Algorithm- and Data-Dependent Generalization Bounds for Score-Based Generative Models](https://arxiv.org/abs/2506.03849)
*Benjamin Dupuis, Dario Shariatian, Maxime Haddouche, Alain Durmus, Umut Simsekli*

**主要类别:** stat.ML

**AI概要:** 这篇论文提出了一个更准确的分数生成模型（SGMs）泛化性能分析方法，结合了优化动力学的影响，并通过实证结果支持了理论发现。


<details>
  <summary>更多</summary>
  
**动机:** 现有对SGMs的统计性能分析过于悲观且不够精确，无法解释其在实践中的成功，因此需要一种更精确的理论分析方法。

**方法:** 该论文通过实验和理论分析，结合算法和数据相关的因素，研究SGMs的泛化能力。

**结果:** 论文提供了首个结合优化动力学的SGMs泛化界，为理解SGMs的泛化行为提供了新见解。

**结论:** 论文得出了一种新的SGMs泛化分析方法，考虑了优化动力学，并通过实证结果验证了理论发现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Algorithm-+and+Data-Dependent+Generalization+Bounds+for+Score-Based+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.03849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.03849&send_immediately=true&force_search=false)

**原文摘要:** Score-based generative models (SGMs) have emerged as one of the most popular
classes of generative models. A substantial body of work now exists on the
analysis of SGMs, focusing either on discretization aspects or on their
statistical performance. In the latter case, bounds have been derived, under
various metrics, between the true data distribution and the distribution
induced by the SGM, often demonstrating polynomial convergence rates with
respect to the number of training samples. However, these approaches adopt a
largely approximation theory viewpoint, which tends to be overly pessimistic
and relatively coarse. In particular, they fail to fully explain the empirical
success of SGMs or capture the role of the optimization algorithm used in
practice to train the score network. To support this observation, we first
present simple experiments illustrating the concrete impact of optimization
hyperparameters on the generalization ability of the generated distribution.
Then, this paper aims to bridge this theoretical gap by providing the first
algorithmic- and data-dependent generalization analysis for SGMs. In
particular, we establish bounds that explicitly account for the optimization
dynamics of the learning algorithm, offering new insights into the
generalization behavior of SGMs. Our theoretical findings are supported by
empirical results on several datasets.

</details>


### [127] [Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness](https://arxiv.org/abs/2506.04193)
*Stephen R. Pfohl, Natalie Harris, Chirag Nagpal, David Madras, Vishwali Mhasawade, Olawale Salaudeen, Awa Dieng, Shannon Sequeira, Santiago Arciniegas, Lillian Sung, Nnamdi Ezeanochie, Heather Cole-Lewis, Katherine Heller, Sanmi Koyejo, Alexander D'Amour*

**主要类别:** stat.ML

**AI概要:** 该论文讨论了在评估机器学习模型公平性时，单纯使用分组评估可能存在问题，并提出应结合因果假设和分析方法进行改进。


<details>
  <summary>更多</summary>
  
**动机:** 动机是为了解决当前机器学习模型评估中，分组评估方法可能误导实践者的问题。

**方法:** 论文采用了因果图模型来预测在不同的数据生成过程中，评估指标在各子群体间的稳定性。

**结果:** 结果表明，在数据具有代表性但反映现实差异的情况下，各子群体性能相同并不能可靠地衡量公平性；而在存在选择偏差的情况下，分组评估和其他方法可能无效。

**结论:** 论文得出结论，分组评估如果不结合明确的因果假设和分析，可能无法可靠地衡量机器学习模型的公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+challenges+to+the+interpretation+of+disaggregated+evaluations+of+algorithmic+fairness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.04193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.04193&send_immediately=true&force_search=false)

**原文摘要:** Disaggregated evaluation across subgroups is critical for assessing the
fairness of machine learning models, but its uncritical use can mislead
practitioners. We show that equal performance across subgroups is an unreliable
measure of fairness when data are representative of the relevant populations
but reflective of real-world disparities. Furthermore, when data are not
representative due to selection bias, both disaggregated evaluation and
alternative approaches based on conditional independence testing may be invalid
without explicit assumptions regarding the bias mechanism. We use causal
graphical models to predict metric stability across subgroups under different
data generating processes. Our framework suggests complementing disaggregated
evaluations with explicit causal assumptions and analysis to control for
confounding and distribution shift, including conditional independence testing
and weighted performance estimation. These findings have broad implications for
how practitioners design and interpret model assessments given the ubiquity of
disaggregated evaluation.

</details>
