<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 88]
- [cs.AI](#cs.AI) [总数: 20]
- [stat.ML](#stat.ML) [总数: 9]
- [cs.CR](#cs.CR) [总数: 1]
- [cs.CL](#cs.CL) [总数: 2]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen, Xiaotian Song, Yanan Sun*

**Main category:** cs.LG

**TL;DR:** 提出了一种名为LAS的无损ANN-SNN转换方法，可以有效解决现有方法的问题并提高LLMs的性能。


<details>
  <summary>Details</summary>
**Motivation:** 现有的转换方法难以处理基于ANN的LLMs中的极端激活异常值和不兼容的非线性操作。

**Method:** LAS引入了两种新型神经元来转换ANN-LMMs中的激活异常值和非线性操作，并为脉冲LLMs定制了等效Transformer组件。

**Result:** 实验结果表明，LAS在六个语言模型和两个视觉语言模型上实现了无损转换，并且在OPT-66B上提高了WSC任务的准确率2%。参数和消融研究进一步验证了LAS的有效性。

**Conclusion:** LAS实现了一种无损的ANN-SNN转换，能够确保全脉冲转换而不损失任何性能。实验结果显示，LAS在六种语言模型和两种视觉语言模型上实现了无损转换，并且在OPT-66B上甚至提高了WSC任务的准确率2%。参数和消融研究进一步验证了LAS的有效性。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LAS%3A+Loss-less+ANN-SNN+Conversion+for+Fully+Spike-Driven+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09659，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09659&send_immediately=true&force_search=false)

**Abstract:** Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [2] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian*

**Main category:** cs.LG

**TL;DR:** A novel method to adapt large language models for execution on analog in-memory computing hardware achieving performance comparable to 4-bit weight and 8-bit activation baselines.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenge of deploying large language models on analog in-memory computing hardware which suffers from noisy computations and strict constraints on input and output quantization.

**Method:** We introduce a general and scalable method to robustly adapt large language models for execution on noisy, low-precision analog hardware.

**Result:** Our models retain performance comparable to 4-bit weight and 8-bit activation baselines despite analog noise and quantization constraints. They also benefit from test-time compute scaling and can be quantized for inference on low-precision digital hardware.

**Conclusion:** Our method enables state-of-the-art large language models to be executed on analog hardware with performance comparable to 4-bit weight and 8-bit activation baselines.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Analog+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09663&send_immediately=true&force_search=false)

**Abstract:** Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [3] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu, Prathyush Poduval, Wenjun Huang, Yang Ni, Hanning Chen, Mohsen Imani*

**Main category:** cs.LG

**TL;DR:** 本文提出了一种名为FGU的公平图无学习方法，该方法在保持隐私和准确性的同时实现了优越的公平性，并且对各种无学习请求具有鲁棒性。


<details>
  <summary>Details</summary>
**Motivation:** 现有的图无学习方法主要关注于在删除用户信息的同时维持模型预测性能，但当用户信息从模型中删除时，不同敏感组之间的预测分布往往会发生变化。并且图模型容易放大偏差，因此研究图无学习中的公平性尤为重要。

**Method:** FGU方法通过在划分的小图上训练分片模型，在对应的子图中无学习请求的数据，并在修改后的子图上重新训练分片模型来保证隐私。为了确保公平性，FGU采用双层去偏过程：首先在分片模型重新训练时加入公平正则化器实现分片级公平性；然后通过使所有分片模型对齐来最小化全局差异，从而实现全局级公平性。

**Result:** 提出的FGU方法在保持隐私和准确性的同时实现了优越的公平性，并且对各种无学习请求具有鲁棒性，确保了不同数据分布下的公平性和实用性性能。

**Conclusion:** 研究发现图无学习过程确实引入了偏差。为了解决这个问题，提出了公平图无学习方法FGU。实验表明，FGU在保持隐私和准确性的同时实现了优越的公平性，并且对各种无学习请求具有鲁棒性，确保了不同数据分布下的公平性和实用性性能。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enabling+Group+Fairness+in+Graph+Unlearning+via+Bi-level+Debiasing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09702&send_immediately=true&force_search=false)

**Abstract:** Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [4] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira, Fernanda Famá, Charalampos Kalalas, Paolo Dini*

**Main category:** cs.LG

**TL;DR:** 本文研究了AIoT中联邦学习的能量消耗，并通过聚类引导的方法优化设备选择，提高了收敛率并降低了能耗。


<details>
  <summary>Details</summary>
**Motivation:** 现有文献对联邦学习在AIoT场景中的能量影响关注不足，而本文旨在填补这一空白。

**Method:** 提出两种聚类引导的方法，用于优化AIoT设备的选择，减少联邦学习过程中的能量消耗。

**Result:** 实验表明，所提出的聚类策略在提高收敛率和降低能耗方面优于其他现有方法。

**Conclusion:** 本文研究了联邦学习在AIoT场景中的能量消耗问题，提出了两种聚类引导的方法来优化设备选择，从而在保持低能耗的同时提高了模型训练的收敛速度。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Efficient+Federated+Learning+for+AIoT+using+Clustering+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09704&send_immediately=true&force_search=false)

**Abstract:** While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [5] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos, Petros Maragos*

**Main category:** cs.LG

**TL;DR:** Investigate deep morphological neural networks (DMNNs), propose new architectures with different parameter constraints, show they can be successfully trained and are more prunable than linear networks.


<details>
  <summary>Details</summary>
**Motivation:** To explore the potential of DMNNs and their applications.

**Method:** Propose several new architectures for DMNNs with different constraints on their parameters.

**Result:** The proposed networks can be successfully trained and are more prunable than linear networks.

**Conclusion:** Successfully trained DMNNs under certain constraints, but their generalization capabilities remain limited. Hybrid network architecture combining linear and morphological layers accelerates the convergence of gradient descent with large batches.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+Deep+Morphological+Neural+Networks+as+Universal+Approximators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09710&send_immediately=true&force_search=false)

**Abstract:** We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [6] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

**Main category:** cs.LG

**TL;DR:** This paper explores the problem of out-of-distribution (OOD) generalization in intelligent systems. It argues that simply testing on an OOD setup is insufficient to confirm that an algorithm learns compositional structures from data. The authors demonstrate this by examining two tasks where common neural networks fail to solve OOD problems. They also introduce two new network architectures with specific biases that enable successful OOD performance, yet still may not correctly learn the features necessary for compositional generalization.


<details>
  <summary>Details</summary>
**Motivation:** To investigate whether current algorithms can truly learn compositional structures from data, beyond just achieving good performance on OOD setups.

**Method:** Testing various neural networks including MLP, CNN, and Transformer on tasks with clear OOD metrics, and developing two novel network architectures with specific biases.

**Result:** Common neural networks fail to solve OOD problems, while the new architectures perform well but might still fail to learn the right features for compositional generalization.

**Conclusion:** Achieving good OOD performance doesn't necessarily mean an algorithm has learned compositional structures; additional confirmation is needed.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Out-of-distribution+generalisation+is+hard%3A+evidence+from+ARC-like+tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09716&send_immediately=true&force_search=false)

**Abstract:** Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [7] [Rapid Overfitting of Multi-Pass Stochastic Gradient Descent in Stochastic Convex Optimization](https://arxiv.org/abs/2505.08306)
*Shira Vansover-Hager, Tomer Koren, Roi Livni*

**Main category:** cs.LG

**TL;DR:** This paper studies the out-of-sample performance of multi-pass stochastic gradient descent (SGD) in stochastic convex optimization (SCO), revealing that multi-pass SGD can lead to overfitting and poor out-of-sample performance in non-smooth cases.


<details>
  <summary>Details</summary>
**Motivation:** To investigate the performance of multi-pass SGD in SCO compared to one-pass SGD which is known to perform optimally.

**Method:** Theoretical analysis of multi-pass SGD's out-of-sample performance in SCO.

**Result:** Multi-pass SGD can significantly harm out-of-sample performance and cause overfitting in non-smooth SCO problems, showing a phase-transition after the first epoch.

**Conclusion:** The findings highlight the risks of using multi-pass SGD in non-smooth SCO and suggest careful tuning of parameters to avoid overfitting.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rapid+Overfitting+of+Multi-Pass+Stochastic+Gradient+Descent+in+Stochastic+Convex+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.08306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.08306&send_immediately=true&force_search=false)

**Abstract:** We study the out-of-sample performance of multi-pass stochastic gradient
descent (SGD) in the fundamental stochastic convex optimization (SCO) model.
While one-pass SGD is known to achieve an optimal $\Theta(1/\sqrt{n})$ excess
population loss given a sample of size $n$, much less is understood about the
multi-pass version of the algorithm which is widely used in practice. Somewhat
surprisingly, we show that in the general non-smooth case of SCO, just a few
epochs of SGD can already hurt its out-of-sample performance significantly and
lead to overfitting. In particular, using a step size $\eta =
\Theta(1/\sqrt{n})$, which gives the optimal rate after one pass, can lead to
population loss as large as $\Omega(1)$ after just one additional pass. More
generally, we show that the population loss from the second pass onward is of
the order $\Theta(1/(\eta T) + \eta \sqrt{T})$, where $T$ is the total number
of steps. These results reveal a certain phase-transition in the out-of-sample
behavior of SGD after the first epoch, as well as a sharp separation between
the rates of overfitting in the smooth and non-smooth cases of SCO.
Additionally, we extend our results to with-replacement SGD, proving that the
same asymptotic bounds hold after $O(n \log n)$ steps. Finally, we also prove a
lower bound of $\Omega(\eta \sqrt{n})$ on the generalization gap of one-pass
SGD in dimension $d = \smash{\widetilde O}(n)$, improving on recent results of
Koren et al.(2022) and Schliserman et al.(2024).

</details>


### [8] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen, Ali Boyaci*

**Main category:** cs.LG

**TL;DR:** This study introduces a federated learning method that improves model performance by addressing data quality issues like noisy labels, missing classes, and imbalanced distributions.


<details>
  <summary>Details</summary>
**Motivation:** To solve data quality problems in federated learning which hinder its effectiveness.

**Method:** Adaptive noise cleaning, synthetic data generation using collaborative conditional GANs, and robust federated model training.

**Result:** Significant improvements in federated model performance on benchmark datasets under various noise and class imbalance conditions.

**Conclusion:** The proposed method effectively tackles common data quality challenges in federated learning, offering a robust, scalable, and privacy-compliant solution.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Federated+Learning+with+Confidence-Weighted+Filtering+and+GAN-Based+Completion+under+Noisy+and+Incomplete+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09733&send_immediately=true&force_search=false)

**Abstract:** Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [9] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni, Guilherme Weigert Cassales, Bernhard Pfahringer, Albert Bifet, Giacomo Boracchi*

**Main category:** cs.LG

**TL;DR:** Propose a new method called Online-iForest for anomaly detection in streaming conditions.


<details>
  <summary>Details</summary>
**Motivation:** Existing methods have impractical assumptions or fail to adapt to online contexts.

**Method:** Online-iForest designed specifically for streaming conditions

**Result:** Performs well compared to online alternatives and rivals state-of-the-art offline techniques in real-world datasets

**Conclusion:** Online-iForest is efficient and suitable for applications requiring fast anomaly detection like cybersecurity, fraud and fault detection.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Online+Isolation+Forest，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09593&send_immediately=true&force_search=false)

**Abstract:** The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [10] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang, Massimiliano Di Ventra*

**Main category:** cs.LG

**TL;DR:** 提出了一种用于黑盒组合优化的生成式端到端求解器，该求解器在NP问题上既强调样本效率又注重解决方案质量。通过从基于退火的算法中汲取灵感，将黑盒目标视为能量函数，并训练神经网络来建模相关的玻尔兹曼分布。


<details>
  <summary>Details</summary>
**Motivation:** 提高黑盒组合优化的样本效率和解决方案质量。

**Method:** 将黑盒目标视为能量函数并训练神经网络来建模相关的玻尔兹曼分布。

**Result:** 验证了该方法在具有挑战性的组合任务中的有效性，在有限和无限查询预算下都表现出了与最先进的黑盒优化器相当的性能。

**Conclusion:** 提出的求解器在黑盒组合优化中表现出色，特别是在样本效率和解决方案质量方面。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Generative+Neural+Annealer+for+Black-Box+Combinatorial+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09742&send_immediately=true&force_search=false)

**Abstract:** We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [11] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

**Main category:** cs.LG

**TL;DR:** A novel multi-agent reinforcement learning framework is proposed to capture flexible coordination patterns through community structures.


<details>
  <summary>Details</summary>
**Motivation:** To develop a more flexible and efficient way for agents to coordinate in dynamic networks with overlapping communities.

**Method:** Agents can belong to multiple overlapping communities maintaining shared policy and value functions, which they use based on personalized membership weights. Actor-critic algorithms exploit this structure for policy updates and value learning.

**Result:** The framework supports transfer learning and active learning with theoretical convergence guarantees.

**Conclusion:** This is the first MARL framework integrating community structure, transferability, and active learning with provable guarantees.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Community-based+Multi-Agent+Reinforcement+Learning+with+Transfer+and+Active+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09756，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09756&send_immediately=true&force_search=false)

**Abstract:** We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [12] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao, Olurotimi Seton, Himadeep Reddy Reddivari, Suvendu Jena, Shadow Zhao, Rachit Kumar, Changshuai Wei*

**Main category:** cs.LG

**TL;DR:** An approach named Causal Predictive Optimization and Generation is introduced for sales optimization and business AI, featuring three layers: causal ML prediction, constraint optimization with contextual bandit, and Generative AI serving layer with feedback-loop.


<details>
  <summary>Details</summary>
**Motivation:** Optimizing the sales process is crucial for the success of B2B businesses as it converts leads to customers and increases sales to existing ones.

**Method:** The approach includes three layers: 1) causal ML prediction, 2) constraint optimization and contextual bandit optimization, and 3) Generative AI serving layer with feedback-loop.

**Result:** The system was implemented and deployed at LinkedIn, showing significant improvements over previous systems.

**Conclusion:** This principled approach to sales optimization and business AI has broad applicability in the field.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Predictive+Optimization+and+Generation+for+Business+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09847，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09847&send_immediately=true&force_search=false)

**Abstract:** The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [13] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei, Xueru Zhang*

**Main category:** cs.LG

**TL;DR:** Generative models create synthetic data that can lead to 'self-consuming loops' causing model collapse or instability. When data is curated based on user preferences, models may converge towards these preferences but can be disrupted by noisy or adversarial curation. This paper studies the impact of such curation on models within retraining loops and designs attack algorithms for adversarial competitive scenarios.


<details>
  <summary>Details</summary>
**Motivation:** To understand the effect of noisy and adversarially curated data on generative models in self-consuming retraining loops and design methods to counteract adverse impacts.

**Method:** Theoretical analysis of the impact of noisy data curation on generative models and development of attack algorithms for adversarial competitive scenarios.

**Result:** Experiments on synthetic and real-world datasets show the effectiveness of the proposed algorithms.

**Conclusion:** Noisy and adversarial data curation can significantly affect generative models in self-consuming retraining loops, and specific conditions for robustness were identified.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Consuming+Generative+Models+with+Adversarially+Curated+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09768&send_immediately=true&force_search=false)

**Abstract:** Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [14] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen, Shengbo Wang, Nian Si*

**Main category:** cs.LG

**TL;DR:** 研究了分布鲁棒平均奖励强化学习问题，提出了两种新算法并证明了它们的样本复杂度。


<details>
  <summary>Details</summary>
**Motivation:** 在机器人学、运筹学和医疗保健等实际应用中，稳定长期性能至关重要，因此研究了分布鲁棒平均奖励强化学习问题。

**Method:** 提出了两种新的算法，第一种算法将问题转化为分布鲁棒折扣马尔可夫决策过程；第二种算法引入锚状态来稳定不确定性集内的控制转移核。

**Result:** 两种算法都达到了近似的最优样本复杂度，并且在名义马尔可夫决策过程一致遍历的情况下，证明了这两种算法对于估计最优策略和鲁棒平均奖励具有样本复杂度。这是首次对分布鲁棒平均奖励强化学习提供有限样本收敛保证。

**Conclusion:** 提出了两种新的算法，分别通过将问题转化为分布鲁棒折扣马尔可夫决策过程和引入锚状态来稳定不确定性集内的控制转移核来实现。在假设名义马尔可夫决策过程一致遍历的情况下，证明了这两种算法对于估计最优策略和鲁棒平均奖励具有样本复杂度。这是首次对分布鲁棒平均奖励强化学习提供有限样本收敛保证。并且通过数值实验验证了算法的收敛率。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+Complexity+of+Distributionally+Robust+Average-Reward+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10007，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10007&send_immediately=true&force_search=false)

**Abstract:** Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [15] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni, Luca Magri, Giacomo Boracchi, Cesare Alippi*

**Main category:** cs.LG

**TL;DR:** This paper presents PIF, a novel anomaly detection method that combines adaptive isolation methods with preference embedding, showing superior performance over existing techniques on various datasets.


<details>
  <summary>Details</summary>
**Motivation:** To address the problem of detecting anomalies with respect to structured patterns.

**Method:** PIF method combining adaptive isolation methods with preference embedding in a high dimensional space and using PI-Forest to compute anomaly scores.

**Result:** Experiments on synthetic and real datasets demonstrate that PIF outperforms state-of-the-art anomaly detection techniques and PI-Forest is better at measuring arbitrary distances and isolating points in the preference space.

**Conclusion:** PIF, which combines adaptive isolation methods with preference embedding, outperforms existing techniques on various datasets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PIF%3A+Anomaly+detection+via+preference+embedding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10441&send_immediately=true&force_search=false)

**Abstract:** We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [16] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington, Cornel Constantinescu*

**Main category:** cs.LG

**TL;DR:** This paper analyzes checkpoint data used in Large Language Models training and proposes an optimized compression solution called Language Model Compressor (LMC).


<details>
  <summary>Details</summary>
**Motivation:** To maximize the use of lossless compression to reduce the volume of data during checkpointing.

**Method:** Experimental analysis of checkpoint data and building an effective compression solution based on byte-grouping and Huffman encoding.

**Result:** LMC offers better compression performance than BZ2 with significantly less time needed for compression. A 16-core parallel implementation of LMC achieves 2.78 GiB/s compression and 3.76 GiB/s decompression throughput.

**Conclusion:** LMC reduces the CPU resources needed and allows for higher-frequency checkpoints.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lossless+Compression+for+LLM+Tensor+Incremental+Snapshots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09810&send_immediately=true&force_search=false)

**Abstract:** During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [17] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu, Yizhou Liu, Jeff Gore, Max Tegmark*

**Main category:** cs.LG

**TL;DR:** This paper introduces Neural Thermodynamic Laws (NTL), a framework that applies thermodynamic concepts to understand large language model training dynamics.


<details>
  <summary>Details</summary>
**Motivation:** To explore laws beyond neural scaling laws for large language models.

**Method:** Demonstrates emergence of thermodynamic quantities and principles on river-valley loss landscape assumptions.

**Result:** Provides intuitive guidelines for designing learning rate schedules.

**Conclusion:** Introduces NTL as a new framework offering insights into LLM training dynamics.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Thermodynamic+Laws+for+Large+Language+Model+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10559&send_immediately=true&force_search=false)

**Abstract:** Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


### [18] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova, Stefan Eftimov, Bojan Ristov, Slobodan Kalajdziski*

**Main category:** cs.LG

**TL;DR:** This study examines the effectiveness of machine learning algorithms in predicting stroke risk using various data types. It addresses issues like class imbalance and missing data, evaluates different models, identifies important predictive features, and suggests ways to improve stroke prediction models.


<details>
  <summary>Details</summary>
**Motivation:** To develop more reliable and interpretable models for early assessment of stroke risk by using machine learning algorithms.

**Method:** Using demographic, clinical, and lifestyle data from the Stroke Prediction Dataset, evaluating the performance of Logistic Regression, Random Forest, and XGBoost models while addressing class imbalance and missing data issues.

**Result:** The models achieved high accuracy but had low sensitivity which is a limiting factor for real-world clinical applications. The most influential predictive features were identified.

**Conclusion:** The study contributes to the development of more reliable and interpretable machine learning models for predicting stroke risk.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+Analysis+of+Stroke+Prediction+Models+Using+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09812，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09812&send_immediately=true&force_search=false)

**Abstract:** Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [19] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu*

**Main category:** cs.LG

**TL;DR:** This paper introduces an intrinsic optimization technique using exponentiated gradient descent and Bregman projection for jailbreaking large language models more efficiently.


<details>
  <summary>Details</summary>
**Motivation:** To address the vulnerability of large language models to jailbreaking attacks and improve their safety and potential usage.

**Method:** Developing an intrinsic optimization technique with exponentiated gradient descent and Bregman projection to optimize continuous token embeddings while ensuring they stay within the probability simplex.

**Result:** The technique achieved a higher success rate with great efficiency compared to three other state-of-the-art jailbreaking techniques on five open-source LLMs and four datasets.

**Conclusion:** This study demonstrates the effectiveness of the proposed method in jailbreaking large language models more efficiently.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Attack+on+Large+Language+Models+using+Exponentiated+Gradient+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09820，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09820&send_immediately=true&force_search=false)

**Abstract:** As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [20] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi, Gal Mishne*

**Main category:** cs.LG

**TL;DR:** This paper studies the problem of learning a Kronecker-structured product graph from smooth signals and proposes an alternating scheme to optimize each factor graph.


<details>
  <summary>Details</summary>
**Motivation:** Graph learning, or network inference, is a prominent problem in graph signal processing (GSP). GSP generalizes the Fourier transform to non-Euclidean domains, and graph learning is pivotal to applying GSP when these domains are unknown.

**Method:** We propose an alternating scheme to optimize each factor graph and provide theoretical guarantees for its asymptotic convergence.

**Result:** The proposed algorithm is also modified to learn factor graphs of the strong product. We conduct experiments on synthetic and real-world graphs and demonstrate our approach's efficacy and superior performance compared to existing methods.

**Conclusion:** We study the problem of learning a Kronecker-structured product graph from smooth signals.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Kronecker-Structured+Graphs+from+Smooth+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09822，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09822&send_immediately=true&force_search=false)

**Abstract:** Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [21] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj, Golrokh Mirzaei*

**Main category:** cs.LG

**TL;DR:** This study presents a new method that uses both brain images and gene data to identify different stages of Alzheimer's disease and find important genes related to these stages.


<details>
  <summary>Details</summary>
**Motivation:** To integrate imaging and genomic data to uncover new information about diseases.

**Method:** A novel heterogeneous bipartite graph representation learning approach with two types of nodes: genes and images.

**Result:** Effectively classifies Alzheimer's disease into three stages using a small dataset and identifies significant genes for each stage.

**Conclusion:** The approach has potential for extending radiogenomic-based disease classification to other diseases.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Radiogenomic+Bipartite+Graph+Representation+Learning+for+Alzheimer%27s+Disease+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09848&send_immediately=true&force_search=false)

**Abstract:** Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [22] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang, Shun-Li Shang, Zi-Kui Liu, Wenrui Hao*

**Main category:** cs.LG

**TL;DR:** 提出了一种基于zentropy理论的新型神经网络模型ZENN，用于从异构数据中有效学习，具有良好的泛化能力和鲁棒性，在科学问题中显示出广泛应用前景。


<details>
  <summary>Details</summary>
**Motivation:** 传统基于熵的方法在处理异构数据集时面临挑战，特别是当这些数据集存在固有差异时。因此，需要一种新的方法来更有效地从异构数据源中学习。

**Method:** 引入了内在熵的概念，设计了一种同时学习能量和内在熵成分的zentropy增强神经网络(ZENN)，并重新设计了神经网络架构以更好地反映多源数据的内在特性和变异性。

**Result:** ZENN在分类任务和能量景观重建任务上表现出了优越的性能，特别是在预测高阶导数方面。此外，ZENN成功地重建了Fe3Pt的Helmholtz能量景观，并捕捉到了关键的材料行为。

**Conclusion:** 提出了基于zentropy理论的ZENN模型，该模型在处理异构数据集时表现出优越的泛化能力和鲁棒性，特别是在预测高阶导数方面。ZENN能够重建Fe3Pt的Helmholtz能量景观并捕捉关键材料行为，展示了其在科学问题中的应用潜力。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZENN%3A+A+Thermodynamics-Inspired+Computational+Framework+for+Heterogeneous+Data-Driven+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09851，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09851&send_immediately=true&force_search=false)

**Abstract:** Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [23] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil, Katia Obraczka*

**Main category:** cs.LG

**TL;DR:** Distributed learning at the network edge faces challenges in connectivity and synchronization. This paper introduces Chisme, a novel suite of protocols that addresses these issues by including both synchronous DFL and asynchronous GL variants.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to implement robust intelligence in the network edge characterized by heterogeneous data distributions, episodic connectivity, and lack of infrastructure.

**Method:** Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training. A data similarity heuristic is introduced to allow agents to infer affinity with each other using existing communication of model updates.

**Result:** Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.

**Conclusion:** This paper presents a new approach to distributed learning at the network edge that addresses challenges in connectivity and synchronization.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chisme%3A+Fully+Decentralized+Differentiated+Deep+Learning+for+Edge+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09854&send_immediately=true&force_search=false)

**Abstract:** As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [24] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku, Thomas L. Griffiths, Stephanie C. Y. Chan*

**Main category:** cs.LG

**TL;DR:** This paper explores how environmental factors affect the balance between in-weights and in-context learning in transformers, drawing analogies from evolutionary biology.


<details>
  <summary>Details</summary>
**Motivation:** To understand the interplay between in-weights learning and in-context learning in transformer models by drawing parallels from evolutionary biology's adaptive strategies.

**Method:** Experiments were conducted using regression and classification tasks to investigate the effect of environmental stability and cue reliability on the ICL/IWL balance in Transformers.

**Result:** High environmental stability strongly favors IWL, whereas high cue reliability enhances ICL efficacy, especially under low stability. Task-contingent temporal evolution also occurs, showcasing different patterns of learning mode transitions.

**Conclusion:** The study concludes that environmental predictability significantly influences the balance between in-weights learning (IWL) and in-context learning (ICL) in transformers.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predictability+Shapes+Adaptation%3A+An+Evolutionary+Perspective+on+Modes+of+Learning+in+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09855，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09855&send_immediately=true&force_search=false)

**Abstract:** Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [25] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei*

**Main category:** cs.LG

**TL;DR:** This paper introduces a unified transformer-based attribution approach for marketing interactions.


<details>
  <summary>Details</summary>
**Motivation:** To assign conversion credits to marketing interactions based on causal patterns learned from data.

**Method:** A unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors.

**Result:** The approach has been implemented at LinkedIn with significant impact.

**Conclusion:** The approach and insights shared in this paper are broadly applicable to the marketing and ad tech fields.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LiDDA%3A+Data+Driven+Attribution+at+LinkedIn，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09861&send_immediately=true&force_search=false)

**Abstract:** Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [26] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni, Filippo Leveni, Diego Stucchi, Luca Frittoli, Giacomo Boracchi*

**Main category:** cs.LG

**TL;DR:** KQT-EWMA is a non-parametric change-detection algorithm that combines Kernel-QuantTree histogram and EWMA statistic to monitor multivariate data streams online, with ability to control false alarms and achieve competitive detection delays.


<details>
  <summary>Details</summary>
**Motivation:** To develop a flexible and practical non-parametric change-detection algorithm for monitoring multivariate data streams online.

**Method:** Combining Kernel-QuantTree histogram and the EWMA statistic.

**Result:** KQT-EWMA can control false alarms and achieve detection delays comparable to or lower than state-of-the-art methods.

**Conclusion:** KQT-EWMA can control false alarms and achieve detection delays comparable to or lower than state-of-the-art methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Change+Detection+in+Multivariate+data+streams%3A+Online+Analysis+with+Kernel-QuantTree，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2410.13778，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2410.13778&send_immediately=true&force_search=false)

**Abstract:** We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [27] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

**Main category:** cs.LG

**TL;DR:** BINGO is a method introduced to reduce the computational cost of training large machine learning models by pruning insignificant weights in one shot.


<details>
  <summary>Details</summary>
**Motivation:** The high cost of training large models limits their accessibility and affordability.

**Method:** BINGO studies specific subsets of a neural network during training to determine the significance of each weight.

**Result:** BINGO achieves accuracy-preserving pruning with lower computational intensity compared to existing methods.

**Conclusion:** BINGO offers a solution to make AI development more accessible and affordable without sacrificing model accuracy.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BINGO%3A+A+Novel+Pruning+Mechanism+to+Reduce+the+Size+of+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09864，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09864&send_immediately=true&force_search=false)

**Abstract:** Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [28] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora, Dipen Pradhan, Ankit Shetgaonkar, Aman Raj*

**Main category:** cs.LG

**TL;DR:** This paper surveys adversarial attacks targeting all four modalities (text, image, video, and audio) in multimodal models, providing a view of the adversarial attack landscape and presenting how multimodal adversarial threats have evolved. It fills a gap in the practitioner-focused view of attack types.


<details>
  <summary>Details</summary>
**Motivation:** To help machine learning practitioners view the threat landscape and take preventive actions against adversarial attacks in multimodal models.

**Method:** Surveying adversarial attacks targeting all four modalities: text, image, video, and audio.

**Result:** Provides a comprehensive summarization of the threat landscape in the multimodal world.

**Conclusion:** This is the first comprehensive summarization of the threat landscape in the multimodal world.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Attacks+in+Multimodal+Systems%3A+A+Practitioner%27s+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.03084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.03084&send_immediately=true&force_search=false)

**Abstract:** The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [29] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang, Darcy Wang, Ningyuan Chen, Rodrigo Mansur, Vahid Sarhangian*

**Main category:** cs.LG

**TL;DR:** Large language models (LLMs) were analyzed in terms of their exploration-exploitation (E&E) tradeoff strategies compared to humans and MAB algorithms. Reasoning was found to shift LLMs toward more human-like behavior.


<details>
  <summary>Details</summary>
**Motivation:** To examine if LLMs exhibit similar decision-making behavior to humans and can achieve comparable (or superior) performance.

**Method:** Canonical multi-armed bandit (MAB) tasks were used to compare the E&E strategies of LLMs, humans, and MAB algorithms. Interpretable choice models were employed to capture the E&E strategies.

**Result:** In simple stationary tasks, reasoning-enabled LLMs exhibited similar levels of random and directed exploration compared to humans. However, in complex, non-stationary environments, LLMs struggled to match human adaptability, particularly in effective directed exploration.

**Conclusion:** LLMs show promise as simulators of human behavior and tools for automated decision-making but have limitations, especially in complex, non-stationary environments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparing+Exploration-Exploitation+Strategies+of+LLMs+and+Humans%3A+Insights+from+Standard+Multi-armed+Bandit+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09901，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09901&send_immediately=true&force_search=false)

**Abstract:** Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [30] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang, LuFeng, Ruijia Liang*

**Main category:** cs.LG

**TL;DR:** Propose a hybrid deep learning model (TCN-MLP-Attention) for Hass avocado price forecasting which combines TCN, MLP, and attention mechanisms. This model shows better performance than traditional methods.


<details>
  <summary>Details</summary>
**Motivation:** To meet the increasing demand for healthy food, Hass avocado price forecasting is crucial due to its complex price fluctuations affected by seasonality, region, and weather.

**Method:** Develop a TCN-MLP-Attention model that uses TCN for sequential feature extraction, MLP for nonlinear interactions, and attention for dynamic feature weighting.

**Result:** The proposed model achieved an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.

**Conclusion:** The TCN-MLP-Attention model provides a scalable and effective solution for time series forecasting in agricultural markets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Avocado+Price+Prediction+Using+a+Hybrid+Deep+Learning+Model%3A+TCN-MLP-Attention+Architecture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09907&send_immediately=true&force_search=false)

**Abstract:** With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [31] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu, Wei Zhang, Tiejun Li*

**Main category:** cs.LG

**TL;DR:** This paper studies Euclidean diffusion models for manifold-constrained data without explicit manifold structure usage. It presents two novel methods, Niso-DM and Tango-DM, to improve sampling accuracy by addressing the singularity of the score function.


<details>
  <summary>Details</summary>
**Motivation:** To develop methods for improving sampling accuracy of diffusion models on complex manifold-constrained data without relying on explicit manifold structures.

**Method:** Introduce non-isotropic noise along the normal direction (Niso-DM) and train only the tangential component of the score function using a tangential-only loss function (Tango-DM).

**Result:** The proposed methods achieve better performance on distributions over various manifolds with complex geometries.

**Conclusion:** Direct sampling of Euclidean diffusion models can be improved by addressing the singularity of the score function through non-isotropic noise introduction and tangential component training.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+the+Euclidean+Diffusion+Generation+of+Manifold+Data+by+Mitigating+Score+Function+Singularity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09922，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09922&send_immediately=true&force_search=false)

**Abstract:** Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [32] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang, Jie Zhou, Junsong Li, Qianjun Pan, Bihao Zhan, Qin Chen, Xipeng Qiu, Liang He*

**Main category:** cs.LG

**TL;DR:** This paper proposes RiCL, a Reinforced interactive Continual Learning framework, which allows AI models to learn new skills dynamically from real-time human feedback while preserving previous knowledge. It also handles noisy feedback and shows better performance than existing methods.


<details>
  <summary>Details</summary>
**Motivation:** Traditional continual learning approaches have limitations such as relying on static datasets and assuming clean labels, which do not apply to real-world scenarios where data is often noisy and constantly changing.

**Method:** RiCL framework includes a purifier for cleaning data, a strategy for aligning model behavior with human intent, and a module for capturing robust representations, all leveraging LLMs.

**Result:** Experiments on two benchmark datasets with realistic noise show that RiCL outperforms existing methods.

**Conclusion:** The proposed RiCL framework successfully addresses limitations of traditional continual learning by handling dynamic and noisy feedback.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforced+Interactive+Continual+Learning+via+Real-time+Noisy+Human+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09925，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09925&send_immediately=true&force_search=false)

**Abstract:** This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [33] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Samgyu Yang, Abdulrahman Faden*

**Main category:** cs.LG

**TL;DR:** This research uses a fine-tuned large language model to analyze freeway crash data and identify primary crash causes, demonstrating its effectiveness and potential for improving traffic safety measures.


<details>
  <summary>Details</summary>
**Motivation:** To understand the factors contributing to traffic crashes and develop strategies to mitigate their severity.

**Method:** Leveraging large language model (LLM) to analyze freeway crash data and provide crash causation analysis. Fine-tuning the Llama3 8B model using QLoRA. Using zero-shot classification to identify crash causation without pre-labeled data.

**Result:** The fine-tuned Llama3 8B model effectively identified primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data offers more profound insights. There was a high level of agreement among researchers in the field of traffic safety.

**Conclusion:** LLMs can effectively identify primary crash causes and provide comprehensive explanations. They have practical applicability and potential to improve traffic safety measures.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advanced+Crash+Causation+Analysis+for+Freeway+Safety%3A+A+Large+Language+Model+Approach+to+Identifying+Key+Contributing+Factors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09949&send_immediately=true&force_search=false)

**Abstract:** Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [34] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai, Jie Zhou, Yuxuan Cai, Qin Chen, Wen Wu, Xingjiao Wu, Xipeng Qiu, Liang He*

**Main category:** cs.LG

**TL;DR:** This paper addresses the problem of catastrophic forgetting in long-term continual learning, proposing a novel framework called Long-CL inspired by human memory mechanisms. It introduces a task-core memory management strategy and a long-term memory consolidation mechanism, outperforming previous state-of-the-art methods by 7.4% and 6.5% AP on two newly constructed benchmarks.


<details>
  <summary>Details</summary>
**Motivation:** To handle the challenge of catastrophic forgetting in long-term continual learning involving a large number of tasks.

**Method:** Proposes a novel framework named Long-CL with a task-core memory management strategy and a long-term memory consolidation mechanism.

**Result:** Experimental results show that Long-CL outperforms previous state-of-the-art methods by 7.4% and 6.5% AP on the two constructed benchmarks.

**Conclusion:** The proposed Long-CL framework effectively mitigates catastrophic forgetting in long-term continual learning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Task-Core+Memory+Management+and+Consolidation+for+Long-term+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09952，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09952&send_immediately=true&force_search=false)

**Abstract:** In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [35] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim, Seulki Lee*

**Main category:** cs.LG

**TL;DR:** A new method called TransPL is proposed to improve unsupervised domain adaptation for time series data by explicitly modeling temporal transitions and channel-wise shifts.


<details>
  <summary>Details</summary>
**Motivation:** Traditional pseudo-labeling strategies fail to capture temporal patterns and channel-wise shifts between domains, resulting in sub-optimal pseudo-labels.

**Method:** TransPL uses code transition matrices derived from vector quantization of time series patches and applies Bayes' rule for target domain adaptation to generate pseudo-labels.

**Result:** TransPL outperforms existing methods by 6.1% accuracy and 4.9% F1 on four time series UDA benchmarks.

**Conclusion:** TransPL provides an effective and explainable solution for unsupervised domain adaptation of time series data.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TransPL%3A+VQ-Code+Transition+Matrices+for+Pseudo-Labeling+of+Time+Series+Unsupervised+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09955，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09955&send_immediately=true&force_search=false)

**Abstract:** Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [36] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo, Bohui An, Zhongqi Lu*

**Main category:** cs.LG

**TL;DR:** This paper proposes FedRAG, a federated reinforcement learning framework that enhances performance by sharing approximated behavior metric-based state projection functions while protecting sensitive information.


<details>
  <summary>Details</summary>
**Motivation:** To improve the performance of federated reinforcement learning while ensuring privacy protection by sharing useful information without disclosing sensitive task-specific details.

**Method:** Introducing FedRAG which learns a projection function of states for each client and aggregates the parameters of these functions at a central server.

**Result:** Extensive experiments on the DeepMind Control Suite show insightful results.

**Conclusion:** Sharing approximated behavior metric-based state projection functions is a promising way to enhance federated reinforcement learning performance while effectively protecting sensitive information.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Approximated+Behavioral+Metric-based+State+Projection+for+Federated+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09959，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09959&send_immediately=true&force_search=false)

**Abstract:** Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [37] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir, Shiva Razzagzadeh, Zeynab Rezaei*

**Main category:** cs.LG

**TL;DR:** This study developed a machine learning framework using Logistic Regression, KNN, and Random Forest for heart disease prediction. The Random Forest classifier showed the best performance with 91% accuracy and 0.89 F1-score.


<details>
  <summary>Details</summary>
**Motivation:** To predict heart disease using the heart-disease dataset.

**Method:** Data preprocessing, model training with Logistic Regression, KNN, and Random Forest, hyperparameter tuning with GridSearchCV and RandomizedSearchCV.

**Result:** The Random Forest classifier achieved 91% accuracy and 0.89 F1-score, showing balanced performance across classes.

**Conclusion:** The proposed machine learning model has strong potential for aiding clinical decision-making in heart disease prediction but needs further development with larger and more diverse datasets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comprehensive+Machine+Learning+Framework+for+Heart+Disease+Prediction%3A+Performance+Evaluation+and+Future+Perspectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09969&send_immediately=true&force_search=false)

**Abstract:** This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [38] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao, Zhuoran Xiao, Yihang Huang, Chenhui Ye, Yijia Feng, Liyu Cai, Jiang Chang, Fangkun Liu, Yin Xu, Dazhi He, Yunfeng Guan, Wenjun Zhang*

**Main category:** cs.LG

**TL;DR:** This paper proposes AI2MMUM, a scalable task-aware artificial intelligence-air interface multi-modal universal model for processing multi-modal data and executing diverse air interface tasks in 6G systems.


<details>
  <summary>Details</summary>
**Motivation:** To design a universal model capable of handling multi-modal data and executing diverse air interface tasks in future wireless systems.

**Method:** AI2MMUM uses a LLM backbone with contextual comprehension and generalization capabilities, frozen radio modality encoders, adapter layers, and lightweight task-specific heads. Task instructions include fixed keywords and learnable prefix prompts.

**Result:** AI2MMUM achieved SOTA performance across five representative physical environment/wireless channel-based downstream tasks using the WAIR-D and DeepMIMO datasets.

**Conclusion:** The proposed AI2MMUM model demonstrates flexibility and effectiveness in performing various physical layer tasks according to subtle task instructions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI2MMUM%3A+AI-AI+Oriented+Multi-Modal+Universal+Model+Leveraging+Telecom+Domain+Large+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10003&send_immediately=true&force_search=false)

**Abstract:** Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [39] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu*

**Main category:** cs.LG

**TL;DR:** 提出ImagineBench，首个综合基准，用于评估结合真实与想象滚动的离线强化学习算法，发现现有算法在利用想象滚动方面表现不佳，强调了算法改进的必要性。


<details>
  <summary>Details</summary>
**Motivation:** 解决强化学习依赖大量真实世界交互数据的问题，通过利用大语言模型生成的虚拟经验（想象滚动）来减少对真实数据的依赖。

**Method:** 提出ImagineBench，一个综合基准，用于评估离线强化学习算法，该算法利用真实滚动和大语言模型生成的想象滚动。

**Result:** 现有的离线强化学习算法在未见过的任务上表现不佳，成功率为35.44%，而针对真实滚动训练的方法在困难任务中的成功率为64.37%。

**Conclusion:** 需要改进算法以更好地利用大语言模型生成的想象滚动，并提出了未来研究的关键方向，包括更好的利用想象滚动、快速在线适应和持续学习以及多模态任务的扩展。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ImagineBench%3A+Evaluating+Reinforcement+Learning+with+Large+Language+Model+Rollouts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10010，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10010&send_immediately=true&force_search=false)

**Abstract:** A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [40] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito, Lysenko Artem, Tatsuhiko Tsunoda*

**Main category:** cs.LG

**TL;DR:** This paper introduces a novel normalization strategy for Quantum-classical Hybrid Machine Learning models to improve their performance and stability in predicting anti-cancer drug responses, especially given limited datasets.


<details>
  <summary>Details</summary>
**Motivation:** To overcome the sensitivity of hybrid models to data encoding and improve stability and performance for small datasets in anti-cancer drug response prediction.

**Method:** Proposes a normalization function based on a moderated gradient version of tanh to transform neural network outputs avoiding extreme value ranges.

**Result:** Demonstrated that Quantum-classical Hybrid Machine Learning models outperform classical models when data is optimally normalized using the proposed method.

**Conclusion:** Introduces a new approach that enhances the capabilities of quantum machine learning in biomedical data analysis.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+normalization+in+quantum-classical+hybrid+models+for+anti-cancer+drug+response+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10037&send_immediately=true&force_search=false)

**Abstract:** Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [41] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang*

**Main category:** cs.LG

**TL;DR:** This paper introduces a framework that combines noising-based and denoising-based interventions to identify and distinguish logic gates in neural networks, ensuring faithfulness and completeness in circuit discovery.


<details>
  <summary>Details</summary>
**Motivation:** To address the incompleteness issue in circuit discovery caused by partially detected OR gates and ensure the fixed nature of circuits across different runs.

**Method:** Systematically introducing three types of logic gates (AND, OR, ADDER) and decomposing circuits into combinations of these gates; proposing a framework that integrates noising-based and denoising-based interventions.

**Result:** The framework can fully identify and distinguish logic gates within circuits, restoring their faithfulness, completeness, and sparsity. It also reveals fundamental properties of the logic gates and explores their functionality in language models.

**Conclusion:** The proposed framework provides a systematic approach to achieving faithful and complete circuit discovery, enhancing mechanistic interpretability.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Circuit+Completeness+in+Language+Models%3A+AND%2C+OR%2C+and+ADDER+Gates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10039，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10039&send_immediately=true&force_search=false)

**Abstract:** Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [42] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song, Jiaxing Li, Shihan Guan, Youyong Kong*

**Main category:** cs.LG

**TL;DR:** This paper introduces Instance-Prototype Affinity Learning (IPAL), a novel approach for Non-Exemplar Continual Graph Learning (NECGL) that uses Topology-Integrated Gaussian Prototypes (TIGP) and Instance-Prototype Affinity Distillation (IPAD) to improve knowledge assimilation and retention in graph neural networks.


<details>
  <summary>Details</summary>
**Motivation:** To address catastrophic forgetting in graph neural networks and provide a solution that avoids memory explosion and privacy concerns while enhancing knowledge retention.

**Method:** Proposes IPAL which includes TIGP to guide feature distributions and DBP to improve inter-class discriminability, using Prototype Contrastive Learning (PCL) to reduce feature drift.

**Result:** Demonstrates superior performance compared to existing methods on four node classification benchmarks, achieving a better balance between plasticity and stability.

**Conclusion:** IPAL is an effective method for continual learning in graph neural networks, addressing key challenges such as catastrophic forgetting, memory explosion, and privacy concerns.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Instance-Prototype+Affinity+Learning+for+Non-Exemplar+Continual+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10040&send_immediately=true&force_search=false)

**Abstract:** Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [43] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki, Mehedi Masud*

**Main category:** cs.LG

**TL;DR:** This study introduces a fraud detection framework that uses a stacking ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) combined with XAI techniques (SHAP, LIME, PDP, PFI) for enhanced transparency and interpretability. It achieved 99% accuracy and 0.99 AUC-ROC on the IEEE-CIS dataset.


<details>
  <summary>Details</summary>
**Motivation:** To address the lack of transparency in traditional machine learning models, making it hard for organizations to meet regulatory requirements and gain stakeholder trust.

**Method:** Stacking ensemble of XGBoost, LightGBM, CatBoost with XAI techniques including SHAP for feature selection, and LIME, PDP, PFI for explaining predictions.

**Result:** Achieved 99% accuracy and 0.99 AUC-ROC on the IEEE-CIS Fraud Detection dataset, outperforming several recent related approaches.

**Conclusion:** Combining high prediction accuracy with transparent interpretability is feasible and could promote ethical and trustworthy solutions in financial fraud detection.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Financial+Fraud+Detection+Using+Explainable+AI+and+Stacking+Ensemble+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10050&send_immediately=true&force_search=false)

**Abstract:** Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [44] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

**Main category:** cs.LG

**TL;DR:** A new method that uses synthetic tasks to enhance neural network performance in predicting multiple molecular properties simultaneously.


<details>
  <summary>Details</summary>
**Motivation:** To inject rule-based models into differentiable neural networks and improve molecular property prediction performance.

**Method:** Jointly training a single Graph Transformer neural network on both experimental and synthetic tasks.

**Result:** Consistent and significant performance improvement across all 19 molecular property prediction tasks.

**Conclusion:** Our proposed approach shows significant performance improvements in multitask molecular property prediction.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是All+You+Need+Is+Synthetic+Task+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10120，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10120&send_immediately=true&force_search=false)

**Abstract:** Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [45] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng, Ying Zhang, Yuxuan Liang, Roger Zimmermann, Zhiwen Yu, Bin Guo*

**Main category:** cs.LG

**TL;DR:** This work explores how the multi-task distillation could be used to improve the unified modeling of depth estimation and scene segmentation.


<details>
  <summary>Details</summary>
**Motivation:** reduce the requirement for both the storage and training efforts

**Method:** self-adaptive distillation method, knowledge trajectory, trajectory-based distillation loss

**Result:** clearly improvement

**Conclusion:** Compared to the state-of-the-art solutions, our method achieves a clearly improvement.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JointDistill%3A+Adaptive+Multi-Task+Distillation+for+Joint+Depth+Estimation+and+Scene+Segmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10057，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10057&send_immediately=true&force_search=false)

**Abstract:** Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [46] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le, Latif U. Khan, Choong Seon Hong*

**Main category:** cs.LG

**TL;DR:** A new method called FedAPC is proposed to improve Federated Learning performance under domain heterogeneity.


<details>
  <summary>Details</summary>
**Motivation:** Federated Learning faces challenges due to statistical heterogeneity, especially domain heterogeneity.

**Method:** FedAPC uses prototype augmentation to enhance feature diversity and model robustness.

**Result:** Experimental results on Office-10 and Digits datasets show FedAPC outperforms SOTA baselines.

**Conclusion:** FedAPC improves the generalization ability of the FL global model under domain heterogeneity.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Federated+Learning+on+Edge+Devices+with+Domain+Heterogeneity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10128&send_immediately=true&force_search=false)

**Abstract:** Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [47] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang, Qi Qi, Zhongwen Rao, Lujia Pan, Jingyu Wang, Jianxin Liao*

**Main category:** cs.LG

**TL;DR:** ChronoSteer combines textual and temporal data using a novel framework, achieving superior performance in time series forecasting.


<details>
  <summary>Details</summary>
**Motivation:** Existing forecasting methods are limited by their reliance on unimodal time series data, failing to utilize rich textual information. The integration of LLMs and TSFMs aims to concurrently leverage both temporal and textual information for future inference.

**Method:** A decoupled framework is proposed where an LLM transforms textual events into revision instructions to guide the output of TSFM. ChronoSteer, a multimodal TSFM, is developed to handle these instructions. A two-stage training strategy using synthetic data is also introduced to alleviate data shortages.

**Result:** ChronoSteer improves prediction accuracy by 25.7% over the unimodal backbone and 22.5% over the previous best multimodal method.

**Conclusion:** ChronoSteer, when integrated with an LLM, shows significant improvement in prediction accuracy compared to unimodal methods and previous multimodal approaches.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ChronoSteer%3A+Bridging+Large+Language+Model+and+Time+Series+Foundation+Model+via+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10083&send_immediately=true&force_search=false)

**Abstract:** Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [48] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique*

**Main category:** cs.LG

**TL;DR:** This paper introduces QuXAI, a framework using Q-MEDLEY to explain feature importance in hybrid quantum-classical machine learning models, improving their interpretability and reliability.


<details>
  <summary>Details</summary>
**Motivation:** To address the lack of robust global and local explainability approaches for hybrid quantum-classical machine learning models employing quantized feature encoding and classical learning.

**Method:** Creating HQML models with quantum feature maps, using Q-MEDLEY which combines feature-based inferences, preserves the quantum transformation stage, and visualizes attributions.

**Result:** Q-MEDLEY effectively delineates influential classical aspects in HQML models, separates noise, and performs well against established XAI techniques in classical validation settings.

**Conclusion:** QuXAI provides a pathway to enhance the interpretability and reliability of HQML models, fostering greater confidence and safe/responsible use of quantum-enhanced AI technology.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QuXAI%3A+Explainers+for+Hybrid+Quantum+Machine+Learning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10167&send_immediately=true&force_search=false)

**Abstract:** The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [49] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu, Ziwei Wang, Junjie Sheng, Wenhao Li, Xiangfei Wang, Jun Luo*

**Main category:** cs.LG

**TL;DR:** This paper introduces MiCo, a hierarchical language agent framework that uses large language models to solve the Online Dynamic Multidimensional Bin Packing problem in cloud VM scheduling.


<details>
  <summary>Details</summary>
**Motivation:** Traditional methods struggle to handle the large-scale complexity and fluctuating demands of VM scheduling in cloud services. Existing learning-based methods also lack generalizability and interpretability.

**Method:** The paper proposes a two-stage architecture called MiCo, which includes Option Miner and Option Composer. The former discovers diverse non-context-aware strategies using LLMs, while the latter integrates these strategies with contextual ones.

**Result:** Extensive experiments on real-world datasets show that MiCo achieves a competitive ratio of 96.9% in large-scale scenarios with more than 10,000 virtual machines. It also performs well under nonstationary request flows and diverse configurations.

**Conclusion:** MiCo, a hierarchical language agent framework, provides a novel approach to VM scheduling by formulating it as a SMDP-Option problem and utilizing LLMs to generate both non-context-aware and context-aware strategies.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Virtual+Machine+Scheduling+in+Cloud+Computing+through+Language+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10117，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10117&send_immediately=true&force_search=false)

**Abstract:** In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [50] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li, Libing Chen, Yin Tang*

**Main category:** cs.LG

**TL;DR:** 提出Alinear模型，它使用极少量的参数就能在时间序列预测任务上达到与大规模模型相当的表现。通过引入适应性分解机制和频率衰减策略，Alinear能够动态调整不同预测长度上的组件重点，并在各种预测范围内实现稳定的预测效果，同时避免了注意力机制带来的计算开销。实验表明，Alinear在多个基准数据集上优于大型模型，其参数量不到后者的1%。此外，还提出了一种新的参数感知评估指标，强调了Alinear在受限模型预算下的优越性。这项工作挑战了“更大的模型总是更好”的普遍观念，提倡向更高效的时间序列建模范式转变。


<details>
  <summary>Details</summary>
**Motivation:** 质疑时间序列预测中是否需要不断增加模型规模来获得性能提升，探索更轻量级且高效的解决方案。

**Method:** 提出Alinear模型，包含两个关键机制：（1）时域感知的自适应分解机制，用于动态调整不同预测长度上的组件重点；（2）逐步频率衰减策略，确保在各种预测范围内稳定预测，无需依赖注意力机制。

**Result:** Alinear模型在七个基准数据集上展现出色性能，不仅超越了大规模模型，而且仅使用它们不到1%的参数量，同时保持了短时和超长时预测的高精度。另外，新提出的参数感知评估指标进一步凸显了Alinear的优势。

**Conclusion:** 本研究挑战了时间序列预测领域内关于更大模型必然更好的传统观点，倡导采用更高效的设计思路。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Scaling+Law+Apply+in+Time+Series+Forecasting%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10172&send_immediately=true&force_search=false)

**Abstract:** Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [51] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia*

**Main category:** cs.LG

**TL;DR:** This paper introduces a novel data reconstruction attack in Federated Learning (FL), which allows for perfect recovery of arbitrarily large data batches in classification tasks without prior knowledge of clients' data.


<details>
  <summary>Details</summary>
**Motivation:** Recent studies have shown vulnerabilities in FL where a malicious central server can manipulate model updates to reconstruct clients' private training data. However, existing attacks have limitations like assumptions about data distribution or inefficiency with larger batch sizes.

**Method:** The method uses a new geometric perspective on fully connected layers to create malicious model parameters for perfect data recovery.

**Result:** Experiments on image and tabular datasets show that the new attack method outperforms existing methods and can perfectly reconstruct data batches two orders of magnitude larger than previous state-of-the-art methods.

**Conclusion:** This research highlights significant vulnerabilities in FL and proposes an improved method for data reconstruction attacks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cutting+Through+Privacy%3A+A+Hyperplane-Based+Data+Reconstruction+Attack+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10264&send_immediately=true&force_search=false)

**Abstract:** Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [52] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou, Shu Ding, ZeLin Li, Wei Wang*

**Main category:** cs.LG

**TL;DR:** This paper introduces the adaptability of local models and proposes a method to enhance the performance of the global model by improving the adaptability of local models in federated learning.


<details>
  <summary>Details</summary>
**Motivation:** Due to the heterogeneous data distributions over clients and data privacy in federated learning, it is difficult to train local models to achieve a well-performed global model.

**Method:** We provide the property of an appropriate local model which has good adaptability on the data distributions over clients, then formalize the property into the local training objective with a constraint and propose a feasible solution to train the local model.

**Result:** Extensive experiments on federated learning benchmarks demonstrate that our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods.

**Conclusion:** Our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+the+Performance+of+Global+Model+by+Improving+the+Adaptability+of+Local+Models+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10125&send_immediately=true&force_search=false)

**Abstract:** Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [53] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma, Youcheng Sun, Mustafa A. Mustafa*

**Main category:** cs.LG

**TL;DR:** A novel defense mechanism called FeRA is proposed for detecting backdoor attacks in federated learning systems with non-IID data.


<details>
  <summary>Details</summary>
**Motivation:** The heterogeneity of edge devices leads to non-IID data which makes backdoor attacks detection more difficult.

**Method:** FeRA uses cross-client attention over internal feature representations to compute anomaly scores based on representation reconstruction errors.

**Result:** FeRA shows robust performance in various FL scenarios including challenging non-IID data distributions, reducing backdoor attack success rates while maintaining high accuracy on the main task.

**Conclusion:** FeRA is model-agnostic, attack-agnostic, and does not need labeled reference data, making it suitable for heterogeneous and resource-limited edge deployments.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Defending+the+Edge%3A+Representative-Attention+for+Mitigating+Backdoor+Attacks+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10297，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10297&send_immediately=true&force_search=false)

**Abstract:** Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [54] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

**Main category:** cs.LG

**TL;DR:** This dissertation explores how to make RL agents adapt efficiently to changing environments by focusing on exploration strategies and preserving prior knowledge.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to enable RL agents to efficiently adapt their behavior when encountering novel environmental changes during deployment without catastrophically forgetting useful prior knowledge.

**Method:** The method involves using structured representations that allow updating without disrupting reusable components and implementing prioritized exploration and sampling strategies.

**Result:** The results demonstrate that efficient online adaptation is possible with the right strategies and representations.

**Conclusion:** This dissertation shows that efficient online adaptation in RL agents for non-stationary environments requires two key capabilities: prioritized exploration and sampling strategies, and selective preservation of prior knowledge.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Adaptation+of+Reinforcement+Learning+Agents+to+Sudden+Environmental+Change，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10330，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10330&send_immediately=true&force_search=false)

**Abstract:** Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [55] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash, Nikhil Karamchandani, Avishek Ghosh*

**Main category:** cs.LG

**TL;DR:** This paper studies the best arm identification problem for multi-agent multi-armed bandits. It proposes two algorithms, Cl-BAI and BAI-Cl, which efficiently identify the best arm for each agent with low sample and communication complexity.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenge of identifying the best arm for each agent in a multi-agent multi-armed bandit setting with unknown agent-bandit mapping, aiming for both sample and communication efficiency.

**Method:** Proposes two algorithms: Cl-BAI (clusters agents first, then identifies best arms) and BAI-Cl (identifies best arms first, then clusters agents). Both use the successive elimination framework.

**Result:** Establishes δ-PC guarantees, derives bounds on sample complexity, and provides a lower bound for the problem class. Shows that a variant of BAI-Cl is minimax optimal when M is small.

**Conclusion:** The proposed algorithms perform well in terms of sample and communication efficiency, especially when the number of clusters is much smaller than the number of agents.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Near+Optimal+Best+Arm+Identification+for+Clustered+Bandits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10147，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10147&send_immediately=true&force_search=false)

**Abstract:** This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [56] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera, Luigi Loreti, Giovanni Todeschini, Alessio Fumagalli, Francesco Regazzoni*

**Main category:** cs.LG

**TL;DR:** 研究随机分类器集合中集体行为的出现，证明了存在一个最优温度使得分类最优化，且此温度与教师分类器和随机分类器数量无关。


<details>
  <summary>Details</summary>
**Motivation:** 研究由随机组件组成的系统中从微观无序到宏观组织的过渡，特别是研究随机分类器集合中集体行为的出现。

**Method:** 引入了一个理论模型来研究随机分类器集合中集体行为的出现。使用Gibbs测度通过分类损失定义能量，并证明了存在一个有限温度参数使得分类相对于损失（或能量）是最优的。

**Result:** 证明并确认了对于由高斯分布生成样本和使用教师感知机构建标签的情况，最优温度既不依赖于教师分类器也不依赖于随机分类器的数量。MNIST数据集上的实验强调了这种现象在高质量、无噪声的数据集中的相关性。

**Conclusion:** 研究了随机分类器集合中集体行为的出现。证明并确认了对于由高斯分布生成样本和使用教师感知机构建标签的情况，最优温度既不依赖于教师分类器也不依赖于随机分类器的数量，表明了观察到的行为具有普遍性。MNIST数据集上的实验强调了这种现象在高质量、无噪声的数据集中的相关性。物理类比揭示了所研究现象的自组织性质。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergence+of+Structure+in+Ensembles+of+Random+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10331&send_immediately=true&force_search=false)

**Abstract:** Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [57] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama, Valdir Grassi Jr*

**Main category:** cs.LG

**TL;DR:** Evaluate whether Specialized Multi-Task Optimizers (SMTOs) outperform equally weighted tasks in complex multi-task problems.


<details>
  <summary>Details</summary>
**Motivation:** Address critiques suggesting SMTOs' superior performance is due to poor hyperparameter optimization and lack of regularization rather than inherent advantages.

**Method:** Extensive empirical evaluation of SMTOs including latest methods on complex multi-task problems.

**Result:** SMTOs perform well compared to uniform loss. Fixed weights can achieve competitive performance compared to SMTOs.

**Conclusion:** Uniform loss can perform similarly to SMTOs in some instances.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uniform+Loss+vs.+Specialized+Optimization%3A+A+Comparative+Analysis+in+Multi-Task+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10347&send_immediately=true&force_search=false)

**Abstract:** Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [58] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen, Lasse Krogsbøll, Jonas Lyngsø, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maaløe*

**Main category:** cs.LG

**TL;DR:** AI-scribing solutions for healthcare need improvement. Introducing FactsR method for real-time clinical information extraction and recursive note generation.


<details>
  <summary>Details</summary>
**Motivation:** Current AI-scribing solutions rely on one-shot or few-shot prompts without reasoning, leading to potential errors in generated notes which could compromise patient safety.

**Method:** Introduce FactsR method for real-time clinical information extraction and use it recursively to generate final notes.

**Result:** FactsR method generates more accurate and concise notes by involving clinicians in note generation process and opens up new possibilities for real-time decision support.

**Conclusion:** The proposed method enhances the accuracy and reliability of AI-scribed notes in healthcare.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FactsR%3A+A+Safer+Method+for+Producing+High+Quality+Healthcare+Documentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10360&send_immediately=true&force_search=false)

**Abstract:** There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [59] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde, Priyadarshini P. Pai, Shashishekar P. Adiga, K. Subramanya Mayya, Yongbeom Seo, Myungsoo Hwang, Heeyoung Go, Changmin Park*

**Main category:** cs.LG

**TL;DR:** This paper investigates the application of deep learning in EUV pattering defect detection using synthetically generated SEM images. It demonstrates that YOLOv8 performs best in detecting smaller defects.


<details>
  <summary>Details</summary>
**Motivation:** The lack of defect-annotated quality data with good representation of smaller defects hinders the deployment of deep learning-based defect detection models in fabrication lines.

**Method:** Artificially generating SEM images with known defect distributions and autonomously annotating them for training deep learning models.

**Result:** YOLOv8 shows superior performance in defect detection with 96% mean average precision, outperforming EfficientNet and SSD. It also detects smaller defects more reliably and achieves 84.6% accuracy for Bridge defects and 78.3% for Break defects on real SEM data.

**Conclusion:** Synthetic data can serve as an alternative to real-world data for developing robust machine-learning models in defect detection.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Defect+Detection+in+Photolithographic+Patterns+Using+Deep+Learning+Models+Trained+on+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10192&send_immediately=true&force_search=false)

**Abstract:** In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [60] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra, Lizhen Lin*

**Main category:** cs.LG

**TL;DR:** This paper proposes Schrier-Coset Graph Propagation (SCGP), a novel approach to improve graph neural networks' long-range message passing without increasing memory usage, achieving better performance on several benchmarks.


<details>
  <summary>Details</summary>
**Motivation:** To solve the over-squashing issue in GNNs where information from distant nodes is compressed into fixed-size vectors, while avoiding scalability issues introduced by other methods like Cayley and expander graphs.

**Method:** Introduces Schrier-Coset Graph Propagation (SCGP), which uses Schreier-coset embeddings to enrich node features without changing the graph's original topology.

**Result:** SCGP demonstrates competitive or superior performance compared to baselines such as expander graph and rewired GNNs, particularly excelling in handling hierarchical and modular graph structures with lower latency and memory usage.

**Conclusion:** Schrier-Coset Graph Propagation (SCGP) offers a new way to enhance GNNs by using group-theoretic methods, showing promising results in various benchmarks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Schreier-Coset+Graph+Propagation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10392，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10392&send_immediately=true&force_search=false)

**Abstract:** Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [61] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero, José Omar Chelotti, Luciano Sebastián Martinez-Rau, Leandro Vignolo, Martín Pires, Julio Ricardo Galli, Leonardo Luis Giovanini, Hugo Leonardo Rufiner*

**Main category:** cs.LG

**TL;DR:** A deep neural network that fuses acoustic and inertial signals is proposed for monitoring feeding behavior in grazing cattle, achieving better performance than previous methods.


<details>
  <summary>Details</summary>
**Motivation:** Monitoring feeding behavior is crucial for efficient herd management and resource utilization in grazing cattle. Automatic recognition of feeding activities can improve diet formulation, detect metabolic problems and animal discomfort early.

**Method:** Deep neural network based on the fusion of acoustic and inertial signals, composed of convolutional, recurrent, and dense layers.

**Result:** The model achieved an F1-score value of 0.802, surpassing previous methods by 14%. Feature-level fusion has outperformed data and decision-level fusion.

**Conclusion:** The proposed model, which combines acoustic and inertial signals using deep neural networks, has shown superior performance in monitoring feeding behavior of grazing cattle.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+multi-head+deep+fusion+model+for+recognition+of+cattle+foraging+events+using+sound+and+movement+signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10198，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10198&send_immediately=true&force_search=false)

**Abstract:** Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [62] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo, Alireza Moradi*

**Main category:** cs.LG

**TL;DR:** This paper proposes a new method to enhance LLMs' performance in time series forecasting by transferring structured temporal information, showing significant improvement over a naive baseline.


<details>
  <summary>Details</summary>
**Motivation:** There is a growing need to establish best practices for leveraging the capabilities of LLMs beyond traditional natural language tasks.

**Method:** A novel cross-domain knowledge transfer framework is proposed to enhance the performance of LLMs in time series forecasting.

**Result:** Knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization.

**Conclusion:** Results show that knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Informed+Forecasting%3A+Leveraging+Auxiliary+Knowledge+to+Boost+LLM+Performance+on+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10213，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10213&send_immediately=true&force_search=false)

**Abstract:** With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [63] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella, Vicente Javier Castro Solar, Manuel Roveri*

**Main category:** cs.LG

**TL;DR:** SEAL is a NAS-based framework for data-incremental learning that dynamically adapts model structure and maintains stability through selective expansion and cross-distillation training.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenge of balancing plasticity and stability in incremental learning, especially in resource-constrained environments.

**Method:** A NAS-based framework that jointly searches for architecture and optimal expansion policy, with cross-distillation training after each expansion step.

**Result:** SEAL shows effectiveness in reducing forgetting and enhancing accuracy compared to prior methods.

**Conclusion:** SEAL reduces forgetting and enhances accuracy while maintaining a smaller model size.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SEAL%3A+Searching+Expandable+Architectures+for+Incremental+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10457&send_immediately=true&force_search=false)

**Abstract:** Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [64] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng*

**Main category:** cs.LG

**TL;DR:** Introduce ComplexFormer, which improves Transformer's ability to integrate positional information with semantic data using Complex Multi-Head Attention (CMHA).


<details>
  <summary>Details</summary>
**Motivation:** Address limitations in Transformers' handling of positional information while maintaining MHA flexibility.

**Method:** Propose CMHA that models semantic and positional differences within the complex plane. Introduce per-head Euler transformation and adaptive differential rotation mechanism.

**Result:** Outperforms strong baselines in language modeling, text/code generation, and math reasoning tasks with lower perplexity and better long-context coherence.

**Conclusion:** ComplexFormer provides a more expressive and adaptable attention mechanism, demonstrating strong parameter efficiency.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ComplexFormer%3A+Disruptively+Advancing+Transformer+Inference+Ability+via+Head-Specific+Complex+Vector+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10222，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10222&send_immediately=true&force_search=false)

**Abstract:** Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [65] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu, Ziming Liu, Jeff Gore*

**Main category:** cs.LG

**TL;DR:** Large language models (LLMs) perform better as they get larger, but why? This paper suggests that representation superposition is key.


<details>
  <summary>Details</summary>
**Motivation:** To understand the origin of the neural scaling law, which states that loss decreases as a power law with model size.

**Method:** Constructed a toy model based on two empirical principles: representations are superposed and words/concepts occur with varying frequencies.

**Result:** Under weak superposition, loss scaling depends on feature frequency; under strong superposition, loss is inversely proportional to model dimension. Four families of open-sourced LLMs exhibit strong superposition and match the predictions of the toy model.

**Conclusion:** Representation superposition is an important mechanism underlying the observed neural scaling laws.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Superposition+Yields+Robust+Neural+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10465，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10465&send_immediately=true&force_search=false)

**Abstract:** The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [66] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge, Xu Shen, Zeyu Wang, Fan Dang, Xuan Ding, Danyang Li, Yahui Han, Tianxiang Hao, Zheng Yang*

**Main category:** cs.LG

**TL;DR:** SpecOffload is a high-throughput inference engine that improves GPU core utilization and inference throughput by embedding speculative decoding into offloading.


<details>
  <summary>Details</summary>
**Motivation:** Efficient LLM inference on resource-constrained devices presents significant challenges in compute and memory utilization.

**Method:** SpecOffload embeds speculative decoding into offloading and unlocks latent GPU resources for storing and executing a draft model used for speculative decoding.

**Result:** SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x.

**Conclusion:** SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpecOffload%3A+Unlocking+Latent+GPU+Capacity+for+LLM+Inference+on+Resource-Constrained+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10259，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10259&send_immediately=true&force_search=false)

**Abstract:** Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [67] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang, Jiaxuan Gao, Feng Gao, Yi Wu, Chao Yu*

**Main category:** cs.LG

**TL;DR:** This paper introduces NCDPO, a novel framework that reformulates diffusion policies as noise-conditioned deterministic policies, enabling more efficient training and better performance in continuous robot control and multi-agent game scenarios.


<details>
  <summary>Details</summary>
**Motivation:** To address the limitations of sub-optimal and limited coverage of demonstration data in diffusion policies, especially in decision-making scenarios like robotics, gaming, and autonomous driving.

**Method:** Reformulating diffusion policy as a noise-conditioned deterministic policy, allowing for tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps.

**Result:** NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks.

**Conclusion:** NCDPO is robust to the number of denoising timesteps in the diffusion policy and shows promise in improving the efficiency and effectiveness of diffusion policies in various applications.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-tuning+Diffusion+Policies+with+Backpropagation+Through+Diffusion+Timesteps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10482，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10482&send_immediately=true&force_search=false)

**Abstract:** Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [68] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Lajos Hanzo*

**Main category:** cs.LG

**TL;DR:** A study uses deep reinforcement learning to optimize electric bus charging schedules.


<details>
  <summary>Details</summary>
**Motivation:** Optimizing electric bus charging schedules to reduce costs while ensuring they are charged adequately.

**Method:** Proposes a hierarchical deep reinforcement learning approach to decouple the original MDP into high-level SMDP and low-level MDPs, then develops an HDDQN-HER algorithm.

**Result:** Numerical experiments using real-world data show the effectiveness of the proposed algorithm.

**Conclusion:** The hierarchical approach allows for effective and efficient optimization of electric bus charging schedules.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Electric+Bus+Charging+Schedules+Relying+on+Real+Data-Driven+Targets+Based+on+Hierarchical+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10262&send_immediately=true&force_search=false)

**Abstract:** The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [69] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim, Sol A Kim, Geonhyeong Kim, Enver Menadjiev, Chanwoo Lee, Seongwook Chung, Nari Kim, Jaesik Choi*

**Main category:** cs.LG

**TL;DR:** This paper presents PnPXAI, a universal plug-and-play framework that supports diverse data modalities and neural network models in an automatic way.


<details>
  <summary>Details</summary>
**Motivation:** Existing XAI frameworks have limitations such as inflexibility to various model architectures and data modalities, restricted supported XAI methods, and sub-optimal recommendations of explanations.

**Method:** PnPXAI automatically identifies model architectures, recommends applicable explanation methods, and optimizes hyperparameters for the best explanations.

**Result:** The authors validated the effectiveness of the PnPXAI framework through user surveys and demonstrated its adaptability across different fields like medicine and finance.

**Conclusion:** The paper introduces PnPXAI, a universal XAI framework that can support different data types and neural network models flexibly.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PnPXAI%3A+A+Universal+XAI+Framework+Providing+Automatic+Explanations+Across+Diverse+Modalities+and+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10515&send_immediately=true&force_search=false)

**Abstract:** Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [70] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia, Joachim Nyborg, Morten Birk, Jeppe Liborius Sjørup, Anders Lillevang Vesterholt, Ira Assent*

**Main category:** cs.LG

**TL;DR:** A deep learning model is developed for high-resolution precipitation forecasting over 8 hours in Europe, integrating multiple data sources effectively and providing accurate forecasts with uncertainty quantification.


<details>
  <summary>Details</summary>
**Motivation:** Existing radar-only deep learning models have limitations with short forecast lead times.

**Method:** The model combines radar, satellite, and physics-based numerical weather prediction data, capturing long-range interactions.

**Result:** The model outperforms operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models.

**Conclusion:** This model sets a new benchmark for high-resolution precipitation forecasting in Europe, balancing accuracy, interpretability, and computational efficiency.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RainPro-8%3A+An+Efficient+Deep+Learning+Model+to+Estimate+Rainfall+Probabilities+Over+8+Hours，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10271&send_immediately=true&force_search=false)

**Abstract:** We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [71] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer, Sascha Gaudlitz, Johannes Schmidt-Hieber*

**Main category:** cs.LG

**TL;DR:** This paper explores a Hebbian spike-timing-dependent plasticity rule and relates it to noisy gradient descent and noisy mirror descent, proving it can identify the most active presynaptic neuron.


<details>
  <summary>Details</summary>
**Motivation:** To explore learning rules that account for precise spike-timing in Hebbian learning.

**Method:** Relating the rule to noisy gradient descent and discovering its connection to noisy mirror descent.

**Result:** The rule eventually identifies the presynaptic neuron with the highest activity.

**Conclusion:** The study proves that the Hebbian spike-timing-dependent plasticity rule can identify the presynaptic neuron with the highest activity.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spike-timing-dependent+Hebbian+learning+as+noisy+gradient+descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10272&send_immediately=true&force_search=false)

**Abstract:** Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [72] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Dusit Niyato*

**Main category:** cs.LG

**TL;DR:** This paper presents a Hierarchical Deep Reinforcement Learning (HDRL) approach called Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E) to optimize electric bus charging schedules by reformulating the original Markov Decision Process (MDP) into two augmented MDPs.


<details>
  <summary>Details</summary>
**Motivation:** Optimizing electric bus charging schedules is challenging due to uncertainties in travel time, energy consumption, and fluctuating electricity prices, and current solutions need to handle multiple time scales and scale for large fleets.

**Method:** The paper proposes a HDRL approach that introduces DAC-MAPPO-E algorithm which enhances the DAC algorithm for large-scale electric bus fleets by redesigning the decentralized actor network with attention mechanism and integrating MAPPO algorithm for decentralized and coordinated charging power decisions.

**Result:** Extensive experiments using real-world data show that DAC-MAPPO-E outperforms existing methods in optimizing electric bus fleet charging schedules and demonstrates good scalability.

**Conclusion:** The proposed HDRL approach effectively addresses the challenges in optimizing electric bus charging schedules under various uncertainties and can be applied to large-scale electric bus fleets.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Electric+Bus+Charging+Scheduling+with+Uncertainties+Using+Hierarchical+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10296，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10296&send_immediately=true&force_search=false)

**Abstract:** The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [73] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao, Chengpei Wu, Lilin Zhang, Ning Yang*

**Main category:** cs.LG

**TL;DR:** This paper introduces NML-GCL, a novel graph contrastive learning approach that uses a learnable NMN to enhance negative metric learning, improving performance by distinguishing false negatives more effectively. It also proposes a joint training scheme with bi-level optimization to utilize self-supervision signals.


<details>
  <summary>Details</summary>
**Motivation:** Addressing the false negative issue in graph contrastive learning without relying on human prior knowledge.

**Method:** Introducing NML-GCL with a learnable NMN to create a negative metric space and a joint training scheme with bi-level optimization.

**Result:** The proposed method outperforms existing approaches on benchmark datasets, as verified by experiments and theoretical analysis.

**Conclusion:** NML-GCL improves graph contrastive learning by effectively distinguishing false negatives through a learnable negative metric network and a joint training scheme.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Negative+Metric+Learning+for+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10307&send_immediately=true&force_search=false)

**Abstract:** Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [74] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou, Shi Pu*

**Main category:** cs.LG

**TL;DR:** This paper proposes ADSGD, which improves convergence in decentralized optimization with practical constraints.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenges of heterogeneous computation speeds and unpredictable communication delays in decentralized optimization.

**Method:** Asynchronous Decentralized Stochastic Gradient Descent (ADSGD)

**Result:** ADSGD converges under computation-delay-independent step sizes and performs better than existing methods in wall-clock convergence time.

**Conclusion:** ADSGD is well-suited for real-world decentralized learning tasks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asynchronous+Decentralized+SGD+under+Non-Convexity%3A+A+Block-Coordinate+Descent+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10322，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10322&send_immediately=true&force_search=false)

**Abstract:** Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [75] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras, Blaz Bertalanic, George Floros, Kostas Kolomvatsos, Panagiotis Sarigiannidis, Carolina Fortuna*

**Main category:** cs.LG

**TL;DR:** 提出了一种名为ALERT的方法，该方法可以检测特征分布变化并触发模型重新训练，适用于无线指纹识别和链路异常检测。


<details>
  <summary>Details</summary>
**Motivation:** 在真实部署中，特征分布的变化可能会降低AI模型的性能并导致不良行为。为了应对未检测到的模型退化，我们提出了ALERT方法。

**Method:** ALERT包括表示学习、统计测试和效用评估三个组件。我们依赖MLP进行表示学习组件的设计，Kolmogorov-Smirnov和Population Stability Index测试用于设计统计测试，以及一种新函数用于效用评估。

**Result:** ALERT方法在两个无线网络使用案例上表现优异。

**Conclusion:** ALERT方法在两个无线网络使用案例中表现优于十种标准漂移检测方法。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Representation+Learning+Approach+to+Feature+Drift+Detection+in+Wireless+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10325，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10325&send_immediately=true&force_search=false)

**Abstract:** AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [76] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares, Liyuan Liu*

**Main category:** cs.LG

**TL;DR:** This tutorial introduces discrete variational autoencoders (VAEs) with a rigorous derivation from first principles, providing a practical training recipe and an example implementation.


<details>
  <summary>Details</summary>
**Motivation:** Discrete latent spaces are becoming popular due to their potential suitability for many data modalities like text.

**Method:** A rigorous derivation of discrete VAEs where the latent space consists of categorical distributed latent variables.

**Result:** Provides a concrete training recipe and an example implementation of discrete VAEs.

**Conclusion:** Discrete VAEs offer a principled approach to probabilistic unsupervised learning with neural networks, especially suitable for data modalities like text.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Introduction+to+Discrete+Variational+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10344&send_immediately=true&force_search=false)

**Abstract:** Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [77] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding, Choon Hwai Yap, Kangjun Ji, Simão Castro*

**Main category:** cs.LG

**TL;DR:** AneuG, a two-stage VAE-based IA mesh generator, improves the accuracy of intracranial aneurysm pouch shape encoding using GHD tokens and enables the generation of parent vessels conditioned on these tokens.


<details>
  <summary>Details</summary>
**Motivation:** To address the lack of large IA image datasets and improve the physiological realism of generated aneurysm shapes with controllable morphological measurements for disease progression prediction.

**Method:** Proposes AneuG with two stages: 1) Generating GHD tokens for encoding and reconstructing aneurysm pouch shapes constrained by morphing energy statistics truths; 2) Generating parent vessels based on GHD tokens by creating vascular centerlines and propagating cross-sections.

**Result:** AneuG can generate realistic IA shapes with specific morphological measurements, useful for understanding shape variations and flow simulations related to clinical measurements.

**Conclusion:** AneuG improves the physiological realism and controllability of IA shape generation, which is crucial for training networks to predict blood flow forces in real-time.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-Stage+Generative+Model+for+Intracranial+Aneurysm+Meshes+with+Morphological+Marker+Conditioning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10407，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10407&send_immediately=true&force_search=false)

**Abstract:** A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [78] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp, Christopher MacLellan, Erik Harpstead, Kenneth Koedinger*

**Main category:** cs.LG

**TL;DR:** 研究发现分解学习为多个机制可以提高数据效率，有助于缩小机器学习和人类学习之间的差距。


<details>
  <summary>Details</summary>
**Motivation:** 研究人类如何能够仅从几十个例子就快速学习，而现代神经网络需要成千上万的例子。

**Method:** 通过在线辅导环境中的归纳式人类学习模拟的消融分析，比较强化学习与更高效的数据符号规则归纳方法。

**Result:** 分解学习为多个不同的机制显著提高了数据效率，使数据驱动的机器学习更加接近人类学习。

**Conclusion:** 集成多种专门的学习机制可能是弥合数据驱动机器学习与人类学习之间差距的关键。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decomposed+Inductive+Procedure+Learning%3A+Learning+Academic+Tasks+with+Human-Like+Data+Efficiency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10422，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10422&send_immediately=true&force_search=false)

**Abstract:** Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [79] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer, Eran Malach*

**Main category:** cs.LG

**TL;DR:** This paper explores the connection between gradient-based optimization in parametric models and optimization of random features. It demonstrates that under certain conditions, the target function can be approximated using a polynomial-sized combination of random features. The findings highlight the limitations of distribution-free learning in neural networks trained by gradient descent.


<details>
  <summary>Details</summary>
**Motivation:** To understand the fundamental limitations of distribution-free learning in neural networks trained by gradient descent.

**Method:** Studying the relationship between gradient-based optimization and random features optimization, introducing a new theoretical framework called average probabilistic dimension complexity (adc).

**Result:** If a parametric model can be learned using mini-batch stochastic gradient descent without assumptions about the data distribution, then the target function can be approximated using a polynomial-sized combination of random features. This highlights the importance of making assumptions about data distributions in practical applications.

**Conclusion:** The study reveals the limitations of distribution-free learning in neural networks trained by gradient descent and introduces a new theoretical framework, average probabilistic dimension complexity (adc), which demonstrates an infinite separation between adc and standard dimension complexity.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Power+of+Random+Features+and+the+Limits+of+Distribution-Free+Gradient+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10423，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10423&send_immediately=true&force_search=false)

**Abstract:** We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [80] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang, Wenwen Qiang, Zeen Song, Changwen Zheng, Hui Xiong*

**Main category:** cs.LG

**TL;DR:** This paper presents Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework designed to enhance large language models' reasoning effectiveness while reducing computational costs by optimizing token usage.


<details>
  <summary>Details</summary>
**Motivation:** Existing methods lack balance between reasoning effectiveness and computational efficiency, often resulting in unnecessarily long reasoning chains and wasted tokens.

**Method:** L2T introduces a universal dense process reward based on quantified parameter information gain without additional annotations or task-specific evaluators. It also includes a fast estimation method using PAC-Bayes bounds and Fisher information matrix.

**Result:** The approach demonstrates improved reasoning effectiveness and efficiency across various benchmarks and base models.

**Conclusion:** Learning to Think (L2T) effectively optimizes large language models' reasoning processes with fewer tokens, enhancing both performance and computational efficiency.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Think%3A+Information-Theoretic+Reinforcement+Fine-Tuning+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10425，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10425&send_immediately=true&force_search=false)

**Abstract:** Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [81] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase, Katherine Haynes, Lander Ver Hoef, Imme Ebert-Uphoff*

**Main category:** cs.LG

**TL;DR:** A new method called score-based diffusion is used to nowcast (forecast for zero to three hours) clouds and precipitation using geostationary infrared imagery. Three types of diffusion models are experimented with, showing ability to advect, generate, and decay clouds. Results show the CorrDiff approach outperforms other models and traditional methods.


<details>
  <summary>Details</summary>
**Motivation:** Traditional methods for simulating clouds and precipitation are challenging due to sub-grid parameterizations. Early machine learning methods produced blurry forecasts.

**Method:** Exploring score-based diffusion models including Diff, CorrDiff, and LDM for nowcasting clouds and precipitation using geostationary infrared imagery.

**Result:** Diffusion models can advect, generate, and decay clouds, including convective initiation. High-resolution features are preserved longer than with a conventional U-Net. CorrDiff approach outperforms other models and traditional methods.

**Conclusion:** Score-based diffusion models show promise for nowcasting clouds and precipitation, with CorrDiff being the most effective model.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Score-based+diffusion+nowcasting+of+GOES+imagery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10432&send_immediately=true&force_search=false)

**Abstract:** Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [82] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

**Main category:** cs.LG

**TL;DR:** This paper discusses the limitations of conventional experimental methods for deriving gas turbine models and proposes data-driven approaches to overcome these limitations. It introduces the use of sparse identification and Koopman eigenfunctions to create a globally optimal nonlinear feedback controller which performs better than traditional controllers.


<details>
  <summary>Details</summary>
**Motivation:** To develop more accurate and efficient models and controllers for gas turbine engines by overcoming the limitations of conventional methods.

**Method:** Employing data-driven identification techniques like sparse identification of nonlinear dynamics and mapping the system's dynamics into an optimally constructed Koopman eigenfunction space.

**Result:** The proposed Koopman-based controller outperforms classical and gain-scheduled proportional-integral controllers as well as an internal model control approach in reference tracking and disturbance rejection under various conditions.

**Conclusion:** Data-driven approaches can effectively improve the modeling and control of complex nonlinear systems like gas turbines.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Identification+and+Optimal+Nonlinear+Control+of+Turbojet+Engine+Using+Koopman+Eigenfunction+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10438&send_immediately=true&force_search=false)

**Abstract:** Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [83] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu*

**Main category:** cs.LG

**TL;DR:** 提出了一种新的并行扩展方法(ParScale)，可以在不显著增加内存和延迟的情况下，有效地扩展语言模型。


<details>
  <summary>Details</summary>
**Motivation:** 传统的语言模型扩展方法通常需要显著的空间或时间成本，通过增加参数或输出令牌来实现扩展。然而，这些方法可能会导致较大的内存和延迟增加。因此，研究者们希望找到一种更高效的方法来扩展语言模型。

**Method:** 提出了一种新的并行扩展(ParScale)方法，该方法对输入应用P个不同的可学习变换，在并行执行模型的前向传递后动态聚合P个输出。

**Result:** 实验结果表明，使用ParScale方法的模型在达到相同性能提升的情况下，可以使用多达22倍少的内存增加和6倍少的延迟增加。此外，该方法还可以通过少量的后训练将现有的预训练模型转换为并行扩展模型，进一步减少了训练预算。

**Conclusion:** 提出了一种新的并行扩展(ParScale)方法，该方法通过在训练和推理期间增加模型的并行计算来实现更高效的推理。理论上提出了一个新的扩展定律，并通过大规模预训练验证了其有效性。这种方法可以显著减少内存和延迟的增加，同时也可以通过少量的后训练将现有的预训练模型转换为并行扩展模型。这项研究可能有助于在资源匮乏的情况下部署更强大的模型，并为机器学习中的计算作用提供了另一种视角。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parallel+Scaling+Law+for+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10475&send_immediately=true&force_search=false)

**Abstract:** It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [84] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero, Rupali Bhati, Shuo Liu, Aathira Pillai, Christopher Amato*

**Main category:** cs.LG

**TL;DR:** This paper introduces QFIX, a simpler and more effective method for value function decomposition in cooperative multi-agent reinforcement learning, outperforming previous methods including QPLEX.


<details>
  <summary>Details</summary>
**Motivation:** Existing value function decomposition methods have limited representation capabilities or are unnecessarily complex.

**Method:** A novel family of value function decomposition models named QFIX is derived from a simple formulation of the full class of IGM values.

**Result:** QFIX improves the performance of prior methods, learns more stably, and performs better than QPLEX in multiple SMACv2 and Overcooked environments.

**Conclusion:** QFIX successfully enhances the performance of prior methods, outperforms QPLEX, and achieves this with the simplest and smallest mixing models.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fixing+Incomplete+Value+Function+Decomposition+for+Multi-Agent+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10484，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10484&send_immediately=true&force_search=false)

**Abstract:** Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [85] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi, Tushar Vatsa, Dewang Sultania, Suhas Suresha, Ishita Verma, Cheng Chen, Tracy Holloway King, Michael Friedrich*

**Main category:** cs.LG

**TL;DR:** This paper presents a novel router-based architecture for generating high-quality synthetic training data to fine-tune large language models for function-calling tasks without real user interaction data.


<details>
  <summary>Details</summary>
**Motivation:** To address the lack of real-world task-specific data and privacy constraints for training on it, which are common in digital content creation tools.

**Method:** A router-based architecture that uses domain resources like content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models.

**Result:** The architecture's flexible routing mechanism generates synthetic data that matches real-world distributions, improving function classification accuracy and API parameter selection.

**Conclusion:** Models fine-tuned with the synthetic data generated by this architecture outperform traditional approaches, setting new benchmarks for function calling tasks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RouteNator%3A+A+Router-Based+Multi-Modal+Architecture+for+Generating+Synthetic+Training+Data+for+Function+Calling+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10495，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10495&send_immediately=true&force_search=false)

**Abstract:** This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [86] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa*

**Main category:** cs.LG

**TL;DR:** Speculative decoding is a technique used to accelerate language model inference by using a lightweight draft model to predict multiple tokens for a larger target model to verify. This study introduces MASSV, a method that adapts small language models into effective multimodal drafters for vision-language models by connecting the target VLM's vision encoder to the draft model and applying self-distilled visual instruction tuning. Experiments show that MASSV can increase accepted length by up to 30% and deliver end-to-end inference speedups of up to 1.46x on visually-grounded tasks.


<details>
  <summary>Details</summary>
**Motivation:** Speculative decoding can significantly accelerate language model inference but faces challenges when applied to vision-language models due to the lack of architectural components in small language models to process visual inputs and their token predictions failing to match those of VLM target models.

**Method:** MASSV uses a two-phase approach to adapt small language models into effective multimodal drafters. It first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions.

**Result:** MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks.

**Conclusion:** MASSV provides a scalable, architecture-compatible method for accelerating both current and future VLMs.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MASSV%3A+Multimodal+Adaptation+and+Self-Data+Distillation+for+Speculative+Decoding+of+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10526，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10526&send_immediately=true&force_search=false)

**Abstract:** Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [87] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar, Barnabas Poczos, Newell Washburn*

**Main category:** cs.LG

**TL;DR:** This paper introduces PharmaDiff, a pharmacophore-conditioned diffusion model for 3D molecular generation that outperforms ligand-based drug design methods and achieves high docking scores without requiring target protein structures.


<details>
  <summary>Details</summary>
**Motivation:** To develop a more efficient method for generating bioactive molecules, especially for novel targets lacking structural or functional data.

**Method:** PharmaDiff uses a transformer-based architecture to incorporate an atom-based representation of the 3D pharmacophore into the generative process, allowing for precise generation of 3D molecular graphs aligned with predefined pharmacophore hypotheses.

**Result:** PharmaDiff demonstrates better performance in matching 3D pharmacophore constraints and achieves higher docking scores across various proteins compared to ligand-based drug design methods.

**Conclusion:** The integration of pharmacophore modeling with 3D generative techniques provides a powerful and flexible framework for rational drug design.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pharmacophore-Conditioned+Diffusion+Model+for+Ligand-Based+De+Novo+Drug+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10545，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10545&send_immediately=true&force_search=false)

**Abstract:** Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [88] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani, Sadjad Naderi, Claire H. Dilliway, Claire E. Heaney, Shrreya Behll, Boyang Chen, Hisham Abubakar-Waziri, Alexandra E. Porter, Marc Chadeau-Hyam, Fangxin Fang, Ian M. Adcock, Kian Fan Chung, Christopher C. Pain*

**Main category:** cs.LG

**TL;DR:** The paper presents a new workflow using AI to predict personalized health responses to air pollution based on data from wearable devices and environmental exposures.


<details>
  <summary>Details</summary>
**Motivation:** To address the health risks posed by air pollution and extreme weather events exacerbated by climate change, utilizing advancements in personal sensing and AI for better healthcare.

**Method:** Integrating physiological data from wearable fitness devices with real-time environmental exposures using an Adversarial Autoencoder neural network within a cloud-based, modular framework.

**Result:** The AI model accurately reconstructs health signals and captures nonlinear responses to pollution, with transfer learning improving its generalization ability using user-generated data from a smartwatch.

**Conclusion:** This novel workflow has the potential to improve monitoring and prediction of individual health responses to pollution, capitalizing on personal sensing and AI advancements.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+AI-driven+framework+for+the+prediction+of+personalised+health+response+to+air+pollution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10556&send_immediately=true&force_search=false)

**Abstract:** Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

**Main category:** cs.AI

**TL;DR:** This paper addresses the lack of studies evaluating the generality of search algorithms in two-player zero-sum games with perfect information. It proposes a new search algorithm that performs better than existing ones in shorter search times and outperforms all others in 17 out of 22 games with medium search times.


<details>
  <summary>Details</summary>
**Motivation:** To evaluate the generality of search algorithms in games and fill the research gap.

**Method:** Proposing a new search algorithm for two-player zero-sum games with perfect information.

**Result:** The new algorithm outperforms all studied algorithms on all games in a short search time and on 17 of the 22 studied games in a medium search time.

**Conclusion:** The proposed algorithm demonstrates superior performance compared to existing algorithms in certain conditions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Study+and+improvement+of+search+algorithms+in+two-players+perfect+information+games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09639&send_immediately=true&force_search=false)

**Abstract:** Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [90] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle, Santiago Cifuentes*

**Main category:** cs.AI

**TL;DR:** This paper improves existing techniques and algorithms for identifying relevant and necessary features in complex models like neural networks. It also introduces a new concept of 'usefulness' and develops efficient algorithms to detect it in different types of models.


<details>
  <summary>Details</summary>
**Motivation:** To improve the existing techniques and algorithms for identifying crucial aspects of input features in classification models.

**Method:** Proposing a new global notion of 'usefulness' and developing efficient algorithms to detect it in various models.

**Result:** Efficient detection of necessity in complex models like neural networks and successful generalization of the notion of relevancy. Also, practical utility analysis of the new 'usefulness' concept on three datasets.

**Conclusion:** The paper successfully enhances the understanding of feature importance in classification models and introduces a new concept of 'usefulness' with efficient detection methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature+Relevancy%2C+Necessity+and+Usefulness%3A+Complexity+and+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09640&send_immediately=true&force_search=false)

**Abstract:** Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [91] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad, Reuth Mirsky*

**Main category:** cs.AI

**TL;DR:** This paper introduces the General Dynamic GR problem and proposes a model-free goal-conditioned RL approach to enable fast adaptation for GR across various changing tasks.


<details>
  <summary>Details</summary>
**Motivation:** To enable real-time GR systems and foster further research in dynamic environments where goals are numerous and constantly evolving.

**Method:** model-free goal-conditioned RL approach

**Result:** The proposed method can achieve fast adaptation for GR across various changing tasks.

**Conclusion:** This paper introduces the General Dynamic GR problem and proposes a model-free goal-conditioned RL approach to achieve fast adaptation for GR across various changing tasks.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是General+Dynamic+Goal+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09737&send_immediately=true&force_search=false)

**Abstract:** Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [92] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty, Rishi Ramaesh, Ajitha Rajan*

**Main category:** cs.AI

**TL;DR:** XpertXAI是一种新的人类中心模型，用于多病理检测并提供更好的解释性，其性能优于现有技术。


<details>
  <summary>Details</summary>
**Motivation:** 解释性不足限制了深度学习模型在临床中的广泛应用。本研究旨在通过引入ClinicXAI和进一步扩展为XpertXAI来解决这一问题，使模型决策更加透明且可理解。

**Method:** XpertXAI是一种基于InceptionV3的分类器，能够检测多种肺部病理，并与放射科医生的注释和医学真实数据进行比较验证。

**Result:** XpertXAI在预测准确性上优于现有的后处理解释方法和无监督CBM（XCBs），并且其概念级解释更符合专家推理。

**Conclusion:** XpertXAI展示了如何通过以人为本的设计扩展到更广泛的诊断领域，为医学诊断中的可解释人工智能提供了一条可扩展的路径。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainability+Through+Human-Centric+Design+for+XAI+in+Lung+Cancer+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09755&send_immediately=true&force_search=false)

**Abstract:** Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [93] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi, Ting Xiao, Mark V. Albert*

**Main category:** cs.AI

**TL;DR:** 提出一种多模态多智能体框架用于放射学报告生成，该框架与临床推理工作流程一致，包括检索、草稿生成、视觉分析、优化和合成等任务特定智能体。实验表明，该方法在自动指标和基于大型语言模型的评估中都优于强基准模型，生成更准确、结构化且可解释的报告。


<details>
  <summary>Details</summary>
**Motivation:** 提高临床工作流程效率并减轻放射科医生的工作负担，解决现有方法如多模态大语言模型和检索增强生成面临的问题如事实不一致、幻觉和跨模态错位。

**Method:** 开发了一种多模态多智能体框架，其中包含五个任务特定的智能体来执行不同的步骤：检索、草稿生成、视觉分析、优化和合成。

**Result:** 实验显示所提出的方法在自动度量标准和基于大型语言模型的评估中表现优异，生成的报告更加精确、结构化且易于理解。

**Conclusion:** 临床对齐的多智能体框架展示了支持可解释和可信临床人工智能应用的潜力。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multimodal+Multi-Agent+Framework+for+Radiology+Report+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09787，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09787&send_immediately=true&force_search=false)

**Abstract:** Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [94] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang, Yongli Zhu*

**Main category:** cs.AI

**TL;DR:** This paper studies offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration.


<details>
  <summary>Details</summary>
**Motivation:** To address the issue where environment interaction is unviable due to technical or safety reasons, the authors aim to train an applicable model through offline-style training on a previously collected dataset.

**Method:** Different offline reinforcement learning algorithms are used for microgrid voltage regulation.

**Result:** The experiment results on the IEEE 33-bus system show the feasibility and effectiveness of the proposed approach on different offline datasets, even with low-quality experience.

**Conclusion:** The proposed approach can effectively lower the negative impact of lacking online environment interactions.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Offline+Reinforcement+Learning+for+Microgrid+Voltage+Regulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09920&send_immediately=true&force_search=false)

**Abstract:** This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [95] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin, Donghyun Kim, Jeh-Kwang Ryu*

**Main category:** cs.AI

**TL;DR:** This study defines good questions and provides a systematic evaluation framework based on appropriateness and effectiveness.


<details>
  <summary>Details</summary>
**Motivation:** Limited research on comprehensive assessment of question quality.

**Method:** Developed a rubric-based scoring system with semi-adaptive criteria.

**Result:** Framework can evaluate both well-formed and problematic questions and adapt to different contexts.

**Conclusion:** Established a flexible and comprehensive framework for question evaluation.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%22There+Is+No+Such+Thing+as+a+Dumb+Question%2C%22+But+There+Are+Good+Ones，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09923&send_immediately=true&force_search=false)

**Abstract:** Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [96] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara, Rhea Pritham Marpu*

**Main category:** cs.AI

**TL;DR:** 本文档记录了人工智能从简单的基于规则的系统到复杂的自主代理的演变过程，探讨了这些系统的能力和技术基础，并强调了在这个新的智能时代中导航的必要性。


<details>
  <summary>Details</summary>
**Motivation:** 探索人工智能的发展历程和里程碑，以及其对社会的影响。

**Method:** 通过分析关键的技术进步，如提示方法、训练方法、硬件能力和架构创新等，来描述人工智能的发展。

**Result:** 论文指出当前的人工智能系统可能代表了我们目前所理解的‘最后一代’人工智能，并且强调了在这一强大的新智能时代中导航所带来的机会和挑战需要智慧和远见。

**Conclusion:** 需要智慧和远见来应对人工智能带来的机会和挑战。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying+AI+Agents%3A+The+Final+Generation+of+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09932&send_immediately=true&force_search=false)

**Abstract:** The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [97] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, Roberto Pieraccini*

**Main category:** cs.AI

**TL;DR:** Pre-Act是一种增强代理性能的新方法，通过创建多步执行计划和详细推理来处理用户输入。它适用于对话式和非对话式代理，并在任务导向型代理的评估中显示出优于ReAct的表现。此外，该方法通过微调较小模型如Llama 3.1 (8B & 70B)，显著提升了其在行动准确性和目标完成率上的表现。


<details>
  <summary>Details</summary>
**Motivation:** 提高大型语言模型在代理系统中的推理能力并解决小规模模型在复杂推理任务上的局限性。

**Method:** 提出Pre-Act方法，生成多步执行计划与详细推理，适用于对话式和非对话式代理，并设计了两层评估框架以衡量任务导向型代理的性能。

**Result:** Pre-Act在Almita数据集上比ReAct提高了70%的行动召回率，在微调后的70B模型上实现了69.5%的行动准确率提升和28%的目标完成率改进。

**Conclusion:** Pre-Act不仅增强了大型模型的推理能力，还通过微调较小模型扩大了其实用性，特别是在行动准确性和目标完成率方面有显著提升。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pre-Act%3A+Multi-Step+Planning+and+Reasoning+Improves+Acting+in+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09970&send_immediately=true&force_search=false)

**Abstract:** The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [98] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu, Zelin Fu, Xinhe Kuang, Jiacheng Dong, Qi Zhang, Kaifeng Su, Yikai Su, Wenbo Shi, Junfeng Yao, Yuliang Zhao, Shiqi Zhao, Jiadong Wang, Siyang Song, Chaoran Liu, Yuichiro Yoshikawa, Björn Schuller, Hiroshi Ishiguro*

**Main category:** cs.AI

**TL;DR:** A new challenge aims to improve depression detection across all ages by including individual differences and multimodal data.


<details>
  <summary>Details</summary>
**Motivation:** Current depression detection methods mainly focus on young adults, neglecting the broader age spectrum and individual differences.

**Method:** The challenge includes two tracks using age-specific datasets and provides a baseline model that combines audio, video, and individual difference information.

**Result:** Not yet determined

**Conclusion:** This challenge could lead to more personalized and accurate depression detection methods, benefiting mental health research.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+First+MPDD+Challenge%3A+Multimodal+Personality-aware+Depression+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10034&send_immediately=true&force_search=false)

**Abstract:** Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [99] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied, Mohamed Amine Chatti, Shoeb Joarder, Qurat Ul Ain, Rawaa Alatrash*

**Main category:** cs.AI

**TL;DR:** This paper proposes a Graph RAG pipeline using Educational Knowledge Graphs and Personal Knowledge Graphs to help learners understand new knowledge concepts in MOOCs.


<details>
  <summary>Details</summary>
**Motivation:** MOOCs lack direct interaction between learners and instructors, making it difficult for learners to understand new knowledge concepts. Although LLMs can be used, they are prone to hallucinations, limiting their reliability. RAG can solve this problem but is limited by unstructured learning materials and does not actively guide learners' learning needs.

**Method:** Propose a Graph RAG pipeline that uses Educational Knowledge Graphs and Personal Knowledge Graphs. Specifically, implement a PKG-based Question Generation method and an EduKG-based Question Answering method.

**Result:** The evaluation results show the potential of Graph RAG to empower learners to understand new knowledge concepts in a personalized learning experience.

**Conclusion:** Graph RAG can effectively guide learners to understand knowledge concepts in MOOCs through personalized learning.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Graph+Retrieval-Augmented+Generation+to+Support+Learners%27+Understanding+of+Knowledge+Concepts+in+MOOCs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10074&send_immediately=true&force_search=false)

**Abstract:** Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [100] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

**Main category:** cs.AI

**TL;DR:** This study uses AI techniques to convert unstructured academic texts into structured knowledge representations for Taiwanese China Studies, creating a knowledge graph and vector database that allow users to explore intellectual trajectories and research gaps.


<details>
  <summary>Details</summary>
**Motivation:** The motivation is to systematically revisit and reorganize decades of Taiwan based China Studies scholarship and provide a scalable, data driven alternative to traditional ontology construction.

**Method:** Generative AI techniques and large language models are used to extract and standardize entity relation triples from peer reviewed articles, which are then visualized through a lightweight D3.js based system.

**Result:** The result is a domain specific knowledge graph and vector database for the field, enabling exploration of conceptual nodes and semantic relationships across the corpus, revealing intellectual trajectories, thematic clusters, and research gaps.

**Conclusion:** This work shows how generative AI can enhance scholarly access to literature and support a reimagined scholarly infrastructure for regional knowledge systems.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Text+to+Network%3A+Constructing+a+Knowledge+Graph+of+Taiwan-Based+China+Studies+Using+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10093&send_immediately=true&force_search=false)

**Abstract:** Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [101] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker, Olivia Sanchez-Graillet, Moana Seidler, Christian Brandt, Jörg Wellmer, Philipp Cimiano*

**Main category:** cs.AI

**TL;DR:** This paper explores different methods of generating explanations in XAI to improve transparency and trust in ML systems used in medical diagnosis.


<details>
  <summary>Details</summary>
**Motivation:** To establish mutual trust between doctors and ML systems in shared decision-making scenarios.

**Method:** User study with physicians to investigate their perceptions of various types of AI-generated explanations.

**Result:** The study identifies the most effective and useful explanations that enhance the diagnostic process.

**Conclusion:** Understanding the types of explanations that are most effective contributes to improving the diagnostic process.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+User+Study+Evaluating+Argumentative+Explanations+in+Diagnostic+Decision+Support，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10188，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10188&send_immediately=true&force_search=false)

**Abstract:** As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [102] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo, Haiyang Shen, Jinsheng Huang, Zhengyang Mao, Junyu Luo, Zhuoru Chen, Xuhui Liu, Bingyu Xia, Luchen Liu, Yun Ma, Ming Zhang*

**Main category:** cs.AI

**TL;DR:** This paper introduces MASS, a multi-agent system for portfolio construction using large-scale simulations to achieve continuous excess returns. It outperforms six state-of-the-art baselines across three challenging A-share stock pools.


<details>
  <summary>Details</summary>
**Motivation:** Existing LLM-based multi-agent systems are limited to pure simulations or predefined workflows, which restricts their applicability and effectiveness.

**Method:** MASS progressively increases the number of agents for large-scale simulations and optimizes agent distribution end-to-end through a reverse optimization process.

**Result:** Performance experiments, ablation studies, backtesting experiments, experiments on updated data and stock pools, scaling experiments, parameter sensitivity experiments, and visualization experiments demonstrate MASS's superiority over six state-of-the-art baselines.

**Conclusion:** The paradigm established by MASS is expected to be expanded to other tasks with similar characteristics.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MASS%3A+Multi-Agent+Simulation+Scaling+for+Portfolio+Construction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10278，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10278&send_immediately=true&force_search=false)

**Abstract:** LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [103] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen, Duncan J. Watts, Mark E. Whiting*

**Main category:** cs.AI

**TL;DR:** This paper introduces a new method for evaluating AI's commonsense intelligence by accounting for human variability, finding that smaller models outperform larger ones, and emphasizing the importance of cultural context in AI evaluation.


<details>
  <summary>Details</summary>
**Motivation:** The motivation of this paper is to address the issue of assessing commonsense intelligence in machines, particularly large language models (LLMs), by considering the heterogeneity among humans.

**Method:** The method involves measuring the correspondence between a model's judgment and that of a human population, treating LLMs as independent survey respondents and as simulators of a hypothetical population.

**Result:** The results show that most LLMs are below the human median in individual commonsense competence. When used as population simulators, LLMs only modestly correlate with real humans in agreeing on the same set of statements. Surprisingly, smaller, open-weight models perform better than larger, proprietary frontier models.

**Conclusion:** The paper concludes by suggesting that the evaluation framework ties commonsense intelligence to its cultural basis, aligning with the need to adapt AI models to diverse human collectivities.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Empirically+evaluating+commonsense+intelligence+in+large+language+models+with+large-scale+human+judgments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10309&send_immediately=true&force_search=false)

**Abstract:** Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [104] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink, Stephie Do, Kristofer Bengtsson, Sabino Francesco Roselli, Martin Fabian*

**Main category:** cs.AI

**TL;DR:** This study compares SMT and MILP solvers for healthcare personnel scheduling, finding that each performs best under different conditions.


<details>
  <summary>Details</summary>
**Motivation:** To apply Satisfiability Modulo Theories (SMT) to healthcare scheduling, which has been understudied despite its challenges.

**Method:** Proposes generic constraint formulations and models them as SMT and MILP problems to compare Z3 and Gurobi solvers on academic and real-world rostering problems.

**Result:** MILP solver performs better with highly constrained or infeasible problems, while SMT solver excels with varied shifts and personnel in real-world inspired problems.

**Conclusion:** SMT-based methods offer a promising approach for future personnel scheduling research.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comparative+Study+of+SMT+and+MILP+for+the+Nurse+Rostering+Problem，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10328，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10328&send_immediately=true&force_search=false)

**Abstract:** The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [105] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel, Michael Bowling, André Barreto, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan, Khimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark Rowland, Tom Schaul, Satinder Singh*

**Main category:** cs.AI

**TL;DR:** This paper introduces the concept of plasticity as a measure of how agents are influenced by their past observations. It establishes a connection between plasticity and empowerment, revealing a tension between them.


<details>
  <summary>Details</summary>
**Motivation:** To understand how agents are influenced by their past observations and to establish a connection between plasticity and empowerment.

**Method:** Defining plasticity using a new information-theoretic quantity called the generalized directed information.

**Result:** Plasticity is found to be the mirror of empowerment, and there is a tension between plasticity and empowerment.

**Conclusion:** Plasticity, empowerment, and their relationship are essential to understanding agency.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Plasticity+as+the+Mirror+of+Empowerment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10361，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10361&send_immediately=true&force_search=false)

**Abstract:** Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [106] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal, Zihao Fu, Eoin Delaney, Chris Russell*

**Main category:** cs.AI

**TL;DR:** This paper proposes AXE, a new framework for evaluating and comparing model explanations without relying on ground-truth explanations or model sensitivity.


<details>
  <summary>Details</summary>
**Motivation:** Current explanation evaluation methods have limitations such as requiring ground-truth explanations or model sensitivity, which can be impractical or misleading.

**Method:** The authors propose three principles for explanation evaluation and develop the AXE framework based on these principles.

**Result:** AXE is verified by comparing with baselines and shown to be useful for detecting explanation fairwashing.

**Conclusion:** AXE provides an independent measure of explanation quality without requiring access to ideal ground-truth explanations or relying on model sensitivity.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Model+Explanations+without+Ground+Truth，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10399&send_immediately=true&force_search=false)

**Abstract:** There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [107] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee*

**Main category:** cs.AI

**TL;DR:** This study distinguishes between AI Agents and Agentic AI, offering a taxonomy, application mapping, and challenge analysis to clarify their design philosophies and capabilities.


<details>
  <summary>Details</summary>
**Motivation:** To provide a clear distinction and understanding between AI Agents and Agentic AI, their design philosophies, and capabilities.

**Method:** The study outlines a search strategy and foundational definitions, characterizes AI Agents as modular systems driven by LLMs and LIMs, and positions Generative AI as a precursor. It contrasts AI Agents with Agentic AI systems which represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. A comparative analysis across both paradigms is presented through an evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels.

**Result:** The study maps application domains like customer support, scheduling, and data summarization against Agentic AI deployments in research automation, robotic coordination, and medical decision support. It examines challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and proposes solutions such as ReAct loops, RAG, orchestration layers, and causal modeling.

**Conclusion:** This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Agents+vs.+Agentic+AI%3A+A+Conceptual+Taxonomy%2C+Applications+and+Challenge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10468，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10468&send_immediately=true&force_search=false)

**Abstract:** This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [108] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong, Thomas Bäck, Aske Plaat, Niki van Stein, Anna V. Kononova*

**Main category:** cs.AI

**TL;DR:** This study examines the effectiveness of different prompting techniques on large language models in dynamic environments. The results suggest that larger models perform better than smaller ones, but strategic prompting can help close the performance gap. Advanced prompting techniques benefit smaller models more on complex tasks, but advanced reasoning methods yield inconsistent results. Overall, current large language models still have limitations in reasoning and spatial coordination.


<details>
  <summary>Details</summary>
**Motivation:** To evaluate the adaptive capabilities of large language models as self-learning and reasoning agents in dynamic environments.

**Method:** Systematically testing self-reflection, heuristic mutation, and planning as prompting techniques using various open-source language models.

**Result:** Larger models outperform smaller ones, strategic prompting can close performance gaps, advanced prompting benefits smaller models on complex tasks, and advanced reasoning methods yield inconsistent results.

**Conclusion:** Current large language models still have significant limitations in crucial areas such as planning, reasoning, and spatial coordination, suggesting that self-reflective prompting alone cannot fully address these issues.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+a+Deeper+Understanding+of+Reasoning+Capabilities+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10543，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10543&send_immediately=true&force_search=false)

**Abstract:** While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [109] [On Measuring Intrinsic Causal Attributions in Deep Neural Networks](https://arxiv.org/abs/2505.09660)
*Saptarshi Saha, Dhruv Vansraj Rathore, Soumadeep Saha, Utpal Garain, David Doermann*

**Main category:** stat.ML

**TL;DR:** This work proposes a new method for quantifying intrinsic causal contributions (ICC) within neural networks by treating them as structural causal models (SCMs), demonstrating through experiments that ICC provides more intuitive and reliable explanations than existing global explanation techniques.


<details>
  <summary>Details</summary>
**Motivation:** To develop a more intuitive and reliable way to quantify causal influences of input features within neural networks.

**Method:** Proposing an identifiable generative post-hoc framework to quantify ICC and drawing a relationship between ICC and Sobol' indices.

**Result:** Experiments on both synthetic and real-world datasets showed that ICC generates more intuitive and reliable explanations compared to existing global explanation techniques.

**Conclusion:** The proposed method using ICC as part of SCMs offers improved understanding of neural network causal influences.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Measuring+Intrinsic+Causal+Attributions+in+Deep+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09660&send_immediately=true&force_search=false)

**Abstract:** Quantifying the causal influence of input features within neural networks has
become a topic of increasing interest. Existing approaches typically assess
direct, indirect, and total causal effects. This work treats NNs as structural
causal models (SCMs) and extends our focus to include intrinsic causal
contributions (ICC). We propose an identifiable generative post-hoc framework
for quantifying ICC. We also draw a relationship between ICC and Sobol'
indices. Our experiments on synthetic and real-world datasets demonstrate that
ICC generates more intuitive and reliable explanations compared to existing
global explanation techniques.

</details>


### [110] [Learning Multi-Attribute Differential Graphs with Non-Convex Penalties](https://arxiv.org/abs/2505.09748)
*Jitendra K Tugnait*

**Main category:** stat.ML

**TL;DR:** Estimating differences in two multi-attribute Gaussian graphical models using penalized D-trace loss function with non-convex penalties.


<details>
  <summary>Details</summary>
**Motivation:** To improve existing methods for multi-attribute differential graph estimation by considering non-convex penalties.

**Method:** Using a penalized D-trace loss function with non-convex penalties and presenting two proximal gradient descent methods to optimize the objective function.

**Result:** Theoretical analysis establishes sufficient conditions for consistency in support recovery, convexity and estimation in high-dimensional settings. Numerical examples based on synthetic and real data illustrate the approaches.

**Conclusion:** The proposed method provides a promising approach for estimating differences in two multi-attribute Gaussian graphical models.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Multi-Attribute+Differential+Graphs+with+Non-Convex+Penalties，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09748，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09748&send_immediately=true&force_search=false)

**Abstract:** We consider the problem of estimating differences in two multi-attribute
Gaussian graphical models (GGMs) which are known to have similar structure,
using a penalized D-trace loss function with non-convex penalties. The GGM
structure is encoded in its precision (inverse covariance) matrix. Existing
methods for multi-attribute differential graph estimation are based on a group
lasso penalized loss function. In this paper, we consider a penalized D-trace
loss function with non-convex (log-sum and smoothly clipped absolute deviation
(SCAD)) penalties. Two proximal gradient descent methods are presented to
optimize the objective function. Theoretical analysis establishing sufficient
conditions for consistency in support recovery, convexity and estimation in
high-dimensional settings is provided. We illustrate our approaches with
numerical examples based on synthetic and real data.

</details>


### [111] [LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data](https://arxiv.org/abs/2505.09803)
*Antony Sikorski, Michael Ivanitskiy, Nathan Lenssen, Douglas Nychka, Daniel McKenzie*

**Main category:** stat.ML

**TL;DR:** This paper introduces an innovative approach to parameter estimation for non-stationary spatially autoregressive models by treating inputs and outputs as images, enabling faster and more accurate estimation using image-to-image networks.


<details>
  <summary>Details</summary>
**Motivation:** To provide a cost-effective solution for acquiring large ensembles of spatially distributed quantities when direct data acquisition is expensive.

**Method:** Utilizing image-to-image networks to estimate parameters for non-stationary spatially autoregressive models by viewing both inputs and outputs as images.

**Result:** The proposed method enables faster and more accurate parameter estimation for complex non-stationary spatially autoregressive models.

**Conclusion:** Image-to-image networks offer a promising alternative for parameter estimation in spatially autoregressive models, particularly beneficial for large, non-stationary fields where traditional methods are computationally prohibitive.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LatticeVision%3A+Image+to+Image+Networks+for+Modeling+Non-Stationary+Spatial+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09803&send_immediately=true&force_search=false)

**Abstract:** In many scientific and industrial applications, we are given a handful of
instances (a 'small ensemble') of a spatially distributed quantity (a 'field')
but would like to acquire many more. For example, a large ensemble of global
temperature sensitivity fields from a climate model can help farmers, insurers,
and governments plan appropriately. When acquiring more data is prohibitively
expensive -- as is the case with climate models -- statistical emulation offers
an efficient alternative for simulating synthetic yet realistic fields.
However, parameter inference using maximum likelihood estimation (MLE) is
computationally prohibitive, especially for large, non-stationary fields. Thus,
many recent works train neural networks to estimate parameters given spatial
fields as input, sidestepping MLE completely. In this work we focus on a
popular class of parametric, spatially autoregressive (SAR) models. We make a
simple yet impactful observation; because the SAR parameters can be arranged on
a regular grid, both inputs (spatial fields) and outputs (model parameters) can
be viewed as images. Using this insight, we demonstrate that image-to-image
(I2I) networks enable faster and more accurate parameter estimation for a class
of non-stationary SAR models with unprecedented complexity.

</details>


### [112] [A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection](https://arxiv.org/abs/2505.10099)
*Sarat Moka, Matias Quiroz, Vali Asimit, Samuel Muller*

**Main category:** stat.ML

**TL;DR:** This paper proposes a new method for sparse portfolio selection that is faster and more scalable than traditional approaches.


<details>
  <summary>Details</summary>
**Motivation:** The computational costs of traditional methods increase exponentially with k and p, making them inefficient for moderate-sized problems.

**Method:** A gradient-based approach transforming the combinatorial sparse selection problem into a constrained continuous optimization task via Boolean relaxation.

**Result:** Our method matches commercial solvers in asset selection and shows negligible error in portfolio variance.

**Conclusion:** Our algorithm provides a fast and scalable way to solve the sparse portfolio selection problem.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Scalable+Gradient-Based+Optimization+Framework+for+Sparse+Minimum-Variance+Portfolio+Selection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10099，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10099&send_immediately=true&force_search=false)

**Abstract:** Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial sparse selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a sparse binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.

</details>


### [113] [Path Gradients after Flow Matching](https://arxiv.org/abs/2505.10139)
*Lorenz Vaitl, Leon Klein*

**Main category:** stat.ML

**TL;DR:** This paper investigates using path gradients to refine Continuous Normalizing Flows (CNFs) initially trained by Flow Matching for molecular systems sampling, showing a threefold increase in efficiency.


<details>
  <summary>Details</summary>
**Motivation:** To improve the sampling efficiency of molecular systems using CNFs.

**Method:** Hybrid approach combining Flow Matching and path gradients.

**Result:** Up to threefold increase in sampling efficiency for molecular systems with the same model and computational budget.

**Conclusion:** Path gradients can significantly enhance the performance of CNFs in molecular systems sampling without additional sampling needs.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Path+Gradients+after+Flow+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10139&send_immediately=true&force_search=false)

**Abstract:** Boltzmann Generators have emerged as a promising machine learning tool for
generating samples from equilibrium distributions of molecular systems using
Normalizing Flows and importance weighting. Recently, Flow Matching has helped
speed up Continuous Normalizing Flows (CNFs), scale them to more complex
molecular systems, and minimize the length of the flow integration
trajectories. We investigate the benefits of using path gradients to fine-tune
CNFs initially trained by Flow Matching, in the setting where a target energy
is known. Our experiments show that this hybrid approach yields up to a
threefold increase in sampling efficiency for molecular systems, all while
using the same model, a similar computational budget and without the need for
additional sampling. Furthermore, by measuring the length of the flow
trajectories during fine-tuning, we show that path gradients largely preserve
the learned structure of the flow.

</details>


### [114] [One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees](https://arxiv.org/abs/2505.10160)
*Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi*

**Main category:** stat.ML

**TL;DR:** This paper introduces a new one-stage Top-k Learning-to-Defer framework that outperforms previous Top-1 deferral methods.


<details>
  <summary>Details</summary>
**Motivation:** Existing one-stage L2D methods are limited to deferring to a single expert.

**Method:** We introduce the first one-stage Top-$k$ Learning-to-Defer framework.

**Result:** Our one-stage Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves superior accuracy-cost trade-offs.

**Conclusion:** Our approach provides a new way to unify prediction and deferral through a single end-to-end objective.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One-Stage+Top-%24k%24+Learning-to-Defer%3A+Score-Based+Surrogates+with+Theoretical+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10160&send_immediately=true&force_search=false)

**Abstract:** We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which
unifies prediction and deferral by learning a shared score-based model that
selects the $k$ most cost-effective entities-labels or experts-per input. While
existing one-stage L2D methods are limited to deferring to a single expert, our
approach jointly optimizes prediction and deferral across multiple entities
through a single end-to-end objective. We define a cost-sensitive loss and
derive a novel convex surrogate that is independent of the cardinality
parameter $k$, enabling generalization across Top-$k$ regimes without
retraining. Our formulation recovers the Top-1 deferral policy of prior
score-based methods as a special case, and we prove that our surrogate is both
Bayes-consistent and $\mathcal{H}$-consistent under mild assumptions. We
further introduce an adaptive variant, Top-$k(x)$, which dynamically selects
the number of consulted entities per input to balance predictive accuracy and
consultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage
Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves
superior accuracy-cost trade-offs by tailoring allocations to input complexity.

</details>


### [115] [Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods](https://arxiv.org/abs/2505.10448)
*Conor Rosato, Harvinder Lehal, Simon Maskell, Lee Devlin, Malcolm Strens*

**Main category:** stat.ML

**TL;DR:** We introduce new sampling algorithms using subset evaluations to reduce computational overhead for Bayesian inference with MCMC when the likelihood function is irregular and expensive to compute.


<details>
  <summary>Details</summary>
**Motivation:** To address the challenges posed by irregular and computationally expensive likelihood functions in Bayesian inference with MCMC.

**Method:** Adapting subset samplers without gradient information, introducing data-driven proxies instead of Taylor expansions, and defining a computation-cost aware adaptive controller.

**Result:** The improved version of Hierarchical Importance with Nested Training Samples (HINTS) achieves the best sampling error within a fixed computational budget.

**Conclusion:** Subset evaluations and a data-driven proxy combined with hierarchical delayed acceptance enable efficient and exact sampling.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+MCMC+Sampling+with+Expensive-to-Compute+and+Irregular+Likelihoods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10448，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10448&send_immediately=true&force_search=false)

**Abstract:** Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when
the likelihood function is irregular and expensive to compute. We explore
several sampling algorithms that make use of subset evaluations to reduce
computational overhead. We adapt the subset samplers for this setting where
gradient information is not available or is unreliable. To achieve this, we
introduce data-driven proxies in place of Taylor expansions and define a novel
computation-cost aware adaptive controller. We undertake an extensive
evaluation for a challenging disease modelling task and a configurable task
with similar irregularity in the likelihood surface. We find our improved
version of Hierarchical Importance with Nested Training Samples (HINTS), with
adaptive proposals and a data-driven proxy, obtains the best sampling error in
a fixed computational budget. We conclude that subset evaluations can provide
cheap and naturally-tempered exploration, while a data-driven proxy can
pre-screen proposals successfully in explored regions of the state space. These
two elements combine through hierarchical delayed acceptance to achieve
efficient, exact sampling.

</details>


### [116] [FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering](https://arxiv.org/abs/2505.10466)
*Juehang Qin, Shixiao Liang, Christopher Tunnell*

**Main category:** stat.ML

**TL;DR:** Introduce FlowVAT, a conditional tempering approach for normalizing flow variational inference, which addresses mode-seeking behavior and collapse issues in multi-modal and high-dimensional posteriors.


<details>
  <summary>Details</summary>
**Motivation:** Traditional annealing methods fall short of the goal of truly black-box variational inference due to their requirement for temperature schedules and hyperparameter tuning.

**Method:** Simultaneously temper both the base and target distributions, leveraging overparameterized neural networks' generalization capabilities to train a single flow representing the posterior across a range of temperatures.

**Result:** FlowVAT outperforms traditional and adaptive annealing methods, finding more modes and achieving better ELBO values, especially in higher dimensions.

**Conclusion:** FlowVAT advances toward fully-automatic black-box variational inference for complicated posteriors with minimal hyperparameter tuning and no annealing schedule.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlowVAT%3A+Normalizing+Flow+Variational+Inference+with+Affine-Invariant+Tempering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10466，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10466&send_immediately=true&force_search=false)

**Abstract:** Multi-modal and high-dimensional posteriors present significant challenges
for variational inference, causing mode-seeking behavior and collapse despite
the theoretical expressiveness of normalizing flows. Traditional annealing
methods require temperature schedules and hyperparameter tuning, falling short
of the goal of truly black-box variational inference. We introduce FlowVAT, a
conditional tempering approach for normalizing flow variational inference that
addresses these limitations. Our method tempers both the base and target
distributions simultaneously, maintaining affine-invariance under tempering. By
conditioning the normalizing flow on temperature, we leverage overparameterized
neural networks' generalization capabilities to train a single flow
representing the posterior across a range of temperatures. This preserves modes
identified at higher temperatures when sampling from the variational posterior
at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In
experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT
outperforms traditional and adaptive annealing methods, finding more modes and
achieving better ELBO values, particularly in higher dimensions where existing
approaches fail. Our method requires minimal hyperparameter tuning and does not
require an annealing schedule, advancing toward fully-automatic black-box
variational inference for complicated posteriors.

</details>


### [117] [Batched Nonparametric Bandits via k-Nearest Neighbor UCB](https://arxiv.org/abs/2505.10498)
*Sakshi Arya*

**Main category:** stat.ML

**TL;DR:** 研究了批量非参数上下文多臂老虎机中的顺序决策问题，提出了一种结合自适应k近邻回归和UCB原则的非参数算法BaNk-UCB。该方法完全是非参数的，适应上下文维度，并且易于实现。它使用局部几何来估计奖励并自适应地平衡探索和利用。提供了接近最优的遗憾保证，并在合成和真实数据集上的实证评估表明，BaNk-UCB始终优于基于分箱的基线。


<details>
  <summary>Details</summary>
**Motivation:** 在医学和营销等领域，由于在线反馈有限，需要研究批量非参数上下文多臂老虎机中的顺序决策问题。

**Method:** 提出了一个非参数算法BaNk-UCB，结合了自适应k近邻回归和UCB原则。该方法使用局部几何来估计奖励并自适应地平衡探索和利用。

**Result:** 提供了接近最优的遗憾保证，并在合成和真实数据集上的实证评估表明，BaNk-UCB始终优于基于分箱的基线。

**Conclusion:** 所提出的BaNk-UCB方法是一种有效的解决批量非参数上下文多臂老虎机中的顺序决策问题的方法，具有重要的应用价值。

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Batched+Nonparametric+Bandits+via+k-Nearest+Neighbor+UCB，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10498，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10498&send_immediately=true&force_search=false)

**Abstract:** We study sequential decision-making in batched nonparametric contextual
bandits, where actions are selected over a finite horizon divided into a small
number of batches. Motivated by constraints in domains such as medicine and
marketing -- where online feedback is limited -- we propose a nonparametric
algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the
upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully
nonparametric, adapts to the context dimension, and is simple to implement.
Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB
uses local geometry to estimate rewards and adaptively balances exploration and
exploitation. We provide near-optimal regret guarantees under standard
Lipschitz smoothness and margin assumptions, using a theoretically motivated
batch schedule that balances regret across batches and achieves minimax-optimal
rates. Empirical evaluations on synthetic and real-world datasets demonstrate
that BaNk-UCB consistently outperforms binning-based baselines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [118] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu, Qilong Wu, Lingjuan Lyu, Shibei Xue*

**Main category:** cs.CR

**TL;DR:** This paper proposes a sybil-based virtual data poisoning attack for federated learning, which reduces computational complexity using gradient matching and provides three target model acquisition schemes for different scenarios.


<details>
  <summary>Details</summary>
**Motivation:** To address the vulnerability of federated learning to poisoning attacks with lower costs.

**Method:** Generating sybil nodes to amplify poisoning impact and developing a virtual data generation method based on gradient matching.

**Result:** The proposed method outperforms other attack algorithms in simulations, especially under non-independent uniformly distributed data.

**Conclusion:** A novel sybil-based virtual data poisoning attack is developed for federated learning, achieving better performance than existing methods.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sybil-based+Virtual+Data+Poisoning+Attacks+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09983&send_immediately=true&force_search=false)

**Abstract:** Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [119] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

**Main category:** cs.CL

**TL;DR:** This case study applies a phased hyperparameter optimization process to compare multitask natural language model variants.


<details>
  <summary>Details</summary>
**Motivation:** To utilize multiphase learning rate scheduling and optimizer parameter grouping.

**Method:** Employing short Bayesian optimization sessions with multi-fidelity, hyperparameter space pruning, progressive halving, and human guidance. Using Optuna TPE sampler and Hyperband pruner, as well as Scikit-Learn Gaussian process minimization.

**Result:** Efficient low-fidelity sprints to prune the hyperparameter space and subsequent sprints progressively increasing their model fidelity employing hyperband pruning for efficiency.

**Conclusion:** Using a meta-learner to tune threshold values to resolve classification probabilities during inference.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interim+Report+on+Human-Guided+Adaptive+Hyperparameter+Optimization+with+Multi-Fidelity+Sprints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.09792，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.09792&send_immediately=true&force_search=false)

**Abstract:** This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [120] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha, Victoria Churchill, Anny D. Rodriguez, Ugur Kursuncu, Muhammed Y. Idris*

**Main category:** cs.CL

**TL;DR:** This study examines the effectiveness of various large language models (LLMs) in creating accurate and accessible information on breast and cervical cancer, finding that general-purpose LLMs have better linguistic quality and affectiveness, but medical LLMs offer greater communication accessibility despite potential harm and bias.


<details>
  <summary>Details</summary>
**Motivation:** To evaluate the capabilities and limitations of LLMs in providing clear and safe cancer-related information to improve public understanding and address health communication challenges.

**Method:** Mixed-methods evaluation of five general-purpose and three medical LLMs using quantitative metrics, expert ratings, and statistical analyses.

**Result:** General-purpose LLMs perform better in linguistic quality and affectiveness, whereas medical LLMs excel in communication accessibility but face issues related to safety and trustworthiness due to higher toxicity, harm, and bias.

**Conclusion:** There is a trade-off between domain-specific knowledge and safety in health communications using LLMs. Future models should focus on mitigating harm and bias while enhancing safety and affectiveness.

**Ai:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+for+Cancer+Communication%3A+Evaluating+Linguistic+Quality%2C+Safety%2C+and+Accessibility+in+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.10472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.10472&send_immediately=true&force_search=false)

**Abstract:** Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>
