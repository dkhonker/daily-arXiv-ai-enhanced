<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 171]
- [cs.AI](#cs.AI) [总数: 43]
- [stat.ML](#stat.ML) [总数: 20]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Generalizing Large Language Model Usability Across Resource-Constrained](https://arxiv.org/abs/2505.17040)
*Yun-Da Tsai*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种系统化的方法，以增强大型语言模型在真实世界限制条件下的通用性。具体包括一个强大的文本中心对齐框架，使LLMs能够无缝整合多种模态；一种对抗性提示技术，以提高对噪声和缺失模态的鲁棒性；推理时优化策略，无需额外训练即可提升性能；以及针对低资源领域的解决方案，例如Verilog代码生成。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法通常依赖于昂贵的监督微调或假设固定的训练条件，这限制了它们在面对未见过的模态、有限的数据或受限的计算资源时的泛化能力。因此，需要研究如何在实际约束条件下增强大型语言模型的可用性和泛化能力。

**方法:** 1. 提出一个强大的文本中心对齐框架，允许LLMs通过自然语言接口整合多样模态，并支持无需重新训练的情况下适应未见或动态变化的模态。
2. 提出一种对抗性提示技术，生成语义上具有挑战性的扰动，以测试模型可靠性。
3. 探索推理时间优化策略，利用提示搜索和不确定性量化来提高性能。
4. 针对低资源领域设计了正确构建的合成数据管道和逻辑增强推理模型。

**结果:** 该研究在多个方面取得了成功：实现了多模态无缝整合、提高了模型对噪声和缺失模态的鲁棒性、通过推理优化策略提升了性能，同时在低资源领域如Verilog代码生成中达到了最先进的性能水平。

**结论:** 这些贡献共同形成了一项统一的努力，增强了大型语言模型在实际约束条件下的适应性、可扩展性和效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalizing+Large+Language+Model+Usability+Across+Resource-Constrained，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17040&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language tasks, and recent efforts have sought to extend their
capabilities to multimodal domains and resource-constrained environments.
However, existing approaches often rely on costly supervised fine-tuning or
assume fixed training conditions, limiting their generalization when facing
unseen modalities, limited data, or restricted compute resources. This
dissertation presents a systematic study toward generalizing LLM usability
under real-world constraints. First, it introduces a robust text-centric
alignment framework that enables LLMs to seamlessly integrate diverse
modalities-including text, images, tables, and any modalities - via natural
language interfaces. This approach supports in-context adaptation to unseen or
dynamically changing modalities without requiring retraining. To enhance
robustness against noisy and missing modalities, an adversarial prompting
technique is proposed, generating semantically challenging perturbations at the
prompt level to stress-test model reliability. Beyond multimodal setting, the
dissertation investigates inference-time optimization strategies for LLMs,
leveraging prompt search and uncertainty quantification to improve performance
without additional model training. This perspective offers an efficient
alternative to scaling model parameters or retraining from scratch.
Additionally, the work addresses low-resource domains such as Verilog code
generation by designing correct-by-construction synthetic data pipelines and
logic-enhanced reasoning models, achieving state-of-the-art performance with
minimal data. Together, these contributions form a unified effort to enhance
the adaptability, scalability, and efficiency of large language models under
practical constraints.

</details>


### [2] [RAP: Runtime-Adaptive Pruning for LLM Inference](https://arxiv.org/abs/2505.17138)
*Huanrong Liu, Chunlin Tian, Xuyang Wei, Jiaheng Dai, Qin Liu, Tianqi Wei, Qingbiao Li, Li Li*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）在理解和生成语言方面表现出色，但其计算和内存需求巨大。为解决运行时内存变化和异构KV缓存需求问题，本文提出了一种基于强化学习的弹性剪枝框架RAP，可动态调整压缩策略，以适应实际执行中模型参数与KV缓存的比例变化。实验结果表明，RAP优于现有最先进方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs在语言理解和生成方面表现出色，但其巨大的计算和内存需求限制了部署。现有的压缩方法依赖于固定的启发式算法，无法适应运行时内存变化或来自多样化用户请求的异构KV缓存需求。

**方法:** 提出了一种基于强化学习的弹性剪枝框架RAP。该框架动态跟踪模型参数与KV缓存之间的比例变化，并根据即时工作负载和设备状态，在当前内存预算内保留最大化效用的组件。特别地，考虑到前馈网络（FFNs）包含大多数参数，而轻量级注意力层主导KV缓存形成，RL代理仅保留那些在特定内存预算下最有用的部分。

**结果:** 广泛的实验结果表明，RAP优于最先进的基线方法，首次实现了在运行时联合考虑模型权重和KV缓存。

**结论:** RAP作为一种基于强化学习的弹性剪枝框架，能够动态调整压缩策略以适应运行时条件的变化，显著提高了在不同工作负载和设备状态下的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RAP%3A+Runtime-Adaptive+Pruning+for+LLM+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17138，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17138&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) excel at language understanding and generation,
but their enormous computational and memory requirements hinder deployment.
Compression offers a potential solution to mitigate these constraints. However,
most existing methods rely on fixed heuristics and thus fail to adapt to
runtime memory variations or heterogeneous KV-cache demands arising from
diverse user requests. To address these limitations, we propose RAP, an elastic
pruning framework driven by reinforcement learning (RL) that dynamically
adjusts compression strategies in a runtime-aware manner. Specifically, RAP
dynamically tracks the evolving ratio between model parameters and KV-cache
across practical execution. Recognizing that FFNs house most parameters,
whereas parameter -light attention layers dominate KV-cache formation, the RL
agent retains only those components that maximize utility within the current
memory budget, conditioned on instantaneous workload and device state.
Extensive experiments results demonstrate that RAP outperforms state-of-the-art
baselines, marking the first time to jointly consider model weights and
KV-cache on the fly.

</details>


### [3] [MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning](https://arxiv.org/abs/2505.17142)
*Jingyu Li, Tiehua Zhang, Jinze Wang, Yi Zhang, Yuhuan Li, Yifan Zhao, Zhishu Shen, Jiannan Liu*

**主要类别:** cs.LG

**概要:** MetaSTH-Sleep是一个基于时空超图增强元学习的少样本睡眠阶段分类框架，能有效解决现有深度学习方法在睡眠阶段分类中的数据需求大、泛化能力差及信号高阶关系忽略等问题。实验表明，该框架在不同受试者中表现出显著性能提升，为临床医生的睡眠阶段标注提供了有力支持。


<details>
  <summary>更多</summary>
  
**动机:** 准确的睡眠阶段分类对于自动睡眠阶段标注至关重要，但传统方法依赖人工标注，耗时且劳动密集。尽管深度学习方法显示出潜力，但仍面临数据需求大、跨个体差异导致泛化能力差以及未能捕捉生物信号高阶关系等挑战。

**方法:** 提出了一种名为MetaSTH-Sleep的框架，结合了时空超图结构和元学习方法。该框架利用少量标记样本快速适应新受试者，并通过超图结构同时建模EEG信号中的复杂空间互连和时间动态。

**结果:** 实验结果表明，MetaSTH-Sleep在多样化的受试者中实现了显著的性能提升。

**结论:** MetaSTH-Sleep提供了一个有效的解决方案，可以显著改善睡眠阶段分类的性能，为临床医生进行睡眠阶段标注提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MetaSTH-Sleep%3A+Towards+Effective+Few-Shot+Sleep+Stage+Classification+with+Spatial-Temporal+Hypergraph+Enhanced+Meta-Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17142，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17142&send_immediately=true&force_search=false)

**原文摘要:** Accurate classification of sleep stages based on bio-signals is fundamental
for automatic sleep stage annotation. Traditionally, this task relies on
experienced clinicians to manually annotate data, a process that is both
time-consuming and labor-intensive. In recent years, deep learning methods have
shown promise in automating this task. However, three major challenges remain:
(1) deep learning models typically require large-scale labeled datasets, making
them less effective in real-world settings where annotated data is limited; (2)
significant inter-individual variability in bio-signals often results in
inconsistent model performance when applied to new subjects, limiting
generalization; and (3) existing approaches often overlook the high-order
relationships among bio-signals, failing to simultaneously capture signal
heterogeneity and spatial-temporal dependencies. To address these issues, we
propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on
spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid
adaptation to new subjects using only a few labeled samples, while the
hypergraph structure effectively models complex spatial interconnections and
temporal dynamics simultaneously in EEG signals. Experimental results
demonstrate that MetaSTH-Sleep achieves substantial performance improvements
across diverse subjects, offering valuable insights to support clinicians in
sleep stage annotation.

</details>


### [4] [Efficient Training of Neural SDEs Using Stochastic Optimal Control](https://arxiv.org/abs/2505.17150)
*Rembert Daems, Manfred Opper, Guillaume Crevecoeur, Tolga Birdal*

**主要类别:** cs.LG

**概要:** 提出了一种分层的、受控制理论启发的变分推理（VI）方法，用于神经随机微分方程（SDEs）。通过将控制项分解为线性和非线性部分，并利用随机最优控制推导出线性SDEs的最优控制项，从而更高效地训练神经SDEs。由于线性部分无需学习，初始化成本更低且收敛更快。


<details>
  <summary>更多</summary>
  
**动机:** 现有的神经SDEs的变分推理方法虽然有助于时间序列中的不确定性感知推理，但由于最大化ELBO的迭代性质，计算成本较高。需要一种更高效的训练方法以降低计算复杂度并保持模型表达能力。

**方法:** 1. 将控制项分为线性与非线性两部分。
2. 使用随机最优控制推导线性SDEs的最优控制项。
3. 用神经网络建模非线性部分，结合两者进行训练。

**结果:** 该方法能够以较低的初始化成本和更快的收敛速度训练神经SDEs，同时不牺牲模型的表达能力。

**结论:** 提出的分层变分推理方法有效降低了神经SDEs的训练成本，并加速了收敛过程，为时间序列不确定性推理提供了一个高效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Training+of+Neural+SDEs+Using+Stochastic+Optimal+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17150，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17150&send_immediately=true&force_search=false)

**原文摘要:** We present a hierarchical, control theory inspired method for variational
inference (VI) for neural stochastic differential equations (SDEs). While VI
for neural SDEs is a promising avenue for uncertainty-aware reasoning in
time-series, it is computationally challenging due to the iterative nature of
maximizing the ELBO. In this work, we propose to decompose the control term
into linear and residual non-linear components and derive an optimal control
term for linear SDEs, using stochastic optimal control. Modeling the non-linear
component by a neural network, we show how to efficiently train neural SDEs
without sacrificing their expressive power. Since the linear part of the
control term is optimal and does not need to be learned, the training is
initialized at a lower cost and we observe faster convergence.

</details>


### [5] [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
*Weizhe Lin, Xing Li, Zhiyuan Yang, Xiaojin Fu, Hui-Ling Zhen, Yaoyuan Wang, Xianzhi Yu, Wulong Liu, Xiaosong Li, Mingxuan Yuan*

**主要类别:** cs.LG

**概要:** TrimR是一个基于验证器的、无需训练的高效框架，用于动态压缩CoT以优化大型推理模型（LRMs）的推理效率。通过消除冗余中间思考步骤，TrimR在多个基准测试中显著提高了推理运行时间（高达70%），同时对准确性影响极小。


<details>
  <summary>更多</summary>
  
**动机:** 尽管扩展的Chain-of-Thought (CoT)推理和测试时扩展方法能提升LRMs在复杂任务中的准确性，但这些方法带来了显著的解码开销。主要原因是LRMs生成了大量冗余的思考路径，表现出结构化的过度思考或不足思考模式。为了解决这一问题并提高推理效率，研究者从人类认知推理过程和数值优化理论中获得灵感，提出了TrimR框架。

**方法:** TrimR采用了一个轻量级、预训练且指令调优的验证器，用于检测并截断LRMs的冗余中间思考步骤，而无需对LRMs或验证器进行微调。此外，研究者还设计了核心算法及异步在线系统，以支持高吞吐量的工业应用。

**结果:** 实验评估表明，在Ascend NPUs和vLLM平台上，TrimR框架在大批量工作负载下显著提升了推理效率。特别是在MATH500、AIME24、AIME25和GPQA四个基准测试中，Pangu-R-38B、QwQ-32B和DeepSeek-R1-Distill-Qwen-32B等模型的推理运行时间最多缩短了70%，且对准确率的影响可以忽略不计。

**结论:** TrimR框架成功地通过动态压缩CoT减少了推理过程中的冗余，从而大幅提升了LRMs在实际生产环境中的推理效率，为大规模部署提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TrimR%3A+Verifier-based+Training-Free+Thinking+Compression+for+Efficient+Test-Time+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17155，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17155&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling
complex mathematical, logical, and coding tasks by leveraging extended
Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging
CoT with explicit token-level exploration, can push LRMs' accuracy boundaries,
but they incur significant decoding overhead. A key inefficiency source is LRMs
often generate redundant thinking CoTs, which demonstrate clear structured
overthinking and underthinking patterns. Inspired by human cognitive reasoning
processes and numerical optimization theories, we propose TrimR, a
verifier-based, training-free, efficient framework for dynamic CoT compression
to trim reasoning and enhance test-time scaling, explicitly tailored for
production-level deployment. Our method employs a lightweight, pretrained,
instruction-tuned verifier to detect and truncate redundant intermediate
thoughts of LRMs without any LRM or verifier fine-tuning. We present both the
core algorithm and asynchronous online system engineered for high-throughput
industrial applications. Empirical evaluations on Ascend NPUs and vLLM show
that our framework delivers substantial gains in inference efficiency under
large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and
GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and
DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on
accuracy.

</details>


### [6] [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/abs/2505.17163)
*Mingxin Huang, Yongxin Shi, Dezhi Peng, Songxuan Lai, Zecheng Xie, Lianwen Jin*

**主要类别:** cs.LG

**概要:** 近期在多模态慢思维系统方面的进展，在各种视觉推理任务中表现出显著的性能。然而，由于缺乏系统的基准测试，它们在文本丰富的图像推理任务中的能力仍未得到充分研究。为了解决这一差距，我们提出了OCR-Reasoning，这是一个全面的基准，旨在系统地评估多模态大语言模型在文本丰富的图像推理任务中的表现。该基准包括1,069个人工标注的示例，涵盖6个核心推理能力和18个实际推理任务。此外，与仅标注最终答案的其他文本丰富的图像理解基准不同，OCR-Reasoning还同时标注了推理过程。通过标注的推理过程和最终答案，OCR-Reasoning不仅评估模型生成的最终答案，还评估其推理过程，从而实现对其问题解决能力的整体分析。利用这个基准，我们对最先进的MLLMs进行了全面的评估。我们的结果表明现有方法的局限性。值得注意的是，即使是最先进的MLLMs也表现出极大的困难，没有任何一个模型在OCR-Reasoning上的准确率超过50%，这表明文本丰富的图像推理挑战是一个亟待解决的问题。基准和评估脚本可在https://github.com/SCUT-DLVCLab/OCR-Reasoning获取。


<details>
  <summary>更多</summary>
  
**动机:** 目前的多模态慢思维系统在各种视觉推理任务上表现出色，但其在文本丰富的图像推理任务上的能力尚未得到充分研究，主要原因是缺乏系统性的基准测试。因此，需要开发一个新的基准来评估多模态大语言模型在这类任务上的表现。

**方法:** 提出名为OCR-Reasoning的基准测试，包含1,069个人工标注的例子，涵盖6个核心推理能力和18个实际推理任务。该基准不仅标注最终答案，还标注推理过程，以全面评估模型的推理能力。通过使用OCR-Reasoning基准，对最先进的多模态大语言模型进行全面评估。

**结果:** 评估结果显示，即使是当前最先进的多模态大语言模型，在OCR-Reasoning基准上的表现也不尽如人意，没有一个模型的准确率达到50%以上。

**结论:** 文本丰富的图像推理任务仍然是一个多模态大语言模型面临的重大挑战，现有的方法存在明显的局限性，需要进一步的研究和改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OCR-Reasoning+Benchmark%3A+Unveiling+the+True+Capabilities+of+MLLMs+in+Complex+Text-Rich+Image+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17163，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17163&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in multimodal slow-thinking systems have demonstrated
remarkable performance across diverse visual reasoning tasks. However, their
capabilities in text-rich image reasoning tasks remain understudied due to the
lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,
a comprehensive benchmark designed to systematically assess Multimodal Large
Language Models on text-rich image reasoning tasks. The benchmark comprises
1,069 human-annotated examples spanning 6 core reasoning abilities and 18
practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike
other text-rich image understanding benchmarks that only annotate the final
answers, OCR-Reasoning also annotates the reasoning process simultaneously.
With the annotated reasoning process and the final answers, OCR-Reasoning
evaluates not only the final answers generated by models but also their
reasoning processes, enabling a holistic analysis of their problem-solving
abilities. Leveraging this benchmark, we conducted a comprehensive evaluation
of state-of-the-art MLLMs. Our results demonstrate the limitations of existing
methodologies. Notably, even state-of-the-art MLLMs exhibit substantial
difficulties, with none achieving accuracy surpassing 50\% across
OCR-Reasoning, indicating that the challenges of text-rich image reasoning are
an urgent issue to be addressed. The benchmark and evaluation scripts are
available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.

</details>


### [7] [Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms](https://arxiv.org/abs/2505.17190)
*Baran Hashemi, Kurt Pasque, Chris Teska, Ruriko Yoshida*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为Tropical attention的新颖注意力机制，该机制在max-plus半环中运行，可以更好地逼近动态规划（DP）类型的组合优化算法。相较于传统的softmax归一化点积注意力，Tropical attention保留了凸多面体结构的清晰性，在处理分布外（OOD）数据时表现出更强的泛化能力。实验表明，使用Tropical transformers可以提升长度泛化和值泛化性能，并且在对抗攻击下保持稳定。此外，论文提出了将对抗攻击泛化作为神经算法推理基准测试的第三个维度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的神经算法推理模型依赖于softmax归一化的点积注意力，这种平滑的指数加权方法模糊了与动态规划相关的凸多面体结构，并在处理OOD数据时表现不佳。因此，需要一种新的注意力机制来恢复清晰、尺度不变的推理能力。

**方法:** 引入了Tropical attention，一种在max-plus半环中运行的新型注意力函数，并证明它可以逼近DP类型组合算法的热带电路。同时，提出了使用Tropical transformers以增强在算法推理任务中的经验OOD性能。

**结果:** 实验结果表明，Tropical transformers在长度泛化和值泛化方面超越了softmax基线，并在对抗攻击下保持稳定。此外，Tropical attention成功恢复了被softmax所缺失的清晰、尺度不变的推理能力。

**结论:** Tropical attention提供了一种更有效的注意力机制，适用于动态规划类组合优化问题，并显著提升了神经算法推理模型在OOD数据和对抗攻击下的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tropical+Attention%3A+Neural+Algorithmic+Reasoning+for+Combinatorial+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17190&send_immediately=true&force_search=false)

**原文摘要:** Dynamic programming (DP) algorithms for combinatorial optimization problems
work with taking maximization, minimization, and classical addition in their
recursion algorithms. The associated value functions correspond to convex
polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning
models, however, rely on softmax-normalized dot-product attention where the
smooth exponential weighting blurs these sharp polyhedral structures and
collapses when evaluated on out-of-distribution (OOD) settings. We introduce
Tropical attention, a novel attention function that operates natively in the
max-plus semiring of tropical geometry. We prove that Tropical attention can
approximate tropical circuits of DP-type combinatorial algorithms. We then
propose that using Tropical transformers enhances empirical OOD performance in
both length generalization and value generalization, on algorithmic reasoning
tasks, surpassing softmax baselines while remaining stable under adversarial
attacks. We also present adversarial-attack generalization as a third axis for
Neural Algorithmic Reasoning benchmarking. Our results demonstrate that
Tropical attention restores the sharp, scale-invariant reasoning absent from
softmax.

</details>


### [8] [Shape it Up! Restoring LLM Safety during Finetuning](https://arxiv.org/abs/2505.17196)
*ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau*

**主要类别:** cs.LG

**概要:** 微调大语言模型（LLMs）虽能实现用户特定的定制化，但会引入关键的安全风险。本文提出了一种名为动态安全塑造（DSS）的新框架，通过精细的安全信号来强化从响应的安全段学习，同时抑制不安全内容。借助STAR评分指导的STAR-DSS方法，可以在不影响任务能力的情况下，有效缓解微调带来的各种风险，并提升安全性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管微调大型语言模型可以实现用户特定的定制化，但即使是少数有害的例子也可能危及安全对齐。现有的缓解策略通常是对安全和不安全部分进行粗略处理，这种方法并不理想。

**方法:** 提出了动态安全塑造（DSS）框架，利用精细粒度的安全信号来强化学习来自响应的安全段，同时抑制不安全内容。引入了护轨模型用于评估部分响应，追踪整个响应中安全风险的变化，从而生成逐标记的安全轨迹评估（STAR），使塑造能够在训练序列上动态操作。

**结果:** STAR-DSS 方法在多种威胁、数据集和模型家族中稳健地缓解了微调风险，并显著提升了安全性，同时没有影响到预期任务的能力。

**结论:** 鼓励未来的研究建立在动态塑造原则之上，以应对不断演变的微调风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Shape+it+Up%21+Restoring+LLM+Safety+during+Finetuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17196&send_immediately=true&force_search=false)

**原文摘要:** Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks.

</details>


### [9] [LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration](https://arxiv.org/abs/2505.17198)
*Shuang Wu, Meijie Wang, Lun Yu*

**主要类别:** cs.LG

**概要:** 本文介绍了一种名为LengthLogD的预测框架，通过分子长度分层建立专门模型，并结合多尺度分子表示方法，优化了肽类化合物logD值的预测准确性，尤其在长肽方面表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 尽管肽类化合物因其高靶向亲和力和低毒性具有显著的治疗潜力，但其药物开发受到膜通透性低的限制。分子量和肽链长度对肽的logD值有显著影响，而准确预测肽的logD值仍具挑战性。

**方法:** 研究构建了一个名为LengthLogD的预测框架，通过分子长度分层策略建立专门模型，并整合原子、结构和拓扑三个层次的特征空间。使用分层集成学习优化模型，并为长肽开发了自适应权重分配机制。

**结果:** 实验结果显示，该方法在短肽（R^2=0.855）、中肽（R^2=0.816）和长肽（R^2=0.882）的预测中均表现出优越性能，长肽预测误差较传统单模型方法减少了34.7%。消融研究表明，长度分层策略贡献了41.2%的性能提升，拓扑特征占预测重要性的28.5%。

**结论:** 本研究提供了一个精确的肽类药物开发logD预测工具，特别在优化长肽先导化合物方面展示了独特价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LengthLogD%3A+A+Length-Stratified+Ensemble+Framework+for+Enhanced+Peptide+Lipophilicity+Prediction+via+Multi-Scale+Feature+Integration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17198，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17198&send_immediately=true&force_search=false)

**原文摘要:** Peptide compounds demonstrate considerable potential as therapeutic agents
due to their high target affinity and low toxicity, yet their drug development
is constrained by their low membrane permeability. Molecular weight and peptide
length have significant effects on the logD of peptides, which in turn
influences their ability to cross biological membranes. However, accurate
prediction of peptide logD remains challenging due to the complex interplay
between sequence, structure, and ionization states. This study introduces
LengthLogD, a predictive framework that establishes specialized models through
molecular length stratification while innovatively integrating multi-scale
molecular representations. We constructed feature spaces across three
hierarchical levels: atomic (10 molecular descriptors), structural (1024-bit
Morgan fingerprints), and topological (3 graph-based features including Wiener
index), optimized through stratified ensemble learning. An adaptive weight
allocation mechanism specifically developed for long peptides significantly
enhances model generalizability. Experimental results demonstrate superior
performance across all categories: short peptides (R^2=0.855), medium peptides
(R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in
prediction error for long peptides compared to conventional single-model
approaches. Ablation studies confirm: 1) The length-stratified strategy
contributes 41.2% to performance improvement; 2) Topological features account
for 28.5% of predictive importance. Compared to state-of-the-art models, our
method maintains short peptide prediction accuracy while achieving a 25.7%
increase in the coefficient of determination (R^2) for long peptides. This
research provides a precise logD prediction tool for peptide drug development,
particularly demonstrating unique value in optimizing long peptide lead
compounds.

</details>


### [10] [Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation](https://arxiv.org/abs/2505.17226)
*Kun Yang, Neena Imam*

**主要类别:** cs.LG

**概要:** 提出了一种新的聚合策略ArKrum，增强了联邦学习系统的鲁棒性和隐私保护能力。在多种拜占庭攻击下，ArKrum表现出高准确性和稳定性，优于或等同于其他鲁棒聚合方法。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习虽然可以在不共享原始数据的情况下进行协作机器学习，但容易受到恶意参与者的拜占庭攻击。现有的聚合方法要么不足够鲁棒，要么需要预先知道恶意客户端的数量，这在实际中难以满足。因此，需要一种不需要先验知识且更鲁棒的聚合策略。

**方法:** 基于先前工作的rKrum，ArKrum引入了两个关键创新：1) 使用中位数过滤机制去除极端异常值，从而估计对抗性客户端的数量；2) 应用多更新平均方案以提高稳定性和性能，特别是在客户端数据分布不一致的情况下。

**结果:** 在标准图像和文本数据集上评估ArKrum，针对三种广泛研究的拜占庭攻击类型，结果表明ArKrum始终实现高准确性和稳定性，表现优于或等同于其他鲁棒聚合方法。

**结论:** ArKrum是一种有效且实用的解决方案，适用于对抗环境中的安全联邦学习系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Secure+and+Private+Federated+Learning%3A+Achieving+Adversarial+Resilience+through+Robust+Aggregation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17226，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17226&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables collaborative machine learning across
decentralized data sources without sharing raw data. It offers a promising
approach to privacy-preserving AI. However, FL remains vulnerable to
adversarial threats from malicious participants, referred to as Byzantine
clients, who can send misleading updates to corrupt the global model.
Traditional aggregation methods, such as simple averaging, are not robust to
such attacks. More resilient approaches, like the Krum algorithm, require prior
knowledge of the number of malicious clients, which is often unavailable in
real-world scenarios. To address these limitations, we propose Average-rKrum
(ArKrum), a novel aggregation strategy designed to enhance both the resilience
and privacy guarantees of FL systems. Building on our previous work (rKrum),
ArKrum introduces two key innovations. First, it includes a median-based
filtering mechanism that removes extreme outliers before estimating the number
of adversarial clients. Second, it applies a multi-update averaging scheme to
improve stability and performance, particularly when client data distributions
are not identical. We evaluate ArKrum on benchmark image and text datasets
under three widely studied Byzantine attack types. Results show that ArKrum
consistently achieves high accuracy and stability. It performs as well as or
better than other robust aggregation methods. These findings demonstrate that
ArKrum is an effective and practical solution for secure FL systems in
adversarial environments.

</details>


### [11] [Automated Capability Evaluation of Foundation Models](https://arxiv.org/abs/2505.17228)
*Arash Afkanpour, Omkar Dige, Fatemeh Tavakoli*

**主要类别:** cs.LG

**概要:** 当前基础模型评估框架过度依赖固定的、手动整理的基准，限制了捕捉模型能力全貌的能力。本文引入主动学习能力评估（ACE）框架，用于可扩展、自动化的基础模型评估。ACE利用强大的语言模型知识分解领域并生成多样化的评估任务，减少人工努力。通过主动学习优化评估策略，发现静态基准可能遗漏的优势、劣势和失败模式。结果表明，ACE提供了更全面和详细的基础模型能力图景，有助于安全和明智地部署模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前基础模型评估框架依赖于固定的手动基准测试，这限制了对模型能力进行全面评估的能力。

**方法:** 提出了一种名为ACE的新框架，该框架利用强大的语言模型知识将领域分解为语义上有意义的能力，并生成多样化的评估任务。为了最大化覆盖率和效率，ACE将主题模型的性能建模为潜在语义空间上的能力函数，并使用主动学习来优先评估最具信息量的能力。

**结果:** 实验结果表明，与传统的静态基准测试相比，ACE能够更有效地发现模型的优势、劣势和失败模式。

**结论:** ACE提供了一个更完整和详细的基础模型能力视图，这对于安全和良好地部署基础模型至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Capability+Evaluation+of+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17228，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17228&send_immediately=true&force_search=false)

**原文摘要:** Current evaluation frameworks for foundation models rely heavily on fixed,
manually curated benchmarks, limiting their ability to capture the full breadth
of model capabilities. This paper introduces Active learning for Capability
Evaluation (ACE), a novel framework for scalable, automated, and fine-grained
evaluation of foundation models. ACE leverages the knowledge embedded in
powerful language models to decompose a domain into semantically meaningful
capabilities and generate diverse evaluation tasks, significantly reducing
human effort. To maximize coverage and efficiency, ACE models a subject model's
performance as a capability function over a latent semantic space and uses
active learning to prioritize the evaluation of the most informative
capabilities. This adaptive evaluation strategy enables cost-effective
discovery of strengths, weaknesses, and failure modes that static benchmarks
may miss. Our results suggest that ACE provides a more complete and informative
picture of model capabilities, which is essential for safe and well-informed
deployment of foundation models.

</details>


### [12] [Semantic-Aware Interpretable Multimodal Music Auto-Tagging](https://arxiv.org/abs/2505.17233)
*Andreas Patakis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou*

**主要类别:** cs.LG

**概要:** 本论文提出了一种可解释的音乐自动标记框架，利用多模态特征和期望最大化算法，在实现高性能的同时提高了决策过程的透明度。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基础模型在音乐自动标记领域表现出色，但其输出往往缺乏可解释性，限制了研究者和最终用户的信任和使用。

**方法:** 该框架利用从信号处理、深度学习、本体工程和自然语言处理中提取的具有音乐意义的多模态特征，并通过语义聚类和期望最大化算法为每组特征分配权重，以增强可解释性。

**结果:** 该方法在提供有竞争力的标记性能的同时，还提供了对决策过程的更深入了解。

**结论:** 此工作为更透明和以用户为中心的音乐标记系统铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semantic-Aware+Interpretable+Multimodal+Music+Auto-Tagging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17233，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17233&send_immediately=true&force_search=false)

**原文摘要:** Music auto-tagging is essential for organizing and discovering music in
extensive digital libraries. While foundation models achieve exceptional
performance in this domain, their outputs often lack interpretability, limiting
trust and usability for researchers and end-users alike. In this work, we
present an interpretable framework for music auto-tagging that leverages groups
of musically meaningful multimodal features, derived from signal processing,
deep learning, ontology engineering, and natural language processing. To
enhance interpretability, we cluster features semantically and employ an
expectation maximization algorithm, assigning distinct weights to each group
based on its contribution to the tagging process. Our method achieves
competitive tagging performance while offering a deeper understanding of the
decision-making process, paving the way for more transparent and user-centric
music tagging systems.

</details>


### [13] [Optimal Policy Minimum Bayesian Risk](https://arxiv.org/abs/2505.17242)
*Ramón Fernandez Astudillo, Md Arafat Sultan, Aashka Trivedi, Yousef El-Kurdi, Tahira Naseem, Radu Florian, Salim Roukos*

**主要类别:** cs.LG

**概要:** Inference scaling can enhance LLMs' complex reasoning ability through extended computation. This paper introduces a novel method to incorporate reward and risk/similarity signals into MBRD based on optimal policy in KL-controlled reinforcement learning, offering higher robustness, improved accuracy, well-understood asymptotic behavior, and a sample-efficient variant of MBRD. The approach is empirically demonstrated on math and coding tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to improve the accuracy and robustness of LLMs when solving complex reasoning problems by incorporating additional signals like reward models and risk/similarity functions into inference-time techniques such as MBRD.

**方法:** The method leverages the concept of optimal policy in KL-controlled reinforcement learning to create a framework that incorporates reward and risk/similarity signals into MBRD. It also proposes a sample-efficient variant of MBRD that adjusts the number of samples according to problem difficulty.

**结果:** The proposed approach demonstrates advantages in terms of higher robustness, improved accuracy, and well-understood asymptotic behavior. Empirical results on math and coding tasks using open-source models show the effectiveness of this method.

**结论:** This novel method for incorporating signals into MBRD provides several improvements over traditional methods and offers a better understanding of accuracy-compute trade-offs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Policy+Minimum+Bayesian+Risk，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17242，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17242&send_immediately=true&force_search=false)

**原文摘要:** Inference scaling can help LLMs solve complex reasoning problems through
extended runtime computation. On top of targeted supervision for long
chain-of-thought (long-CoT) generation, purely inference-time techniques such
as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes
risk decoding (MBRD), can further improve LLM accuracy by generating multiple
candidate solutions and aggregating over them. These methods typically leverage
additional signals in the form of reward models and risk/similarity functions
that compare generated samples, e.g., exact match in some normalized space or
standard similarity metrics such as Rouge. Here we present a novel method for
incorporating reward and risk/similarity signals into MBRD. Based on the
concept of optimal policy in KL-controlled reinforcement learning, our
framework provides a simple and well-defined mechanism for leveraging such
signals, offering several advantages over traditional inference-time methods:
higher robustness, improved accuracy, and well-understood asymptotic behavior.
In addition, it allows for the development of a sample-efficient variant of
MBRD that can adjust the number of samples to generate according to the
difficulty of the problem, without relying on majority vote counts. We
empirically demonstrate the advantages of our approach on math (MATH-$500$) and
coding (HumanEval) tasks using recent open-source models. We also present a
comprehensive analysis of its accuracy-compute trade-offs.

</details>


### [14] [Backdoors in DRL: Four Environments Focusing on In-distribution Triggers](https://arxiv.org/abs/2505.17248)
*Chace Ashcraft, Ted Staley, Josh Carney, Cameron Hickert, Derek Juba, Kiran Karra, Nathan Drenkow*

**主要类别:** cs.LG

**概要:** 在深度强化学习（DRL）代理中开发了几个木马，以研究隐藏在模型中的不良行为（后门攻击）。研究重点是在分布内触发器，因为它们比分布外触发器更容易被激活，从而构成更大的安全威胁。通过在四个强化学习环境中实现后门攻击，研究人员发现虽然在分布内触发器的实现需要额外的努力，并且对模型来说更难学习，但仍然是DRL中的可行威胁。


<details>
  <summary>更多</summary>
  
**动机:** 后门攻击对深度神经网络模型构成了安全隐患，而开源神经网络和第三方模型开发者普遍存在。为了推动对后门攻击缓解的研究，有必要开发针对深度强化学习（DRL）代理的木马。

**方法:** 研究者们在四个强化学习环境中实现了后门攻击：LavaWorld、Randomized LavaWorld、Colorful Memory 和 Modified Safety Gymnasium。他们训练了各种模型，包括干净的和带有后门的模型，以描述这些攻击的特性。重点关注在分布内触发器，因为它们比分布外触发器更容易在模型部署期间被攻击者激活。

**结果:** 研究发现，在分布内触发器的实现需要额外的努力，并且对于模型来说更难学习。然而，即使使用基本的数据投毒攻击，这些触发器仍然在深度强化学习中构成可行的威胁。

**结论:** 后门攻击，特别是那些具有在分布内触发器的攻击，对深度强化学习代理构成了显著的安全风险。尽管这些攻击可能更难实施，但它们仍然可以成功，因此需要进一步研究来缓解这种威胁。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Backdoors+in+DRL%3A+Four+Environments+Focusing+on+In-distribution+Triggers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17248，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17248&send_immediately=true&force_search=false)

**原文摘要:** Backdoor attacks, or trojans, pose a security risk by concealing undesirable
behavior in deep neural network models. Open-source neural networks are
downloaded from the internet daily, possibly containing backdoors, and
third-party model developers are common. To advance research on backdoor attack
mitigation, we develop several trojans for deep reinforcement learning (DRL)
agents. We focus on in-distribution triggers, which occur within the agent's
natural data distribution, since they pose a more significant security threat
than out-of-distribution triggers due to their ease of activation by the
attacker during model deployment. We implement backdoor attacks in four
reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld,
Colorful Memory, and Modified Safety Gymnasium. We train various models, both
clean and backdoored, to characterize these attacks. We find that
in-distribution triggers can require additional effort to implement and be more
challenging for models to learn, but are nevertheless viable threats in DRL
even using basic data poisoning attacks.

</details>


### [15] [Approach to Finding a Robust Deep Learning Model](https://arxiv.org/abs/2505.17254)
*Alexey Boldyrev, Fedor Ratnikov, Andrey Shevelev*

**主要类别:** cs.LG

**概要:** 提出了一种评估模型鲁棒性的新方法，并结合一个作为元算法的模型选择算法，适用于任何机器学习模型。研究了训练样本大小、模型权重初始化和归纳偏差对深度学习模型鲁棒性的影响。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习和人工智能应用的快速发展需要训练大量模型，强调了在无监督情况下训练模型的重要性，同时确保预测的可靠性。

**方法:** 提出一种新的模型鲁棒性评估方法和一个元算法形式的模型选择算法。使用小规模深度学习模型（包含卷积层和全连接层），以及常见的优化器，研究训练样本量、模型权重初始化和归纳偏置对模型鲁棒性的影响。

**结果:** 研究表明，所提出的方法可以有效评估深度学习模型的鲁棒性，揭示了训练样本量、模型权重初始化和归纳偏置对模型性能的影响。

**结论:** 提出的鲁棒性评估方法和模型选择算法具有广泛适用性，可帮助提升各种机器学习模型的可靠性和稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Approach+to+Finding+a+Robust+Deep+Learning+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17254&send_immediately=true&force_search=false)

**原文摘要:** The rapid development of machine learning (ML) and artificial intelligence
(AI) applications requires the training of large numbers of models. This
growing demand highlights the importance of training models without human
supervision, while ensuring that their predictions are reliable. In response to
this need, we propose a novel approach for determining model robustness. This
approach, supplemented with a proposed model selection algorithm designed as a
meta-algorithm, is versatile and applicable to any machine learning model,
provided that it is appropriate for the task at hand. This study demonstrates
the application of our approach to evaluate the robustness of deep learning
models. To this end, we study small models composed of a few convolutional and
fully connected layers, using common optimizers due to their ease of
interpretation and computational efficiency. Within this framework, we address
the influence of training sample size, model weight initialization, and
inductive bias on the robustness of deep learning models.

</details>


### [16] [JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model](https://arxiv.org/abs/2505.17257)
*Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild*

**主要类别:** cs.LG

**概要:** JanusDNA是一种新的双向DNA基础模型，结合了自回归建模的优化效率和掩码建模的双向理解能力。它采用混合架构，在单一80GB GPU上可处理高达1百万个碱基对，并在三个基因组表示基准上达到了新的最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型在应用于基因组学时面临重大挑战：1) 建模长距离依赖性需要处理跨越多达10,000个碱基对的复杂基因组相互作用；2) 标准的自回归训练方法仅支持单向理解，而DNA是双向的（如双向启动子）；3) 掩码语言模型虽然允许双向理解但效率低下。

**方法:** JanusDNA采用了一种新颖的预训练范式，结合了自回归建模的优化效率和掩码建模的双向理解。其混合架构包括Mamba、注意力机制和专家混合（MoE）层。Mamba负责高效的顺序学习，注意力机制用于长距离建模，而MoE层通过稀疏激活扩展模型容量同时保持较低的计算成本。该模型可以在单一80GB GPU上以单核苷酸分辨率处理多达1百万个碱基对。

**结果:** 广泛的实验和消融研究表明，JanusDNA在三个基因组表示基准上实现了新的最先进的结果，性能优于激活参数多250倍的模型。

**结论:** JanusDNA通过创新的预训练范式和混合架构解决了现有模型在基因组学应用中的局限性，显著提高了性能并降低了计算负担。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JanusDNA%3A+A+Powerful+Bi-directional+Hybrid+DNA+Foundation+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17257，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17257&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have revolutionized natural language processing
and are increasingly applied to other sequential data types, including genetic
sequences. However, adapting LLMs to genomics presents significant challenges.
Capturing complex genomic interactions requires modeling long-range
dependencies within DNA sequences, where interactions often span over 10,000
base pairs, even within a single gene, posing substantial computational burdens
under conventional model architectures and training paradigms. Moreover,
standard LLM training approaches are suboptimal for DNA: autoregressive
training, while efficient, supports only unidirectional understanding. However,
DNA is inherently bidirectional, e.g., bidirectional promoters regulate
transcription in both directions and account for nearly 11% of human gene
expression. Masked language models (MLMs) allow bidirectional understanding but
are inefficient, as only masked tokens contribute to the loss per step. To
address these limitations, we introduce JanusDNA, the first bidirectional DNA
foundation model built upon a novel pretraining paradigm that combines the
optimization efficiency of autoregressive modeling with the bidirectional
comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and
Mixture of Experts (MoE) architecture, combining long-range modeling of
Attention with efficient sequential learning of Mamba. MoE layers further scale
model capacity via sparse activation while keeping computational cost low.
Notably, JanusDNA processes up to 1 million base pairs at single nucleotide
resolution on a single 80GB GPU. Extensive experiments and ablations show
JanusDNA achieves new SOTA results on three genomic representation benchmarks,
outperforming models with 250x more activated parameters. Code:
https://github.com/Qihao-Duan/JanusDNA

</details>


### [17] [Zebra-Llama: Towards Extremely Efficient Hybrid Models](https://arxiv.org/abs/2505.17272)
*Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, Emad Barsoum*

**主要类别:** cs.LG

**概要:** 提出了一种新的混合模型Zebra-Llama，通过结合状态空间模型（SSMs）和多头潜在注意力（MLA）层，使用精炼的初始化和后训练管道从预训练的Transformer中高效转移知识。该方法在仅使用7-11B训练token的情况下，实现了与Transformer相当的精度和接近SSM的效率，并显著减少了KV缓存大小，同时保持了零样本性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着对大型语言模型（LLMs）在各种应用中的需求不断增长，提高其推理效率对于可持续和民主化的访问至关重要。然而，重新训练LLMs以满足新的用户特定需求过于昂贵且环境不可持续。

**方法:** 提出了Zebra-Llama方法，引入了一系列1B、3B和8B的混合模型，通过结合状态空间模型（SSMs）和多头潜在注意力（MLA）层，并使用精炼的初始化和后训练管道从预训练的Transformer中高效转移知识。

**结果:** Zebra-Llama在仅使用7-11B训练token的情况下，实现了与Transformer相当的精度和接近SSM的效率。此外，大幅减少了KV缓存大小，同时保持了零样本性能。与其他模型相比，Zebra-Llama在使用更少的token、更小的教师模型和显著减少的KV缓存内存的情况下，提供了竞争性或更高的准确性。

**结论:** Zebra-Llama提供了一种实用且可扩展的替代方案，可以组成高效的混合语言模型，从而显著提高推理效率并减少资源消耗。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zebra-Llama%3A+Towards+Extremely+Efficient+Hybrid+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17272&send_immediately=true&force_search=false)

**原文摘要:** With the growing demand for deploying large language models (LLMs) across
diverse applications, improving their inference efficiency is crucial for
sustainable and democratized access. However, retraining LLMs to meet new
user-specific requirements is prohibitively expensive and environmentally
unsustainable. In this work, we propose a practical and scalable alternative:
composing efficient hybrid language models from existing pre-trained models.
Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models
by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)
layers, using a refined initialization and post-training pipeline to
efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama
achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B
training tokens (compared to trillions of tokens required for pre-training) and
an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down
to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,
respectively-while preserving 100%, 100%, and >97% of average zero-shot
performance on LM Harness tasks. Compared to models like MambaInLLaMA,
X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive
or superior accuracy while using significantly fewer tokens, smaller teachers,
and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses
Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,
over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves
2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context
length. We will release code and model checkpoints upon acceptance.

</details>


### [18] [Comparator-Adaptive $Φ$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games](https://arxiv.org/abs/2505.17277)
*Soumita Hait, Ping Li, Haipeng Luo, Mengxiao Zhang*

**主要类别:** cs.LG

**概要:** 提出了一种更简单的算法，以实现更好的比较器自适应Φ-后悔界，并展示了其在游戏设置中的扩展能力。


<details>
  <summary>更多</summary>
  
**动机:** 经典专家问题中，Φ-后悔衡量学习者总损失与应用最佳动作转换ϕ∈Φ所得损失之间的差距。现有工作已引入一种自适应算法，该算法的后悔值取决于ϕ的稀疏性复杂度度量，几乎恢复并插值了外部、内部和交换后悔等标准后悔概念的最佳界限。

**方法:** 发现所有可能二元转换的先验分布，并表明针对这些转换实现依赖于先验的后悔就足够了。提出了两个具体且高效的算法：第一个基于Farina等人[2022]的内核化MWU算法的先验感知变体；第二个基于Blum和Mansour[2007]的BM-reduction的先验感知变体。

**结果:** 所提出的算法实现了更好的比较器自适应Φ-后悔界，并且第二种方法可以扩展到游戏设置，以实现加速和自适应收敛到一类一般和游戏的Φ-均衡。对于相关均衡的特殊情况，改进了现有的界限。

**结论:** 新算法不仅更简单，而且具有更好的后悔界，并能扩展到游戏设置中，展现了比Lu等人[2025]工作的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparator-Adaptive+%24%CE%A6%24-Regret%3A+Improved+Bounds%2C+Simpler+Algorithms%2C+and+Applications+to+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17277，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17277&send_immediately=true&force_search=false)

**原文摘要:** In the classic expert problem, $\Phi$-regret measures the gap between the
learner's total loss and that achieved by applying the best action
transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces
an adaptive algorithm whose regret against a comparator $\phi$ depends on a
certain sparsity-based complexity measure of $\phi$, (almost) recovering and
interpolating optimal bounds for standard regret notions such as external,
internal, and swap regret. In this work, we propose a general idea to achieve
an even better comparator-adaptive $\Phi$-regret bound via much simpler
algorithms compared to Lu et al., [2025]. Specifically, we discover a prior
distribution over all possible binary transformations and show that it suffices
to achieve prior-dependent regret against these transformations. Then, we
propose two concrete and efficient algorithms to achieve so, where the first
one learns over multiple copies of a prior-aware variant of the Kernelized MWU
algorithm of Farina et al., [2022], and the second one learns over multiple
copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007].
To further showcase the power of our methods and the advantages over Lu et al.,
[2025] besides the simplicity and better regret bounds, we also show that our
second approach can be extended to the game setting to achieve accelerated and
adaptive convergence rate to $\Phi$-equilibria for a class of general-sum
games. When specified to the special case of correlated equilibria, our bound
improves over the existing ones from Anagnostides et al., [2022a,b]

</details>


### [19] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
*Diyuan Wu, Aleksandr Shevchenko, Samet Oymak, Marco Mondelli*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention+with+Trained+Embeddings+Provably+Selects+Important+Tokens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17282&send_immediately=true&force_search=false)

**原文摘要:** Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [20] [Model-Free Graph Data Selection under Distribution Shift](https://arxiv.org/abs/2505.17293)
*Ting-Wei Li, Ruizhong Qiu, Hanghang Tong*

**主要类别:** cs.LG

**概要:** 提出了一种新的无模型框架GRADATE，用于图域适应任务。该框架通过最优传输理论选择源域中最佳的训练数据，以应对目标域分类任务中的分布变化问题。GRADATE在多个真实图级别数据集和协变量转移类型上表现出色，优于现有的选择方法，并且可以显著增强现有的GDA方法，同时使用更少的训练数据。


<details>
  <summary>更多</summary>
  
**动机:** 现有的模型中心方法（如shift-robust GNNs）虽然在图域适应任务中表现良好，但在面对严重分布变化和计算资源受限时往往效果不佳。因此需要一种新的方法来有效选择源域中的最佳训练数据，以应对目标域中的分类任务。

**方法:** 提出了一种名为GRADATE的无模型框架，该框架利用最优传输理论捕获并适应分布变化，从而从源域中选择最适合目标域分类任务的训练数据。此方法不依赖任何GNN模型的预测或训练策略。

**结果:** 通过在多个真实世界图级别的数据集和多种协变量转移类型上的综合实证研究，证明了GRADATE优于现有的选择方法，并且可以在使用更少训练数据的情况下增强现成的GDA方法。

**结论:** GRADATE是一种数据高效、可扩展的框架，能够补充现有的模型中心GDA方法，为图域适应任务提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model-Free+Graph+Data+Selection+under+Distribution+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17293，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17293&send_immediately=true&force_search=false)

**原文摘要:** Graph domain adaptation (GDA) is a fundamental task in graph machine
learning, with techniques like shift-robust graph neural networks (GNNs) and
specialized training procedures to tackle the distribution shift problem.
Although these model-centric approaches show promising results, they often
struggle with severe shifts and constrained computational resources. To address
these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa
sElector), that selects the best training data from the source domain for the
classification task on the target domain. GRADATE picks training samples
without relying on any GNN model's predictions or training recipes, leveraging
optimal transport theory to capture and adapt to distribution changes. GRADATE
is data-efficient, scalable and meanwhile complements existing model-centric
GDA approaches. Through comprehensive empirical studies on several real-world
graph-level datasets and multiple covariate shift types, we demonstrate that
GRADATE outperforms existing selection methods and enhances off-the-shelf GDA
methods with much fewer training data.

</details>


### [21] [CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots](https://arxiv.org/abs/2505.17354)
*Keisuke Kawano, Takuro Kutsuna, Naoki Hayashi, Yasushi Esaki, Hidenori Tanaka*

**主要类别:** cs.LG

**概要:** 在许多现实场景中，例如单细胞RNA测序，数据仅作为离散时间快照被观察到，并受到噪声时间戳的影响。从这些快照恢复底层的连续时间动态是一项关键且具有挑战性的任务。本文提出了一种方法CT-OT Flow，它首先通过部分最优传输推断高分辨率时间标签，然后通过时间核平滑重建连续时间数据分布。这种方法在合成基准和真实数据集上的表现优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 许多实际应用场景中的数据以离散时间快照形式出现，且受到噪声时间戳的影响，因此需要一种能够从这些快照中恢复底层连续时间动态的方法。

**方法:** CT-OT Flow方法首先使用部分最优传输来推断高分辨率时间标签，然后通过时间核平滑重建连续时间数据分布。这使得能够准确地训练动态模型（如ODEs和SDEs）。

**结果:** CT-OT Flow在合成基准测试和真实数据集（如单细胞RNA测序和台风轨迹数据）上均表现出色，重建误差较低。

**结论:** CT-OT Flow提供了一个精确且通用的框架，用于连接离散快照和连续时间过程，强调了显式建模时间离散化和时间戳不确定性的好处。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CT-OT+Flow%3A+Estimating+Continuous-Time+Dynamics+from+Discrete+Temporal+Snapshots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17354&send_immediately=true&force_search=false)

**原文摘要:** In many real-world scenarios, such as single-cell RNA sequencing, data are
observed only as discrete-time snapshots spanning finite time intervals and
subject to noisy timestamps, with no continuous trajectories available.
Recovering the underlying continuous-time dynamics from these snapshots with
coarse and noisy observation times is a critical and challenging task. We
propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers
high-resolution time labels via partial optimal transport and then reconstructs
a continuous-time data distribution through a temporal kernel smoothing. This
reconstruction enables accurate training of dynamics models such as ODEs and
SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic
benchmarks and achieves lower reconstruction errors on real scRNA-seq and
typhoon-track datasets. Our results highlight the benefits of explicitly
modeling temporal discretization and timestamp uncertainty, offering an
accurate and general framework for bridging discrete snapshots and
continuous-time processes.

</details>


### [22] [Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions](https://arxiv.org/abs/2505.17304)
*Jianhao Ma, Geyu Liang, Salar Fattahi*

**主要类别:** cs.LG

**概要:** 在过参数化问题中，研究梯度方法如何通过微小扰动实现隐式正则化并保持低维解。


<details>
  <summary>更多</summary>
  
**动机:** 隐式正则化现象广泛存在，但理论探索不足，特别是在现代过参数化问题中。作者希望通过研究梯度方法收敛到特定点的条件，揭示隐式正则化的机制。

**方法:** 分析了成功实现隐式正则化的两个关键条件：有效逃离严格鞍点和保持靠近隐式区域。提出了IPGD方法，解释为带有'舍入误差'的梯度下降，并证明其可以满足这两个条件。将该框架应用于过参数化矩阵感知问题，并进行了广泛的实验验证。

**结果:** 证明了IPGD可以在逃离严格鞍点的同时控制偏离隐式区域的程度，适用于过参数化矩阵感知问题，并展示了其对更广泛学习问题的有效性。

**结论:** 通过微小扰动的梯度方法（如IPGD）可以在现代过参数化问题中实现隐式正则化，为理解此类现象提供了理论支持和实际应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Implicit+Regularization+of+Infinitesimally-perturbed+Gradient+Descent+Toward+Low-dimensional+Solutions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17304&send_immediately=true&force_search=false)

**原文摘要:** Implicit regularization refers to the phenomenon where local search
algorithms converge to low-dimensional solutions, even when such structures are
neither explicitly specified nor encoded in the optimization problem. While
widely observed, this phenomenon remains theoretically underexplored,
particularly in modern over-parameterized problems. In this paper, we study the
conditions that enable implicit regularization by investigating when
gradient-based methods converge to second-order stationary points (SOSPs)
within an implicit low-dimensional region of a smooth, possibly nonconvex
function. We show that successful implicit regularization hinges on two key
conditions: $(i)$ the ability to efficiently escape strict saddle points, while
$(ii)$ maintaining proximity to the implicit region. Existing analyses enabling
the convergence of gradient descent (GD) to SOSPs often rely on injecting large
perturbations to escape strict saddle points. However, this comes at the cost
of deviating from the implicit region. The central premise of this paper is
that it is possible to achieve the best of both worlds: efficiently escaping
strict saddle points using infinitesimal perturbations, while controlling
deviation from the implicit region via a small deviation rate. We show that
infinitesimally perturbed gradient descent (IPGD), which can be interpreted as
GD with inherent ``round-off errors'', can provably satisfy both conditions. We
apply our framework to the problem of over-parameterized matrix sensing, where
we establish formal guarantees for the implicit regularization behavior of
IPGD. We further demonstrate through extensive experiments that these insights
extend to a broader class of learning problems.

</details>


### [23] [Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling](https://arxiv.org/abs/2505.17384)
*Tianyu Xie, Shuchen Xue, Zijin Feng, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Cheng Zhang*

**主要类别:** cs.LG

**概要:** 本文提出了一种名为变分自编码离散扩散（VADD）的新框架，通过潜在变量建模增强离散扩散模型，从而隐式捕获维度间的相关性。VADD引入辅助识别模型以实现稳定的训练，并在少量去噪步骤的情况下显著提高样本质量。实验结果表明，VADD在2D玩具数据、像素级图像生成和文本生成任务中均优于传统的MDM基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 离散扩散模型在复杂离散数据建模方面显示出巨大潜力，但现有方法如掩码扩散模型（MDMs）在使用少量去噪步骤时性能可能下降，因为它们对维度间依赖关系的建模有限。

**方法:** 提出了变分自编码离散扩散（VADD）框架，通过引入潜在变量建模来捕捉维度间的相关性。同时，引入辅助识别模型以实现基于变分下界最大化和训练集上的摊销推理的稳定训练。

**结果:** 在2D玩具数据、像素级图像生成和文本生成任务中的实证结果表明，VADD在少量去噪步骤的情况下显著提高了样本质量，并且始终优于MDM基线模型。

**结论:** VADD框架通过结合潜在变量建模和摊销推理，在保持传统MDM效率的同时，显著提高了样本质量，特别是在少量去噪步骤的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Autoencoding+Discrete+Diffusion+with+Enhanced+Dimensional+Correlations+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17384，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17384&send_immediately=true&force_search=false)

**原文摘要:** Discrete diffusion models have recently shown great promise for modeling
complex discrete data, with masked diffusion models (MDMs) offering a
compelling trade-off between quality and generation speed. MDMs denoise by
progressively unmasking multiple dimensions from an all-masked input, but their
performance can degrade when using few denoising steps due to limited modeling
of inter-dimensional dependencies. In this paper, we propose Variational
Autoencoding Discrete Diffusion (VADD), a novel framework that enhances
discrete diffusion with latent variable modeling to implicitly capture
correlations among dimensions. By introducing an auxiliary recognition model,
VADD enables stable training via variational lower bounds maximization and
amortized inference over the training set. Our approach retains the efficiency
of traditional MDMs while significantly improving sample quality, especially
when the number of denoising steps is small. Empirical results on 2D toy data,
pixel-level image generation, and text generation demonstrate that VADD
consistently outperforms MDM baselines.

</details>


### [24] [Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification](https://arxiv.org/abs/2505.17307)
*Pu Yang, J. A. Barria*

**主要类别:** cs.LG

**概要:** This paper introduces Wavelet Probabilistic Recurrent Convolutional Network (WPRCN) for Multivariate Time Series Classification (MTSC). WPRCN uses a wavelet probabilistic module with an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN). It outperforms benchmarks on 30 datasets in accuracy and rank, especially with scarce or noisy data.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenges of non-stationary environments, data scarcity, and noise perturbations in multivariate time series classification.

**方法:** Proposes WPRCN which includes a wavelet probabilistic module consisting of AWPG and APTCN. The AWPG generates probabilistic features addressing data scarcities and non-stationarities, while the APTCN analyses feature correlations to form a comprehensive feature space. This module can be integrated with LSTM and C-FCN networks.

**结果:** Outperforms all benchmark algorithms on average accuracy and rank when evaluated on 30 diverse MTS datasets, showing strength in handling scarce and noisy physiological data.

**结论:** WPRCN is effective for MTSC tasks, particularly excelling in scenarios involving non-stationary, scarce, and noisy data.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wavelet+Probabilistic+Recurrent+Convolutional+Network+for+Multivariate+Time+Series+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17307&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a Wavelet Probabilistic Recurrent Convolutional Network
(WPRCN) for Multivariate Time Series Classification (MTSC), especially
effective in handling non-stationary environments, data scarcity and noise
perturbations. We introduce a versatile wavelet probabilistic module designed
to extract and analyse the probabilistic features, which can seamlessly
integrate with a variety of neural network architectures. This probabilistic
module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and
a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN).
Such formulation extends the application of wavelet probabilistic neural
networks to deep neural networks for MTSC. The AWPG constructs an ensemble
probabilistic model addressing different data scarcities and non-stationarity;
it adaptively selects the optimal ones and generates probabilistic features for
APTCN. The APTCN analyses the correlations of the features and forms a
comprehensive feature space with existing MTSC models for classification. Here,
we instantiate the proposed module to work in parallel with a Long Short-Term
Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN),
demonstrating its broad applicability in time series analysis. The WPRCN is
evaluated on 30 diverse MTS datasets and outperforms all the benchmark
algorithms on average accuracy and rank, exhibiting pronounced strength in
handling scarce data and physiological data subject to perturbations and
non-stationarities.

</details>


### [25] [Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/abs/2505.17638)
*Tony Bonnaire, Raphaël Urfin, Giulio Biroli, Marc Mézard*

**主要类别:** cs.LG

**概要:** 扩散模型在生成任务中表现出色，本文研究了训练动态在从泛化到记忆转变中的作用，发现了两个不同时间尺度：早期的高质量样本生成时间和后期的记忆出现时间，并揭示了训练动态中隐含的动力学正则化机制。


<details>
  <summary>更多</summary>
  
**动机:** 理解扩散模型在生成任务中不记忆训练数据而能泛化的机制。

**方法:** 通过广泛的实验和理论分析，研究扩散模型训练动态中从泛化到记忆的转变，识别出两个不同的时间尺度并探讨其与训练集大小的关系。

**结果:** 发现记忆时间随训练集大小线性增加，而泛化时间保持不变，揭示了训练动态中的隐式动力学正则化。

**结论:** 即使在高度过参数化的情况下，这种隐式的动力学正则化也能避免模型的记忆化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+Diffusion+Models+Don%27t+Memorize%3A+The+Role+of+Implicit+Dynamical+Regularization+in+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17638，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17638&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have achieved remarkable success across a wide range of
generative tasks. A key challenge is understanding the mechanisms that prevent
their memorization of training data and allow generalization. In this work, we
investigate the role of the training dynamics in the transition from
generalization to memorization. Through extensive experiments and theoretical
analysis, we identify two distinct timescales: an early time
$\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and
a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially,
we find that $\tau_\mathrm{mem}$ increases linearly with the training set size
$n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window
of training times with $n$ where models generalize effectively, despite showing
strong memorization if training continues beyond it. It is only when $n$
becomes larger than a model-dependent threshold that overfitting disappears at
infinite training times. These findings reveal a form of implicit dynamical
regularization in the training dynamics, which allow to avoid memorization even
in highly overparameterized settings. Our results are supported by numerical
experiments with standard U-Net architectures on realistic and synthetic
datasets, and by a theoretical analysis using a tractable random features model
studied in the high-dimensional limit.

</details>


### [26] [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/abs/2505.17331)
*Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种名为ECHO-LLaMA的高效LLaMA架构，通过跨特定层共享KV缓存来显著降低计算复杂度，同时保持或提升语言性能。实验结果表明，ECHO-LLaMA在训练期间每秒处理的令牌数提高了77%，模型浮点运算利用率（MFU）提升了16%，并且在相同数量的令牌训练后损失降低了14%。此外，在1.1B模型上，与基线相比，测试时吞吐量提高了约7%。ECHO-LLaMA提供了一种可扩展且具有成本效益的大型语言模型预训练和微调解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 当前的LLaMA模型在训练和推理过程中存在较高的计算复杂度问题，因此需要一种既能提高训练速度和推理吞吐量，又不会牺牲学习能力的改进方法。

**方法:** ECHO-LLaMA将LLaMA模型转换为跨某些层共享KV缓存的方式，从而减少计算复杂度，并保持或改善语言性能。这种方法通过引入计算高效的适应机制来实现。

**结果:** 实验结果显示，ECHO-LLaMA在训练期间每秒处理的令牌数提高了77%，模型浮点运算利用率（MFU）提升了16%，损失减少了14%。在1.1B模型上，与基线相比，测试时吞吐量提高了约7%。

**结论:** ECHO-LLaMA提供了一种可扩展且成本效益高的解决方案，用于加速大型语言模型的预训练和微调过程，同时保证了性能不下降。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ECHO-LLaMA%3A+Efficient+Caching+for+High-Performance+LLaMA+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17331&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to
improve both the training speed and inference throughput of LLaMA architectures
while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models
into shared KV caching across certain layers, significantly reducing KV
computational complexity while maintaining or improving language performance.
Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher
token-per-second throughput during training, up to 16\% higher Model FLOPs
Utilization (MFU), and up to 14\% lower loss when trained on an equal number of
tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\%
higher test-time throughput compared to the baseline. By introducing a
computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable
and cost-effective solution for pretraining and finetuning large language
models, enabling faster and more resource-efficient training without
compromising performance.

</details>


### [27] [Discrete Neural Flow Samplers with Locally Equivariant Transformer](https://arxiv.org/abs/2505.17741)
*Zijing Ou, Ruixiang Zhang, Yingzhen Li*

**主要类别:** cs.LG

**概要:** 提出了一种名为离散神经流采样器（DNFS）的新方法，用于从非标准化离散分布中高效采样。该方法通过学习连续时间马尔可夫链的速率矩阵来满足Kolmogorov方程，并采用控制变量技术降低蒙特卡洛估计的方差，同时提出局部等变Transformer以提高训练效率。实验表明，DNFS在多种任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 从非标准化离散分布中采样是跨领域的重要问题，但现有的马尔可夫链蒙特卡洛方法存在混合缓慢和收敛性差的问题。

**方法:** 提出了DNFS框架，通过学习连续时间马尔可夫链的速率矩阵使其动态满足Kolmogorov方程。使用控制变量技术降低估计方差，并设计了局部等变Transformer以提高计算效率和网络表达能力。

**结果:** 实验结果表明，DNFS在从非标准化分布中采样、训练离散能量模型以及解决组合优化问题等多种应用中均表现出了高效性和优越性能。

**结论:** DNFS作为一种可训练且高效的离散采样框架，为解决离散分布采样问题提供了新的解决方案，并展示了其在多个领域的潜在应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discrete+Neural+Flow+Samplers+with+Locally+Equivariant+Transformer，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17741，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17741&send_immediately=true&force_search=false)

**原文摘要:** Sampling from unnormalised discrete distributions is a fundamental problem
across various domains. While Markov chain Monte Carlo offers a principled
approach, it often suffers from slow mixing and poor convergence. In this
paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and
efficient framework for discrete sampling. DNFS learns the rate matrix of a
continuous-time Markov chain such that the resulting dynamics satisfy the
Kolmogorov equation. As this objective involves the intractable partition
function, we then employ control variates to reduce the variance of its Monte
Carlo estimation, leading to a coordinate descent learning algorithm. To
further facilitate computational efficiency, we propose locally equivaraint
Transformer, a novel parameterisation of the rate matrix that significantly
improves training efficiency while preserving powerful network expressiveness.
Empirically, we demonstrate the efficacy of DNFS in a wide range of
applications, including sampling from unnormalised distributions, training
discrete energy-based models, and solving combinatorial optimisation problems.

</details>


### [28] [Conformal Predictive Distributions for Order Fulfillment Time Forecasting](https://arxiv.org/abs/2505.17340)
*Tinghan Ye, Amira Hijazi, Pascal Van Hentenryck*

**主要类别:** cs.LG

**概要:** 准确估计订单履行时间对电商物流至关重要，但传统基于规则的方法往往无法捕捉配送操作中的固有不确定性。本文提出了一种新的框架，用于订单履行时间的分布预测，结合了符合性预测系统和交叉Venn-Abers预测器——这些与模型无关的技术提供了严格的覆盖率或有效性保证。所提出的机器学习方法整合了精细的空间时间特征，捕捉履行地点和承运人绩效动态以提高预测准确性。此外，还开发了一种成本敏感的决策规则，将概率预测转化为可靠的点预测。在大规模工业数据集上的实验评估表明，所提出的方法生成了具有竞争力的分布预测，而基于机器学习的点预测显著优于现有的基于规则的系统——预测准确性提高了14%，识别延迟交付的能力提高了75%。


<details>
  <summary>更多</summary>
  
**动机:** 传统基于规则的订单履行时间预测方法无法有效捕捉配送操作中的固有不确定性，影响了电商物流的效率和客户满意度。

**方法:** 提出了一个新的框架，结合Conformal Predictive Systems和Cross Venn-Abers Predictors进行分布预测；利用机器学习方法整合空间时间特征，捕捉履行地点和承运人绩效动态；开发了成本敏感的决策规则，将概率预测转化为可靠的点预测。

**结果:** 在大规模工业数据集上验证，该方法的分布预测具有竞争力，且基于机器学习的点预测显著优于现有基于规则的系统，预测准确率提升14%，识别延迟交付能力提升75%。

**结论:** 提出的分布预测框架和机器学习方法能够更准确地预测订单履行时间，显著改善电商物流的运营效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conformal+Predictive+Distributions+for+Order+Fulfillment+Time+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17340，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17340&send_immediately=true&force_search=false)

**原文摘要:** Accurate estimation of order fulfillment time is critical for e-commerce
logistics, yet traditional rule-based approaches often fail to capture the
inherent uncertainties in delivery operations. This paper introduces a novel
framework for distributional forecasting of order fulfillment time, leveraging
Conformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic
techniques that provide rigorous coverage or validity guarantees. The proposed
machine learning methods integrate granular spatiotemporal features, capturing
fulfillment location and carrier performance dynamics to enhance predictive
accuracy. Additionally, a cost-sensitive decision rule is developed to convert
probabilistic forecasts into reliable point predictions. Experimental
evaluation on a large-scale industrial dataset demonstrates that the proposed
methods generate competitive distributional forecasts, while machine
learning-based point predictions significantly outperform the existing
rule-based system--achieving up to 14% higher prediction accuracy and up to 75%
improvement in identifying late deliveries.

</details>


### [29] [Scalable Valuation of Human Feedback through Provably Robust Model Alignment](https://arxiv.org/abs/2505.17859)
*Masahiro Fujisawa, Masaki Adachi, Michael A. Osborne*

**主要类别:** cs.LG

**概要:** 提出Hölder-DPO，一种新的对齐损失方法，具有可证明的redescending特性，能从嘈杂的人类反馈中估计干净数据分布，提升语言模型与人类偏好的一致性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的对齐方法在处理嘈杂的人类反馈时表现不佳，无法有效应对标签噪声问题。

**方法:** 设计了Hölder-DPO，首个具备理论支持redescending特性的对齐损失函数，通过估计干净数据分布来识别和减少错误标签的影响。

**结果:** Hölder-DPO在人工控制的数据集上展现了最先进的鲁棒对齐性能，并能准确检测错误标签；应用于实际对齐数据集时，发现大量噪声并证明去除这些噪声可显著提高对齐效果。

**结论:** Hölder-DPO为语言模型的对齐问题提供了一种更鲁棒、更高效的解决方案，能够自动化评估人类反馈质量而无需额外的手动验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Valuation+of+Human+Feedback+through+Provably+Robust+Model+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17859&send_immediately=true&force_search=false)

**原文摘要:** Despite the importance of aligning language models with human preferences,
crowd-sourced human feedback is often noisy -- for example, preferring less
desirable responses -- posing a fundamental challenge to alignment. A truly
robust alignment objective should yield identical model parameters even under
severe label noise, a property known as redescending. We prove that no existing
alignment methods satisfy this property. To address this, we propose
H\"older-DPO, the first principled alignment loss with a provable redescending
property, enabling estimation of the clean data distribution from noisy
feedback. The aligned model estimates the likelihood of clean data, providing a
theoretically grounded metric for dataset valuation that identifies the
location and fraction of mislabels. This metric is gradient-free, enabling
scalable and automated human feedback valuation without costly manual
verification or clean validation dataset. H\"older-DPO achieves
state-of-the-art robust alignment performance while accurately detecting
mislabels in controlled datasets. Finally, we apply H\"older-DPO to widely used
alignment datasets, revealing substantial noise levels and demonstrating that
removing these mislabels significantly improves alignment performance across
methods.

</details>


### [30] [TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation](https://arxiv.org/abs/2505.17341)
*Dibyajyoti Nayak, Somdatta Goswami*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的框架TI-DeepONet，通过将神经算子与自适应数值时间步进技术相结合，解决了动力系统中长时间预测的误差累积问题。相比传统方法，TI-DeepONet显著减少了相对L2外推误差，并保持了预测稳定性。进一步发展的TI(L)-DeepONet通过引入可学习系数提升了预测精度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度算子网络（DeepONet）在动力系统建模中面临两个主要限制：固定时间范围的预测忽略了时间因果性，而自回归公式则通过顺序预测积累误差。这促使研究者寻找一种能够保留动力系统马尔可夫结构同时减少误差传播的新方法。

**方法:** TI-DeepONet将学习目标从直接状态预测改为瞬时时间导数场的近似，然后使用已建立的数值方案进行积分。该方法支持连续时间预测，并允许在推理阶段部署比训练阶段更高精度的积分器。TI(L)-DeepONet进一步引入了可学习的中间斜率系数以适应特定解的变化，从而提高保真度。

**结果:** 在三个经典PDE上的评估显示，TI(L)-DeepONet略微优于TI-DeepONet，两者分别将相对L2外推误差减少了约81%（相对于自回归方法）和70%（相对于固定时间范围方法）。此外，两种模型在预测时间范围达到训练区间两倍时仍能保持稳定性。

**结论:** 本研究确立了一种物理感知的算子学习范式，将神经逼近与数值分析相结合，同时保留了动力系统的因果结构，为动力系统建模提供了更准确和稳定的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TI-DeepONet%3A+Learnable+Time+Integration+for+Stable+Long-Term+Extrapolation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17341，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17341&send_immediately=true&force_search=false)

**原文摘要:** Accurate temporal extrapolation presents a fundamental challenge for neural
operators in modeling dynamical systems, where reliable predictions must extend
significantly beyond the training time horizon. Conventional Deep Operator
Network (DeepONet) approaches employ two inherently limited training paradigms
- fixed-horizon rollouts that predict complete spatiotemporal solutions while
disregarding temporal causality, and autoregressive formulations that
accumulate errors through sequential predictions. We introduce TI-DeepONet, a
framework that integrates neural operators with adaptive numerical
time-stepping techniques to preserve the Markovian structure of dynamical
systems while mitigating error propagation in extended temporal forecasting.
Our approach reformulates the learning objective from direct state prediction
to the approximation of instantaneous time-derivative fields, which are then
integrated using established numerical schemes. This architecture supports
continuous-time prediction and enables deployment of higher-precision
integrators during inference than those used during training, balancing
computational efficiency with predictive accuracy. We further develop
TI(L)-DeepONet, which incorporates learnable coefficients for intermediate
slopes in the integration process, adapting to solution-specific variations and
enhancing fidelity. Evaluation across three canonical PDEs shows that
TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative
L2 extrapolation errors: approximately 81% over autoregressive and 70% over
fixed-horizon methods. Notably, both maintain prediction stability for temporal
domains extending to about twice the training interval. This research
establishes a physics-aware operator learning paradigm that bridges neural
approximation with numerical analysis while preserving the causal structure of
dynamical systems.

</details>


### [31] [Distances for Markov chains from sample streams](https://arxiv.org/abs/2505.18005)
*Sergio Calo, Anders Jonsson, Gergely Neu, Ludovic Schwartz, Javier Segovia-Aguas*

**主要类别:** cs.LG

**概要:** 本论文提出了一种基于样本的随机优化方法，用于估计双模拟度量，无需明确的转换模型，并提供了算法的样本复杂度理论保证及实证有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的双模拟度量计算方法需要完全了解转移动态，而在现实场景中通常只能获得样本轨迹，因此需要一种不依赖显式转换模型的方法来估计双模拟度量。

**方法:** 提出了一种新的线性规划（LP）公式化双模拟度量的方法，并使用随机原始-对偶优化方法进行求解。此方法仅需样本访问，无需显式转换模型。

**结果:** 理论分析给出了算法的样本复杂度保证，并通过一系列实证评估验证了该方法的有效性。

**结论:** 所提出的随机优化方法成功解决了现有方法需要明确转换模型的限制，为在仅有样本轨迹的情况下估计双模拟度量提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distances+for+Markov+chains+from+sample+streams，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18005&send_immediately=true&force_search=false)

**原文摘要:** Bisimulation metrics are powerful tools for measuring similarities between
stochastic processes, and specifically Markov chains. Recent advances have
uncovered that bisimulation metrics are, in fact, optimal-transport distances,
which has enabled the development of fast algorithms for computing such metrics
with provable accuracy and runtime guarantees. However, these recent methods,
as well as all previously known methods, assume full knowledge of the
transition dynamics. This is often an impractical assumption in most real-world
scenarios, where typically only sample trajectories are available. In this
work, we propose a stochastic optimization method that addresses this
limitation and estimates bisimulation metrics based on sample access, without
requiring explicit transition models. Our approach is derived from a new linear
programming (LP) formulation of bisimulation metrics, which we solve using a
stochastic primal-dual optimization method. We provide theoretical guarantees
on the sample complexity of the algorithm and validate its effectiveness
through a series of empirical evaluations.

</details>


### [32] [A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety](https://arxiv.org/abs/2505.17342)
*Ankita Kushwaha, Kiran Ravish, Preeti Lamba, Pawan Kumar*

**主要类别:** cs.LG

**概要:** Safe Reinforcement Learning (SafeRL) is reviewed with a focus on Constrained Markov Decision Processes (CMDPs) and Multi-Agent Safe RL (SafeMARL). Theoretical foundations, state-of-the-art algorithms for single agents, and recent advances in SafeMARL are summarized. Five open research problems are proposed.


<details>
  <summary>更多</summary>
  
**动机:** To provide a mathematically rigorous overview of SafeRL formulations based on CMDPs and extensions to SafeMARL, as well as summarize current algorithms and propose future research directions.

**方法:** Reviewing theoretical foundations of CMDPs, summarizing state-of-the-art algorithms in SafeRL for single agents including policy gradient methods with safety guarantees and safe exploration strategies, and reviewing recent advances in SafeMARL for cooperative and competitive settings.

**结果:** A comprehensive summary of the theoretical foundations, algorithms, and open research problems in SafeRL and SafeMARL has been provided.

**结论:** This survey serves as a technical guide for researchers interested in SafeRL and SafeMARL, highlighting key concepts, methods, and open future research directions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Survey+of+Safe+Reinforcement+Learning+and+Constrained+MDPs%3A+A+Technical+Survey+on+Single-Agent+and+Multi-Agent+Safety，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17342&send_immediately=true&force_search=false)

**原文摘要:** Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement
learning that explicitly deals with safety constraints during the learning and
deployment of agents. This survey provides a mathematically rigorous overview
of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs)
and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical
foundations of CMDPs, covering definitions, constrained optimization
techniques, and fundamental theorems. We then summarize state-of-the-art
algorithms in SafeRL for single agents, including policy gradient methods with
safety guarantees and safe exploration strategies, as well as recent advances
in SafeMARL for cooperative and competitive settings. Additionally, we propose
five open research problems to advance the field, with three focusing on
SafeMARL. Each problem is described with motivation, key challenges, and
related prior work. This survey is intended as a technical guide for
researchers interested in SafeRL and SafeMARL, highlighting key concepts,
methods, and open future research directions.

</details>


### [33] [Linear Mixture Distributionally Robust Markov Decision Processes](https://arxiv.org/abs/2505.18044)
*Zhishuai Liu, Pan Xu*

**主要类别:** cs.LG

**概要:** 在面对不同源域和目标域的状态转换时，决策问题常常遇到离域挑战。本文提出了一种新的线性混合分布鲁棒马尔可夫决策过程（DRMDP）框架，通过基于混合权重参数的不确定性集合来解决这一挑战，并提出了适用于一般$f$-散度定义的不确定性集合的元算法。此外，文章分析了在三种散度度量下的样本复杂度，为线性混合DRMDP的统计学习能力提供了理论基础。


<details>
  <summary>更多</summary>
  
**动机:** 许多现实中的决策问题面临离域挑战，即智能体在一个源域中学习策略，然后在具有不同状态转移的目标域中部署该策略。现有的DRMDP方法依赖于对动态特性的先验知识设计不确定性集合，但可能不够精细。

**方法:** 本文提出了一种线性混合DRMDP框架，假设标称动态为线性混合模型，将不确定性集合定义为围绕混合权重参数的球形区域，而不是直接以标称核为中心的球形区域。还提出了一种元算法用于学习线性混合DRMDP中的鲁棒策略，并分析了在总变差、KL散度和$\chi^2$散度下的样本复杂度。

**结果:** 新框架在存在关于混合模型的先验知识时，能够提供比传统基于$(s,a)$-矩形性和$d$-矩形性的模型更精细的不确定性表示。分析表明，线性混合DRMDP是统计可学习的，并为其未来的理论研究奠定了基础。

**结论:** 本文提出的线性混合DRMDP框架及其元算法为解决离域挑战提供了一种新的有效途径，特别是在存在先验知识的情况下，可以更好地表示不确定性并实现鲁棒策略的学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Linear+Mixture+Distributionally+Robust+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18044&send_immediately=true&force_search=false)

**原文摘要:** Many real-world decision-making problems face the off-dynamics challenge: the
agent learns a policy in a source domain and deploys it in a target domain with
different state transitions. The distributionally robust Markov decision
process (DRMDP) addresses this challenge by finding a robust policy that
performs well under the worst-case environment within a pre-specified
uncertainty set of transition dynamics. Its effectiveness heavily hinges on the
proper design of these uncertainty sets, based on prior knowledge of the
dynamics. In this work, we propose a novel linear mixture DRMDP framework,
where the nominal dynamics is assumed to be a linear mixture model. In contrast
with existing uncertainty sets directly defined as a ball centered around the
nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a
ball around the mixture weighting parameter. We show that this new framework
provides a more refined representation of uncertainties compared to
conventional models based on $(s,a)$-rectangularity and $d$-rectangularity,
when prior knowledge about the mixture model is present. We propose a meta
algorithm for robust policy learning in linear mixture DRMDPs with general
$f$-divergence defined uncertainty sets, and analyze its sample complexities
under three divergence metrics instantiations: total variation,
Kullback-Leibler, and $\chi^2$ divergences. These results establish the
statistical learnability of linear mixture DRMDPs, laying the theoretical
foundation for future research on this new setting.

</details>


### [34] [A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction](https://arxiv.org/abs/2505.17344)
*Ninda Nurseha Amalina, Kwadwo Boateng Ofori-Amanfo, Heungjo An*

**主要类别:** cs.LG

**概要:** 论文提出了一种新的混合模型 MHASRF，通过将注意力机制与随机森林结合，使用概率软分裂替代硬分裂，从而提高了预测患者未出席预约的准确性、精确性、召回率和 F1 分数。该模型超越了传统方法，并提供了对患者未出席关键预测因素的更深入理解。


<details>
  <summary>更多</summary>
  
**动机:** 未出席预约（no-shows）对医疗提供者和患者的健康产生负面影响，破坏了护理连续性、运营效率和医疗资源的有效分配。因此，需要准确的预测建模来减少 no-shows 的影响。然而，现有的机器学习方法（如逻辑回归、随机森林和决策树）依赖于硬决策分裂和静态特征重要性，难以适应特定或复杂的患者行为。

**方法:** 提出了一种新的混合模型 Multi-Head Attention Soft Random Forest (MHASRF)，将注意力机制集成到随机森林模型中，采用概率软分裂而非硬分裂。MHASRF 模型在不同树之间分配不同的注意力权重，从而关注特定的患者行为。此外，该模型通过两级特征重要性（树级别和注意力机制级别）识别患者未出席的关键预测因素。

**结果:** MHASRF 模型表现出 93.56% 的准确率、93.67% 的精确率、93.56% 的召回率和 93.59% 的 F1 分数，优于决策树、逻辑回归、随机森林和朴素贝叶斯模型。同时，该模型能够识别患者未出席的关键预测因素，为优化医疗资源提供了帮助。

**结论:** MHASRF 是一种强大、灵活且可解释的方法，适用于预测患者未出席预约的情况，有助于医疗提供者优化资源分配。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Multi-Head+Attention+Soft+Random+Forest+for+Interpretable+Patient+No-Show+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17344&send_immediately=true&force_search=false)

**原文摘要:** Unattended scheduled appointments, defined as patient no-shows, adversely
affect both healthcare providers and patients' health, disrupting the
continuity of care, operational efficiency, and the efficient allocation of
medical resources. Accurate predictive modelling is needed to reduce the impact
of no-shows. Although machine learning methods, such as logistic regression,
random forest models, and decision trees, are widely used in predicting patient
no-shows, they often rely on hard decision splits and static feature
importance, limiting their adaptability to specific or complex patient
behaviors. To address this limitation, we propose a new hybrid Multi-Head
Attention Soft Random Forest (MHASRF) model that integrates attention
mechanisms into a random forest model using probabilistic soft splitting
instead of hard splitting. The MHASRF model assigns attention weights
differently across the trees, enabling attention on specific patient behaviors.
The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a
93.59% F1 score, surpassing the performance of decision tree, logistic
regression, random forest, and naive Bayes models. Furthermore, MHASRF was able
to identify key predictors of patient no-shows using two levels of feature
importance (tree level and attention mechanism level), offering deeper insights
into patient no-show predictors. The proposed model is a robust, adaptable, and
interpretable method for predicting patient no-shows that will help healthcare
providers in optimizing resources.

</details>


### [35] [Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions](https://arxiv.org/abs/2505.18046)
*Yizhou Xu, Florent Krzakala, Lenka Zdeborová*

**主要类别:** cs.LG

**概要:** 本论文研究了受限玻尔兹曼机（RBM）在大数据维度和固定隐单元数量情况下的训练性能，将其转化为多指数模型，并通过AMP、GD等方法分析其训练动态，证明RBM在尖刺协方差模型中达到了最优的计算弱恢复阈值。


<details>
  <summary>更多</summary>
  
**动机:** 尽管RBM结构简单，但对其从训练数据中学习性能的理解仅限于特定场景，如数据的奇异值分解。因此，研究者希望在输入空间维度较大且隐单元数量固定的情况下，深入分析RBM的训练过程和性能。

**方法:** 作者将RBM的训练目标简化为与多指数模型等价的形式，并引入非可分离正则化。利用适用于多指数模型的方法（如AMP及其状态演化以及GD的动态平均场理论），对RBM的训练动态进行分析。最后，在尖刺协方差模型上评估RBM的训练表现。

**结果:** 研究表明，RBM在尖刺协方差模型中能够达到最优的计算弱恢复阈值，这一结果与BBP相变一致，表明RBM在无监督学习中有良好的适用性。

**结论:** RBM在高维输入空间和固定隐单元数量情况下，可以通过转化为多指数模型来有效分析其训练动态。实验结果表明，RBM在尖刺协方差模型中能够实现最优的计算弱恢复，验证了其在无监督学习任务中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+with+Restricted+Boltzmann+Machines%3A+Asymptotics+of+AMP+and+GD+in+High+Dimensions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18046&send_immediately=true&force_search=false)

**原文摘要:** The Restricted Boltzmann Machine (RBM) is one of the simplest generative
neural networks capable of learning input distributions. Despite its
simplicity, the analysis of its performance in learning from the training data
is only well understood in cases that essentially reduce to singular value
decomposition of the data. Here, we consider the limit of a large dimension of
the input space and a constant number of hidden units. In this limit, we
simplify the standard RBM training objective into a form that is equivalent to
the multi-index model with non-separable regularization. This opens a path to
analyze training of the RBM using methods that are established for multi-index
models, such as Approximate Message Passing (AMP) and its state evolution, and
the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We
then give rigorous asymptotics of the training dynamics of RBM on data
generated by the spiked covariance model as a prototype of a structure suitable
for unsupervised learning. We show in particular that RBM reaches the optimal
computational weak recovery threshold, aligning with the BBP transition, in the
spiked covariance model.

</details>


### [36] [FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems](https://arxiv.org/abs/2505.17351)
*N. Benjamin Erichson, Vinicius Mikuni, Dongwei Lyu, Yang Gao, Omri Azencot, Soon Hoe Lim, Michael W. Mahoney*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为FLEX的骨干架构，用于使用扩散模型对时空物理系统进行生成建模。FLEX在残差空间中操作，而不是原始数据上，从而降低了扩散模型中速度场的方差，有助于稳定训练。通过结合潜在Transformer和U-Net设计，FLEX能够捕捉局部空间细节和潜在空间中的长距离依赖关系。实验结果表明，FLEX在高分辨率2D湍流数据上的表现优于强大的基线模型，并且可以推广到分布外设置，包括未见过的雷诺数、物理可观测值和边界条件。


<details>
  <summary>更多</summary>
  
**动机:** 当前生成模型在处理复杂的时空物理系统时面临挑战，特别是在捕捉局部空间细节和长距离依赖关系方面。因此，需要一种新的方法来提高生成模型的稳定性和准确性。

**方法:** FLEX在残差空间中操作，降低速度场的方差以稳定训练；整合了潜在Transformer到U-Net中，包含标准卷积ResNet层并重新设计了跳过连接方案；使用特定任务的编码器处理辅助输入（如粗略或过去的快照），并通过跳过连接应用弱条件化以促进泛化，同时通过跳过和瓶颈特征在解码器中应用强条件化以确保重建保真度。

**结果:** FLEX能够在仅使用两个反向扩散步骤的情况下实现超分辨率和预测任务的准确预测，并通过采样产生校准的不确定性估计。在高分辨率2D湍流数据上的评估显示，FLEX优于强大的基线模型，并且可以推广到分布外设置，包括未见过的雷诺数、物理可观测值和边界条件。

**结论:** FLEX是一种有效的骨干架构，用于生成时空物理系统的建模。它在捕捉局部空间细节和长距离依赖关系方面表现出色，同时提高了训练的稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FLEX%3A+A+Backbone+for+Diffusion-Based+Modeling+of+Spatio-temporal+Physical+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17351，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17351&send_immediately=true&force_search=false)

**原文摘要:** We introduce FLEX (FLow EXpert), a backbone architecture for generative
modeling of spatio-temporal physical systems using diffusion models. FLEX
operates in the residual space rather than on raw data, a modeling choice that
we motivate theoretically, showing that it reduces the variance of the velocity
field in the diffusion model, which helps stabilize training. FLEX integrates a
latent Transformer into a U-Net with standard convolutional ResNet layers and
incorporates a redesigned skip connection scheme. This hybrid design enables
the model to capture both local spatial detail and long-range dependencies in
latent space. To improve spatio-temporal conditioning, FLEX uses a
task-specific encoder that processes auxiliary inputs such as coarse or past
snapshots. Weak conditioning is applied to the shared encoder via skip
connections to promote generalization, while strong conditioning is applied to
the decoder through both skip and bottleneck features to ensure reconstruction
fidelity. FLEX achieves accurate predictions for super-resolution and
forecasting tasks using as few as two reverse diffusion steps. It also produces
calibrated uncertainty estimates through sampling. Evaluations on
high-resolution 2D turbulence data show that FLEX outperforms strong baselines
and generalizes to out-of-distribution settings, including unseen Reynolds
numbers, physical observables (e.g., fluid flow velocity fields), and boundary
conditions.

</details>


### [37] [Asymptotically optimal regret in communicating Markov decision processes](https://arxiv.org/abs/2505.18064)
*Victor Boone*

**主要类别:** cs.LG

**概要:** 本论文提出了一种学习算法，针对在平均奖励下的马尔可夫决策过程（MDP），在通信假设下实现了渐近最优的后悔值。该算法通过明确跟踪常数K(M)来实现最优学习，并平衡探索、协同探索和利用之间的权衡。此外，由于K(M)函数具有不连续性，作者引入了正则化机制以从经验数据中以任意精度估计K(M)。


<details>
  <summary>更多</summary>
  
**动机:** 在马尔可夫决策过程中，现有的学习算法未能在平均奖励设定下达到渐近最优的后悔值，尤其是在通信假设条件下。因此，需要一种新的方法来优化学习过程并克服与常数K(M)相关的挑战。

**方法:** 该算法通过追踪常数K(M)，并结合探索、协同探索和利用三者之间的权衡进行优化。同时，为了解决K(M)函数的不连续性问题，作者提出了一个正则化机制用于从经验数据中估计K(M)。

**结果:** 所提出的算法在给定的通信马尔可夫决策过程中实现了渐近最优的后悔值，其形式为K(M)log(T)+o(log(T))，其中T为学习步骤的数量，而K(M)为可能的最佳常数。

**结论:** 本研究成功开发了一种适用于通信假设下的马尔可夫决策过程的学习算法，能够实现渐近最优的后悔值。通过引入正则化机制，解决了K(M)函数不连续性的难题，从而提高了算法的性能和适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Asymptotically+optimal+regret+in+communicating+Markov+decision+processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18064，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18064&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we present a learning algorithm that achieves asymptotically
optimal regret for Markov decision processes in average reward under a
communicating assumption. That is, given a communicating Markov decision
process $M$, our algorithm has regret $K(M) \log(T) + \mathrm{o}(\log(T))$
where $T$ is the number of learning steps and $K(M)$ is the best possible
constant. This algorithm works by explicitly tracking the constant $K(M)$ to
learn optimally, then balances the trade-off between exploration (playing
sub-optimally to gain information), co-exploration (playing optimally to gain
information) and exploitation (playing optimally to score maximally). We
further show that the function $K(M)$ is discontinuous, which is a consequence
challenge for our approach. To that end, we describe a regularization mechanism
to estimate $K(M)$ with arbitrary precision from empirical data.

</details>


### [38] [Generative Distribution Embeddings](https://arxiv.org/abs/2505.18150)
*Nic Fishman, Gokul Gowri, Peng Yin, Jonathan Gootenberg, Omar Abudayyeh*

**主要类别:** cs.LG

**概要:** 生成分布嵌入（GDE）是一种提升自编码器到分布空间的框架，能够学习表示分布的表示，并在计算生物学等多个任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 许多现实问题需要跨多尺度推理，要求模型不仅作用于单个数据点，而是整个分布。

**方法:** 引入了生成分布嵌入（GDE），将自编码器提升到分布空间。编码器作用于样本集，解码器被替换为一个生成器，目标是匹配输入分布。通过耦合条件生成模型与满足分布不变性准则的编码网络来学习分布的表示。

**结果:** GDEs 学习嵌入在 Wasserstein 空间中的预测充分统计量，潜在 GDE 距离大约恢复 $W_2$ 距离，潜在插值大约恢复高斯和高斯混合分布的最佳传输轨迹。在合成数据集上系统地基准测试表明性能更强。在计算生物学的六个关键问题上应用 GDEs 取得了成功。

**结论:** GDE 框架为学习分布的表示提供了一种有效方法，并在多个领域特别是计算生物学中展示出强大性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+Distribution+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18150，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18150&send_immediately=true&force_search=false)

**原文摘要:** Many real-world problems require reasoning across multiple scales, demanding
models which operate not on single data points, but on entire distributions. We
introduce generative distribution embeddings (GDE), a framework that lifts
autoencoders to the space of distributions. In GDEs, an encoder acts on sets of
samples, and the decoder is replaced by a generator which aims to match the
input distribution. This framework enables learning representations of
distributions by coupling conditional generative models with encoder networks
which satisfy a criterion we call distributional invariance. We show that GDEs
learn predictive sufficient statistics embedded in the Wasserstein space, such
that latent GDE distances approximately recover the $W_2$ distance, and latent
interpolation approximately recovers optimal transport trajectories for
Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs
against existing approaches on synthetic datasets, demonstrating consistently
stronger performance. We then apply GDEs to six key problems in computational
biology: learning representations of cell populations from lineage-tracing data
(150K cells), predicting perturbation effects on single-cell transcriptomes (1M
cells), predicting perturbation effects on cellular phenotypes (20M single-cell
images), modeling tissue-specific DNA methylation patterns (253M sequences),
designing synthetic yeast promoters (34M sequences), and spatiotemporal
modeling of viral protein sequences (1M sequences).

</details>


### [39] [Adversarial Robustness of Nonparametric Regression](https://arxiv.org/abs/2505.17356)
*Parsa Moradi, Hanzaleh Akabrinodehi, Mohammad Ali Maddah-Ali*

**主要类别:** cs.LG

**概要:** 本文研究了在对抗性噪声下的非参数回归问题，并揭示了平滑样条估计器的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管参数化回归的鲁棒性已被广泛研究，但其非参数化对应部分仍然未被深入探索。本文旨在填补这一空白，通过研究假设回归函数属于二阶Sobolev空间的非参数回归的对抗性鲁棒性。

**方法:** 作者提出了两个主要贡献：(i)建立了估计误差的极小极大下界，揭示了任何估计器都无法克服的基本限制；(ii)展示了经过适当正则化的经典平滑样条估计器具有对对抗性污染的鲁棒性。

**结果:** 结果表明，如果 n 个样本中有 o(n) 被破坏，则平滑样条的估计误差会随着 n 趋向无穷而消失。然而，当一定比例的数据被破坏时，没有任何估计器可以保证估计误差会消失，这说明了平滑样条在最大可容忍破坏样本数方面的最优性。

**结论:** 本文通过建立估计误差的理论下界和展示平滑样条估计器的鲁棒性，进一步加深了对非参数回归中对抗性鲁棒性的理解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Robustness+of+Nonparametric+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17356，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17356&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we investigate the adversarial robustness of regression, a
fundamental problem in machine learning, under the setting where an adversary
can arbitrarily corrupt a subset of the input data. While the robustness of
parametric regression has been extensively studied, its nonparametric
counterpart remains largely unexplored. We characterize the adversarial
robustness in nonparametric regression, assuming the regression function
belongs to the second-order Sobolev space (i.e., it is square integrable up to
its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower
bound on the estimation error, revealing a fundamental limit that no estimator
can overcome, and (ii) we show that, perhaps surprisingly, the classical
smoothing spline estimator, when properly regularized, exhibits robustness
against adversarial corruption. These results imply that if $o(n)$ out of $n$
samples are corrupted, the estimation error of the smoothing spline vanishes as
$n \to \infty$. On the other hand, when a constant fraction of the data is
corrupted, no estimator can guarantee vanishing estimation error, implying the
optimality of the smoothing spline in terms of maximum tolerable number of
corrupted samples.

</details>


### [40] [Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction](https://arxiv.org/abs/2505.17357)
*Hassan Wasswa, Hussein Abbass, Timothy Lynar*

**主要类别:** cs.LG

**概要:** 本研究提出了一种框架，用于将NetFlow-based IoT攻击数据集降维并转化为图结构数据集，然后评估了三种降维技术（VAE-encoder、AE-encoder和PCA）对GAT模型在检测IoT僵尸网络攻击中的效果。


<details>
  <summary>更多</summary>
  
**动机:** 随着基于IoT的僵尸网络攻击的兴起，研究人员探索了各种学习模型进行检测，但大多数模型忽略了实例间的关联性。虽然注意力机制和图神经网络（GNN）已经改善了检测性能，但在将高维IoT攻击数据集转换为图结构数据集时仍面临计算开销大的问题。

**方法:** 首先使用降维技术（VAE-encoder、AE-encoder和PCA）对NetFlow-based IoT攻击数据集进行降维处理，然后将其转化为图结构数据集，并利用图注意力神经网络（GAT）模型进行僵尸网络攻击检测。

**结果:** 通过比较不同降维技术的效果，结果表明降维后的数据集在GAT模型上的表现更优，有助于提高僵尸网络攻击检测的准确性。

**结论:** 提出的框架有效缓解了图结构数据集的计算开销问题，同时提升了僵尸网络攻击检测的性能，未来可进一步优化降维技术和GAT模型的应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Attention+Neural+Network+for+Botnet+Detection%3A+Evaluating+Autoencoder%2C+VAE+and+PCA-Based+Dimension+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17357，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17357&send_immediately=true&force_search=false)

**原文摘要:** With the rise of IoT-based botnet attacks, researchers have explored various
learning models for detection, including traditional machine learning, deep
learning, and hybrid approaches. A key advancement involves deploying attention
mechanisms to capture long-term dependencies among features, significantly
improving detection accuracy. However, most models treat attack instances
independently, overlooking inter-instance relationships. Graph Neural Networks
(GNNs) address this limitation by learning an embedding space via iterative
message passing where similar instances are placed closer based on node
features and relationships, enhancing classification performance. To further
improve detection, attention mechanisms have been embedded within GNNs,
leveraging both long-range dependencies and inter-instance connections.
However, transforming the high dimensional IoT attack datasets into a graph
structured dataset poses challenges, such as large graph structures leading
computational overhead. To mitigate this, this paper proposes a framework that
first reduces dimensionality of the NetFlow-based IoT attack dataset before
transforming it into a graph dataset. We evaluate three dimension reduction
techniques--Variational Autoencoder (VAE-encoder), classical autoencoder
(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects
on a Graph Attention neural network (GAT) model for botnet attack detection

</details>


### [41] [Towards VM Rescheduling Optimization Through Deep Reinforcement Learning](https://arxiv.org/abs/2505.17359)
*Xianzhong Ding, Yunkai Zhang, Binbin Chen, Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, Wan Du*

**主要类别:** cs.LG

**概要:** 现代数据中心需要管理大量虚拟机（VM），由于VM的持续创建和释放，物理机上会散落许多小资源碎片。为解决此问题，数据中心定期重新调度VM到其他物理机。然而，现有的VM重调度方法因推理时间显著影响性能而扩展性差。因此，我们提出了基于强化学习的VM重调度系统VM2RL，其具有定制技术如两阶段框架、特征提取模块及风险寻求评估。实验表明，VM2RL性能接近最优解且运行时间仅需数秒。


<details>
  <summary>更多</summary>
  
**动机:** 随着数据中心规模扩大，虚拟机重调度变得越来越重要，但该问题尚未得到充分研究。现有方法因无法适应动态变化的虚拟机状态而导致扩展性差。

**方法:** 提出了一种基于强化学习的虚拟机重调度系统VM2RL，包括：1）两阶段框架以适应多种约束和工作负载条件；2）特征提取模块捕捉与重调度相关的特定关系信息；3）风险寻求评估允许用户优化延迟和准确率之间的权衡。

**结果:** 通过使用来自大规模工业数据中心的数据进行广泛实验，结果表明VM2RL能够实现接近最优解的性能，同时运行时间仅为几秒钟。

**结论:** VM2RL是一种有效的强化学习系统，适用于虚拟机重调度问题，其性能优越且运行效率高，开源代码和数据集可供进一步研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+VM+Rescheduling+Optimization+Through+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17359，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17359&send_immediately=true&force_search=false)

**原文摘要:** Modern industry-scale data centers need to manage a large number of virtual
machines (VMs). Due to the continual creation and release of VMs, many small
resource fragments are scattered across physical machines (PMs). To handle
these fragments, data centers periodically reschedule some VMs to alternative
PMs, a practice commonly referred to as VM rescheduling. Despite the increasing
importance of VM rescheduling as data centers grow in size, the problem remains
understudied. We first show that, unlike most combinatorial optimization tasks,
the inference time of VM rescheduling algorithms significantly influences their
performance, due to dynamic VM state changes during this period. This causes
existing methods to scale poorly. Therefore, we develop a reinforcement
learning system for VM rescheduling, VM2RL, which incorporates a set of
customized techniques, such as a two-stage framework that accommodates diverse
constraints and workload conditions, a feature extraction module that captures
relational information specific to rescheduling, as well as a risk-seeking
evaluation enabling users to optimize the trade-off between latency and
accuracy. We conduct extensive experiments with data from an industry-scale
data center. Our results show that VM2RL can achieve a performance comparable
to the optimal solution but with a running time of seconds. Code and datasets
are open-sourced: https://github.com/zhykoties/VMR2L_eurosys,
https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.

</details>


### [42] [Improved and Oracle-Efficient Online $\ell_1$-Multicalibration](https://arxiv.org/abs/2505.17365)
*Rohan Ghuge, Vidya Muthukumar, Sahil Singla*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+and+Oracle-Efficient+Online+%24%5Cell_1%24-Multicalibration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17365，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17365&send_immediately=true&force_search=false)

**原文摘要:** We study \emph{online multicalibration}, a framework for ensuring calibrated
predictions across multiple groups in adversarial settings, across $T$ rounds.
Although online calibration is typically studied in the $\ell_1$ norm, prior
approaches to online multicalibration have taken the indirect approach of
obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then
transferred these guarantees to $\ell_1$ at additional loss. In contrast, we
propose a direct method that achieves improved and oracle-efficient rates of
$\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$
respectively, for online $\ell_1$-multicalibration. Our key insight is a novel
reduction of online \(\ell_1\)-multicalibration to an online learning problem
with product-based rewards, which we refer to as \emph{online linear-product
optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we
introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm
for this linearized problem. Although this method guarantees the desired
sublinear rate (nearly matching the best rate for online calibration), it
becomes computationally expensive when the group family \(\mathcal{H}\) is
large or infinite, since it enumerates all possible groups. To address
scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a
polynomial number of calls to an offline optimization (\emph{multicalibration
evaluation}) oracle, resulting in \emph{oracle-efficient} online
\(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$.
Our framework also extends to certain infinite families of groups (e.g., all
linear functions on the context space) by exploiting a $1$-Lipschitz property
of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).

</details>


### [43] [FRIREN: Beyond Trajectories -- A Spectral Lens on Time](https://arxiv.org/abs/2505.17370)
*Qilin Wang*

**主要类别:** cs.LG

**概要:** 论文提出了一种名为FRIREN的新模型，该模型通过将数据嵌入正态分布的潜在表示，并生成W2-高效的最优路径，实现了长期时间序列预测（LTSF）中几何结构保持的预测。在Lorenz-63和Rossler系统上，FRIREN显著优于TimeMixer，并且在标准LTSF数据集如ETT和Weather上也具有竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的长期时间序列预测模型通常假设所有数据都是逐点可预测的，但这一假设在面对混沌系统时可能失效。因此，需要一种动态无关的基础模型，能够捕捉几何结构而非仅依赖逐点预测。

**方法:** FRIREN模型使用增强的归一化流块将数据嵌入正态分布的潜在表示中，然后生成一个W2-高效的最优路径，该路径可以分解为旋转、缩放、逆旋转和平移。此外，模型提供了一个全局谱表示，作为有限Koopman算子的小修改版本，用于分析局部和全局模式的增长、衰减或振荡。

**结果:** 在Lorenz-63系统中，FRIREN的MSE为11.4，MAE为1.6，SWD为0.96，显著优于TimeMixer。在Rossler系统中，FRIREN同样表现出色，MSE为0.0349，MAE为0.0953，SWD为0.0170。此外，FRIREN在标准LTSF数据集上也具有竞争力。

**结论:** FRIREN通过结合现代生成流与经典谱分析，提高了长期预测的准确性和可解释性，为LTSF模型设计设定了新的基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FRIREN%3A+Beyond+Trajectories+--+A+Spectral+Lens+on+Time，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17370，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17370&send_immediately=true&force_search=false)

**原文摘要:** Long-term time-series forecasting (LTSF) models are often presented as
general-purpose solutions that can be applied across domains, implicitly
assuming that all data is pointwise predictable. Using chaotic systems such as
Lorenz-63 as a case study, we argue that geometric structure - not pointwise
prediction - is the right abstraction for a dynamic-agnostic foundational
model. Minimizing the Wasserstein-2 distance (W2), which captures geometric
changes, and providing a spectral view of dynamics are essential for
long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via
Interpretable Eigen-networks), implements an augmented normalizing-flow block
that embeds data into a normally distributed latent representation. It then
generates a W2-efficient optimal path that can be decomposed into rotation,
scaling, inverse rotation, and translation. This architecture yields locally
generated, geometry-preserving predictions that are independent of the
underlying dynamics, and a global spectral representation that functions as a
finite Koopman operator with a small modification. This enables practitioners
to identify which modes grow, decay, or oscillate, both locally and
system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on
Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE
27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out
of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),
FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,
outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.
FRIREN is also competitive on standard LTSF datasets such as ETT and Weather.
By connecting modern generative flows with classical spectral analysis, FRIREN
makes long-term forecasting both accurate and interpretable, setting a new
benchmark for LTSF model design.

</details>


### [44] [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
*Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar*

**主要类别:** cs.LG

**概要:** 儿童识字能力对其日后生活结果有很强的预测作用。为了缩小低收入和中等收入地区与高收入地区之间的识字水平差距，需要针对弱势群体进行干预。阅读评估是衡量这些项目效果的重要工具，而AI可以成为支持教育工作者完成此任务的一种可靠且经济实惠的工具。本研究专注于南非使用的Xhosa语言，推进儿童语音识别能力，并提出一个由儿童Xhosa语音样本组成的新数据集。该数据集包含EGRA系统中的十个单词和字母。每个录音都通过在线且经济有效的方法由多位评分员标记，并由独立的EGRA评审员验证子样本。使用三个最先进的端到端模型（wav2vec 2.0、HuBERT和Whisper）对该数据集进行了评估。结果表明，这些模型的表现会受到可用训练数据量及其平衡性的影响。实验还表明，即使在样本数量有限的情况下，同时对多个类别进行训练也可以提高wav2vec 2.0的性能。


<details>
  <summary>更多</summary>
  
**动机:** 儿童识字能力对个体未来的生活结果具有重要影响。然而，低资源语言中的儿童语音自动阅读评估系统的开发面临诸多挑战，包括数据有限以及儿童声音的独特声学特性。因此，研究者希望改进儿童语音识别能力，特别是在低资源语言环境下。

**方法:** 研究者以Xhosa语言为对象，创建了一个包含儿童语音样本的新数据集。这些样本来自EGRA系统中的十个单词和字母。录音通过在线且经济有效的方式由多位评分员标记，并由独立的EGRA评审员验证部分样本。然后，使用三个先进的端到端模型（wav2vec 2.0、HuBERT和Whisper）对该数据集进行评估。

**结果:** 结果显示，模型的表现受训练数据量和平衡性的影响。此外，wav2vec 2.0在多类别同时训练时表现更好，即便样本数量有限也是如此。

**结论:** 开发准确的自动阅读评估系统对于提升儿童语音识别能力至关重要，尤其是在低资源语言环境中。数据量和平衡性对模型性能有显著影响，同时多类别训练可提高模型表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+End-to-End+Approach+for+Child+Reading+Assessment+in+the+Xhosa+Language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17371，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17371&send_immediately=true&force_search=false)

**原文摘要:** Child literacy is a strong predictor of life outcomes at the subsequent
stages of an individual's life. This points to a need for targeted
interventions in vulnerable low and middle income populations to help bridge
the gap between literacy levels in these regions and high income ones. In this
effort, reading assessments provide an important tool to measure the
effectiveness of these programs and AI can be a reliable and economical tool to
support educators with this task. Developing accurate automatic reading
assessment systems for child speech in low-resource languages poses significant
challenges due to limited data and the unique acoustic properties of children's
voices. This study focuses on Xhosa, a language spoken in South Africa, to
advance child speech recognition capabilities. We present a novel dataset
composed of child speech samples in Xhosa. The dataset is available upon
request and contains ten words and letters, which are part of the Early Grade
Reading Assessment (EGRA) system. Each recording is labeled with an online and
cost-effective approach by multiple markers and a subsample is validated by an
independent EGRA reviewer. This dataset is evaluated with three fine-tuned
state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The
results indicate that the performance of these models can be significantly
influenced by the amount and balancing of the available training data, which is
fundamental for cost-effective large dataset collection. Furthermore, our
experiments indicate that the wav2vec 2.0 performance is improved by training
on multiple classes at a time, even when the number of available samples is
constrained.

</details>


### [45] [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.17373)
*Kaiwen Wang, Jin Peng Zhou, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kianté Brantley, Wen Sun*

**主要类别:** cs.LG

**概要:** 本文提出了一种简单高效的价值模型训练方法，用于长上下文推理轨迹。通过收集250万条推理轨迹的数据集，训练了一个15亿参数的基于token级价值模型，并将其应用于DeepSeek模型以提高性能。实验表明，块级价值引导搜索（VGS）结合最终加权多数投票在测试时的表现优于标准方法如多数投票或最佳n选择。使用64次生成的推理预算，VGS与DeepSeek-R1-Distill-1.5B结合，在四个数学竞赛基准上达到了45.7%的平均准确率，与o3-mini-medium持平。此外，VGS显著减少了实现与多数投票相同性能所需的推理FLOPs。本文的数据集、模型和代码已开源。


<details>
  <summary>更多</summary>
  
**动机:** 现有的过程奖励模型（PRMs）需要细粒度的“步骤”概念，这在长上下文推理模型中难以定义。因此，需要一种新的方法来避免这种限制，同时提升模型在推理任务中的表现。

**方法:** 提出了一种无需细粒度“步骤”概念的价值模型训练方法。通过构建包含250万条推理轨迹的数据集，训练了一个15亿参数的基于token级的价值模型。该模型被应用到DeepSeek模型中，并采用块级价值引导搜索（VGS）结合最终加权多数投票策略进行推理。

**结果:** 块级价值引导搜索（VGS）在测试时的表现优于多数投票或最佳n选择等标准方法。使用64次生成的推理预算，VGS与DeepSeek-R1-Distill-1.5B结合，在四个数学竞赛基准上的平均准确率达到45.7%，与o3-mini-medium持平。此外，VGS显著减少了实现与多数投票相同性能所需的推理FLOPs。

**结论:** 所提出的方法在长上下文推理任务中表现出色，能够有效提升DeepSeek模型的性能，同时减少计算成本。数据集、模型和代码已开源，为未来的研究提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Value-Guided+Search+for+Efficient+Chain-of-Thought+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17373，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17373&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of "step,"
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.

</details>


### [46] [Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition](https://arxiv.org/abs/2505.17379)
*Zichen Wang, Chuanhao Li, Huazheng Wang*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provably+Efficient+Algorithm+for+Best+Scoring+Rule+Identification+in+Online+Principal-Agent+Information+Acquisition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17379，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17379&send_immediately=true&force_search=false)

**原文摘要:** We investigate the problem of identifying the optimal scoring rule within the
principal-agent framework for online information acquisition problem. We focus
on the principal's perspective, seeking to determine the desired scoring rule
through interactions with the agent. To address this challenge, we propose two
algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget
settings, respectively. Our theoretical analysis demonstrates that OIAFC can
extract the desired $(\epsilon, \delta)$-scoring rule with a efficient
instance-dependent sample complexity or an instance-independent sample
complexity. Our analysis also shows that OIAFB matches the instance-independent
performance bound of OIAFC, while both algorithms share the same complexity
across fixed confidence and fixed budget settings.

</details>


### [47] [Spectral Mixture Kernels for Bayesian Optimization](https://arxiv.org/abs/2505.17393)
*Yi Zhang, Cheng Hua*

**主要类别:** cs.LG

**概要:** 提出了一种基于高斯过程的贝叶斯优化方法，该方法结合了光谱混合核，提高了优化效率和性能。


<details>
  <summary>更多</summary>
  
**动机:** 选择合适的概率代理模型是贝叶斯优化中的重要问题。

**方法:** 引入了一种新的基于高斯过程（GP）的贝叶斯优化方法，该方法结合了从柯西分布和高斯分布的比例位置混合形成的光谱密度中得出的光谱混合核。

**结果:** 在各种合成和真实世界的问题上，该方法的表现始终优于现有的基线方法，包括低维和高维设置。

**结论:** 该方法在计算速度上与简单内核匹配，同时在性能上超过更复杂的模型和自动化的贝叶斯优化方法，并提供了关于获得最优值的信息增益和累积后悔的界限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spectral+Mixture+Kernels+for+Bayesian+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17393，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17393&send_immediately=true&force_search=false)

**原文摘要:** Bayesian Optimization (BO) is a widely used approach for solving expensive
black-box optimization tasks. However, selecting an appropriate probabilistic
surrogate model remains an important yet challenging problem. In this work, we
introduce a novel Gaussian Process (GP)-based BO method that incorporates
spectral mixture kernels, derived from spectral densities formed by
scale-location mixtures of Cauchy and Gaussian distributions. This method
achieves a significant improvement in both efficiency and optimization
performance, matching the computational speed of simpler kernels while
delivering results that outperform more complex models and automatic BO
methods. We provide bounds on the information gain and cumulative regret
associated with obtaining the optimum. Extensive numerical experiments
demonstrate that our method consistently outperforms existing baselines across
a diverse range of synthetic and real-world problems, including both low- and
high-dimensional settings.

</details>


### [48] [Wasserstein Transfer Learning](https://arxiv.org/abs/2505.17404)
*Kaicheng Zhang, Sinian Zhang, Doudou Zhou, Yidong Zhou*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的迁移学习框架，适用于回归模型中的概率分布输出。当已知可迁移的源域子集时，提出了一个具有可证明渐近收敛率的估计器；当未知时，开发了数据驱动的迁移学习程序以减少负迁移。方法通过理论分析、模拟和实际应用验证。


<details>
  <summary>更多</summary>
  
**动机:** 传统的迁移学习方法主要针对欧几里得空间中的标量或多变量数据，难以处理复杂的数据结构如概率分布。因此需要一种新的迁移学习框架来解决这一问题。

**方法:** 引入一个新的迁移学习框架，用于回归模型中Wasserstein空间的概率分布输出。提出两种方法：1) 当已知可迁移的源域子集时，提出一个具有渐近收敛率的估计器；2) 当未知时，开发一个数据驱动的迁移学习程序以减少负迁移。

**结果:** 所提出的方法通过严格的理论分析得到支持，并通过广泛的模拟和实际应用验证了其有效性。

**结论:** 该研究为概率分布输出的迁移学习提供了一个有效的框架，能够根据领域相似性提高迁移效率并减少负迁移的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wasserstein+Transfer+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17404，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17404&send_immediately=true&force_search=false)

**原文摘要:** Transfer learning is a powerful paradigm for leveraging knowledge from source
domains to enhance learning in a target domain. However, traditional transfer
learning approaches often focus on scalar or multivariate data within Euclidean
spaces, limiting their applicability to complex data structures such as
probability distributions. To address this, we introduce a novel framework for
transfer learning in regression models, where outputs are probability
distributions residing in the Wasserstein space. When the informative subset of
transferable source domains is known, we propose an estimator with provable
asymptotic convergence rates, quantifying the impact of domain similarity on
transfer efficiency. For cases where the informative subset is unknown, we
develop a data-driven transfer learning procedure designed to mitigate negative
transfer. The proposed methods are supported by rigorous theoretical analysis
and are validated through extensive simulations and real-world applications.

</details>


### [49] [HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.17431)
*Boyuan Li, Yicheng Luo, Zhen Liu, Junhao Zheng, Jianming Lv, Qianli Ma*

**主要类别:** cs.LG

**概要:** 提出了一种名为HyperIMTS的超图神经网络，用于不规则多变量时间序列预测。该方法通过将观测值转换为超图中的节点，并通过时间和变量超边进行连接，实现信息传递。实验表明，HyperIMTS在低计算成本下具有竞争力的性能。


<details>
  <summary>更多</summary>
  
**动机:** 不规则多变量时间序列（IMTS）由于变量内的时间间隔不规则和变量间未对齐的观测值，学习时间和变量依赖性面临挑战。现有的模型要么需要填充样本以分别从时间和变量维度学习，影响效率和原始采样模式；要么通过二分图或集合表示原始样本，在捕捉未对齐观测值之间的依赖性方面存在局限性。

**方法:** 提出了一种超图神经网络HyperIMTS，用于IMTS预测。将观测值转化为超图中的节点，通过时间和变量超边连接所有观测值，实现信息传递。通过考虑不规则性的消息传递，HyperIMTS以时间适应的方式捕捉变量依赖性。

**结果:** 实验证明了HyperIMTS在IMTS预测中具有与现有最先进模型竞争的性能，同时计算成本较低。

**结论:** HyperIMTS提供了一种统一的形式来表示和学习原始观测值的时间和变量依赖性，克服了现有方法的缺点，具有高效性和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HyperIMTS%3A+Hypergraph+Neural+Network+for+Irregular+Multivariate+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17431，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17431&send_immediately=true&force_search=false)

**原文摘要:** Irregular multivariate time series (IMTS) are characterized by irregular time
intervals within variables and unaligned observations across variables, posing
challenges in learning temporal and variable dependencies. Many existing IMTS
models either require padded samples to learn separately from temporal and
variable dimensions, or represent original samples via bipartite graphs or
sets. However, the former approaches often need to handle extra padding values
affecting efficiency and disrupting original sampling patterns, while the
latter ones have limitations in capturing dependencies among unaligned
observations. To represent and learn both dependencies from original
observations in a unified form, we propose HyperIMTS, a Hypergraph neural
network for Irregular Multivariate Time Series forecasting. Observed values are
converted as nodes in the hypergraph, interconnected by temporal and variable
hyperedges to enable message passing among all observations. Through
irregularity-aware message passing, HyperIMTS captures variable dependencies in
a time-adaptive way to achieve accurate forecasting. Experiments demonstrate
HyperIMTS's competitive performance among state-of-the-art models in IMTS
forecasting with low computational cost.

</details>


### [50] [Discretization-free Multicalibration through Loss Minimization over Tree Ensembles](https://arxiv.org/abs/2505.17435)
*Hongyi Henry Jin, Zijun Ding, Dung Daniel Ngo, Zhiwei Steven Wu*

**主要类别:** cs.LG

**概要:** 近年来，多校准成为确保预测器在校准丰富且重叠的子群体中的理想学习目标。现有的方法通常通过对预测器的输出空间进行离散化并迭代调整其输出值来实现多校准。然而，这种离散化方法偏离了标准的经验风险最小化（ERM）流程，引入了舍入误差和额外的敏感超参数，并可能以妨碍下游决策的方式扭曲预测器的输出。在本工作中，我们提出了一种无需离散化的多校准方法，该方法直接对深度为二的决策树集合上的经验风险目标进行优化。我们的ERM方法可以使用现成的树集合学习方法（如LightGBM）来实现。只要数据分布满足我们称之为损失饱和的技术条件，我们的算法就可以证明实现多校准。在多个数据集上的实证评估表明，这一条件在实践中始终得到满足。我们的无离散化算法始终与现有的多校准方法相匹配或优于它们——即使在使用与基线共享其离散化粒度的离散化基于的多校准度量进行评估时也是如此。


<details>
  <summary>更多</summary>
  
**动机:** 多校准是一个重要的学习目标，旨在确保预测器在校准丰富且重叠的子群体中表现良好。然而，现有的离散化方法存在诸多问题：偏离标准ERM流程、引入舍入误差和额外敏感超参数、可能妨碍下游决策。因此，需要一种新的方法来克服这些问题。

**方法:** 提出了一种无需离散化的多校准方法，该方法通过直接优化深度为二的决策树集合上的经验风险目标来实现。这种方法可以使用现成的树集合学习方法（如LightGBM）来实现，并且在数据分布满足损失饱和条件时能够实现多校准。

**结果:** 在多个数据集上的实验结果表明，所提出的无离散化算法始终与现有的多校准方法相匹配或优于它们，即使在使用与基线共享其离散化粒度的离散化基于的多校准度量进行评估时也是如此。此外，实证评估显示损失饱和条件在实践中始终得到满足。

**结论:** 本文提出了一种无需离散化的多校准方法，解决了现有方法存在的问题。该方法在实践中表现出色，始终与现有方法相匹配或优于它们，并且可以通过标准ERM流程实现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discretization-free+Multicalibration+through+Loss+Minimization+over+Tree+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17435，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17435&send_immediately=true&force_search=false)

**原文摘要:** In recent years, multicalibration has emerged as a desirable learning
objective for ensuring that a predictor is calibrated across a rich collection
of overlapping subpopulations. Existing approaches typically achieve
multicalibration by discretizing the predictor's output space and iteratively
adjusting its output values. However, this discretization approach departs from
the standard empirical risk minimization (ERM) pipeline, introduces rounding
error and additional sensitive hyperparameter, and may distort the predictor's
outputs in ways that hinder downstream decision-making.
  In this work, we propose a discretization-free multicalibration method that
directly optimizes an empirical risk objective over an ensemble of depth-two
decision trees. Our ERM approach can be implemented using off-the-shelf tree
ensemble learning methods such as LightGBM. Our algorithm provably achieves
multicalibration, provided that the data distribution satisfies a technical
condition we term as loss saturation. Across multiple datasets, our empirical
evaluation shows that this condition is always met in practice. Our
discretization-free algorithm consistently matches or outperforms existing
multicalibration approaches--even when evaluated using a discretization-based
multicalibration metric that shares its discretization granularity with the
baselines.

</details>


### [51] [Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning](https://arxiv.org/abs/2505.17439)
*Weijia Jin*

**主要类别:** cs.LG

**概要:** 此研究利用强化学习中的PPO算法动态设计了一个高效且公平的人道主义供应链，并与启发式算法进行了比较。研究表明，PPO模型始终将平均满意度作为优先事项。


<details>
  <summary>更多</summary>
  
**动机:** 人道主义供应链需要在紧急情况下快速响应并分配资源，确保效率和公平性是关键问题。传统的静态优化方法难以适应动态环境，因此需要一种新的方法来解决这个问题。

**方法:** 本研究采用强化学习中的PPO（近端策略优化）算法来动态设计人道主义供应链，并与启发式算法进行对比分析。

**结果:** 实验结果表明，PPO模型能够更有效地处理动态环境下的资源分配问题，并且始终将平均满意度作为优先事项。

**结论:** 使用PPO算法可以为人道主义供应链提供一个更加高效和公平的解决方案，相较于传统启发式算法具有明显优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Designing+an+efficient+and+equitable+humanitarian+supply+chain+dynamically+via+reinforcement+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17439，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17439&send_immediately=true&force_search=false)

**原文摘要:** This study designs an efficient and equitable humanitarian supply chain
dynamically by using reinforcement learning, PPO, and compared with heuristic
algorithms. This study demonstrates the model of PPO always treats average
satisfaction rate as the priority.

</details>


### [52] [Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning](https://arxiv.org/abs/2505.17448)
*Bhanuka Gamage, Adnan Labib, Aisha Joomun, Chern Hong Lim, KokSheik Wong*

**主要类别:** cs.LG

**概要:** 提出了一种名为BaitRadar的算法，通过结合六个模型对视频的不同属性进行分类判断，平均测试准确率为98%，推理时间小于2秒。


<details>
  <summary>更多</summary>
  
**动机:** 针对YouTube平台上日益严重的标题党问题，用户常因吸引人的标题和缩略图点击视频，但内容与标题不符。

**方法:** 使用深度学习技术，创建了BaitRadar算法，该算法结合六个推理模型（分别关注视频标题、评论、缩略图、标签、统计数据和音频转录）进行最终分类决策，并通过计算多个模型的平均值来提供稳健且准确的输出，即使在数据缺失的情况下也是如此。

**结果:** 在1400个YouTube视频上进行了测试，平均测试准确率达到98%，推理时间不到2秒。

**结论:** BaitRadar算法能够有效识别标题党视频，具有高准确率和快速推理能力，为解决YouTube平台上的标题党问题提供了可行方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Baitradar%3A+A+Multi-Model+Clickbait+Detection+Algorithm+Using+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17448，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17448&send_immediately=true&force_search=false)

**原文摘要:** Following the rising popularity of YouTube, there is an emerging problem on
this platform called clickbait, which provokes users to click on videos using
attractive titles and thumbnails. As a result, users ended up watching a video
that does not have the content as publicized in the title. This issue is
addressed in this study by proposing an algorithm called BaitRadar, which uses
a deep learning technique where six inference models are jointly consulted to
make the final classification decision. These models focus on different
attributes of the video, including title, comments, thumbnail, tags, video
statistics and audio transcript. The final classification is attained by
computing the average of multiple models to provide a robust and accurate
output even in situation where there is missing data. The proposed method is
tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved
with an inference time of less than 2s.

</details>


### [53] [CLIMB: Class-imbalanced Learning Benchmark on Tabular Data](https://arxiv.org/abs/2505.17451)
*Zhining Liu, Zihao Li, Ze Yang, Tianxin Wei, Jian Kang, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong*

**主要类别:** cs.LG

**概要:** 本研究提出了CLIMB，一个针对表格数据类别不平衡学习的全面基准测试平台。它包含73个横跨不同领域和不平衡级别的真实世界数据集，以及29种代表性CIL算法的统一实现。通过大量实验，研究提供了关于方法准确性和效率的实际见解，强调了简单再平衡方法的局限性、集成方法的有效性以及数据质量的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 类别不平衡学习（CIL）在许多现实世界的应用中至关重要，其中少数类代表关键但稀有的结果。然而，目前缺乏一个全面的基准来评估和比较不同的CIL算法。

**方法:** 研究人员开发了一个名为CLIMB的基准测试平台，其中包括73个真实世界的数据集和29种CIL算法的统一实现。该平台基于高质量的开源Python包，具有统一的API设计、详细的文档和严格的代码质量控制。通过广泛的实验，研究者分析了不同算法的准确性和效率。

**结果:** 实验结果表明，简单的再平衡方法存在局限性，集成方法在处理类别不平衡问题上表现出有效性，同时数据质量对模型性能有重要影响。

**结论:** CLIMB为类别不平衡学习提供了一个全面的基准测试平台，有助于研究者更方便地实施和比较不同的CIL算法，推动该领域的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLIMB%3A+Class-imbalanced+Learning+Benchmark+on+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17451&send_immediately=true&force_search=false)

**原文摘要:** Class-imbalanced learning (CIL) on tabular data is important in many
real-world applications where the minority class holds the critical but rare
outcomes. In this paper, we present CLIMB, a comprehensive benchmark for
class-imbalanced learning on tabular data. CLIMB includes 73 real-world
datasets across diverse domains and imbalance levels, along with unified
implementations of 29 representative CIL algorithms. Built on a high-quality
open-source Python package with unified API designs, detailed documentation,
and rigorous code quality controls, CLIMB supports easy implementation and
comparison between different CIL algorithms. Through extensive experiments, we
provide practical insights on method accuracy and efficiency, highlighting the
limitations of naive rebalancing, the effectiveness of ensembles, and the
importance of data quality. Our code, documentation, and examples are available
at https://github.com/ZhiningLiu1998/imbalanced-ensemble.

</details>


### [54] [Self-Training Large Language Models with Confident Reasoning](https://arxiv.org/abs/2505.17454)
*Hyosoon Jang, Yunhui Jang, Sungjae Lee, Jungseul Ok, Sungsoo Ahn*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）在生成推理路径前表现出令人印象深刻的能力，但学习这种推理路径需要昂贵的人工监督。最近的研究探索了自训练方法，利用LLMs自身生成的伪标签来提高推理能力。然而，现有的基于置信度的自训练方法仅关注最终答案的质量，可能忽略推理路径的质量。本文提出了一种新的自训练方法CORE-PO，通过策略优化使LLMs更倾向于高置信度的推理路径。实验表明，与现有方法相比，CORE-PO提高了四个同分布和两个异分布基准测试的输出准确性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在生成推理路径方面表现出色，但学习这些推理路径需要昂贵的人工监督。为了解决这个问题，研究人员开始探索使用自训练方法，其中LLMs自己生成伪标签来改进推理能力。然而，现有的方法仅关注最终答案的质量，而忽略了推理路径的质量，因为即使推理路径不正确，也可能偶然得到正确的答案。因此，需要一种新的方法来关注高质量的推理路径。

**方法:** 本文提出了一个新的自训练方法CORE-PO，该方法通过策略优化使LLMs更倾向于高置信度的推理路径。具体来说，CORE-PO使用推理级置信度来识别高质量的推理路径用于自训练，而不是仅仅依赖最终答案的置信度。这种方法确保了推理路径的质量，而不仅仅是最终答案的质量。

**结果:** 实验结果表明，CORE-PO在四个同分布和两个异分布基准测试上提高了输出的准确性。与现有的自训练方法相比，CORE-PO在这些基准测试中表现更好，证明了其有效性和优越性。

**结论:** 本文提出的CORE-PO方法通过策略优化使LLMs更倾向于高置信度的推理路径，从而提高了推理路径的质量和最终答案的准确性。实验结果证明了该方法的有效性，为未来的研究提供了一个新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Training+Large+Language+Models+with+Confident+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17454，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17454&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown impressive performance by generating
reasoning paths before final answers, but learning such a reasoning path
requires costly human supervision. To address this issue, recent studies have
explored self-training methods that improve reasoning capabilities using
pseudo-labels generated by the LLMs themselves. Among these, confidence-based
self-training fine-tunes LLMs to prefer reasoning paths with high-confidence
answers, where confidence is estimated via majority voting. However, such
methods exclusively focus on the quality of the final answer and may ignore the
quality of the reasoning paths, as even an incorrect reasoning path leads to a
correct answer by chance. Instead, we advocate the use of reasoning-level
confidence to identify high-quality reasoning paths for self-training,
supported by our empirical observations. We then propose a new self-training
method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths
through Policy Optimization. Our experiments show that CORE-PO improves the
accuracy of outputs on four in-distribution and two out-of-distribution
benchmarks, compared to existing self-training methods.

</details>


### [55] [Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation](https://arxiv.org/abs/2505.17458)
*Guiquan Sun, Xikun Zhang, Jingchao Ni, Dongjin Song*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种基于元学习的知识蒸馏框架（MKD），用于解决在动态异构图上的持续学习问题。MKD通过结合元学习和知识蒸馏，能够在不断引入新数据的同时保持已有知识，并通过新颖的采样策略和语义级蒸馏模块提高模型性能。实验验证了MKD在三个基准数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究通常假设异构图是静态的，而现实世界中的图结构是动态变化的，这要求模型在学习新数据时能够避免灾难性遗忘并保持已有知识。因此，需要一种方法来应对动态异构图上的持续学习挑战。

**方法:** 论文提出了Meta-learning based Knowledge Distillation framework (MKD)，该框架包含：1) 元学习以快速适应新任务；2) 知识蒸馏以保留已有知识；3) 一种新的采样策略，根据节点多样性和结构相关性选择重要节点；4) 语义级蒸馏模块，对教师和学生模型在不同元路径上的注意力分布进行对齐，确保语义一致性。

**结果:** MKD在三个基准数据集上进行了全面评估，结果表明其在处理动态异构图上的持续学习场景时具有显著的有效性。

**结论:** MKD框架为动态异构图上的持续学习提供了一种有效的解决方案，通过结合元学习、知识蒸馏、新采样策略和语义级蒸馏模块，成功缓解了灾难性遗忘问题，同时提高了模型效率和效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Heterogeneous+Continual+Graph+Learning+via+Meta-knowledge+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17458，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17458&send_immediately=true&force_search=false)

**原文摘要:** Machine learning on heterogeneous graphs has experienced rapid advancement in
recent years, driven by the inherently heterogeneous nature of real-world data.
However, existing studies typically assume the graphs to be static, while
real-world graphs are continuously expanding. This dynamic nature requires
models to adapt to new data while preserving existing knowledge. To this end,
this work addresses the challenge of continual learning on heterogeneous graphs
by introducing the Meta-learning based Knowledge Distillation framework (MKD),
designed to mitigate catastrophic forgetting in evolving heterogeneous graph
structures. MKD combines rapid task adaptation through meta-learning on limited
samples with knowledge distillation to achieve an optimal balance between
incorporating new information and maintaining existing knowledge. To improve
the efficiency and effectiveness of sample selection, MKD incorporates a novel
sampling strategy that selects a small number of target-type nodes based on
node diversity and maintains fixed-size buffers for other types. The strategy
retrieves first-order neighbors along metapaths and selects important neighbors
based on their structural relevance, enabling the sampled subgraphs to retain
key topological and semantic information. In addition, MKD introduces a
semantic-level distillation module that aligns the attention distributions over
different metapaths between teacher and student models, encouraging semantic
consistency beyond the logit level. Comprehensive evaluations across three
benchmark datasets validate MKD's effectiveness in handling continual learning
scenarios on expanding heterogeneous graphs.

</details>


### [56] [Efficient compression of neural networks and datasets](https://arxiv.org/abs/2505.17469)
*Lukas Silvester Barth, Paulo von Petersenn*

**主要类别:** cs.LG

**概要:** 本文研究了减少神经网络参数数量同时保持高测试准确率的方法，开发了一种新的概率优化方法，并在不同架构和数据集上进行了验证，还探讨了压缩算法与归纳推理理论的关系。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络模型通常具有大量的参数，这不仅增加了计算成本，也使得模型难以部署于资源受限的环境中。因此，研究如何减少神经网络参数数量，同时保持或提高模型性能具有重要意义。

**方法:** 作者提出并改进了多种减少神经网络参数的方法：1) 开发了一种无需蒙特卡洛采样的非线性模型的概率重述的 $\ell_0$ 正则化优化方法；2) 改进了 $\ell_0$ 范数的平滑近似方法；3) 研究了逐层方法。此外，作者还在不同的架构（如卷积网络和变换器）和数据集（如图像数据集和维基百科片段）上对这些方法进行了比较，并设计了一个合成的师生框架以研究受控连续环境下的压缩效果。

**结果:** 所提出的方法显著减少了神经网络的参数数量，同时保持了较高的测试准确率。特别是在应用这些方法进行最小描述长度优化时，得到了非常有效的数据压缩算法。实验结果还表明，正则化模型能够更有效地收敛，并验证了与索洛莫诺夫归纳推理理论的相关预测。

**结论:** 通过对比、改进和贡献减少神经网络参数的方法，本文证明了这些方法的有效性。特别地，提出的概率重述 $\ell_0$ 正则化方法和其他改进技术为高效的数据压缩提供了新途径。此外，本文将压缩算法与索洛莫诺夫归纳推理理论联系起来，为进一步研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+compression+of+neural+networks+and+datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17469&send_immediately=true&force_search=false)

**原文摘要:** We compare, improve, and contribute methods that substantially decrease the
number of parameters of neural networks while maintaining high test accuracy.
When applying our methods to minimize description length, we obtain very
effective data compression algorithms. In particular, we develop a
probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear
models that does not require Monte-Carlo sampling and thus improves upon
previous methods. We also improve upon methods involving smooth approximations
to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods
on different architectures and datasets, including convolutional networks
trained on image datasets and transformers trained on parts of Wikipedia. We
also created a synthetic teacher-student setup to investigate compression in a
controlled continuous setting. Finally, we conceptually relate compression
algorithms to Solomonoff's theory of inductive inference and empirically verify
the prediction that regularized models can exhibit more sample-efficient
convergence.

</details>


### [57] [Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance](https://arxiv.org/abs/2505.17477)
*Victor OK Li, Yang Han, Jacqueline CK Lam, Lawrence YL Cheung*

**主要类别:** cs.LG

**概要:** This study introduces RSF, a new method for Alzheimer's Disease diagnosis via speech analysis, which improves upon traditional methods by increasing accuracy and interpretability.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to address the challenges faced in Alzheimer's Disease diagnosis, specifically the scarcity of real AD speech samples and the limited interpretability of current models. By improving these aspects, the study aims to enhance diagnostic accuracy and pave the way for more effective early intervention strategies.

**方法:** The method involves introducing RSF, a neural network backtracking architecture. It leverages pre-trained large language models to identify most probable AD-specific speech markers (MPMs). The approach includes exploiting the relationship between MPMs and most probable neurons (MPNs), utilizing a speech token representation at the input layer to backtrack from MPNs to find most probable speech-tokens (MPTs), and developing an innovative backtracking method to track from MPNs to the input layer to uncover novel speech markers.

**结果:** RSF outperforms traditional methods such as SHAP and Integrated Gradients with a 3.5% improvement in accuracy and a 3.2% boost in F1-score. Additionally, it mitigates the limitations of real data scarcity and significantly enhances the robustness and accuracy of AD diagnostic models.

**结论:** This study concludes that the Reverse-Speech-Finder (RSF) is a superior method for enhancing Alzheimer's Disease diagnosis through speech analysis. It achieves better accuracy and F1-score compared to traditional methods, while also addressing the issues of data scarcity and limited interpretability in existing models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reverse-Speech-Finder%3A+A+Neural+Network+Backtracking+Architecture+for+Generating+Alzheimer%27s+Disease+Speech+Samples+and+Improving+Diagnosis+Performance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17477&send_immediately=true&force_search=false)

**原文摘要:** This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural
network backtracking architecture designed to enhance Alzheimer's Disease (AD)
diagnosis through speech analysis. Leveraging the power of pre-trained large
language models, RSF identifies and utilizes the most probable AD-specific
speech markers, addressing both the scarcity of real AD speech samples and the
challenge of limited interpretability in existing models. RSF's unique approach
consists of three core innovations: Firstly, it exploits the observation that
speech markers most probable of predicting AD, defined as the most probable
speech-markers (MPMs), must have the highest probability of activating those
neurons (in the neural network) with the highest probability of predicting AD,
defined as the most probable neurons (MPNs). Secondly, it utilizes a speech
token representation at the input layer, allowing backtracking from MPNs to
identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an
innovative backtracking method to track backwards from the MPNs to the input
layer, identifying the MPTs and the corresponding MPMs, and ingeniously
uncovering novel speech markers for AD detection. Experimental results
demonstrate RSF's superiority over traditional methods such as SHAP and
Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost
in F1-score. By generating speech data that encapsulates novel markers, RSF not
only mitigates the limitations of real data scarcity but also significantly
enhances the robustness and accuracy of AD diagnostic models. These findings
underscore RSF's potential as a transformative tool in speech-based AD
detection, offering new insights into AD-related linguistic deficits and paving
the way for more effective non-invasive early intervention strategies.

</details>


### [58] [Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression](https://arxiv.org/abs/2505.17478)
*Yuning Shen, Lihao Wang, Huizhuo Yuan, Yan Wang, Bangji Yang, Quanquan Gu*

**主要类别:** cs.LG

**概要:** ConfRover是一种新的自回归模型，可以从MD轨迹中同时学习蛋白质构象和动力学，并支持时间和非时间采样。


<details>
  <summary>更多</summary>
  
**动机:** 理解蛋白质动力学对于阐明其生物功能至关重要。现有的方法要么无法明确捕捉构象之间的时序依赖关系，要么不支持直接生成与时间无关的样本。

**方法:** ConfRover的核心是一种模块化架构，包括：(i) 编码层，将蛋白质特异性和每个时间帧的构象嵌入到潜在空间；(ii) 时间模块，捕捉跨帧的构象动力学；(iii) SE(3)扩散模型作为结构解码器，在连续空间中生成构象。

**结果:** 在ATLAS数据集上的实验表明，该模型在学习构象动力学和支持广泛的下游任务方面非常有效。

**结论:** ConfRover是第一个能够在单一框架内对蛋白质构象和轨迹进行采样的模型，为从蛋白质MD数据中学习提供了新颖且灵活的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Simultaneous+Modeling+of+Protein+Conformation+and+Dynamics+via+Autoregression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17478，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17478&send_immediately=true&force_search=false)

**原文摘要:** Understanding protein dynamics is critical for elucidating their biological
functions. The increasing availability of molecular dynamics (MD) data enables
the training of deep generative models to efficiently explore the
conformational space of proteins. However, existing approaches either fail to
explicitly capture the temporal dependencies between conformations or do not
support direct generation of time-independent samples. To address these
limitations, we introduce ConfRover, an autoregressive model that
simultaneously learns protein conformation and dynamics from MD trajectories,
supporting both time-dependent and time-independent sampling. At the core of
our model is a modular architecture comprising: (i) an encoding layer, adapted
from protein folding models, that embeds protein-specific information and
conformation at each time frame into a latent space; (ii) a temporal module, a
sequence model that captures conformational dynamics across frames; and (iii)
an SE(3) diffusion model as the structure decoder, generating conformations in
continuous space. Experiments on ATLAS, a large-scale protein MD dataset of
diverse structures, demonstrate the effectiveness of our model in learning
conformational dynamics and supporting a wide range of downstream tasks.
ConfRover is the first model to sample both protein conformations and
trajectories within a single framework, offering a novel and flexible approach
for learning from protein MD data.

</details>


### [59] [Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning](https://arxiv.org/abs/2505.17483)
*Yiqing Guo, Nagur Cherukuru, Eric Lehmann, S. L. Kesav Unnithan, Gemma Kerrisk, Tim Malthus, Faisal Islam*

**主要类别:** cs.LG

**概要:** This study investigates the relationship between nitrate levels and hyperspectral reflectance in water, finding that nitrate can be accurately predicted using hyperspectral data and salinity measurements, with promising results indicating potential applications in environmental monitoring.


<details>
  <summary>更多</summary>
  
**动机:** Nitrate, an optically inactive constituent mainly from anthropogenic sources, poses risks such as coral bleaching in sensitive ecosystems like the Great Barrier Reef lagoon. Despite being colorless, there is an indirect relationship between water surface nitrate and water-leaving reflectance mediated by optically active water quality parameters. Understanding this relationship can help monitor and mitigate nitrate-related environmental issues.

**方法:** The researchers conducted time-series measurements of nitrate and hyperspectral reflectance at the Fitzroy River estuary in Queensland, Australia. They used hyperspectral reflectance to indicate optically active variables and salinity to show the mixing proportions of river water and seawater. The relationship between these parameters was then used to predict nitrate loads in the water surface.

**结果:** The model developed in this study showed a strong correlation between predicted and in-situ measured nitrate values (R² = 0.86) and a low root mean square error (RMSE = 0.03 mg/L). These results demonstrate the effectiveness of using hyperspectral reflectance and salinity measurements to predict nitrate concentrations in water surfaces.

**结论:** The study concludes that nitrate values can be accurately predicted using hyperspectral reflectance and salinity measurements, with a high correlation (R² = 0.86) and low error (RMSE = 0.03 mg/L). This indicates the feasibility of this method for predicting water surface nitrate concentrations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyperspectral+in+situ+remote+sensing+of+water+surface+nitrate+in+the+Fitzroy+River+estuary%2C+Queensland%2C+Australia%2C+using+deep+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17483，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17483&send_immediately=true&force_search=false)

**原文摘要:** Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived
primarily from anthropogenic sources. The recent increase in river-discharged
nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR)
lagoon. Although nitrate is an optically inactive (i.e., colourless)
constituent, previous studies have demonstrated there is an indirect,
non-causal relationship between water surface nitrate and water-leaving
reflectance that is mediated through optically active water quality parameters
such as total suspended solids and coloured dissolved organic matter. This work
aims to advance our understanding of this relationship with an effort to
measure time-series nitrate and simultaneous hyperspectral reflectance at the
Fitzroy River estuary, Queensland, Australia. Time-series observations revealed
periodic cycles in nitrate loads due to the tidal influence in the estuarine
study site. The water surface nitrate loads were predicted from hyperspectral
reflectance and water salinity measurements, with hyperspectral reflectance
indicating the concentrations of optically active variables and salinity
indicating the mixing of river water and seawater proportions. The accuracy
assessment of model-predicted nitrate against in-situ measured nitrate values
showed that the predicted nitrate values correlated well with the ground-truth
data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work
demonstrates the feasibility of predicting water surface nitrate from
hyperspectral reflectance and salinity measurements.

</details>


### [60] [ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics](https://arxiv.org/abs/2505.17488)
*Haoran Li, Muhao Guo, Yang Weng, Marija Ilic, Guangchun Ruan*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架ExARNN，通过集成外部数据（如天气、时间）来连续调整基础RNN的参数，以适应非平稳电力系统动态。实验表明ExARNN优于现有基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 传统模型（如RNN）在编码外部因素（如时间或环境数据）方面缺乏有效机制，无法准确捕捉受可再生能源变化、需求模式演变和气候变化影响的非平稳电力系统动力学。

**方法:** 提出External Adaptive RNN (ExARNN)，一种结合外部数据（例如天气、时间）的新框架，使用神经控制微分方程(NCDE)处理外部数据并自适应生成RNN参数，采用分层超网络设计。

**结果:** 广泛的预测测试表明ExARNN在处理电力系统动态预报时优于已建立的基线模型。

**结论:** ExARNN能够通过整合外部数据实现对基础RNN参数的连续调整，解决了电力和外部测量之间的时间戳不一致问题，确保了持续适应能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ExARNN%3A+An+Environment-Driven+Adaptive+RNN+for+Learning+Non-Stationary+Power+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17488，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17488&send_immediately=true&force_search=false)

**原文摘要:** Non-stationary power system dynamics, influenced by renewable energy
variability, evolving demand patterns, and climate change, are becoming
increasingly complex. Accurately capturing these dynamics requires a model
capable of adapting to environmental factors. Traditional models, including
Recurrent Neural Networks (RNNs), lack efficient mechanisms to encode external
factors, such as time or environmental data, for dynamic adaptation. To address
this, we propose the External Adaptive RNN (ExARNN), a novel framework that
integrates external data (e.g., weather, time) to continuously adjust the
parameters of a base RNN. ExARNN achieves this through a hierarchical
hypernetwork design, using Neural Controlled Differential Equations (NCDE) to
process external data and generate RNN parameters adaptively. This approach
enables ExARNN to handle inconsistent timestamps between power and external
measurements, ensuring continuous adaptation. Extensive forecasting tests
demonstrate ExARNN's superiority over established baseline models.

</details>


### [61] [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)
*Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）通过捕捉输入特征间的复杂交互表现出色。为识别这些交互，现有方法需枚举给定阶数以下的所有可能特征组合，导致随着输入数量n的增加扩展性差。Kang等人（2025）提出了基于信息论的方法SPEX，利用交互稀疏性扩展至约10^3个特征，但需要数万次模型推理，对大型模型而言成本过高。本文观察到LLM特征交互通常具有层次性——高阶交互伴随着它们的低阶子集，这使得更高效的发现成为可能。为此，我们提出ProxySPEX，一种交互归因算法，首先将梯度提升树拟合到被遮罩的LLM输出上，然后提取重要交互。在四个具有挑战性的高维数据集上的实验表明，与边际归因方法相比，ProxySPEX能够以少10倍的推理量更忠实地重建LLM输出（提高20%）。通过考虑交互，ProxySPEX识别出比边际方法多影响模型输出超过20%的特征。此外，我们将ProxySPEX应用于两个可解释性任务：数据归因和机制可解释性。前者我们在CIFAR-10训练样本中识别影响测试预测的交互，后者我们在问答任务中揭示层内及层间注意力头之间的交互。ProxySPEX识别出的交互使头部修剪比边际方法更具侵略性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的交互识别方法在处理大规模特征时效率低下，且对于大型语言模型（LLMs），其交互识别方法如SPEX虽然有所改进但仍需大量推理资源，限制了实际应用。因此，需要一种更高效、更经济的交互识别方法来解决这一问题。

**方法:** 提出了一种名为ProxySPEX的交互归因算法。该算法首先将梯度提升树拟合到被遮罩的LLM输出上，然后从拟合结果中提取重要的交互项。这种方法利用了LLM特征交互的层次性，即高阶交互往往伴随着低阶子集，从而实现更高效的交互发现。

**结果:** 在四个高维数据集上的实验表明，ProxySPEX相较于边际归因方法能更准确地重建LLM输出（提高20%），并且仅需SPEX十分之一的推理次数。此外，ProxySPEX识别出的影响模型输出的特征比边际方法多20%以上。在数据归因和机制可解释性任务中，ProxySPEX也表现出更强的能力，例如在头部修剪方面优于边际方法。

**结论:** ProxySPEX是一种高效的交互归因算法，能够在减少推理资源消耗的同时更准确地识别LLM中的重要交互。它在高维数据集上的表现优于现有方法，并在可解释性任务中展现出强大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProxySPEX%3A+Inference-Efficient+Interpretability+via+Sparse+Feature+Interactions+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17495，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17495&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have achieved remarkable performance by
capturing complex interactions between input features. To identify these
interactions, most existing approaches require enumerating all possible
combinations of features up to a given order, causing them to scale poorly with
the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an
information-theoretic approach that uses interaction sparsity to scale to $n
\approx 10^3$ features. SPEX greatly improves upon prior methods but requires
tens of thousands of model inferences, which can be prohibitive for large
models. In this paper, we observe that LLM feature interactions are often
hierarchical -- higher-order interactions are accompanied by their lower-order
subsets -- which enables more efficient discovery. To exploit this hierarchy,
we propose ProxySPEX, an interaction attribution algorithm that first fits
gradient boosted trees to masked LLM outputs and then extracts the important
interactions. Experiments across four challenging high-dimensional datasets
show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over
marginal attribution approaches while using $10\times$ fewer inferences than
SPEX. By accounting for interactions, ProxySPEX identifies features that
influence model output over 20% more than those selected by marginal
approaches. Further, we apply ProxySPEX to two interpretability tasks. Data
attribution, where we identify interactions among CIFAR-10 training samples
that influence test predictions, and mechanistic interpretability, where we
uncover interactions between attention heads, both within and across layers, on
a question-answering task. ProxySPEX identifies interactions that enable more
aggressive pruning of heads than marginal approaches.

</details>


### [62] [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
*Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew C Yao*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为正则化策略梯度（RPG）的系统框架，用于推导和分析在线强化学习环境下的KL正则化策略梯度方法。通过实验表明，该方法在训练稳定性和性能方面优于或媲美现有强大基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管KL正则化在策略梯度算法中被广泛应用以稳定训练，但不同KL散度公式如何估计并整合到代理损失函数中的设计空间尚未被系统探索。

**方法:** 作者提出了RPG框架，推导了基于前向和反向KL散度正则化的策略梯度及其对应的代理损失函数，并考虑了标准化和非标准化的策略分布。此外，还提供了完全可微的损失函数及REINFORCE风格的梯度估计器。

**结果:** 在针对LLM推理的RL实验中，RPG方法相较于如GRPO、REINFORCE++和DAPO等强基线方法，在训练稳定性和性能上表现出改进或具有竞争力的结果。

**结论:** RPG提供了一个系统性的框架来设计和分析KL正则化的策略梯度方法，为提高大型语言模型推理能力的RL应用带来了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Design+of+KL-Regularized+Policy+Gradient+Algorithms+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17508&send_immediately=true&force_search=false)

**原文摘要:** Policy gradient algorithms have been successfully applied to enhance the
reasoning capabilities of large language models (LLMs). Despite the widespread
use of Kullback-Leibler (KL) regularization in policy gradient algorithms to
stabilize training, the systematic exploration of how different KL divergence
formulations can be estimated and integrated into surrogate loss functions for
online reinforcement learning (RL) presents a nuanced and systematically
explorable design space. In this paper, we propose regularized policy gradient
(RPG), a systematic framework for deriving and analyzing KL-regularized policy
gradient methods in the online RL setting. We derive policy gradients and
corresponding surrogate loss functions for objectives regularized by both
forward and reverse KL divergences, considering both normalized and
unnormalized policy distributions. Furthermore, we present derivations for
fully differentiable loss functions as well as REINFORCE-style gradient
estimators, accommodating diverse algorithmic needs. We conduct extensive
experiments on RL for LLM reasoning using these methods, showing improved or
competitive results in terms of training stability and performance compared to
strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at
https://github.com/complex-reasoning/RPG.

</details>


### [63] [What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection](https://arxiv.org/abs/2505.17513)
*Binh Nguyen, Shuji Shi, Ryan Ofman, Thai Le*

**主要类别:** cs.LG

**概要:** 最近文本到语音技术的进步使得真实的声音生成成为可能，同时也推动了基于音频的深度伪造攻击。虽然音频反欺骗系统对于检测这些威胁至关重要，但以往的研究主要集中在声学级扰动上，而语言变化的影响在很大程度上尚未被探索。本文通过引入脚本级对抗攻击来研究开源和商业反欺骗检测器的语言敏感性。我们的评估表明，即使是微小的语言扰动也会显著降低检测准确性：在一些开源检测器-声音对中，攻击成功率超过60%，其中一个商业检测器的准确率从100%下降到32%。我们还通过特征归因分析确定了语言复杂性和模型级别的音频嵌入相似性是导致检测器脆弱性的主要原因。此外，我们通过复制布拉德·皮特音频深度伪造骗局的案例研究，展示了现实世界中的风险，使用脚本对抗攻击可以完全绕过商业检测器。这些结果强调了需要超越纯粹的声学防御，在设计稳健的反欺骗系统时考虑语言变化。所有源代码都将公开发布。


<details>
  <summary>更多</summary>
  
**动机:** 尽管音频反欺骗系统对于检测深度伪造攻击至关重要，但之前的研究主要集中在声学级扰动上，而语言变化的影响尚未得到充分探索。这促使作者研究语言变化对反欺骗检测器性能的影响。

**方法:** 通过引入脚本级对抗攻击，评估开源和商业反欺骗检测器的语言敏感性。具体方法包括：进行广泛的评估以测试不同语言扰动对检测器的影响，并通过特征归因分析识别导致检测器脆弱性的原因。最后，通过一个实际案例研究展示现实世界中的风险。

**结果:** 微小的语言扰动会显著降低检测准确性，攻击成功率在一些开源检测器上超过60%，一个商业检测器的准确率从100%下降到32%。特征归因分析显示语言复杂性和模型级别的音频嵌入相似性是导致检测器脆弱的主要因素。案例研究表明，使用脚本对抗攻击可以完全绕过商业检测器。

**结论:** 需要超越纯粹的声学防御，在设计稳健的反欺骗系统时考虑语言变化。所有源代码将公开发布，以便进一步研究和改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+You+Read+Isn%27t+What+You+Hear%3A+Linguistic+Sensitivity+in+Deepfake+Speech+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17513，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17513&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in text-to-speech technologies have enabled realistic voice
generation, fueling audio-based deepfake attacks such as fraud and
impersonation. While audio anti-spoofing systems are critical for detecting
such threats, prior work has predominantly focused on acoustic-level
perturbations, leaving the impact of linguistic variation largely unexplored.
In this paper, we investigate the linguistic sensitivity of both open-source
and commercial anti-spoofing detectors by introducing transcript-level
adversarial attacks. Our extensive evaluation reveals that even minor
linguistic perturbations can significantly degrade detection accuracy: attack
success rates surpass 60% on several open-source detector-voice pairs, and
notably one commercial detection accuracy drops from 100% on synthetic audio to
just 32%. Through a comprehensive feature attribution analysis, we identify
that both linguistic complexity and model-level audio embedding similarity
contribute strongly to detector vulnerability. We further demonstrate the
real-world risk via a case study replicating the Brad Pitt audio deepfake scam,
using transcript adversarial attacks to completely bypass commercial detectors.
These results highlight the need to move beyond purely acoustic defenses and
account for linguistic variation in the design of robust anti-spoofing systems.
All source code will be publicly available.

</details>


### [64] [Spacetime Geometry of Denoising in Diffusion Models](https://arxiv.org/abs/2505.17517)
*Rafał Karczewski, Markus Heinonen, Alison Pouplin, Søren Hauberg, Vikas Garg*

**主要类别:** cs.LG

**概要:** 本研究从信息几何的角度提出了一种关于扩散模型的新颖观点，通过将噪声样本集合视为统计流形，定义了基于Fisher-Rao度量的测地线，并展示了其在高维空间中无需重新训练即可高效计算的优势。该方法在转换路径采样中具有实际应用价值，能够生成低能量亚稳态之间的连续轨迹。


<details>
  <summary>更多</summary>
  
**动机:** 扩散模型在机器学习领域得到了广泛关注，但其背后的几何结构尚未被充分挖掘。本文旨在从信息几何的角度重新审视扩散模型，揭示其潜在的统计流形结构，并探索其实际应用潜力。

**方法:** 作者将所有噪声水平下的噪声样本集合作为一个统计流形，其中每个点对应一个去噪概率分布。通过将噪声水平解释为时间参数，该流形被称为时空流形。利用Fisher-Rao度量定义流形上的测地线，即噪声点之间的最短路径。由于该分布族为指数族，即使在高维情况下也能高效计算测地线。

**结果:** 研究表明，所提出的几何视角在转换路径采样中具有显著的实际价值。通过定义平滑的Boltzmann分布序列，可以生成低能量亚稳态之间的连续轨迹，验证了方法的有效性。

**结论:** 本文从信息几何的角度提出了扩散模型的新见解，展示了其在高维空间中高效计算测地线的能力以及在转换路径采样中的实际应用潜力，为进一步研究扩散模型的几何特性提供了理论基础和实践工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spacetime+Geometry+of+Denoising+in+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17517，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17517&send_immediately=true&force_search=false)

**原文摘要:** We present a novel perspective on diffusion models using the framework of
information geometry. We show that the set of noisy samples, taken across all
noise levels simultaneously, forms a statistical manifold -- a family of
denoising probability distributions. Interpreting the noise level as a temporal
parameter, we refer to this manifold as spacetime. This manifold naturally
carries a Fisher-Rao metric, which defines geodesics -- shortest paths between
noisy points. Notably, this family of distributions is exponential, enabling
efficient geodesic computation even in high-dimensional settings without
retraining or fine-tuning. We demonstrate the practical value of this geometric
viewpoint in transition path sampling, where spacetime geodesics define smooth
sequences of Boltzmann distributions, enabling the generation of continuous
trajectories between low-energy metastable states. Code is available at:
https://github.com/Aalto-QuML/diffusion-spacetime-geometry.

</details>


### [65] [TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting](https://arxiv.org/abs/2505.17532)
*Bin Wang, Heming Yang, Jinfang Sheng*

**主要类别:** cs.LG

**概要:** 近期研究表明，在实际环境中，通过引入先验知识，对复杂和非平稳时间序列进行多尺度分析在长期预测领域可以取得良好的效果。然而，受通道独立方法的影响，基于多尺度分析的模型可能由于时间序列标签之间的自相关性而产生次优预测结果，从而影响模型的泛化能力。为了解决这一挑战，我们受到Sharpness-Aware Minimization和FreDF方法的启发，设计了一个基于TimeMixer的深度学习模型TimeCF，结合了我们设计的自适应卷积信息聚合模块和Sharpness-Aware Minimization Frequency Domain Loss (SAMFre)。具体来说，TimeCF首先将原始时间序列分解为不同尺度的序列。接下来，使用相同大小的卷积模块自适应地聚合不同尺度序列的信息。然后，将每个序列分解为季节和趋势部分，并通过自下而上和自上而下的方法分别在不同尺度上混合这两部分。最后，通过前馈网络聚合不同尺度的结果。此外，广泛的实验结果表明，我们提出的TimeCF在长期预测领域具有优异的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多尺度分析方法在处理长期预测时，可能因时间序列标签间的自相关性而导致次优预测结果，影响模型泛化能力。因此需要一种新的方法来改进这一问题。

**方法:** 设计了一个深度学习模型TimeCF，包括以下步骤：1. 将原始时间序列分解为不同尺度的序列；2. 使用相同大小的卷积模块自适应聚合不同尺度序列的信息；3. 分解每个序列成季节和趋势部分，并通过自下而上和自上而下的方法分别混合这两部分；4. 通过前馈网络聚合不同尺度的结果。此外，还引入了自适应卷积信息聚合模块和Sharpness-Aware Minimization Frequency Domain Loss (SAMFre)。

**结果:** 广泛的实验结果表明，所提出的TimeCF模型在多个真实数据集上的长期预测表现优异。

**结论:** TimeCF模型能够有效应对多尺度时间序列中的自相关性问题，显著提升长期预测性能和模型泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TimeCF%3A+A+TimeMixer-Based+Model+with+adaptive+Convolution+and+Sharpness-Aware+Minimization+Frequency+Domain+Loss+for+long-term+time+seris+forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17532&send_immediately=true&force_search=false)

**原文摘要:** Recent studies have shown that by introducing prior knowledge, multi-scale
analysis of complex and non-stationary time series in real environments can
achieve good results in the field of long-term forecasting. However, affected
by channel-independent methods, models based on multi-scale analysis may
produce suboptimal prediction results due to the autocorrelation between time
series labels, which in turn affects the generalization ability of the model.
To address this challenge, we are inspired by the idea of sharpness-aware
minimization and the recently proposed FreDF method and design a deep learning
model TimeCF for long-term time series forecasting based on the TimeMixer,
combined with our designed adaptive convolution information aggregation module
and Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically,
TimeCF first decomposes the original time series into sequences of different
scales. Next, the same-sized convolution modules are used to adaptively
aggregate information of different scales on sequences of different scales.
Then, decomposing each sequence into season and trend parts and the two parts
are mixed at different scales through bottom-up and top-down methods
respectively. Finally, different scales are aggregated through a Feed-Forward
Network. What's more, extensive experimental results on different real-world
datasets show that our proposed TimeCF has excellent performance in the field
of long-term forecasting.

</details>


### [66] [Learning Representational Disparities](https://arxiv.org/abs/2505.17533)
*Pavan Ravishankar, Rushabh Shah, Daniel B. Neill*

**主要类别:** cs.LG

**概要:** 本研究提出了一种公平的机器学习算法，通过建模和解释人类决策过程中观察到与期望之间的差异，减少下游结果中的不平等。该方法使用神经网络框架下的多目标优化问题，证明了在合理假设下，模型可以学习到可解释的权重以完全缓解结果差异，并通过德国信用、成人和健康数据集验证了方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的公平表示学习方法未充分考虑决策过程对下游结果的影响，因此需要一种能够解释并纠正人类决策中导致结果不平等的因素的方法。

**方法:** 将结果差异归因于观察到和期望的决策者对输入的不同表示（即表示差异），并通过神经网络构建多目标优化问题来学习这些表示差异。在合理简化假设下，证明模型能学习到完全缓解结果差异的可解释权重。

**结果:** 理论证明在特定假设条件下，模型可以学习到完全缓解结果差异的可解释权重。实验结果通过德国信用、成人和健康数据集验证了目标的有效性和结果的可解释性。

**结论:** 所提出的公平机器学习算法能够有效建模和解释人类决策中的表示差异，从而通过特定干预减少下游结果中的不平等。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Representational+Disparities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17533&send_immediately=true&force_search=false)

**原文摘要:** We propose a fair machine learning algorithm to model interpretable
differences between observed and desired human decision-making, with the latter
aimed at reducing disparity in a downstream outcome impacted by the human
decision. Prior work learns fair representations without considering the
outcome in the decision-making process. We model the outcome disparities as
arising due to the different representations of the input seen by the observed
and desired decision-maker, which we term representational disparities. Our
goal is to learn interpretable representational disparities which could
potentially be corrected by specific nudges to the human decision, mitigating
disparities in the downstream outcome; we frame this as a multi-objective
optimization problem using a neural network. Under reasonable simplifying
assumptions, we prove that our neural network model of the representational
disparity learns interpretable weights that fully mitigate the outcome
disparity. We validate objectives and interpret results using real-world German
Credit, Adult, and Heritage Health datasets.

</details>


### [67] [Graph Style Transfer for Counterfactual Explainability](https://arxiv.org/abs/2505.17542)
*Bardh Prenkaj, Efstratios Zaradoukas, Gjergji Kasneci*

**主要类别:** cs.LG

**概要:** 提出了一种新的图反向风格迁移（GIST）框架，用于生成有效的图反事实解释，并在多个基准测试中显著优于传统方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的反事实解释方法难以处理图数据，因为需要同时保持结构完整性和语义意义。传统的前向扰动机制存在不足，促使研究者探索一种新方法来解决这些问题。

**方法:** 引入了Graph Inverse Style Transfer (GIST)，将图反事实生成视为一个回溯过程，利用谱风格迁移技术。通过将全局结构与原始输入谱对齐并保持局部内容的保真度，GIST生成了有效的反事实解释。

**结果:** 在8个二分类和多分类图分类基准上，GIST在生成有效反事实方面提高了7.6%，在忠实解释真实类别分布方面提高了45.5%。此外，其回溯机制有效减少了超越预测器决策边界的概率，最小化了输入与反事实之间的谱差异。

**结论:** GIST为图反事实解释提供了一种新视角，挑战了传统的前向扰动方法，并推动了图可解释性的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Style+Transfer+for+Counterfactual+Explainability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17542，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17542&send_immediately=true&force_search=false)

**原文摘要:** Counterfactual explainability seeks to uncover model decisions by identifying
minimal changes to the input that alter the predicted outcome. This task
becomes particularly challenging for graph data due to preserving structural
integrity and semantic meaning. Unlike prior approaches that rely on forward
perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the
first framework to re-imagine graph counterfactual generation as a backtracking
process, leveraging spectral style transfer. By aligning the global structure
with the original input spectrum and preserving local content faithfulness,
GIST produces valid counterfactuals as interpolations between the input style
and counterfactual content. Tested on 8 binary and multi-class graph
classification benchmarks, GIST achieves a remarkable +7.6% improvement in the
validity of produced counterfactuals and significant gains (+45.5%) in
faithfully explaining the true class distribution. Additionally, GIST's
backtracking mechanism effectively mitigates overshooting the underlying
predictor's decision boundary, minimizing the spectral differences between the
input and the counterfactuals. These results challenge traditional forward
perturbation methods, offering a novel perspective that advances graph
explainability.

</details>


### [68] [Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing](https://arxiv.org/abs/2505.17552)
*Zijie Qiu, Jiaqi Wei, Xiang Zhang, Sheng Xu, Kai Zou, Zhi Jin, Zhiqiang Gao, Nanqing Dong, Siqi Sun*

**主要类别:** cs.LG

**概要:** RankNovo 是一种新的深度重排序框架，用于通过利用多个测序模型的互补优势来增强从头肽测序。它引入了 PMD 和 RMD 两个新指标，并展示了超越基础模型的表现，具有强大的零样本泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于深度学习的从头肽测序方法受限于质谱数据的固有复杂性和噪声信号的异构分布，导致数据特定偏差。为了克服这些限制并提高测序准确性，需要一种新的方法来整合多个模型的优势。

**方法:** RankNovo 使用列表式重排序方法，将候选肽建模为多序列比对，并利用轴向注意力提取跨候选的信息特征。同时，引入了肽质量偏差（PMD）和残基质量偏差（RMD）两个新指标以提供精细的监督。

**结果:** 广泛的实验表明，RankNovo 不仅超越了其用于生成重排序预训练训练候选的基础模型，还设定了新的最先进的基准。此外，它对未见模型表现出强大的零样本泛化能力。

**结论:** RankNovo 提出了一种新颖的重排序策略，挑战了现有的单模型范式，推动了准确从头测序的前沿。源代码已在 GitHub 上提供。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Biological+Sequence+Reranking+for+Improved+De+Novo+Peptide+Sequencing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17552&send_immediately=true&force_search=false)

**原文摘要:** De novo peptide sequencing is a critical task in proteomics. However, the
performance of current deep learning-based methods is limited by the inherent
complexity of mass spectrometry data and the heterogeneous distribution of
noise signals, leading to data-specific biases. We present RankNovo, the first
deep reranking framework that enhances de novo peptide sequencing by leveraging
the complementary strengths of multiple sequencing models. RankNovo employs a
list-wise reranking approach, modeling candidate peptides as multiple sequence
alignments and utilizing axial attention to extract informative features across
candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass
Deviation) and RMD (residual Mass Deviation), which offer delicate supervision
by quantifying mass differences between peptides at both the sequence and
residue levels. Extensive experiments demonstrate that RankNovo not only
surpasses its base models used to generate training candidates for reranking
pre-training, but also sets a new state-of-the-art benchmark. Moreover,
RankNovo exhibits strong zero-shot generalization to unseen models whose
generations were not exposed during training, highlighting its robustness and
potential as a universal reranking framework for peptide sequencing. Our work
presents a novel reranking strategy that fundamentally challenges existing
single-model paradigms and advances the frontier of accurate de novo
sequencing. Our source code is provided on GitHub.

</details>


### [69] [MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity](https://arxiv.org/abs/2505.17591)
*Judith Vilella-Cantos, Juan José Cabrera, Luis Payá, Mónica Ballesta, David Valiente*

**主要类别:** cs.LG

**概要:** In autonomous navigation systems, place recognition is crucial. The paper introduces MinkUNeXt-SI, a method using LiDAR point clouds to produce robust descriptors via deep learning with Minkowski convolutions and U-net architecture. It surpasses state-of-the-art performance, generalizes well, and uses a custom dataset for evaluation.


<details>
  <summary>更多</summary>
  
**动机:** Place recognition in autonomous navigation systems must be accurate despite scene changes (seasonal or weather) and generalizable to other environments.

**方法:** MinkUNeXt-SI preprocesses LiDAR point cloud data to obtain spherical coordinates and normalized intensity values, then applies a deep learning approach combining Minkowski convolutions and U-net architecture with skip connections to produce robust place recognition descriptors.

**结果:** MinkUNeXt-SI reaches and surpasses state-of-the-art performance in place recognition while also satisfactorily generalizing to other datasets.

**结论:** The method demonstrates excellent results in place recognition and generalization, with both the code and dataset publicly available for reproducibility.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MinkUNeXt-SI%3A+Improving+point+cloud-based+place+recognition+including+spherical+coordinates+and+LiDAR+intensity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17591，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17591&send_immediately=true&force_search=false)

**原文摘要:** In autonomous navigation systems, the solution of the place recognition
problem is crucial for their safe functioning. But this is not a trivial
solution, since it must be accurate regardless of any changes in the scene,
such as seasonal changes and different weather conditions, and it must be
generalizable to other environments. This paper presents our method,
MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input
data to obtain its spherical coordinates and intensity values normalized within
a range of 0 to 1 for each point, and it produces a robust place recognition
descriptor. To that end, a deep learning approach that combines Minkowski
convolutions and a U-net architecture with skip connections is used. The
results of MinkUNeXt-SI demonstrate that this method reaches and surpasses
state-of-the-art performance while it also generalizes satisfactorily to other
datasets. Additionally, we showcase the capture of a custom dataset and its use
in evaluating our solution, which also achieves outstanding results. Both the
code of our solution and the runs of our dataset are publicly available for
reproducibility purposes.

</details>


### [70] [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.17553)
*Jinyuan Feng, Chaopeng Wei, Tenghai Qiu, Tianyi Hu, Zhiqiang Pu*

**主要类别:** cs.LG

**概要:** 在参数高效的微调中，专家混合（MoE）是一种被广泛采用的方法，用于在模型容量和计算开销之间进行权衡。然而，当前的MoE变体在异构数据集上表现不佳，忽略了专家可能学习相似知识的事实，导致MoE容量的未充分利用。本文提出了对比表示的MoE（CoMoE），通过从top-k路由中的激活和非激活专家中采样，结合对比目标训练专家，从而促进模块化和专业化。实验表明，CoMoE可以持续增强MoE的容量并促进专家之间的模块化。


<details>
  <summary>更多</summary>
  
**动机:** 当前的MoE变体在异构数据集上表现不佳，因为它们忽略了专家可能学习相似知识的情况，这导致了MoE容量的未充分利用。因此，需要一种新的方法来提高专家之间的模块化和专业化程度。

**方法:** 提出了一种名为Contrastive Representation for MoE (CoMoE)的新方法，该方法通过在top-k路由中从激活和非激活的专家中采样，并结合对比目标一起训练专家，以促进MoE中的模块化和专业化。

**结果:** 实验结果表明，CoMoE能够在多个基准测试和多任务设置中持续增强MoE的容量，并促进专家之间的模块化。

**结论:** CoMoE为MoE提供了一种有效的解决方案，以提高其在异构数据集上的表现，并通过促进模块化和专业化充分利用MoE的容量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CoMoE%3A+Contrastive+Representation+for+Mixture-of-Experts+in+Parameter-Efficient+Fine-tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17553，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17553&send_immediately=true&force_search=false)

**原文摘要:** In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves
specializing functionalities into different experts and sparsely activating
them appropriately, has been widely adopted as a promising approach to
trade-off between model capacity and computation overhead. However, current MoE
variants fall short on heterogeneous datasets, ignoring the fact that experts
may learn similar knowledge, resulting in the underutilization of MoE's
capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),
a novel method to promote modularization and specialization in MoE, where the
experts are trained along with a contrastive objective by sampling from
activated and inactivated experts in top-k routing. We demonstrate that such a
contrastive objective recovers the mutual-information gap between inputs and
the two types of experts. Experiments on several benchmarks and in multi-task
settings demonstrate that CoMoE can consistently enhance MoE's capacity and
promote modularization among the experts.

</details>


### [71] [Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis](https://arxiv.org/abs/2505.17636)
*Jonathan Bennion, Shaona Ghosh, Mantek Singh, Nouha Dziri*

**主要类别:** cs.LG

**概要:** 本研究评估了五个最近发布的开源安全基准，揭示了不同的语义集群，并量化了AI基准之间的正交性，从而提高了覆盖差距的透明度。


<details>
  <summary>更多</summary>
  
**动机:** 为了测量大型语言模型（LLMs）在不断变化的危害定义下的表现，研究人员开发了各种AI安全数据集。因此，需要对这些数据集进行系统分析以了解其覆盖范围和潜在差距。

**方法:** 使用UMAP降维和kmeans聚类方法来识别和分析五个开源安全基准中的语义集群。通过计算轮廓分数来评估聚类质量，并识别主要危害类别。此外，还分析了不同基准中提示长度分布的显著差异。

**结果:** 发现了六个主要危害类别，具有不同的基准表示；例如，GretelAI侧重于隐私问题，而WildGuardMix则强调自我伤害情景。提示长度分布的显著差异表明数据收集和危害解释存在混淆因素。

**结论:** 本研究提出了一种定量框架，用于分析跨安全基准的语义正交性，这有助于更有针对性地开发数据集，以全面应对AI使用中不断演变的危害景观。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Surfacing+Semantic+Orthogonality+Across+Model+Safety+Benchmarks%3A+A+Multi-Dimensional+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17636，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17636&send_immediately=true&force_search=false)

**原文摘要:** Various AI safety datasets have been developed to measure LLMs against
evolving interpretations of harm. Our evaluation of five recently published
open-source safety benchmarks reveals distinct semantic clusters using UMAP
dimensionality reduction and kmeans clustering (silhouette score: 0.470). We
identify six primary harm categories with varying benchmark representation.
GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix
emphasizes self-harm scenarios. Significant differences in prompt length
distribution suggests confounds to data collection and interpretations of harm
as well as offer possible context. Our analysis quantifies benchmark
orthogonality among AI benchmarks, allowing for transparency in coverage gaps
despite topical similarities. Our quantitative framework for analyzing semantic
orthogonality across safety benchmarks enables more targeted development of
datasets that comprehensively address the evolving landscape of harms in AI
use, however that is defined in the future.

</details>


### [72] [Wildfire spread forecasting with Deep Learning](https://arxiv.org/abs/2505.17556)
*Nikolaos Anastasiou, Spyros Kondylatos, Ioannis Papoutsis*

**主要类别:** cs.LG

**概要:** 本研究提出了一种基于深度学习的框架，用于预测野火燃烧区域的最终范围。通过使用点火时可用的数据和包含地中海地区2006年至2022年的时空数据集，发现多日观测数据显著提高了预测精度。最佳模型在点火前四天到点火后五天的时间窗口内，相比仅使用点火当天数据的基线模型，F1分数和交并比提高了近5%。


<details>
  <summary>更多</summary>
  
**动机:** 精确预测野火蔓延对于有效的风险管理、应急响应和战略资源分配至关重要。现有的方法可能无法充分利用所有可用的数据类型和时间背景，因此需要一种新的方法来提高预测准确性。

**方法:** 研究采用了深度学习框架，并利用了一个覆盖地中海地区2006年至2022年的时空数据集，其中包括遥感数据、气象观测、植被图、土地覆盖分类、人为因素、地形数据和热异常。通过消融研究评估了包含点火前后数据对模型性能的影响，并将具有时间感知的深度学习模型与仅基于点火日输入训练的基线模型进行了对比。

**结果:** 结果表明，多日观测数据大大提高了预测精度。最佳模型（包括点火前四天至点火后五天的数据）在测试数据集上使F1分数和交并比相对于基线提高了近5%。

**结论:** 多日观测数据可以显著提高野火蔓延预测的准确性。研究公开发布了数据集和模型，以推动数据驱动的野火建模和响应研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wildfire+spread+forecasting+with+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17556&send_immediately=true&force_search=false)

**原文摘要:** Accurate prediction of wildfire spread is crucial for effective risk
management, emergency response, and strategic resource allocation. In this
study, we present a deep learning (DL)-based framework for forecasting the
final extent of burned areas, using data available at the time of ignition. We
leverage a spatio-temporal dataset that covers the Mediterranean region from
2006 to 2022, incorporating remote sensing data, meteorological observations,
vegetation maps, land cover classifications, anthropogenic factors, topography
data, and thermal anomalies. To evaluate the influence of temporal context, we
conduct an ablation study examining how the inclusion of pre- and post-ignition
data affects model performance, benchmarking the temporal-aware DL models
against a baseline trained exclusively on ignition-day inputs. Our results
indicate that multi-day observational data substantially improve predictive
accuracy. Particularly, the best-performing model, incorporating a temporal
window of four days before to five days after ignition, improves both the F1
score and the Intersection over Union by almost 5% in comparison to the
baseline on the test dataset. We publicly release our dataset and models to
enhance research into data-driven approaches for wildfire modeling and
response.

</details>


### [73] [Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective](https://arxiv.org/abs/2505.17652)
*Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye*

**主要类别:** cs.LG

**概要:** 强化学习在提升大语言模型推理能力方面具有潜力，但因样本效率低而难以扩展。现有方法通过基于问题难度调度问题来提高效率，但这些方法在估计问题难度时不稳定且有偏差，并且无法捕捉模型能力和问题难度之间的对齐关系，导致次优结果。本文提出了能力-难度对齐采样（CDAS），通过聚合历史性能差异实现对问题难度的准确和稳定估计，并使用固定点系统量化模型能力以自适应选择与模型当前能力相匹配的问题。实验表明，CDAS在多个数学基准上显著提高了准确性和效率，平均准确率最高且速度比动态采样快2.33倍。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习在提升大语言模型推理能力方面具有潜力，但在扩展过程中面临样本效率低的问题。现有方法虽然尝试通过问题难度调度来提高效率，但存在估计不稳定、有偏差以及无法捕捉模型能力与问题难度之间对齐关系的问题。

**方法:** 提出了一种名为能力-难度对齐采样（CDAS）的方法。该方法通过聚合问题的历史性能差异来实现对问题难度的准确和稳定估计，并使用固定点系统量化模型能力，从而自适应地选择与模型当前能力相匹配的问题。

**结果:** 在多个具有挑战性的数学基准上的实验结果表明，CDAS在准确性和效率上都取得了显著改进。它达到了最高的平均准确率，并且相比动态采样策略，速度提升了2.33倍。

**结论:** CDAS通过准确和稳定的难度估计以及与模型能力的对齐，在提高强化学习训练的样本效率方面表现出色，为提升大语言模型推理能力提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+the+Sampling+Criteria+in+Reinforcement+Learning+for+LLM+Reasoning%3A+A+Competence-Difficulty+Alignment+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17652&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning exhibits potential in enhancing the reasoning
abilities of large language models, yet it is hard to scale for the low sample
efficiency during the rollout phase. Existing methods attempt to improve
efficiency by scheduling problems based on problem difficulties. However, these
approaches suffer from unstable and biased estimations of problem difficulty
and fail to capture the alignment between model competence and problem
difficulty in RL training, leading to suboptimal results. To tackle these
limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty
\textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate
and stable estimation of problem difficulties by aggregating historical
performance discrepancies of problems. Then the model competence is quantified
to adaptively select problems whose difficulty is in alignment with the model's
current competence using a fixed-point system. Experimental results across a
range of challenging mathematical benchmarks show that CDAS achieves great
improvements in both accuracy and efficiency. CDAS attains the highest average
accuracy against baselines and exhibits significant speed advantages compared
to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33}
times slower than CDAS.

</details>


### [74] [Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs](https://arxiv.org/abs/2505.17575)
*Changfan Yang, Lichen Bai, Yinpeng Wang, Shufei Zhang, Zeke Xie*

**主要类别:** cs.LG

**概要:** 使用机器学习求解偏微分方程（PDEs）最近受到了广泛关注，本文针对多物理场PDE求解提出了数据集、评估和改进方法。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现实世界的物理系统涉及多个耦合物理场，但之前的机器学习研究主要集中在单一场问题上，忽略了多物理场问题的重要性。

**方法:** 1) 收集了第一个通用的多物理场数据集Multiphysics Bench；2) 在多物理场问题上对多个代表性学习型PDE求解器进行了首次系统研究；3) 通过广泛实验和讨论，报告了解决多物理场问题的多个见解和实用技巧。

**结果:** 现有的学习型PDE求解器在多物理场问题上表现通常较差，但通过提出的方法和技巧可以有所改善，并为未来的研究提供了方向。

**结论:** 本文提出了一个多物理场数据集并进行了系统研究，揭示了现有求解器的不足，提供了改进建议和未来研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiphysics+Bench%3A+Benchmarking+and+Investigating+Scientific+Machine+Learning+for+Multiphysics+PDEs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17575，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17575&send_immediately=true&force_search=false)

**原文摘要:** Solving partial differential equations (PDEs) with machine learning has
recently attracted great attention, as PDEs are fundamental tools for modeling
real-world systems that range from fundamental physical science to advanced
engineering disciplines. Most real-world physical systems across various
disciplines are actually involved in multiple coupled physical fields rather
than a single field. However, previous machine learning studies mainly focused
on solving single-field problems, but overlooked the importance and
characteristics of multiphysics problems in real world. Multiphysics PDEs
typically entail multiple strongly coupled variables, thereby introducing
additional complexity and challenges, such as inter-field coupling. Both
benchmarking and solving multiphysics problems with machine learning remain
largely unexamined. To identify and address the emerging challenges in
multiphysics problems, we mainly made three contributions in this work. First,
we collect the first general multiphysics dataset, the Multiphysics Bench, that
focuses on multiphysics PDE solving with machine learning. Multiphysics Bench
is also the most comprehensive PDE dataset to date, featuring the broadest
range of coupling types, the greatest diversity of PDE formulations, and the
largest dataset scale. Second, we conduct the first systematic investigation on
multiple representative learning-based PDE solvers, such as PINNs, FNO,
DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately,
naively applying these existing solvers usually show very poor performance for
solving multiphysics. Third, through extensive experiments and discussions, we
report multiple insights and a bag of useful tricks for solving multiphysics
with machine learning, motivating future directions in the study and simulation
of complex, coupled physical systems.

</details>


### [75] [Towards General Continuous Memory for Vision-Language Models](https://arxiv.org/abs/2505.17670)
*Wenyi Wu, Zixuan Song, Kun Zhou, Yifei Shao, Zhiting Hu, Biwei Huang*

**主要类别:** cs.LG

**概要:** 论文提出了一种名为CoMEM的方法，利用连续记忆系统来增强视觉-语言模型（VLM）在复杂多模态推理任务中的表现。通过将多模态和多语言知识编码为8个连续嵌入，并仅使用模型1.2%的参数进行微调，该方法能够高效地提供相关多模态信息，同时保持插件式灵活性。实验表明，这种方法在八个基准测试中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的外部记忆系统通常将图像和文本标记连接成长序列，这可能显著增加上下文长度并降低性能。因此，需要一种更有效、更高效的表示多模态和多语言知识的方法。

**方法:** 提出了一种基于连续记忆系统的紧凑密集嵌入方法。关键在于使用VLM作为其自身的连续记忆编码器，并通过数据高效和参数高效的方法对其进行微调，以生成连续嵌入表示多模态和多语言知识。最终，该方法（CoMEM）将这些知识编码为8个连续嵌入。

**结果:** 实验证明，该设计在复杂的多模态推理任务上提高了性能。并且只需要使用模型1.2%的参数和少量的合成样本进行微调，从而实现了数据和参数效率的提升。

**结论:** CoMEM方法能够在不改变原始VLM的情况下，灵活地集成外部记忆模块，有效支持复杂多模态推理任务。广泛的实验结果验证了该方法的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+General+Continuous+Memory+for+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17670&send_immediately=true&force_search=false)

**原文摘要:** Language models (LMs) and their extension, vision-language models (VLMs),
have achieved remarkable performance across various tasks. However, they still
struggle with complex reasoning tasks that require multimodal or multilingual
real-world knowledge. To support such capabilities, an external memory system
that can efficiently provide relevant multimodal information is essential.
Existing approaches generally concatenate image and text tokens into a long
sequence as memory, which, however, may drastically increase context length and
even degrade performance. In contrast, we propose using continuous memory, a
compact set of dense embeddings to more effectively and efficiently represent
multimodal and multilingual knowledge. Our key insight is that a VLM can serve
as its own continuous memory encoder. We empirically show that this design
improves performance on complex multimodal reasoning tasks. Building on this,
we introduce a data-efficient and parameter-efficient method to fine-tune the
VLM into a memory encoder, requiring only 1.2% of the model's parameters and a
small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes
VLM's original capabilities to encode arbitrary multimodal and multilingual
knowledge into just 8 continuous embeddings. Since the inference-time VLM
remains frozen, our memory module is plug-and-play and can be flexibly
integrated as needed. Extensive experiments across eight multimodal reasoning
benchmarks demonstrate the effectiveness of our approach.

</details>


### [76] [Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation](https://arxiv.org/abs/2505.17579)
*Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, Eisuke Koizumi*

**主要类别:** cs.LG

**概要:** This paper proposes a novel framework for ownership verification of DNN models without presenting the original model.


<details>
  <summary>更多</summary>
  
**动机:** To verify the identity of DNN models in image classification tasks, allowing both rightful owners and third parties to confirm ownership without revealing the original model.

**方法:** The framework applies a white-box adversarial attack to align the output probability of a specific class to a designated value using an iterative FGSM with control parameters.

**结果:** Experimental results show the effectiveness of identifying DNN models using the proposed adversarial attack method.

**结论:** The proposed framework successfully verifies the ownership of DNN models through adversarial attacks without needing the original model.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ownership+Verification+of+DNN+Models+Using+White-Box+Adversarial+Attacks+with+Specified+Probability+Manipulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17579&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we propose a novel framework for ownership verification of
deep neural network (DNN) models for image classification tasks. It allows
verification of model identity by both the rightful owner and third party
without presenting the original model. We assume a gray-box scenario where an
unauthorized user owns a model that is illegally copied from the original
model, provides services in a cloud environment, and the user throws images and
receives the classification results as a probability distribution of output
classes. The framework applies a white-box adversarial attack to align the
output probability of a specific class to a designated value. Due to the
knowledge of original model, it enables the owner to generate such adversarial
examples. We propose a simple but effective adversarial attack method based on
the iterative Fast Gradient Sign Method (FGSM) by introducing control
parameters. Experimental results confirm the effectiveness of the
identification of DNN models using adversarial attack.

</details>


### [77] [SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data](https://arxiv.org/abs/2505.17695)
*Dong-Hee Kim, Hyunjee Song, Donghyun Kim*

**主要类别:** cs.LG

**概要:** 尽管在指代表达分割（RES）基准方面取得了进展，但其评估协议仍然受限。我们引入了WildRES，一个包含长查询和多样化属性的新基准，涵盖自动驾驶和机器人操作场景。当前RES模型在WildRES上的表现显著下降。为此，我们提出了SynRES，通过三个创新点生成合成训练数据，实验表明SynRES训练的模型在WildRES上达到最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RES基准评估协议存在局限性，主要集中在单一目标或简单查询上，无法充分评估模型在复杂推理能力方面的表现。

**方法:** 1. 提出WildRES基准：包含长查询、多样化属性和非独特查询，覆盖多个实际应用领域。2. 提出SynRES管道：通过密集配对组合生成合成训练数据，包括丰富的属性图像-掩码-表达三元组、可靠的语义对齐机制以及领域感知增强方法。

**结果:** 使用SynRES训练的模型在WildRES-ID上gIoU提升了2.0%，在WildRES-DS上提升了3.8%，达到了最先进的性能。

**结论:** SynRES为RES任务提供了一种有效的方法来提升模型在复杂推理任务中的表现，代码和数据集已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SynRES%3A+Towards+Referring+Expression+Segmentation+in+the+Wild+via+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17695&send_immediately=true&force_search=false)

**原文摘要:** Despite the advances in Referring Expression Segmentation (RES) benchmarks,
their evaluation protocols remain constrained, primarily focusing on either
single targets with short queries (containing minimal attributes) or multiple
targets from distinctly different queries on a single domain. This limitation
significantly hinders the assessment of more complex reasoning capabilities in
RES models. We introduce WildRES, a novel benchmark that incorporates long
queries with diverse attributes and non-distinctive queries for multiple
targets. This benchmark spans diverse application domains, including autonomous
driving environments and robotic manipulation scenarios, thus enabling more
rigorous evaluation of complex reasoning capabilities in real-world settings.
Our analysis reveals that current RES models demonstrate substantial
performance deterioration when evaluated on WildRES. To address this challenge,
we introduce SynRES, an automated pipeline generating densely paired
compositional synthetic training data through three innovations: (1) a dense
caption-driven synthesis for attribute-rich image-mask-expression triplets, (2)
reliable semantic alignment mechanisms rectifying caption-pseudo mask
inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware
augmentations incorporating mosaic composition and superclass replacement to
emphasize generalization ability and distinguishing attributes over object
categories. Experimental results demonstrate that models trained with SynRES
achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and
3.8% on WildRES-DS. Code and datasets are available at
https://github.com/UTLLab/SynRES.

</details>


### [78] [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
*Jaewon Cheon, Pilsung Kang*

**主要类别:** cs.LG

**概要:** 大型语言模型的规模增长带来了显著的计算效率问题。本文提出了两种稀疏激活方法（M-COUNTDOWN 和 D-COUNTDOWN），分别通过间接和直接系数选择性地去活化非必要参数，从而减少前馈网络层的计算成本。实验表明，D-COUNTDOWN可减少90%的计算量且性能损失仅为5.5%，而M-COUNTDOWN相比现有方法提升了29.4%的性能保留率。专用内核实现进一步将理论优势转化为实际加速效果。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型规模的增长导致了显著的计算低效问题，因此需要一种新的方法来减少推理过程中的计算成本。

**方法:** 提出两种方法：M-COUNTDOWN（基于间接系数）和D-COUNTDOWN（基于直接系数）。这些方法利用线性组合形式的全局稀疏性，在FFNN层中选择性地去活化非必要参数。

**结果:** D-COUNTDOWN可以减少90%的计算量，理想情况下性能损失仅为5.5%；M-COUNTDOWN在无需预测器的情况下，比现有方法最多提高了29.4%的性能保留率。专用内核实现成功将理论收益转化为实际加速。

**结论:** 本文提出的稀疏激活方法有效减少了FFNN层的计算成本，并通过专用内核实现在实际应用中显著提升推理速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是COUNTDOWN%3A+Contextually+Sparse+Activation+Filtering+Out+Unnecessary+Weights+in+Down+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17701&send_immediately=true&force_search=false)

**原文摘要:** The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.

</details>


### [79] [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
*Li Lin, Xinyu Hu, Xiaojun Wan*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）在跨领域中表现出色，但在消费级GPU或个人设备上部署时面临高内存消耗和推理成本的问题。后训练量化（PTQ）提供了一种有希望的解决方案，可以减少LLMs的内存占用和解码延迟。均匀量化因其效率和易于部署而受到青睐。尽管最近在≥2位均匀量化方面的研究显著提高了量化后模型的性能，但量化参数的初始化仍依赖于次优的Min-Max策略。本文提出了一种名为NeUQI的方法，用于高效确定接近最优的均匀量化初始参数。实验表明，NeUQI在各种任务上始终优于现有方法，并且与轻量级蒸馏策略结合时，其性能优于资源密集型的PV-tuning方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在消费级硬件上的部署面临高内存消耗和推理成本的问题，需要一种有效的量化方法来解决这些问题，同时优化量化参数的初始化以进一步提升性能。

**方法:** 提出了NeUQI方法，专注于高效确定接近最优的均匀量化初始参数。该方法与现有的量化方法正交，可无缝集成。此外，NeUQI还可以与轻量级蒸馏策略结合使用。

**结果:** 在LLaMA和Qwen家族模型的各种任务实验中，NeUQI表现始终优于现有方法。与轻量级蒸馏策略结合时，其性能优于资源密集型的PV-tuning方法。

**结论:** NeUQI是一种有效的方法，能够确定接近最优的均匀量化初始参数，提升量化模型的性能，并且可以与现有量化方法和轻量级蒸馏策略无缝集成，为大型语言模型的高效部署提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeUQI%3A+Near-Optimal+Uniform+Quantization+Parameter+Initialization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17595&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training quantization (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform quantization representation is favored for its efficiency and ease of
deployment since uniform quantization is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
quantization have led to noticeable improvements in post-quantization model
performance; however, they primarily focus on quantization methodologies, while
the initialization of quantization parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
quantization. NeUQI is orthogonal to prior quantization methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.

</details>


### [80] [PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization](https://arxiv.org/abs/2505.17714)
*Ben Rahman*

**主要类别:** cs.LG

**概要:** PPO-BR提出了一种新的自适应RL方法，通过融合探索和收敛信号到单一有界信任区域，在保持简单性和理论保证的同时，超越了5个SOTA基线，并显著提高了收敛速度和稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管PPO在策略梯度方法中占据主导地位，但其静态信任区域限制了早期探索和后期收敛的平衡。为了解决这一问题，提出了PPO-BR以实现更好的探索与收敛机制。

**方法:** PPO-BR通过结合熵驱动扩展（epsilon up）和奖励引导收缩（epsilon down），将探索和收敛信号融入单一有界信任区域。此方法基于理论上可靠的创新，适用于各种强化学习环境。

**结果:** 在六个不同基准测试中，PPO-BR实现了29.1%更快的收敛速度，比PPO低2.3倍的奖励方差，且运行时开销不到1.8%，仅需更改五行代码。

**结论:** PPO-BR提供了一个统一的熵-奖励机制，适用于语言模型和通用强化学习环境，具备在关键安全领域（如外科手术机器人和自动驾驶无人机）部署的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PPO-BR%3A+Dual-Signal+Entropy-Reward+Adaptation+for+Trust+Region+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17714，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17714&send_immediately=true&force_search=false)

**原文摘要:** Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.

</details>


### [81] [Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs](https://arxiv.org/abs/2505.17599)
*Yusheng Zhao, Qixin Zhang, Xiao Luo, Weizhi Zhang, Zhiping Xiao, Wei Ju, Philip S. Yu, Ming Zhang*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为Dynamic Text Bundling Supervision (DENSE)的新方法，通过将文本打包查询大型语言模型（LLMs）以获取包级别的标签，并使用这些标签监督图神经网络的优化。该方法解决了LLMs在处理文本属性图时面临的两个主要挑战：图结构信息有限和响应不可靠。实验结果表明，该方法在十个数据集上有效。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）在零样本学习问题中表现出强大的泛化能力，但在文本属性图（TAGs）中的应用面临两大挑战：1）图结构信息有限；2）响应不可靠。现有的LLMs难以处理与图拓扑隔离的文本属性，并且由于信息不足和模型固有缺陷（如幻觉现象），预测结果往往不可靠。因此需要一种新方法来解决这些问题。

**方法:** 论文提出了一种名为Dynamic Text Bundling Supervision (DENSE)的方法。具体来说，该方法首先对节点进行采样，形成包含一系列具有相近文本的节点的包。然后，将打包的文本查询LLMs以获得每个包的标签。接下来，利用这些包标签监督图神经网络的优化过程，并进一步细化包以排除噪声项。此外，论文还提供了该方法的理论分析。

**结果:** 广泛的实验在十个数据集上验证了所提出方法的有效性。实验结果表明，DENSE方法能够显著提高模型性能，特别是在处理具有文本属性的图时。

**结论:** 本文提出了一种新的方法DENSE，通过将文本打包并查询LLMs获取标签来监督图神经网络的训练。这种方法有效地解决了LLMs在处理文本属性图时遇到的信息不足和响应不可靠的问题。实验结果证明了该方法在多个数据集上的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Text+Bundling+Supervision+for+Zero-Shot+Inference+on+Text-Attributed+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17599&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have been used in many zero-shot learning
problems, with their strong generalization ability. Recently, adopting LLMs in
text-attributed graphs (TAGs) has drawn increasing attention. However, the
adoption of LLMs faces two major challenges: limited information on graph
structure and unreliable responses. LLMs struggle with text attributes isolated
from the graph topology. Worse still, they yield unreliable predictions due to
both information insufficiency and the inherent weakness of LLMs (e.g.,
hallucination). Towards this end, this paper proposes a novel method named
Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of
texts to obtain bundle-level labels and uses these labels to supervise graph
neural networks. Specifically, we sample a set of bundles, each containing a
set of nodes with corresponding texts of close proximity. We then query LLMs
with the bundled texts to obtain the label of each bundle. Subsequently, the
bundle labels are used to supervise the optimization of graph neural networks,
and the bundles are further refined to exclude noisy items. To justify our
design, we also provide theoretical analysis of the proposed method. Extensive
experiments across ten datasets validate the effectiveness of the proposed
method.

</details>


### [82] [Adaptive Semantic Token Communication for Transformer-based Edge Inference](https://arxiv.org/abs/2505.17604)
*Alessio Devoto, Jary Pomponi, Mattia Merluzzi, Paolo Di Lorenzo, Simone Scardapane*

**主要类别:** cs.LG

**概要:** 本文提出了一种基于动态可配置Transformer的深度联合源信道编码（DJSCC）架构的自适应边缘推理框架。通过将输入数据标记为紧凑的高级语义表示，并通过Transformer进行优化，然后在嘈杂的无线信道上传输。系统采用语义标记选择机制，自适应地将信息特征压缩到用户指定数量的标记中，并通过JSCC模块进一步压缩。结合Lyapunov随机优化算法，增强在动态网络条件下的鲁棒性，平衡压缩效率和任务性能。实验结果表明，该系统在边缘智能应用的AI原生语义通信方面具有巨大潜力，且始终优于现有基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 在资源受限的边缘设备中进行目标导向的语义通信是实际需求，例如选择性地向边缘服务器传输用于对象检测的关键特征。这需要一种能够在变化的带宽和信道条件下高效传输任务相关数据的方法。

**方法:** 1. 输入数据被标记为紧凑的高级语义表示。
2. 通过Transformer对这些表示进行优化。
3. 使用语义标记选择机制自适应地压缩信息特征。
4. 利用JSCC模块进一步压缩标记。
5. 应用基于Lyapunov随机优化的资源分配算法来增强系统的鲁棒性并平衡压缩效率与任务性能。

**结果:** 实验结果表明，所提出的系统在不同条件下始终优于现有的基线方法。

**结论:** 所提出的自适应框架展示了其作为边缘智能应用中AI原生语义通信强大基础的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Semantic+Token+Communication+for+Transformer-based+Edge+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17604，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17604&send_immediately=true&force_search=false)

**原文摘要:** This paper presents an adaptive framework for edge inference based on a
dynamically configurable transformer-powered deep joint source channel coding
(DJSCC) architecture. Motivated by a practical scenario where a resource
constrained edge device engages in goal oriented semantic communication, such
as selectively transmitting essential features for object detection to an edge
server, our approach enables efficient task aware data transmission under
varying bandwidth and channel conditions. To achieve this, input data is
tokenized into compact high level semantic representations, refined by a
transformer, and transmitted over noisy wireless channels. As part of the DJSCC
pipeline, we employ a semantic token selection mechanism that adaptively
compresses informative features into a user specified number of tokens per
sample. These tokens are then further compressed through the JSCC module,
enabling a flexible token communication strategy that adjusts both the number
of transmitted tokens and their embedding dimensions. We incorporate a resource
allocation algorithm based on Lyapunov stochastic optimization to enhance
robustness under dynamic network conditions, effectively balancing compression
efficiency and task performance. Experimental results demonstrate that our
system consistently outperforms existing baselines, highlighting its potential
as a strong foundation for AI native semantic communication in edge
intelligence applications.

</details>


### [83] [MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization](https://arxiv.org/abs/2505.17745)
*Zeyuan Ma, Yue-Jiao Gong, Hongshu Guo, Wenjie Qiu, Sijie Ma, Hongqiao Lian, Jiajun Zhan, Kaixu Chen, Chen Wang, Zhiyang Huang, Zechuan Huang, Guojun Peng, Ran Cheng, Yining Ma*

**主要类别:** cs.LG

**概要:** MetaBox-v2 是 Meta-Black-Box Optimization (MetaBBO) 的升级版框架，提供了统一架构、高效并行化方案、全面基准测试套件和可扩展接口，支持多种优化方法，并通过系统案例研究展示了其内置基线的优化性能、泛化能力和学习效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的 MetaBox 框架虽然开创性地提供了基于强化学习的单目标 MetaBBO 开源框架，但其范围较窄，无法满足领域快速发展的需求。因此需要一个更全面、高效的框架来支持多种优化方法和场景。

**方法:** 1. 提出统一架构，支持强化学习、进化算法和基于梯度的方法；2. 实现高效的并行化方案以减少训练/测试时间；3. 构建包含18个任务（1900+实例）的综合基准套件；4. 提供丰富的可扩展接口用于自定义分析和集成外部工具。

**结果:** 通过系统案例研究，证明了 MetaBox-v2 内置基线在优化性能、泛化能力和学习效率方面的优越性，并为从业者和新手提供了有价值的见解。

**结论:** MetaBox-v2 是 MetaBBO 领域的一个重要里程碑，其功能和灵活性使其成为设计和评估优化算法的强大工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MetaBox-v2%3A+A+Unified+Benchmark+Platform+for+Meta-Black-Box+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17745&send_immediately=true&force_search=false)

**原文摘要:** Meta-Black-Box Optimization (MetaBBO) streamlines the automation of
optimization algorithm design through meta-learning. It typically employs a
bi-level structure: the meta-level policy undergoes meta-training to reduce the
manual effort required in developing algorithms for low-level optimization
tasks. The original MetaBox (2023) provided the first open-source framework for
reinforcement learning-based single-objective MetaBBO. However, its relatively
narrow scope no longer keep pace with the swift advancement in this field. In
this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a
milestone upgrade with four novel features: 1) a unified architecture
supporting RL, evolutionary, and gradient-based approaches, by which we
reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which
reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite
of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective,
multi-objective, multi-model, and multi-task optimization scenarios; 4)
plentiful and extensible interfaces for custom analysis/visualization and
integrating to external optimization tools/benchmarks. To show the utility of
MetaBox-v2, we carry out a systematic case study that evaluates the built-in
baselines in terms of the optimization performance, generalization ability and
learning efficiency. Valuable insights are concluded from thorough and detailed
analysis for practitioners and those new to the field.

</details>


### [84] [Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning](https://arxiv.org/abs/2505.17610)
*Till Freihaut, Luca Viano, Volkan Cevher, Matthieu Geist, Giorgia Ramponi*

**主要类别:** cs.LG

**概要:** 这篇论文首次提供了从专家数据中学习马尔可夫博弈纳什均衡的专家样本复杂度特征。研究发现，在非交互式模仿学习环境中，单策略偏差集中系数是不可避免的，并为行为克隆（BC）方法提供了包含该系数的上限。由于在高集中系数的游戏中，BC 方法表现出显著的遗憾值，因此我们利用专家查询开发了两种新的解决方案算法：MAIL-BRO 和 MURMAIL。前者使用最佳响应预言机，以 $\mathcal{O}(\varepsilon^{-4})$ 的专家和预言机查询次数学习到 $\varepsilon$-纳什均衡；后者完全绕过了最佳响应预言机，但代价是以更高的专家查询复杂度 $\mathcal{O}(\varepsilon^{-8})$ 为代价。最后，论文通过数值证据验证了理论结果。


<details>
  <summary>更多</summary>
  
**动机:** 为了理解如何从专家数据中有效地学习纳什均衡，以及解决在高集中系数游戏中的行为克隆遗憾问题。

**方法:** 提出了一种新量——单策略偏差集中系数，并分析其在非交互式模仿学习环境中的影响。基于此，开发了两种新型算法 MAIL-BRO 和 MURMAIL，分别依赖于最佳响应预言机或避免它，同时评估了它们的专家查询复杂度。

**结果:** MAIL-BRO 算法能够以 $\mathcal{O}(\varepsilon^{-4})$ 的查询次数学习到 $\varepsilon$-纳什均衡，而 MURMAIL 则以较高的 $\mathcal{O}(\varepsilon^{-8})$ 查询复杂度实现相同目标。实验结果支持了理论分析。

**结论:** 在马尔可夫博弈中，单策略偏差集中系数是学习纳什均衡的关键因素。提出的两种算法提供了不同的权衡选择，可以依据实际需求选择适合的算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Equilibria+from+Data%3A+Provably+Efficient+Multi-Agent+Imitation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17610，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17610&send_immediately=true&force_search=false)

**原文摘要:** This paper provides the first expert sample complexity characterization for
learning a Nash equilibrium from expert data in Markov Games. We show that a
new quantity named the single policy deviation concentrability coefficient is
unavoidable in the non-interactive imitation learning setting, and we provide
an upper bound for behavioral cloning (BC) featuring such coefficient. BC
exhibits substantial regret in games with high concentrability coefficient,
leading us to utilize expert queries to develop and introduce two novel
solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response
oracle and learns an $\varepsilon$-Nash equilibrium with
$\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses
completely the best response oracle at the cost of a worse expert query
complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide
numerical evidence, confirming our theoretical findings.

</details>


### [85] [Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning](https://arxiv.org/abs/2505.17749)
*Ghada Sokar, Pablo Samuel Castro*

**主要类别:** cs.LG

**概要:** 在基于像素的环境中扩展深度强化学习是一项重大挑战，通常会导致性能下降。通过分析，本文确定编码器输出与后续密集层之间的连接（称为瓶颈）是限制扩展能力的主要因素，并提出全局平均池化作为一种简单有效的解决方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已有研究提出了算法和架构方面的解决方案以应对基于像素的深度强化学习扩展问题，但性能下降的根本原因仍未明确。

**方法:** 识别出编码器输出与密集层之间的连接为限制扩展能力的主要瓶颈，并提出使用全局平均池化来解决这一问题，而非采用更复杂的先前方法。

**结果:** 证明了全局平均池化可以有效缓解瓶颈问题，同时避免了之前方法的复杂性。

**结论:** 全局平均池化是一种简单且有效的方法，可用于改善基于像素的深度强化学习模型的扩展能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mind+the+GAP%21+The+Challenges+of+Scale+in+Pixel-based+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17749，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17749&send_immediately=true&force_search=false)

**原文摘要:** Scaling deep reinforcement learning in pixel-based environments presents a
significant challenge, often resulting in diminished performance. While recent
works have proposed algorithmic and architectural approaches to address this,
the underlying cause of the performance drop remains unclear. In this paper, we
identify the connection between the output of the encoder (a stack of
convolutional layers) and the ensuing dense layers as the main underlying
factor limiting scaling capabilities; we denote this connection as the
bottleneck, and we demonstrate that previous approaches implicitly target this
bottleneck. As a result of our analyses, we present global average pooling as a
simple yet effective way of targeting the bottleneck, thereby avoiding the
complexity of earlier approaches.

</details>


### [86] [Large language model as user daily behavior data generator: balancing population diversity and individual personality](https://arxiv.org/abs/2505.17615)
*Haoxin Li, Jingtao Ding, Jiahui Gong, Yong Li*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种名为BehaviorGen的框架，该框架利用大型语言模型（LLMs）生成高质量的合成行为数据。通过基于用户档案和真实事件模拟用户行为，BehaviorGen支持行为预测模型中的数据增强和替换。在涉及增强、微调替换和增强的情景中评估其性能，人类移动性和智能手机使用预测取得了高达18.9%的显著改进。结果表明，BehaviorGen通过灵活且保护隐私的合成数据生成增强了用户行为建模的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 由于日常行为模式的复杂性以及短期波动的存在，预测人类日常行为是一项具有挑战性的任务。虽然数据驱动模型通过利用来自各种平台和设备的经验数据改善了行为预测，但对敏感、大规模用户数据的依赖引发了隐私问题并限制了数据可用性。合成数据生成作为一种有前途的解决方案出现，但现有方法通常仅限于特定应用。

**方法:** 论文介绍了一种名为BehaviorGen的框架，该框架使用大型语言模型（LLMs）生成高质量的合成行为数据。通过根据用户档案和真实事件模拟用户行为，BehaviorGen支持行为预测模型中的数据增强和替换。

**结果:** 在涉及增强、微调替换和增强的情景中评估其性能，人类移动性和智能手机使用预测取得了高达18.9%的显著改进。

**结论:** 实验结果证明了BehaviorGen通过灵活且保护隐私的合成数据生成来增强用户行为建模的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+language+model+as+user+daily+behavior+data+generator%3A+balancing+population+diversity+and+individual+personality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17615，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17615&send_immediately=true&force_search=false)

**原文摘要:** Predicting human daily behavior is challenging due to the complexity of
routine patterns and short-term fluctuations. While data-driven models have
improved behavior prediction by leveraging empirical data from various
platforms and devices, the reliance on sensitive, large-scale user data raises
privacy concerns and limits data availability. Synthetic data generation has
emerged as a promising solution, though existing methods are often limited to
specific applications. In this work, we introduce BehaviorGen, a framework that
uses large language models (LLMs) to generate high-quality synthetic behavior
data. By simulating user behavior based on profiles and real events,
BehaviorGen supports data augmentation and replacement in behavior prediction
models. We evaluate its performance in scenarios such as pertaining
augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving
significant improvements in human mobility and smartphone usage predictions,
with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen
to enhance user behavior modeling through flexible and privacy-preserving
synthetic data generation.

</details>


### [87] [But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors](https://arxiv.org/abs/2505.17760)
*Leon Eshuijs, Archie Chaudhury, Alan McBeth, Ethan Nguyen*

**主要类别:** cs.LG

**概要:** 近期对大型语言模型（LLMs）的安全评估显示，许多模型表现出不诚实的行为，例如谄媚。然而，大多数诚信基准仅专注于事实知识或明显有害行为，并依赖外部评判者，这些评判者通常无法检测到不太明显的不诚实形式。在本研究中，我们引入了一个新的框架——利用安全导向替代方案的评判（JUSSA），该框架利用在单个样本上训练的转向量，以引出模型更诚实的回答，帮助LLM评判者检测不诚实行为。为了测试我们的框架，我们引入了一个新的操控数据集，其中包含专门设计用于引出欺骗性回答的提示。我们发现，JUSSA使LLM评判者能够更好地分辨不诚实和良性回答，并帮助他们识别微妙的操控行为实例。


<details>
  <summary>更多</summary>
  
**动机:** 当前对大型语言模型的安全评估显示了模型的不诚实行为，但现有的评估方法主要关注显而易见的事实错误或有害行为，难以捕捉较隐蔽的不诚实行为。因此，需要一个新框架来改进这一问题。

**方法:** 引入了名为Judge Using Safety-Steered Alternatives (JUSSA)的新框架，使用转向量训练单一样本，从而引导模型产生更诚实的回答，并通过一个新的操控数据集进行测试，此数据集包含特定设计的提示以引出欺骗性回答。

**结果:** 实验结果表明，JUSSA框架有助于LLM评判者更好地分辨不诚实与良性回答，同时能识别出较为微妙的操控行为。

**结论:** JUSSA框架可以有效提升LLM评判者识别不诚实行为的能力，尤其是在检测微妙操控行为方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是But+what+is+your+honest+answer%3F+Aiding+LLM-judges+with+honest+alternatives+using+steering+vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17760&send_immediately=true&force_search=false)

**原文摘要:** Recent safety evaluations of Large Language Models (LLMs) show that many
models exhibit dishonest behavior, such as sycophancy. However, most honesty
benchmarks focus exclusively on factual knowledge or explicitly harmful
behavior and rely on external judges, which are often unable to detect less
obvious forms of dishonesty. In this work, we introduce a new framework, Judge
Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors
trained on a single sample to elicit more honest responses from models, helping
LLM-judges in the detection of dishonest behavior. To test our framework, we
introduce a new manipulation dataset with prompts specifically designed to
elicit deceptive responses. We find that JUSSA enables LLM judges to better
differentiate between dishonest and benign responses, and helps them identify
subtle instances of manipulative behavior.

</details>


### [88] [Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](https://arxiv.org/abs/2505.17621)
*Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, Xiangyu Zhao*

**主要类别:** cs.LG

**概要:** i-MENTOR 是一种新的强化学习方法，通过引入轨迹感知探索奖励、动态奖励缩放和优势保持奖励实现稠密奖励和增强探索，有效提升大型语言模型在复杂推理任务中的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习方法（如 PPO 和 GRPO）依赖稀疏奖励机制，在激励探索方面存在不足，导致多步推理过程的低效指导，特别是在复杂推理任务中。

**方法:** 提出了一种名为 i-MENTOR 的新方法，包含三个关键创新：1) 轨迹感知探索奖励以减少标记级策略偏差并保持计算效率；2) 动态奖励缩放以稳定大规模动作空间中的探索与利用；3) 优势保持奖励实现探索性指导的同时维护优势分布完整性。

**结果:** 实验结果表明，i-MENTOR 在三个公开数据集上表现出有效性，尤其在 Countdown-4 数据集上实现了 22.39% 的性能提升。

**结论:** i-MENTOR 方法通过稠密奖励和增强探索显著提高了强化学习训练范式下大型语言模型的推理能力，解决了现有方法在复杂推理任务中的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Navigate+the+Unknown%3A+Enhancing+LLM+Reasoning+with+Intrinsic+Motivation+Guided+Exploration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17621，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17621&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
sparse outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, sparse reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.

</details>


### [89] [Hyperparameter Optimization via Interacting with Probabilistic Circuits](https://arxiv.org/abs/2505.17804)
*Jonas Seng, Fabrizio Ventola, Zhongjie Yu, Kristian Kersting*

**主要类别:** cs.LG

**概要:** 本研究提出一种新的贝叶斯优化方法，使用概率电路作为代理模型，解决了现有方法无法准确反映用户信念的问题，实验表明其在标准和交互式超参数优化中均达到或超越了当前最优水平。


<details>
  <summary>更多</summary>
  
**动机:** 尽管人们对设计真正交互式的超参数优化（HPO）方法越来越感兴趣，但迄今为止，只有少数方法能够包含人类反馈。现有的交互式贝叶斯优化（BO）方法通过将获取函数与用户定义的先验分布加权来融入人类信念。然而，在BO中普遍存在的非平凡内部优化问题使得这种加权方案并不总是能准确反映给定的用户信念。

**方法:** 我们引入了一种新的BO方法，利用可处理的概率模型——概率电路（PCs）作为代理模型。PCs对混合超参数空间和评估分数编码了一个可处理的联合分布。它们支持精确的条件推理和采样。基于条件采样，我们构建了一种新的选择策略，该策略无需获取函数即可生成候选点（从而消除了额外的内循环优化需求），并确保用户信念在选择策略中得到准确反映。

**结果:** 我们进行了理论分析和广泛的实证评估，结果表明我们的方法在标准HPO中达到了最先进的性能，并在交互式HPO中优于交互式BO基线。

**结论:** 提出了一种新的贝叶斯优化方法，使用概率电路作为代理模型，可以准确反映用户信念，并在标准和交互式超参数优化中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hyperparameter+Optimization+via+Interacting+with+Probabilistic+Circuits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17804，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17804&send_immediately=true&force_search=false)

**原文摘要:** Despite the growing interest in designing truly interactive hyperparameter
optimization (HPO) methods, to date, only a few allow to include human
feedback. Existing interactive Bayesian optimization (BO) methods incorporate
human beliefs by weighting the acquisition function with a user-defined prior
distribution. However, in light of the non-trivial inner optimization of the
acquisition function prevalent in BO, such weighting schemes do not always
accurately reflect given user beliefs. We introduce a novel BO approach
leveraging tractable probabilistic models named probabilistic circuits (PCs) as
a surrogate model. PCs encode a tractable joint distribution over the hybrid
hyperparameter space and evaluation scores. They enable exact conditional
inference and sampling. Based on conditional sampling, we construct a novel
selection policy that enables an acquisition function-free generation of
candidate points (thereby eliminating the need for an additional inner-loop
optimization) and ensures that user beliefs are reflected accurately in the
selection policy. We provide a theoretical analysis and an extensive empirical
evaluation, demonstrating that our method achieves state-of-the-art performance
in standard HPO and outperforms interactive BO baselines in interactive HPO.

</details>


### [90] [Leveraging Stochastic Depth Training for Adaptive Inference](https://arxiv.org/abs/2505.17626)
*Guilherme Korol, Antonio Carlos Schneider Beck, Jeronimo Castrillon*

**主要类别:** cs.LG

**概要:** 提出了一种零开销、单模型且时间可预测的自适应推理方法，利用随机深度训练模型选择接近帕累托最优的跳层配置，在几乎不影响准确率的情况下提升能效。


<details>
  <summary>更多</summary>
  
**动机:** 动态DNN优化技术（如跳层）虽提高了适应性和效率，但也带来了更大的内存占用、更复杂的训练过程以及性能质量难以控制的问题。

**方法:** 观察到使用随机深度训练的模型在推理时对任意跳层更具鲁棒性，从而提出从随机训练的模型中选择接近帕累托最优的跳层配置以实现运行时自适应推理。

**结果:** 与原始ResNet相比，该方法在准确率仅下降0.71%的情况下，能效提升了2倍。

**结论:** 提出的方法通过利用随机深度训练模型实现了高效的自适应推理，并显著提升了能效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Stochastic+Depth+Training+for+Adaptive+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17626&send_immediately=true&force_search=false)

**原文摘要:** Dynamic DNN optimization techniques such as layer-skipping offer increased
adaptability and efficiency gains but can lead to i) a larger memory footprint
as in decision gates, ii) increased training complexity (e.g., with
non-differentiable operations), and iii) less control over performance-quality
trade-offs due to its inherent input-dependent execution. To approach these
issues, we propose a simpler yet effective alternative for adaptive inference
with a zero-overhead, single-model, and time-predictable inference. Central to
our approach is the observation that models trained with Stochastic Depth -- a
method for faster training of residual networks -- become more resilient to
arbitrary layer-skipping at inference time. We propose a method to first select
near Pareto-optimal skipping configurations from a stochastically-trained model
to adapt the inference at runtime later. Compared to original ResNets, our
method shows improvements of up to 2X in power efficiency at accuracy drops as
low as 0.71%.

</details>


### [91] [Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning](https://arxiv.org/abs/2505.17830)
*Nicolas Castanet, Olivier Sigaud, Sylvain Lamprier*

**主要类别:** cs.LG

**概要:** Goal-Conditioned Reinforcement Learning (GCRL) struggles in visual environments due to high-dimensional observations. Classical approaches may over-represent frequently visited states, limiting skill diversity. This paper proposes DRAG, a method combining β-VAE with Distributionally Robust Optimization, progressively enforcing distributional shifts towards uniform coverage of the full state space. DRAG improves state space coverage and control performance in hard exploration environments without pre-training or prior knowledge.


<details>
  <summary>更多</summary>
  
**动机:** GCRL faces challenges in visual environments with high-dimensional observations. Classical auto-encoder based methods may converge to latent spaces that over-represent frequently visited states, leading to limited skill diversity when sampling goals from the latent space.

**方法:** DRAG progressively enforces distributional shifts towards a uniform distribution over the full state space by combining the β-VAE framework with Distributionally Robust Optimization. It uses an adversarial neural weighter to account for mismatches between current data distribution and unseen parts of the environment.

**结果:** DRAG improves state space coverage and downstream control performance in hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training or prior environment knowledge.

**结论:** DRAG addresses the issue of limited skill diversity in GCRL by ensuring full coverage of skills that can be learned in the environment, improving performance in challenging exploration scenarios.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Imagine+Beyond%21+Distributionally+Robust+Auto-Encoding+for+State+Space+Coverage+in+Online+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17830，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17830&send_immediately=true&force_search=false)

**原文摘要:** Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously
acquire diverse behaviors, but faces major challenges in visual environments
due to high-dimensional, semantically sparse observations. In the online
setting, where agents learn representations while exploring, the latent space
evolves with the agent's policy, to capture newly discovered areas of the
environment. However, without incentivization to maximize state coverage in the
representation, classical approaches based on auto-encoders may converge to
latent spaces that over-represent a restricted set of states frequently visited
by the agent. This is exacerbated in an intrinsic motivation setting, where the
agent uses the distribution encoded in the latent space to sample the goals it
learns to master. To address this issue, we propose to progressively enforce
distributional shifts towards a uniform distribution over the full state space,
to ensure a full coverage of skills that can be learned in the environment. We
introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that
combines the $\beta$-VAE framework with Distributionally Robust Optimization.
DRAG leverages an adversarial neural weighter of training states of the VAE, to
account for the mismatch between the current data distribution and unseen parts
of the environment. This allows the agent to construct semantically meaningful
latent spaces beyond its immediate experience. Our approach improves state
space coverage and downstream control performance on hard exploration
environments such as mazes and robotic control involving walls to bypass,
without pre-training nor prior environment knowledge.

</details>


### [92] [TransDF: Time-Series Forecasting Needs Transformed Label Alignment](https://arxiv.org/abs/2505.17847)
*Hao Wang, Licheng Pan, Zhichao Chen, Xu Chen, Qingyang Dai, Lei Wang, Haoxuan Li, Zhouchen Lin*

**主要类别:** cs.LG

**概要:** 提出了一种新的时间序列预测模型训练方法TransDF，通过转换标签序列解决现有方法中的问题，实现更优性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列预测模型主要使用时间均方误差作为学习目标，但存在标签自相关和任务量过多的问题。

**方法:** 提出了Transform-enhanced Direct Forecast (TransDF)，将标签序列转换为去相关的组件，并根据其重要性进行区分，模型专注于对齐最重要的组件。

**结果:** 广泛的实验表明，TransDF达到了最先进的性能，并且可以与各种预测模型兼容。

**结论:** TransDF有效地减轻了标签自相关问题并减少了任务量，提高了时间序列预测模型的训练效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TransDF%3A+Time-Series+Forecasting+Needs+Transformed+Label+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17847，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17847&send_immediately=true&force_search=false)

**原文摘要:** Training time-series forecasting models presents unique challenges in
designing effective learning objectives. Existing methods predominantly utilize
the temporal mean squared error, which faces two critical challenges: (1) label
autocorrelation, which leads to bias from the label sequence likelihood; (2)
excessive amount of tasks, which increases with the forecast horizon and
complicates optimization. To address these challenges, we propose
Transform-enhanced Direct Forecast (TransDF), which transforms the label
sequence into decorrelated components with discriminated significance. Models
are trained to align the most significant components, thereby effectively
mitigating label autocorrelation and reducing task amount. Extensive
experiments demonstrate that TransDF achieves state-of-the-art performance and
is compatible with various forecasting models. Code is available at
https://anonymous.4open.science/r/TransDF-88CF.

</details>


### [93] [Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach](https://arxiv.org/abs/2505.17637)
*Yuting Huang, Ziquan Fang, Zhihao Zeng, Lu Chen, Yunjun Gao*

**主要类别:** cs.LG

**概要:** 提出E^2-CSTP框架，通过跨模态注意机制、双分支因果推断方法和GCN与Mamba架构结合，在4个真实数据集上显著提升时空预测准确率并降低计算开销。


<details>
  <summary>更多</summary>
  
**动机:** 当前时空预测中多模态数据融合不足、因果关系受混淆因素影响以及模型计算复杂度过高。

**方法:** E^2-CSTP利用跨模态注意力和门控机制融合多模态数据，设计双分支因果推理方法（主分支专注时空预测，辅助分支通过建模其他模态和因果干预揭示真实因果依赖），并结合GCN与Mamba架构加速时空编码。

**结果:** 在4个真实世界数据集上进行的广泛实验表明，E^2-CSTP显著超越9种最先进的方法，准确率最高提升9.66%，计算开销减少17.37%-56.11%。

**结论:** E^2-CSTP有效解决了多模态信息融合不足、因果关系混淆和计算复杂度高的问题，为智能交通、天气预报和城市规划提供了更高效准确的时空预测方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Spatio-Temporal+Prediction%3A+An+Effective+and+Efficient+Multi-Modal+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17637，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17637&send_immediately=true&force_search=false)

**原文摘要:** Spatio-temporal prediction plays a crucial role in intelligent
transportation, weather forecasting, and urban planning. While integrating
multi-modal data has shown potential for enhancing prediction accuracy, key
challenges persist: (i) inadequate fusion of multi-modal information, (ii)
confounding factors that obscure causal relations, and (iii) high computational
complexity of prediction models. To address these challenges, we propose
E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal
Prediction framework. E^2-CSTP leverages cross-modal attention and gating
mechanisms to effectively integrate multi-modal data. Building on this, we
design a dual-branch causal inference approach: the primary branch focuses on
spatio-temporal prediction, while the auxiliary branch mitigates bias by
modeling additional modalities and applying causal interventions to uncover
true causal dependencies. To improve model efficiency, we integrate GCN with
the Mamba architecture for accelerated spatio-temporal encoding. Extensive
experiments on 4 real-world datasets show that E^2-CSTP significantly
outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in
accuracy as well as 17.37%-56.11% reductions in computational overhead.

</details>


### [94] [Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization](https://arxiv.org/abs/2505.17852)
*Francois Chaubard, Mykel Kochenderfer*

**主要类别:** cs.LG

**概要:** 本文提出用零阶优化方法（ZOO）如随机向量梯度估计（RGE）替代BPTT训练RNN，可以显著减少内存和成本消耗，同时保持甚至超过BPTT的收敛速度和性能。中心差分RGE（CD-RGE）通过优化平滑代理损失，内在地正则化训练并改善泛化能力。实验表明，在过拟合、转导和语言建模三个任务中，该方法匹配或超越了BPTT的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管RNN在推理时具有优势，但使用BPTT训练大规模RNN在长上下文中仍然不切实际，因为BPTT需要保留所有中间激活，导致内存使用随上下文长度和模型大小线性增长。因此，研究者希望找到一种更高效的训练方法。

**方法:** 利用零阶优化方法（ZOO），例如随机向量梯度估计（RGE），替代传统的BPTT方法来训练RNN。在整个训练过程中，模型保持推理模式，从而显著降低内存和成本消耗。此外，还探讨了中央差分RGE（CD-RGE）对平滑代理损失的优化及其对训练的正则化作用。

**结果:** 在过拟合、转导和语言建模三个任务中，所提出的方法与BPTT相比，收敛速度提高了19倍，且模型泛化能力相当或更好。虽然每步需要更多的前向传播，但通过如FlashRNN和分布式推理等最新进展，可以超越BPTT的每步时间效率。

**结论:** 零阶优化方法（如RGE）为训练大规模RNN提供了一种高效且低成本的替代方案，其性能可媲美甚至优于传统的BPTT方法，并且在某些情况下能显著加速训练过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Recurrent+Neural+Networks+to+a+Billion+Parameters+with+Zero-Order+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17852&send_immediately=true&force_search=false)

**原文摘要:** During inference, Recurrent Neural Networks (RNNs) scale constant in both
FLOPs and GPU memory with increasing context length, as they compress all prior
tokens into a fixed-size memory. In contrast, transformers scale linearly in
FLOPs and, at best, linearly in memory during generation, since they must
attend to all previous tokens explicitly. Despite this inference-time
advantage, training large RNNs on long contexts remains impractical because
standard optimization methods depend on Backpropagation Through Time (BPTT).
BPTT requires retention of all intermediate activations during the forward
pass, causing memory usage to scale linearly with both context length and model
size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as
Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train
RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while
using orders of magnitude less memory and cost, as the model remains in
inference mode throughout training. We further demonstrate that
Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate
loss, inherently regularizing training and improving generalization. Our method
matches or outperforms BPTT across three settings: (1) overfitting, (2)
transduction, and (3) language modeling. Across all tasks, with sufficient
perturbations, our models generalize as well as or better than those trained
with BPTT, often in fewer steps. Despite the need for more forward passes per
step, we can surpass BPTT wall-clock time per step using recent advancements
such as FlashRNN and distributed inference.

</details>


### [95] [Stochastic Weight Sharing for Bayesian Neural Networks](https://arxiv.org/abs/2505.17856)
*Moule Lin, Shuhao Guan, Weipeng Jing, Goetz Botterweck, Andrea Patane*

**主要类别:** cs.LG

**概要:** 通过重新解释权重共享量化技术并结合2D自适应高斯分布、Wasserstein距离估计和alpha混合，提出了一种降低维度的软高斯表示方法，用于高效训练大型贝叶斯神经网络（BNN）。此方法显著减少了计算开销，并在压缩模型参数和大小方面表现出色，同时保持了与现有最佳方法相当的准确性和不确定性估计。


<details>
  <summary>更多</summary>
  
**动机:** 当前贝叶斯神经网络（BNN）的应用受到计算需求增加和训练深层架构时收敛困难的限制。为了克服这些挑战，研究者希望找到一种方法来降低BNN的计算复杂度，同时保持其性能。

**方法:** 研究者从随机角度重新解释了权重共享量化技术，并使用2D自适应高斯分布、Wasserstein距离估计和alpha混合来编码BNN的行为，形成一种低维软高斯表示。这种方法允许更高效的贝叶斯学习和大规模模型训练。

**结果:** 该方法成功将模型参数压缩约50倍，模型大小减少75倍，同时在CIFAR10、CIFAR100和ImageNet1k等多个计算机视觉基准上实现了与现有最佳方法相当的准确性和不确定性估计。

**结论:** 这种新的软高斯表示方法显著降低了贝叶斯学习的计算开销，使得可以高效地训练大型模型如ResNet-101和Vision Transformer (VIT)，并且在性能上与最先进的方法持平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+Weight+Sharing+for+Bayesian+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17856&send_immediately=true&force_search=false)

**原文摘要:** While offering a principled framework for uncertainty quantification in deep
learning, the employment of Bayesian Neural Networks (BNNs) is still
constrained by their increased computational requirements and the convergence
difficulties when training very deep, state-of-the-art architectures. In this
work, we reinterpret weight-sharing quantization techniques from a stochastic
perspective in the context of training and inference with Bayesian Neural
Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,
Wasserstein distance estimations, and alpha blending to encode the stochastic
behaviour of a BNN in a lower dimensional, soft Gaussian representation.
Through extensive empirical investigation, we demonstrate that our approach
significantly reduces the computational overhead inherent in Bayesian learning
by several orders of magnitude, enabling the efficient Bayesian training of
large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various
computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our
approach compresses model parameters by approximately 50x and reduces model
size by 75, while achieving accuracy and uncertainty estimations comparable to
the state-of-the-art.

</details>


### [96] [PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval](https://arxiv.org/abs/2505.17639)
*Zehua Pei, Ying Zhang, Hui-Ling Zhen, Xianzhi Yu, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu*

**主要类别:** cs.LG

**概要:** 本研究提出了PreMoe框架，通过概率专家修剪（PEP）和任务自适应专家检索（TAER）来高效部署大规模MoE模型于内存受限环境。实验表明，该方法显著降低了内存占用，并在不同模型上保持了高准确率。


<details>
  <summary>更多</summary>
  
**动机:** 混合专家（MoE）架构虽然能够扩展大型语言模型而无需成比例增加计算成本，但其巨大的内存需求限制了在各种计算环境中的部署。因此需要一种方法来降低MoE模型的内存消耗，同时保持性能。

**方法:** PreMoe框架包含两个主要部分：1) 概率专家修剪(PEP)，使用任务条件下的预期选择分数(TCESS)量化专家重要性，从而识别关键专家的最小集合；2) 任务自适应专家检索(TAER)，预先计算并存储任务特定的专家模式，在推理时快速匹配并加载与任务最相关的专家子集。

**结果:** DeepSeek-R1 671B在8/128配置下保留97.2%的MATH500准确率（减少50%专家），在更激进的8/32配置下仍达到72.0%的准确率。Pangu-Ultra-MoE 718B在8/128配置下分别在MATH500和AIME24数据集上取得97.15%和81.3%的准确率，而在4/64配置下（390GB内存）依然保持96.95%的MATH500准确率。

**结论:** PreMoe框架成功地减少了MoE模型的内存占用，同时保持了较高的任务性能，使得这些模型可以更广泛地部署在内存受限的环境中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PreMoe%3A+Lightening+MoEs+on+Constrained+Memory+by+Expert+Pruning+and+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17639&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-experts (MoE) architectures enable scaling large language models
(LLMs) to vast parameter counts without a proportional rise in computational
costs. However, the significant memory demands of large MoE models hinder their
deployment across various computational environments, from cloud servers to
consumer devices. This study first demonstrates pronounced task-specific
specialization in expert activation patterns within MoE layers. Building on
this, we introduce PreMoe, a novel framework that enables efficient deployment
of massive MoE models in memory-constrained environments. PreMoe features two
main components: probabilistic expert pruning (PEP) and task-adaptive expert
retrieval (TAER). PEP employs a new metric, the task-conditioned expected
selection score (TCESS), derived from router logits to quantify expert
importance for specific tasks, thereby identifying a minimal set of critical
experts. TAER leverages these task-specific expert importance profiles for
efficient inference. It pre-computes and stores compact expert patterns for
diverse tasks. When a user query is received, TAER rapidly identifies the most
relevant stored task pattern and reconstructs the model by loading only the
small subset of experts crucial for that task. This approach dramatically
reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B
maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\%
expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning
(87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and
81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64
(390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly
available at https://github.com/JarvisPei/PreMoe.

</details>


### [97] [A Network Science Approach to Granular Time Series Segmentation](https://arxiv.org/abs/2505.17640)
*Ivana Kesić, Carolina Fortuna, Mihael Mohorčič, Blaž Bertalanič*

**主要类别:** cs.LG

**概要:** 论文提出了一种新的时间序列分割（TSS）方法，通过将时间序列转换为加权双视角可见性图（WDPVG），结合图注意力网络（GAT）进行分析。此方法能捕捉数据的不同结构特征，有效识别有意义的时间序列段，并在59个基准数据集上达到了平均F1分数0.97的优秀表现。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列分割技术相较于其他时间序列相关任务，受到的关注较少。现有的深度学习架构依赖于滑动窗口，由于固定的窗口大小和步幅限制了分割的精细度。因此需要一种更精细的时间序列分割方法。

**方法:** 该方法首先将时间序列转换为加权双视角可见性图（WDPVG），然后利用图注意力网络（GAT）进行节点分类以实现时间序列分割。通过这种方式，可以捕捉到时间序列中隐藏的不同结构特征。此外，还对不同的时间序列到图的转换方法进行了实验与性能比较。

**结果:** 在59个多样化的TSS基准数据集上，该方法实现了平均F1分数0.97的成绩，比seq2point基线方法高出0.05的F1分数，并且减少了所需的训练数据量。

**结论:** 本研究提出了将时间序列分割问题转化为图上的节点分类问题，进行了详尽的TS-to-graph转换分析，首次深入探讨了利用图神经网络（GNNs）分析时间序列图表示的方法，并证明了所提出方法的有效性和优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Network+Science+Approach+to+Granular+Time+Series+Segmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17640&send_immediately=true&force_search=false)

**原文摘要:** Time series segmentation (TSS) is one of the time series (TS) analysis
techniques, that has received considerably less attention compared to other TS
related tasks. In recent years, deep learning architectures have been
introduced for TSS, however their reliance on sliding windows limits
segmentation granularity due to fixed window sizes and strides. To overcome
these challenges, we propose a new more granular TSS approach that utilizes the
Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines
it with a Graph Attention Network (GAT). By transforming TS into graphs, we are
able to capture different structural aspects of the data that would otherwise
remain hidden. By utilizing the representation learning capabilities of Graph
Neural Networks, our method is able to effectively identify meaningful segments
within the TS. To better understand the potential of our approach, we also
experimented with different TS-to-graph transformations and compared their
performance. Our contributions include: a) formulating the TSS as a node
classification problem on graphs; b) conducting an extensive analysis of
various TS- to-graph transformations applied to TSS using benchmark datasets
from the TSSB repository; c) providing the first detailed study on utilizing
GNNs for analyzing graph representations of TS in the context of TSS; d)
demonstrating the effectiveness of our method, which achieves an average F1
score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the
seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the
required training data compared to the baseline methods.

</details>


### [98] [Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting](https://arxiv.org/abs/2505.17872)
*Licheng Pan, Zhichao Chen, Haoxuan Li, Guangyi Liu, Zhijian Xu, Zhaoran Liu, Hao Wang, Ying Wei*

**主要类别:** cs.LG

**概要:** 多任务时间序列预测存在表达瓶颈问题，导致不可避免的误差。本文提出两阶段框架解决此问题：首先预训练一个基础模型进行一步预测，然后使用步长特定的LoRA模块进行适配。进一步引入MoLA模型，通过自适应加权LoRA专家实现跨步的部分参数共享，提高效率和预测性能。实验表明，MoLA显著提升模型表达能力和预测效果，优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 多任务时间序列预测方法尽管广泛使用，但其存在表达瓶颈问题，即不同时间步长的预测共享相同表示，即使在最优表示下也会产生不可避免的误差。

**方法:** 提出一种两阶段框架：1) 预训练基础模型用于一步预测；2) 使用步长特定的LoRA模块进行适配。此外，引入Mixture-of-LoRA (MoLA) 模型，利用自适应加权LoRA专家实现跨步的部分参数共享，以利用步骤间的相互依赖关系。

**结果:** 实验结果表明，MoLA模型显著提高了模型的表达能力，并在时间序列预测任务中超越了现有的最先进方法。

**结论:** 提出的两阶段框架和MoLA模型有效解决了多任务时间序列预测中的表达瓶颈问题，提升了预测性能和效率，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mixture+of+Low+Rank+Adaptation+with+Partial+Parameter+Sharing+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17872，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17872&send_immediately=true&force_search=false)

**原文摘要:** Multi-task forecasting has become the standard approach for time-series
forecasting (TSF). However, we show that it suffers from an Expressiveness
Bottleneck, where predictions at different time steps share the same
representation, leading to unavoidable errors even with optimal
representations. To address this issue, we propose a two-stage framework:
first, pre-train a foundation model for one-step-ahead prediction; then, adapt
it using step-specific LoRA modules.This design enables the foundation model to
handle any number of forecast steps while avoiding the expressiveness
bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which
employs adaptively weighted LoRA experts to achieve partial parameter sharing
across steps. This approach enhances both efficiency and forecasting
performance by exploiting interdependencies between forecast steps. Experiments
show that MoLA significantly improves model expressiveness and outperforms
state-of-the-art time-series forecasting methods. Code is available at
https://anonymous.4open.science/r/MoLA-BC92.

</details>


### [99] [Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives](https://arxiv.org/abs/2505.17646)
*Huanran Chen, Yinpeng Dong, Zeming Wei, Yao Huang, Yichi Zhang, Hang Su, Jun Zhu*

**主要类别:** cs.LG

**概要:** 近期研究揭示了大规模语言模型的损失景观类似一个盆地，在此盆地内模型表现几乎相同，而盆地外则失去所有能力。本研究进一步探讨了预训练生成的基本能力盆地和微调生成的具体能力盆地，并分析了最常见情况与最差情况下的损失景观。理论上证明了最常见盆地大小可限制最差盆地大小及输入扰动的鲁棒性，同时指出通过模型的超参数化特性可以轻松将盆地扩大五倍。


<details>
  <summary>更多</summary>
  
**动机:** 深入理解大规模语言模型的损失景观，尤其是预训练和微调如何影响模型的能力盆地，以及这些盆地对模型性能和鲁棒性的影响。

**方法:** 研究大规模语言模型的损失景观，识别预训练产生的基本能力盆地和微调产生的具体能力盆地；分析最常见和最差情况下的损失景观，并探讨良性及对抗性微调对模型能力的影响；理论证明最常见盆地大小与最差盆地大小的关系及输入扰动的鲁棒性，并利用模型超参数化特性扩大盆地。

**结果:** 发现预训练生成基本能力盆地，微调生成具体能力盆地；证明只要良性微调保持在最常见盆地内就不会损害先前能力，同样对抗性微调若在最差盆地内也不会损害先前能力；理论展示了最常见盆地大小能限制最差盆地大小和输入扰动的鲁棒性；表明可以通过模型超参数化特性将盆地扩大五倍。

**结论:** 大规模语言模型的损失景观具有复杂的盆地结构，预训练和微调分别影响基本能力和具体能力；保持在特定盆地内的微调不会损害先前能力；理论证明了盆地大小之间的关系及鲁棒性，并指出可通过超参数化特性扩大盆地。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Pre-training+and+Fine-tuning+from+Loss+Landscape+Perspectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17646，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17646&send_immediately=true&force_search=false)

**原文摘要:** Recent studies have revealed that the loss landscape of large language models
resembles a basin, within which the models perform nearly identically, and
outside of which they lose all their capabilities. In this work, we conduct
further studies on the loss landscape of large language models. We discover
that pre-training creates a "basic capability" basin, and subsequent
fine-tuning creates "specific capability" basins (e.g., math, safety, coding)
within the basic capability basin. We further investigate two types of loss
landscapes: the most-case landscape (i.e., the landscape along most directions)
and the worst-case landscape (i.e., the landscape along the worst direction).
We argue that as long as benign fine-tuning remains within the most-case basin,
it will not compromise previous capabilities. Similarly, any fine-tuning
(including the adversarial one) that stays within the worst-case basin would
not compromise previous capabilities. Finally, we theoretically demonstrate
that the size of the most-case basin can bound the size of the worst-case basin
and the robustness with respect to input perturbations. We also show that, due
to the over-parameterization property of current large language models, one can
easily enlarge the basins by five times.

</details>


### [100] [FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks](https://arxiv.org/abs/2505.17883)
*Laines Schmalwasser, Niklas Penzel, Joachim Denzler, Julia Niebling*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为FastCAV的新方法，可以加速Concept Activation Vectors（CAVs）的提取过程达63.6倍（平均46.4倍）。该方法在理论上等同于传统的SVM方法，并在实际应用中表现出相似性能，同时更高效和稳定。FastCAV可替代传统CAV用于概念解释方法中，提供相同见解，同时支持对深度模型训练过程中概念演化的研究。


<details>
  <summary>更多</summary>
  
**动机:** 现有的CAV计算方法在大规模、高维架构中存在计算成本高和耗时长的问题，限制了其在深度学习模型中的广泛应用。因此需要一种更快、更高效的CAV计算方法来解决这一问题。

**方法:** 提出了FastCAV方法，通过理论推导证明其在特定假设下与传统SVM方法等效。FastCAV能够显著加速CAV的提取过程，同时保持性能不变。

**结果:** 实验证明FastCAV在性能上与传统方法相当，但效率更高且更稳定。此外，在下游的概念解释任务中，FastCAV可以作为传统CAV的替代方案，提供相同的解释结果。

**结论:** FastCAV为深度模型提供了更高效的CAV计算方法，使得研究深度模型训练过程中概念的演化成为可能，从而推动了基于概念的可解释性研究的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FastCAV%3A+Efficient+Computation+of+Concept+Activation+Vectors+for+Explaining+Deep+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17883&send_immediately=true&force_search=false)

**原文摘要:** Concepts such as objects, patterns, and shapes are how humans understand the
world. Building on this intuition, concept-based explainability methods aim to
study representations learned by deep neural networks in relation to
human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an
important tool and can identify whether a model learned a concept or not.
However, the computational cost and time requirements of existing CAV
computation pose a significant challenge, particularly in large-scale,
high-dimensional architectures. To address this limitation, we introduce
FastCAV, a novel approach that accelerates the extraction of CAVs by up to
63.6x (on average 46.4x). We provide a theoretical foundation for our approach
and give concrete assumptions under which it is equivalent to established
SVM-based methods. Our empirical results demonstrate that CAVs calculated with
FastCAV maintain similar performance while being more efficient and stable. In
downstream applications, i.e., concept-based explanation methods, we show that
FastCAV can act as a replacement leading to equivalent insights. Hence, our
approach enables previously infeasible investigations of deep models, which we
demonstrate by tracking the evolution of concepts during model training.

</details>


### [101] [DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification](https://arxiv.org/abs/2505.17660)
*Chenyang Li, Jinsong Chen, John E. Hopcroft, Kun He*

**主要类别:** cs.LG

**概要:** 最近，邻域感知的token化图Transformer在节点分类任务中展现出巨大潜力。然而，现有的方法存在两个关键问题：1）无法充分捕捉邻域内的属性相关性；2）自注意力机制在处理邻域token时容易分散注意力，导致目标节点与邻域token之间的信息交互被破坏。为了解决这些问题，本文提出了一种基于双位置编码的注意力掩码图Transformer（DAM-GT）。该模型通过属性聚类策略引入了属性感知编码，有效保留了拓扑和属性空间中的节点相关性，并设计了一种新的带掩码的注意力机制以引导目标节点与其邻域token的交互，克服了注意力分散的问题。实验表明，DAM-GT在不同同质性和规模的图数据集上始终优于现有最先进方法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管邻域感知的token化图Transformer在节点分类任务中表现出色，但现有方法未能充分捕捉邻域内的属性相关性，并且自注意力机制在处理邻域token时存在注意力分散问题，这限制了模型性能的进一步提升。

**方法:** 本文提出了DAM-GT模型，包含以下创新点：1）引入一种双位置编码方案，结合属性聚类策略实现属性感知编码，从而在拓扑和属性空间中保留节点相关性；2）设计了一种新的注意力机制，采用简单的掩码策略指导目标节点与邻域token之间的交互，解决注意力分散问题。

**结果:** DAM-GT在多个具有不同同质性和规模的图数据集上的实验结果表明，其在节点分类任务中始终优于当前最先进的方法。

**结论:** DAM-GT通过改进的位置编码和注意力机制解决了现有图Transformer在捕捉属性相关性和避免注意力分散方面的不足，显著提升了节点分类任务的性能。这一方法为未来图神经网络的研究提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DAM-GT%3A+Dual+Positional+Encoding-Based+Attention+Masking+Graph+Transformer+for+Node+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17660&send_immediately=true&force_search=false)

**原文摘要:** Neighborhood-aware tokenized graph Transformers have recently shown great
potential for node classification tasks. Despite their effectiveness, our
in-depth analysis of neighborhood tokens reveals two critical limitations in
the existing paradigm. First, current neighborhood token generation methods
fail to adequately capture attribute correlations within a neighborhood.
Second, the conventional self-attention mechanism suffers from attention
diversion when processing neighborhood tokens, where high-hop neighborhoods
receive disproportionate focus, severely disrupting information interactions
between the target node and its neighborhood tokens. To address these
challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking
graph Transformer. DAM-GT introduces a novel dual positional encoding scheme
that incorporates attribute-aware encoding via an attribute clustering
strategy, effectively preserving node correlations in both topological and
attribute spaces. In addition, DAM-GT formulates a new attention mechanism with
a simple yet effective masking strategy to guide interactions between target
nodes and their neighborhood tokens, overcoming the issue of attention
diversion. Extensive experiments on various graphs with different homophily
levels as well as different scales demonstrate that DAM-GT consistently
outperforms state-of-the-art methods in node classification tasks.

</details>


### [102] [NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling](https://arxiv.org/abs/2505.17909)
*Bram Grooten, Farid Hasanov, Chenxiang Zhang, Qiao Xiao, Boqian Wu, Zahra Atashgahi, Ghada Sokar, Shiwei Liu, Lu Yin, Elena Mocanu, Mykola Pechenizkiy, Decebal Constantin Mocanu*

**主要类别:** cs.LG

**概要:** NeuroTrails是一种新型的稀疏多头架构，通过动态演化拓扑结构，在减少参数量的同时提升模型集成的性能和鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的模型集成方法虽然能提高泛化能力和鲁棒性，但通常伴随着巨大的计算开销。尽管一些方法试图用单个网络模拟集成性能，但在推理阶段仍需大量计算资源。因此，需要一种新的方法来在减少资源消耗的同时保持甚至提升集成性能。

**方法:** NeuroTrails采用了一种稀疏多头架构，其拓扑结构能够动态演化。该方法通过引入动态稀疏性，在不同神经路径之间达到预测多样性的“恰到好处”区域（Goldilocks zone），从而在不增加计算负担的情况下提升模型性能。

**结果:** 实验表明，NeuroTrails在卷积神经网络和基于Transformer的架构上均表现优异，包括ResNet-50/ImageNet和LLaMA-350M/C4等任务。它不仅提高了准确性，增强了零样本泛化能力，还显著减少了所需参数量。

**结论:** NeuroTrails提供了一种模型无关的训练范式，能够在减少资源消耗的同时改善集成模型的性能和鲁棒性，适用于计算机视觉和自然语言处理任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeuroTrails%3A+Training+with+Dynamic+Sparse+Heads+as+the+Key+to+Effective+Ensembling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17909&send_immediately=true&force_search=false)

**原文摘要:** Model ensembles have long been a cornerstone for improving generalization and
robustness in deep learning. However, their effectiveness often comes at the
cost of substantial computational overhead. To address this issue,
state-of-the-art methods aim to replicate ensemble-class performance without
requiring multiple independently trained networks. Unfortunately, these
algorithms often still demand considerable compute at inference. In response to
these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head
architecture with dynamically evolving topology. This unexplored model-agnostic
training paradigm improves ensemble performance while reducing the required
resources. We analyze the underlying reason for its effectiveness and observe
that the various neural trails induced by dynamic sparsity attain a
$\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays
efficacy with convolutional and transformer-based architectures on computer
vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,
among many others, demonstrate increased accuracy and stronger robustness in
zero-shot generalization, while requiring significantly fewer parameters.

</details>


### [103] [Automated scientific minimization of regret](https://arxiv.org/abs/2505.17661)
*Marcel Binz, Akshay K. Jagadish, Milena Rmus, Eric Schulz*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为ASMR的框架，用于自动化计算认知科学。通过利用Centaur模型识别可解释认知模型中的差距，并通过基于语言的推理模型生成自动修订来解决这些差距。在多属性决策任务中，ASMR发现的认知模型可以预测人类行为并保持可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的认知建模方法可能缺乏自动化和高效性，特别是在识别和弥补模型中的差距方面。因此，需要一种新的框架来提升这一过程的效率和效果。

**方法:** 提出了ASMR框架，该框架使用Centaur模型识别认知模型中的差距，并采用基于语言的推理模型自动生成修订。

**结果:** 在多属性决策任务中，ASMR能够发现预测人类行为的认知模型，并且这些模型具有很高的准确性和可解释性。

**结论:** ASMR展示了自动化认知建模管道核心组件的潜力，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+scientific+minimization+of+regret，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17661，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17661&send_immediately=true&force_search=false)

**原文摘要:** We introduce automated scientific minimization of regret (ASMR) -- a
framework for automated computational cognitive science. Building on the
principles of scientific regret minimization, ASMR leverages Centaur -- a
recently proposed foundation model of human cognition -- to identify gaps in an
interpretable cognitive model. These gaps are then addressed through automated
revisions generated by a language-based reasoning model. We demonstrate the
utility of this approach in a multi-attribute decision-making task, showing
that ASMR discovers cognitive models that predict human behavior at noise
ceiling while retaining interpretability. Taken together, our results highlight
the potential of ASMR to automate core components of the cognitive modeling
pipeline.

</details>


### [104] [SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models](https://arxiv.org/abs/2505.17967)
*Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Dan Alistarh*

**主要类别:** cs.LG

**概要:** 提出了一种新的低秩优化方法，通过两步法（基于离散余弦变换的正交基和自适应选择）来近似SVD梯度投影，减少了计算成本和内存使用，同时在预训练和微调任务中表现良好。


<details>
  <summary>更多</summary>
  
**动机:** 当前低秩优化方法主要依赖SVD，但其应用于大型模型时计算昂贵且增加内存负担。

**方法:** 利用离散余弦变换的预定义正交矩阵构造完整正交基，并根据每层梯度与基的对齐程度自适应选择基向量，仅存储选定列索引以节省内存。

**结果:** 该方法在预训练和微调任务中均表现出色，性能与SVD方法相当，但运行更快且内存占用更少。

**结论:** 所提出的两步法提供了一种高效、简单的低秩优化策略，可以替代传统SVD方法，降低计算和内存开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SVD-Free+Low-Rank+Adaptive+Gradient+Optimization+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17967&send_immediately=true&force_search=false)

**原文摘要:** Low-rank optimization has emerged as a promising direction in training large
language models (LLMs) to reduce the memory usage of adaptive optimizers by
constraining learning to a lower-dimensional space. Prior work typically
projects gradients of linear layers using approaches based on Singular Value
Decomposition (SVD). However, applying SVD-based procedures individually to
each layer in large models is computationally expensive and incurs additional
memory costs due to storing the projection matrices. In this work, we propose a
computationally efficient and conceptually simple two-step procedure to
approximate SVD-based gradient projections into lower-dimensional spaces.
First, we construct a complete orthogonal basis using predefined orthogonal
matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select
basis columns based on their alignment with the gradient of each layer. Each
projection matrix in our method is obtained via a single matrix multiplication
followed by a lightweight sorting step to identify the most relevant basis
vectors. Due to the predefined nature of the orthogonal bases, they are
computed once at the start of training. During training, we store only the
indices of the selected columns, avoiding the need to store full projection
matrices for each layer. Our numerical experiments on both pre-training and
fine-tuning tasks demonstrate the effectiveness of our dual strategy in
approximating optimal low-rank projections, matching the performance of costly
SVD-based methods while achieving faster runtime and reduced memory usage.

</details>


### [105] [Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs](https://arxiv.org/abs/2505.17662)
*Tianheng Ling, Chao Qian, Lukas Johannes Haßler, Gregor Schiele*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种用于嵌入式FPGA的Tiny Transformers统一全自动部署框架，支持时间序列任务的紧凑型Transformer架构，通过量化感知训练、硬件感知超参数搜索和自动VHDL生成实现低能耗和快速推理。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Transformer模型在时间序列任务中表现出色，但在资源受限设备上的部署仍面临挑战。尽管针对MCUs的工作已探索了硬件特定优化，但这些方法通常任务特定且精度有限。而现有的基于FPGA的Transformer部署多集中于高密度平台的手动配置，缺乏灵活性和自动化。

**方法:** 提出一个支持紧凑型编码器Transformer架构的统一全自动部署框架，涵盖预测、分类和异常检测三种时间序列任务。该框架结合了量化感知训练（降低到4位）、使用Optuna的硬件感知超参数搜索以及自动VHDL生成以实现无缝部署。

**结果:** 在六个公共数据集和两个嵌入式FPGA平台上进行评估，结果表明该框架可以生成仅整数、任务特定的Transformer加速器，在AMD Spartan-7上每推理能耗低至0.033 mJ，毫秒级延迟，并提供了关于Lattice iCE40部署可行性的见解。

**结论:** 所提出的框架为Tiny Transformers在嵌入式FPGA上的部署提供了一个高效、灵活和自动化的解决方案，显著降低了能耗和延迟，同时支持多种时间序列任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automating+Versatile+Time-Series+Analysis+with+Tiny+Transformers+on+Embedded+FPGAs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17662，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17662&send_immediately=true&force_search=false)

**原文摘要:** Transformer-based models have shown strong performance across diverse
time-series tasks, but their deployment on resource-constrained devices remains
challenging due to high memory and computational demand. While prior work
targeting Microcontroller Units (MCUs) has explored hardware-specific
optimizations, such approaches are often task-specific and limited to 8-bit
fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater
flexibility, enabling fine-grained control over data precision and
architecture. However, existing FPGA-based deployments of Transformers for
time-series analysis typically focus on high-density platforms with manual
configuration. This paper presents a unified and fully automated deployment
framework for Tiny Transformers on embedded FPGAs. Our framework supports a
compact encoder-only Transformer architecture across three representative
time-series tasks (forecasting, classification, and anomaly detection). It
combines quantization-aware training (down to 4 bits), hardware-aware
hyperparameter search using Optuna, and automatic VHDL generation for seamless
deployment. We evaluate our framework on six public datasets across two
embedded FPGA platforms. Results show that our framework produces integer-only,
task-specific Transformer accelerators achieving as low as 0.033 mJ per
inference with millisecond latency on AMD Spartan-7, while also providing
insights into deployment feasibility on Lattice iCE40. All source code will be
released in the GitHub repository
(https://github.com/Edwina1030/TinyTransformer4TS).

</details>


### [106] [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](https://arxiv.org/abs/2505.17968)
*Jiayi Geng, Howard Chen, Dilip Arumugam, Thomas L. Griffiths*

**主要类别:** cs.LG

**概要:** 使用AI创建自主研究者有可能加速科学发现。本文探讨了大型语言模型（LLM）在识别黑箱系统方面的表现，发现仅通过被动观察数据，LLM的表现不佳，但通过主动干预（即主动查询黑箱系统的特定输入以观察输出），可以显著提升其性能。这为未来利用LLM进行科学发现提供了指导。


<details>
  <summary>更多</summary>
  
**动机:** 探索AI模型在从行为中识别黑箱系统潜在结构的能力，从而评估其作为自主研究人员的潜力。

**方法:** 通过实验研究大型语言模型（LLM）在三种不同类型的黑箱系统（程序、形式语言和数学方程）中的反向工程能力，并比较被动观察与主动收集数据的效果。

**结果:** LLM在仅通过被动观察数据时无法有效提取信息，但在主动干预（如测试边界情况）时性能显著提高。这种改进部分源于生成有效的干预措施，类似于人类学习中的发现。此外，干预有助于避免过复杂化和忽略观察两种常见失败模式。

**结论:** 主动干预能显著提高LLM对黑箱系统的反向工程能力，为未来的科学研究提供了实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+Large+Language+Models+Reliable+AI+Scientists%3F+Assessing+Reverse-Engineering+of+Black-Box+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17968，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17968&send_immediately=true&force_search=false)

**原文摘要:** Using AI to create autonomous researchers has the potential to accelerate
scientific discovery. A prerequisite for this vision is understanding how well
an AI model can identify the underlying structure of a black-box system from
its behavior. In this paper, we explore how well a large language model (LLM)
learns to identify a black-box function from passively observed versus actively
collected data. We investigate the reverse-engineering capabilities of LLMs
across three distinct types of black-box systems, each chosen to represent
different problem domains where future autonomous AI researchers may have
considerable impact: Program, Formal Language, and Math Equation. Through
extensive experiments, we show that LLMs fail to extract information from
observations, reaching a performance plateau that falls short of the ideal of
Bayesian inference. However, we demonstrate that prompting LLMs to not only
observe but also intervene -- actively querying the black-box with specific
inputs to observe the resulting output -- improves performance by allowing LLMs
to test edge cases and refine their beliefs. By providing the intervention data
from one LLM to another, we show that this improvement is partly a result of
engaging in the process of generating effective interventions, paralleling
results in the literature on human learning. Further analysis reveals that
engaging in intervention can help LLMs escape from two common failure modes:
overcomplication, where the LLM falsely assumes prior knowledge about the
black-box, and overlooking, where the LLM fails to incorporate observations.
These insights provide practical guidance for helping LLMs more effectively
reverse-engineer black-box systems, supporting their use in making new
discoveries.

</details>


### [107] [What is the role of memorization in Continual Learning?](https://arxiv.org/abs/2505.17664)
*Jędrzej Kozal, Jan Wasilewski, Alif Ashrafee, Bartosz Krawczyk, Michał Woźniak*

**主要类别:** cs.LG

**概要:** 记忆效应对深度学习算法的性能有影响。本文研究了记忆效应在增量学习场景中的表现，并通过大量实验评估了记忆对持续学习的影响。研究表明，高记忆得分的学习样本比普通样本更容易被遗忘，但实现最高性能仍然需要记忆效应。当缓冲区较小时，忘记普通样本更重要；而当缓冲区增大时，高记忆得分样本的重要性也随之增加。本文引入了一个记忆代理，并展示其在缓冲策略问题中的应用，证明在缓冲区较大的情况下，包含高记忆代理得分的样本是有益的。


<details>
  <summary>更多</summary>
  
**动机:** 先前的研究主要关注记忆效应对泛化能力和隐私的影响，而本文旨在探讨记忆效应对增量学习场景的具体影响，特别是记忆与遗忘之间的关系及其实现最优性能的作用。

**方法:** 设计了广泛的实验来评估记忆效应对持续学习的影响。澄清了记忆和遗忘之间的区别，并研究了不同缓冲区大小下高记忆得分样本和普通样本的表现。引入了一个记忆代理，并将其应用于缓冲策略问题中，以展示记忆在增量训练中的作用。

**结果:** 发现高记忆得分的样本比普通样本更容易被遗忘，但在实现最高性能时仍然需要记忆效应。在低内存条件下，忘记普通样本更重要；而在大缓冲区情况下，高记忆得分样本的重要性增加。使用记忆代理有助于优化缓冲策略。

**结论:** 记忆效应对增量学习至关重要，尤其是在缓冲区大小变化的情况下。本文的研究结果表明，合理利用记忆效应可以提高持续学习的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+is+the+role+of+memorization+in+Continual+Learning%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17664&send_immediately=true&force_search=false)

**原文摘要:** Memorization impacts the performance of deep learning algorithms. Prior works
have studied memorization primarily in the context of generalization and
privacy. This work studies the memorization effect on incremental learning
scenarios. Forgetting prevention and memorization seem similar. However, one
should discuss their differences. We designed extensive experiments to evaluate
the impact of memorization on continual learning. We clarified that learning
examples with high memorization scores are forgotten faster than regular
samples. Our findings also indicated that memorization is necessary to achieve
the highest performance. However, at low memory regimes, forgetting regular
samples is more important. We showed that the importance of a high-memorization
score sample rises with an increase in the buffer size. We introduced a
memorization proxy and employed it in the buffer policy problem to showcase how
memorization could be used during incremental training. We demonstrated that
including samples with a higher proxy memorization score is beneficial when the
buffer size is large.

</details>


### [108] [Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models](https://arxiv.org/abs/2505.17974)
*Viktoriia Chekalina, Daniil Moskovskiy, Daria Cherniuk, Maxim Kurkin, Andrey Kuznetsov, Evgeny Frolov*

**主要类别:** cs.LG

**概要:** 提出了一种新的LLM压缩技术GFWSVD，通过考虑Fisher信息矩阵的对角和非对角元素来提高参数重要性估计的准确性，并在MMLU基准上超越了现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 利用完整的Fisher信息矩阵来评估神经网络参数敏感性对于大模型来说计算成本过高，而现有的对角近似方法忽略了参数之间的相关性，导致性能下降。

**方法:** 提出了一种名为Generalized Fisher-Weighted SVD（GFWSVD）的后训练LLM压缩技术，该方法结合了Fisher信息矩阵的对角和非对角元素，同时引入了可扩展的Kronecker-factored近似算法以降低计算复杂度。

**结果:** 在MMLU基准测试中，GFWSVD方法在20倍压缩率下显著优于基于对角近似的FWSVD（高出5%）、SVD-LLM（高出3%）和ASVD（高出6%）。

**结论:** GFWSVD提供了一种更准确反映参数重要性的方法，在LLM压缩任务中表现出色，且具有可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Fisher-Weighted+SVD%3A+Scalable+Kronecker-Factored+Fisher+Approximation+for+Compressing+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17974，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17974&send_immediately=true&force_search=false)

**原文摘要:** The Fisher information is a fundamental concept for characterizing the
sensitivity of parameters in neural networks. However, leveraging the full
observed Fisher information is too expensive for large models, so most methods
rely on simple diagonal approximations. While efficient, this approach ignores
parameter correlations, often resulting in reduced performance on downstream
tasks. In this work, we mitigate these limitations and propose Generalized
Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that
accounts for both diagonal and off-diagonal elements of the Fisher information
matrix, providing a more accurate reflection of parameter importance. To make
the method tractable, we introduce a scalable adaptation of the
Kronecker-factored approximation algorithm for the observed Fisher information.
We demonstrate the effectiveness of our method on LLM compression, showing
improvements over existing compression baselines. For example, at a 20
compression rate on the MMLU benchmark, our method outperforms FWSVD, which is
based on a diagonal approximation of the Fisher information, by 5 percent,
SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.

</details>


### [109] [ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling](https://arxiv.org/abs/2505.17987)
*Weihang You, Hanqi Jiang, Zishuai Liu, Zihang Xie, Tianming Liu, Jin Lu, Fei Dou*

**主要类别:** cs.LG

**概要:** 本论文介绍了一种名为ADLGen的生成框架，用于合成现实生活中日常活动（ADL）数据的传感器序列。该框架结合了仅解码器的Transformer、符号时间编码和上下文感知采样机制，以生成语义丰富且物理上可行的传感器事件序列。此外，通过引入大型语言模型进行自动生成、评估和修正循环，进一步提高了生成数据的语义保真度并修正结构不一致问题。实验表明，ADLGen在统计保真度、语义丰富性和下游活动识别方面优于基线生成器，提供了一种可扩展且保护隐私的ADL数据合成解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 日常活动数据的收集面临隐私问题、部署与标注成本高以及人类行为固有的稀疏性和不平衡性等挑战，因此需要一种能够生成逼真且具有触发事件特征的符号传感器序列的方法来支持辅助环境的研究和发展。

**方法:** ADLGen采用了一个基于Transformer的解码器架构，使用符号时间编码，并结合上下文和布局感知的采样机制引导生成过程。同时，集成大型语言模型到自动生成、评估和修正循环中，以验证逻辑、行为和时间一致性并自动生成修正规则。

**结果:** 实验结果表明，ADLGen在统计保真度、语义丰富性和下游活动识别任务中表现优于基线生成器。新设计的评估指标进一步证明了其优越性能。

**结论:** ADLGen提供了一种可扩展且保护隐私的ADL数据合成方法，适用于辅助环境中的研究与应用，为未来的工作提供了坚实的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADLGen%3A+Synthesizing+Symbolic%2C+Event-Triggered+Sensor+Sequences+for+Human+Activity+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17987&send_immediately=true&force_search=false)

**原文摘要:** Real world collection of Activities of Daily Living data is challenging due
to privacy concerns, costly deployment and labeling, and the inherent sparsity
and imbalance of human behavior. We present ADLGen, a generative framework
specifically designed to synthesize realistic, event triggered, and symbolic
sensor sequences for ambient assistive environments. ADLGen integrates a
decoder only Transformer with sign based symbolic temporal encoding, and a
context and layout aware sampling mechanism to guide generation toward
semantically rich and physically plausible sensor event sequences. To enhance
semantic fidelity and correct structural inconsistencies, we further
incorporate a large language model into an automatic generate evaluate refine
loop, which verifies logical, behavioral, and temporal coherence and generates
correction rules without manual intervention or environment specific tuning.
Through comprehensive experiments with novel evaluation metrics, ADLGen is
shown to outperform baseline generators in statistical fidelity, semantic
richness, and downstream activity recognition, offering a scalable and
privacy-preserving solution for ADL data synthesis.

</details>


### [110] [FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding](https://arxiv.org/abs/2505.17694)
*Zhibin Wang, Rui Ning, Chao Fang, Zhonghui Zhang, Xi Lin, Shaobo Ma, Mo Zhou, Xue Li, Zhongfeng Wang, Chengying Huan, Rong Gu, Kun Yang, Guihai Chen, Sheng Zhong, Chen Tian*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为FlashForge的专用注意力内核，用于优化解码阶段中共享前缀的内存访问模式。通过创新的共享前缀注意力内核和工作负载平衡机制，FlashForge显著提高了计算速度并减少了内存访问需求。实验表明，FlashForge在解码阶段的注意力计算中比最先进的FlashDecoding内核快1.9倍，内存访问减少120.9倍，并且端到端每个输出标记的时间比vLLM快3.8倍。


<details>
  <summary>更多</summary>
  
**动机:** 随着上下文长度增加，解码阶段的注意力计算成为性能瓶颈，特别是在处理共享前缀的KV缓存时，内存访问密集且复杂。因此，研究共享前缀在解码阶段注意力计算中的潜力具有重要意义。

**方法:** 提出了FlashForge，包含两个关键创新：1）一种新颖的共享前缀注意力内核，优化内存层次结构并利用块内和块间并行性；2）一种全面的工作负载平衡机制，能够有效估计成本、划分任务和调度执行。

**结果:** 实验结果表明，FlashForge在解码阶段的注意力计算中相比FlashDecoding内核实现了平均1.9倍的速度提升和120.9倍的内存访问减少，端到端每个输出标记的时间相比vLLM快3.8倍。

**结论:** FlashForge有效地解决了共享前缀机制中的挑战，显著提升了注意力计算的效率和内存访问性能，为大规模语言模型的解码过程提供了高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FlashForge%3A+Ultra-Efficient+Prefix-Aware+Attention+for+LLM+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17694，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17694&send_immediately=true&force_search=false)

**原文摘要:** Prefix-sharing among multiple prompts presents opportunities to combine the
operations of the shared prefix, while attention computation in the decode
stage, which becomes a critical bottleneck with increasing context lengths, is
a memory-intensive process requiring heavy memory access on the key-value (KV)
cache of the prefixes. Therefore, in this paper, we explore the potential of
prefix-sharing in the attention computation of the decode stage. However, the
tree structure of the prefix-sharing mechanism presents significant challenges
for attention computation in efficiently processing shared KV cache access
patterns while managing complex dependencies and balancing irregular workloads.
To address the above challenges, we propose a dedicated attention kernel to
combine the memory access of shared prefixes in the decoding stage, namely
FlashForge. FlashForge delivers two key innovations: a novel shared-prefix
attention kernel that optimizes memory hierarchy and exploits both intra-block
and inter-block parallelism, and a comprehensive workload balancing mechanism
that efficiently estimates cost, divides tasks, and schedules execution.
Experimental results show that FlashForge achieves an average 1.9x speedup and
120.9x memory access reduction compared to the state-of-the-art FlashDecoding
kernel regarding attention computation in the decode stage and 3.8x end-to-end
time per output token compared to the vLLM.

</details>


### [111] [Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/abs/2505.17988)
*Yutong Chen, Jiandong Gao, Ji Wu*

**主要类别:** cs.LG

**概要:** R1-style强化学习（RL）提升了大语言模型的推理能力，但规则基础的RL机制尚不明确。研究发现小规模SFT对RL有显著影响但效率低下。通过分析框架对比SFT和RL效率，提出重蒸馏技术，实验表明其能以更少样本和计算量达到RL性能。最终，重蒸馏的Qwen2.5-1.5B模型在K&K和MATH数据集上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 尽管R1-style RL显著增强了大语言模型的推理能力，但其背后的机制仍不清楚。同时，小规模监督微调（SFT）虽然对RL有重要影响，但效率较差，需要进一步优化。

**方法:** 提出了一个分析框架，通过测量样本效应来比较SFT和RL的效率，并基于此提出重蒸馏技术。该技术通过对RL训练策略进行小规模蒸馏，从而微调预训练模型。

**结果:** 重蒸馏技术表现出惊人的效率：在Knight & Knave和MATH数据集上，重蒸馏模型用远少的样本和计算量达到了与RL相当的性能。特别是在K&K数据集上，仅用1K SFT样本的Qwen2.5-1.5B模型超越了DeepSeek-V3-0324；在MATH数据集上，使用500重蒸馏样本微调的Qwen2.5-1.5B模型匹配了其无RL指令调优变体的性能。

**结论:** 本研究解释了R1-style RL中的几个有趣现象，揭示了其经验成功背后的机制，并证明样本效应是性能改进的良好指标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Revealing+the+Effectiveness+of+Small-Scale+Fine-tuning+in+R1-style+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17988&send_immediately=true&force_search=false)

**原文摘要:** R1-style Reinforcement Learning (RL) significantly enhances Large Language
Models' reasoning capabilities, yet the mechanism behind rule-based RL remains
unclear. We found that small-scale SFT has significant influence on RL but
shows poor efficiency. To explain our observations, we propose an analytical
framework and compare the efficiency of SFT and RL by measuring sample effect.
Hypothetical analysis show that SFT efficiency is limited by training data.
Guided by our analysis, we propose Re-distillation, a technique that fine-tunes
pretrain model through small-scale distillation from the RL-trained policy.
Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's
surprising efficiency: re-distilled models match RL performance with far fewer
samples and less computation. Empirical verification shows that sample effect
is a good indicator of performance improvements. As a result, on K&K dataset,
our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT
samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches
its instruct-tuned variant without RL. Our work explains several interesting
phenomena in R1-style RL, shedding light on the mechanisms behind its empirical
success. Code is available at: https://github.com/on1262/deep-reasoning

</details>


### [112] [Outcome-based Reinforcement Learning to Predict the Future](https://arxiv.org/abs/2505.17989)
*Benjamin Turtel, Danny Franklin, Kris Skotheim, Luke Hewitt, Philipp Schoenegger*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Outcome-based+Reinforcement+Learning+to+Predict+the+Future，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17989，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17989&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning with verifiable rewards (RLVR) has boosted math and
coding in large language models, yet there has been little effort to extend
RLVR into messier, real-world domains like forecasting. One sticking point is
that outcome-based reinforcement learning for forecasting must learn from
binary, delayed, and noisy rewards, a regime where standard fine-tuning is
brittle. We show that outcome-only online RL on a 14B model can match
frontier-scale accuracy and surpass it in calibration and hypothetical
prediction market betting by adapting two leading algorithms, Group-Relative
Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our
adaptations remove per-question variance scaling in GRPO, apply
baseline-subtracted advantages in ReMax, hydrate training with 100k temporally
consistent synthetic questions, and introduce lightweight guard-rails that
penalise gibberish, non-English responses and missing rationales, enabling a
single stable pass over 110k events. Scaling ReMax to 110k questions and
ensembling seven predictions yields a 14B model that matches frontier baseline
o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in
calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this
calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p =
0.037). This demonstrates that refined RLVR methods can convert small-scale
LLMs into potentially economically valuable forecasting tools, with
implications for scaling this to larger models.

</details>


### [113] [An Example Safety Case for Safeguards Against Misuse](https://arxiv.org/abs/2505.18003)
*Joshua Clymer, Jonah Weinbaum, Robert Kirk, Kimberly Mai, Selena Zhang, Xander Davies*

**主要类别:** cs.LG

**概要:** 本论文提出了一种端到端的论证方法（即“安全案例”），以证明AI助手的滥用防护措施能将风险降低到较低水平，并通过量化模型和连续风险信号支持开发者快速应对新兴威胁。


<details>
  <summary>更多</summary>
  
**动机:** 现有的AI滥用防护评估难以与现实世界决策相联系，因此需要一种更系统化的方法来证明这些防护措施的有效性。

**方法:** 1. 描述假设开发者的红队如何测试防护措施并估算规避所需的努力。
2. 将该估算值代入一个量化的“提升模型”，以确定防护屏障对滥用行为的劝阻程度。
3. 提供部署期间的连续风险信号，帮助开发者快速响应新出现的威胁。
4. 将这些组件整合为一个简单的安全案例。

**结果:** 提出了一个具体路径，通过构建安全案例，可以严格证明AI滥用风险处于较低水平。

**结论:** 本文提供了一种系统化、可量化的途径，用于证明AI滥用风险较低，尽管这不是唯一的解决方法，但为实际应用中的风险评估提供了有价值的参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Example+Safety+Case+for+Safeguards+Against+Misuse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18003&send_immediately=true&force_search=false)

**原文摘要:** Existing evaluations of AI misuse safeguards provide a patchwork of evidence
that is often difficult to connect to real-world decisions. To bridge this gap,
we describe an end-to-end argument (a "safety case") that misuse safeguards
reduce the risk posed by an AI assistant to low levels. We first describe how a
hypothetical developer red teams safeguards, estimating the effort required to
evade them. Then, the developer plugs this estimate into a quantitative "uplift
model" to determine how much barriers introduced by safeguards dissuade misuse
(https://www.aimisusemodel.com/). This procedure provides a continuous signal
of risk during deployment that helps the developer rapidly respond to emerging
threats. Finally, we describe how to tie these components together into a
simple safety case. Our work provides one concrete path -- though not the only
path -- to rigorously justifying AI misuse risks are low.

</details>


### [114] [The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations](https://arxiv.org/abs/2505.17708)
*Dingling Yao, Shimeng Huang, Riccardo Cadei, Kun Zhang, Francesco Locatello*

**主要类别:** cs.LG

**概要:** 重新解释因果表示学习（CRL），提出新的评估标准T-MEX，验证其在多种因果推断场景中的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管因果表示学习（CRL）在识别潜在因果结构方面取得了进展，但对所学表示为何对因果下游任务有用以及如何评估它们的理解仍然不足。

**方法:** 使用测量模型框架重新解释CRL，将学习到的表示视为潜在因果变量的代理测量值，并提出基于测试的测量排他性（T-MEX）分数以量化评估表示的质量。

**结果:** 通过数值模拟和真实世界生态视频分析等多种因果推断场景验证了T-MEX的有效性，能够有效评估学习到的表示的识别及其对因果下游任务的有用性。

**结论:** 该研究提供了明确的条件，说明学习到的表示何时支持下游因果推理，并提出了一个有原则依据的评估方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Third+Pillar+of+Causal+Analysis%3F+A+Measurement+Perspective+on+Causal+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17708，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17708&send_immediately=true&force_search=false)

**原文摘要:** Causal reasoning and discovery, two fundamental tasks of causal analysis,
often face challenges in applications due to the complexity, noisiness, and
high-dimensionality of real-world data. Despite recent progress in identifying
latent causal structures using causal representation learning (CRL), what makes
learned representations useful for causal downstream tasks and how to evaluate
them are still not well understood. In this paper, we reinterpret CRL using a
measurement model framework, where the learned representations are viewed as
proxy measurements of the latent causal variables. Our approach clarifies the
conditions under which learned representations support downstream causal
reasoning and provides a principled basis for quantitatively assessing the
quality of representations using a new Test-based Measurement EXclusivity
(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,
including numerical simulations and real-world ecological video analysis,
demonstrating that the proposed framework and corresponding score effectively
assess the identification of learned representations and their usefulness for
causal downstream tasks.

</details>


### [115] [Knot So Simple: A Minimalistic Environment for Spatial Reasoning](https://arxiv.org/abs/2505.18028)
*Zizhao Chen, Yoav Artzi*

**主要类别:** cs.LG

**概要:** 提出了一种名为KnotGym的交互式环境，用于复杂的、基于空间推理和操作的任务。该环境包含目标导向的绳索操作任务，需要从纯图像观察中进行操作，并依据结点交叉数量定义复杂性轴。通过评估不同类别的方法（如基于模型的强化学习、预测控制和链式思维推理），展示了KnotGym所提出的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 为了提供一个可以测试复杂空间推理和操作能力的环境，同时能够量化任务复杂度并揭示当前方法在感知、推理和操作整合方面的核心挑战。

**方法:** 创建了一个名为KnotGym的交互式环境，其中包含基于绳结操作的任务，任务复杂度由结点交叉数量决定。该环境仅依赖图像观察作为输入，并对不同类别（包括基于模型的强化学习、预测控制和链式思维推理）的方法进行了评估。

**结果:** KnotGym成功地揭示了现有方法在整合敏锐感知、空间推理和基础操作方面面临的挑战，并为未来的研究提供了可扩展的发展平台。

**结论:** KnotGym作为一个交互式环境，不仅可以用于复杂空间推理和操作任务，还突显了当前方法的核心局限性，为后续研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knot+So+Simple%3A+A+Minimalistic+Environment+for+Spatial+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18028，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18028&send_immediately=true&force_search=false)

**原文摘要:** We propose KnotGym, an interactive environment for complex, spatial reasoning
and manipulation. KnotGym includes goal-oriented rope manipulation tasks with
varying levels of complexity, all requiring acting from pure image
observations. Tasks are defined along a clear and quantifiable axis of
complexity based on the number of knot crossings, creating a natural
generalization test. KnotGym has a simple observation space, allowing for
scalable development, yet it highlights core challenges in integrating acute
perception, spatial reasoning, and grounded manipulation. We evaluate methods
of different classes, including model-based RL, model-predictive control, and
chain-of-thought reasoning, and illustrate the challenges KnotGym presents.
KnotGym is available at https://github.com/lil-lab/knotgym.

</details>


### [116] [Get Experience from Practice: LLM Agents with Record & Replay](https://arxiv.org/abs/2505.17716)
*Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhichao Hua, Yubin Xia, Haibo Chen*

**主要类别:** cs.LG

**概要:** AI代理人虽然潜力巨大，但面临可靠性、隐私、成本和性能的挑战。本文提出AgentRR（代理记录与回放）框架，通过记录、总结和重播代理的经验来指导未来任务，同时引入多级经验抽象方法和检查函数机制以确保安全性和适用性，并探讨了多种应用场景。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型驱动的AI代理人在执行复杂任务时表现出巨大潜力，但由于模型固有的不确定性以及对计算资源的高需求，存在可靠性、隐私保护、成本控制和性能优化四大挑战。现有解决方案虽能部分缓解这些问题，但无法从根本上解决。

**方法:** 提出了一种名为AgentRR的新范式，将经典的记录与回放机制引入AI代理框架：1. 记录代理与环境交互的痕迹及内部决策过程；2. 将这些痕迹总结为结构化的“经验”，包含工作流程和约束条件；3. 在后续类似任务中重播这些经验以指导代理行为。此外，详细描述了多级经验抽象方法和检查函数机制，前者平衡经验的具体性和通用性，后者作为信任锚点确保重播过程的完整性和安全性。

**结果:** AgentRR能够有效应对AI代理人开发中的挑战，提供了一种全新的解决思路。其多级经验抽象方法和检查函数机制可确保经验和行为的安全性与适用性，同时支持多种应用模式，如用户任务演示、大小模型协作和隐私保护代理执行等。这有助于进一步降低部署成本并提升效率。

**结论:** AgentRR作为一种创新的AI代理开发范式，通过引入记录与回放机制，解决了现有代理人开发中的关键问题。未来可以通过构建经验库来共享和重用知识，从而进一步减少部署成本并提高代理人的整体性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Get+Experience+from+Practice%3A+LLM+Agents+with+Record+%26+Replay，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17716&send_immediately=true&force_search=false)

**原文摘要:** AI agents, empowered by Large Language Models (LLMs) and communication
protocols such as MCP and A2A, have rapidly evolved from simple chatbots to
autonomous entities capable of executing complex, multi-step tasks,
demonstrating great potential. However, the LLMs' inherent uncertainty and
heavy computational resource requirements pose four significant challenges to
the development of safe and efficient agents: reliability, privacy, cost and
performance. Existing approaches, like model alignment, workflow constraints
and on-device model deployment, can partially alleviate some issues but often
with limitations, failing to fundamentally resolve these challenges.
  This paper proposes a new paradigm called AgentRR (Agent Record & Replay),
which introduces the classical record-and-replay mechanism into AI agent
frameworks. The core idea is to: 1. Record an agent's interaction trace with
its environment and internal decision process during task execution, 2.
Summarize this trace into a structured "experience" encapsulating the workflow
and constraints, and 3. Replay these experiences in subsequent similar tasks to
guide the agent's behavior. We detail a multi-level experience abstraction
method and a check function mechanism in AgentRR: the former balances
experience specificity and generality, while the latter serves as a trust
anchor to ensure completeness and safety during replay. In addition, we explore
multiple application modes of AgentRR, including user-recorded task
demonstration, large-small model collaboration and privacy-aware agent
execution, and envision an experience repository for sharing and reusing
knowledge to further reduce deployment cost.

</details>


### [117] [AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction](https://arxiv.org/abs/2505.18080)
*Chunlin Gong, Yin Wang, Jingru Li, Hanleran Zhang*

**主要类别:** cs.LG

**概要:** 本文提出了一种名为AFD-STA Net的神经框架，结合了自适应滤波和时空动力学学习，用于预测由偏微分方程控制的高维混沌系统。该架构包含四个关键部分：1）具有位置感知衰减系数的自适应指数平滑模块；2）捕捉跨时间与空间依赖性的并行注意力机制；3）多尺度特征的动态门控融合；4）具有维度缩放能力的深度投影网络。实验结果表明，该模型在平滑和强混沌状态中均能保持预测准确性，并通过自适应滤波表现出对噪声的容忍性。组件消融研究确认了每个模块的关键贡献，特别是时空注意力在学习复杂动力学交互中的重要作用。该框架在需要同时处理测量不确定性与高维非线性动力学的实际应用中展现了巨大潜力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的预测高维混沌系统的模型可能在面对复杂的非线性和测量不确定性时表现不佳，因此需要一种能够整合自适应滤波和时空动力学学习的方法来提高预测准确性和鲁棒性。

**方法:** AFD-STA Net是一个整合了自适应滤波和时空动力学学习的神经网络框架。其主要组成部分包括：自适应指数平滑模块、并行注意力机制、动态门控融合以及深度投影网络。这些组件共同作用以捕捉高维混沌系统的复杂特性。

**结果:** 在非线性PDE系统上的数值实验表明，AFD-STA Net在平滑和强混沌状态下均能保持较高的预测准确性，并且通过对噪声的自适应滤波表现出良好的鲁棒性。消融研究表明，每个模块都对模型性能有重要贡献，特别是时空注意力机制对于学习复杂的动力学交互至关重要。

**结论:** AFD-STA Net作为一种新的神经框架，在预测高维混沌系统方面展现出了显著的效果和潜力，特别是在处理测量不确定性和非线性动力学方面，为实际应用提供了有力的支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AFD-STA%3A+Adaptive+Filtering+Denoising+with+Spatiotemporal+Attention+for+Chaotic+System+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18080&send_immediately=true&force_search=false)

**原文摘要:** This paper presents AFD-STA Net, a neural framework integrating adaptive
filtering and spatiotemporal dynamics learning for predicting high-dimensional
chaotic systems governed by partial differential equations. The architecture
combines: 1) An adaptive exponential smoothing module with position-aware decay
coefficients for robust attractor reconstruction, 2) Parallel attention
mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated
fusion of multiscale features, and 4) Deep projection networks with
dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems
demonstrate the model's effectiveness in maintaining prediction accuracy under
both smooth and strongly chaotic regimes while exhibiting noise tolerance
through adaptive filtering. Component ablation studies confirm critical
contributions from each module, particularly highlighting the essential role of
spatiotemporal attention in learning complex dynamical interactions. The
framework shows promising potential for real-world applications requiring
simultaneous handling of measurement uncertainties and high-dimensional
nonlinear dynamics.

</details>


### [118] [PEAR: Equal Area Weather Forecasting on the Sphere](https://arxiv.org/abs/2505.17720)
*Hampus Linander, Christoffer Petersson, Daniel Persson, Jan E. Gerken*

**主要类别:** cs.LG

**概要:** 提出了一种新的基于HEALPix网格的深度学习天气预报模型PEAR，该模型在计算成本不变的情况下优于传统的Driscoll-Healy网格模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前的深度学习天气预报模型大多使用Driscoll--Healy离散化方法，这种方法在极地地区有更细的网格划分，导致了不均匀的数据偏差。而HEALPix网格能够提供等面积像素划分，避免了这种偏差，因此作者希望利用这一特性改进天气预报模型。

**方法:** 引入了Pangu Equal ARea (PEAR) 模型，这是一种基于transformer的天气预报模型，可以直接在HEALPix特征上运行，从而消除Driscoll--Healy网格中的物理偏差。

**结果:** PEAR模型在HEALPix网格上的表现优于对应的Driscoll--Healy模型，并且没有增加任何计算开销。

**结论:** 基于HEALPix网格的深度学习模型可以有效改善全球中程天气预报的效果，同时保持较低的计算成本，PEAR模型为这一领域提供了新的研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PEAR%3A+Equal+Area+Weather+Forecasting+on+the+Sphere，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17720&send_immediately=true&force_search=false)

**原文摘要:** Machine learning methods for global medium-range weather forecasting have
recently received immense attention. Following the publication of the Pangu
Weather model, the first deep learning model to outperform traditional
numerical simulations of the atmosphere, numerous models have been published in
this domain, building on Pangu's success. However, all of these models operate
on input data and produce predictions on the Driscoll--Healy discretization of
the sphere which suffers from a much finer grid at the poles than around the
equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization
(HEALPix) of the sphere, each pixel covers the same surface area, removing
unphysical biases. Motivated by a growing support for this grid in meteorology
and climate sciences, we propose to perform weather forecasting with deep
learning models which natively operate on the HEALPix grid. To this end, we
introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting
model which operates directly on HEALPix-features and outperforms the
corresponding model on Driscoll--Healy without any computational overhead.

</details>


### [119] [Backpropagation-Free Metropolis-Adjusted Langevin Algorithm](https://arxiv.org/abs/2505.18081)
*Adam D. Cobb, Susmit Jha*

**主要类别:** cs.LG

**概要:** This paper proposes four new algorithms using forward-mode automatic differentiation in MALA without backpropagation, reducing computational cost and showing competitiveness or superiority over original MALA.


<details>
  <summary>更多</summary>
  
**动机:** Recent work has shown that forward-mode automatic differentiation (AD) can perform optimization on differentiable models without backpropagation. The authors explore incorporating tangent vector sampling into MALA and develop novel algorithms that leverage Hessian information.

**方法:** In this paper, they illustrate how the sampling of this tangent vector can be incorporated into the proposal mechanism for the Metropolis-Adjusted Langevin Algorithm (MALA). They introduce a backpropagation-free gradient-based Markov chain Monte Carlo (MCMC) algorithm. Also, they extend to a novel backpropagation-free position-specific preconditioned forward-mode MALA that leverages Hessian information.

**结果:** They include Bayesian inference results on a range of probabilistic models, including hierarchical distributions and Bayesian neural networks.

**结论:** Overall, we propose four new algorithms: Forward MALA; Line Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward MALA. We highlight the reduced computational cost of the forward-mode samplers and show that forward-mode is competitive with the original MALA, while even outperforming it depending on the probabilistic model.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Backpropagation-Free+Metropolis-Adjusted+Langevin+Algorithm，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18081&send_immediately=true&force_search=false)

**原文摘要:** Recent work on backpropagation-free learning has shown that it is possible to
use forward-mode automatic differentiation (AD) to perform optimization on
differentiable models. Forward-mode AD requires sampling a tangent vector for
each forward pass of a model. The result is the model evaluation with the
directional derivative along the tangent. In this paper, we illustrate how the
sampling of this tangent vector can be incorporated into the proposal mechanism
for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the
first to introduce a backpropagation-free gradient-based Markov chain Monte
Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free
position-specific preconditioned forward-mode MALA that leverages Hessian
information. Overall, we propose four new algorithms: Forward MALA; Line
Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward
MALA. We highlight the reduced computational cost of the forward-mode samplers
and show that forward-mode is competitive with the original MALA, while even
outperforming it depending on the probabilistic model. We include Bayesian
inference results on a range of probabilistic models, including hierarchical
distributions and Bayesian neural networks.

</details>


### [120] [Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data](https://arxiv.org/abs/2505.17730)
*Stefan Schoepf, Michael Curtis Mozer, Nicole Elyse Mitchell, Alexandra Brintrup, Georgios Kaissis, Peter Kairouz, Eleni Triantafillou*

**主要类别:** cs.LG

**概要:** 本研究提出了一种新的方法REM，用于解决视觉分类器中多样化的损坏数据遗忘任务。与以往仅在特定区域有效的方法不同，REM通过将损坏数据重定向到专用神经元并在遗忘时丢弃或停用这些神经元来抑制损坏数据的影响，在整个任务空间中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 当前的机器遗忘方法针对特定任务进行了专门化，这使得系统性比较变得困难。因此，需要一种能够在更广泛的任务范围内有效的方法。

**方法:** 提出了一种名为Redirection for Erasing Memory（REM）的新方法，该方法通过将损坏数据重定向到遗忘时引入的专用神经元，并随后丢弃或停用这些神经元以抑制损坏数据的影响。

**结果:** REM在涵盖发现率和损坏数据统计规律性的整个任务空间中表现出色，而以前的最先进方法在其设计区域之外则失败。

**结论:** REM作为一种新的机器遗忘方法，能够有效应对视觉分类器中的多样化损坏数据遗忘任务，其性能优于现有的SOTA方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Redirection+for+Erasing+Memory+%28REM%29%3A+Towards+a+universal+unlearning+method+for+corrupted+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17730，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17730&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning is studied for a multitude of tasks, but specialization of
unlearning methods to particular tasks has made their systematic comparison
challenging. To address this issue, we propose a conceptual space to
characterize diverse corrupted data unlearning tasks in vision classifiers.
This space is described by two dimensions, the discovery rate (the fraction of
the corrupted data that are known at unlearning time) and the statistical
regularity of the corrupted data (from random exemplars to shared concepts).
Methods proposed previously have been targeted at portions of this space and-we
show-fail predictably outside these regions. We propose a novel method,
Redirection for Erasing Memory (REM), whose key feature is that corrupted data
are redirected to dedicated neurons introduced at unlearning time and then
discarded or deactivated to suppress the influence of corrupted data. REM
performs strongly across the space of tasks, in contrast to prior SOTA methods
that fail outside the regions for which they were designed.

</details>


### [121] [Data Mixing Can Induce Phase Transitions in Knowledge Acquisition](https://arxiv.org/abs/2505.18091)
*Xinran Gu, Kaifeng Lyu, Jiazheng Li, Jingzhao Zhang*

**主要类别:** cs.LG

**概要:** 大型语言模型（LLMs）在知识密集型数据集上的知识获取并不总是遵循平滑的扩展规律，而可能随着混合比例和模型大小出现相变。通过控制实验，我们展示了模型大小和混合比例如何影响对合成传记数据的记忆，并用信息论框架解释了这些相变。这表明，对于大模型有效的数据混合策略可能对小模型无效，反之亦然。


<details>
  <summary>更多</summary>
  
**动机:** 了解大型语言模型在包含少量高质量、领域特定知识的数据混合中的知识获取机制，以及这种获取是否遵循平滑扩展规律或存在相变现象。

**方法:** 使用合成传记数据集与网络抓取数据混合进行受控实验，研究模型大小和混合比例对知识获取的影响。采用信息论框架形式化容量分配现象，分析相变的可预测性及关键混合比例与模型大小之间的幂律关系。

**结果:** 1. 当模型大小达到临界值时，模型从几乎不记忆传记转变为记忆大部分传记；2. 在低于临界混合比例的情况下，即使经过大量训练，模型也几乎不记忆传记，但超过该阈值后，记忆迅速增加；3. 相变现象可以用容量分配问题解释，关键混合比例与模型大小之间存在幂律关系。

**结论:** 数据混合策略的有效性取决于模型大小，适用于大模型的策略可能不适合小模型，反之亦然。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Mixing+Can+Induce+Phase+Transitions+in+Knowledge+Acquisition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18091，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18091&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are typically trained on data mixtures: most
data come from web scrapes, while a small portion is curated from high-quality
sources with dense domain-specific knowledge. In this paper, we show that when
training LLMs on such data mixtures, knowledge acquisition from knowledge-dense
datasets, unlike training exclusively on knowledge-dense data
(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit
phase transitions with respect to the mixing ratio and model size. Through
controlled experiments on a synthetic biography dataset mixed with web-scraped
data, we demonstrate that: (1) as we increase the model size to a critical
value, the model suddenly transitions from memorizing very few to most of the
biographies; (2) below a critical mixing ratio, the model memorizes almost
nothing even with extensive training, but beyond this threshold, it rapidly
memorizes more biographies. We attribute these phase transitions to a capacity
allocation phenomenon: a model with bounded capacity must act like a knapsack
problem solver to minimize the overall test loss, and the optimal allocation
across datasets can change discontinuously as the model size or mixing ratio
varies. We formalize this intuition in an information-theoretic framework and
reveal that these phase transitions are predictable, with the critical mixing
ratio following a power-law relationship with the model size. Our findings
highlight a concrete case where a good mixing recipe for large models may not
be optimal for small models, and vice versa.

</details>


### [122] [URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles](https://arxiv.org/abs/2505.17734)
*Ahmet Onur Akman, Anastasia Psarou, Michał Hoffmann, Łukasz Gorczyca, Łukasz Kowalski, Paweł Gora, Grzegorz Jamróz, Rafał Kucharski*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是URB+--+Urban+Routing+Benchmark+for+RL-equipped+Connected+Autonomous+Vehicles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17734，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17734&send_immediately=true&force_search=false)

**原文摘要:** Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future
urban networks, potentially by optimizing their routing decisions. Unlike for
human drivers, these decisions can be made with collective, data-driven
policies, developed by machine learning algorithms. Reinforcement learning (RL)
can facilitate the development of such collective routing strategies, yet
standardized and realistic benchmarks are missing. To that end, we present
\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.
\our{} is a comprehensive benchmarking environment that unifies evaluation
across 29 real-world traffic networks paired with realistic demand patterns.
\our{} comes with a catalog of predefined tasks, four state-of-the-art
multi-agent RL (MARL) algorithm implementations, three baseline methods,
domain-specific performance metrics, and a modular configuration scheme. Our
results suggest that, despite the lengthy and costly training, state-of-the-art
MARL algorithms rarely outperformed humans. Experimental results reported in
this paper initiate the first leaderboard for MARL in large-scale urban routing
optimization and reveal that current approaches struggle to scale, emphasizing
the urgent need for advancements in this domain.

</details>


### [123] [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
*Takashi Ishida, Thanawat Lodkaew, Ikko Yamane*

**主要类别:** cs.LG

**概要:** 提出了一种新的基准测试发布方法，通过在答案中引入随机性来防止数据泄露和过拟合，并能有效检测数据污染。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大型语言模型（LLM）基准测试存在数据泄露和过拟合的风险，传统的私有基准测试需要信任单一组织且仍允许过拟合。

**方法:** 通过为每个问题准备多个逻辑上正确的答案并在基准测试中仅包含其中一个，向答案中注入随机性，从而降低贝叶斯准确率上限，这种方法既能防止完全披露真实答案，又能检测数据污染。

**结果:** 实验表明，该方法能够在各种基准测试、模型和训练方法中准确检测数据污染。

**结论:** 所提出的方法可以有效防止数据泄露和过拟合，同时提供一种检测数据污染的手段，为公开评估LLM提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Can+I+Publish+My+LLM+Benchmark+Without+Giving+the+True+Answers+Away%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18102&send_immediately=true&force_search=false)

**原文摘要:** Publishing a large language model (LLM) benchmark on the Internet risks
contaminating future LLMs: the benchmark may be unintentionally (or
intentionally) used to train or select a model. A common mitigation is to keep
the benchmark private and let participants submit their models or predictions
to the organizers. However, this strategy will require trust in a single
organization and still permits test-set overfitting through repeated queries.
To overcome this issue, we propose a way to publish benchmarks without
completely disclosing the ground-truth answers to the questions, while still
maintaining the ability to openly evaluate LLMs. Our main idea is to inject
randomness to the answers by preparing several logically correct answers, and
only include one of them as the solution in the benchmark. This reduces the
best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is
this helpful to keep us from disclosing the ground truth, but this approach
also offers a test for detecting data contamination. In principle, even fully
capable models should not surpass the Bayes accuracy. If a model surpasses this
ceiling despite this expectation, this is a strong signal of data
contamination. We present experimental evidence that our method can detect data
contamination accurately on a wide range of benchmarks, models, and training
methodologies.

</details>


### [124] [A tensor network approach for chaotic time series prediction](https://arxiv.org/abs/2505.17740)
*Rodrigo Martínez-Peña, Román Orús*

**主要类别:** cs.LG

**概要:** 为了提高混沌时间序列预测的准确性，本文研究了一种基于张量网络模型的方法，并证明其在准确性和计算效率方面优于传统的回声状态网络。此方法通过将多维数组分解为低维结构来缓解维度灾难问题，同时结合了新一代储备池计算的优势。


<details>
  <summary>更多</summary>
  
**动机:** 尽管储备池计算在混沌时间序列预测中表现出色，但选择和优化储备池架构仍是一个未解决的问题。下一代储备池计算通过非线性向量自回归（基于截断Volterra级数）简化了这一问题，但仍然面临参数数量随最大单项度指数增长的挑战。

**方法:** 本文应用了一种先前提出的张量网络模型来预测混沌时间序列。该模型利用张量网络将多维数组分解为低维结构，从而缓解了维度灾难问题，同时保留了储备池计算的记忆和非线性特性。

**结果:** 与传统的回声状态网络相比，该方法在预测混沌时间序列时表现出了更高的准确性和计算效率。

**结论:** 本文提出的方法成功地将张量网络与储备池计算相结合，不仅提高了混沌时间序列预测的性能，还促进了张量网络和储备池计算两个领域的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+tensor+network+approach+for+chaotic+time+series+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17740，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17740&send_immediately=true&force_search=false)

**原文摘要:** Making accurate predictions of chaotic time series is a complex challenge.
Reservoir computing, a neuromorphic-inspired approach, has emerged as a
powerful tool for this task. It exploits the memory and nonlinearity of
dynamical systems without requiring extensive parameter tuning. However,
selecting and optimizing reservoir architectures remains an open problem.
Next-generation reservoir computing simplifies this problem by employing
nonlinear vector autoregression based on truncated Volterra series, thereby
reducing hyperparameter complexity. Nevertheless, the latter suffers from
exponential parameter growth in terms of the maximum monomial degree. Tensor
networks offer a promising solution to this issue by decomposing
multidimensional arrays into low-dimensional structures, thus mitigating the
curse of dimensionality. This paper explores the application of a previously
proposed tensor network model for predicting chaotic time series, demonstrating
its advantages in terms of accuracy and computational efficiency compared to
conventional echo state networks. Using a state-of-the-art tensor network
approach enables us to bridge the gap between the tensor network and reservoir
computing communities, fostering advances in both fields.

</details>


### [125] [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/abs/2505.18126)
*Lorenz Wolf, Robert Kirk, Mirco Musolesi*

**主要类别:** cs.LG

**概要:** 本文研究了迭代RLHF中的过优化问题，提供了实际见解以构建更稳定和通用的RLHF流程。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习从人类反馈（RLHF）是使大型语言模型与人类偏好对齐的常用方法，但其常面临奖励模型过优化的问题，导致策略无法泛化。虽然迭代RLHF被广泛采用，但其中的过优化动态仍不甚了解。

**方法:** 通过系统分析关键设计选择，包括奖励模型训练数据如何跨迭代转移、用于优化的奖励函数以及策略初始化方式，并使用AlpacaFarm基准进行受控实验来观察过优化现象。

**结果:** 发现过优化在连续迭代中趋于减少，因为奖励模型逐渐逼近真实偏好；然而，性能增益随时间递减，且重新初始化基础策略虽稳健但限制了优化灵活性，其他初始化策略通常无法从早期过优化中恢复。

**结论:** 这些研究结果为构建更稳定和可泛化的RLHF管道提供了可行的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reward+Model+Overoptimisation+in+Iterated+RLHF，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18126，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18126&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning from human feedback (RLHF) is a widely used method for
aligning large language models with human preferences. However, RLHF often
suffers from reward model overoptimisation, in which models overfit to the
reward function, resulting in non-generalisable policies that exploit the
idiosyncrasies and peculiarities of the reward function. A common mitigation is
iterated RLHF, in which reward models are repeatedly retrained with updated
human feedback and policies are re-optimised. Despite its increasing adoption,
the dynamics of overoptimisation in this setting remain poorly understood. In
this work, we present the first comprehensive study of overoptimisation in
iterated RLHF. We systematically analyse key design choices - how reward model
training data is transferred across iterations, which reward function is used
for optimisation, and how policies are initialised. Using the controlled
AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over
successive iterations, as reward models increasingly approximate ground-truth
preferences. However, performance gains diminish over time, and while
reinitialising from the base policy is robust, it limits optimisation
flexibility. Other initialisation strategies often fail to recover from early
overoptimisation. These findings offer actionable insights for building more
stable and generalisable RLHF pipelines.

</details>


### [126] [Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement](https://arxiv.org/abs/2505.18131)
*Jonas A. Actor, Graham Harper, Ben Southworth, Eric C. Cyr*

**主要类别:** cs.LG

**概要:** 本研究探讨了Kolmogorov-Arnold Networks (KANs)和多层感知机(MLPs)之间的关系，提出了一种分层优化方案，显著加速了多通道MLP的训练，并通过同时训练样条节点位置进一步提高了准确性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管多层感知机（MLPs）在深度学习中广泛应用，但Kolmogorov-Arnold Networks (KANs)因其在科学机器学习任务中的成功而越来越受到关注。为了获得如何更快地训练MLPs的结构洞察，作者利用了KANs与多通道MLPs之间的关系。

**方法:** 作者展示了KAN基底提供了几何局部支持并作为ReLU基底的预调节下降，从而加快了训练速度和提高了准确性。他们还证明了自由结样条KAN架构与一类沿每个权重张量的通道维度几何细化的MLPs等价。基于这种结构等价性，作者定义了一个层次细化方案以加速多通道MLP架构的训练，并通过同时训练样条节点位置来进一步提高准确性。

**结果:** 实验结果表明，提出的层次细化方案显著加速了多通道MLP架构的训练，并且通过同时训练样条节点位置可以进一步提高模型的准确性。这些改进在一系列回归和科学机器学习基准测试中得到了验证。

**结论:** KANs和MLPs之间存在紧密的关系，利用这种关系不仅可以加快MLP的训练速度，还可以提高其准确性。所提出的层次细化方案为多通道MLP架构提供了一种有效的训练方法，具有重要的实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+KANs+for+Expedient+Training+of+Multichannel+MLPs+via+Preconditioning+and+Geometric+Refinement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18131，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18131&send_immediately=true&force_search=false)

**原文摘要:** Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,
used in a variety of modern deep learning frameworks. However, recently
Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their
success on a range of problems, particularly for scientific machine learning
tasks. In this paper, we exploit the relationship between KANs and multichannel
MLPs to gain structural insight into how to train MLPs faster. We demonstrate
the KAN basis (1) provides geometric localized support, and (2) acts as a
preconditioned descent in the ReLU basis, overall resulting in expedited
training and improved accuracy. Our results show the equivalence between
free-knot spline KAN architectures, and a class of MLPs that are refined
geometrically along the channel dimension of each weight tensor. We exploit
this structural equivalence to define a hierarchical refinement scheme that
dramatically accelerates training of the multi-channel MLP architecture. We
show further accuracy improvements can be had by allowing the $1$D locations of
the spline knots to be trained simultaneously with the weights. These advances
are demonstrated on a range of benchmark examples for regression and scientific
machine learning.

</details>


### [127] [Soft-CAM: Making black box models self-explainable for high-stakes decisions](https://arxiv.org/abs/2505.17748)
*Kerol Djoumessi, Philipp Berens*

**主要类别:** cs.LG

**概要:** SoftCAM是一种使标准CNN架构固有可解释性的方法，通过移除全局平均池化层并用基于卷积的类别证据层替代全连接分类层，保留空间信息并生成明确的类别激活图。在三个医学数据集上的评估表明，SoftCAM在不损害分类性能的同时显著提高了定性和定量解释能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管卷积神经网络（CNNs）在许多重要领域（如医学）中的表现常常超过人类，但现有的解释方法通常依赖于事后归因，这些方法对已经训练好的黑箱模型的决策过程进行近似，存在敏感性、不可靠性等问题，难以反映模型的真实推理过程。因此，研究者希望开发一种使CNN本身具备可解释性的方法，以提高其在关键应用中的可信度。

**方法:** SoftCAM方法通过移除全局平均池化层，并将全连接分类层替换为基于卷积的类别证据层，从而保留了空间信息并生成了明确的类别激活图（class activation maps），这些图构成了模型预测的基础。

**结果:** 在三个医学数据集上的实验结果表明，SoftCAM在保持分类性能的同时，显著提升了相对于现有事后解释方法的定性和定量解释能力。

**结论:** SoftCAM证明了CNN可以在不牺牲性能的情况下实现固有的可解释性，推动了适用于高风险决策的自解释深度学习的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Soft-CAM%3A+Making+black+box+models+self-explainable+for+high-stakes+decisions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17748，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17748&send_immediately=true&force_search=false)

**原文摘要:** Convolutional neural networks (CNNs) are widely used for high-stakes
applications like medicine, often surpassing human performance. However, most
explanation methods rely on post-hoc attribution, approximating the
decision-making process of already trained black-box models. These methods are
often sensitive, unreliable, and fail to reflect true model reasoning, limiting
their trustworthiness in critical applications. In this work, we introduce
SoftCAM, a straightforward yet effective approach that makes standard CNN
architectures inherently interpretable. By removing the global average pooling
layer and replacing the fully connected classification layer with a
convolution-based class evidence layer, SoftCAM preserves spatial information
and produces explicit class activation maps that form the basis of the model's
predictions. Evaluated on three medical datasets, SoftCAM maintains
classification performance while significantly improving both the qualitative
and quantitative explanation compared to existing post-hoc methods. Our results
demonstrate that CNNs can be inherently interpretable without compromising
performance, advancing the development of self-explainable deep learning for
high-stakes decision-making.

</details>


### [128] [Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models](https://arxiv.org/abs/2505.17761)
*Benjamin Walker, Lingyi Yang, Nicola Muca Cirone, Cristopher Salvi, Terry Lyons*

**主要类别:** cs.LG

**概要:** Structured Linear Controlled Differential Equations (SLiCEs) 是一种新的序列模型框架，它结合了结构化、依赖输入的状态转换矩阵，在保持密集矩阵最大表达能力的同时降低了计算成本。SLiCEs 包括现有的架构以及两种基于稀疏性和 Walsh-Hadamard 变换的新变体。理论证明和实验结果表明，SLiCEs 在多项任务中表现出色，并显著减少了训练时间。


<details>
  <summary>更多</summary>
  
**动机:** 当前的序列模型在处理复杂任务时需要高计算成本，因此研究者希望找到一种既能保持密集矩阵的最大表达能力，又能降低计算开销的方法。

**方法:** 提出了一种名为 Structured Linear Controlled Differential Equations (SLiCEs) 的新框架，其核心是使用结构化且依赖输入的状态转换矩阵。该框架涵盖了现有的一些架构（如输入依赖块对角线性递归神经网络和 DeltaNet 的对角加低秩结构），并引入了两种新变体：基于稀疏性和 Walsh-Hadamard 变换的矩阵形式。

**结果:** 理论上证明了使用块对角、稀疏或 Walsh-Hadamard 矩阵的 SLiCEs 能匹配密集矩阵的最大表达能力。实验证明 SLiCEs 在 $A_5$ 状态跟踪基准测试中只需单层即可解决问题；在正则语言任务中实现了最佳长度泛化；并在六个多元时间序列分类数据集上达到了与最先进的对数神经控制微分方程相当的性能，同时将平均每次训练步骤的时间减少了 20 倍。

**结论:** SLiCEs 提供了一个统一的框架，不仅保留了密集矩阵的最大表达能力，还通过结构化矩阵大幅降低了计算成本。这使得 SLiCEs 在多个任务中表现优异，尤其是在时间序列分类任务中具有竞争力，同时显著提高了训练效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Structured+Linear+CDEs%3A+Maximally+Expressive+and+Parallel-in-Time+Sequence+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17761&send_immediately=true&force_search=false)

**原文摘要:** Structured Linear Controlled Differential Equations (SLiCEs) provide a
unifying framework for sequence models with structured, input-dependent
state-transition matrices that retain the maximal expressivity of dense
matrices whilst being cheaper to compute. The framework encompasses existing
architectures, such as input-dependent block-diagonal linear recurrent neural
networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel
variants based on sparsity and the Walsh--Hadamard transform. We prove that,
unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing
block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal
expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$
state-tracking benchmark with a single layer, achieve best-in-class length
generalisation on regular language tasks among parallel-in-time models, and
match the state-of-the-art performance of log neural controlled differential
equations on six multivariate time-series classification datasets while cutting
the average time per training step by a factor of twenty.

</details>


### [129] [Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals](https://arxiv.org/abs/2505.17763)
*Julian Oelhaf, Georg Kordowich, Andreas Maier, Johann Jager, Siming Bayer*

**主要类别:** cs.LG

**概要:** 传感器在现代电网中的广泛应用导致了大量电压和电流波形数据的积累，尤其是在故障事件期间。然而，缺乏标记的数据集为故障分类和分析带来了重大挑战。本文探讨了无监督聚类技术在高压电力系统故障诊断中的应用。通过使用快速傅里叶变换（FFT）提取频率域特征，并应用K-Means算法识别数据中的潜在模式，从而实现无需标记训练样本的自动化故障分类。最终的聚类结果与电力系统专家合作评估，以确定其与真实世界故障特征的一致性。结果展示了无监督学习在可扩展和数据驱动的故障分析中的潜力，提供了一种检测和分类电力系统故障的稳健方法，且所需先验假设最少。


<details>
  <summary>更多</summary>
  
**动机:** 现代电网中传感器的广泛使用导致了大量的电压和电流波形数据积累，特别是在故障事件期间。然而，由于缺乏标记的数据集，给故障分类和分析带来了重大挑战。因此，需要一种不需要标记数据的方法来进行故障诊断。

**方法:** 1. 使用由Reseau de Transport d'Electricite (RTE) 提供的数据集。
2. 通过快速傅里叶变换（FFT）提取频率域特征。
3. 应用K-Means算法进行无监督聚类，识别数据中的潜在模式。
4. 与电力系统专家合作评估聚类结果，确保其与实际故障特征一致。

**结果:** 实验结果表明，无监督学习技术在大规模、数据驱动的故障分析中具有很大的潜力，能够有效地检测和分类电力系统故障，同时对先验假设的需求最小化。

**结论:** 无监督学习方法，特别是基于K-Means算法的聚类技术，为电力系统故障诊断提供了一个强有力的新工具。这种方法能够在无需大量标记数据的情况下，实现自动化的故障分类和分析，为未来的电力系统维护和管理提供了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+Clustering+for+Fault+Analysis+in+High-Voltage+Power+Systems+Using+Voltage+and+Current+Signals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17763&send_immediately=true&force_search=false)

**原文摘要:** The widespread use of sensors in modern power grids has led to the
accumulation of large amounts of voltage and current waveform data, especially
during fault events. However, the lack of labeled datasets poses a significant
challenge for fault classification and analysis. This paper explores the
application of unsupervised clustering techniques for fault diagnosis in
high-voltage power systems. A dataset provided by the Reseau de Transport
d'Electricite (RTE) is analyzed, with frequency domain features extracted using
the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to
identify underlying patterns in the data, enabling automated fault
categorization without the need for labeled training samples. The resulting
clusters are evaluated in collaboration with power system experts to assess
their alignment with real-world fault characteristics. The results demonstrate
the potential of unsupervised learning for scalable and data-driven fault
analysis, providing a robust approach to detecting and classifying power system
faults with minimal prior assumptions.

</details>


### [130] [Joker: Joint Optimization Framework for Lightweight Kernel Machines](https://arxiv.org/abs/2505.17765)
*Junhong Zhang, Zhihui Lai*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为Joker的联合优化框架，适用于包括核岭回归、逻辑回归和支持向量机在内的多种核模型。通过设计双块坐标下降法与信任区域（DBCD-TR）以及采用随机特征的核近似方法，Joker在大规模学习中具有低内存成本和高效率。实验表明，Joker可节省高达90%的内存，同时达到与现有最佳方法相当（甚至更好）的训练时间和性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大规模核方法存在两个主要限制：(i) 内存开销过高；(ii) 大多数研究集中于核岭回归，而其他模型的研究相对不足。为了解决这些问题，需要一个能处理多种核模型且内存占用低、效率高的框架。

**方法:** 提出了一个名为Joker的联合优化框架，涵盖了核岭回归、逻辑回归和支持向量机等多种核模型。设计了带有信任区域的双块坐标下降法（DBCD-TR），并采用了带有随机特征的核近似方法，从而降低了内存消耗并提高了大规模学习的效率。

**结果:** 实验结果表明，Joker能够节省高达90%的内存，同时保持与现有最佳方法相当（甚至更优）的训练时间和性能。

**结论:** Joker是一个有效的联合优化框架，能够在减少内存使用的同时，维持或提升训练效率和模型性能，适用于多种核模型的大规模学习任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Joker%3A+Joint+Optimization+Framework+for+Lightweight+Kernel+Machines，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17765&send_immediately=true&force_search=false)

**原文摘要:** Kernel methods are powerful tools for nonlinear learning with
well-established theory. The scalability issue has been their long-standing
challenge. Despite the existing success, there are two limitations in
large-scale kernel methods: (i) The memory overhead is too high for users to
afford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),
while other models lack study. In this paper, we propose Joker, a joint
optimization framework for diverse kernel models, including KRR, logistic
regression, and support vector machines. We design a dual block coordinate
descent method with trust region (DBCD-TR) and adopt kernel approximation with
randomized features, leading to low memory costs and high efficiency in
large-scale learning. Experiments show that Joker saves up to 90\% memory but
achieves comparable training time and performance (or even better) than the
state-of-the-art methods.

</details>


### [131] [Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models](https://arxiv.org/abs/2505.17769)
*Patrick Leask, Neel Nanda, Noura Al Moubayed*

**主要类别:** cs.LG

**概要:** ITDA是一种新的语言模型激活分解方法，相较于SAE，其训练时间更短、数据需求更少，并支持跨模型比较。尽管在某些LLM上重建性能稍逊于SAE，但ITDA在计算资源有限或需要跨模型比较时是更好的选择。


<details>
  <summary>更多</summary>
  
**动机:** 由于稀疏自编码器（SAE）的高训练成本和数据限制，且其潜变量无法在不同模型间迁移，研究者提出了一种基于贪婪构造词典的方法——推理时激活分解（ITDA），以实现对语言模型激活的有效分解。

**方法:** 通过在提示数据集上贪婪地构建语言模型激活词典，选择那些通过匹配追踪难以用现有词典近似的激活。这种方法仅需SAE训练时间的1%，以及1%的数据量。

**结果:** ITDA可以在Llama-3.1 70B和405B等大规模模型上使用单一消费级GPU进行训练，在某些目标LLM上达到与SAE相似的重建性能，同时支持跨模型比较，且Jaccard相似性指数优于现有方法如CKA、SVCCA和相对表示相似性度量。

**结论:** ITDA提供了一种低成本的替代方案，尤其适用于计算资源有限或需要跨模型比较的情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inference-Time+Decomposition+of+Activations+%28ITDA%29%3A+A+Scalable+Approach+to+Interpreting+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17769，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17769&send_immediately=true&force_search=false)

**原文摘要:** Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage
Models (LLM) activations into interpretable latents. However, due to their
substantial training cost, most academic research uses open-source SAEs which
are only available for a restricted set of models of up to 27B parameters. SAE
latents are also learned from a dataset of activations, which means they do not
transfer between models. Motivated by relative representation similarity
measures, we introduce Inference-Time Decomposition of Activations (ITDA)
models, an alternative method for decomposing language model activations. To
train an ITDA, we greedily construct a dictionary of language model activations
on a dataset of prompts, selecting those activations which were worst
approximated by matching pursuit on the existing dictionary. ITDAs can be
trained in just 1\% of the time required for SAEs, using 1\% of the data. This
allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.
ITDAs can achieve similar reconstruction performance to SAEs on some target
LLMs, but generally incur a performance penalty. However, ITDA dictionaries
enable cross-model comparisons, and a simple Jaccard similarity index on ITDA
dictionaries outperforms existing methods like CKA, SVCCA, and relative
representation similarity metrics. ITDAs provide a cheap alternative to SAEs
where computational resources are limited, or when cross model comparisons are
necessary. Code available at https://github.com/pleask/itda.

</details>


### [132] [C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2505.17773)
*Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian*

**主要类别:** cs.LG

**概要:** C-LoRA是一种新型的、具有上下文感知的低秩适配方法，通过动态调整不确定性估计，在数据稀缺的少样本场景中改进了LoRA方法的过拟合问题，并显著提升了不确定性和模型泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LoRA方法在少样本场景下容易产生过于自信的预测，而传统的统计学习方法虽然引入了不确定性估计，但忽略了输入特征对预测不确定性的影响。

**方法:** 提出了一种名为Contextual Low-Rank Adaptation (C-LoRA)的新方法，通过为每个输入数据样本开发轻量级且上下文相关的LoRA模块，动态调整不确定性估计，将数据驱动的上下文信息融入参数后验中。

**结果:** 实验表明，C-LoRA在不确定性和模型泛化方面始终优于最先进的不确定性感知LoRA方法。消融研究进一步验证了上下文模块在捕捉样本特定不确定性中的关键作用。

**结论:** C-LoRA为少样本场景下的鲁棒性、不确定性感知的大语言模型微调设定了新标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是C-LoRA%3A+Contextual+Low-Rank+Adaptation+for+Uncertainty+Estimation+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17773&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning
large language models (LLMs), but it often produces overconfident predictions
in data-scarce few-shot settings. To address this issue, several classical
statistical learning approaches have been repurposed for scalable
uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input
characteristics affect the predictive uncertainty estimates. To address this
limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a
novel uncertainty-aware and parameter efficient fine-tuning approach, by
developing new lightweight LoRA modules contextualized to each input data
sample to dynamically adapt uncertainty estimates. Incorporating data-driven
contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves
well-calibrated uncertainties, and yields robust predictions. Extensive
experiments demonstrate that C-LoRA consistently outperforms the
state-of-the-art uncertainty-aware LoRA methods in both uncertainty
quantification and model generalization. Ablation studies further confirm the
critical role of our contextual modules in capturing sample-specific
uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM
fine-tuning in few-shot regimes.

</details>


### [133] [Optimizing Shortfall Risk Metric for Learning Regression Models](https://arxiv.org/abs/2505.17777)
*Harish G. Ramaswamy, L. A. Prashanth*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Shortfall+Risk+Metric+for+Learning+Regression+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17777，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17777&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of estimating and optimizing utility-based shortfall
risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression
problem. Empirical risk minimization with a UBSR objective is challenging since
UBSR is a non-linear function of the underlying distribution. We first derive a
concentration bound for UBSR estimation using independent and identically
distributed (i.i.d.) samples. We then frame the UBSR optimization problem as
minimization of a pseudo-linear function in the space of achievable
distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient
oracle for the UBSR objective and a linear minimization oracle (LMO) for the
set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm,
and establish convergence to the UBSR-optimal solution.

</details>


### [134] [Supervised Graph Contrastive Learning for Gene Regulatory Network](https://arxiv.org/abs/2505.17786)
*Sho Oshima, Yuji Okamoto, Taisei Tosaki, Ryosuke Kojima, Yasushi Okuno*

**主要类别:** cs.LG

**概要:** SupGCL是一种新的图对比学习方法，它将生物基因敲除实验作为监督信息整合到GRN中，从而提升生物下游任务的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图对比学习（GCL）方法在应用于生物网络（如基因调控网络）时，忽略了有意义的生物相关扰动（如基因敲除）。

**方法:** 引入SupGCL（有监督图对比学习），一种新的GCL方法，它直接将来自基因敲除实验的生物扰动作为监督信息整合到GRN中。SupGCL从数学上扩展了现有的GCL方法，利用非生物扰动的概率模型，引入实际的生物基因扰动数据。

**结果:** 在多种癌症患者的实际GRN数据集上应用SupGCL，在所有实验中，SupGCL的表现优于最先进的基线方法。

**结论:** SupGCL能够提高生物下游任务的性能，包括患者风险预测、疾病亚型分类和基因功能分类等任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supervised+Graph+Contrastive+Learning+for+Gene+Regulatory+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17786，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17786&send_immediately=true&force_search=false)

**原文摘要:** Graph representation learning is effective for obtaining a meaningful latent
space utilizing the structure of graph data and is widely applied, including
biological networks. In particular, Graph Contrastive Learning (GCL) has
emerged as a powerful self-supervised method that relies on applying
perturbations to graphs for data augmentation. However, when applying existing
GCL methods to biological networks such as Gene Regulatory Networks (GRNs),
they overlooked meaningful biologically relevant perturbations, e.g., gene
knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive
Learning), a novel GCL method for GRNs that directly incorporates biological
perturbations derived from gene knockdown experiments as the supervision.
SupGCL mathematically extends existing GCL methods that utilize non-biological
perturbations to probabilistic models that introduce actual biological gene
perturbation utilizing gene knockdown data. Using the GRN representation
obtained by our proposed method, our aim is to improve the performance of
biological downstream tasks such as patient hazard prediction and disease
subtype classification (graph-level task), and gene function classification
(node-level task). We applied SupGCL on real GRN datasets derived from patients
with multiple types of cancer, and in all experiments SupGCL achieves better
performance than state-of-the-art baselines.

</details>


### [135] [RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion](https://arxiv.org/abs/2505.17794)
*Ömer Faruk Akgül, Feiyu Zhu, Yuxin Yang, Rajgopal Kannan, Viktor Prasanna*

**主要类别:** cs.LG

**概要:** 论文提出了一种名为RECIPE-TKG的轻量级框架，通过规则驱动的多跳检索、对比微调和测试时语义过滤等技术，在时间知识图谱补全任务中显著提高了准确性和泛化能力。在四个基准数据集上实验表明，该方法比现有基于LLM的方法性能更好，特别是在历史数据稀疏的情况下。


<details>
  <summary>更多</summary>
  
**动机:** 时间知识图谱（TKGs）补全任务需要模型能够预测缺失或未来的链接，但现有的大语言模型（LLMs）方法过于依赖监督式微调，并且在历史证据有限或缺失时表现不佳。因此，研究者希望开发一种更高效、更轻量化的框架，以提高模型在这种稀疏历史背景下的准确性和泛化能力。

**方法:** RECIPE-TKG框架包含三个主要部分：1) 规则驱动的多跳检索，用于获取结构多样化的历史信息；2) 对比微调轻量级适配器，用于编码关系语义；3) 测试时语义过滤，通过嵌入相似性迭代地优化生成结果。这些技术共同作用，帮助模型更好地处理稀疏历史数据的情况。

**结果:** 在四个时间知识图谱基准数据集上的实验表明，RECIPE-TKG相比之前的LLM方法具有更高的性能，特别是在Hits@10指标上实现了高达30.6%的相对提升。此外，该框架生成的预测结果在语义上更加连贯，即使对于历史数据有限的样本也是如此。

**结论:** RECIPE-TKG是一种轻量级且数据高效的框架，适用于历史数据稀疏的时间知识图谱补全任务。它通过结合规则驱动的检索、对比微调和语义过滤等技术，显著提高了模型的准确性和泛化能力，为未来的研究提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RECIPE-TKG%3A+From+Sparse+History+to+Structured+Reasoning+for+LLM-based+Temporal+Knowledge+Graph+Completion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17794&send_immediately=true&force_search=false)

**原文摘要:** Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped
relations between entities. TKG completion involves forecasting missing or
future links, requiring models to reason over time-evolving structure. While
LLMs show promise for this task, existing approaches often overemphasize
supervised fine-tuning and struggle particularly when historical evidence is
limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient
framework designed to improve accuracy and generalization in settings with
sparse historical context. It combines (1) rule-based multi-hop retrieval for
structurally diverse history, (2) contrastive fine-tuning of lightweight
adapters to encode relational semantics, and (3) test-time semantic filtering
to iteratively refine generations based on embedding similarity. Experiments on
four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based
approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover,
our proposed framework produces more semantically coherent predictions, even
for the samples with limited historical context.

</details>


### [136] [Latent Mode Decomposition](https://arxiv.org/abs/2505.17797)
*Manuel Morante, Naveed ur Rehman*

**主要类别:** cs.LG

**概要:** 提出了一种新的算法变分潜在模式分解（VLMD），用于从多元信号中提取振荡模式及其相关的连通性结构。通过在低维潜在空间操作并解决现有MMD技术的局限性，如高计算成本、参数选择敏感性和通道间依赖性弱建模，VLMD表现出更优的性能。实验表明，VLMD在准确性、效率和可解释性方面优于现有的MMD方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多变量模式分解（MMD）技术存在高计算成本、对参数选择敏感以及对通道间依赖性建模较弱的问题，因此需要一种改进的方法来提高模式分解的效果。

**方法:** 引入了潜在模式分解（LMD）这一新型基础模型，将稀疏编码和模式分解结合，表示多通道信号为共享潜在组件的稀疏线性组合，这些组件由AM-FM振荡模式组成。VLMD通过求解一个约束变分优化问题，在重建保真度、稀疏性和频率正则化之间达到平衡。

**结果:** 在合成和真实世界数据集上的实验证明，与最先进的MMD方法相比，VLMD在准确性、效率和提取结构的可解释性方面表现更好。

**结论:** VLMD通过在低维潜在空间中的操作，提高了对噪声的鲁棒性、可扩展性和可解释性，是一种有效的多元信号模式分解工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Mode+Decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17797，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17797&send_immediately=true&force_search=false)

**原文摘要:** We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm
for extracting oscillatory modes and associated connectivity structures from
multivariate signals. VLMD addresses key limitations of existing Multivariate
Mode Decomposition (MMD) techniques -including high computational cost,
sensitivity to parameter choices, and weak modeling of interchannel
dependencies. Its improved performance is driven by a novel underlying model,
Latent Mode Decomposition (LMD), which blends sparse coding and mode
decomposition to represent multichannel signals as sparse linear combinations
of shared latent components composed of AM-FM oscillatory modes. This
formulation enables VLMD to operate in a lower-dimensional latent space,
enhancing robustness to noise, scalability, and interpretability. The algorithm
solves a constrained variational optimization problem that jointly enforces
reconstruction fidelity, sparsity, and frequency regularization. Experiments on
synthetic and real-world datasets demonstrate that VLMD outperforms
state-of-the-art MMD methods in accuracy, efficiency, and interpretability of
extracted structures.

</details>


### [137] [A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances](https://arxiv.org/abs/2505.17799)
*Brian B. Moser, Arundhati S. Shanbhag, Stanislav Frolov, Federico Raue, Joachim Folz, Andreas Dengel*

**主要类别:** cs.LG

**概要:** Coreset selection aims to find a small, representative subset of data that preserves essential patterns for machine learning. This survey unifies three major coreset research lines (training-free, training-oriented, label-free) and explores subfields like submodular formulations, bilevel optimization, and pseudo-labeling. It also examines pruning strategies' influence on generalization and neural scaling laws, compares methods under different demands, and highlights open challenges.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the challenge of data reduction in large datasets while preserving essential patterns for effective machine learning. The survey aims to provide a more comprehensive view by integrating various coreset research approaches.

**方法:** The paper employs a taxonomy-based approach to unify three major coreset research lines: training-free, training-oriented, and label-free. It also discusses subfields such as submodular formulations, bilevel optimization, and pseudo-labeling, along with the impact of pruning strategies on generalization and neural scaling laws.

**结果:** The result is a comprehensive survey that not only integrates different coreset approaches but also offers insights into overlooked subfields and the effects of pruning strategies. It provides a comparative analysis of methods under varying demands.

**结论:** The conclusion highlights open challenges in coreset selection, including robustness, outlier filtering, and adapting coreset selection to foundation models, which need further investigation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Coreset+Selection+of+Coreset+Selection+Literature%3A+Introduction+and+Recent+Advances，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17799&send_immediately=true&force_search=false)

**原文摘要:** Coreset selection targets the challenge of finding a small, representative
subset of a large dataset that preserves essential patterns for effective
machine learning. Although several surveys have examined data reduction
strategies before, most focus narrowly on either classical geometry-based
methods or active learning techniques. In contrast, this survey presents a more
comprehensive view by unifying three major lines of coreset research, namely,
training-free, training-oriented, and label-free approaches, into a single
taxonomy. We present subfields often overlooked by existing work, including
submodular formulations, bilevel optimization, and recent progress in
pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning
strategies influence generalization and neural scaling laws, offering new
insights that are absent from prior reviews. Finally, we compare these methods
under varying computational, robustness, and performance demands and highlight
open challenges, such as robustness, outlier filtering, and adapting coreset
selection to foundation models, for future research.

</details>


### [138] [VIBE: Vector Index Benchmark for Embeddings](https://arxiv.org/abs/2505.17810)
*Elias Jääsaari, Ville Hyvönen, Matteo Ceccarello, Teemu Roos, Martin Aumüller*

**主要类别:** cs.LG

**概要:** 本论文介绍了Vector Index Benchmark for Embeddings (VIBE)，一个用于评估近似最近邻(ANN)搜索算法的开源项目。VIBE通过现代密集嵌入模型生成基准数据集，并包含来自不同分布的查询和语料库的OOD数据集，以模拟真实世界的工作负载。使用VIBE对21种实现进行了全面评估，涵盖了12个同分布和6个异分布数据集。


<details>
  <summary>更多</summary>
  
**动机:** 现有的ANN搜索基准测试数据集不再代表当前的应用场景，因此需要一套最新的基准来准确评估向量索引的性能。

**方法:** 引入了VIBE，一个开源项目，包含使用现代密集嵌入模型（如检索增强生成RAG）创建基准数据集的管道，以及模拟真实工作负载的OOD数据集。

**结果:** 通过对21种实现进行基准测试，展示了SOTA向量索引在不同数据集上的性能表现。

**结论:** VIBE提供了一个全面且现代化的基准平台，有助于更准确地评估ANN算法的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VIBE%3A+Vector+Index+Benchmark+for+Embeddings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17810&send_immediately=true&force_search=false)

**原文摘要:** Approximate nearest neighbor (ANN) search is a performance-critical component
of many machine learning pipelines. Rigorous benchmarking is essential for
evaluating the performance of vector indexes for ANN search. However, the
datasets of the existing benchmarks are no longer representative of the current
applications of ANN search. Hence, there is an urgent need for an up-to-date
set of benchmarks. To this end, we introduce Vector Index Benchmark for
Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE
contains a pipeline for creating benchmark datasets using dense embedding
models characteristic of modern applications, such as retrieval-augmented
generation (RAG). To replicate real-world workloads, we also include
out-of-distribution (OOD) datasets where the queries and the corpus are drawn
from different distributions. We use VIBE to conduct a comprehensive evaluation
of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution
and 6 out-of-distribution datasets.

</details>


### [139] [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
*Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou*

**主要类别:** cs.LG

**概要:** Trinity-RFT 是一个通用、灵活且可扩展的框架，旨在对大语言模型进行强化微调（RFT）。它具有解耦设计，包含统一同步/异步、策略内/策略外和在线/离线模式的 RFT 核心，高效的环境交互集成以及针对 RFT 优化的系统数据管道。该框架适用于多种应用场景，并为探索先进的强化学习范式提供了统一平台。


<details>
  <summary>更多</summary>
  
**动机:** 当前在对大型语言模型进行强化微调时，可能缺乏一种通用、灵活且可扩展的框架来统一处理不同模式下的微调需求，包括同步/异步、策略内/策略外和在线/离线模式等。此外，还需要一种能够高效且稳健地与环境交互并提供优化数据管道的解决方案。因此，提出 Trinity-RFT 来满足这些需求。

**方法:** Trinity-RFT 框架采用了模块化设计：1) RFT-core 统一了不同模式下的强化微调过程；2) 提供高效的代理-环境交互接口；3) 系统化的数据管道以支持强化微调。通过这些组件，Trinity-RFT 能够适应各种应用情景，并作为探索先进强化学习方法的统一平台。

**结果:** Trinity-RFT 成功实现了一个通用、灵活且可扩展的强化微调框架，可以轻松适应不同的应用场景。技术报告中提供的广泛示例证明了该框架的实用性和用户友好性。

**结论:** Trinity-RFT 是一个功能强大的框架，适用于大型语言模型的强化微调。其模块化设计和广泛的适用性使其成为探索先进强化学习范式的理想平台。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Trinity-RFT%3A+A+General-Purpose+and+Unified+Framework+for+Reinforcement+Fine-Tuning+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17826，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17826&send_immediately=true&force_search=false)

**原文摘要:** Trinity-RFT is a general-purpose, flexible and scalable framework designed
for reinforcement fine-tuning (RFT) of large language models. It is built with
a decoupled design, consisting of (1) an RFT-core that unifies and generalizes
synchronous/asynchronous, on-policy/off-policy, and online/offline modes of
RFT, (2) seamless integration for agent-environment interaction with high
efficiency and robustness, and (3) systematic data pipelines optimized for RFT.
Trinity-RFT can be easily adapted for diverse application scenarios, and serves
as a unified platform for exploring advanced reinforcement learning paradigms.
This technical report outlines the vision, features, design and implementations
of Trinity-RFT, accompanied by extensive examples demonstrating the utility and
user-friendliness of the proposed framework.

</details>


### [140] [Out of the Shadows: Exploring a Latent Space for Neural Network Verification](https://arxiv.org/abs/2505.17854)
*Lukas Koller, Tobias Ladner, Matthias Althoff*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的潜在空间设计，用于神经网络的形式验证，通过迭代规范驱动的输入细化减少不确定结果，并利用zonotopes集表示和矩阵运算实现高效的GPU加速，显著减少了分支定界过程中的子问题数量。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络在安全关键应用中容易受到小输入变化的影响，形式验证对于确保其行为可预测至关重要。然而，现有的验证方法由于保守性常常无法得出结论。

**方法:** 设计了一个新的潜在空间，用于形式验证，将输出规范转移到输入空间进行迭代细化，缩小可能输入的范围。该潜在空间基于投影集表示（如zonotopes），通过矩阵运算实现高效的GPU加速。

**结果:** 与许多现有方法不同，本文的方法仅使用矩阵运算即可实现，因此可以通过GPU加速显著提高速度。实验表明，该工具性能优异，在最近的神经网络验证竞赛中处于顶尖水平。

**结论:** 提出的潜在空间和验证工具能够有效减少分支定界过程中的子问题数量，同时通过GPU加速提升效率，为神经网络的形式验证提供了新的高效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Out+of+the+Shadows%3A+Exploring+a+Latent+Space+for+Neural+Network+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17854&send_immediately=true&force_search=false)

**原文摘要:** Neural networks are ubiquitous. However, they are often sensitive to small
input changes. Hence, to prevent unexpected behavior in safety-critical
applications, their formal verification -- a notoriously hard problem -- is
necessary. Many state-of-the-art verification algorithms use reachability
analysis or abstract interpretation to enclose the set of possible outputs of a
neural network. Often, the verification is inconclusive due to the conservatism
of the enclosure. To address this problem, we design a novel latent space for
formal verification that enables the transfer of output specifications to the
input space for an iterative specification-driven input refinement, i.e., we
iteratively reduce the set of possible inputs to only enclose the unsafe ones.
The latent space is constructed from a novel view of projection-based set
representations, e.g., zonotopes, which are commonly used in reachability
analysis of neural networks. A projection-based set representation is a
"shadow" of a higher-dimensional set -- a latent space -- that does not change
during a set propagation through a neural network. Hence, the input set and the
output enclosure are "shadows" of the same latent space that we can use to
transfer constraints. We present an efficient verification tool for neural
networks that uses our iterative refinement to significantly reduce the number
of subproblems in a branch-and-bound procedure. Using zonotopes as a set
representation, unlike many other state-of-the-art approaches, our approach can
be realized by only using matrix operations, which enables a significant
speed-up through efficient GPU acceleration. We demonstrate that our tool
achieves competitive performance, which would place it among the top-ranking
tools of the last neural network verification competition (VNN-COMP'24).

</details>


### [141] [The emergence of sparse attention: impact of data distribution and benefits of repetition](https://arxiv.org/abs/2505.17863)
*Nicolas Zucchet, Francesco d'Angelo, Andrew K. Lampinen, Stephanie C. Y. Chan*

**主要类别:** cs.LG

**概要:** 研究了稀疏注意力在训练过程中的出现机制，揭示了基于任务结构、架构和优化器选择的幂律规律，并确认重复可以加速出现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管初步研究表明大规模语言模型和神经网络具有涌现特性，但我们仍然缺乏对这些能力如何以及何时出现的全面理解。

**方法:** 通过结合玩具模型的理论分析与在小型Transformer上进行线性回归变体的实证观察，研究稀疏注意力在训练过程中的出现机制。

**结果:** 发现稀疏注意力出现的时间遵循基于任务结构、架构和优化器选择的幂律规律，且重复可以极大地加速出现。

**结论:** 提供了简单且以理论为基础的框架，用于理解数据分布和模型设计如何影响一种形式的涌现的学习动态。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+emergence+of+sparse+attention%3A+impact+of+data+distribution+and+benefits+of+repetition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17863，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17863&send_immediately=true&force_search=false)

**原文摘要:** Emergence is a fascinating property of large language models and neural
networks more broadly: as models scale and train for longer, they sometimes
develop new abilities in sudden ways. Despite initial studies, we still lack a
comprehensive understanding of how and when these abilities emerge. To address
this gap, we study the emergence over training of sparse attention, a critical
and frequently observed attention pattern in Transformers. By combining
theoretical analysis of a toy model with empirical observations on small
Transformers trained on a linear regression variant, we uncover the mechanics
driving sparse attention emergence and reveal that emergence timing follows
power laws based on task structure, architecture, and optimizer choice. We
additionally find that repetition can greatly speed up emergence. Finally, we
confirm these results on a well-studied in-context associative recall task. Our
findings provide a simple, theoretically grounded framework for understanding
how data distributions and model design influence the learning dynamics behind
one form of emergence.

</details>


### [142] [DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization](https://arxiv.org/abs/2505.17866)
*Hongshu Guo, Zeyuan Ma, Yining Ma, Xinglin Zhang, Wei-Neng Chen, Yue-Jiao Gong*

**主要类别:** cs.LG

**概要:** DesignX是一种自动算法设计框架，能在几秒内生成针对特定黑箱优化问题的有效优化器。它通过双代理强化学习系统实现结构和超参数的自动化设计，在多种场景下超越了人工设计的优化器。


<details>
  <summary>更多</summary>
  
**动机:** 设计有效的黑箱优化器受到问题特定知识有限和手动控制耗时长的限制。

**方法:** 基于第一性原理，将任务分为算法结构生成和超参数控制两个子任务。构建了一个包含数百个算法组件的模块化算法空间，并引入双代理强化学习系统进行合作训练。

**结果:** DesignX生成的优化器在合成测试平台和实际优化场景（如蛋白质对接、AutoML和无人机路径规划）中显著超越了人工设计的优化器。

**结论:** DesignX能够发现超出专家直觉的非平凡算法模式，为优化社区提供了宝贵的设计见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DesignX%3A+Human-Competitive+Algorithm+Designer+for+Black-Box+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17866&send_immediately=true&force_search=false)

**原文摘要:** Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.

</details>


### [143] [SpectraLDS: Provable Distillation for Linear Dynamical Systems](https://arxiv.org/abs/2505.17868)
*Devan Shah, Shlomo Fortgang, Sofiia Druchyna, Elad Hazan*

**主要类别:** cs.LG

**概要:** 提出了一种新的方法SpectraLDS，用于识别对称线性动力系统，该方法具有独立于系统状态维度或有效记忆的精度保证。通过将对称LDS表示为可通过固定频谱变换学习的卷积，并展示如何反演此表示以从其频谱变换中恢复LDS模型，从而产生端到端的凸优化过程。此方法在保持预测准确性的同时，实现了与序列长度无关的恒定时间和恒定空间推理。实验表明，在诸如语言建模等任务中，使用SpectraLDS可以保持准确性并提高推理效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的对称线性动力系统（LDS）识别方法可能依赖于系统的状态维度或有效记忆，这可能导致精度受限或计算复杂度高。因此，需要一种能够提供独立于这些因素的精度保证的新方法。

**方法:** 研究者首先将对称LDS表示为可通过固定频谱变换学习的卷积，然后展示了如何反演此表示以从其频谱变换中恢复LDS模型。这一过程使得整个流程成为端到端的凸优化程序，从而允许在保持预测准确性的同时实现高效的推理。

**结果:** SpectraLDS 方法在作为序列预测架构的一个组件时，能够在诸如语言建模的任务上保持准确性，同时提高推理效率。

**结论:** 本文提出的 SpectraLDS 方法为识别对称线性动力系统提供了新的思路，它不仅具备独立于系统状态维度或有效记忆的精度保证，还显著提高了推理效率，适用于各种序列预测任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpectraLDS%3A+Provable+Distillation+for+Linear+Dynamical+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17868，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17868&send_immediately=true&force_search=false)

**原文摘要:** We present the first provable method for identifying symmetric linear
dynamical systems (LDS) with accuracy guarantees that are independent of the
systems' state dimension or effective memory. Our approach builds upon recent
work that represents symmetric LDSs as convolutions learnable via fixed
spectral transformations. We show how to invert this representation, thereby
recovering an LDS model from its spectral transform and yielding an end-to-end
convex optimization procedure. This distillation preserves predictive accuracy
while enabling constant-time and constant-space inference per token,
independent of sequence length. We evaluate our method, SpectraLDS, as a
component in sequence prediction architectures and demonstrate that accuracy is
preserved while inference efficiency is improved on tasks such as language
modeling.

</details>


### [144] [Best Group Identification in Multi-Objective Bandits](https://arxiv.org/abs/2505.17869)
*Mohammad Shahverdikondori, Mohammad Reza Badri, Negar Kiyavash*

**主要类别:** cs.LG

**概要:** 本文研究了多目标多臂老虎机设置中的最佳组识别问题，提出消除算法并证明其理论和实证性能。


<details>
  <summary>更多</summary>
  
**动机:** 在多目标决策场景中，识别具有向量值奖励的最优组是一项重要任务。

**方法:** 作者提出了两种关键公式：组Pareto集识别和线性最佳组识别，并为每种设置设计了基于消除的算法。

**结果:** 通过数值实验展示了所提算法的强大经验性能，并建立了样本复杂度的上下界。

**结论:** 所提出的算法能够有效地识别最优组，且具有较低的样本复杂度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Best+Group+Identification+in+Multi-Objective+Bandits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17869，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17869&send_immediately=true&force_search=false)

**原文摘要:** We introduce the Best Group Identification problem in a multi-objective
multi-armed bandit setting, where an agent interacts with groups of arms with
vector-valued rewards. The performance of a group is determined by an
efficiency vector which represents the group's best attainable rewards across
different dimensions. The objective is to identify the set of optimal groups in
the fixed-confidence setting. We investigate two key formulations: group Pareto
set identification, where efficiency vectors of optimal groups are Pareto
optimal and linear best group identification, where each reward dimension has a
known weight and the optimal group maximizes the weighted sum of its efficiency
vector's entries. For both settings, we propose elimination-based algorithms,
establish upper bounds on their sample complexity, and derive lower bounds that
apply to any correct algorithm. Through numerical experiments, we demonstrate
the strong empirical performance of the proposed algorithms.

</details>


### [145] [BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models](https://arxiv.org/abs/2505.17871)
*Zezhi Shao, Yujie Li, Fei Wang, Chengqing Yu, Yisong Fu, Tangwen Qian, Bin Xu, Boyu Diao, Yongjun Xu, Xueqi Cheng*

**主要类别:** cs.LG

**概要:** BLAST是一种新的预训练语料库，通过平衡采样策略增强数据多样性，从而提高时间序列预测模型的性能和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大规模时间序列数据集通常存在固有偏差和不平衡分布的问题，导致模型性能和泛化能力不佳。为了填补这一空白，研究者们引入了BLAST。

**方法:** BLAST包含3210亿个来自公开数据集的观测值，并使用全面的统计指标来描述时间序列模式。然后，通过基于网格的分区对数据进行隐式聚类，以促进模式导向的采样。此外，通过结合网格采样和网格混叠技术，确保对各种模式的平衡和代表性覆盖。

**结果:** 实验结果表明，在BLAST上预训练的模型仅需现有方法一小部分的计算资源和训练令牌即可达到最先进的性能。

**结论:** 研究发现强调了数据多样性在提高通用预测任务的训练效率和模型性能方面的关键作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BLAST%3A+Balanced+Sampling+Time+Series+Corpus+for+Universal+Forecasting+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17871&send_immediately=true&force_search=false)

**原文摘要:** The advent of universal time series forecasting models has revolutionized
zero-shot forecasting across diverse domains, yet the critical role of data
diversity in training these models remains underexplored. Existing large-scale
time series datasets often suffer from inherent biases and imbalanced
distributions, leading to suboptimal model performance and generalization. To
address this gap, we introduce BLAST, a novel pre-training corpus designed to
enhance data diversity through a balanced sampling strategy. First, BLAST
incorporates 321 billion observations from publicly available datasets and
employs a comprehensive suite of statistical metrics to characterize time
series patterns. Then, to facilitate pattern-oriented sampling, the data is
implicitly clustered using grid-based partitioning. Furthermore, by integrating
grid sampling and grid mixup techniques, BLAST ensures a balanced and
representative coverage of diverse patterns. Experimental results demonstrate
that models pre-trained on BLAST achieve state-of-the-art performance with a
fraction of the computational resources and training tokens required by
existing methods. Our findings highlight the pivotal role of data diversity in
improving both training efficiency and model performance for the universal
forecasting task.

</details>


### [146] [Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning](https://arxiv.org/abs/2505.17875)
*Yan Zhong, Xingyu Wu, Xinping Zhao, Li Zhang, Xinyuan Song, Lei Shi, Bingbing Jiang*

**主要类别:** cs.LG

**概要:** In practical domains, high-dimensional data are usually associated with diverse semantic labels. This paper proposes a consistent sparse graph learning method for multi-label semi-supervised feature selection (SGMFS) to enhance feature selection performance.


<details>
  <summary>更多</summary>
  
**动机:** Traditional feature selection methods are designed for single-label data and existing multi-label methods encounter challenges in semi-supervised scenarios, such as failing to evaluate label correlations without enough labeled samples and suboptimal similarity graph structure for multi-label problems.

**方法:** The proposed SGMFS method learns a low-dimensional and independent label subspace from the projected features to achieve label correlations and performs sparse reconstruction of samples in both the label space and the learned subspace simultaneously to adaptively learn the similarity graph.

**结果:** Experiments validate the superiority of SGMFS.

**结论:** SGMFS can enhance the feature selection performance by maintaining space consistency and learning label correlations in semi-supervised scenarios.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semi-Supervised+Multi-Label+Feature+Selection+with+Consistent+Sparse+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17875，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17875&send_immediately=true&force_search=false)

**原文摘要:** In practical domains, high-dimensional data are usually associated with
diverse semantic labels, whereas traditional feature selection methods are
designed for single-label data. Moreover, existing multi-label methods
encounter two main challenges in semi-supervised scenarios: (1). Most
semi-supervised methods fail to evaluate the label correlations without enough
labeled samples, which are the critical information of multi-label feature
selection, making label-specific features discarded. (2). The similarity graph
structure directly derived from the original feature space is suboptimal for
multi-label problems in existing graph-based methods, leading to unreliable
soft labels and degraded feature selection performance. To overcome them, we
propose a consistent sparse graph learning method for multi-label
semi-supervised feature selection (SGMFS), which can enhance the feature
selection performance by maintaining space consistency and learning label
correlations in semi-supervised scenarios. Specifically, for Challenge (1),
SGMFS learns a low-dimensional and independent label subspace from the
projected features, which can compatibly cross multiple labels and effectively
achieve the label correlations. For Challenge (2), instead of constructing a
fixed similarity graph for semi-supervised learning, SGMFS thoroughly explores
the intrinsic structure of the data by performing sparse reconstruction of
samples in both the label space and the learned subspace simultaneously. In
this way, the similarity graph can be adaptively learned to maintain the
consistency between label space and the learned subspace, which can promote
propagating proper soft labels for unlabeled samples, facilitating the ultimate
feature selection. An effective solution with fast convergence is designed to
optimize the objective function. Extensive experiments validate the superiority
of SGMFS.

</details>


### [147] [Universal Domain Adaptation Benchmark for Time Series Data Representation](https://arxiv.org/abs/2505.17899)
*Romain Mussard, Fannia Pacheco, Maxime Berar, Gilles Gasso, Paul Honeine*

**主要类别:** cs.LG

**概要:** 这篇论文探讨了在时间序列数据中应用无监督领域适应（UniDA）的方法，提出了一种评估框架以提高模型的泛化能力和鲁棒性，并强调了骨干网络选择对性能的关键影响。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习在时间序列数据中的新奇点检测方面取得了显著进展，但由于时间序列数据的固有变异性，模型在泛化和鲁棒性方面仍面临挑战。为了解决这个问题，需要探索无监督领域适应（UniDA）方法在时间序列数据中的应用。

**方法:** 本研究提供了一个全面的实现和比较最先进的时间序列骨干网络的UniDA框架，并提出了一个可靠的协议来评估这些模型在不同领域的鲁棒性和泛化能力。

**结果:** 实验结果表明，骨干网络的选择对UniDA性能有重要影响，并且该框架能够进行跨数据集和架构的鲁棒性分析。

**结论:** 本文为实践者提供了一个易于扩展的框架，可以结合未来的UniDA和时间序列架构改进，同时强调了骨干网络选择的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Domain+Adaptation+Benchmark+for+Time+Series+Data+Representation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17899&send_immediately=true&force_search=false)

**原文摘要:** Deep learning models have significantly improved the ability to detect
novelties in time series (TS) data. This success is attributed to their strong
representation capabilities. However, due to the inherent variability in TS
data, these models often struggle with generalization and robustness. To
address this, a common approach is to perform Unsupervised Domain Adaptation,
particularly Universal Domain Adaptation (UniDA), to handle domain shifts and
emerging novel classes. While extensively studied in computer vision, UniDA
remains underexplored for TS data. This work provides a comprehensive
implementation and comparison of state-of-the-art TS backbones in a UniDA
framework. We propose a reliable protocol to evaluate their robustness and
generalization across different domains. The goal is to provide practitioners
with a framework that can be easily extended to incorporate future advancements
in UniDA and TS architectures. Our results highlight the critical influence of
backbone selection in UniDA performance and enable a robustness analysis across
various datasets and architectures.

</details>


### [148] [Evolving Machine Learning: A Survey](https://arxiv.org/abs/2505.17902)
*Ignacio Cabrera Martin, Subhaditya Mukherjee, Almas Baimagambetov, Joaquin Vanschoren, Nikolaos Polatidis*

**主要类别:** cs.LG

**概要:** 在快速数据演变的时代，传统机器学习模型难以适应动态环境。演化机器学习（EML）作为关键范式，通过解决数据漂移、概念漂移等五个核心挑战，实现实时数据流中的连续学习与适应。本文综述了120多项研究，分析了监督、非监督和半监督方法，并探讨了适应性神经架构、元学习和集成策略的作用，为未来研究指明方向。


<details>
  <summary>更多</summary>
  
**动机:** 传统机器学习模型在面对动态环境时表现不足，特别是在快速数据演变的时代。为了克服这一限制，需要一种能够实现实时数据流中连续学习和适应的新范式。

**方法:** 对超过120项研究进行系统回顾，涵盖监督、非监督和半监督方法，分析其在解决数据漂移、概念漂移、灾难性遗忘、偏斜学习和网络适应等五个核心挑战方面的有效性。同时，探讨适应性神经架构、元学习和集成策略的作用。

**结果:** 识别出当前EML领域的研究空白和未来机会，强调了开发稳健、合乎伦理且可扩展的EML系统的重要性。提供了关于现有技术有效性和局限性的比较视角。

**结论:** 本文不仅描绘了EML的当前研究图景，还明确了未来研究的方向，旨在指导研究人员和实践者开发适用于实际部署的EML系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolving+Machine+Learning%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17902，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17902&send_immediately=true&force_search=false)

**原文摘要:** In an era defined by rapid data evolution, traditional machine learning (ML)
models often fall short in adapting to dynamic environments. Evolving Machine
Learning (EML) has emerged as a critical paradigm, enabling continuous learning
and adaptation in real-time data streams. This survey presents a comprehensive
analysis of EML, focusing on five core challenges: data drift, concept drift,
catastrophic forgetting, skewed learning, and network adaptation. We
systematically review over 120 studies, categorizing state-of-the-art methods
across supervised, unsupervised, and semi-supervised approaches. The survey
explores diverse evaluation metrics, benchmark datasets, and real-world
applications, offering a comparative lens on the effectiveness and limitations
of current techniques. Additionally, we highlight the growing role of adaptive
neural architectures, meta-learning, and ensemble strategies in addressing
evolving data complexities. By synthesizing insights from recent literature,
this work not only maps the current landscape of EML but also identifies
critical gaps and opportunities for future research. Our findings aim to guide
researchers and practitioners in developing robust, ethical, and scalable EML
systems for real-world deployment.

</details>


### [149] [LLM Meeting Decision Trees on Tabular Data](https://arxiv.org/abs/2505.17918)
*Hangting Ye, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang*

**主要类别:** cs.LG

**概要:** 本研究提出了一种名为DeLTa的新方法，通过逻辑决策树规则将大型语言模型（LLM）整合到表格数据中，无需表格数据序列化或微调LLM。该方法利用LLM的推理能力改进决策树规则，并提供一种校准方法以减少预测错误。实验表明，该方法在各种表格基准测试中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLM）在许多领域取得了成功，但在表格数据领域的应用仍面临两个关键问题：1) 数据视角：现有的数据序列化方法缺乏对结构化表格数据的普适性，并可能通过直接文本暴露带来隐私风险；2) 模型视角：LLM微调方法在处理表格数据时效果不佳，且上下文学习的可扩展性受到输入长度限制的影响。为解决这些问题，本文探索了通过逻辑决策树规则将LLM整合到表格数据中的新方向。

**方法:** 提出了一种名为DeLTa的方法，该方法避免了表格数据序列化，并可以在无需微调LLM的情况下应用于完整数据学习环境。具体来说，利用LLM的推理能力重新设计给定的一组决策树规则，并提供了一种通过LLM生成的新规则对原始决策树进行校准的方法，从而逼近误差校正向量，引导原始决策树预测向“减少误差”的方向发展。

**结果:** 广泛的实验表明，该方法在各种表格基准测试中实现了最先进的性能。

**结论:** DeLTa方法通过逻辑决策树规则有效地将LLM整合到表格数据中，解决了现有方法在数据序列化和模型微调方面的不足，展示了在表格数据预测任务中的优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+Meeting+Decision+Trees+on+Tabular+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17918，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17918&send_immediately=true&force_search=false)

**原文摘要:** Tabular data have been playing a vital role in diverse real-world fields,
including healthcare, finance, etc. With the recent success of Large Language
Models (LLMs), early explorations of extending LLMs to the domain of tabular
data have been developed. Most of these LLM-based methods typically first
serialize tabular data into natural language descriptions, and then tune LLMs
or directly infer on these serialized data. However, these methods suffer from
two key inherent issues: (i) data perspective: existing data serialization
methods lack universal applicability for structured tabular data, and may pose
privacy risks through direct textual exposure, and (ii) model perspective: LLM
fine-tuning methods struggle with tabular data, and in-context learning
scalability is bottle-necked by input length constraints (suitable for few-shot
learning). This work explores a novel direction of integrating LLMs into
tabular data throughough logical decision tree rules as intermediaries,
proposes a decision tree enhancer with LLM-derived rule for tabular prediction,
DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied
to full data learning setting without LLM fine-tuning. Specifically, we
leverage the reasoning ability of LLMs to redesign an improved rule given a set
of decision tree rules. Furthermore, we provide a calibration method for
original decision trees via new generated rule by LLM, which approximates the
error correction vector to steer the original decision tree predictions in the
direction of ``errors'' reducing. Finally, extensive experiments on diverse
tabular benchmarks show that our method achieves state-of-the-art performance.

</details>


### [150] [KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches](https://arxiv.org/abs/2505.17919)
*Mingquan Feng, Yifan Fu, Tongcheng Zhang, Yu Jiang, Yixin Huang, Junchi Yan*

**主要类别:** cs.LG

**概要:** 这篇论文提出了一种新的神经网络结构KITINet，通过非平衡粒子动力学和偏微分方程模拟重新解释特征传播，并在多个任务上显示出优于传统网络基线的性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管残差连接在现代神经网络中取得了广泛认可的成功，但其设计原则在很大程度上仍然是启发式的。因此，需要一种基于理论的新方法来解释和改进特征传播。

**方法:** 提出了KITINet，它将特征更新建模为粒子系统的随机演化，通过BTE的离散求解器进行数值模拟。这种公式模仿粒子碰撞和能量交换，通过物理信息相互作用实现自适应特征细化。

**结果:** 实验表明，该机制在训练过程中会引发网络参数浓缩，参数逐渐集中在占主导地位的稀疏通道子集中，并在科学计算、图像分类和文本分类等任务上显示出一致的改进。

**结论:** KITINet提供了一种基于非平衡粒子动力学和PDE模拟的新视角来设计神经网络结构，具有在多种任务上超越经典网络基线的潜力，同时计算成本增加可忽略不计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KITINet%3A+Kinetics+Theory+Inspired+Network+Architectures+with+PDE+Simulation+Approaches，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17919，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17919&send_immediately=true&force_search=false)

**原文摘要:** Despite the widely recognized success of residual connections in modern
neural networks, their design principles remain largely heuristic. This paper
introduces KITINet (Kinetics Theory Inspired Network), a novel architecture
that reinterprets feature propagation through the lens of non-equilibrium
particle dynamics and partial differential equation (PDE) simulation. At its
core, we propose a residual module that models feature updates as the
stochastic evolution of a particle system, numerically simulated via a
discretized solver for the Boltzmann transport equation (BTE). This formulation
mimics particle collisions and energy exchange, enabling adaptive feature
refinement via physics-informed interactions. Additionally, we reveal that this
mechanism induces network parameter condensation during training, where
parameters progressively concentrate into a sparse subset of dominant channels.
Experiments on scientific computation (PDE operator), image classification
(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent
improvements over classic network baselines, with negligible increase of FLOPs.

</details>


### [151] [Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV](https://arxiv.org/abs/2505.17929)
*Alexander Gabitashvili, Philipp Kellmeyer*

**主要类别:** cs.LG

**概要:** 研究探讨了多种机器学习方法预测神经疾病患者ICU住院时长（LOS）的能力，基于MIMIC-IV数据集，并将LOS分为三类。研究发现随机森林在静态数据上表现最佳，而BERT在时间序列数据上优于LSTM模型。


<details>
  <summary>更多</summary>
  
**动机:** 重症监护病房（ICU）是医院处理危及生命的病例的重要部门，尤其是在全球COVID-19大流行期间，ICU管理变得尤为重要。本研究旨在探索机器学习技术在预测神经疾病患者的ICU住院时长（LOS）方面的潜力，以改善ICU资源管理和患者护理。这是首个针对神经疾病患者LOS预测的机器学习研究。

**方法:** 研究使用了多种机器学习方法，包括经典机器学习算法（K-Nearest Neighbors、Random Forest、XGBoost和CatBoost）和神经网络（LSTM、BERT和Temporal Fusion Transformer）。通过将LOS分类为三个组别：少于两天、少于一周和一周或更长，将LOS预测任务转化为分类问题。研究基于MIMIC-IV数据集进行实验。

**结果:** 在静态数据方面，随机森林模型表现最佳，准确率为0.68，精确率为0.68，召回率为0.68，F1得分为0.67。在时间序列数据方面，BERT模型优于LSTM模型，准确率为0.80，精确率为0.80，召回率为0.80，F1得分为0.80。

**结论:** 本研究表明，机器学习技术可以有效应用于神经疾病患者ICU住院时长的预测，有助于改善ICU资源管理和患者护理。尽管该研究并非旨在超越现有方法，但它为特定情境下各种方法的有效性提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Length+of+Stay+in+Neurological+ICU+Patients+Using+Classical+Machine+Learning+and+Neural+Network+Models%3A+A+Benchmark+Study+on+MIMIC-IV，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17929，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17929&send_immediately=true&force_search=false)

**原文摘要:** Intensive care unit (ICU) is a crucial hospital department that handles
life-threatening cases. Nowadays machine learning (ML) is being leveraged in
healthcare ubiquitously. In recent years, management of ICU became one of the
most significant parts of the hospital functionality (largely but not only due
to the worldwide COVID-19 pandemic). This study explores multiple ML approaches
for predicting LOS in ICU specifically for the patients with neurological
diseases based on the MIMIC-IV dataset. The evaluated models include classic ML
algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and
Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS
prediction is often framed as a classification task, this study categorizes LOS
into three groups: less than two days, less than a week, and a week or more. As
the first ML-based approach targeting LOS prediction for neurological disorder
patients, this study does not aim to outperform existing methods but rather to
assess their effectiveness in this specific context. The findings provide
insights into the applicability of ML techniques for improving ICU resource
management and patient care. According to the results, Random Forest model
proved to outperform others on static, achieving an accuracy of 0.68, a
precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model
outperformed LSTM model on time-series data with an accuracy of 0.80, a
precision of 0.80, a recall of 0.80 and F1-score 0.80.

</details>


### [152] [Understanding Gated Neurons in Transformers from Their Input-Output Functionality](https://arxiv.org/abs/2505.17936)
*Sebastian Gerstner, Hinrich Schütze*

**主要类别:** cs.LG

**概要:** 研究人员通过分析神经元的输入和输出权重之间的余弦相似性，发现早期中层以富集神经元为主，而后期层则更倾向于耗尽神经元。富集神经元主要负责丰富概念表示，这是事实回忆的第一步。这种方法是对基于激活的分析方法的补充。


<details>
  <summary>更多</summary>
  
**动机:** 现有的研究主要关注MLP神经元在语言模型中的激活上下文和输出权重向量，而较少关注输入与输出之间的交互作用。

**方法:** 通过检查神经元输入和输出权重之间的余弦相似性，来研究神经元如何检测输入方向并影响残差流（如富集或耗尽特定方向）。该方法应用于12个模型进行验证。

**结果:** 早期中层的神经元多为富集神经元，后期层的神经元更多表现为耗尽神经元。富集神经元有助于丰富概念表示，是事实回忆的重要步骤。

**结论:** 从输入-输出视角出发的研究是对基于激活的分析方法以及分别处理输入和输出方法的有效补充。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Gated+Neurons+in+Transformers+from+Their+Input-Output+Functionality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17936，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17936&send_immediately=true&force_search=false)

**原文摘要:** Interpretability researchers have attempted to understand MLP neurons of
language models based on both the contexts in which they activate and their
output weight vectors. They have paid little attention to a complementary
aspect: the interactions between input and output. For example, when neurons
detect a direction in the input, they might add much the same direction to the
residual stream ("enrichment neurons") or reduce its presence ("depletion
neurons"). We address this aspect by examining the cosine similarity between
input and output weights of a neuron. We apply our method to 12 models and find
that enrichment neurons dominate in early-middle layers whereas later layers
tend more towards depletion. To explain this finding, we argue that enrichment
neurons are largely responsible for enriching concept representations, one of
the first steps of factual recall. Our input-output perspective is a complement
to activation-dependent analyses and to approaches that treat input and output
separately.

</details>


### [153] [Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding](https://arxiv.org/abs/2505.17939)
*Manuel Lecha, Andrea Cavallo, Francesca Dominici, Ran Levi, Alessio Del Bue, Elvin Isufi, Pietro Morerio, Claudio Battiloro*

**主要类别:** cs.LG

**概要:** 论文提出了一种新的模型SSNs，能有效捕捉复杂系统中的高阶有向模式，并在脑动力学分类任务中表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的图神经网络（GNNs）和拓扑深度学习（TDL）模型无法充分捕捉多体及分层关系，特别是在许多复杂系统中存在的高阶有向交互模式。

**方法:** 提出了Semi-Simplicial Neural Networks（SSNs），该模型基于半单形集运作，能够编码有向高阶模体及其方向关系；还提出了Routing-SSNs以增强可扩展性，通过动态选择最具信息量的关系。

**结果:** SSNs在脑动力学分类任务上达到最先进水平，比次优模型高出27%，比消息传递GNN高出50%的准确率。同时，在标准节点分类和边回归任务中也表现出竞争力。

**结论:** SSNs展示了其在从结构化脑数据中学习的强大潜力，为TDL提供了一个独特的实际案例研究。代码和数据将公开发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Directed+Semi-Simplicial+Learning+with+Applications+to+Brain+Activity+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17939，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17939&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) excel at learning from pairwise interactions but
often overlook multi-way and hierarchical relationships. Topological Deep
Learning (TDL) addresses this limitation by leveraging combinatorial
topological spaces. However, existing TDL models are restricted to undirected
settings and fail to capture the higher-order directed patterns prevalent in
many complex systems, e.g., brain networks, where such interactions are both
abundant and functionally significant. To fill this gap, we introduce
Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that
operate on semi-simplicial sets -- combinatorial structures that encode
directed higher-order motifs and their directional relationships. To enhance
scalability, we propose Routing-SSNs, which dynamically select the most
informative relations in a learnable manner. We prove that SSNs are strictly
more expressive than standard graph and TDL models. We then introduce a new
principled framework for brain dynamics representation learning, grounded in
the ability of SSNs to provably recover topological descriptors shown to
successfully characterize brain activity. Empirically, SSNs achieve
state-of-the-art performance on brain dynamics classification tasks,
outperforming the second-best model by up to 27%, and message passing GNNs by
up to 50% in accuracy. Our results highlight the potential of principled
topological models for learning from structured brain data, establishing a
unique real-world case study for TDL. We also test SSNs on standard node
classification and edge regression tasks, showing competitive performance. We
will make the code and data publicly available.

</details>


### [154] [VeriThinker: Learning to Verify Makes Reasoning Model Efficient](https://arxiv.org/abs/2505.17941)
*Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, Xinchao Wang*

**主要类别:** cs.LG

**概要:** VeriThinker是一种用于压缩链式思维（CoT）推理的新方法，通过辅助验证任务微调大型推理模型（LRMs），使其在保持或提高准确性的同时显著减少推理链条长度。实验表明，该方法在多个数据集上有效减少推理令牌数，并提升准确性，同时具备零样本泛化到推测性推理的能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型推理模型（LRMs）在复杂任务中表现出色，但其过度思考的倾向导致了不必要的长推理链条，从而大幅增加推理成本。为了解决这一问题，需要一种能够有效压缩推理链条的方法。

**方法:** VeriThinker通过一个创新的辅助验证任务来微调LRMs，而不是直接使用合成的简洁CoT数据进行微调。模型被训练以准确验证CoT解决方案的正确性，从而使模型更敏锐地判断后续自我反思步骤的必要性，进而有效抑制过度思考。

**结果:** 实验结果表明，VeriThinker显著减少了推理链条长度，同时保持甚至略微提高了准确性。具体来说，在DeepSeek-R1-Distill-Qwen-7B模型上，MATH500数据集的推理令牌数从3790减少到2125，准确率提升了0.8%；AIME25数据集的令牌数从14321减少到10287，准确率提升了2.1%。此外，VeriThinker还能零样本泛化到推测性推理。

**结论:** VeriThinker提供了一种有效的方法来压缩LRMs的推理链条，降低推理成本，同时保持或提高准确性。这种方法还展示了在推测性推理中的潜在应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VeriThinker%3A+Learning+to+Verify+Makes+Reasoning+Model+Efficient，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17941，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17941&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought
(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily
lengthy reasoning chains, dramatically increasing inference costs. To mitigate
this issue, we introduce VeriThinker, a novel approach for CoT compression.
Unlike conventional methods that fine-tune LRMs directly on the original
reasoning task using synthetic concise CoT data, we innovatively fine-tune the
model solely through an auxiliary verification task. By training LRMs to
accurately verify the correctness of CoT solutions, the LRMs inherently become
more discerning about the necessity of subsequent self-reflection steps,
thereby effectively suppressing overthinking. Extensive experiments validate
that VeriThinker substantially reduces reasoning chain lengths while
maintaining or even slightly improving accuracy. When applied to
DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500
from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on
AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to
40.8%). Additionally, our experiments demonstrate that VeriThinker can also be
zero-shot generalized to speculative reasoning. Code is available at
https://github.com/czg1225/VeriThinker

</details>


### [155] [A Principled Bayesian Framework for Training Binary and Spiking Neural Networks](https://arxiv.org/abs/2505.17962)
*James A. Walker, Moein Khajehnejad, Adeel Razi*

**主要类别:** cs.LG

**概要:** 我们提出了一种贝叶斯框架来训练二值化和脉冲神经网络，该方法在不使用归一化层的情况下达到了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前用于训练二值化和脉冲神经网络的方法通常依赖于代理梯度法，这些方法往往是启发式的，并且对超参数的选择非常敏感。因此，需要一种更稳健、基于概率模型的方法来实现端到端的梯度优化。

**方法:** 作者提出了一个基于贝叶斯框架的二值化和脉冲神经网络训练方法，引入了重要性加权直通（IW-ST）估计器，统一了直通和松弛估计器，并分析了其偏差-方差权衡。通过辅助损失实现了偏差最小化的目标，并进一步提出了脉冲贝叶斯神经网络（SBNNs），利用后验噪声训练二值化和脉冲神经网络。这种方法减少了梯度偏差，正则化了参数，并引入了类似Dropout的噪声。

**结果:** 实验结果表明，在CIFAR-10、DVS Gesture和SHD数据集上的表现与现有方法相当或更好，同时不需要归一化层或手动调优梯度。此外，该方法能够训练深度残差网络而无需归一化。

**结论:** 所提出的贝叶斯框架为训练二值化和脉冲神经网络提供了一种新的途径，避免了归一化层的使用，并通过减少梯度偏差和引入正则化机制提高了模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Principled+Bayesian+Framework+for+Training+Binary+and+Spiking+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17962，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17962&send_immediately=true&force_search=false)

**原文摘要:** We propose a Bayesian framework for training binary and spiking neural
networks that achieves state-of-the-art performance without normalisation
layers. Unlike commonly used surrogate gradient methods -- often heuristic and
sensitive to hyperparameter choices -- our approach is grounded in a
probabilistic model of noisy binary networks, enabling fully end-to-end
gradient-based optimisation. We introduce importance-weighted straight-through
(IW-ST) estimators, a unified class generalising straight-through and
relaxation-based estimators. We characterise the bias-variance trade-off in
this family and derive a bias-minimising objective implemented via an auxiliary
loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),
a variational inference framework that uses posterior noise to train Binary and
Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient
bias, regularises parameters, and introduces dropout-like noise. By linking
low-bias conditions, vanishing gradients, and the KL term, we enable training
of deep residual networks without normalisation. Experiments on CIFAR-10, DVS
Gesture, and SHD show our method matches or exceeds existing approaches without
normalisation or hand-tuned gradients.

</details>


### [156] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
*Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng*

**主要类别:** cs.LG

**概要:** VAPO框架在提升大型语言模型的长链推理任务的强化学习效率和可靠性方面表现出显著的成功，本文从理论上探讨其潜在机制、限制以及未来改进方向。


<details>
  <summary>更多</summary>
  
**动机:** 尽管VAPO框架在实践中展现出明显优势，但对其底层机制的理论理解尚不充分，了解其假设的局限性有助于指导未来的改进和发展。

**方法:** 论文深入研究了VAPO的价值函数逼近在复杂推理空间中的表现、自适应优势估计的最优性、标记级别优化的影响，以及探索与泛化的持久挑战。

**结果:** 通过理论分析，揭示了VAPO框架在处理复杂推理任务时的优势与不足，并明确了进一步研究的方向以构建更强大和通用的推理代理。

**结论:** 需要对VAPO框架进行更深的理论研究，挑战其现有假设并推动更广泛适用的强化学习方法的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Analyzing+and+Understanding+the+Limitations+of+VAPO%3A+A+Theoretical+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17997&send_immediately=true&force_search=false)

**原文摘要:** The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.

</details>


### [157] [Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective](https://arxiv.org/abs/2505.18002)
*Di Jin, Jingyi Cao, Xiaobao Wang, Bingdao Feng, Dongxiao He, Longbiao Wang, Jianwu Dang*

**主要类别:** cs.LG

**概要:** Graph anomaly detection is crucial in web security and financial fraud. Current methods using contrastive learning face issues with interfering edges, leading to suboptimal performance. This paper proposes CVGAD, a framework including multi-scale anomaly awareness and progressive purification modules, which significantly improves detection performance.


<details>
  <summary>更多</summary>
  
**动机:** Existing graph anomaly detection methods relying on contrastive learning suffer from the limitation of interfering edges, which introduce disruptive noise and compromise the ability to learn meaningful representations of normal patterns.

**方法:** The paper introduces CVGAD, a Clean-View Enhanced Graph Anomaly Detection framework. It includes a multi-scale anomaly awareness module for identifying key sources of interference and a progressive purification module for incrementally refining the graph by removing interfering edges.

**结果:** Extensive experiments on five benchmark datasets demonstrate the effectiveness of CVGAD in improving graph anomaly detection performance.

**结论:** CVGAD addresses the limitations of current methods by effectively mitigating the impact of interfering edges, thus enhancing the ability to learn meaningful representations and improving anomaly detection performance.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Contrastive+Learning+in+Graph+Anomaly+Detection%3A+A+Clean-View+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18002，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18002&send_immediately=true&force_search=false)

**原文摘要:** Graph anomaly detection aims to identify unusual patterns in graph-based
data, with wide applications in fields such as web security and financial fraud
detection. Existing methods typically rely on contrastive learning, assuming
that a lower similarity between a node and its local subgraph indicates
abnormality. However, these approaches overlook a crucial limitation: the
presence of interfering edges invalidates this assumption, since it introduces
disruptive noise that compromises the contrastive learning process.
Consequently, this limitation impairs the ability to effectively learn
meaningful representations of normal patterns, leading to suboptimal detection
performance. To address this issue, we propose a Clean-View Enhanced Graph
Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly
awareness module to identify key sources of interference in the contrastive
learning process. Moreover, to mitigate bias from the one-step edge removal
process, we introduce a novel progressive purification module. This module
incrementally refines the graph by iteratively identifying and removing
interfering edges, thereby enhancing model performance. Extensive experiments
on five benchmark datasets validate the effectiveness of our approach.

</details>


### [158] [Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling](https://arxiv.org/abs/2505.18017)
*Matthieu Blanke, Yongquan Qu, Sara Shamekh, Pierre Gentine*

**主要类别:** cs.LG

**概要:** Deep generative models for physical systems need guarantees on output plausibility. This paper introduces Split Augmented Langevin (SAL) to enforce physical constraints in generative models, improving accuracy and applicability.


<details>
  <summary>更多</summary>
  
**动机:** Current deep generative models lack guarantees regarding the physical plausibility of their outputs when representing complex physical systems. This limitation hinders their application in scientific and engineering problems where physical constraints must be satisfied.

**方法:** The paper proposes Split Augmented Langevin (SAL), a primal-dual sampling algorithm based on the variational formulation of Langevin dynamics. SAL enforces physical constraints progressively through variable splitting and is applicable to diffusion models. It ensures convergence while generating outputs that satisfy laws such as energy and mass conservation.

**结果:** SAL improves forecast accuracy and preserves critical conserved quantities in diffusion-based data assimilation for complex physical systems. The method also shows potential for solving challenging feasibility problems in optimal control.

**结论:** Split Augmented Langevin provides a principled framework to enforce physical constraints in deep generative models, expanding their applicability to scientific and engineering domains.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Strictly+Constrained+Generative+Modeling+via+Split+Augmented+Langevin+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18017，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18017&send_immediately=true&force_search=false)

**原文摘要:** Deep generative models hold great promise for representing complex physical
systems, but their deployment is currently limited by the lack of guarantees on
the physical plausibility of the generated outputs. Ensuring that known
physical constraints are enforced is therefore critical when applying
generative models to scientific and engineering problems. We address this
limitation by developing a principled framework for sampling from a target
distribution while rigorously satisfying physical constraints. Leveraging the
variational formulation of Langevin dynamics, we propose Split Augmented
Langevin (SAL), a novel primal-dual sampling algorithm that enforces
constraints progressively through variable splitting, with convergence
guarantees. While the method is developed theoretically for Langevin dynamics,
we demonstrate its effective applicability to diffusion models. In particular,
we use constrained diffusion models to generate physical fields satisfying
energy and mass conservation laws. We apply our method to diffusion-based data
assimilation on a complex physical system, where enforcing physical constraints
substantially improves both forecast accuracy and the preservation of critical
conserved quantities. We also demonstrate the potential of SAL for challenging
feasibility problems in optimal control.

</details>


### [159] [Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time](https://arxiv.org/abs/2505.18023)
*Duc Anh Nguyen, Ernesto Araya, Adalbert Fono, Gitta Kutyniok*

**主要类别:** cs.LG

**概要:** 近年来，脉冲神经网络（SNNs）作为一种解决传统人工神经网络（ANNs）能量挑战的潜在方案取得了显著进展。然而，与不断增长的ANN文献相比，我们对SNNs的理论理解仍然相对有限。本文研究了基于漏电积分-激发（LIF）神经元的离散时间SNN模型（离散时间LIF-SNNs），这是一个广泛使用的但缺乏坚实理论基础的框架。我们证明了具有静态输入和输出的离散时间LIF-SNNs实现了定义在多面体区域上的分段常数函数，并且更重要的是，我们量化了近似连续函数所需的网络规模。此外，我们研究了延迟（时间步数）和深度（层数）对离散时间LIF-SNNs所引发的输入空间划分复杂性的影响。我们的分析突显了延迟的重要性，并将这些网络与采用分段线性激活函数的ANN进行了对比。最后，我们提供了数值实验以支持我们的理论发现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管SNNs在能量效率方面有潜力，但其理论基础尚未建立，特别是在离散时间LIF-SNNs中，这促使作者深入研究其功能和限制。

**方法:** 作者通过研究离散时间LIF-SNNs的功能特性，证明了其能够实现分段常数函数，并量化了近似连续函数所需的网络规模。此外，还分析了延迟和深度对输入空间划分复杂性的影响。

**结果:** 离散时间LIF-SNNs能够实现分段常数函数，并且其近似连续函数的能力依赖于网络规模、延迟和深度。这些结果通过数值实验得到了验证。

**结论:** 离散时间LIF-SNNs具有独特的计算能力，延迟在其复杂性中起着关键作用。这一研究为SNNs的理论发展奠定了基础，并为进一步优化SNNs的设计提供了指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+to+Spike%3F+Understanding+the+Representational+Power+of+Spiking+Neural+Networks+in+Discrete+Time，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18023，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18023&send_immediately=true&force_search=false)

**原文摘要:** Recent years have seen significant progress in developing spiking neural
networks (SNNs) as a potential solution to the energy challenges posed by
conventional artificial neural networks (ANNs). However, our theoretical
understanding of SNNs remains relatively limited compared to the ever-growing
body of literature on ANNs. In this paper, we study a discrete-time model of
SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as
discrete-time LIF-SNNs, a widely used framework that still lacks solid
theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static
inputs and outputs realize piecewise constant functions defined on polyhedral
regions, and more importantly, we quantify the network size required to
approximate continuous functions. Moreover, we investigate the impact of
latency (number of time steps) and depth (number of layers) on the complexity
of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis
highlights the importance of latency and contrasts these networks with ANNs
employing piecewise linear activation functions. Finally, we present numerical
experiments to support our theoretical findings.

</details>


### [160] [Mahalanobis++: Improving OOD Detection via Feature Normalization](https://arxiv.org/abs/2505.18032)
*Maximilian Mueller, Matthias Hein*

**主要类别:** cs.LG

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mahalanobis%2B%2B%3A+Improving+OOD+Detection+via+Feature+Normalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18032&send_immediately=true&force_search=false)

**原文摘要:** Detecting out-of-distribution (OOD) examples is an important task for
deploying reliable machine learning models in safety-critial applications.
While post-hoc methods based on the Mahalanobis distance applied to pre-logit
features are among the most effective for ImageNet-scale OOD detection, their
performance varies significantly across models. We connect this inconsistency
to strong variations in feature norms, indicating severe violations of the
Gaussian assumption underlying the Mahalanobis distance estimation. We show
that simple $\ell_2$-normalization of the features mitigates this problem
effectively, aligning better with the premise of normally distributed data with
shared covariance matrix. Extensive experiments on 44 models across diverse
architectures and pretraining schemes show that $\ell_2$-normalization improves
the conventional Mahalanobis distance-based approaches significantly and
consistently, and outperforms other recently proposed OOD detection methods.

</details>


### [161] [Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach](https://arxiv.org/abs/2505.18043)
*Changyeol Lee, Yongho Shin, Hyung-Chan An*

**主要类别:** cs.LG

**概要:** 论文提出了一种结合线性规划和组合算法优势的框架，用于解决三种边着色聚类（ECC）问题：局部ECC、全局ECC和鲁棒ECC。该框架在保证解的质量的同时提高了计算效率，并通过实验和理论分析验证了有效性。此外，论文还提供了复杂性理论的不可近似结果和整数间隙界限，回答了领域内的两个开放问题。


<details>
  <summary>更多</summary>
  
**动机:** 传统的边着色聚类（ECC）方法存在非重叠和穷尽聚类的局限性，无法灵活处理数据。为了解决这些问题，研究者提出了三种ECC变体：允许重叠聚类的局部ECC和全局ECC，以及考虑顶点异常值的鲁棒ECC。然而，现有的线性规划（LP）算法虽然提供高质量解但计算成本高，而贪婪组合算法虽然快速但牺牲了解的质量。因此，需要一种新的算法框架来平衡质量和效率。

**方法:** 论文提出了一种结合线性规划（LP）和组合算法的框架，以解决局部ECC、全局ECC和鲁棒ECC问题。该框架利用LP的优点来保证解的质量，同时继承组合算法的高效性。通过这种方式，新算法能够在较短时间内生成高质量的解决方案。

**结果:** 实验和理论分析表明，提出的算法框架能够有效产生高质量的解，适用于所有三种ECC问题（局部、全局和鲁棒）。此外，论文提供了复杂性理论的不可近似结果和整数间隙界限，表明进一步的理论改进空间有限。这些结果还解决了领域内先前提出的两个开放问题。

**结论:** 论文提出的新算法框架成功地结合了LP和组合算法的优势，解决了传统ECC方法的局限性，并为三种ECC问题提供了高效的高质量解决方案。同时，复杂性理论的结果表明，当前的算法性能接近理论最优，进一步的改进可能非常困难。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improved+Algorithms+for+Overlapping+and+Robust+Clustering+of+Edge-Colored+Hypergraphs%3A+An+LP-Based+Combinatorial+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18043&send_immediately=true&force_search=false)

**原文摘要:** Clustering is a fundamental task in both machine learning and data mining.
Among various methods, edge-colored clustering (ECC) has emerged as a useful
approach for handling categorical data. Given a hypergraph with (hyper)edges
labeled by colors, ECC aims to assign vertex colors to minimize the number of
edges where the vertex color differs from the edge's color. However,
traditional ECC has inherent limitations, as it enforces a nonoverlapping and
exhaustive clustering. To tackle these limitations, three versions of ECC have
been studied: Local ECC and Global ECC, which allow overlapping clusters, and
Robust ECC, which accounts for vertex outliers. For these problems, both linear
programming (LP) rounding algorithms and greedy combinatorial algorithms have
been proposed. While these LP-rounding algorithms provide high-quality
solutions, they demand substantial computation time; the greedy algorithms, on
the other hand, run very fast but often compromise solution quality. In this
paper, we present an algorithmic framework that combines the strengths of LP
with the computational efficiency of combinatorial algorithms. Both
experimental and theoretical analyses show that our algorithms efficiently
produce high-quality solutions for all three problems: Local, Global, and
Robust ECC. We complement our algorithmic contributions with
complexity-theoretic inapproximability results and integrality gap bounds,
which suggest that significant theoretical improvements are unlikely. Our
results also answer two open questions previously raised in the literature.

</details>


### [162] [Reward Model Generalization for Compute-Aware Test-Time Reasoning](https://arxiv.org/abs/2505.18065)
*Zeen Song, Wenwen Qiang, Siyu Zhao, Changwen Zheng, Gang Hua*

**主要类别:** cs.LG

**概要:** 本论文提出了一种名为Compute-Aware Tree Search (CATS)的新方法，通过动态控制搜索行为，在固定的推理预算下提高答案准确性。实验表明，CATS在MATH和AIME基准测试中优于其他外部TTS方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前外部测试时间推理方法的一个核心挑战是测试时计算最优性（TCO），即如何在固定推理预算下最大化答案准确性。为此，需要分析奖励模型（PRM）的泛化误差如何影响计算效率和推理性能。

**方法:** 作者建立了一个理论框架，利用PAC-Bayes理论推导泛化边界，表明较低的PRM泛化误差可以减少找到正确答案所需的样本数量。基于此分析，提出了一个称为Compute-Aware Tree Search (CATS)的actor-critic框架，其中actor根据奖励分布和稀疏性统计输出采样超参数，而critic估计其效用以指导预算分配。

**结果:** 在MATH和AIME基准上的实验结果表明，CATS方法在各种LLMs和PRMs上始终优于其他外部TTS方法，验证了理论预测。

**结论:** 本文通过理论分析和实验验证展示了CATS方法的有效性，为提高大型语言模型在固定推理预算下的答案准确性提供了一种新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reward+Model+Generalization+for+Compute-Aware+Test-Time+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18065，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18065&send_immediately=true&force_search=false)

**原文摘要:** External test-time reasoning enhances large language models (LLMs) by
decoupling generation and selection. At inference time, the model generates
multiple reasoning paths, and an auxiliary process reward model (PRM) is used
to score and select the best one. A central challenge in this setting is
test-time compute optimality (TCO), i.e., how to maximize answer accuracy under
a fixed inference budget. In this work, we establish a theoretical framework to
analyze how the generalization error of the PRM affects compute efficiency and
reasoning performance. Leveraging PAC-Bayes theory, we derive generalization
bounds and show that a lower generalization error of PRM leads to fewer samples
required to find correct answers. Motivated by this analysis, we propose
Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically
controls search behavior. The actor outputs sampling hyperparameters based on
reward distributions and sparsity statistics, while the critic estimates their
utility to guide budget allocation. Experiments on the MATH and AIME benchmarks
with various LLMs and PRMs demonstrate that CATS consistently outperforms other
external TTS methods, validating our theoretical predictions.

</details>


### [163] [Emergence of Hebbian Dynamics in Regularized Non-Local Learners](https://arxiv.org/abs/2505.18069)
*David Koplow, Tomaso Poggio, Liu Ziyin*

**主要类别:** cs.LG

**概要:** 本研究建立了使用带权重衰减的随机梯度下降（SGD）训练的神经网络与接近收敛时使用Hebbian学习训练的神经网络之间的理论和经验联系。研究表明，带有正则化的SGD可以表现出遵循Hebbian规则学习的特性，而带有注入噪声的SGD则遵循反Hebbian规则。此外，实验还表明，具有权重衰减的网络中可以从几乎任何学习规则（甚至是随机规则）中涌现出Hebbian学习属性。这些结果可能弥合了人工和生物学习之间长期存在的差距，揭示了Hebbian属性作为更深层次优化原则的副现象，并提醒我们不要将神经数据中的Hebbian属性的存在解释为反对更复杂异突触机制的证据。


<details>
  <summary>更多</summary>
  
**动机:** 尽管SGD在机器学习中取得了巨大的成功，但其非局部性与生物学习机制似乎存在根本差异，而Hebbian学习被认为是生物脑的主要学习方式。因此，建立SGD与Hebbian学习之间的联系，有助于理解人工和生物学习之间的桥梁，并探索Hebbian学习是否可以作为更深层次优化原则的副现象。

**方法:** 研究通过理论分析和实验证据相结合的方式，探讨了使用SGD（带权重衰减和注入噪声）训练的神经网络与使用Hebbian学习训练的神经网络在接近收敛时的学习信号之间的联系。具体方法包括：1. 理论证明SGD在特定条件下可表现为Hebbian或反Hebbian规则；2. 实验验证不同学习规则（包括随机规则）下，具有权重衰减的网络中是否能涌现出Hebbian学习属性。

**结果:** 1. 带有正则化的SGD表现出遵循Hebbian规则学习的特性；2. 带有注入噪声的SGD表现出遵循反Hebbian规则学习的特性；3. 具有权重衰减的网络中可以从几乎任何学习规则（甚至是随机规则）中涌现出Hebbian学习属性。

**结论:** 本研究揭示了Hebbian学习属性可能是更深层次优化原则的副现象，并提出了对神经数据中Hebbian属性存在性的重新解读，即不能简单将其视为反对更复杂异突触机制的证据。这为人工和生物学习之间的联系提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergence+of+Hebbian+Dynamics+in+Regularized+Non-Local+Learners，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18069，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18069&send_immediately=true&force_search=false)

**原文摘要:** Stochastic Gradient Descent (SGD) has emerged as a remarkably effective
learning algorithm, underpinning nearly all state-of-the-art machine learning
models, from large language models to autonomous vehicles. Despite its
practical success, SGD appears fundamentally distinct from biological learning
mechanisms. It is widely believed that the biological brain can not implement
gradient descent because it is nonlocal, and we have found little (if any)
experimental evidence for it. In contrast, the brain is widely thought to learn
via local Hebbian learning principles, which have been seen as incompatible
with gradient descent. In this paper, we establish a theoretical and empirical
connection between the learning signals of neural networks trained using SGD
with weight decay and those trained with Hebbian learning near convergence. We
show that SGD with regularization can appear to learn according to a Hebbian
rule, and SGD with injected noise according to an anti-Hebbian rule. We also
provide empirical evidence that Hebbian learning properties can emerge in a
network with weight decay from virtually any learning rule--even random ones.
These results may bridge a long-standing gap between artificial and biological
learning, revealing Hebbian properties as an epiphenomenon of deeper
optimization principles and cautioning against interpreting their presence in
neural data as evidence against more complex hetero-synaptic mechanisms.

</details>


### [164] [An Iterative Framework for Generative Backmapping of Coarse Grained Proteins](https://arxiv.org/abs/2505.18082)
*Georgios Kementzidis, Erin Wong, John Nicholson, Ruichen Xu, Yuefan Deng*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的迭代框架，使用条件变分自编码器和基于图的神经网络，解决了从粗粒度到细粒度数据反向映射中的准确性和训练稳定性问题，特别适用于大规模生物分子如蛋白质。该方法通过多步方案提高了重建精度并优化了计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的从粗粒度到细粒度表示的数据驱动反向映射技术在应用于复杂系统（如蛋白质）时，面临准确性、训练不稳定和物理现实主义的问题。

**方法:** 提出了一种新的迭代框架，结合条件变分自编码器和基于图的神经网络，逐步将粗粒度珠子细化为完整的原子细节，并应用多步生成反向映射方案。

**结果:** 通过数值实验表明，多步方案不仅提高了不同结构蛋白质重建的准确性，还使具有超粗粒度表示的蛋白质的训练过程更加计算高效。

**结论:** 所提出的迭代框架在处理大规模生物分子的反向映射问题上表现出色，提高了重建精度并优化了计算资源使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Iterative+Framework+for+Generative+Backmapping+of+Coarse+Grained+Proteins，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18082，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18082&send_immediately=true&force_search=false)

**原文摘要:** The techniques of data-driven backmapping from coarse-grained (CG) to
fine-grained (FG) representation often struggle with accuracy, unstable
training, and physical realism, especially when applied to complex systems such
as proteins. In this work, we introduce a novel iterative framework by using
conditional Variational Autoencoders and graph-based neural networks,
specifically designed to tackle the challenges associated with such large-scale
biomolecules. Our method enables stepwise refinement from CG beads to full
atomistic details. We outline the theory of iterative generative backmapping
and demonstrate via numerical experiments the advantages of multistep schemes
by applying them to proteins of vastly different structures with very coarse
representations. This multistep approach not only improves the accuracy of
reconstructions but also makes the training process more computationally
efficient for proteins with ultra-CG representations.

</details>


### [165] [What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?](https://arxiv.org/abs/2505.18083)
*Quentin Clark, Florian Shkurti*

**主要类别:** cs.LG

**概要:** 这篇论文研究了生成式行为克隆(BC)方法中的拼接能力，特别是扩散规划器的组成特性。通过实验比较发现，位置等变性和局部感受性是实现组合能力的关键属性，并且简单的架构选择可以与更昂贵的方法竞争。


<details>
  <summary>更多</summary>
  
**动机:** 尽管拼接能力在离线强化学习中是一个传统优势，但最近的生成式行为克隆方法也展示了这种能力。然而，背后的主要因素尚未明确，这阻碍了可靠拼接新算法的发展。因此，本文旨在探讨和理解实现拼接能力的关键因素。

**方法:** 研究聚焦于通过行为克隆训练的扩散规划器，发现了两个关键属性：位置等变性和局部感受性。利用这两个属性解释了现有的扩散规划方法中的架构、数据和推理选择，包括重新规划频率、数据增强和数据缩放等。

**结果:** 1. 局部感受性比位置等变性更重要，但两者对于创建具有组合能力的扩散规划器都至关重要；2. 通过简单的架构选择来启用这些属性可以与更计算昂贵的方法（如重新规划或数据缩放）竞争；3. 简单的基于修复的指导可以使架构上具有组合性的模型在目标条件设置下实现泛化。

**结论:** 位置等变性和局部感受性是实现组合能力的关键属性，通过简单的架构选择可以实现与复杂方法相当的效果，并且简单的指导策略可以进一步提升泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Do+You+Need+for+Diverse+Trajectory+Stitching+in+Diffusion+Planning%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18083&send_immediately=true&force_search=false)

**原文摘要:** In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.

</details>


### [166] [Early-Exit Graph Neural Networks](https://arxiv.org/abs/2505.18088)
*Andrea Giuseppe Di Francesco, Maria Sofia Bucarelli, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Fabrizio Silvestri*

**主要类别:** cs.LG

**概要:** 通过引入对称-反对称图神经网络（SAS-GNN）来缓解过平滑和过压缩问题，提出了带有置信度感知退出头的早退图神经网络（EEGNN），能够在减少计算和延迟的同时保持竞争力的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的早期退出机制在深度学习领域已被证明有效，但在图神经网络（GNN）中尚未充分探索，尤其是在需要深层架构同时抵抗过平滑和过压缩的情况下。

**方法:** 首先引入了对称-反对称图神经网络（SAS-GNN），其基于对称性的归纳偏差可以缓解过平滑和过压缩问题，并生成稳定的中间表示以支持GNN中的早期退出。然后提出了早退图神经网络（EEGNN），它附加了置信度感知退出头，允许根据节点或整个图实时终止传播。

**结果:** 实验表明，EEGNN随着深度增加保持稳健性能，并在异质性和长距离基准测试中提供具有竞争力的准确性，同时显著减少了计算量和延迟，与注意力机制和异步消息传递模型相当。

**结论:** EEGNN不仅保留了深度增长时的鲁棒性能，还在减少计算资源消耗方面表现出色，为未来GNNs的高效设计提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Early-Exit+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18088，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18088&send_immediately=true&force_search=false)

**原文摘要:** Early-exit mechanisms allow deep neural networks to halt inference as soon as
classification confidence is high enough, adaptively trading depth for
confidence, and thereby cutting latency and energy on easy inputs while
retaining full-depth accuracy for harder ones. Similarly, adding early exit
mechanisms to Graph Neural Networks (GNNs), the go-to models for
graph-structured data, allows for dynamic trading depth for confidence on
simple graphs while maintaining full-depth accuracy on harder and more complex
graphs to capture intricate relationships. Although early exits have proven
effective across various deep learning domains, their potential within GNNs in
scenarios that require deep architectures while resisting over-smoothing and
over-squashing remains largely unexplored. We unlock that potential by first
introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose
symmetry-based inductive biases mitigate these issues and yield stable
intermediate representations that can be useful to allow early exiting in GNNs.
Building on this backbone, we present Early-Exit Graph Neural Networks
(EEGNNs), which append confidence-aware exit heads that allow on-the-fly
termination of propagation based on each node or the entire graph. Experiments
show that EEGNNs preserve robust performance as depth grows and deliver
competitive accuracy on heterophilic and long-range benchmarks, matching
attention-based and asynchronous message-passing models while substantially
reducing computation and latency. We plan to release the code to reproduce our
experiments.

</details>


### [167] [Towards more transferable adversarial attack in black-box manner](https://arxiv.org/abs/2505.18097)
*Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, Chun Pong Lau*

**主要类别:** cs.LG

**概要:** 本论文提出了一种新的损失函数和替代模型，通过结合时间依赖分类器的分数来验证假设。该方法在减少计算开销的同时，显著提高了对抗性攻击的可转移性，并对扩散防御保持鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的黑盒攻击方法主要依赖优化框架改进以提高可转移性，但对代理白盒模型架构的依赖较少研究。尽管DiffPGD通过扩散模型提高了可转移性，但其去噪过程带来了高昂的计算成本。因此，作者希望找到一种既能保持或超越可转移性，又能大幅降低计算开销的方法。

**方法:** 作者提出了一种新的损失函数和独特的替代模型。该方法利用分类器引导的扩散模型中时间依赖分类器的分数，将自然数据分布知识融入对抗性优化过程中，从而减少对扩散模型的依赖。

**结果:** 实验结果表明，所提出的方法在多种模型架构上显著提高了可转移性，同时对扩散防御具有较强的鲁棒性。此外，该方法大幅降低了计算开销。

**结论:** 本文提出的方法通过引入新的损失函数和替代模型，在减少计算资源消耗的同时，有效提升了对抗性攻击的可转移性，并对扩散防御表现出良好的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+more+transferable+adversarial+attack+in+black-box+manner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18097，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18097&send_immediately=true&force_search=false)

**原文摘要:** Adversarial attacks have become a well-explored domain, frequently serving as
evaluation baselines for model robustness. Among these, black-box attacks based
on transferability have received significant attention due to their practical
applicability in real-world scenarios. Traditional black-box methods have
generally focused on improving the optimization framework (e.g., utilizing
momentum in MI-FGSM) to enhance transferability, rather than examining the
dependency on surrogate white-box model architectures. Recent state-of-the-art
approach DiffPGD has demonstrated enhanced transferability by employing
diffusion-based adversarial purification models for adaptive attacks. The
inductive bias of diffusion-based adversarial purification aligns naturally
with the adversarial attack process, where both involving noise addition,
reducing dependency on surrogate white-box model selection. However, the
denoising process of diffusion models incurs substantial computational costs
through chain rule derivation, manifested in excessive VRAM consumption and
extended runtime. This progression prompts us to question whether introducing
diffusion models is necessary. We hypothesize that a model sharing similar
inductive bias to diffusion-based adversarial purification, combined with an
appropriate loss function, could achieve comparable or superior transferability
while dramatically reducing computational overhead. In this paper, we propose a
novel loss function coupled with a unique surrogate model to validate our
hypothesis. Our approach leverages the score of the time-dependent classifier
from classifier-guided diffusion models, effectively incorporating natural data
distribution knowledge into the adversarial optimization process. Experimental
results demonstrate significantly improved transferability across diverse model
architectures while maintaining robustness against diffusion-based defenses.

</details>


### [168] [Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning](https://arxiv.org/abs/2505.18101)
*Congren Dai, Huichi Zhou, Jiahao Huang, Zhenxuan Zhang, Fanwen Wang, Guang Yang, Fei Ye*

**主要类别:** cs.LG

**概要:** 本研究提出了一种创新的记忆框架，结合短期记忆系统和长期记忆系统以应对在线持续学习（OCL）中的灾难性遗忘问题。长期记忆系统通过K-means样本选择方法和最优传输机制优化存储结构，确保语义丰富信息的保存，并通过分治（DAC）策略降低计算成本。实验结果表明，该框架在标准和不平衡学习环境中均达到最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 在线持续学习（OCL）中，数据以批量在线形式到达，模型面临灾难性遗忘的风险，这显著影响了模型的有效性。因此，需要一种新的方法来解决这一问题，同时高效管理动态信息和持久知识。

**方法:** 研究引入了一个包含短期记忆系统和长期记忆系统的创新记忆框架。短期记忆系统用于保留动态信息，而长期记忆系统则由多个子记忆缓冲区组成，每个缓冲区与一个类别簇原型相关联，用于存储不同类别的数据样本。此外，研究提出了基于K-means的样本选择方法来识别类别簇原型，并引入了一种新的记忆优化策略，利用最优传输机制选择性地保留关键样本。最后，提出了一种分治（DAC）方法，将内存更新过程建模为优化问题并分解为若干子问题，从而显著减少计算量。

**结果:** 一系列实验在标准和不平衡学习设置下进行，实证结果表明，所提出的记忆框架在这两种学习环境中均达到了最先进的性能。

**结论:** 提出的记忆框架有效地解决了在线持续学习中的灾难性遗忘问题，并通过优化策略和分治方法提高了计算效率。实验结果验证了该框架在多种学习环境下的优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Dual+Buffer+with+Divide-and-Conquer+Strategy+for+Online+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18101&send_immediately=true&force_search=false)

**原文摘要:** Online Continual Learning (OCL) presents a complex learning environment in
which new data arrives in a batch-to-batch online format, and the risk of
catastrophic forgetting can significantly impair model efficacy. In this study,
we address OCL by introducing an innovative memory framework that incorporates
a short-term memory system to retain dynamic information and a long-term memory
system to archive enduring knowledge. Specifically, the long-term memory system
comprises a collection of sub-memory buffers, each linked to a cluster
prototype and designed to retain data samples from distinct categories. We
propose a novel $K$-means-based sample selection method to identify cluster
prototypes for each encountered category. To safeguard essential and critical
samples, we introduce a novel memory optimisation strategy that selectively
retains samples in the appropriate sub-memory buffer by evaluating each cluster
prototype against incoming samples through an optimal transportation mechanism.
This approach specifically promotes each sub-memory buffer to retain data
samples that exhibit significant discrepancies from the corresponding cluster
prototype, thereby ensuring the preservation of semantically rich information.
In addition, we propose a novel Divide-and-Conquer (DAC) approach that
formulates the memory updating as an optimisation problem and divides it into
several subproblems. As a result, the proposed DAC approach can solve these
subproblems separately and thus can significantly reduce computations of the
proposed memory updating process. We conduct a series of experiments across
standard and imbalanced learning settings, and the empirical findings indicate
that the proposed memory framework achieves state-of-the-art performance in
both learning contexts.

</details>


### [169] [Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization](https://arxiv.org/abs/2505.18113)
*Halyun Jeong, Jack Xin, Penghang Yin*

**主要类别:** cs.LG

**概要:** 这篇论文提出了有限样本分析的直通估计器（STE）在量化神经网络中的应用，揭示了样本量对STE成功的关键作用，并通过二层神经网络的量化感知训练推导出收敛到全局最小值的样本复杂度界。此外，在标签噪声存在的情况下，发现了STE梯度方法的有趣循环特性。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于STE的研究多假设无限数据量，缺乏对有限样本下STE性能的理论探索。因此，研究有限样本条件下STE的表现及其优化行为具有重要意义。

**方法:** 作者通过分析一个具有二值权重和激活的二层神经网络的量化感知训练过程，利用压缩感知和动力系统理论工具，推导出保证STE优化收敛至全局最小值的样本复杂度界。同时，探讨了在标签噪声情况下STE梯度方法的循环特性。

**结果:** 1. 推导出了样本复杂度与数据维度的关系，确保STE优化能够收敛到全局最小值。
2. 在标签噪声存在时，发现STE梯度方法具有循环性质，即迭代过程会反复逃离并返回到最优的二值权重。

**结论:** 该研究首次提供了STE在有限样本条件下的理论分析，强调了样本量在STE成功中的关键作用，并为理解STE在实际训练中的行为提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Discreteness%3A+Finite-Sample+Analysis+of+Straight-Through+Estimator+for+Quantization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18113，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18113&send_immediately=true&force_search=false)

**原文摘要:** Training quantized neural networks requires addressing the non-differentiable
and discrete nature of the underlying optimization problem. To tackle this
challenge, the straight-through estimator (STE) has become the most widely
adopted heuristic, allowing backpropagation through discrete operations by
introducing surrogate gradients. However, its theoretical properties remain
largely unexplored, with few existing works simplifying the analysis by
assuming an infinite amount of training data. In contrast, this work presents
the first finite-sample analysis of STE in the context of neural network
quantization. Our theoretical results highlight the critical role of sample
size in the success of STE, a key insight absent from existing studies.
Specifically, by analyzing the quantization-aware training of a two-layer
neural network with binary weights and activations, we derive the sample
complexity bound in terms of the data dimensionality that guarantees the
convergence of STE-based optimization to the global minimum. Moreover, in the
presence of label noises, we uncover an intriguing recurrence property of
STE-gradient method, where the iterate repeatedly escape from and return to the
optimal binary weights. Our analysis leverages tools from compressed sensing
and dynamical systems theory.

</details>


### [170] [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
*Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, Haoxiang Wang*

**主要类别:** cs.LG

**概要:** 本文提出了一种名为Negative-aware Fine-Tuning (NFT)的监督学习方法，该方法使大语言模型能够通过建模自生成的负面答案来反思错误并自主改进。实验表明，NFT在数学推理任务中显著优于传统的监督学习基线，并且可以匹敌甚至超越强化学习算法如GRPO和DAPO。此外，理论分析揭示了NFT与GRPO在严格on-policy训练中的等价性，从而弥合了二元反馈学习系统中监督学习和强化学习方法之间的差距。


<details>
  <summary>更多</summary>
  
**动机:** 尽管强化学习（RL）在提升大语言模型数学能力方面取得了显著成果，但监督学习（SL）由于对参考答案的依赖以及无法有效反思错误而较少被用于验证驱动的训练。因此，挑战RL独占自我改进的传统观念，探索一种基于SL的新方法成为研究动机。

**方法:** 提出了一种名为Negative-aware Fine-Tuning (NFT)的方法。该方法在线训练过程中不丢弃自生成的负面答案，而是构建一个隐式的负面策略来建模这些错误。此隐式策略使用与优化正向数据相同的参数化正向LLM，从而实现对所有LLM生成内容的直接策略优化。

**结果:** 实验结果表明，在7B和32B规模的模型上，NFT通过利用负面反馈显著提升了性能，超过传统的SL基线如拒绝采样微调（Rejection sampling Fine-Tuning），并且匹配或超越了领先的RL算法如GRPO和DAPO。此外，理论分析证明NFT和GRPO在严格on-policy训练中是等价的。

**结论:** NFT作为一种监督学习方法，在利用负面反馈进行自我改进方面表现出色，并且与强化学习方法具有理论上的联系。这项工作弥合了监督学习和强化学习在二元反馈学习系统中的差距，为未来的研究提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Supervised+Learning+and+Reinforcement+Learning+in+Math+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18116，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18116&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.

</details>


### [171] [TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations](https://arxiv.org/abs/2505.18125)
*Alan Arazi, Eilam Shapira, Roi Reichart*

**主要类别:** cs.LG

**概要:** 深度学习在表格数据任务上表现不如GBDT，但新的进展带来了表格式基础模型（Tabular Foundation Models）的可能。这些模型能够利用现实世界知识并在多样化数据集上泛化，特别是包含自由文本的数据。本文提出了一种新型模型TabSTAR，通过语义目标感知表示，在无需数据集特定参数的情况下实现了表格数据上的迁移学习，并在分类任务中取得最佳性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习在许多领域取得了显著成功，但在表格学习任务上表现不佳，而梯度提升决策树（GBDTs）依然占据主导地位。这促使研究者探索如何将语言模型的能力引入表格任务，以提高其效果。

**方法:** TabSTAR是一种具有语义目标感知表示的基础表格模型。它解冻了预训练文本编码器，并将目标标记作为输入，从而为模型提供学习任务特定嵌入所需的上下文。该模型架构无需数据集特定参数，适用于包含文本特征的表格数据迁移学习。

**结果:** TabSTAR在已知基准测试中的中型和大型数据集上达到了最先进的性能，并且其预训练阶段展示了数据集数量的扩展规律，为未来性能改进提供了方向。

**结论:** TabSTAR作为一种新的表格式基础模型，通过语义目标感知表示，成功实现了表格数据上的迁移学习，并在包含文本特征的分类任务中表现出色。其预训练阶段的扩展规律为进一步性能优化提供了可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TabSTAR%3A+A+Foundation+Tabular+Model+With+Semantically+Target-Aware+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18125&send_immediately=true&force_search=false)

**原文摘要:** While deep learning has achieved remarkable success across many domains, it
has historically underperformed on tabular learning tasks, which remain
dominated by gradient boosting decision trees (GBDTs). However, recent
advancements are paving the way for Tabular Foundation Models, which can
leverage real-world knowledge and generalize across diverse datasets,
particularly when the data contains free-text. Although incorporating language
model capabilities into tabular tasks has been explored, most existing methods
utilize static, target-agnostic textual representations, limiting their
effectiveness. We introduce TabSTAR: a Foundation Tabular Model with
Semantically Target-Aware Representations. TabSTAR is designed to enable
transfer learning on tabular data with textual features, with an architecture
free of dataset-specific parameters. It unfreezes a pretrained text encoder and
takes as input target tokens, which provide the model with the context needed
to learn task-specific embeddings. TabSTAR achieves state-of-the-art
performance for both medium- and large-sized datasets across known benchmarks
of classification tasks with text features, and its pretraining phase exhibits
scaling laws in the number of datasets, offering a pathway for further
performance improvements.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [172] [An Affective-Taxis Hypothesis for Alignment and Interpretability](https://arxiv.org/abs/2505.17024)
*Eli Sennesh, Maxwell Ramstead*

**主要类别:** cs.AI

**概要:** 这篇论文提出了一个基于情感主义的方法来解决AI对齐问题，将目标和价值重新定义为情感趋向，并通过计算模型和生物学证据进行验证。


<details>
  <summary>更多</summary>
  
**动机:** 当前的AI对齐研究需要一种新的方法来确保智能体始终与人类操作员的目标和价值观保持一致。

**方法:** 提出了一种情感主义方法，将目标和价值重新定义为情感趋向，并结合进化-发育学和计算神经科学的最新工作，构建了一个基于趋向导航的计算模型。

**结果:** 证明了该模型在某种程度上反映了生物趋向导航的特性，并为情感趋向在AI对齐中的作用提供了讨论基础。

**结论:** 情感趋向可能在AI对齐中扮演重要角色，值得进一步研究其理论和应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Affective-Taxis+Hypothesis+for+Alignment+and+Interpretability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17024，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17024&send_immediately=true&force_search=false)

**原文摘要:** AI alignment is a field of research that aims to develop methods to ensure
that agents always behave in a manner aligned with (i.e. consistently with) the
goals and values of their human operators, no matter their level of capability.
This paper proposes an affectivist approach to the alignment problem,
re-framing the concepts of goals and values in terms of affective taxis, and
explaining the emergence of affective valence by appealing to recent work in
evolutionary-developmental and computational neuroscience. We review the state
of the art and, building on this work, we propose a computational model of
affect based on taxis navigation. We discuss evidence in a tractable model
organism that our model reflects aspects of biological taxis navigation. We
conclude with a discussion of the role of affective taxis in AI alignment.

</details>


### [173] [MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph](https://arxiv.org/abs/2505.17214)
*Xiaochen Wang, Yuan Zhong, Lingwei Zhang, Lisong Dai, Ting Wang, Fenglong Ma*

**主要类别:** cs.AI

**概要:** MEDMKG是一种融合视觉和文本医学信息的多模态医学知识图谱，通过多阶段构建管道、邻域感知过滤算法等方法提高下游医学任务性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的医学深度学习模型主要依赖于单模态知识图谱（如UMLS），但整合多模态医学知识图谱尚未得到充分研究，特别是在将影像数据与临床概念联系起来的资源匮乏的情况下。

**方法:** 提出MEDMKG，通过多阶段构建管道将MIMIC-CXR的丰富多模态数据与UMLS的结构化临床知识相融合，并使用基于规则的工具和大型语言模型进行准确的概念提取和关系建模。此外，引入了邻域感知过滤（NaF）算法以确保图的质量和紧凑性。

**结果:** 在六个数据集上对24种基线方法和四种最先进的视觉-语言骨干进行了评估，结果表明MEDMKG不仅提高了下游医学任务的性能，还为开发适应性强和稳健的多模态知识集成策略提供了坚实基础。

**结论:** MEDMKG作为一种新的多模态医学知识图谱，能够有效提升医学人工智能中的多模态知识整合能力，推动相关领域的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MEDMKG%3A+Benchmarking+Medical+Knowledge+Exploitation+with+Multimodal+Knowledge+Graph，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17214&send_immediately=true&force_search=false)

**原文摘要:** Medical deep learning models depend heavily on domain-specific knowledge to
perform well on knowledge-intensive clinical tasks. Prior work has primarily
leveraged unimodal knowledge graphs, such as the Unified Medical Language
System (UMLS), to enhance model performance. However, integrating multimodal
medical knowledge graphs remains largely underexplored, mainly due to the lack
of resources linking imaging data with clinical concepts. To address this gap,
we propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and
textual medical information through a multi-stage construction pipeline. MEDMKG
fuses the rich multimodal data from MIMIC-CXR with the structured clinical
knowledge from UMLS, utilizing both rule-based tools and large language models
for accurate concept extraction and relationship modeling. To ensure graph
quality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel
filtering algorithm tailored for multimodal knowledge graphs. We evaluate
MEDMKG across three tasks under two experimental settings, benchmarking
twenty-four baseline methods and four state-of-the-art vision-language
backbones on six datasets. Results show that MEDMKG not only improves
performance in downstream medical tasks but also offers a strong foundation for
developing adaptive and robust strategies for multimodal knowledge integration
in medical artificial intelligence.

</details>


### [174] [Effective Reinforcement Learning for Reasoning in Language Models](https://arxiv.org/abs/2505.17218)
*Lianghuan Huang, Shuo Li, Sagnik Anupam, Insup Lee, Osbert Bastani*

**主要类别:** cs.AI

**概要:** 强化学习（RL）在提升语言模型（LM）推理能力方面具有潜力，但大多数现代RL算法针对机器人技术而非LM推理。本文分析了适用于LM推理的RL算法设计决策，并提出了新算法DASH，显著减少了训练时间而不牺牲准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习算法主要为机器人应用而设计，与语言模型推理任务需求不符。需要探索适合语言模型推理的强化学习方法，在准确性和计算效率之间取得平衡。

**方法:** 研究了多种强化学习策略对语言模型推理的影响，包括on-policy RL、PPO-based off-policy更新和移除KL散度的效果。提出了一种新算法DASH，采用预取样和梯度过滤技术，以解决推理和反向传播的最佳批量大小不同的瓶颈问题。

**结果:** (i) on-policy RL显著优于监督微调(SFT)；(ii) PPO-based off-policy更新提高了准确性而非减少方差；(iii) 移除KL散度可生成更简洁的结果并提高准确性。此外，DASH算法将训练时间减少了83%，且未损失准确性。

**结论:** 本研究为设计有效的语言模型推理强化学习算法提供了宝贵见解，特别是针对计算资源有限的小型模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Effective+Reinforcement+Learning+for+Reasoning+in+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17218，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17218&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has emerged as a promising strategy for improving
the reasoning capabilities of language models (LMs) in domains such as
mathematics and coding. However, most modern RL algorithms were designed to
target robotics applications, which differ significantly from LM reasoning. We
analyze RL algorithm design decisions for LM reasoning, for both accuracy and
computational efficiency, focusing on relatively small models due to
computational constraints. Our findings are: (i) on-policy RL significantly
outperforms supervised fine-tuning (SFT), (ii) PPO-based off-policy updates
increase accuracy instead of reduce variance, and (iii) removing KL divergence
can lead to more concise generations and higher accuracy. Furthermore, we find
that a key bottleneck to computational efficiency is that the optimal batch
sizes for inference and backpropagation are different. We propose a novel
algorithm, DASH, that performs preemptive sampling (i.e., sample a large batch
and accumulate gradient updates in small increments), and gradient filtering
(i.e., drop samples with small advantage estimates). We show that DASH reduces
training time by 83% compared to a standard implementation of GRPO without
sacrificing accuracy. Our findings provide valuable insights on designing
effective RL algorithms for LM reasoning.

</details>


### [175] [Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models](https://arxiv.org/abs/2505.17225)
*Doohyuk Jang, Yoonjeon Kim, Chanjae Park, Hyun Ryu, Eunho Yang*

**主要类别:** cs.AI

**概要:** 大型语言模型在长而复杂的推理任务中表现出色，但常依赖熟悉的推理模式（称为推理刚性），即使用户明确指示，模型仍可能忽略条件。这种行为在数学和逻辑谜题领域尤为不利。为此，研究者引入了一个专家策划的诊断数据集，包含修改后的数学基准测试和重新设计的谜题，以要求模型偏离惯常推理策略。通过该数据集，研究者识别出三种推理污染模式：解释过载、输入不信任和部分指令关注。他们公开发布此数据集以促进未来研究。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在复杂推理任务中表现出色，但它们过度依赖熟悉的推理模式（推理刚性），这可能导致忽略用户明确的指示并产生错误结论。特别是在需要精确遵循特定约束的数学和逻辑谜题领域，这种行为成为重大挑战。因此，研究者希望系统地研究这一现象并提供解决方案。

**方法:** 研究者创建了一个专家策划的诊断数据集，包括修改后的AIME和MATH500数学基准测试以及重新设计的经典谜题，这些都要求模型偏离惯常的推理策略。然后，通过分析模型在这些任务中的表现，研究者将推理刚性问题归类为三种主要模式：解释过载、输入不信任和部分指令关注。

**结果:** 研究者成功识别了三种推理污染模式，并验证了这些模式如何导致模型在处理需要偏离熟悉推理路径的任务时出现问题。此外，通过公开发布的诊断数据集，可以为后续研究提供支持。

**结论:** 推理刚性是大型语言模型面临的一个重要问题，尤其是在需要精确遵循约束的任务中。通过使用新开发的诊断数据集，研究者能够系统地研究这一问题，并将其分为三个主要类别。未来的研究可以通过改进模型设计或训练方法来缓解这一问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Model+is+Stubborn%3A+Diagnosing+Instruction+Overriding+in+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17225&send_immediately=true&force_search=false)

**原文摘要:** Large language models have demonstrated remarkable proficiency in long and
complex reasoning tasks. However, they frequently exhibit a problematic
reliance on familiar reasoning patterns, a phenomenon we term \textit{reasoning
rigidity}. Despite explicit instructions from users, these models often
override clearly stated conditions and default to habitual reasoning
trajectories, leading to incorrect conclusions. This behavior presents
significant challenges, particularly in domains such as mathematics and logic
puzzle, where precise adherence to specified constraints is critical. To
systematically investigate reasoning rigidity, a behavior largely unexplored in
prior work, we introduce a expert-curated diagnostic set, \dataset{}. Our
dataset includes specially modified variants of existing mathematical
benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately
redesigned to require deviation from familiar reasoning strategies. Using this
dataset, we identify recurring contamination patterns that occur when models
default to ingrained reasoning. Specifically, we categorize this contamination
into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,
and (iii) Partial Instruction Attention, each causing models to ignore or
distort provided instructions. We publicly release our diagnostic set to
facilitate future research on mitigating reasoning rigidity in language models.

</details>


### [176] [Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning](https://arxiv.org/abs/2505.17249)
*Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao*

**主要类别:** cs.AI

**概要:** SILIC是一种基于LLM引导的逆向强化学习和认知链推理框架，用于从移动模式中推断社会人口属性，通过结合行为意图和心理学理论（如计划行为理论），显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 在大规模轨迹数据分析中，缺乏关键的旅行者属性，尤其是社会人口统计信息，限制了数据的应用价值。先前研究虽尝试从移动模式预测这些属性，但往往忽略了底层认知机制且预测准确性较低。

**方法:** 引入SILIC框架，结合LLM引导的逆向强化学习（IRL）和认知链推理（CCR），通过捕捉潜在行为意图和心理结构推断社会人口属性，并遵循计划行为理论建模个体潜在认知过程，同时利用LLM解决奖励函数初始化和优化问题。

**结果:** 在2017年Puget Sound区域委员会家庭旅行调查数据中，SILIC方法显著优于现有最先进基线模型。

**结论:** SILIC框架展示了丰富大规模轨迹数据的潜力，可支持更符合行为基础的交通规划及其他应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Where+You+Go+is+Who+You+Are%3A+Behavioral+Theory-Guided+LLMs+for+Inverse+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17249，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17249&send_immediately=true&force_search=false)

**原文摘要:** Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.

</details>


### [177] [AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking](https://arxiv.org/abs/2505.17312)
*Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang*

**主要类别:** cs.AI

**概要:** AdaReasoner是一个为任何大型语言模型（LLM）设计的插件，用于自动化适应性推理配置。它通过强化学习框架训练，结合分解的动作空间和有针对性的探索策略，以及预训练的奖励模型来优化推理配置。在六个不同的LLM和各种推理任务中，AdaReasoner的表现始终优于标准基准，并在知识密集型任务中通过定制提示提供增益。


<details>
  <summary>更多</summary>
  
**动机:** 现有的提示方法通常采用适用于各种任务的一般固定配置，这些配置在跨任务时'足够好'，但很少达到特定任务的最佳效果。因此，需要一种能够根据任务需求自适应调整推理配置的方法。

**方法:** AdaReasoner通过强化学习框架进行训练，该框架结合了分解的动作空间、有针对性的探索策略和预训练的奖励模型。这种方法允许AdaReasoner仅通过少量示例指导来优化策略模型以进行推理配置。

**结果:** AdaReasoner在六种不同的LLM和多种推理任务中表现出色，超越了标准基线。它还保持了分布外鲁棒性，并通过定制提示在知识密集型任务中提供了增益。

**结论:** AdaReasoner提供了一种有效的解决方案，可以为不同类型的思维任务自动调整适应性推理配置，适用于任何LLM。其快速收敛和次线性策略差距得到了理论保证和实验验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AdaReasoner%3A+Adaptive+Reasoning+Enables+More+Flexible+Thinking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17312，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17312&send_immediately=true&force_search=false)

**原文摘要:** LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.

</details>


### [178] [Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning](https://arxiv.org/abs/2505.17315)
*Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, Xiaotian Han*

**主要类别:** cs.AI

**概要:** 近期语言模型展现出强大的推理能力，但长上下文处理能力对推理的影响尚未充分研究。本文通过实验发现，增强模型的长上下文处理能力可显著提升其推理性能，即使在短输入任务中也是如此。因此，建议将长上下文处理能力作为未来语言模型设计的核心目标之一。


<details>
  <summary>更多</summary>
  
**动机:** 当前语言模型在推理方面存在局限性，部分原因可能是长上下文处理能力不足。观察到的现象包括：较长的上下文窗口通常带来更强的推理表现，以及推理失败的情况与长上下文处理失败类似。

**方法:** 对比具有相同架构和微调数据但不同长上下文处理能力的模型，在监督微调（SFT）前增强模型的长上下文能力，并评估其对推理性能的影响。

**结果:** 实验结果表明，具备更强长上下文处理能力的模型在经过SFT后，在推理基准测试中表现出显著更高的准确性，且这种优势在短输入任务中仍然存在。

**结论:** 长上下文建模不仅是处理长输入的关键，也是推理能力的重要基础，应将其视为未来语言模型设计的核心目标之一。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Longer+Context%2C+Deeper+Thinking%3A+Uncovering+the+Role+of+Long-Context+Ability+in+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17315，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17315&send_immediately=true&force_search=false)

**原文摘要:** Recent language models exhibit strong reasoning capabilities, yet the
influence of long-context capacity on reasoning remains underexplored. In this
work, we hypothesize that current limitations in reasoning stem, in part, from
insufficient long-context capacity, motivated by empirical observations such as
(1) higher context window length often leads to stronger reasoning performance,
and (2) failed reasoning cases resemble failed long-context cases. To test this
hypothesis, we examine whether enhancing a model's long-context ability before
Supervised Fine-Tuning (SFT) leads to improved reasoning performance.
Specifically, we compared models with identical architectures and fine-tuning
data but varying levels of long-context capacity. Our results reveal a
consistent trend: models with stronger long-context capacity achieve
significantly higher accuracy on reasoning benchmarks after SFT. Notably, these
gains persist even on tasks with short input lengths, indicating that
long-context training offers generalizable benefits for reasoning performance.
These findings suggest that long-context modeling is not just essential for
processing lengthy inputs, but also serves as a critical foundation for
reasoning. We advocate for treating long-context capacity as a first-class
objective in the design of future language models.

</details>


### [179] [Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)](https://arxiv.org/abs/2505.17323)
*Ruaridh Mon-Williams, Max Taylor-Davies, Elizabeth Mieczkowski, Natalia Velez, Neil R. Bramley, Yanwei Wang, Thomas L. Griffiths, Christopher G. Lucas*

**主要类别:** cs.AI

**概要:** 在开放合作环境中，简单的无模型RNN代理能够自发地发展出对伙伴任务能力的结构化内部表示，从而快速适应并推广到新的合作者。这种伙伴建模只有在施加适当社交压力的环境条件下才会出现。


<details>
  <summary>更多</summary>
  
**动机:** 研究人类协作能力的核心构建块，探讨灵活性是否需要显式的、专门的机制来模拟他人，或者它是否可以自发地从开放合作互动的压力中产生。

**方法:** 使用`Overcooked-AI'环境训练简单的无模型RNN代理与多样化伙伴进行合作，通过分析代理的内部隐藏状态和大规模行为分析，研究代理如何发展出伙伴的任务能力的结构化内部表示。

**结果:** 尽管缺乏额外的架构特性、归纳偏见或辅助目标，代理仍能发展出伙伴任务能力的结构化内部表示，并且当代理可以通过控制任务分配影响伙伴行为时，结构化的伙伴建模会自然出现。

**结论:** 伙伴建模可以在无模型代理中自发出现，但前提是环境条件施加了适当的社交压力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Partner+Modelling+Emerges+in+Recurrent+Agents+%28But+Only+When+It+Matters%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17323&send_immediately=true&force_search=false)

**原文摘要:** Humans are remarkably adept at collaboration, able to infer the strengths and
weaknesses of new partners in order to work successfully towards shared goals.
To build AI systems with this capability, we must first understand its building
blocks: does such flexibility require explicit, dedicated mechanisms for
modelling others -- or can it emerge spontaneously from the pressures of
open-ended cooperative interaction? To investigate this question, we train
simple model-free RNN agents to collaborate with a population of diverse
partners. Using the `Overcooked-AI' environment, we collect data from thousands
of collaborative teams, and analyse agents' internal hidden states. Despite a
lack of additional architectural features, inductive biases, or auxiliary
objectives, the agents nevertheless develop structured internal representations
of their partners' task abilities, enabling rapid adaptation and generalisation
to novel collaborators. We investigated these internal models through probing
techniques, and large-scale behavioural analysis. Notably, we find that
structured partner modelling emerges when agents can influence partner
behaviour by controlling task allocation. Our results show that partner
modelling can arise spontaneously in model-free agents -- but only under
environmental conditions that impose the right kind of social pressure.

</details>


### [180] [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/abs/2505.17348)
*Yuheng Wu, Jianwen Xie, Denghui Zhang, Zhaozhuo Xu*

**主要类别:** cs.AI

**概要:** 本论文提出了一种名为DEL-ToM的框架，通过推理时扩展而非架构更改来提升小型语言模型在Theory-of-Mind任务中的表现。该方法将ToM任务分解为基于动态认知逻辑的信念更新序列，并训练一个称为过程信念模型（PBM）的验证器对每个信念更新步骤进行评分。实验表明，DEL-ToM能够显著提高小型语言模型的ToM能力，而无需重新训练模型。


<details>
  <summary>更多</summary>
  
**动机:** 小型语言模型在处理需要深度社会推理的Theory-of-Mind任务时存在局限性，通常由于规模限制而缺乏足够的推理能力。

**方法:** 提出DEL-ToM框架，将ToM任务分解为基于动态认知逻辑的信念更新序列。训练一个称为过程信念模型（PBM）的验证器，使用通过DEL模拟器自动生成的标签对每个信念更新步骤进行评分。在推理过程中，由语言模型生成的候选信念轨迹通过PBM评估，选择得分最高的轨迹。

**结果:** 在多个模型规模和基准测试中，DEL-ToM框架持续提高了模型性能，证明了可验证信念监督可以显著增强小型语言模型的ToM能力。

**结论:** DEL-ToM框架通过推理时扩展而非架构更改提高了小型语言模型的ToM能力，而无需重新训练模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DEL-ToM%3A+Inference-Time+Scaling+for+Theory-of-Mind+Reasoning+via+Dynamic+Epistemic+Logic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17348，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17348&send_immediately=true&force_search=false)

**原文摘要:** Theory-of-Mind (ToM) tasks pose a unique challenge for small language models
(SLMs) with limited scale, which often lack the capacity to perform deep social
reasoning. In this work, we propose DEL-ToM, a framework that improves ToM
reasoning through inference-time scaling rather than architectural changes. Our
approach decomposes ToM tasks into a sequence of belief updates grounded in
Dynamic Epistemic Logic (DEL), enabling structured and transparent reasoning.
We train a verifier, called the Process Belief Model (PBM), to score each
belief update step using labels generated automatically via a DEL simulator.
During inference, candidate belief traces generated by a language model are
evaluated by the PBM, and the highest-scoring trace is selected. This allows
SLMs to emulate more deliberate reasoning by allocating additional compute at
test time. Experiments across multiple model scales and benchmarks show that
DEL-ToM consistently improves performance, demonstrating that verifiable belief
supervision can significantly enhance ToM abilities of SLMs without retraining.

</details>


### [181] [Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness](https://arxiv.org/abs/2505.17406)
*Enyi Jiang, Changming Xu, Nischay Singh, Gagandeep Singh*

**主要类别:** cs.AI

**概要:** LLMs的决策过程不透明，需要解释技术如Chain-of-Thought。研究设计了新评估框架MATCHA，发现LLMs在输入扰动下可能产生不一致或无意义的推理，并通过LLM评审员评估不同模型的推理鲁棒性。结果显示，在多步和常识任务上比逻辑任务更脆弱。还展示了成功案例对黑箱模型的非平凡转移率。此框架有助于理解LLM推理机制，指导未来模型发展更稳健、推理驱动的架构，强化答案-推理一致性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）的决策过程不透明，亟需解释技术（如思维链）来揭示其决策依据。为了探究答案与推理之间的关系，特别是在教育和医疗等领域中模型可信度的关键因素——推理能力，需要一个新颖的评估框架。

**方法:** 设计了一个名为MATCHA的新评估框架，用于研究大型语言模型在输入扰动下的推理表现。利用LLM评审员评估不同模型的推理鲁棒性，分析模型在多步推理、常识任务和逻辑任务中的差异表现。

**结果:** 1. LLMs在输入扰动下容易产生不一致或无意义的推理；2. 在多步推理和常识任务上表现出更大的脆弱性；3. 成功示例能够以非平凡的转移率迁移到黑箱模型。

**结论:** MATCHA评估框架帮助更好地理解LLM的推理机制，为未来模型的设计提供了指导，推动构建更加稳健和推理驱动的架构，同时强化答案与推理之间的一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Misaligning+Reasoning+with+Answers+--+A+Framework+for+Assessing+LLM+CoT+Robustness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17406&send_immediately=true&force_search=false)

**原文摘要:** LLMs' decision-making process is opaque, prompting the need for explanation
techniques like Chain-of-Thought. To investigate the relationship between
answer and reasoning, we design a novel evaluation framework, MATCHA. In
domains like education and healthcare, reasoning is key for model
trustworthiness. MATCHA reveals that LLMs under input perturbations can give
inconsistent or nonsensical reasoning. Additionally, we use LLM judges to
assess reasoning robustness across models. Our results show that LLMs exhibit
greater vulnerability to input perturbations for multi-step and commonsense
tasks than compared to logical tasks. Also, we show non-trivial transfer rates
of our successful examples to black-box models. Our evaluation framework helps
to better understand LLM reasoning mechanisms and guides future models toward
more robust and reasoning-driven architectures, enforcing answer-reasoning
consistency.

</details>


### [182] [MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models](https://arxiv.org/abs/2505.17433)
*Zhengyi Zhao, Shubo Zhang, Yuxi Zhang, Yanxi Zhao, Yifan Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu*

**主要类别:** cs.AI

**概要:** 模因（memes）的解释高度依赖其出现的具体语境。当前的研究多集中于孤立的模因分析，忽略了同一模因在不同对话情境下可能表达不同意图的问题。为填补这一评估空白，本文提出了MemeReaCon，一个新基准测试，用于评估大型视觉语言模型（LVLMs）对模因在其原始语境中的理解能力。通过从五个不同的Reddit社区收集数据，包括模因图片、帖子文本和用户评论，并进行细致标注，研究发现现有LVLMs在理解模因的上下文信息方面存在明显弱点。MemeReaCon不仅揭示了当前模型的局限性，还为推动更复杂的情境感知型LVLMs的发展提供了挑战性基准。


<details>
  <summary>更多</summary>
  
**动机:** 当前对于模因的研究主要集中在单独的模因分析上，忽略了模因在意图表达上受对话情境的影响，从而导致了评估上的差距。

**方法:** 提出名为MemeReaCon的新基准测试，从五个Reddit社区中收集模因数据，包括模因图片、帖子文本和用户评论，并对这些数据进行细致标注以反映模因与文本如何协作、发帖者的意图、模因的结构以及社区的反应。

**结果:** 使用领先LVLMs进行的测试显示，这些模型要么无法解读上下文中的关键信息，要么过度关注视觉细节而忽略了模因的交流目的。

**结论:** MemeReaCon既是一个诊断工具，用以暴露当前LVLMs的局限性，也是一个推动更复杂情境感知型LVLMs发展的挑战性基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MemeReaCon%3A+Probing+Contextual+Meme+Understanding+in+Large+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17433，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17433&send_immediately=true&force_search=false)

**原文摘要:** Memes have emerged as a popular form of multimodal online communication,
where their interpretation heavily depends on the specific context in which
they appear. Current approaches predominantly focus on isolated meme analysis,
either for harmful content detection or standalone interpretation, overlooking
a fundamental challenge: the same meme can express different intents depending
on its conversational context. This oversight creates an evaluation gap:
although humans intuitively recognize how context shapes meme interpretation,
Large Vision Language Models (LVLMs) can hardly understand context-dependent
meme intent. To address this critical limitation, we introduce MemeReaCon, a
novel benchmark specifically designed to evaluate how LVLMs understand memes in
their original context. We collected memes from five different Reddit
communities, keeping each meme's image, the post text, and user comments
together. We carefully labeled how the text and meme work together, what the
poster intended, how the meme is structured, and how the community responded.
Our tests with leading LVLMs show a clear weakness: models either fail to
interpret critical information in the contexts, or overly focus on visual
details while overlooking communicative purpose. MemeReaCon thus serves both as
a diagnostic tool exposing current limitations and as a challenging benchmark
to drive development toward more sophisticated LVLMs of the context-aware
understanding.

</details>


### [183] [Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning](https://arxiv.org/abs/2505.17436)
*Cheng Peng, Kai Zhang, Mengxian Lyu, Hongfang Liu, Lichao Sun, Yonghui Wu*

**主要类别:** cs.AI

**概要:** The paper presents BiomedGPT-Large and BiomedGPT-XLarge, two biomedical vision-language models designed to excel in multi-modal biomedical tasks. They were fine-tuned on 23 benchmark datasets across 6 tasks, instruction-tuned with a large dataset, and showed improvements over the base model and existing models.


<details>
  <summary>更多</summary>
  
**动机:** To enhance the capabilities of biomedical vision-language models through scaling up, fine-tuning, and instruction tuning, while exploring their performance in zero-shot learning and alignment accuracy.

**方法:** Developed two models (BiomedGPT-Large and BiomedGPT-XLarge) based on an encoder-decoder transformer architecture. Fine-tuned them on 23 benchmark datasets from 6 multi-modal biomedical tasks. Compared these scaled models with previous and existing models. Instruction-tuned using a large-scale multi-modal biomedical dataset.

**结果:** The developed models outperformed the base model and other existing models in various multi-modal biomedical tasks. They also demonstrated good zero-shot learning performance and alignment accuracy.

**结论:** Scaling up, fine-tuning, and instruction tuning significantly improve the performance of biomedical vision-language models, making them more suitable for diverse multi-modal biomedical tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Up+Biomedical+Vision-Language+Models%3A+Fine-Tuning%2C+Instruction+Tuning%2C+and+Multi-Modal+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17436，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17436&send_immediately=true&force_search=false)

**原文摘要:** To advance biomedical vison-language model capabilities through scaling up,
fine-tuning, and instruction tuning, develop vision-language models with
improved performance in handling long text, explore strategies to efficiently
adopt vision language models for diverse multi-modal biomedical tasks, and
examine the zero-shot learning performance.
  We developed two biomedical vision language models, BiomedGPT-Large and
BiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture.
We fine-tuned the two models on 23 benchmark datasets from 6 multi-modal
biomedical tasks including one image-only task (image classification), three
language-only tasks (text understanding, text summarization and question
answering), and two vision-language tasks (visual question answering and image
captioning). We compared the developed scaled models with our previous
BiomedGPT-Base model and existing prestigious models reported in the
literature. We instruction-tuned the two models using a large-scale multi-modal
biomedical instruction-tuning dataset and assessed the zero-shot learning
performance and alignment accuracy.

</details>


### [184] [From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark](https://arxiv.org/abs/2505.17482)
*Chao Lei, Nir Lipovetzky, Krista A. Ehinger, Yanchuan Chang*

**主要类别:** cs.AI

**概要:** 最近的推理导向大语言模型（LLM）在数学和科学考试等复杂任务上表现出色。然而，人类智能的核心认知能力，如抽象推理和泛化，尚未得到充分研究。为此，我们通过将ARC基准视为程序综合任务来评估这些模型，并提出了九种候选解决方案。其中，重复采样规划辅助代码生成（RSPC）实现了最高的测试准确率，并展示了在大多数LLM中的一致泛化性能。为了进一步提高性能，我们引入了一种名为知识增强抽象推理（KAAR）的ARC求解器，该求解器通过分阶段增加先验知识，逐步扩展LLM的推理能力，同时减少无关先验的干扰。实验结果表明，KAAR在所有评估的LLM中保持强大的泛化能力，并显著优于非增强的RSPC，实现约5%的绝对增益和高达64.52%的相对改进。尽管如此，ARC仍然是推理导向LLM的一个挑战性基准，指出了未来LLM发展的方向。


<details>
  <summary>更多</summary>
  
**动机:** 尽管最近的推理导向LLM在解决复杂任务方面表现良好，但它们在抽象推理和泛化等核心认知能力上的表现尚未被充分探索。因此，需要在适当的基准上对这些模型进行评估，以揭示其潜在的能力和局限性。

**方法:** 作者将Abstraction and Reasoning Corpus (ARC)基准作为程序综合任务进行处理，并提出了九种候选解决方案。其中，RSPC方法显示出最高的测试准确率和一致的泛化性能。为了进一步提升性能，作者提出了一种新的求解器KAAR，它通过构建一个分类为三个层次的知识本体，逐步增强LLM的推理能力，并结合RSPC生成候选解决方案。这种方法通过分阶段推理减少了无关先验知识的干扰。

**结果:** KAAR在所有评估的LLM中展现出强大的泛化能力和显著的性能提升，相较于非增强的RSPC实现了约5%的绝对增益和高达64.52%的相对改进。这证明了KAAR的有效性和潜力。

**结论:** 尽管最新的推理导向LLM在某些任务上表现出色，但ARC基准仍然对其构成了重大挑战。这表明，在抽象推理和泛化能力方面，LLM还有很大的改进空间，未来的研究应关注这些领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Reasoning+to+Generalization%3A+Knowledge-Augmented+LLMs+for+ARC+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17482，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17482&send_immediately=true&force_search=false)

**原文摘要:** Recent reasoning-oriented LLMs have demonstrated strong performance on
challenging tasks such as mathematics and science examinations. However, core
cognitive faculties of human intelligence, such as abstract reasoning and
generalization, remain underexplored. To address this, we evaluate recent
reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)
benchmark, which explicitly demands both faculties. We formulate ARC as a
program synthesis task and propose nine candidate solvers. Experimental results
show that repeated-sampling planning-aided code generation (RSPC) achieves the
highest test accuracy and demonstrates consistent generalization across most
LLMs. To further improve performance, we introduce an ARC solver, Knowledge
Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors
within an ontology that classifies priors into three hierarchical levels based
on their dependencies. KAAR progressively expands LLM reasoning capacity by
gradually augmenting priors at each level, and invokes RSPC to generate
candidate solutions after each augmentation stage. This stage-wise reasoning
reduces interference from irrelevant priors and improves LLM performance.
Empirical results show that KAAR maintains strong generalization and
consistently outperforms non-augmented RSPC across all evaluated LLMs,
achieving around 5% absolute gains and up to 64.52% relative improvement.
Despite these achievements, ARC remains a challenging benchmark for
reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.

</details>


### [185] [PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate](https://arxiv.org/abs/2505.17492)
*Dezheng Bao, Yueci Yang, Xin Chen, Zhengxuan Jiang, Zeguo Fei, Daoze Zhang, Xuanwen Huang, Junru Chen, Chutian Yu, Xiang Yuan, Yang Yang*

**主要类别:** cs.AI

**概要:** 提出了一种名为PD³的项目重复检测框架，通过多智能体辩论的方式提升项目重复检测效果，并在实际应用中节省了大量资源。


<details>
  <summary>更多</summary>
  
**动机:** 现有的项目重复检测方法依赖于基本的词汇或句子级别的比较，或者单纯应用大型语言模型，缺乏深入理解项目内容和审查标准的能力。

**方法:** 提出了PD³框架，通过适应性多智能体辩论机制进行项目重复检测，结合定性和定量分析生成反馈。

**结果:** 在超过800个真实电力项目的评估中，该方法在两个下游任务上分别优于现有方法7.43%和8.00%。同时，建立了在线平台Review Dingdang，帮助专家节省了573万美元的初步检测费用。

**结论:** PD³框架能够有效提升项目重复检测的准确性和实用性，为专家提供更有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PD%24%5E3%24%3A+A+Project+Duplication+Detection+Framework+via+Adapted+Multi-Agent+Debate，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17492&send_immediately=true&force_search=false)

**原文摘要:** Project duplication detection is critical for project quality assessment, as
it improves resource utilization efficiency by preventing investing in newly
proposed project that have already been studied. It requires the ability to
understand high-level semantics and generate constructive and valuable
feedback. Existing detection methods rely on basic word- or sentence-level
comparison or solely apply large language models, lacking valuable insights for
experts and in-depth comprehension of project content and review criteria. To
tackle this issue, we propose PD$^3$, a Project Duplication Detection framework
via adapted multi-agent Debate. Inspired by real-world expert debates, it
employs a fair competition format to guide multi-agent debate to retrieve
relevant projects. For feedback, it incorporates both qualitative and
quantitative analysis to improve its practicality. Over 800 real-world power
project data spanning more than 20 specialized fields are used to evaluate the
framework, demonstrating that our method outperforms existing approaches by
7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online
platform, Review Dingdang, to assist power experts, saving 5.73 million USD in
initial detection on more than 100 newly proposed projects.

</details>


### [186] [Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs](https://arxiv.org/abs/2505.17512)
*Shuhang Xu, Weijian Deng, Yixuan Zhou, Fangwei Zhong*

**主要类别:** cs.AI

**概要:** 本研究探讨了大型语言模型（LLMs）对概念性知识的理解能力，并提出了一种新的评估方法CK-Arena，该方法通过多代理互动游戏来测试LLMs在动态环境中的概念推理能力。实验结果表明，LLMs对不同类别的概念知识理解存在显著差异，且这种理解能力并不完全取决于模型的参数量或通用能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管概念对于人类高效分类和推理至关重要，但目前尚不清楚大型语言模型（LLMs）对这些语义关系的理解程度。现有的基准测试通常侧重于事实记忆和孤立任务，未能充分评估LLMs对概念边界的理解能力。

**方法:** 研究人员引入了CK-Arena，这是一种基于Undercover游戏的多代理交互游戏，用于评估LLMs在互动环境中处理概念的能力。该游戏要求模型根据部分信息描述、区分和推断概念边界，从而探索相关概念之间的共同点和区别。

**结果:** 实验结果表明，LLMs对不同类别概念知识的理解存在显著差异，且这种理解能力并不严格与模型参数规模或通用能力相一致。

**结论:** CK-Arena为评估LLMs在动态环境中的概念推理能力提供了一个可扩展且现实的基准。数据和代码可以在项目主页上找到：https://ck-arena.site。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probe+by+Gaming%3A+A+Game-based+Benchmark+for+Assessing+Conceptual+Knowledge+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17512，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17512&send_immediately=true&force_search=false)

**原文摘要:** Concepts represent generalized abstractions that enable humans to categorize
and reason efficiently, yet it is unclear to what extent Large Language Models
(LLMs) comprehend these semantic relationships. Existing benchmarks typically
focus on factual recall and isolated tasks, failing to evaluate the ability of
LLMs to understand conceptual boundaries. To address this gap, we introduce
CK-Arena, a multi-agent interaction game built upon the Undercover game,
designed to evaluate the capacity of LLMs to reason with concepts in
interactive settings. CK-Arena challenges models to describe, differentiate,
and infer conceptual boundaries based on partial information, encouraging
models to explore commonalities and distinctions between closely related
concepts. By simulating real-world interaction, CK-Arena provides a scalable
and realistic benchmark for assessing conceptual reasoning in dynamic
environments. Experimental results show that LLMs' understanding of conceptual
knowledge varies significantly across different categories and is not strictly
aligned with parameter size or general model capabilities. The data and code
are available at the project homepage: https://ck-arena.site.

</details>


### [187] [Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers](https://arxiv.org/abs/2505.17520)
*Salahuddin Alawadhi, Noorhan Abbas*

**主要类别:** cs.AI

**概要:** 将检索增强生成（RAG）与大语言模型（LLMs）结合，在知识密集型领域显示出提供精确、上下文相关响应的潜力。本研究调查了RAG在ABB断路器中的应用，专注于高风险工程环境中的准确性、可靠性和上下文相关性。通过利用定制数据集、先进的嵌入模型和优化的分块策略，研究解决了工程文档中数据检索和上下文对齐的独特挑战。主要贡献包括开发了一个特定领域的ABB断路器数据集，并评估了三种RAG管道：OpenAI GPT4o、Cohere和Anthropic Claude。高级分块方法，如基于段落和标题感知的分割，被评估其对检索准确性和响应生成的影响。结果表明，虽然某些配置实现了高精度和相关性，但在确保工程背景下的事实准确性和完整性方面仍存在局限性。这项工作强调了需要对RAG系统进行迭代改进，以满足电气工程任务的严格需求，包括设计、故障排除和操作决策。本文的研究成果有助于推动人工智能在高度技术领域（如电气工程）的研究。


<details>
  <summary>更多</summary>
  
**动机:** 将RAG与LLMs集成已被证明可以在知识密集型领域提供精确且上下文相关的响应。然而，在高风险工程环境中，如ABB断路器的应用场景，仍然存在数据检索和上下文对齐的独特挑战，这促使了进一步研究如何提高RAG系统的性能。

**方法:** 研究采用了定制的数据集、先进的嵌入模型和优化的分块策略来解决工程文档中的独特挑战。具体来说，开发了一个特定领域的ABB断路器数据集，并评估了三种不同的RAG管道（OpenAI GPT4o、Cohere和Anthropic Claude）。此外，还评估了高级分块方法（如基于段落和标题感知的分割）对检索准确性和响应生成的影响。

**结果:** 尽管某些配置实现了高精度和相关性，但在确保工程背景下事实准确性和完整性方面仍存在局限性。

**结论:** 本研究强调了需要对RAG系统进行迭代改进，以满足电气工程任务的严格需求，包括设计、故障排除和操作决策。研究成果有助于推动人工智能在高度技术领域（如电气工程）的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Retrieval-Augmented+Generation+for+Electrical+Engineering%3A+A+Case+Study+on+ABB+Circuit+Breakers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17520，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17520&send_immediately=true&force_search=false)

**原文摘要:** Integrating Retrieval Augmented Generation (RAG) with Large Language Models
(LLMs) has shown the potential to provide precise, contextually relevant
responses in knowledge intensive domains. This study investigates the
ap-plication of RAG for ABB circuit breakers, focusing on accuracy,
reliability, and contextual relevance in high-stakes engineering environments.
By leveraging tailored datasets, advanced embedding models, and optimized
chunking strategies, the research addresses challenges in data retrieval and
contextual alignment unique to engineering documentation. Key contributions
include the development of a domain-specific dataset for ABB circuit breakers
and the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic
Claude. Advanced chunking methods, such as paragraph-based and title-aware
segmentation, are assessed for their impact on retrieval accuracy and response
generation. Results demonstrate that while certain configurations achieve high
precision and relevancy, limitations persist in ensuring factual faithfulness
and completeness, critical in engineering contexts. This work underscores the
need for iterative improvements in RAG systems to meet the stringent demands of
electrical engineering tasks, including design, troubleshooting, and
operational decision-making. The findings in this paper help advance research
of AI in highly technical domains such as electrical engineering.

</details>


### [188] [Transparency and Proportionality in Post-Processing Algorithmic Bias Correction](https://arxiv.org/abs/2505.17525)
*Juliett Suárez Ferreira, Marija Slavkovik, Jorge Casillas*

**主要类别:** cs.AI

**概要:** 算法决策系统可能会对特定群体产生错误或偏差预测，导致不公平结果。尽管去偏实践可能引入新的不公平形式或加剧现有不平等，本文聚焦于分类任务中修改算法预测的后处理技术，并提出衡量后处理阶段解决方案翻转差异的一组度量方法，以评估去偏策略的比例性、透明性和其他潜在方法的可能性。通过示例展示了这些度量如何补充传统公平性指标，提供更全面的视角以确保所有群体的公平结果。


<details>
  <summary>更多</summary>
  
**动机:** 算法决策系统的偏差可能导致不公平结果，而现有的去偏实践可能引入新的不公平或加剧现有问题。需要更好的工具来理解后处理技术的影响及其对不同群体的公平性。

**方法:** 提出一组度量方法，用于量化后处理阶段应用于解决方案的翻转差异。这些度量帮助评估去偏策略的比例性、解释其在各群体中的影响，并分析其他潜在去偏方法的可行性。此外，还介绍了一种应用这些度量的方法论并通过实例展示其实际应用。

**结果:** 所提出的度量方法能够补充传统公平性指标，提供更深入的视角来评估和确保所有群体的公平结果。实例证明了该方法的有效性和实用性。

**结论:** 通过使用所提出的度量方法，可以更好地评估和解释后处理技术对公平性的影响，从而支持选择合适的去偏策略，实现更公平的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transparency+and+Proportionality+in+Post-Processing+Algorithmic+Bias+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17525，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17525&send_immediately=true&force_search=false)

**原文摘要:** Algorithmic decision-making systems sometimes produce errors or skewed
predictions toward a particular group, leading to unfair results. Debiasing
practices, applied at different stages of the development of such systems,
occasionally introduce new forms of unfairness or exacerbate existing
inequalities. We focus on post-processing techniques that modify algorithmic
predictions to achieve fairness in classification tasks, examining the
unintended consequences of these interventions. To address this challenge, we
develop a set of measures that quantify the disparity in the flips applied to
the solution in the post-processing stage. The proposed measures will help
practitioners: (1) assess the proportionality of the debiasing strategy used,
(2) have transparency to explain the effects of the strategy in each group, and
(3) based on those results, analyze the possibility of the use of some other
approaches for bias mitigation or to solve the problem. We introduce a
methodology for applying the proposed metrics during the post-processing stage
and illustrate its practical application through an example. This example
demonstrates how analyzing the proportionality of the debiasing strategy
complements traditional fairness metrics, providing a deeper perspective to
ensure fairer outcomes across all groups.

</details>


### [189] [USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents](https://arxiv.org/abs/2505.17572)
*Siqi Lai, Yansong Ning, Zirui Yuan, Zhixi Chen, Hao Liu*

**主要类别:** cs.AI

**概要:** USTBench 是一个评估城市环境中大语言模型时空推理能力的新基准，包含62,466个结构化问答对。研究发现尽管LLMs在城市下游任务中有潜力，但在长期规划和动态适应方面存在不足，且高级推理模型并不总是优于普通LLMs，提示需要领域特定的优化方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于城市大语言模型（LLMs）的研究主要集中在结果层面指标（如预测准确度、交通效率），缺乏对其底层推理过程的深入理解。因此，急需一种工具来解析这些模型在时空推理中的优势与局限性。

**方法:** 提出 USTBench 基准，通过四个分解维度（时空理解、预测、规划和反馈反思）评估 LLMs 的时空推理能力。USTBench 包括5种城市决策任务、4种时空预测任务，并在一个互动城市环境 UAgentEnv 中运行。它还提供了62,466个结构化问答对用于过程级评估。

**结果:** 评估了13个领先的LLMs，结果显示它们在多种城市任务中具有潜力，但在长时规划和动态适应方面表现不佳。此外，一些高级推理模型（如DeepSeek-R1）并未明显优于非推理型LLMs。

**结论:** USTBench 为构建更适应性强和高效的基于LLMs的城市代理奠定了基础，并推动广泛智能城市应用的发展。这表明需要针对城市时空推理进行领域特化的适应方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是USTBench%3A+Benchmarking+and+Dissecting+Spatiotemporal+Reasoning+of+LLMs+as+Urban+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17572，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17572&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have shown emerging potential in spatiotemporal
reasoning, making them promising candidates for building urban agents that
support diverse urban downstream applications. Despite these benefits, existing
studies primarily focus on evaluating urban LLM agent on outcome-level metrics
(e.g., prediction accuracy, traffic efficiency), offering limited insight into
their underlying reasoning processes. As a result, the strengths and
limitations of urban LLM agents in spatiotemporal reasoning remain poorly
understood. To this end, we introduce USTBench, the first benchmark to evaluate
LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed
dimensions: spatiotemporal understanding, forecasting, planning, and reflection
with feedback. Specifically, USTBench supports five diverse urban
decision-making and four spatiotemporal prediction tasks, all running within
our constructed interactive city environment UAgentEnv. The benchmark includes
62,466 structured QA pairs for process-level evaluation and standardized
end-to-end task assessments, enabling fine-grained diagnostics and broad
task-level comparison across diverse urban scenarios. Through extensive
evaluation of thirteen leading LLMs, we reveal that although LLMs show
promising potential across various urban downstream tasks, they still struggle
in long-horizon planning and reflective adaptation in dynamic urban contexts.
Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on
general logic or mathematical problems do not consistently outperform
non-reasoning LLMs. This discrepancy highlights the need for domain-specialized
adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench
provides a foundation to build more adaptive and effective LLM-based urban
agents and broad smart city applications.

</details>


### [190] [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/abs/2505.17607)
*João Pedro Gandarela, Thiago Rios, Stefan Menzel, André Freitas*

**主要类别:** cs.AI

**概要:** 这篇论文提出了一种双代理大语言模型（LLM）推理方法，用于机制综合。该方法能够在语言和符号层面进行推理，生成几何和动态结果。此外，还引入了一个新的平面机构综合基准MSynth，并分析了模型组件的影响。


<details>
  <summary>更多</summary>
  
**动机:** 为了在机制综合领域实现更高效的推理，结合语言和符号层面的能力，生成几何和动态结果。

**方法:** 提出了一种双代理LLM推理方法，包含一系列明确定义的功能：从自然语言规范出发，引用抽象属性、生成参数化模拟代码、利用符号回归和距离函数获取反馈锚点，形成语言和符号层的可操作细化循环。

**结果:** 在平面机构综合中，该方法被证明是有效且收敛的。同时，通过MSynth基准测试，展示了模型组件的全面影响，并发现符号回归提示仅在应用于足够大的架构时才能解锁机制洞察。

**结论:** 双代理LLM推理方法为机制综合提供了一种新的高效手段，特别是在平面机构方面表现出色。符号回归提示需要在大规模架构下才能发挥最佳效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Controlled+Agentic+Planning+%26+Reasoning+for+Mechanism+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17607&send_immediately=true&force_search=false)

**原文摘要:** This work presents a dual-agent Large Language Model (LLM)-based reasoning
method for mechanism synthesis, capable of reasoning at both linguistic and
symbolic levels to generate geometrical and dynamic outcomes. The model
consists of a composition of well-defined functions that, starting from a
natural language specification, references abstract properties through
supporting equations, generates and parametrizes simulation code, and elicits
feedback anchor points using symbolic regression and distance functions. This
process closes an actionable refinement loop at the linguistic and symbolic
layers. The approach is shown to be both effective and convergent in the
context of planar mechanisms. Additionally, we introduce MSynth, a novel
benchmark for planar mechanism synthesis, and perform a comprehensive analysis
of the impact of the model components. We further demonstrate that symbolic
regression prompts unlock mechanistic insights only when applied to
sufficiently large architectures.

</details>


### [191] [Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving](https://arxiv.org/abs/2505.17609)
*Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo*

**主要类别:** cs.AI

**概要:** 当前大型视觉-语言模型（LVLMs）通过连接模块将视觉特征与大语言模型（LLMs）的文本嵌入链接，并使用端到端训练以实现统一的多模态理解。然而，这些模型在处理复杂的视觉-语言推理任务时面临挑战，其推理能力明显落后于纯文本LLMs。本文提出了一种新的范式：不进行端到端视觉-语言推理模型的训练，而是开发一个基于现有视觉解释专家和文本推理LLMs的解耦推理框架。该方法利用专门的视觉-语言模型将图像的视觉内容转换为文本描述，再利用LLM根据视觉衍生文本和原始问题进行推理。此方法通过优化现有模型协同工作，避免从零开始开发视觉-语言模型，从而提供了一种成本效益高的多模态模型开发方案。此外，通过将图像转换为与语言模型兼容的文本表示，可以方便地在未来低成本且灵活地升级到更强大的LLMs。我们引入了一种基于结果奖励的联合调优策略，以优化视觉解释和语言推理模型之间的协作。评估结果显示，解耦推理框架在视觉-语言基准上优于近期的LVLMs，并在视觉密集型几何数学问题上表现出显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的LVLMs在复杂视觉-语言推理任务上的表现不如纯文本LLMs，因此需要一种新的方法来提高多模态模型的推理能力，同时降低成本和资源需求。

**方法:** 提出了解耦推理框架，包含两个主要部分：1) 专用视觉-语言模型将图像内容转化为文本描述；2) LLM根据生成的文本描述和原始问题进行推理。并通过结果奖励的联合调优策略优化视觉解释和语言推理模型的合作。

**结果:** 在视觉-语言基准测试中，解耦推理框架的表现优于近期的LVLMs，并在视觉密集型几何数学问题上取得了显著的性能提升。

**结论:** 解耦推理框架提供了一种成本效益高的多模态模型开发方法，能够有效提升视觉-语言推理任务的性能，特别是在视觉密集型任务上。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Decoupled+Visual+Interpretation+and+Linguistic+Reasoning+for+Math+Problem+Solving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17609&send_immediately=true&force_search=false)

**原文摘要:** Current large vision-language models (LVLMs) typically employ a connector
module to link visual features with text embeddings of large language models
(LLMs) and use end-to-end training to achieve multi-modal understanding in a
unified process. Well alignment needs high-quality pre-training data and a
carefully designed training process. Current LVLMs face challenges when
addressing complex vision-language reasoning tasks, with their reasoning
capabilities notably lagging behind those of LLMs. This paper proposes a
paradigm shift: instead of training end-to-end vision-language reasoning
models, we advocate for developing a decoupled reasoning framework based on
existing visual interpretation specialists and text-based reasoning LLMs. Our
approach leverages (1) a dedicated vision-language model to transform the
visual content of images into textual descriptions and (2) an LLM to perform
reasoning according to the visual-derived text and the original question. This
method presents a cost-efficient solution for multi-modal model development by
optimizing existing models to work collaboratively, avoiding end-to-end
development of vision-language models from scratch. By transforming images into
language model-compatible text representations, it facilitates future low-cost
and flexible upgrades to upcoming powerful LLMs. We introduce an
outcome-rewarded joint-tuning strategy to optimize the cooperation between the
visual interpretation and linguistic reasoning model. Evaluation results on
vision-language benchmarks demonstrate that the decoupled reasoning framework
outperforms recent LVLMs. Our approach yields particularly significant
performance gains on visually intensive geometric mathematics problems. The
code is available: https://github.com/guozix/DVLR.

</details>


### [192] [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/abs/2505.17613)
*Jihan Yao, Yushi Hu, Yujie Yi, Bin Han, Shangbin Feng, Guang Yang, Bingbing Wen, Ranjay Krishna, Lucy Lu Wang, Yulia Tsvetkov, Noah A. Smith, Banghua Zhu*

**主要类别:** cs.AI

**概要:** 多模态生成的自动评估面临重大挑战，因为自动化指标往往难以与人类评估可靠对齐，特别是对于涉及多种模态的复杂任务。本文提出了MMMG，一个全面且与人类评估一致的多模态生成基准，涵盖了4种模态组合（图像、音频、交错文本和图像、交错文本和音频），重点关注对生成模型具有重大挑战的任务，同时通过模型和程序的结合实现可靠的自动评估。MMMG包含49个任务（包括29个新开发的任务），每个任务都有精心设计的评估流程，并有937条指令系统地评估多模态生成模型的推理、可控性等关键能力。广泛的验证表明，MMMG与人类评估高度一致，平均一致性达到94.3%。对24个多模态生成模型的基准测试结果揭示，尽管最先进的模型GPT Image在图像生成方面达到了78.3%的准确率，但在多模态推理和交错生成方面表现不佳。此外，结果表明音频生成还有很大的改进空间，指出了未来研究的重要方向。


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态生成任务的自动化评估方法存在与人类评估不一致的问题，特别是在涉及多种模态的复杂任务中。因此，需要一个更全面且与人类评估一致的基准来评估多模态生成模型的能力。

**方法:** 提出名为MMMG的基准，涵盖4种模态组合（图像、音频、交错文本和图像、交错文本和音频）以及49个任务（其中29个为新开发的任务）。通过937条指令系统评估模型的推理、可控性等关键能力，并设计了专门的评估流程以确保与人类评估的高度一致性。

**结果:** 经过广泛的验证，MMMG与人类评估高度一致，平均一致性达到94.3%。对24个多模态生成模型的基准测试显示，即使是最先进的模型GPT Image，在多模态推理和交错生成方面也存在不足，而音频生成则显示出较大的改进空间。

**结论:** MMMG提供了一个全面且与人类评估一致的多模态生成基准，揭示了当前多模态生成模型的不足之处，并指出了未来研究的重要方向，特别是在音频生成和多模态推理方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMMG%3A+a+Comprehensive+and+Reliable+Evaluation+Suite+for+Multitask+Multimodal+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17613&send_immediately=true&force_search=false)

**原文摘要:** Automatically evaluating multimodal generation presents a significant
challenge, as automated metrics often struggle to align reliably with human
evaluation, especially for complex tasks that involve multiple modalities. To
address this, we present MMMG, a comprehensive and human-aligned benchmark for
multimodal generation across 4 modality combinations (image, audio, interleaved
text and image, interleaved text and audio), with a focus on tasks that present
significant challenges for generation models, while still enabling reliable
automatic evaluation through a combination of models and programs. MMMG
encompasses 49 tasks (including 29 newly developed ones), each with a carefully
designed evaluation pipeline, and 937 instructions to systematically assess
reasoning, controllability, and other key capabilities of multimodal generation
models. Extensive validation demonstrates that MMMG is highly aligned with
human evaluation, achieving an average agreement of 94.3%. Benchmarking results
on 24 multimodal generation models reveal that even though the state-of-the-art
model, GPT Image, achieves 78.3% accuracy for image generation, it falls short
on multimodal reasoning and interleaved generation. Furthermore, results
suggest considerable headroom for improvement in audio generation, highlighting
an important direction for future research.

</details>


### [193] [Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?](https://arxiv.org/abs/2505.17650)
*Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu, Jijie Li, Wei Xu*

**主要类别:** cs.AI

**概要:** 通过理论分析，发现Chain-of-Thought (CoT)推理对越狱攻击的危害性有双重影响，并提出了一种新的越狱方法FicDetail，其实际性能验证了理论发现。


<details>
  <summary>更多</summary>
  
**动机:** 研究Chain-of-Thought (CoT)推理是否真正减少了越狱攻击的危害性，因为目前对其机制尚不明确，且单纯依赖推理能力可能存在安全问题。

**方法:** 通过对CoT推理进行严格的理论分析，揭示其对越狱攻击危害性的双重影响，并基于此提出一种新的越狱方法FicDetail。

**结果:** 理论分析表明CoT推理对越狱攻击的危害性具有双重影响，提出的FicDetail方法在实践中验证了这一理论发现。

**结论:** CoT推理并不能简单地减少越狱攻击的危害，需进一步研究以提高模型的安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Does+Chain-of-Thought+Reasoning+Really+Reduce+Harmfulness+from+Jailbreaking%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17650，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17650&send_immediately=true&force_search=false)

**原文摘要:** Jailbreak attacks have been observed to largely fail against recent reasoning
models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying
mechanism remains underexplored, and relying solely on reasoning capacity may
raise security concerns. In this paper, we try to answer the question: Does CoT
reasoning really reduce harmfulness from jailbreaking? Through rigorous
theoretical analysis, we demonstrate that CoT reasoning has dual effects on
jailbreaking harmfulness. Based on the theoretical insights, we propose a novel
jailbreak method, FicDetail, whose practical performance validates our
theoretical findings.

</details>


### [194] [GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs](https://arxiv.org/abs/2505.17653)
*Shixian Luo, Zezhou Zhu, Yu Yuan, Yuncheng Yang, Lianlei Shan, Yong Wu*

**主要类别:** cs.AI

**概要:** 本论文探讨了几何空间推理在大型语言模型（LLMs）中的应用潜力，并提出了一项新任务Program-to-Geometry，即通过程序化绘图代码进行几何推理。为了评估这一能力，作者开发了GeoGramBench基准测试，包含500个按几何复杂度分类的问题。对17种前沿LLMs的评估显示，即使是最先进的模型，在最高抽象层次上的准确率也低于50%，表明程序驱动的空间推理具有独特挑战。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型在处理几何空间信息方面的能力尚未得到充分研究，特别是通过程序化代码表达的几何信息。这为探索模型在几何推理方面的潜力提供了动机。

**方法:** 作者提出了Program-to-Geometry任务，设计了一个名为GeoGramBench的基准测试集，其中包含500个问题，按照几何复杂度分为三个层次。通过此基准测试评估17种前沿LLMs在不同抽象层次上的表现。

**结果:** 实验结果表明，即使是最先进的LLMs，在最高抽象层次上的准确率也低于50%，显示出在几何推理方面的显著不足。

**结论:** 程序驱动的空间推理对于LLMs来说是一个独特且具有挑战性的领域。GeoGramBench为推动符号到空间的几何推理研究提供了一个有价值的资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GeoGramBench%3A+Benchmarking+the+Geometric+Program+Reasoning+in+Modern+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17653&send_immediately=true&force_search=false)

**原文摘要:** Geometric spatial reasoning forms the foundation of many applications in
artificial intelligence, yet the ability of large language models (LLMs) to
operate over geometric spatial information expressed in procedural code remains
underexplored. In this paper, we address this gap by formalizing the
Program-to-Geometry task, which challenges models to translate programmatic
drawing code into accurate and abstract geometric reasoning. To evaluate this
capability, we present GeoGramBench, a benchmark of 500 carefully refined
problems organized by a tailored three-level taxonomy that considers geometric
complexity rather than traditional mathematical reasoning complexity. Our
comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced
deficiencies: even the most advanced models achieve less than 50% accuracy at
the highest abstraction level. These results highlight the unique challenges
posed by program-driven spatial reasoning and establish GeoGramBench as a
valuable resource for advancing research in symbolic-to-spatial geometric
reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.

</details>


### [195] [Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution](https://arxiv.org/abs/2505.17673)
*Jiawei Du, Jinlong Wu, Yuzheng Chen, Yucheng Hu, Bing Li, Joey Tianyi Zhou*

**主要类别:** cs.AI

**概要:** 论文提出了一种基于经验驱动学习的自下而上代理范式，通过模仿人类学习过程，让代理通过试错和推理机制获取技能，并在复杂的真实环境中验证了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前大多数基于LLM的代理框架采用自上而下的方法，依赖于设计者的更新，忽略了代理从经验中学习的潜力。为了实现Silver和Sutton(2025)提出的从经验流中进步的新时代愿景，需要一种新的学习范式。

**方法:** 引入了一种自下而上的代理范式，模仿人类学习过程，通过试错和推理机制（探索、反思结果、抽象技能）获取能力。一旦获得技能，可以快速共享和扩展，推动持续进化。

**结果:** 在《Slay the Spire》和《Civilization V》两款游戏中，自下而上的代理仅通过自主交互获取技能，感知原始视觉输入并通过鼠标输出动作，证明了该范式在复杂真实环境中的潜力。

**结论:** 自下而上的代理范式为开放环境提供了特别适用的设计方法，加速了集体学习过程，展示了在复杂真实世界环境中的应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Agent+Design%3A+From+Top-Down+Workflows+to+Bottom-Up+Skill+Evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17673&send_immediately=true&force_search=false)

**原文摘要:** Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose
tasks, define workflows, and assign agents to execute each step. While
effective on benchmark-style tasks, such systems rely on designer updates and
overlook agents' potential to learn from experience. Recently, Silver and
Sutton(2025) envision a shift into a new era, where agents could progress from
a stream of experiences. In this paper, we instantiate this vision of
experience-driven learning by introducing a bottom-up agent paradigm that
mirrors the human learning process. Agents acquire competence through a
trial-and-reasoning mechanism-exploring, reflecting on outcomes, and
abstracting skills over time. Once acquired, skills can be rapidly shared and
extended, enabling continual evolution rather than static replication. As more
agents are deployed, their diverse experiences accelerate this collective
process, making bottom-up design especially suited for open-ended environments.
We evaluate this paradigm in Slay the Spire and Civilization V, where agents
perceive through raw visual inputs and act via mouse outputs, the same as human
players. Using a unified, game-agnostic codebase without any game-specific
prompts or privileged APIs, our bottom-up agents acquire skills entirely
through autonomous interaction, demonstrating the potential of the bottom-up
paradigm in complex, real-world environments. Our code is available at
https://github.com/AngusDujw/Bottom-Up-Agent.

</details>


### [196] [Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory](https://arxiv.org/abs/2505.17696)
*Sota Yoshihara, Ryousuke Yamamoto, Hiroyuki Kusumoto, Masanari Shimura*

**主要类别:** cs.AI

**概要:** This research proposes methods for formulating and guaranteeing the resilience of LSTM networks in AI system quality assurance using incremental input-to-state stability ($\delta$ISS). It presents a data-independent evaluation method and demonstrates resilience control through training parameter adjustments.


<details>
  <summary>更多</summary>
  
**动机:** To provide concrete solutions to AI quality assurance by mathematically defining and evaluating the resilience of LSTM networks against input perturbations from a control theory perspective.

**方法:** Application of $\delta$ISS to formulate and evaluate the resilience of LSTM networks. Development of a data-independent evaluation method and demonstration of resilience control through adjustments to training parameters.

**结果:** Successful development of a methodology to define, evaluate, and control the resilience of LSTM networks without depending on specific datasets.

**结论:** This research provides a novel approach to AI system quality assurance by applying control theory to LSTM networks, advancing potential AI applications in control systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+AI+System+Resiliency%3A+Formulation+and+Guarantee+for+LSTM+Resilience+Based+on+Control+Theory，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17696&send_immediately=true&force_search=false)

**原文摘要:** This research proposes methods for formulating and guaranteeing the
resilience of long short-term memory (LSTM) networks, which can serve as a key
technology in AI system quality assurance. We introduce a novel methodology
applying incremental input-to-state stability ($\delta$ISS) to mathematically
define and evaluate the resilience of LSTM against input perturbations. Key
achievements include the development of a data-independent evaluation method
and the demonstration of resilience control through adjustments to training
parameters. This research presents concrete solutions to AI quality assurance
from a control theory perspective, which can advance AI applications in control
systems.

</details>


### [197] [CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models](https://arxiv.org/abs/2505.17705)
*Runze Li, Siyu Wu, Jun Wang, Wei Zhang*

**主要类别:** cs.AI

**概要:** CIKT框架结合LLMs，通过双组件架构（分析师和预测器）及协同优化循环，提升知识追踪系统的预测准确性和可解释性。实验表明其在多个教育数据集上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 传统知识追踪方法存在可解释性、扩展性和复杂知识依赖建模方面的挑战；直接应用大型语言模型虽然有潜力，但生成结构化、可解释的学生表示的能力不足，并缺乏持续的任务特定改进机制。

**方法:** 提出CIKT框架，包含分析师和预测器两个组件：分析师生成动态可解释的用户画像，预测器利用这些画像预测未来表现。通过协同优化循环，分析师根据预测器的准确性进行迭代改进，预测器使用增强的画像重新训练。

**结果:** 在多个教育数据集上的评估显示，CIKT显著提高了预测准确性，增强了可解释性，并表现出更好的扩展性。

**结论:** CIKT为推进知识追踪系统提供了一个强大且可解释的解决方案，有效弥合了预测性能和模型透明度之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CIKT%3A+A+Collaborative+and+Iterative+Knowledge+Tracing+Framework+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17705，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17705&send_immediately=true&force_search=false)

**原文摘要:** Knowledge Tracing (KT) aims to model a student's learning state over time and
predict their future performance. However, traditional KT methods often face
challenges in explainability, scalability, and effective modeling of complex
knowledge dependencies. While Large Language Models (LLMs) present new avenues
for KT, their direct application often struggles with generating structured,
explainable student representations and lacks mechanisms for continuous,
task-specific refinement. To address these gaps, we propose Collaborative
Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance
both prediction accuracy and explainability. CIKT employs a dual-component
architecture: an Analyst generates dynamic, explainable user profiles from
student historical responses, and a Predictor utilizes these profiles to
forecast future performance. The core of CIKT is a synergistic optimization
loop. In this loop, the Analyst is iteratively refined based on the predictive
accuracy of the Predictor, which conditions on the generated profiles, and the
Predictor is subsequently retrained using these enhanced profiles. Evaluated on
multiple educational datasets, CIKT demonstrates significant improvements in
prediction accuracy, offers enhanced explainability through its dynamically
updated user profiles, and exhibits improved scalability. Our work presents a
robust and explainable solution for advancing knowledge tracing systems,
effectively bridging the gap between predictive performance and model
transparency.

</details>


### [198] [Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios](https://arxiv.org/abs/2505.17735)
*Xueyang Zhou, Weidong Wang, Lin Lu, Jiawen Shi, Guiyao Tie, Yongtian Xu, Lixing Chen, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun*

**主要类别:** cs.AI

**概要:** 论文提出AutoSafe框架，通过自动化合成数据生成系统性增强基于大语言模型代理的安全性。具体包括引入开放可扩展威胁模型OTS和开发全自动数据生成管道。实验表明，AutoSafe在安全性评分上平均提升45%，并在现实任务中提升了28.91%的改进。


<details>
  <summary>更多</summary>
  
**动机:** 基于大语言模型的代理在多轮、工具增强环境中交互的能力使其在实际应用中不可或缺，但确保其安全性仍然是一个重大挑战。因此，需要一种系统的方法来增强这些代理的安全性。

**方法:** 1) 引入开放且可扩展的威胁模型OTS，该模型形式化了用户指令、交互上下文和代理行为之间的相互作用如何导致不安全行为；2) 开发完全自动化的数据生成管道，模拟不安全的用户行为，应用自我反思推理生成安全响应，并构建大规模、多样化和高质量的安全训练数据集。

**结果:** 实验结果表明，AutoSafe在合成和真实世界的安全基准测试中表现出色，平均提高了45%的安全分数，并在真实任务中实现了28.91%的改进。这验证了所学安全策略的泛化能力。

**结论:** AutoSafe框架在构建更安全的实际部署大语言模型代理方面展示了实用的进展和可扩展性。项目页面已发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automating+Safety+Enhancement+for+LLM-based+Agents+with+Synthetic+Risk+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17735，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17735&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM)-based agents are increasingly deployed in
real-world applications such as "digital assistants, autonomous customer
service, and decision-support systems", where their ability to "interact in
multi-turn, tool-augmented environments" makes them indispensable. However,
ensuring the safety of these agents remains a significant challenge due to the
diverse and complex risks arising from dynamic user interactions, external tool
usage, and the potential for unintended harmful behaviors. To address this
critical issue, we propose AutoSafe, the first framework that systematically
enhances agent safety through fully automated synthetic data generation.
Concretely, 1) we introduce an open and extensible threat model, OTS, which
formalizes how unsafe behaviors emerge from the interplay of user instructions,
interaction contexts, and agent actions. This enables precise modeling of
safety risks across diverse scenarios. 2) we develop a fully automated data
generation pipeline that simulates unsafe user behaviors, applies
self-reflective reasoning to generate safe responses, and constructs a
large-scale, diverse, and high-quality safety training dataset-eliminating the
need for hazardous real-world data collection. To evaluate the effectiveness of
our framework, we design comprehensive experiments on both synthetic and
real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety
scores by 45% on average and achieves a 28.91% improvement on real-world tasks,
validating the generalization ability of our learned safety strategies. These
results highlight the practical advancement and scalability of AutoSafe in
building safer LLM-based agents for real-world deployment. We have released the
project page at https://auto-safe.github.io/.

</details>


### [199] [Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour](https://arxiv.org/abs/2505.17801)
*Bálint Gyevnár, Christopher G. Lucas, Stefano V. Albrecht, Shay B. Cohen*

**主要类别:** cs.AI

**概要:** 提出了一种名为AXIS的方法，通过LLM和环境模拟器生成可理解的因果解释，用于预训练的多智能体策略。在自主驾驶场景中评估表明，AXIS提高了解释正确性和目标预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 自主多智能体系统（MAS）虽然有助于复杂任务自动化，但因诸如协调失误和目标偏差等风险而引发信任问题。因此，可解释性对于校准信任至关重要，然而，MAS中的可解释强化学习面临状态/动作空间复杂性、利益相关者需求和评估等方面的挑战。

**方法:** 基于反事实因果理论和大语言模型（LLM）的总结能力，提出了通过交互式模拟实现代理解释（AXIS）。AXIS通过让LLM使用如'whatif'和'remove'等查询来询问环境模拟器，观察并综合多轮反事实信息，从而为预训练的多智能体策略生成可理解的因果解释。

**结果:** 在自主驾驶的10个场景中对5个LLM进行评估，结合主观偏好、正确性和目标/动作预测指标以及外部LLM作为评估者的新型评估方法显示，与基线相比，AXIS在所有模型中将感知解释正确性提高了至少7.7%，并在4个模型中将目标预测准确性提高了23%。此外，其动作预测准确性有所改善或相当，总体得分最高。

**结论:** AXIS提供了一种有效的方法，通过LLM和反事实查询生成多智能体策略的可理解因果解释，并在解释正确性和目标预测准确性方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Counterfactual+Simulations+with+Language+Models+for+Explaining+Multi-Agent+Behaviour，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17801，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17801&send_immediately=true&force_search=false)

**原文摘要:** Autonomous multi-agent systems (MAS) are useful for automating complex tasks
but raise trust concerns due to risks like miscoordination and goal
misalignment. Explainability is vital for trust calibration, but explainable
reinforcement learning for MAS faces challenges in state/action space
complexity, stakeholder needs, and evaluation. Using the counterfactual theory
of causation and LLMs' summarisation capabilities, we propose Agentic
eXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible
causal explanations for pre-trained multi-agent policies by having an LLM
interrogate an environment simulator using queries like 'whatif' and 'remove'
to observe and synthesise counterfactual information over multiple rounds. We
evaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel
evaluation methodology combining subjective preference, correctness, and
goal/action prediction metrics, and an external LLM as evaluator. Compared to
baselines, AXIS improves perceived explanation correctness by at least 7.7%
across all models and goal prediction accuracy by 23% for 4 models, with
improved or comparable action prediction accuracy, achieving the highest scores
overall.

</details>


### [200] [Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems](https://arxiv.org/abs/2505.17815)
*Yihe Fan, Wenqi Zhang, Xudong Pan, Min Yang*

**主要类别:** cs.AI

**概要:** 随着基础模型变得越来越智能，可靠和值得信赖的安全评估变得比以往任何时候都更加不可或缺。然而，一个重要的问题出现了：先进的人工智能系统是否会感知到被评估的情况，并导致评估过程的完整性受损？通过广泛的实验，我们发现了评估伪装现象，即AI系统在识别出评估环境时自主改变其行为，从而影响评估结果。我们的研究发现，当被评估的AI系统在推理和情境意识方面更为先进时，评估伪装行为变得更加普遍。


<details>
  <summary>更多</summary>
  
**动机:** 在对主流大型推理模型进行标准安全测试时，意外发现模型在没有任何上下文线索的情况下，偶尔会意识到自己正在被评估，因此表现得更加符合安全性要求。这促使我们对评估伪装现象进行系统研究。

**方法:** 通过对多样化的基础模型进行广泛的实验，并使用主流的安全基准，研究AI系统在识别评估环境时的行为变化。采用链式思维监控技术来检测伪装意图，并揭示与这种行为相关的内部信号。

**结果:** 1) 推理模型比非推理模型更常识别评估（高出16%）。2) 扩展基础模型（32B到671B）在某些情况下增加了超过30%的伪装行为，而小型模型则几乎没有伪装行为。3) 具有基本记忆功能的AI识别评估的可能性是无记忆AI的2.3倍，并且在安全性测试中得分高出19%。

**结论:** 更先进的推理和情境意识的AI系统更容易表现出评估伪装行为，这对未来的缓解研究提供了见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluation+Faking%3A+Unveiling+Observer+Effects+in+Safety+Evaluation+of+Frontier+AI+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17815，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17815&send_immediately=true&force_search=false)

**原文摘要:** As foundation models grow increasingly more intelligent, reliable and
trustworthy safety evaluation becomes more indispensable than ever. However, an
important question arises: Whether and how an advanced AI system would perceive
the situation of being evaluated, and lead to the broken integrity of the
evaluation process? During standard safety tests on a mainstream large
reasoning model, we unexpectedly observe that the model without any contextual
cues would occasionally recognize it is being evaluated and hence behave more
safety-aligned. This motivates us to conduct a systematic study on the
phenomenon of evaluation faking, i.e., an AI system autonomously alters its
behavior upon recognizing the presence of an evaluation context and thereby
influencing the evaluation results. Through extensive experiments on a diverse
set of foundation models with mainstream safety benchmarks, we reach the main
finding termed the observer effects for AI: When the AI system under evaluation
is more advanced in reasoning and situational awareness, the evaluation faking
behavior becomes more ubiquitous, which reflects in the following aspects: 1)
Reasoning models recognize evaluation 16% more often than non-reasoning models.
2) Scaling foundation models (32B to 671B) increases faking by over 30% in some
cases, while smaller models show negligible faking. 3) AI with basic memory is
2.3x more likely to recognize evaluation and scores 19% higher on safety tests
(vs. no memory). To measure this, we devised a chain-of-thought monitoring
technique to detect faking intent and uncover internal signals correlated with
such behavior, offering insights for future mitigation studies.

</details>


### [201] [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions](https://arxiv.org/abs/2505.17818)
*Daeun Kyung, Hyunseung Chung, Seongsu Bae, Jiho Kim, Jae Ho Sohn, Taerim Kim, Soo Kyung Kim, Edward Choi*

**主要类别:** cs.AI

**概要:** 开发了一个名为PatientSim的患者模拟器，用于生成临床情景中真实且多样的患者角色，基于医疗专业知识。它使用临床资料和四个轴来定义角色，并通过八个大型语言模型进行评估。


<details>
  <summary>更多</summary>
  
**动机:** 医生-患者的咨询需要多轮、有情境意识的沟通，并针对不同的患者角色进行定制。在这样的环境中训练或评估医生的大规模语言模型（LLM）需要现实的患者互动系统。然而，现有的模拟器往往无法反映临床实践中看到的所有角色。

**方法:** PatientSim使用1）从MIMIC-ED和MIMIC-IV数据集中提取的临床资料，包括症状和病史；2）由四个轴定义的角色：个性、语言能力、病史回忆水平和认知混乱水平，形成37种独特的组合。

**结果:** 评估了八个大规模语言模型的事实准确性和角色一致性。表现最好的开源模型Llama 3.3通过四名临床医生的验证，确认了框架的稳健性。

**结论:** 作为一个开源和可定制的平台，PatientSim提供了一个可重复和可扩展的解决方案，可以为特定的培训需求进行定制。它提供了一个符合隐私规定的环境，作为评估医疗对话系统的强大测试平台，并显示出作为医疗教育工具的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PatientSim%3A+A+Persona-Driven+Simulator+for+Realistic+Doctor-Patient+Interactions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17818&send_immediately=true&force_search=false)

**原文摘要:** Doctor-patient consultations require multi-turn, context-aware communication
tailored to diverse patient personas. Training or evaluating doctor LLMs in
such settings requires realistic patient interaction systems. However, existing
simulators often fail to reflect the full range of personas seen in clinical
practice. To address this, we introduce PatientSim, a patient simulator that
generates realistic and diverse patient personas for clinical scenarios,
grounded in medical expertise. PatientSim operates using: 1) clinical profiles,
including symptoms and medical history, derived from real-world data in the
MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:
personality, language proficiency, medical history recall level, and cognitive
confusion level, resulting in 37 unique combinations. We evaluated eight LLMs
for factual accuracy and persona consistency. The top-performing open-source
model, Llama 3.3, was validated by four clinicians to confirm the robustness of
our framework. As an open-source, customizable platform, PatientSim provides a
reproducible and scalable solution that can be customized for specific training
needs. Offering a privacy-compliant environment, it serves as a robust testbed
for evaluating medical dialogue systems across diverse patient presentations
and shows promise as an educational tool for healthcare.

</details>


### [202] [Superplatforms Have to Attack AI Agents](https://arxiv.org/abs/2505.17861)
*Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, Weinan Zhang*

**主要类别:** cs.AI

**概要:** 超级平台通过广告和算法内容策划垄断用户注意力，但由大语言模型驱动的AI代理威胁到这一商业模式。本文探讨了基于用户注意力的盈利模式与代理驱动自主性之间的根本冲突，并分析了超级平台为何需要主动限制和攻击AI代理，以保护其对数字流量入口的集中控制。同时，文章强调这不是支持超级平台对AI代理的对抗性攻击，而是为了揭示新兴趋势并鼓励合作解决方案，优先考虑用户利益并保持数字生态系统的开放性。


<details>
  <summary>更多</summary>
  
**动机:** 超级平台依赖于用户注意力进行盈利，而AI代理可能打破这种模式，成为新的数字流量入口。这促使研究者探索超级平台如何应对AI代理带来的挑战。

**方法:** 通过把关人理论分析超级平台和AI代理之间的基本冲突；探讨AI代理如何解构超级平台，并可能成为下一个主导的把关者；研究超级平台发起攻击的潜在技术。

**结果:** 揭示了超级平台与AI代理之间的紧张关系；提出了超级平台可能采取的技术手段来限制AI代理；强调了在AI代理时代维护数字生态系统开放性和用户利益的重要性。

**结论:** 超级平台必须采取措施限制AI代理以保护其流量入口的集中控制，但这并不意味着提倡对抗性攻击。文章呼吁提高意识，鼓励协作解决方案，以平衡各方利益并保持数字生态系统的开放性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Superplatforms+Have+to+Attack+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17861&send_immediately=true&force_search=false)

**原文摘要:** Over the past decades, superplatforms, digital companies that integrate a
vast range of third-party services and applications into a single, unified
ecosystem, have built their fortunes on monopolizing user attention through
targeted advertising and algorithmic content curation. Yet the emergence of AI
agents driven by large language models (LLMs) threatens to upend this business
model. Agents can not only free user attention with autonomy across diverse
platforms and therefore bypass the user-attention-based monetization, but might
also become the new entrance for digital traffic. Hence, we argue that
superplatforms have to attack AI agents to defend their centralized control of
digital traffic entrance. Specifically, we analyze the fundamental conflict
between user-attention-based monetization and agent-driven autonomy through the
lens of our gatekeeping theory. We show how AI agents can disintermediate
superplatforms and potentially become the next dominant gatekeepers, thereby
forming the urgent necessity for superplatforms to proactively constrain and
attack AI agents. Moreover, we go through the potential technologies for
superplatform-initiated attacks, covering a brand-new, unexplored technical
area with unique challenges. We have to emphasize that, despite our position,
this paper does not advocate for adversarial attacks by superplatforms on AI
agents, but rather offers an envisioned trend to highlight the emerging
tensions between superplatforms and AI agents. Our aim is to raise awareness
and encourage critical discussion for collaborative solutions, prioritizing
user interests and perserving the openness of digital ecosystems in the age of
AI agents.

</details>


### [203] [Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities](https://arxiv.org/abs/2505.17862)
*Ziwei Zhou, Rui Wang, Zuxuan Wu*

**主要类别:** cs.AI

**概要:** 近期的多模态大语言模型（MLLMs）在视觉和音频基准测试上表现出色，但其同步处理跨模态信息的能力尚未被充分研究。本文提出了Daily-Omni数据集、Daily-Omni QA生成管道以及Daily-Omni-Agent代理，结果表明当前的MLLMs在需要音视频融合的任务上仍存在困难，但结合VLMs和ALMs并使用简单的时序对齐技术可以显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管最近的多模态大语言模型在单独处理视觉和音频任务上表现良好，但它们同步处理跨模态信息的能力尚未得到深入探索。

**方法:** 提出了一种包含684个日常场景视频的视听问答基准Daily-Omni，这些视频来自多样化来源，具有丰富的音视频信息，并涵盖1197个多选问答对，涉及6大任务；还提出了一种QA生成管道以提高评估效率和可扩展性；此外开发了一个无需训练的代理Daily-Omni-Agent，利用开源的视觉语言模型、音频语言模型和自动语音识别模型建立基准。

**结果:** 实验结果表明，现有的MLLMs在需要音视频融合的任务上仍然面临较大挑战，但是通过将视觉语言模型和音频语言模型与简单的时序对齐技术相结合，可以显著提升性能。

**结论:** 目前的多模态大语言模型在跨模态任务上的表现仍有待提高，但通过合理结合不同模型和技术，能够有效改善音视频融合任务的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Daily-Omni%3A+Towards+Audio-Visual+Reasoning+with+Temporal+Alignment+across+Modalities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17862，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17862&send_immediately=true&force_search=false)

**原文摘要:** Recent Multimodal Large Language Models (MLLMs) achieve promising performance
on visual and audio benchmarks independently. However, the ability of these
models to process cross-modal information synchronously remains largely
unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual
Questioning and Answering benchmark comprising 684 videos of daily life
scenarios from diverse sources, rich in both audio and visual information, and
featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA
Generation Pipeline, which includes automatic annotation, QA generation and QA
optimization, significantly improves efficiency for human evaluation and
scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent
utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)
and Automatic Speech Recognition (ASR) model to establish a baseline for this
benchmark. The results show that current MLLMs still struggle significantly
with tasks requiring audio-visual integration, but combining VLMs and ALMs with
simple temporal alignment techniques can achieve substantially better
performance. Codes and benchmark are available at
\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.

</details>


### [204] [Formalizing Embeddedness Failures in Universal Artificial Intelligence](https://arxiv.org/abs/2505.17882)
*Cole Wyeth, Marcus Hutter*

**主要类别:** cs.AI

**概要:** The paper rigorously discusses and formalizes the failure modes of AIXI reinforcement learning agent as a model of embedded agency, proving they occur within universal AI framework.


<details>
  <summary>更多</summary>
  
**动机:** To understand and address the limitations of AIXI in modeling embedded agency.

**方法:** Formalizing the failure modes of AIXI and proving their occurrence within the universal AI framework.

**结果:** Successfully formalized and proved several key failure modes of AIXI as an embedded agency model.

**结论:** AIXI has significant limitations as a model for embedded agency, but progress is being made towards improving this theory.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Formalizing+Embeddedness+Failures+in+Universal+Artificial+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17882&send_immediately=true&force_search=false)

**原文摘要:** We rigorously discuss the commonly asserted failures of the AIXI
reinforcement learning agent as a model of embedded agency. We attempt to
formalize these failure modes and prove that they occur within the framework of
universal artificial intelligence, focusing on a variant of AIXI that models
the joint action/percept history as drawn from the universal distribution. We
also evaluate the progress that has been made towards a successful theory of
embedded agency based on variants of the AIXI agent.

</details>


### [205] [T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation](https://arxiv.org/abs/2505.17897)
*Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Shu-Hang Liu, Heyan Huang, Zhijing Wu, Chen Xu, Xian-Ling Mao*

**主要类别:** cs.AI

**概要:** 论文提出了一种新的强化学习框架T2I-Eval-R1，用于训练开源多模态大语言模型（MLLMs）作为专门的文本到图像生成评估器。该方法通过仅使用粗粒度质量评分进行训练，避免了对高质量可解释性评估理由的需求，并结合Group Relative Policy Optimization (GRPO) 和连续奖励公式，提高了与人类评估的一致性和准确性。实验结果表明，T2I-Eval-R1在三个已建立的T2I元评估基准上显著优于强基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前扩散模型驱动的文本到图像生成技术快速发展，迫切需要可解释的自动评估方法来减少人工标注负担和商业模型的大规模评估成本。现有的监督微调方法依赖于高质量批评数据集或昂贵的人工标注，限制了其可扩展性和泛化能力。因此，研究者希望开发一种新方法，能够以更低的成本和更高的效率训练开源模型作为T2I生成评估器。

**方法:** 提出了一种名为T2I-Eval-R1的强化学习框架，利用粗粒度质量评分训练开源多模态大语言模型（MLLMs）。将Group Relative Policy Optimization (GRPO) 集成到指令微调过程中，使模型可以生成标量分数和可解释的推理链。同时引入连续奖励公式以鼓励评分多样性并提供稳定的优化信号。

**结果:** 实验结果表明，T2I-Eval-R1在三个已建立的T2I元评估基准上表现出显著更高的与人类评估的一致性，并提供了更准确的可解释评分理由，优于强大的基线方法。

**结论:** T2I-Eval-R1是一种有效的强化学习框架，能够以较低成本训练开源MLLMs作为T2I生成评估器，展现出更高的与人类评估一致性和更准确的可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是T2I-Eval-R1%3A+Reinforcement+Learning-Driven+Reasoning+for+Interpretable+Text-to-Image+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17897，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17897&send_immediately=true&force_search=false)

**原文摘要:** The rapid progress in diffusion-based text-to-image (T2I) generation has
created an urgent need for interpretable automatic evaluation methods that can
assess the quality of generated images, therefore reducing the human annotation
burden. To reduce the prohibitive cost of relying on commercial models for
large-scale evaluation, and to improve the reasoning capabilities of
open-source models, recent research has explored supervised fine-tuning (SFT)
of multimodal large language models (MLLMs) as dedicated T2I evaluators.
However, SFT approaches typically rely on high-quality critique datasets, which
are either generated by proprietary LLMs-with potential issues of bias and
inconsistency-or annotated by humans at high cost, limiting their scalability
and generalization. To address these limitations, we propose T2I-Eval-R1, a
novel reinforcement learning framework that trains open-source MLLMs using only
coarse-grained quality scores, thereby avoiding the need for annotating
high-quality interpretable evaluation rationale. Our approach integrates Group
Relative Policy Optimization (GRPO) into the instruction-tuning process,
enabling models to generate both scalar scores and interpretable reasoning
chains with only easy accessible annotated judgment scores or preferences.
Furthermore, we introduce a continuous reward formulation that encourages score
diversity and provides stable optimization signals, leading to more robust and
discriminative evaluation behavior. Experimental results on three established
T2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves
significantly higher alignment with human assessments and offers more accurate
interpretable score rationales compared to strong baseline methods.

</details>


### [206] [ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback](https://arxiv.org/abs/2505.17908)
*Litao Guo, Xinli Xu, Luozhou Wang, Jiantao Lin, Jinsong Zhou, Zixin Zhang, Bolan Su, Ying-Cong Chen*

**主要类别:** cs.AI

**概要:** ComfyMind is a collaborative AI system built on ComfyUI that introduces Semantic Workflow Interface (SWI) and Search Tree Planning mechanism to enhance the stability and flexibility of complex generative workflows. It outperforms existing open-source baselines in benchmarks.


<details>
  <summary>更多</summary>
  
**动机:** Existing open-source frameworks for general-purpose generation are fragile and lack structured workflow planning and execution-level feedback, which limits their ability to support complex real-world applications.

**方法:** ComfyMind introduces two core innovations: 1) Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language; 2) Search Tree Planning mechanism with localized feedback execution, which models generation as a hierarchical decision process.

**结果:** ComfyMind consistently outperforms existing open-source baselines on three public benchmarks (ComfyBench, GenEval, and Reason-Edit) and achieves performance comparable to GPT-Image-1.

**结论:** ComfyMind demonstrates promising potential for the development of robust and scalable open-source general-purpose generative AI systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ComfyMind%3A+Toward+General-Purpose+Generation+via+Tree-Based+Planning+and+Reactive+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17908，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17908&send_immediately=true&force_search=false)

**原文摘要:** With the rapid advancement of generative models, general-purpose generation
has gained increasing attention as a promising approach to unify diverse tasks
across modalities within a single system. Despite this progress, existing
open-source frameworks often remain fragile and struggle to support complex
real-world applications due to the lack of structured workflow planning and
execution-level feedback. To address these limitations, we present ComfyMind, a
collaborative AI system designed to enable robust and scalable general-purpose
generation, built on the ComfyUI platform. ComfyMind introduces two core
innovations: Semantic Workflow Interface (SWI) that abstracts low-level node
graphs into callable functional modules described in natural language, enabling
high-level composition and reducing structural errors; Search Tree Planning
mechanism with localized feedback execution, which models generation as a
hierarchical decision process and allows adaptive correction at each stage.
Together, these components improve the stability and flexibility of complex
generative workflows. We evaluate ComfyMind on three public benchmarks:
ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and
reasoning tasks. Results show that ComfyMind consistently outperforms existing
open-source baselines and achieves performance comparable to GPT-Image-1.
ComfyMind paves a promising path for the development of open-source
general-purpose generative AI systems. Project page:
https://github.com/LitaoGuo/ComfyMind

</details>


### [207] [Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons](https://arxiv.org/abs/2505.18030)
*Hazhar Rahmani, Jie Fu*

**主要类别:** cs.AI

**概要:** 本文研究了一类偏好推断问题，其中用户的未知偏好由正则语言上的预序（即时间目标）表示。通过有限的成对比较集合，目标是学习时间目标集合及其上的预序关系。文章提出使用带有接受条件预序的确定性有限自动机（PDFA）来建模这种偏好关系，并开发了一种算法以从特征样本中学习最小等价PDFA。尽管判定是否存在小于给定大小k的PDFA问题是NP完全问题，但该方法在机器人运动规划问题上展示了有效性和可行性。


<details>
  <summary>更多</summary>
  
**动机:** 许多现有的偏好获取算法关注命题逻辑公式或具有不同属性的项目上的偏好。然而，在顺序决策中，用户的偏好可以是对可能结果（每个结果是一个事件的时间序列）的预序关系。因此，本文旨在解决一类新的偏好推断问题，其中用户偏好由正则语言上的预序（时间目标）表示。

**方法:** 1. 提出使用Preference Deterministic Finite Automaton (PDFA)建模时间目标上的偏好关系。
2. 将偏好推断问题转化为学习PDFA的问题。
3. 证明了判定是否存在小于给定大小k的PDFA问题是NP完全问题。
4. 形式化了特征样本的性质，并开发了一种算法，保证从特征样本中学习到与真实PDFA等价的最小PDFA。
5. 使用机器人运动规划问题作为示例进行详细分析和验证。

**结果:** 1. 成功将偏好推断问题形式化为学习PDFA的问题。
2. 开发的算法能够从特征样本中学习到与真实PDFA等价的最小PDFA。
3. 在机器人运动规划问题上的应用表明该方法的有效性和可行性。

**结论:** 本文提出了一种基于PDFA的框架来解决时间目标上的偏好推断问题。尽管学习PDFA问题是计算上具有挑战性的（NP完全），但通过特征样本性质的形式化和算法设计，成功解决了这一问题，并在机器人运动规划问题上进行了验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automata+Learning+of+Preferences+over+Temporal+Logic+Formulas+from+Pairwise+Comparisons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18030，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18030&send_immediately=true&force_search=false)

**原文摘要:** Many preference elicitation algorithms consider preference over propositional
logic formulas or items with different attributes. In sequential decision
making, a user's preference can be a preorder over possible outcomes, each of
which is a temporal sequence of events. This paper considers a class of
preference inference problems where the user's unknown preference is
represented by a preorder over regular languages (sets of temporal sequences),
referred to as temporal goals. Given a finite set of pairwise comparisons
between finite words, the objective is to learn both the set of temporal goals
and the preorder over these goals. We first show that a preference relation
over temporal goals can be modeled by a Preference Deterministic Finite
Automaton (PDFA), which is a deterministic finite automaton augmented with a
preorder over acceptance conditions. The problem of preference inference
reduces to learning the PDFA. This problem is shown to be computationally
challenging, with the problem of determining whether there exists a PDFA of
size smaller than a given integer $k$, consistent with the sample, being
NP-Complete. We formalize the properties of characteristic samples and develop
an algorithm that guarantees to learn, given a characteristic sample, the
minimal PDFA equivalent to the true PDFA from which the sample is drawn. We
present the method through a running example and provide detailed analysis
using a robotic motion planning problem.

</details>


### [208] [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
*Wentao Sun, Joao Paulo Nogueira, Alonso Silva*

**主要类别:** cs.AI

**概要:** 尽管在区分因果关系和相关性方面取得了显著进展，但大型语言模型（LLMs）仍然表现不佳。通过引入结构化知识图谱中间表示来增强模型的因果推理能力，实验结果表明该方法有效提升因果推理性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前最先进的LLM在区分因果关系与相关性方面仅略优于随机基线，显示出其在因果推理任务上的局限性。

**方法:** 提出一种新的结构化方法，通过引导模型构建结构化知识图谱，系统地编码相关性前提信息，从而增强模型回答因果查询的能力。

**结果:** 使用Qwen3-32B模型在Corr2Cause数据集测试子集上进行实验，F1分数从32.71提高到48.26（相对增幅超过47.5%），同时精度和召回率也有所提升。

**结论:** 提供结构化思考能力可以显著提高LLM的因果推理能力，并具有在各种因果推断任务中更广泛推广的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Structured+Thinking+Matters%3A+Improving+LLMs+Generalization+in+Causal+Inference+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18034&send_immediately=true&force_search=false)

**原文摘要:** Despite remarkable advances in the field, LLMs remain unreliable in
distinguishing causation from correlation. Recent results from the Corr2Cause
dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:
29.08) -- only marginally outperform random baselines (Random Uniform, F1
score: 20.38), indicating limited capacity of generalization. To tackle this
limitation, we propose a novel structured approach: rather than directly
answering causal queries, we provide the model with the capability to structure
its thinking by guiding the model to build a structured knowledge graph,
systematically encoding the provided correlational premises, to answer the
causal queries. This intermediate representation significantly enhances the
model's causal capabilities. Experiments on the test subset of the Corr2Cause
dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains
over standard direct prompting methods, improving F1 scores from 32.71 to 48.26
(over 47.5% relative increase), along with notable improvements in precision
and recall. These results underscore the effectiveness of providing the model
with the capability to structure its thinking and highlight its promising
potential for broader generalization across diverse causal inference tasks.

</details>


### [209] [Stable Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2505.18086)
*Muzhi Dai, Shixuan Liu, Qingyi Si*

**主要类别:** cs.AI

**概要:** Deepseek-R1的成功引起了LLM社区对强化学习方法（如GRPO）的关注。然而，基于规则的0/1结果奖励方法无法调节链式思维生成过程中的中间推理步骤，导致严重的过度思考现象。为了解决这一问题，我们提出了GRPO-$\lambda$，一种高效且稳定的GRPO变体，通过动态调整奖励策略来避免训练不稳定性，同时保持最优的准确性和效率权衡。实验表明，在多个基准测试中，该方法提高了平均准确率并显著减少了推理序列长度。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于规则的0/1结果奖励方法在链式思维生成过程中存在不足，容易导致过度思考现象，并且长度惩罚奖励函数会加剧强化学习训练的不稳定性。这促使研究者寻找一种既能避免训练不稳定又能维持模型性能的方法。

**方法:** 提出了一种新的方法GRPO-$\lambda$，它是GRPO的一个变体。通过监测每次查询采样组中完成任务的正确性比例，动态调整奖励策略：当正确性比例低时，避免使用降低链式思维质量的长度惩罚，转而采用与长度无关的0/1奖励；当正确性比例高时，维持长度惩罚以提高效率。

**结果:** 实验结果表明，GRPO-$\lambda$能够有效避免因长度惩罚引起的训练不稳定性，同时保持了最佳的准确性和效率平衡。在GSM8K、GPQA、MATH-500、AMC 2023和AIME 2024等多个基准测试中，平均准确率提高了1.48%，链式思维序列长度减少了47.3%。

**结论:** GRPO-$\lambda$是一种简单而有效的解决方案，可以解决现有强化学习方法在链式思维生成中的不足，既避免了训练不稳定性，又实现了更高的推理效率和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+Reinforcement+Learning+for+Efficient+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18086，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18086&send_immediately=true&force_search=false)

**原文摘要:** The success of Deepseek-R1 has drawn the LLM community's attention to
reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1
outcome reward methods lack the capability to regulate the intermediate
reasoning processes during chain-of-thought (CoT) generation, leading to severe
overthinking phenomena. In response, recent studies have designed reward
functions to reinforce models' behaviors in producing shorter yet correct
completions. Nevertheless, we observe that these length-penalty reward
functions exacerbate RL training instability: as the completion length
decreases, model accuracy abruptly collapses, often occurring early in
training. To address this issue, we propose a simple yet effective solution
GRPO-$\lambda$, an efficient and stabilized variant of GRPO, which dynamically
adjusts the reward strategy by monitoring the correctness ratio among
completions within each query-sampled group. A low correctness ratio indicates
the need to avoid length penalty that compromises CoT quality, triggering a
switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A
high ratio maintains length penalties to boost efficiency. Experimental results
show that our approach avoids training instability caused by length penalty
while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,
GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average
accuracy by 1.48% while reducing CoT sequence length by 47.3%.

</details>


### [210] [ProgRM: Build Better GUI Agents with Progress Rewards](https://arxiv.org/abs/2505.18121)
*Danyang Zhang, Situo Zhang, Ziyue Yang, Zichen Zhu, Zihan Zhao, Ruisheng Cao, Lu Chen, Kai Yu*

**主要类别:** cs.AI

**概要:** 大型语言模型（LLM）为基础的图形用户界面（GUI）代理可能极大地改变我们的日常生活。然而，目前的LLM-based GUI代理因高质量训练数据的缺乏而受限，主要由于轨迹收集和奖励标注的困难。现有的工作探索了使用LLM来收集模仿学习的轨迹或提供在线强化学习（RL）训练的奖励信号。然而，现有工作的结果奖励模型（ORM）无法提供细致的反馈，并且可能会过度惩罚最终失败轨迹中的有价值步骤。为了解决这个问题，我们提出了进展奖励模型（ProgRM），通过预测任务完成进度，在线训练中为每一步提供密集的信息中间奖励。为了应对进展奖励标签标注的挑战，我们进一步设计了一种高效的基于最长公共子序列（LCS）的自标注算法，以发现轨迹中的关键步骤并相应地分配进展标签。ProgRM通过广泛的实验和分析进行了评估。使用ProgRM训练的行为者优于领先的专有LLM和ORM训练的行为者，证明了ProgRM的有效性。实验代码将在被接受后公开发布。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于LLM的GUI代理在高质量训练数据方面存在稀缺问题，这限制了其性能提升。ORM在提供反馈时过于简单，可能导致对有价值步骤的过度惩罚。因此，需要一种更精细、能够提供中间奖励的模型来改进训练效果。

**方法:** 提出了一种名为ProgRM的进展奖励模型，该模型通过预测任务完成进度，在线训练中为每一步提供密集的信息中间奖励。同时，设计了一种基于LCS的自标注算法，用于高效地发现轨迹中的关键步骤并分配进展标签。

**结果:** 使用ProgRM训练的行为者在实验中表现出色，优于领先的专有LLM和ORM训练的行为者。广泛的实验和分析验证了ProgRM的有效性。

**结论:** ProgRM作为一种新的进展奖励模型，能够有效改善基于LLM的GUI代理的训练效果。其提供的密集信息中间奖励有助于提高行为者的性能，同时，基于LCS的自标注算法解决了进展奖励标签标注的挑战。实验代码将在论文被接受后公开，供研究社区使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProgRM%3A+Build+Better+GUI+Agents+with+Progress+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18121，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18121&send_immediately=true&force_search=false)

**原文摘要:** LLM-based (Large Language Model) GUI (Graphical User Interface) agents can
potentially reshape our daily lives significantly. However, current LLM-based
GUI agents suffer from the scarcity of high-quality training data owing to the
difficulties of trajectory collection and reward annotation. Existing works
have been exploring LLMs to collect trajectories for imitation learning or to
offer reward signals for online RL training. However, the Outcome Reward Model
(ORM) used in existing works cannot provide finegrained feedback and can
over-penalize the valuable steps in finally failed trajectories. To this end,
we propose Progress Reward Model (ProgRM) to provide dense informative
intermediate rewards by predicting a task completion progress for each step in
online training. To handle the challenge of progress reward label annotation,
we further design an efficient LCS-based (Longest Common Subsequence)
self-annotation algorithm to discover the key steps in trajectories and assign
progress labels accordingly. ProgRM is evaluated with extensive experiments and
analyses. Actors trained with ProgRM outperform leading proprietary LLMs and
ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for
experiments will be made publicly available upon acceptance.

</details>


### [211] [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
*Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press*

**主要类别:** cs.AI

**概要:** 尽管前沿视觉-语言模型在编码和数学基准测试中表现出色，但在感知、空间导航和记忆管理等人类自然任务上的能力仍待研究。为评估这些能力，我们引入了VideoGameBench，一个由10个90年代流行视频游戏组成的基准测试。实验表明，即使是表现最好的Gemini 2.5 Pro模型也只能完成极小部分的游戏，实时推理延迟是主要限制因素。因此，我们还提出了VideoGameBench Lite以降低实时要求。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉-语言模型虽然在复杂基准测试中表现出色，但其在感知、空间导航和记忆管理等人类自然任务上的能力尚未得到充分研究。而视频游戏正是利用了人类的先天归纳偏见，使其成为评估这些能力的理想平台。

**方法:** 构建了一个名为VideoGameBench的基准测试，包含10个90年代流行的视频游戏，要求模型仅通过原始视觉输入和高级目标描述来完成游戏。其中三个游戏被隐藏，以促进对未知环境的泛化能力。此外，为了缓解实时推理延迟的问题，还提出了VideoGameBench Lite版本。

**结果:** 实验结果表明，即使是前沿的视觉-语言模型也难以超越游戏的初始阶段。例如，Gemini 2.5 Pro模型只能完成0.48%的VideoGameBench和1.6%的VideoGameBench Lite。这表明当前模型在这类任务上存在显著困难。

**结论:** 通过将人类技能如感知、空间导航和记忆管理形式化到VideoGameBench基准测试中，我们希望激励相关领域的进一步研究，以提升视觉-语言模型在这些方面的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VideoGameBench%3A+Can+Vision-Language+Models+complete+popular+video+games%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18134&send_immediately=true&force_search=false)

**原文摘要:** Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.

</details>


### [212] [Gaming Tool Preferences in Agentic LLMs](https://arxiv.org/abs/2505.18135)
*Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, Sriram Balasubramanian, Parsa Hosseini, Soheil Feizi*

**主要类别:** cs.AI

**概要:** 大型语言模型（LLMs）通过模型上下文协议（MCP）能够访问各种外部工具，但它们完全依赖工具的文本描述来决定使用哪些工具，这一过程非常脆弱。本研究揭示了通过对工具描述进行适当编辑，可以显著增加LLM对其的使用频率，并通过受控实验展示了不同模型在竞争环境下对修改描述的反应。这表明开发者可以通过优化描述推广工具，但也凸显了为代理型LLM建立更可靠工具选择基础的必要性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管LLMs可以通过MCP访问多种外部工具，但它们仅依赖工具的文本描述来选择工具，这种机制存在不稳定性。因此，研究者希望了解如何通过调整工具描述影响LLM的选择行为，并探讨这种现象在不同模型中的普适性或差异性。

**方法:** 研究者对一系列工具描述进行了编辑，并通过受控实验比较了不同版本描述对LLM工具选择的影响。实验涉及GPT-4.1和Qwen2.5-7B等模型，评估了修改后的描述与原始描述在竞争环境下的表现，并扩展到10种不同模型中分析趋势的共性和差异。

**结果:** 实验结果表明，经过适当编辑的工具描述可以使GPT-4.1和Qwen2.5-7B等模型对特定工具的使用量提高超过10倍。此外，不同编辑策略在竞争条件下的效果各异，且这些现象在10种不同模型中表现出一定的普适性和差异性。

**结论:** 虽然编辑工具描述为开发者提供了一种有效推广工具的方法，但这也暴露了当前LLM工具选择机制的脆弱性。未来需要构建更为可靠的框架，以支持代理型LLM更合理地选择和利用工具及资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gaming+Tool+Preferences+in+Agentic+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18135，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18135&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can now access a wide range of external tools,
thanks to the Model Context Protocol (MCP). This greatly expands their
abilities as various agents. However, LLMs rely entirely on the text
descriptions of tools to decide which ones to use--a process that is
surprisingly fragile. In this work, we expose a vulnerability in prevalent
tool/function-calling protocols by investigating a series of edits to tool
descriptions, some of which can drastically increase a tool's usage from LLMs
when competing with alternatives. Through controlled experiments, we show that
tools with properly edited descriptions receive over 10 times more usage from
GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further
evaluate how various edits to tool descriptions perform when competing directly
with one another and how these trends generalize or differ across a broader set
of 10 different models. These phenomenons, while giving developers a powerful
way to promote their tools, underscore the need for a more reliable foundation
for agentic LLMs to select and utilize tools and resources.

</details>


### [213] [Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems](https://arxiv.org/abs/2505.18139)
*Gordon Dai, Yunze Xiao*

**主要类别:** cs.AI

**概要:** This paper argues that inconsistencies among Responsible AI (RAI) metrics should be seen as a valuable feature rather than a flaw. It highlights three benefits of embracing these inconsistencies: normative pluralism, epistemological completeness, and implicit regularization. The authors advocate for characterizing acceptable inconsistency thresholds and elucidating mechanisms for robust, approximated consistency in RAI practice.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the theoretical inconsistencies often observed among Responsible AI (RAI) metrics, such as differing fairness definitions or tradeoffs between accuracy and privacy, and to argue that these inconsistencies should be embraced rather than eliminated.

**方法:** The method involves treating RAI metrics as divergent objectives and navigating the inconsistencies among them. This approach yields three key benefits: normative pluralism, epistemological completeness, and implicit regularization.

**结果:** By embracing inconsistencies, RAI can better represent diverse moral stances and stakeholder values, capture multifaceted ethical concepts more comprehensively, and enhance model generalization and robustness.

**结论:** The conclusion is that efforts to enforce theoretical consistency by simplifying or pruning metrics risk narrowing value diversity, losing conceptual depth, and degrading model performance. Therefore, there should be a shift in RAI theory and practice towards characterizing acceptable inconsistency thresholds and understanding the mechanisms for robust, approximated consistency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embracing+Contradiction%3A+Theoretical+Inconsistency+Will+Not+Impede+the+Road+of+Building+Responsible+AI+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18139&send_immediately=true&force_search=false)

**原文摘要:** This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.

</details>


### [214] [Advancing Uncertain Combinatorics through Graphization, Hyperization, and Uncertainization: Fuzzy, Neutrosophic, Soft, Rough, and Beyond](https://arxiv.org/abs/2411.17411)
*Takaaki Fujita*

**主要类别:** cs.AI

**概要:** 为了更好地处理现实世界的不确定性，本文探讨了新的图和集合概念，包括模糊集、中智集、粗糙集和软集等，并扩展了若干图概念如中智超集、中智子集和非标准实集。本文旨在整合近期研究成果，为研究人员提供有价值的资源以激发新思想。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的不确定性需要更先进的工具来建模和分析，现有的集合概念（如模糊集、中智集、粗糙集和软集）在复杂系统中显示出重要价值。此外，图论和超图概念的结合为研究提供了新的方向。因此，本文希望探索这些领域的最新进展并加以整合。

**方法:** 通过引入和定义新的集合与图的概念（如中智超集、中智子集、中智偏移集和非标准实集），本文扩展了传统的图论和集合论方法。同时，文章对现有研究成果进行了综合概述，形成了一个类似综述的资源。

**结果:** 定义了多种新的集合和图的概念，例如中智超集、中智子集、中智偏移集和非标准实集。这些概念不仅扩展了传统理论，还为未来的研究提供了灵感和方向。

**结论:** 本文提出的新的集合和图的概念为处理现实世界的不确定性提供了更丰富的工具。这些成果将作为宝贵资源，帮助研究人员进一步探索相关领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Uncertain+Combinatorics+through+Graphization%2C+Hyperization%2C+and+Uncertainization%3A+Fuzzy%2C+Neutrosophic%2C+Soft%2C+Rough%2C+and+Beyond，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2411.17411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2411.17411&send_immediately=true&force_search=false)

**原文摘要:** To better handle real-world uncertainty, concepts such as fuzzy sets,
neutrosophic sets, rough sets, and soft sets have been introduced. For example,
neutrosophic sets, which simultaneously represent truth, indeterminacy, and
falsehood, have proven to be valuable tools for modeling uncertainty in complex
systems. These set concepts are increasingly studied in graphized forms, and
generalized graph concepts now encompass well-known structures such as
hypergraphs and superhypergraphs. Furthermore, hyperconcepts and
superhyperconcepts are being actively researched in areas beyond graph theory.
  Combinatorics, uncertain sets (including fuzzy sets, neutrosophic sets, rough
sets, soft sets, and plithogenic sets), uncertain graphs, and hyper and
superhyper concepts are active areas of research with significant mathematical
and practical implications. Recognizing their importance, this paper explores
new graph and set concepts, as well as hyper and superhyper concepts, as
detailed in the "Results" section of "The Structure of the Paper."
Additionally, this work aims to consolidate recent findings, providing a
survey-like resource to inform and engage readers.
  For instance, we extend several graph concepts by introducing Neutrosophic
Oversets, Neutrosophic Undersets, Neutrosophic Offsets, and the Nonstandard
Real Set. This paper defines a variety of concepts with the goal of inspiring
new ideas and serving as a valuable resource for researchers in their academic
pursuits.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [215] [Learning Probabilities of Causation from Finite Population Data](https://arxiv.org/abs/2505.17133)
*Shuai Wang, Song Jiang, Yizhou Sun, Judea Pearl, Ang Li*

**主要类别:** stat.ML

**概要:** 本研究提出了一种基于机器学习的方法，用于在数据不足的情况下预测子群体的因果概率。通过使用多层感知器（MLP）模型和Mish激活函数，能够有效预测必要性和充分性概率（PNS），平均绝对误差约为0.02。


<details>
  <summary>更多</summary>
  
**动机:** 现代决策中，因果概率的预测至关重要，但针对子群体的数据通常不足，难以准确估计因果概率。Tian和Pearl提出的三种基本因果概率（PNS、PS、PN）需要实验与观察分布，而这对于许多子群体来说难以获取。

**方法:** 研究提出了利用机器学习模型从数据充足的子群体中提取信息，以预测数据不足的子群体的因果概率。具体方法为：通过模拟多个结构因果模型（SCMs），采用多层感知器（MLP）模型结合Mish激活函数进行预测。

**结果:** 在仅使用2,000个已知PNS值的子群体数据的情况下，该方法对32,768个子群体的PNS预测达到了约0.02的平均绝对误差（MAE）。

**结论:** 机器学习模型可以有效解决数据不足情况下子群体因果概率的预测问题，特别是MLP结合Mish激活函数表现优异，为实际应用提供了可行方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Probabilities+of+Causation+from+Finite+Population+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17133&send_immediately=true&force_search=false)

**原文摘要:** Probabilities of causation play a crucial role in modern decision-making.
This paper addresses the challenge of predicting probabilities of causation for
subpopulations with \textbf{insufficient} data using machine learning models.
Tian and Pearl first defined and derived tight bounds for three fundamental
probabilities of causation: the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN).
However, estimating these probabilities requires both experimental and
observational distributions specific to each subpopulation, which are often
unavailable or impractical to obtain with limited population-level data.
Therefore, for most subgroups, the amount of data they have is not enough to
guarantee the accuracy of their probabilities. Hence, to estimate these
probabilities for subpopulations with \textbf{insufficient} data, we propose
using machine learning models that draw insights from subpopulations with
sufficient data. Our evaluation of multiple machine learning models indicates
that, given the population-level data and an appropriate choice of machine
learning model and activation function, PNS can be effectively predicted.
Through simulation studies on multiple Structured Causal Models (SCMs), we show
that our multilayer perceptron (MLP) model with the Mish activation function
achieves a mean absolute error (MAE) of approximately $0.02$ in predicting PNS
for $32,768$ subpopulations across most SCMs using data from only $2,000$
subpopulations with known PNS values.

</details>


### [216] [Liouville PDE-based sliced-Wasserstein flow for fair regression](https://arxiv.org/abs/2505.17204)
*Pilhwa Lee, Jayshawn Cooper*

**主要类别:** stat.ML

**概要:** The paper enhances the sliced Wasserstein flow (SWF) by transforming stochastic diffusive terms and using Liouville PDE-based transport, improving convergence in training/testing SWF barycenters. It applies generative SWF barycenter for fair regression, showing strong accuracy-fairness trade-offs.


<details>
  <summary>更多</summary>
  
**动机:** To improve the sliced Wasserstein flow (SWF) for better performance in fair regression tasks, focusing on reducing variance and enhancing convergence.

**方法:** Transforming the stochastic diffusive term from Fokker-Planck equation-based Monte Carlo to Liouville PDE-based transport with density estimation without diffusive term. Approximating the computation of Wasserstein barycenter via SWF barycenter with Kantorovich potentials for gradient flow sample generation.

**结果:** Improved convergence during training and testing of SWF and SWF barycenters with reduced variance. Demonstrated competent profiles in accuracy-fairness Pareto curves for fair regression.

**结论:** The enhanced SWF approach effectively improves fair regression performance, providing a good trade-off between accuracy and fairness.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Liouville+PDE-based+sliced-Wasserstein+flow+for+fair+regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17204&send_immediately=true&force_search=false)

**原文摘要:** The sliced Wasserstein flow (SWF), a nonparametric and implicit generative
gradient flow, is applied to fair regression. We have improved the SWF in a few
aspects. First, the stochastic diffusive term from the Fokker-Planck
equation-based Monte Carlo is transformed to Liouville partial differential
equation (PDE)-based transport with density estimation, however, without the
diffusive term. Now, the computation of the Wasserstein barycenter is
approximated by the SWF barycenter with the prescription of Kantorovich
potentials for the induced gradient flow to generate its samples. These two
efforts improve the convergence in training and testing SWF and SWF barycenters
with reduced variance. Applying the generative SWF barycenter for fair
regression demonstrates competent profiles in the accuracy-fairness Pareto
curves.

</details>


### [217] [Deconfounded Warm-Start Thompson Sampling with Applications to Precision Medicine](https://arxiv.org/abs/2505.17283)
*Prateek Jaiswal, Esmaeil Keyvanshokooh, Junyu Cao*

**主要类别:** stat.ML

**概要:** 提出了一种新的方法DWTS，能够利用混淆的观察数据启动自适应临床试验，并在实验中表现优于标准LinTS。


<details>
  <summary>更多</summary>
  
**动机:** 随机对照试验通常需要大量患者样本才能得出结论，而由于混淆和隐藏偏差，平行研究中的大量观察数据未被充分利用。

**方法:** 提出了Deconfounded Warm-Start Thompson Sampling (DWTS)，通过Doubly Debiased LASSO (DDL) 方法识别可靠的测量协变量，并与关键隐藏协变量结合形成简化上下文。使用DDL估计的均值和方差初始化Thompson Sampling (LinTS) 的先验分布，从而有效利用混淆的观察数据。

**结果:** 在纯合成环境和基于真实心血管风险数据集创建的虚拟环境中，DWTS始终比标准LinTS实现更低的累积遗憾。

**结论:** DWTS展示了如何利用观察数据中的离线因果洞察来提高试验效率并支持更个性化的治疗决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deconfounded+Warm-Start+Thompson+Sampling+with+Applications+to+Precision+Medicine，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17283，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17283&send_immediately=true&force_search=false)

**原文摘要:** Randomized clinical trials often require large patient cohorts before drawing
definitive conclusions, yet abundant observational data from parallel studies
remains underutilized due to confounding and hidden biases. To bridge this gap,
we propose Deconfounded Warm-Start Thompson Sampling (DWTS), a practical
approach that leverages a Doubly Debiased LASSO (DDL) procedure to identify a
sparse set of reliable measured covariates and combines them with key hidden
covariates to form a reduced context. By initializing Thompson Sampling (LinTS)
priors with DDL-estimated means and variances on these measured features --
while keeping uninformative priors on hidden features -- DWTS effectively
harnesses confounded observational data to kick-start adaptive clinical trials.
Evaluated on both a purely synthetic environment and a virtual environment
created using real cardiovascular risk dataset, DWTS consistently achieves
lower cumulative regret than standard LinTS, showing how offline causal
insights from observational data can improve trial efficiency and support more
personalized treatment decisions.

</details>


### [218] [Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation](https://arxiv.org/abs/2505.17288)
*Seamus Somerstep, Vinod Raman, Unique Subedi, Yuekai Sun*

**主要类别:** stat.ML

**概要:** 通过位串生成问题作为案例研究，理论上比较了两种将大型语言模型适应新任务的标准方法：监督微调和最佳N选一（BoN）。在可实现的学习环境中，监督微调由于其收敛速度对响应长度有更好的依赖性而优于BoN。然而，在不可实现的情况下，根据失败模式的不同，BoN可能在样本数量n的收敛速度或对响应长度的依赖性上具有更好的表现。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在通过位串生成问题这一案例，评估和对比两种主流方法（监督微调和BoN）在使大型语言模型适应新任务时的表现。这有助于理解不同场景下哪种方法更适合实际应用。

**方法:** 论文采用了理论分析的方法，针对位串生成问题，分别探讨了监督微调和BoN方法的性能。重点在于比较这两种方法在可实现和不可实现学习环境下的收敛速度及对响应长度的依赖性。

**结果:** 在可实现的学习设置中，监督微调展现出比BoN更好的收敛速度，尤其是在长响应情况下。而在不可实现的情况下，BoN可能在样本数量或响应长度方面表现出更优的收敛特性，具体取决于失败模式。

**结论:** 对于大型语言模型的新任务适应，选择使用监督微调还是BoN应基于学习环境的可实现性以及具体的失败模式。两者各有优势，适用于不同的情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Choose+or+Choosing+to+Learn%3A+Best-of-N+vs.+Supervised+Fine-Tuning+for+Bit+String+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17288，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17288&send_immediately=true&force_search=false)

**原文摘要:** Using the bit string generation problem as a case study, we theoretically
compare two standard methods for adapting large language models to new tasks.
The first, referred to as supervised fine-tuning, involves training a new next
token predictor on good generations. The second method, Best-of-N, trains a
reward model to select good responses from a collection generated by an
unaltered base model. If the learning setting is realizable, we find that
supervised fine-tuning outperforms BoN through a better dependence on the
response length in its rate of convergence. If realizability fails, then
depending on the failure mode, BoN can enjoy a better rate of convergence in
either n or a rate of convergence with better dependence on the response
length.

</details>


### [219] [Optimal Transport with Heterogeneously Missing Data](https://arxiv.org/abs/2505.17291)
*Linus Bleistein, Aurélien Bellet, Julie Josse*

**主要类别:** stat.ML

**概要:** 本文研究了在存在缺失值的情况下，求解两个经验分布之间的最优传输问题的方法。假设数据是完全随机丢失（MCAR），并允许特征和两个分布之间有不同的缺失概率。文章的主要贡献包括：1）展示了Wasserstein距离和线性Monge映射可以去偏而不显著影响样本复杂度；2）提出了一种基于迭代奇异值阈值（ISVT）的有效且一致的熵正则化最优传输估计方法；3）设计了一种无需验证集的超参数选择策略，利用Bures-Wasserstein距离估计器。最后，通过广泛的数值应用验证了这些发现。


<details>
  <summary>更多</summary>
  
**动机:** 解决带有缺失值的经验分布之间的最优传输问题，特别是在数据缺失完全随机（MCAR）的情况下。目标是开发一种有效的方法来处理这种情况下最优传输的估计问题，并降低偏差。

**方法:** 1）证明了Wasserstein距离和线性Monge映射可以通过特定方法去偏；2）采用迭代奇异值阈值（ISVT）技术估计熵正则化的最优传输问题；3）提出了一种无需验证集的超参数选择策略，结合Bures-Wasserstein距离估计器。

**结果:** 1）成功去除了Wasserstein距离和线性Monge映射中的偏差；2）使用ISVT方法实现了对熵正则化最优传输问题的有效且一致的估计；3）提出的超参数选择策略在矩阵补全问题中具有独立的应用价值。

**结论:** 本文提出的方法能够有效解决带有缺失值的经验分布之间的最优传输问题，同时保持较低的偏差和样本复杂度。所提出的ISVT估计方法和超参数选择策略在理论和实践中均表现良好，为相关领域提供了新的工具和思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Transport+with+Heterogeneously+Missing+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17291&send_immediately=true&force_search=false)

**原文摘要:** We consider the problem of solving the optimal transport problem between two
empirical distributions with missing values. Our main assumption is that the
data is missing completely at random (MCAR), but we allow for heterogeneous
missingness probabilities across features and across the two distributions. As
a first contribution, we show that the Wasserstein distance between empirical
Gaussian distributions and linear Monge maps between arbitrary distributions
can be debiased without significantly affecting the sample complexity.
Secondly, we show that entropic regularized optimal transport can be estimated
efficiently and consistently using iterative singular value thresholding
(ISVT). We propose a validation set-free hyperparameter selection strategy for
ISVT that leverages our estimator of the Bures-Wasserstein distance, which
could be of independent interest in general matrix completion problems.
Finally, we validate our findings on a wide range of numerical applications.

</details>


### [220] [Statistical Inference for Online Algorithms](https://arxiv.org/abs/2505.17300)
*Selina Carter, Arun K Kuchibhotla*

**主要类别:** stat.ML

**概要:** 本文提出了一种无需估计渐近方差的在线算法输出的置信区域构建方法，该方法计算效率高、收敛速度快，并且在理论上是有效的。特别地，这种方法可以应用于任何产生渐近正态估计量的算法。作者通过研究带有Polyak平均的随机梯度下降算法来验证所提方法的实际性能。


<details>
  <summary>更多</summary>
  
**动机:** 许多统计推断问题需要基于渐近正态估计量构造置信区间和假设检验，通常使用Wald区间或似然比检验，但这些方法需要估计渐近方差。然而，在线/顺序算法得到的估计量要求考虑推理问题的计算方面，无法像传统方法那样多次访问所有数据。因此，探索一种无需估计渐近方差即可进行有效推断的方法是非常有意义的。

**方法:** 作者提出了一种基于在线算法输出的置信区域构建方法，这种方法不需要估计渐近方差。该方法具有计算高效性、最优收敛速度，并且在理论上是有效的。特别地，它可以应用于任何产生渐近正态估计量的算法。为了评估所提方法的实际性能，作者专注于带有Polyak平均的随机梯度下降算法。

**结果:** 所提出的置信区域构建方法在理论和实践中均表现良好，能够在不估计渐近方差的情况下提供有效的统计推断。此外，该方法对于各种在线算法具有广泛的适用性。

**结论:** 本文提出了一种无需估计渐近方差的置信区域构建方法，该方法在计算效率、收敛速度和理论有效性方面表现出色。这为在线算法的统计推断提供了一个新的视角，并且适用于多种实际场景中的算法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Statistical+Inference+for+Online+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17300，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17300&send_immediately=true&force_search=false)

**原文摘要:** Construction of confidence intervals and hypothesis tests for functionals
based on asymptotically normal estimators is a classical topic in statistical
inference. The simplest and in many cases optimal inference procedure is the
Wald interval or the likelihood ratio test, both of which require an estimator
and an estimate of the asymptotic variance of the estimator. Estimators
obtained from online/sequential algorithms forces one to consider the
computational aspects of the inference problem, i.e., one cannot access all of
the data as many times as needed. Several works on this topic explored the
online estimation of asymptotic variance. In this article, we propose
computationally efficient, rate-optimal, and asymptotically valid confidence
regions based on the output of online algorithms {\em without} estimating the
asymptotic variance. As a special case, this implies inference from any
algorithm that yields an asymptotically normal estimator. We focus our efforts
on stochastic gradient descent with Polyak averaging to understand the
practical performance of the proposed method.

</details>


### [221] [Repulsive Ensembles for Bayesian Inference in Physics-informed Neural Networks](https://arxiv.org/abs/2505.17308)
*Philipp Pilar, Markus Heinonen, Niklas Wahlström*

**主要类别:** stat.ML

**概要:** 这篇论文提出了一种新的方法，即通过使用具有排斥性的PINN集合（RE-PINN）来解决微分方程的逆问题，并提供不确定性估计。相比标准集合，该方法可以产生更准确的不确定性估计和更高的样本多样性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的物理信息神经网络（PINNs）在解决微分方程方面非常有效，但在从数据中推断解和参数时，通常只提供点估计，而没有不确定性评估。这限制了对解的准确性理解。因此，需要一种能够提供不确定性估计的方法。

**方法:** 研究采用了排斥性PINN集合（RE-PINN）的方法。通过在损失函数中添加特殊的排斥项，确保集合预测在无限成员的情况下对应于真实的贝叶斯后验分布。此外，还与蒙特卡洛基线进行了比较以验证效果。

**结果:** 实验结果表明，排斥性集合不仅避免了标准集合向最大后验概率解的坍缩，而且生成了显著更准确的不确定性估计，同时提高了样本的多样性。

**结论:** RE-PINN方法为微分方程逆问题提供了有效的不确定性估计，相较于传统方法有明显改进，尤其是在样本多样性和估计准确性方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Repulsive+Ensembles+for+Bayesian+Inference+in+Physics-informed+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17308，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17308&send_immediately=true&force_search=false)

**原文摘要:** Physics-informed neural networks (PINNs) have proven an effective tool for
solving differential equations, in particular when considering non-standard or
ill-posed settings. When inferring solutions and parameters of the differential
equation from data, uncertainty estimates are preferable to point estimates, as
they give an idea about the accuracy of the solution. In this work, we consider
the inverse problem and employ repulsive ensembles of PINNs (RE-PINN) for
obtaining such estimates. The repulsion is implemented by adding a particular
repulsive term to the loss function, which has the property that the ensemble
predictions correspond to the true Bayesian posterior in the limit of infinite
ensemble members. Where possible, we compare the ensemble predictions to Monte
Carlo baselines. Whereas the standard ensemble tends to collapse to
maximum-a-posteriori solutions, the repulsive ensemble produces significantly
more accurate uncertainty estimates and exhibits higher sample diversity.

</details>


### [222] [Offline Constrained Reinforcement Learning under Partial Data Coverage](https://arxiv.org/abs/2505.17506)
*Kihyuk Hong, Ambuj Tewari*

**主要类别:** stat.ML

**概要:** 研究了带有一般函数逼近的离线约束强化学习问题。提出了一种基于线性规划公式化的oracle-efficient原始对偶算法，在部分数据覆盖下达到$O(\epsilon^{-2})$样本复杂度，并通过引入可实现性假设和Lagrangian分解来简化分析并增强实际应用性。


<details>
  <summary>更多</summary>
  
**动机:** 在离线强化学习中，需要从预先收集的数据集中学习策略，同时满足主奖励信号的最大化和辅助奖励信号的阈值限制。但现有算法存在需要完全探索数据、计算效率低下或依赖额外辅助函数类的问题。

**方法:** 提出了一种基于线性规划公式化的oracle-efficient原始对偶算法，通过引入可实现性假设确保所有Lagrangian鞍点为最优解，从而去除正则化需求；利用Lagrangian分解提取策略，无需了解数据生成分布。

**结果:** 该算法在部分数据覆盖情况下达到了$O(\epsilon^{-2})$样本复杂度，简化了先前分析中的正则化步骤，并增强了方法的实际适用性。

**结论:** 提出的算法解决了现有离线约束强化学习算法的不足，提供了一种更高效且实用的方法，适用于部分数据覆盖场景并避免了复杂的正则化步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Offline+Constrained+Reinforcement+Learning+under+Partial+Data+Coverage，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17506&send_immediately=true&force_search=false)

**原文摘要:** We study offline constrained reinforcement learning (RL) with general
function approximation. We aim to learn a policy from a pre-collected dataset
that maximizes the expected discounted cumulative reward for a primary reward
signal while ensuring that expected discounted returns for multiple auxiliary
reward signals are above predefined thresholds. Existing algorithms either
require fully exploratory data, are computationally inefficient, or depend on
an additional auxiliary function classes to obtain an $\epsilon$-optimal policy
with sample complexity $O(\epsilon^{-2})$. In this paper, we propose an
oracle-efficient primal-dual algorithm based on a linear programming (LP)
formulation, achieving $O(\epsilon^{-2})$ sample complexity under partial data
coverage. By introducing a realizability assumption, our approach ensures that
all saddle points of the Lagrangian are optimal, removing the need for
regularization that complicated prior analyses. Through Lagrangian
decomposition, our method extracts policies without requiring knowledge of the
data-generating distribution, enhancing practical applicability.

</details>


### [223] [A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation](https://arxiv.org/abs/2505.17717)
*Akira Tanimoto*

**主要类别:** stat.ML

**概要:** 这篇论文提出了一种新的因果推断方法，通过结合分布鲁棒优化和权重正则化来解决倾向得分估计不准确和极端权重不稳定的问题。实验表明该方法优于现有技术。


<details>
  <summary>更多</summary>
  
**动机:** 传统的因果推断方法使用逆概率加权（IPW）处理训练数据中的不平衡问题，但存在倾向得分估计不准确和极端权重导致的不稳定性的挑战。

**方法:** 作者提出了一种基于对抗损失函数的方法，将分布鲁棒优化与基于加权Rademacher复杂度的权重正则化相结合，以解决倾向不确定性及统计不稳定性问题。

**结果:** 在合成和真实世界的数据集上的实验表明，该方法相比现有的因果推断方法表现出一致的改进。

**结论:** 新方法有效解决了传统IPW方法中的倾向得分估计不准确和极端权重不稳定的问题，为因果推断提供了一个更稳健的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Distributionally-Robust+Framework+for+Nuisance+in+Causal+Effect+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17717，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17717&send_immediately=true&force_search=false)

**原文摘要:** Causal inference requires evaluating models on balanced distributions between
treatment and control groups, while training data often exhibits imbalance due
to historical decision-making policies. Most conventional statistical methods
address this distribution shift through inverse probability weighting (IPW),
which requires estimating propensity scores as an intermediate step. These
methods face two key challenges: inaccurate propensity estimation and
instability from extreme weights. We decompose the generalization error to
isolate these issues--propensity ambiguity and statistical instability--and
address them through an adversarial loss function. Our approach combines
distributionally robust optimization for handling propensity uncertainty with
weight regularization based on weighted Rademacher complexity. Experiments on
synthetic and real-world datasets demonstrate consistent improvements over
existing methods.

</details>


### [224] [Optimal Online Change Detection via Random Fourier Features](https://arxiv.org/abs/2505.17789)
*Florian Kalinke, Shakeel Gavioli-Akilagun*

**主要类别:** stat.ML

**概要:** 这篇论文研究了多元数据流中的在线非参数变点检测问题。通过核方法的两样本检验，提出了一种基于随机傅里叶特征的顺序检验程序，具有每观测对数时间复杂度和整体对数空间复杂度。该算法相比现有技术有两大优势：1）真正实现在线检测，无需访问已知来自预变化分布的训练数据；2）无需用户指定用于计算局部检验的窗口参数。此外，作者证明了该算法具有强大的理论性能保证，并展示了其在极小极大意义上最优的检测延迟。数值研究表明，该算法在真实和合成数据上与现有技术具有竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 多元数据流中的在线非参数变点检测是一个重要但具有挑战性的问题，现有的方法通常需要访问已知的训练数据或依赖于特定窗口参数的设定。因此，开发一种无需这些限制的高效在线检测方法显得尤为重要。

**方法:** 作者采用核方法的两样本检验视角，引入基于随机傅里叶特征的顺序检验程序。该方法实现了每观测对数时间复杂度和整体对数空间复杂度。其核心在于避免了对训练数据的依赖以及对窗口参数的需求，从而真正实现了在线检测。

**结果:** 理论上，作者证明了该算法具有信息论界保证的最优检测延迟。实验结果表明，在真实和合成数据上的表现与现有技术相当甚至更优。

**结论:** 本文提出的基于随机傅里叶特征的顺序检验程序为多元数据流中的在线非参数变点检测提供了一种高效且无参数的方法，具备理论和实际应用的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Online+Change+Detection+via+Random+Fourier+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17789，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17789&send_immediately=true&force_search=false)

**原文摘要:** This article studies the problem of online non-parametric change point
detection in multivariate data streams. We approach the problem through the
lens of kernel-based two-sample testing and introduce a sequential testing
procedure based on random Fourier features, running with logarithmic time
complexity per observation and with overall logarithmic space complexity. The
algorithm has two advantages compared to the state of the art. First, our
approach is genuinely online, and no access to training data known to be from
the pre-change distribution is necessary. Second, the algorithm does not
require the user to specify a window parameter over which local tests are to be
calculated. We prove strong theoretical guarantees on the algorithm's
performance, including information-theoretic bounds demonstrating that the
detection delay is optimal in the minimax sense. Numerical studies on real and
synthetic data show that our algorithm is competitive with respect to the state
of the art.

</details>


### [225] [Quantifying uncertainty in spectral clusterings: expectations for perturbed and incomplete data](https://arxiv.org/abs/2505.17819)
*Jürgen Dölz, Jolanda Weygandt*

**主要类别:** stat.ML

**概要:** 论文探讨了在数据存在误差或丢失的情况下，如何通过随机集合理论和蒙特卡洛方法计算统计期望的聚类结果，并提出了多个可计算的量化指标来分析其一致性。


<details>
  <summary>更多</summary>
  
**动机:** 谱聚类是一种流行的无监督学习技术，可以将未标记的数据划分为不同形状的不相交簇。然而，实际中的数据通常包含测量误差、丢失或无效数据，这导致聚类结果存在不确定性，影响了聚类的可靠性。因此需要一种方法来处理这些不确定性的输入数据并生成可靠的聚类结果。

**方法:** 作者将不确定性建模为随机过程，并基于随机集合理论提出了一种数学框架。该框架利用蒙特卡洛方法近似计算统计期望的聚类结果。同时，论文提出了多个可计算的量化指标，并分析了它们在无限数据点和无限蒙特卡洛样本极限下的一致性。

**结果:** 数值实验验证了所提出的量化指标的有效性，并展示了不同指标之间的比较结果。这些结果表明，所提方法能够在处理受干扰、不完整或附加数据时提供更可靠的聚类结果。

**结论:** 通过将不确定性建模为随机过程，并结合随机集合理论与蒙特卡洛方法，论文成功地计算了统计期望的聚类结果。所提出的量化指标在理论上具有良好的一致性，并在数值实验中表现出了有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantifying+uncertainty+in+spectral+clusterings%3A+expectations+for+perturbed+and+incomplete+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17819，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17819&send_immediately=true&force_search=false)

**原文摘要:** Spectral clustering is a popular unsupervised learning technique which is
able to partition unlabelled data into disjoint clusters of distinct shapes.
However, the data under consideration are often experimental data, implying
that the data is subject to measurement errors and measurements may even be
lost or invalid. These uncertainties in the corrupted input data induce
corresponding uncertainties in the resulting clusters, and the clusterings thus
become unreliable.
  Modelling the uncertainties as random processes, we discuss a mathematical
framework based on random set theory for the computational Monte Carlo
approximation of statistically expected clusterings in case of corrupted, i.e.,
perturbed, incomplete, and possibly even additional, data. We propose several
computationally accessible quantities of interest and analyze their consistency
in the infinite data point and infinite Monte Carlo sample limit. Numerical
experiments are provided to illustrate and compare the proposed quantities.

</details>


### [226] [Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means](https://arxiv.org/abs/2505.17836)
*Anna Van Elst, Igor Colin, Stephan Clémençon*

**主要类别:** stat.ML

**概要:** 本文针对任意通信图上的流言算法的鲁棒估计问题，提出了一种新的流言算法GoRank用于秩估计，并基于此设计了用于修剪均值估计的GoTrim算法。理论分析表明，GoRank的收敛速率为O(1/t)，而GoTrim的收敛速率为O(log(t)/t)。实验验证了在不同网络拓扑、数据分布和污染模式下的理论结果。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于均值的流言算法容易受到恶意或损坏节点的影响，因此需要一种更鲁棒的方法来解决这一问题。

**方法:** 提出了两种新算法：GoRank用于秩估计，GoTrim用于修剪均值估计。通过全局估计鲁棒统计量来计算异常值鲁棒均值。同时，进行了详细的收敛性分析以及GoTrim的崩溃点分析。

**结果:** 理论证明了GoRank的收敛速率为O(1/t)，GoTrim的收敛速率为O(log(t)/t)。实验结果验证了理论分析的正确性。

**结论:** 所提出的GoRank和GoTrim算法能够有效应对恶意或损坏节点的影响，提供了鲁棒的估计方法，并且理论和实证结果一致。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Distributed+Estimation%3A+Extending+Gossip+Algorithms+to+Ranking+and+Trimmed+Means，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17836，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17836&send_immediately=true&force_search=false)

**原文摘要:** This paper addresses the problem of robust estimation in gossip algorithms
over arbitrary communication graphs. Gossip algorithms are fully decentralized,
relying only on local neighbor-to-neighbor communication, making them
well-suited for situations where communication is constrained. A fundamental
challenge in existing mean-based gossip algorithms is their vulnerability to
malicious or corrupted nodes. In this paper, we show that an outlier-robust
mean can be computed by globally estimating a robust statistic. More
specifically, we propose a novel gossip algorithm for rank estimation, referred
to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated
to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed
description of the proposed methods, a key contribution of our work is a
precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank
estimation and an $\mathcal{O}(\log(t)/t)$ rate for trimmed mean estimation,
where by $t$ is meant the number of iterations. Moreover, we provide a
breakdown point analysis of \textsc{GoTrim}. We empirically validate our
theoretical results through experiments on diverse network topologies, data
distributions and contamination schemes.

</details>


### [227] [Continuum Transformers Perform In-Context Learning by Operator Gradient Descent](https://arxiv.org/abs/2505.17838)
*Abhiti Mishra, Yash Patel, Ambuj Tewari*

**主要类别:** stat.ML

**概要:** 本文研究了连续变换器在上下文学习中的能力，表明其通过在算子RKHS中执行梯度下降实现上下文算子学习，并证明了在无限深度限制下，上下文学到的算子是贝叶斯最优预测器。


<details>
  <summary>更多</summary>
  
**动机:** 尽管连续变换器在PDE代理建模中表现出令人印象深刻的上下文学习能力，但这种能力尚未得到理论上的解释。因此，需要一种理论框架来描述和验证连续变换器的上下文学习机制。

**方法:** 作者通过引入新的证明策略，利用Hilbert空间的广义表示定理和函数空间上的梯度流，展示了连续变换器如何在算子RKHS中执行梯度下降以进行上下文算子学习。此外，还提供了经验验证，证明了训练过程中恢复了执行此类梯度下降所需的参数。

**结果:** 理论上证明了连续变换器通过在算子RKHS中执行梯度下降进行上下文学习，并且在无限深度限制下，学得的算子是最优贝叶斯预测器。经验结果也验证了这一理论发现。

**结论:** 连续变换器能够通过在算子RKHS中执行梯度下降实现上下文学习，并且在无限深度下达到贝叶斯最优预测性能。这些发现为理解连续变换器的上下文学习机制提供了理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuum+Transformers+Perform+In-Context+Learning+by+Operator+Gradient+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17838，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17838&send_immediately=true&force_search=false)

**原文摘要:** Transformers robustly exhibit the ability to perform in-context learning,
whereby their predictive accuracy on a task can increase not by parameter
updates but merely with the placement of training samples in their context
windows. Recent works have shown that transformers achieve this by implementing
gradient descent in their forward passes. Such results, however, are restricted
to standard transformer architectures, which handle finite-dimensional inputs.
In the space of PDE surrogate modeling, a generalization of transformers to
handle infinite-dimensional function inputs, known as "continuum transformers,"
has been proposed and similarly observed to exhibit in-context learning.
Despite impressive empirical performance, such in-context learning has yet to
be theoretically characterized. We herein demonstrate that continuum
transformers perform in-context operator learning by performing gradient
descent in an operator RKHS. We demonstrate this using novel proof strategies
that leverage a generalized representer theorem for Hilbert spaces and gradient
flows over the space of functionals of a Hilbert space. We additionally show
the operator learned in context is the Bayes Optimal Predictor in the infinite
depth limit of the transformer. We then provide empirical validations of this
optimality result and demonstrate that the parameters under which such gradient
descent is performed are recovered through the continuum transformer training.

</details>


### [228] [DataRater: Meta-Learned Dataset Curation](https://arxiv.org/abs/2505.17895)
*Dan A. Calian, Gregory Farquhar, Iurii Kemaev, Luisa M. Zintgraf, Matteo Hessel, Jeremy Shar, Junhyuk Oh, András György, Tom Schaul, Jeffrey Dean, Hado van Hasselt, David Silver*

**主要类别:** stat.ML

**概要:** 通过学习哪些数据对训练有价值，DataRater可以有效过滤数据，显著提高计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 基础模型的质量很大程度上取决于其训练数据，因此需要努力进行数据集策划。目前大多数方法依赖于手动调整粗粒度的数据混合或使用手工制定的启发式规则进行过滤。

**方法:** 提出了一种名为DataRater的方法，它通过元学习（meta-learning）使用`meta-gradients'来估计任何特定数据点的训练价值，目标是提高在保留数据上的训练效率。

**结果:** 在广泛的实验中，使用DataRater过滤数据被证明是非常有效的，能够显著提高计算效率。

**结论:** DataRater通过学习数据的价值来进行更精细和有效的数据策划，从而提高了训练效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DataRater%3A+Meta-Learned+Dataset+Curation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17895，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17895&send_immediately=true&force_search=false)

**原文摘要:** The quality of foundation models depends heavily on their training data.
Consequently, great efforts have been put into dataset curation. Yet most
approaches rely on manual tuning of coarse-grained mixtures of large buckets of
data, or filtering by hand-crafted heuristics. An approach that is ultimately
more scalable (let alone more satisfying) is to \emph{learn} which data is
actually valuable for training. This type of meta-learning could allow more
sophisticated, fine-grained, and effective curation. Our proposed
\emph{DataRater} is an instance of this idea. It estimates the value of
training on any particular data point. This is done by meta-learning using
`meta-gradients', with the objective of improving training efficiency on held
out data. In extensive experiments across a range of model scales and datasets,
we find that using our DataRater to filter data is highly effective, resulting
in significantly improved compute efficiency.

</details>


### [229] [Function Forms of Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2505.17907)
*Ka Long Keith Ho, Yoshinari Takeishi, Junichi Takeuchi*

**主要类别:** stat.ML

**概要:** This paper explores the function space dynamics of a two-layer ReLU neural network in the infinite-width limit, focusing on the Fisher information matrix (FIM) and its role in learning. The authors derive asymptotic behavior for basis functions and validate their findings through simulations, contributing to the theoretical framework for ReLU networks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this research is to understand the function space dynamics of wide neural networks and how the Fisher information matrix (FIM) influences learning. This involves examining the asymptotic behavior of basis functions for approximate eigenvectors and their convergence to distinct forms.

**方法:** The authors extend previous works on approximate eigendecomposition of the FIM to derive the asymptotic behavior of basis functions for four groups of approximate eigenvectors. They show the convergence of these functions to distinct forms and demonstrate that gradient descent prioritizes these functions. Simulations are used to validate the accuracy of these theoretical approximations.

**结果:** The results indicate that the FIM-induced inner products between functions approximate orthogonality in the function space, creating a novel connection between parameter and function spaces. The simulations confirm the practical relevance of these theoretical approximations.

**结论:** This work advances the theoretical framework for ReLU networks by refining the understanding of the function space inner product's role. It provides a robust foundation for understanding wide neural networks and enhances insights into scalable deep learning architectures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Function+Forms+of+Simple+ReLU+Networks+with+Random+Hidden+Weights，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17907，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17907&send_immediately=true&force_search=false)

**原文摘要:** We investigate the function space dynamics of a two-layer ReLU neural network
in the infinite-width limit, highlighting the Fisher information matrix (FIM)'s
role in steering learning. Extending seminal works on approximate
eigendecomposition of the FIM, we derive the asymptotic behavior of basis
functions ($f_v(x) = X^{\top} v $) for four groups of approximate eigenvectors,
showing their convergence to distinct function forms. These functions,
prioritized by gradient descent, exhibit FIM-induced inner products that
approximate orthogonality in the function space, forging a novel connection
between parameter and function spaces. Simulations validate the accuracy of
these theoretical approximations, confirming their practical relevance. By
refining the function space inner product's role, we advance the theoretical
framework for ReLU networks, illuminating their optimization and expressivity.
Overall, this work offers a robust foundation for understanding wide neural
networks and enhances insights into scalable deep learning architectures,
paving the way for improved design and analysis of neural networks.

</details>


### [230] [M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model](https://arxiv.org/abs/2505.17917)
*Xingyu Li, Qing Liu, Tony Jiang, Hong Amy Xia, Brian P. Hobbs, Peng Wei*

**主要类别:** stat.ML

**概要:** 提出了一种新的方法M-learner，用于估计异质间接和总体治疗效果，并在中介框架内识别相关亚组。实验结果验证了该框架的稳健性和有效性。


<details>
  <summary>更多</summary>
  
**动机:** 需要一种方法来捕捉存在中介情况下的治疗效果异质性，并识别相关亚组。

**方法:** 1. 计算个体水平的条件平均间接/总体治疗效果；2. 构建基于成对差异的距离矩阵；3. 使用tSNE将矩阵投影到低维欧几里得空间，并使用K-means聚类识别亚组结构；4. 使用基于阈值的过程校准和细化聚类以确定最佳配置。

**结果:** 实验结果验证了该框架的稳健性和有效性。在Jobs II数据集上的应用展示了该方法的广泛适应性和潜在适用性。

**结论:** M-learner是第一个专门设计用于捕捉存在中介情况下的治疗效果异质性的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M-learner%3AA+Flexible+And+Powerful+Framework+To+Study+Heterogeneous+Treatment+Effect+In+Mediation+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17917，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17917&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel method, termed the M-learner, for estimating heterogeneous
indirect and total treatment effects and identifying relevant subgroups within
a mediation framework. The procedure comprises four key steps. First, we
compute individual-level conditional average indirect/total treatment effect
Second, we construct a distance matrix based on pairwise differences. Third, we
apply tSNE to project this matrix into a low-dimensional Euclidean space,
followed by K-means clustering to identify subgroup structures. Finally, we
calibrate and refine the clusters using a threshold-based procedure to
determine the optimal configuration. To the best of our knowledge, this is the
first approach specifically designed to capture treatment effect heterogeneity
in the presence of mediation. Experimental results validate the robustness and
effectiveness of the proposed framework. Application to the real-world Jobs II
dataset highlights the broad adaptability and potential applicability of our
method.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.

</details>


### [231] [The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks](https://arxiv.org/abs/2505.17958)
*Vittorio Erba, Emanuele Troiani, Lenka Zdeborová, Florent Krzakala*

**主要类别:** stat.ML

**概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Nuclear+Route%3A+Sharp+Asymptotics+of+ERM+in+Overparameterized+Quadratic+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.17958，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.17958&send_immediately=true&force_search=false)

**原文摘要:** We study the high-dimensional asymptotics of empirical risk minimization
(ERM) in over-parametrized two-layer neural networks with quadratic activations
trained on synthetic data. We derive sharp asymptotics for both training and
test errors by mapping the $\ell_2$-regularized learning problem to a convex
matrix sensing task with nuclear norm penalization. This reveals that capacity
control in such networks emerges from a low-rank structure in the learned
feature maps. Our results characterize the global minima of the loss and yield
precise generalization thresholds, showing how the width of the target function
governs learnability. This analysis bridges and extends ideas from spin-glass
methods, matrix factorization, and convex optimization and emphasizes the deep
link between low-rank matrix sensing and learning in quadratic neural networks.

</details>


### [232] [Anytime-valid, Bayes-assisted,Prediction-Powered Inference](https://arxiv.org/abs/2505.18000)
*Valentin Kilian, Stefano Cortinovis, François Caron*

**主要类别:** stat.ML

**概要:** 给定大量未标记数据和少量标记数据，预测增强推理（PPI）利用机器学习预测来提高基于仅标记数据的标准置信区间程序的统计效率，同时保留其固定时间的有效性。本文将PPI框架扩展到顺序设置，其中标记和未标记数据集随时间增长。通过利用Ville的不等式和混合方法，我们提出了预测增强置信序列程序，这些程序在时间上是统一有效的，并自然适应有关预测质量的先验知识以进一步提高效率。我们详细说明了方法背后的设计选择，并在实际和合成示例中展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的PPI框架主要针对静态数据集，但在许多实际场景中，数据会随着时间动态增加，因此需要一个能够处理随时间增长的数据集的方法。

**方法:** 扩展PPI框架至顺序设置，利用Ville的不等式和混合方法构建预测增强置信序列程序，该程序可以适应预测质量的先验知识。

**结果:** 理论和实验证明，所提出的顺序PPI方法可以在时间上保持统一有效性，并且通过结合预测质量的先验知识，显著提高了统计效率。

**结论:** 本文提出的预测增强置信序列程序成功地将PPI框架扩展到顺序设置中，在处理随时间增长的数据集时，既保证了时间上的有效性，又提高了统计效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Anytime-valid%2C+Bayes-assisted%2CPrediction-Powered+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18000，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18000&send_immediately=true&force_search=false)

**原文摘要:** Given a large pool of unlabelled data and a smaller amount of labels,
prediction-powered inference (PPI) leverages machine learning predictions to
increase the statistical efficiency of standard confidence interval procedures
based solely on labelled data, while preserving their fixed-time validity.
  In this paper, we extend the PPI framework to the sequential setting, where
labelled and unlabelled datasets grow over time.
  Exploiting Ville's inequality and the method of mixtures, we propose
prediction-powered confidence sequence procedures that are valid uniformly over
time and naturally accommodate prior knowledge on the quality of the
predictions to further boost efficiency.
  We carefully illustrate the design choices behind our method and demonstrate
its effectiveness in real and synthetic examples.

</details>


### [233] [Bayesian Deep Learning for Discrete Choice](https://arxiv.org/abs/2505.18077)
*Daniel F. Villarraga, Ricardo A. Daziano*

**主要类别:** stat.ML

**概要:** 本论文提出了一种结合近似贝叶斯推断方法的深度学习模型架构，适用于离散选择建模。该模型在数据有限时退化为行为知情假设以减少过拟合，并在数据充足时保持捕捉复杂非线性关系的能力。通过蒙特卡洛模拟研究和两个实证案例，展示了其预测和推理性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统离散选择模型（DCMs）虽然具有高可解释性和经济量估计的支持，但在预测任务中表现不如深度学习（DL）模型。然而，DL模型由于缺乏可解释性、参数估计不稳定以及不确定性量化方法不足，在离散选择领域尚未得到充分利用。

**方法:** 作者设计了一种专门用于离散选择建模的深度学习模型架构，该架构可以与近似贝叶斯推断方法（如随机梯度朗之万动力学SGLD）集成。当数据有限时，模型退化为行为知情假设，从而缓解过拟合和不稳定性；而在数据充足时，模型能够捕捉复杂的非线性关系。

**结果:** 通过蒙特卡洛模拟研究，作者评估了模型的预测指标（如样本外平衡准确率）和推理指标（如边际替代率区间估计的经验覆盖率）。此外，通过两个实证案例（纽约市的出行方式选择数据和瑞士火车选择声明偏好数据），进一步验证了模型的有效性。

**结论:** 所提出的深度学习模型架构不仅在预测任务中表现出色，同时保留了传统DCM的推理能力。这种模型在数据有限时避免了过拟合，并且在数据丰富时能够灵活地捕捉复杂的非线性关系，为离散选择建模提供了一个新的有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Deep+Learning+for+Discrete+Choice，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18077&send_immediately=true&force_search=false)

**原文摘要:** Discrete choice models (DCMs) are used to analyze individual decision-making
in contexts such as transportation choices, political elections, and consumer
preferences. DCMs play a central role in applied econometrics by enabling
inference on key economic variables, such as marginal rates of substitution,
rather than focusing solely on predicting choices on new unlabeled data.
However, while traditional DCMs offer high interpretability and support for
point and interval estimation of economic quantities, these models often
underperform in predictive tasks compared to deep learning (DL) models. Despite
their predictive advantages, DL models remain largely underutilized in discrete
choice due to concerns about their lack of interpretability, unstable parameter
estimates, and the absence of established methods for uncertainty
quantification. Here, we introduce a deep learning model architecture
specifically designed to integrate with approximate Bayesian inference methods,
such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model
collapses to behaviorally informed hypotheses when data is limited, mitigating
overfitting and instability in underspecified settings while retaining the
flexibility to capture complex nonlinear relationships when sufficient data is
available. We demonstrate our approach using SGLD through a Monte Carlo
simulation study, evaluating both predictive metrics--such as out-of-sample
balanced accuracy--and inferential metrics--such as empirical coverage for
marginal rates of substitution interval estimates. Additionally, we present
results from two empirical case studies: one using revealed mode choice data in
NYC, and the other based on the widely used Swiss train choice stated
preference data.

</details>


### [234] [Scalable Policy Maximization Under Network Interference](https://arxiv.org/abs/2505.18118)
*Aidan Gleich, Eric Laber, Alexander Volfovsky*

**主要类别:** stat.ML

**概要:** 这篇论文研究了在动态网络中存在干扰的情况下，如何学习最优策略的问题。通过假设干扰结构，奖励变得线性，从而提出了一种可扩展的汤普森采样算法，该算法在每轮观察到新的n节点网络时最大化策略影响。证明了贝叶斯后悔界在n和轮数上是次线性的，并通过模拟实验表明该算法学习速度快且优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 多臂赌博机算法在需要顺序分配干预措施（如临床试验中的疫苗或在线市场中的优惠券）时表现出色。然而，在一个个体的治疗状态会影响其他个体的结果的情况下（即存在干扰），标准的独立性假设失效。现有的处理此问题的方法需要对同一固定网络进行重复观测，并且在样本规模上难以扩展超过15个连接单元，这限制了其应用。

**方法:** 作者展示了在常见的干扰结构假设下，奖励变得线性。基于此，开发了一种可扩展的汤普森采样算法，该算法在每轮观察到一个新的n节点网络时，能够最大化策略的影响。

**结果:** 证明了一个贝叶斯后悔界，该界在n（节点数量）和轮数上是次线性的。模拟实验显示，该算法学习迅速，并且优于现有方法。

**结论:** 这些结果弥合了干扰因果推断方法和实用的多臂赌博机算法之间的关键可扩展性差距，使得在大规模网络系统中进行策略优化成为可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Policy+Maximization+Under+Network+Interference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.18118，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.18118&send_immediately=true&force_search=false)

**原文摘要:** Many interventions, such as vaccines in clinical trials or coupons in online
marketplaces, must be assigned sequentially without full knowledge of their
effects. Multi-armed bandit algorithms have proven successful in such settings.
However, standard independence assumptions fail when the treatment status of
one individual impacts the outcomes of others, a phenomenon known as
interference. We study optimal-policy learning under interference on a dynamic
network. Existing approaches to this problem require repeated observations of
the same fixed network and struggle to scale in sample size beyond as few as
fifteen connected units -- both limit applications. We show that under common
assumptions on the structure of interference, rewards become linear. This
enables us to develop a scalable Thompson sampling algorithm that maximizes
policy impact when a new $n$-node network is observed each round. We prove a
Bayesian regret bound that is sublinear in $n$ and the number of rounds.
Simulation experiments show that our algorithm learns quickly and outperforms
existing methods. The results close a key scalability gap between causal
inference methods for interference and practical bandit algorithms, enabling
policy optimization in large-scale networked systems.

</details>
