<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 95]
- [cs.AI](#cs.AI) [总数: 12]
- [stat.ML](#stat.ML) [总数: 7]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain, Ehsan Saghapour, Kevin Song, Jake Y. Chen*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于Llama 3的抗体-抗原结合亲和力预测模型LlamaAffinity，它在性能上超越了现有技术，并且训练效率更高。


<details>
  <summary>更多</summary>
  
**动机:** 为了改进传统的耗时且昂贵的抗体亲和力测量实验方法，并利用人工智能尤其是大型语言模型的进步来设计基于AI的抗体并提高亲和力预测的准确性。

**方法:** 使用开源的Llama 3作为基础架构，结合来自Observed Antibody Space (OAS)数据库的抗体序列数据，开发了LlamaAffinity模型来预测抗体-抗原结合亲和力。

**结果:** LlamaAffinity模型在多个评估指标上表现出色，包括准确率0.9640、F1分数0.9643、精确度0.9702、召回率0.9586以及AUC-ROC 0.9936。此外，该模型的平均累积训练时间仅为0.46小时，远低于先前研究。

**结论:** LlamaAffinity模型在抗体-抗原结合亲和力预测方面优于现有的最先进方法，并且具有更高的计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Llama-Affinity%3A+A+Predictive+Antibody+Antigen+Binding+Model+Integrating+Antibody+Sequences+with+Llama3+Backbone+Architecture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09052&send_immediately=true&force_search=false)

**原文摘要:** Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [2] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen, Mingxi Zou, Zhuo Wang, Qifan Wang, Dongning Sun, Chi Zhang, Zenglin Xu*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为FinHEAR的多代理框架，旨在提高语言模型在金融决策中的表现。它通过结合行为经济学原理、专家指导的检索、基于信心的位置调整以及基于结果的优化来增强解释性和稳健性。实验表明，FinHEAR在趋势预测和交易任务上优于强大的基线模型，表现出更高的准确性和更好的风险调整回报。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）虽然展示了强大的通用推理能力，但在捕捉人类金融决策的核心行为模式方面往往表现不佳，比如在信息不对称下的专家依赖、损失规避敏感度及基于反馈的时间调整等。

**方法:** FinHEAR是一个多代理框架，用于人类专业知识和适应性风险意识推理。该框架组织了专门的基于LLM的代理来分析历史趋势、解读当前事件，并在一个以事件为中心的工作流程中获取由专家提供信息的先例。此外，FinHEAR还融入了从行为经济学中得到灵感的方法，包括专家指导的检索、根据信心调整仓位大小以及基于结果的改进。

**结果:** 在精心策划的金融数据集上的实证结果显示，FinHEAR在趋势预测和交易任务中始终超越了强劲的基准，实现了更高的准确率和更优的风险调整收益。

**结论:** FinHEAR框架通过整合专业领域知识与适应性风险管理策略，在金融决策场景下为语言模型提供了显著改进，特别是在处理需要时间推理、适应性风险评估及对动态事件作出响应的情况时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FinHEAR%3A+Human+Expertise+and+Adaptive+Risk-Aware+Temporal+Reasoning+for+Financial+Decision-Making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09080&send_immediately=true&force_search=false)

**原文摘要:** Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [3] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang, Liang Wu, Yanjie Fu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于用户反馈的奖励机制微调大语言模型的方法PageLLM，用于优化搜索和推荐结果呈现。通过结合页面级别和项目级别的奖励，该方法在公共数据集和工业数据集上都表现出色，并在线上A/B测试中实现了0.44% GMV的增长。


<details>
  <summary>更多</summary>
  
**动机:** 全文优化（WPO）对于提高用户体验和参与度至关重要。然而，预训练的大规模语言模型（LLMs）虽然能够生成连贯且与上下文相关的文本，但在针对复杂任务如WPO进行微调时面临挑战，特别是需要大量人工标注的数据来减少幻觉和模型不稳定性问题。

**方法:** 为了解决使用用户反馈作为监督信号来微调LLMs进行WPO的问题，研究者提出了PageLLM，这是一种基于奖励的微调方法。它采用了混合粒度奖励机制，结合了页面级别奖励（评估整体质量和一致性）和项目级别奖励（关注关键推荐的准确性和相关性）。

**结果:** PageLLM在公开数据集以及实际工业应用中的表现优于基线模型，并在一个涉及超过1000万用户的在线A/B测试中，带来了0.44% GMV增长的实际影响。

**结论:** PageLLM通过利用用户反馈作为监督信号并采用双奖励结构成功解决了WPO中的挑战，不仅优化了整个页面展示效果还提升了单个重要元素的质量，在大规模系统中展现了其实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhanced+Whole+Page+Optimization+via+Mixed-Grained+Reward+Mechanism-Adapted+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09084&send_immediately=true&force_search=false)

**原文摘要:** Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [4] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang, Haoyue Bai, Nanxu Gong, Wangyang Ying, Sixun Dong, Xiquan Cui, Yanjie Fu*

**主要类别:** cs.LG

**AI概要:** 提出了一种结合大语言模型（LLMs）的符号生成和机器学习（ML）梯度优化的团队框架，以解决特征转换中稳定性和有效性的问题。实验显示该方法在下游任务性能上提高了5%，同时错误情况减少近半。


<details>
  <summary>更多</summary>
  
**动机:** 特征转换通过从原始数据派生新特征来增强数据表示。虽然生成式AI在此任务中展示了潜力，但面临着稳定生成和有效生成的挑战。现有的方法要么无法保证语法的有效性，要么表现不稳定。

**方法:** 一种结合了大语言模型（LLMs）的符号生成能力和机器学习（ML）梯度优化搜索的团队框架。该框架包括四个步骤：(1) 生成黄金样例；(2) 特征转换序列嵌入与搜索；(3) 学生LLM进行特征转换；(4) LLM-ML解码器协作。

**结果:** 实验结果表明，该团队策略可以在各种数据集上实现下游性能5%的提升，并且几乎减少了近一半的错误案例。此外，研究还发现LLMs具有理解原始数据的能力。

**结论:** 本研究提出的结合LLMs和ML的团队框架不仅解决了特征转换中的稳定性和有效性问题，而且在效率和鲁棒性方面也表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-ML+Teaming%3A+Integrated+Symbolic+Decoding+and+Gradient+Search+for+Valid+and+Stable+Generative+Feature+Transformation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09085，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09085&send_immediately=true&force_search=false)

**原文摘要:** Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [5] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard, Giulia Mezzadri, Patricia Reynaud-Bouret, Etienne Tanré*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种生物上可信的脉冲神经网络(SNN)模型，用于决策制定，并结合了学习机制。通过建立漂移扩散模型(DDM)和泊松计数器模型之间的联系，证明了这两种模型提供了类似的分类和反应时间，并且DDM可以由发出脉冲的泊松神经元近似。此外，研究显示一个具有相关噪声的特定DDM可以从受局部学习规则支配的霍克斯网络中推导出来。为了评估模型预测，设计了一个在线分类任务。


<details>
  <summary>更多</summary>
  
**动机:** 现有的认知模型（如漂移扩散模型）和生物学模型（如泊松计数器模型）缺乏学习机制，且局限于参与者事先知道类别的任务。为了填补认知模型与生物模型之间的空白，作者提出了一个包含学习机制的SNN模型，旨在整合生物相关的神经机制到认知模型中，以增进对神经活动与行为之间关系的理解。

**方法:** 首先，展示了漂移扩散模型和泊松计数器模型之间的耦合结果，表明两者提供相似的分类和反应时间，并且漂移扩散模型可以通过发放脉冲的泊松神经元来近似；接着，展示了一个特殊的、具有相关噪声的漂移扩散模型如何从遵循局部学习规则的霍克斯过程网络中的尖峰神经元衍生而来；最后，设计并实施了一个在线分类任务来测试所提出的模型。

**结果:** 研究表明，漂移扩散模型能够被基于泊松过程的尖峰神经元有效近似，并且含有相关噪声的漂移扩散模型可从霍克斯过程网络中得出。此外，在线分类任务的结果支持了模型的有效性。

**结论:** 本研究为理解神经活动与行为之间的关系做出了重要贡献，通过引入一种新的SNN模型，它不仅模仿了大脑的决策过程，还包含了学习机制。这项工作促进了将生物相关的神经机制整合进认知模型的努力，有助于更深入地理解大脑功能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spiking+Neural+Models+for+Decision-Making+Tasks+with+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09087&send_immediately=true&force_search=false)

**原文摘要:** In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [6] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan, Nuria Gomez Blas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种增强的异步AdaBoost框架，用于联邦学习，并在五个不同领域进行了应用。通过引入自适应通信调度和延迟权重补偿机制，该框架减少了同步频率和通信开销，同时保持或提高了模型准确性。实证结果表明，与基准AdaBoost相比，训练时间减少了20-35%，通信开销降低了30-40%。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于改进联邦学习中模型训练的效率和鲁棒性，特别是在减少通信开销的同时保持或提高模型准确性。

**方法:** 采用的方法是开发一种增强型异步AdaBoost算法，该算法结合了自适应通信调度和延迟权重补偿技术。

**结果:** 结果显示，在多个领域内，新的AdaBoost框架显著提高了效率和鲁棒性，比如训练时间和通信开销分别减少了20-35%和30-40%。

**结论:** 结论指出，增强版AdaBoost在不同的联邦学习场景下表现出更高的效率和鲁棒性，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Asynchronous+AdaBoost+into+Federated+Learning%3A+Five+Real+World+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09090，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09090&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [7] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson, Igor Oliveira, Amenah Al-Najafi, Fode Zhang, Hon Keung Tony Ng*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于耦合自由能的变分推断优化框架，该方法可以处理重尾分布，并通过在CVAE中应用来提高模型的准确性和鲁棒性。实验结果表明，在CelebA数据集上，相比VAE，CVAE在5个训练周期后性能提高了3%。


<details>
  <summary>更多</summary>
  
**动机:** 为了扩展变分推断技术以考虑耦合指数族的弯曲几何特性，并提高所学模型的准确性和鲁棒性。

**方法:** 引入了基于耦合自由能的优化框架，该框架等同于倒置概率的耦合证据下界（ELBO）。同时，推广了Fisher信息度量和仿射连接，并应用于设计耦合变分自动编码器（CVAE）。通过使用耦合概率对重尾潜在分布进行采样，修改了均方平均损失中的常数。

**结果:** 通过使用具有更快衰减尾部的关联耦合概率对重尾潜在分布进行采样，使得模型能够对尾部施加高惩罚，同时保证训练样本中异常值数量减少。CelebA图像重构的Wasserstein-2或Fréchet初始距离显示，CVAE相比VAE在5个训练周期后的性能提高了3%。

**结论:** 提出的基于耦合自由能的变分推断优化框架提高了模型的准确性和鲁棒性，尤其适用于包含重要重尾分布的情况。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Variational+Inference+Optimized+Using+the+Curved+Geometry+of+Coupled+Free+Energy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09091，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09091&send_immediately=true&force_search=false)

**原文摘要:** We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [8] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen, Jiace Zhu, Qi Fan, Yehan Ma, An Zou*

**主要类别:** cs.LG

**AI概要:** 研究提出了一种名为特征搜索与强化（FSR）的新框架，该框架利用大型语言模型（LLMs）自动生成和优化CUDA程序，以生成高性能GPU内核。实验结果表明，通过FSR增强的LLMs不仅能够保证代码的正确性，还能使自动生成的内核在执行速度上比人类编写的通用代码快高达179倍。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在通用代码生成方面表现出了强大的能力，但对于生成高度依赖硬件特性的、架构感知的以及性能关键的代码，特别是针对大规模并行GPU的代码，仍然是一个复杂的挑战。

**方法:** 提出了一个名为特征搜索与强化（FSR）的新框架，它联合优化了编译和功能正确性以及运行时性能，并通过广泛的测试案例验证这些属性，同时通过实际的目标GPU内核执行延迟来衡量性能。

**结果:** 实验结果显示，使用FSR增强的LLMs可以持续保证代码的正确性，并且自动生成的内核在执行速度上最高可达到人工编写代码的179倍。

**结论:** 结合大型语言模型与性能强化的方法显示出了自动化GPU编程的潜力，尤其是在硬件特定、架构敏感和性能关键的应用中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CUDA-LLM%3A+LLMs+Can+Write+Efficient+CUDA+Kernels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09092&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [9] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko, Alexander Panfilov, Vaclav Voracek, Matthias Hein, Jonas Geiping*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种统一的威胁模型，用于评估针对安全调优的大规模语言模型（LLMs）的越狱攻击。通过构建一个基于1T标记的N-gram语言模型，该研究提供了一个与具体LLM无关、非参数化且易于解释的评价方法，并首次在同等条件下对不同攻击进行了基准测试。


<details>
  <summary>更多</summary>
  
**动机:** 面对多种多样的越狱攻击手段，这些方法虽然能够在原始设定下成功诱导出有害回应，但它们在流畅性和计算成本上存在很大差异。因此，需要一种原则性的比较框架来评估这些攻击方式。

**方法:** 作者们开发了一个基于1T个令牌训练的N-gram语言模型，该模型不同于基于模型的困惑度测量，允许进行与具体大规模语言模型无关、非参数化并且本质上可解释的评估。接着，他们将流行的攻击方法适应于这个新的威胁模型中，并在此基础上首次公平地对比了各种攻击的效果。

**结果:** 经过广泛比较后发现，对于现代的安全调优模型来说，实际攻击成功率低于先前报道；而且基于离散优化的攻击比最近提出的基于大规模语言模型的攻击表现更好。此外还观察到，有效的攻击往往利用和滥用不常见的双词组合，选择那些在现实世界文本中不存在或非常罕见的双词组合，例如特定于Reddit或代码数据集中的双词。

**结论:** 这项工作通过提出一种新型威胁模型，为越狱攻击提供了全面分析和比较的基础。它不仅揭示了现有攻击技术的实际效果，也指出了有效攻击的一些关键特征，如利用罕见的双词组合等。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Interpretable+N-gram+Perplexity+Threat+Model+for+Large+Language+Model+Jailbreaks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2410.16222，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2410.16222&send_immediately=true&force_search=false)

**原文摘要:** A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [10] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés, Daniel Mas Montserrat, Kapal Dev, Alexander G. Ioannidis*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为特征移位定位网络（FSL-Net）的神经网络，它可以快速准确地在大型和高维数据集中定位特征移位。该网络经过大量数据集训练后，能够从先前未见过的数据集和移位中定位特征移位，而无需重新训练。


<details>
  <summary>更多</summary>
  
**动机:** 由于不同步的异构数据源、噪声数据测量或不一致的处理和标准化流程可能导致错误特征的出现，在医疗保健、生物医学、社会经济、金融、调查及多传感器数据等许多应用中都存在数据源之间的特征移位问题。定位移位特征对于解决移位的根本原因并修正或过滤数据以避免损害下游分析非常重要。

**方法:** 作者们引入了特征移位定位网络（FSL-Net），这是一种能够在大型和高维数据集中迅速且精确地定位特征移位的神经网络。该网络通过大量数据集进行训练，学习提取数据集的统计属性，并且能够对之前未见过的数据集和特征移位进行定位，无需再次训练。

**结果:** FSL-Net 能够有效地识别导致分布偏移的具体特征，并且可以在没有额外训练的情况下应用于新的数据集上。这意味着它为处理大规模和高维度数据中的特征移位提供了一个既准确又可扩展的解决方案。

**结论:** FSL-Net 提供了一种快速且准确的方法来定位大型和高维数据集中的特征移位，解决了现有方法不够准确或无法扩展到大数据集的问题。此外，提供了代码和即用型训练模型，方便其他研究人员使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature+Shift+Localization+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09101&send_immediately=true&force_search=false)

**原文摘要:** Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [11] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang, Hongkang Li, Changlong Shi, Guowei Rong, He Zhao, Dongsheng Wang, Dandan Guo, Meng Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法LwPTV，通过构建显著性评分来度量任务向量中参数的冗余性，并进行逐层剪枝以保持预训练模型参数。该方法可以提高多任务学习模型在领域外（OOD）数据集上的性能，同时保留其在领域内（ID）任务上的能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多任务学习（MTL）中的模型合并方法主要关注于提升领域内（ID）数据集的表现，而忽视了它们在领域外（OOD）数据集上的有效性。

**方法:** 提出了LwPTV（Layer-wise Pruning Task Vector），通过建立一个显著性评分来衡量任务向量中参数的冗余程度，并据此为每个任务生成掩码向量，执行逐层的任务向量剪枝，只保留合并模型相应层的预训练模型参数。

**结果:** 大量实验表明，应用我们的方法后，在保持领域内（ID）任务表现的同时，领域外（OOD）任务的表现有了实质性提高。

**结论:** 所提出的方法LwPTV能够与大多数现有的模型合并方法无缝集成，从而改善它们在OOD任务上的表现，同时维持原有的ID任务的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Merging+Smarter%2C+Generalizing+Better%3A+Enhancing+Model+Merging+on+OOD+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09093&send_immediately=true&force_search=false)

**原文摘要:** Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [12] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson, Jhonathan Navott, Piotr Grynfelder, Mengyan Zhang, Makkunda Sharma, Elizaveta Semenova, Seth Flaxman*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的架构BSA-TNP，它通过引入核回归块、群不变注意力偏差和内存高效的偏置扫描注意力机制来提高神经过程模型的准确性和可扩展性，同时保持平移不变性，并能够处理高维固定效应。


<details>
  <summary>更多</summary>
  
**动机:** 随着神经过程(NPs)模型应用于更复杂的数据密集型领域，如地质学、流行病学、气候和机器人技术，对这些模型的可扩展性提出了更高的要求。现有的一些架构为了实现可扩展性而牺牲了准确性。本文旨在展示这种权衡往往是不必要的，特别是在建模完全或部分平移不变的过程时。

**方法:** 研究者们提出了一种名为Biased Scan Attention Transformer Neural Process (BSA-TNP)的新架构，该架构包含Kernel Regression Blocks (KRBlocks)、群不变注意偏差以及内存效率高的Biased Scan Attention (BSA)。

**结果:** BSA-TNP能够在保持甚至超越最佳模型准确度的同时，通常只需要一小部分训练时间；展现平移不变性，允许多分辨率学习；透明地模拟空间和时间演变的过程；支持高维固定效应；并且优雅地扩展——在单个24GB GPU上，使用超过100万个测试点与10万个上下文点进行推理所需时间不到一分钟。

**结论:** BSA-TNP架构为神经过程提供了一个通用且高效的选择，它不仅提高了处理复杂任务的能力，还改善了模型的可扩展性与计算效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Spatiotemporal+Inference+with+Biased+Scan+Attention+Transformer+Neural+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09163，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09163&send_immediately=true&force_search=false)

**原文摘要:** Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [13] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou, Shunyu Liu, Zengmao Wang, Di Wang, Rong-Cheng Tu, Bo Du, Dacheng Tao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种利用生成概率在响应轨迹中建立奖励一致性方法，通过引入轨迹内一致性正则化来提高奖励模型的性能，从而改善了DPO对齐策略和BON推理时验证结果。


<details>
  <summary>更多</summary>
  
**动机:** 当前的奖励建模通常依赖于整体响应得分来学习响应的结果奖励，但这种粗粒度的监督信号使得奖励模型难以识别出真正与分数相关的特定组件，导致对未见响应的泛化能力较差。

**方法:** 本文提出的方法是利用生成概率在响应轨迹中的过程之间建立奖励一致性，使响应级别的监督信号能够跨过程传播，提供额外的细粒度信号用于奖励学习，并基于贝叶斯框架下的分析开发了轨迹内一致性正则化，以确保具有更高下一个令牌生成概率的相邻过程保持更一致的奖励。

**结果:** 应用所提出的正则化到先进的结果奖励模型上，在RewardBench上提高了其性能；并且表明使用所提议正则化训练的奖励模型诱导了更好的DPO对齐策略，并达到了更好的BON推理时验证结果。

**结论:** 通过引入生成概率的一致性以及轨迹内正则化，可以提高奖励模型对于未见响应的学习效果，进而改进语言模型的反馈学习机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Intra-Trajectory+Consistency+for+Reward+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09096，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09096&send_immediately=true&force_search=false)

**原文摘要:** Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [14] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus, Michael U. Gutmann*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的通用缺失数据填补方法——基于条件流匹配的填补法（CFMI），该方法结合了连续归一化流、流匹配和共享条件建模来处理传统多重填补方法中的棘手问题。研究表明，CFMI在24个中小型表格数据集上与九种经典及最新填补方法相比，其性能在多个指标上相匹配或更优。此外，应用于时间序列数据的零样本填补时，CFMI不仅达到了相关扩散方法的准确性，还在计算效率方面表现更佳。总体而言，CFMI对于低维数据至少与传统方法一样好，并且能够扩展到高维设置中，与其它基于深度学习的方法相比，在性能上持平甚至超越。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决传统多重填补方法中存在的难以处理的问题，同时寻找一种适用于从小型到大型维度的各种类型数据的有效填补技术。

**方法:** 提出了一个名为CFMI的新方法，该方法整合了连续归一化流、流匹配以及共享条件建模技术。

**结果:** CFMI在24个中小型表格数据集上的测试表明，它在广泛使用的评价指标上能够匹敌甚至优于九种现有方法；当用于时间序列数据填补时，准确度与另一扩散方法相当，但计算效率更高。

**结论:** CFMI作为一种新型的数据填补方法，不仅对低维数据有效，而且能够很好地适应高维场景，成为处理不同类型和规模数据的理想选择之一。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CFMI%3A+Flow+Matching+for+Missing+Data+Imputation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09258，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09258&send_immediately=true&force_search=false)

**原文摘要:** We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [15] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron, Devin White*

**主要类别:** cs.LG

**AI概要:** 研究通过训练有限容量的Transformer模型在两个合成字符级任务上，探讨了大型语言模型中记忆与泛化之间的关系。结果显示小型模型能够进行算术推断但无法记住事实，而大型模型则相反。当同时训练两个任务时，所有模型都无法成功进行推断。


<details>
  <summary>更多</summary>
  
**动机:** 探索大型语言模型中记忆和泛化之间的关系，并且这种关系如何受到模型容量的影响。

**方法:** 从头开始预训练一系列容量受限的Transformer模型，在两个设计用于分别测试泛化（通过算术外推）和记忆（通过事实回忆）的合成字符级任务上。

**结果:** 观察到一个一致的权衡：小模型可以外推到未见过的算术案例但不能记住事实，而较大的模型可以记住事实但不能外推。当中等容量模型也显示出向记忆化的类似转变时，没有模型（无论大小）能在联合训练两个任务时成功外推。

**结论:** 预训练可能内在地偏向一种学习模式而非另一种。这项研究揭示了模型容量是如何影响学习行为的，并为小型语言模型的设计和部署提供了更广泛的意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Too+Big+to+Think%3A+Capacity%2C+Memorization%2C+and+Generalization+in+Pre-Trained+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09099，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09099&send_immediately=true&force_search=false)

**原文摘要:** The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [16] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt, Max Ruiz Luyten, Antonin Berthon, Mihaela van der Schaar*

**主要类别:** cs.LG

**AI概要:** G-Sim是一个结合了大型语言模型（LLM）的结构设计与严格的经验校准的混合框架，用于自动化构建模拟器。它通过迭代循环来提出和改进模拟器的核心组件和因果关系，并使用灵活的校准技术估计参数，从而处理不可微和随机模拟器，产生可靠的、基于因果信息的模拟器。


<details>
  <summary>更多</summary>
  
**动机:** 现有的模拟器构建方法在超出历史数据进行泛化或使用大型语言模型时存在不准确性和经验对齐性差的问题。为了解决这些问题，研究者提出了G-Sim框架，旨在提供一个更加可靠且能有效指导复杂决策过程的模拟器解决方案。

**方法:** G-Sim采用了一种迭代的方法，利用大型语言模型来建议并精炼模拟器的基本组成部分及其之间的因果联系。此过程由领域知识引导，并通过灵活的校准技术来估计参数值，包括无似然和无梯度优化以及基于仿真的推断等方法。

**结果:** 该研究展示了G-Sim能够有效地结合领域先验知识和实证证据，生成可靠的、具有因果信息的模拟器。这有助于减少数据效率低下的问题，并支持针对复杂决策场景的系统级干预措施。

**结论:** G-Sim框架通过集成领域先验与经验校准克服了现有模拟器构建中的局限性，提供了一个自动化的途径来创建既符合现实又具备良好泛化能力的模拟器，这对诸如医疗保健和物流等关键领域的政策制定有着重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是G-Sim%3A+Generative+Simulations+with+Large+Language+Models+and+Gradient-Free+Calibration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09272，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09272&send_immediately=true&force_search=false)

**原文摘要:** Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [17] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

**主要类别:** cs.LG

**AI概要:** 本文研究了对抗训练中代理风险收敛到最优值的速度，并提供了量化该收敛率的代理风险边界。此外，还推导了在标准（非对抗性）学习设置下的依赖于分布的代理风险边界。


<details>
  <summary>更多</summary>
  
**动机:** 现有工作已经描述了二分类中，何时最小化序列的对抗代理风险同时也是对抗分类风险的最小化序列——即所谓的对抗一致性。然而，这些结果并没有解决对于这样一系列最小化对抗代理风险的函数来说，对抗分类风险以何种速率收敛到其最优值的问题。

**方法:** 通过提供能够量化收敛率的代理风险边界来解决上述问题。同时，在标准学习环境中推导出与分布相关的代理风险边界。

**结果:** 得到了可以衡量对抗分类风险收敛速度的代理风险边界。另外，为标准学习环境中的模型提供了新的、可能独立感兴趣的依赖于数据分布的代理风险边界。

**结论:** 本文填补了关于对抗代理风险和实际对抗分类风险之间收敛速度的知识空白，同时也为常规机器学习任务提供了新的理论见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adversarial+Surrogate+Risk+Bounds+for+Binary+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09348，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09348&send_immediately=true&force_search=false)

**原文摘要:** A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [18] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee, Seungjae Shin, Vinnam Kim, Jaeseong You, An Chen*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的统一渐进量化（UPQ）框架，能够将指令调优的大规模语言模型从FP16逐步量化到INT4再至INT2，同时保持与原始模型的响应一致性。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型（LLM）快速扩展，在资源受限设备上部署面临重大挑战，尤其是对于极低比特量化的兴趣日益增加。尽管先前的研究表明2位大模型在准确性和延迟方面优于其4位小模型，但这些进展仅限于预训练LLM，并未扩展到指令调优模型。

**方法:** 提出了一个名为统一渐进量化（UPQ）的新框架，该框架结合了块级后训练量化（PTQ）和基于蒸馏的量化感知训练（Distill-QAT），用于INT2指令调优LLM量化。首先使用块级PTQ将FP16指令调优模型量化为INT4以减少后续INT2量化引入的量化误差；然后通过最小化两者的广义Jensen-Shannon散度（JSD）来应用Distill-QAT，使INT2指令调优LLM生成与原始FP16版本一致的响应。

**结果:** 实验结果表明，UPQ可以在不依赖专有后训练数据的情况下，将开源指令调优LLM量化为INT2，并且在评估指令调优LLM最具代表性的基准测试MMLU和IFEval上达到了最先进性能。

**结论:** UPQ提供了一个有效的方法来对指令调优的大规模语言模型进行极端低比特量化，同时保持良好的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unifying+Block-wise+PTQ+and+Distillation-based+QAT+for+Progressive+Quantization+toward+2-bit+Instruction-Tuned+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09104，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09104&send_immediately=true&force_search=false)

**原文摘要:** As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [19] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao, Quanchao Lu, Yanfu Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种针对Group SLOPE模型的安全筛选规则，该规则能够有效识别零系数的非活跃组，并在实际高维场景中显著提高计算效率和减少内存使用。


<details>
  <summary>更多</summary>
  
**动机:** 在高维稀疏学习中，特别是存在组结构的情况下，变量选择是一个具有挑战性的问题。虽然Group SLOPE对于自适应地选择预测因子组表现良好，但其块不可分离的组效应使得现有方法要么无效要么低效，导致在实际高维情况下计算成本和内存使用量大。

**方法:** 作者们引入了一种专为Group SLOPE模型设计的安全筛选规则，以解决块不可分离的组效应问题。这个筛选规则可以有效地识别出具有零系数的非活跃组。通过在训练过程中排除这些非活跃组，提高了计算效率并减少了内存使用。

**结果:** 理论分析表明，所提出的筛选规则可以安全地与现有的优化算法结合使用，保证了与原始方法相同的结果。实验结果证实，该方法能够有效地检测到非活跃特征组，并显著提高计算效率而不影响准确性。

**结论:** 通过提出的新筛选规则，研究者们成功地解决了Group SLOPE模型中的计算效率和内存使用问题，从而能够在保持准确性的前提下处理更加复杂的高维数据集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safe+Screening+Rules+for+Group+SLOPE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09451，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09451&send_immediately=true&force_search=false)

**原文摘要:** Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [20] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres, Pranav Deshpande, Archan Ray, Mattia J. Villani, Marco Pistoia, Niraj Kumar*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为MetaTT的统一张量训练适配器框架，用于预训练变换器的全局低秩微调。与LoRA独立微调每个权重矩阵不同，MetaTT使用单个共享TT对所有变换器子模块进行分解，并通过索引结构轴如层和矩阵类型（可选地还包括头和任务）来实现。对于给定的秩，当LoRA添加的参数与模式乘积成比例时，MetaTT只添加与模式之和成比例的参数，从而显著压缩最终适配器。实验表明，MetaTT在保持与LoRA相似准确度的同时，比其他基于张量的方法减少了更多参数。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高预训练变换器模型在进行低秩微调时的效率和效果，研究者希望设计一种新的方法来减少额外引入的参数数量，同时保持或超越现有方法的准确性。

**方法:** 研究者提出了MetaTT，它利用一个共享的Tensor Train (TT)来对所有的Transformer子模块进行因式分解。这种方法通过索引诸如层、矩阵类型等结构性轴线，以及可选的任务和头部，来区别对待不同的子模块。相比于LoRA每次增加与跨模态乘积成比例的参数，MetaTT仅增加了与跨模态总和成比例的参数，这导致了更紧凑的适配器。此外，MetaTT还支持有效的优化算法，比如DMRG风格的秩自适应最小化加上Adam优化器。

**结果:** 实验结果表明，在标准语言建模基准测试中，MetaTT相较于LoRA和其他最近的基于矩阵及张量分解的微调方案，在极大程度上减少了参数量，同时维持了相近甚至更好的准确性。特别是相比CP或其他秩因子分解法，MetaTT得益于成熟的优化过程，使得训练更加简便。

**结论:** MetaTT作为一种新颖且高效的全局低秩微调方法，能够显著降低预训练变换器模型微调时所需的额外参数数量，同时保持良好的性能表现。它的优势在于能够自然地扩展到多任务共享适配器上，而无需重新设计核心张量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MetaTT%3A+A+Global+Tensor-Train+Adapter+for+Parameter-Efficient+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09105，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09105&send_immediately=true&force_search=false)

**原文摘要:** We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [21] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen, Reda Ouhamma, Maryam Kamgarpour*

**主要类别:** cs.LG

**AI概要:** 本文研究了基于人类反馈的强化学习在一般马尔可夫决策过程中的应用，提出了一种基于随机探索的元算法来选择信息量大的偏好查询，并通过批处理和最优实验设计改进了查询复杂度。实证评估表明该方法具有竞争力且需要较少的偏好查询。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决如何在一般的马尔可夫决策过程中，通过轨迹级别的偏好比较让代理学习，同时设计出既能识别潜在奖励又能保证理论担保的算法。

**方法:** 提出了一个基于随机探索的元算法，避免了乐观方法带来的计算挑战并保持了可操作性。为了提高查询效率，引入了一种改进算法，它收集轨迹对批次并运用最优实验设计来选择信息丰富的比较查询。

**结果:** 建立了在温和的强化学习预言假设下的遗憾和最终迭代保证。批量结构还允许并行化偏好查询，这在实际部署中是有意义的，因为可以同时收集反馈。

**结论:** 实证评价证实了所提方法与基于奖励的强化学习相比具有竞争力，同时只需要少量的偏好查询。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Preference-Based+Reinforcement+Learning%3A+Randomized+Exploration+Meets+Experimental+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09508，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09508&send_immediately=true&force_search=false)

**原文摘要:** We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [22] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, Girish Narayanswamy, Maxwell A. Xu, Ahmed A. Metwally, Shawn Xu, Jake Garrison, Xuhai Xu, Tim Althoff, Yun Liu, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Cecilia Mascolo, Xin Liu, Daniel McDuff, Yuzhe Yang*

**主要类别:** cs.LG

**AI概要:** 本文介绍了SensorLM，一种能够使用自然语言理解可穿戴传感器数据的基础模型家族。通过引入一个分层的标题生成流程来捕捉传感器数据中的统计、结构和语义信息，从而构建了迄今为止最大的传感器-语言数据集，并在人类活动分析和医疗保健等实际任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 尽管传感器数据无处不在，但与自然语言对齐和解释这些数据仍然具有挑战性，原因在于缺乏配对的、丰富注释的传感器-文本描述，尤其是在未经整理的真实世界可穿戴设备数据中。

**方法:** 研究者们提出了一种分层的标题生成流程，用于从传感器数据中提取统计、结构和语义信息。此外，SensorLM扩展了一些著名的多模态预训练架构（如CLIP、CoCa），并将其作为通用架构下的特定变体进行恢复。

**结果:** SensorLM在零样本识别、少量学习以及跨模态检索等方面超越了现有最先进方法的表现。该模型还展示了包括扩展行为、标签效率、传感器字幕以及对未见任务的零样本泛化在内的有趣能力。

**结论:** SensorLM为理解和利用可穿戴传感器数据提供了一个强大的新工具，它不仅能够处理大量现实世界的数据，而且在多个关键领域显示出卓越的性能和应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SensorLM%3A+Learning+the+Language+of+Wearable+Sensors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09108，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09108&send_immediately=true&force_search=false)

**原文摘要:** We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [23] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee, Sehee Lim, Kibok Lee, Jy-yong Sohn*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个基于分析正负样本对之间余弦相似性的统一框架来理解对比学习（CL），揭示了全批量设置下完美对齐正样本对的不可达性以及小批量设置下负样本对间相似性方差过大的问题，并引入了一个辅助损失项来减少负样本对相似性的方差，从而在小批量训练中提高CL方法的表现。


<details>
  <summary>更多</summary>
  
**动机:** 虽然已经提出了多种形式的对比损失并从不同角度进行了分析，但先前的工作缺乏一个能够系统解释这些目标的综合性框架。

**方法:** 本文通过分析正负样本对之间的余弦相似性，提出了一种新的统一框架来理解对比学习。对于全批量场景，研究了当负样本对的相似度低于某一阈值时正样本对的对齐情况；对于小批量场景，则展示了较小的批次大小如何导致批内负样本对之间更强的分离，进而导致负样本对相似度的更大方差。为了解决这个问题，引入了一个额外的损失项以减少负样本对相似度的方差。

**结果:** 实验结果表明，在小批量训练中加入提出的损失项可以一致地提高对比学习方法的表现。

**结论:** 本研究提供的统一框架有助于更好地理解对比学习，并且提出的用于减少负样本对相似性方差的方法能有效提升小批量训练中的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Similarities+of+Embeddings+in+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09781&send_immediately=true&force_search=false)

**原文摘要:** Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [24] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma, Feng Wu, Qika Lin, Yucheng Xing, Chenyu Liu, Ziyu Jia, Mengling Feng*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为CodeBrain的新型EEG基础模型，通过TFDual-Tokenizer和EEGSSM来解决现有模型在异构表示能力和多尺度脑依赖性捕捉上的不足。


<details>
  <summary>更多</summary>
  
**动机:** 当前EEG基础模型存在异构表示能力有限及捕捉多尺度脑依赖性效率低的问题，这限制了它们在不同任务中的迁移能力。

**方法:** 设计了一个两阶段训练的CodeBrain模型，第一阶段使用TFDual-Tokenizer独立地对时域和频域成分进行分词以扩展离散表示空间；第二阶段提出了结合全局卷积架构与滑动窗口注意力机制的EEGSSM，用于建模稀疏长距离和局部依赖关系。

**结果:** 在10个公开EEG数据集上进行了全面实验，证明了CodeBrain通过线性探测具有良好的泛化性能。

**结论:** CodeBrain为未来神经科学研究提供了生物信息学支持且可解释的EEG建模方法，并计划在未来版本中发布代码和预训练权重。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CodeBrain%3A+Bridging+Decoupled+Tokenizer+and+Multi-Scale+Architecture+for+EEG+Foundation+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09110，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09110&send_immediately=true&force_search=false)

**原文摘要:** Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [25] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger, Rawad Bitar*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种多阶段方法，通过可验证的秘密共享、安全聚合和定制的对称私有信息检索方案的精心共同设计，以在数据异构性下实现信息论隐私保证和拜占庭容错。


<details>
  <summary>更多</summary>
  
**动机:** 在联邦学习中，确保对拜占庭客户端的弹性同时保持客户端数据的隐私是一个基本挑战。当客户端的数据是同质的时候，从信息论的角度研究了利用安全聚合技术来确保客户梯度的稳健聚合的适当对策。然而，这些对策在客户端数据异质时失效。最近的研究表明，适当的预处理技术（如最近邻混合）可以提高异构环境中这些对策的表现。但是，这些预处理技术无法与引入的隐私保护机制一起使用。

**方法:** 本文提出的方法包括几个关键组件：1. 可验证秘密共享；2. 安全聚合；3. 一个量身定做的对称私有信息检索方案。这个方法旨在提供信息理论级别的隐私保障，并且在面对数据异构性时也能抵抗拜占庭式的攻击。此外，还研究了如何结合零阶估计方法减少通信开销，使得私有聚合在最先进的联邦学习任务中变得可扩展。

**结果:** 所提出的方案在各种攻击下进行了有效性评估，并显示出其优于先前已知的技术。它能够在保证信息论隐私的同时，也能够抵御拜占庭式攻击，即使是在数据异构的情况下。而且，通过与零阶估计方法相结合，该方案还减少了通信成本，从而提高了联邦学习中的私有聚合的可扩展性。

**结论:** 该研究提出了一种新的方法，不仅能够在数据异构条件下实现信息论隐私和拜占庭容错，而且还能通过降低通信开销来提高联邦学习中私有聚合的可扩展性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Private+Aggregation+for+Byzantine-Resilient+Heterogeneous+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09870，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09870&send_immediately=true&force_search=false)

**原文摘要:** Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [26] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen, Ziyu Zhao, Gaukhar Nurbek, Aosong Feng, Ali Maatouk, Leandros Tassiulas, Yifeng Gao, Rex Ying*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为TRACE的通用多模态检索器，它能够将时间序列数据嵌入到对齐的文本上下文中，支持细粒度的通道级对齐和语义有意义的检索，并能在多个领域中作为下游应用的有效编码器以及增强时间序列模型的一般检索器。


<details>
  <summary>更多</summary>
  
**动机:** 由于天气、医疗保健和能源等领域动态数据的普遍存在，对于有效解释和检索时间序列数据的需求日益增长。这些数据与特定领域的上下文紧密相关，例如临床笔记或天气叙述，使得跨模态检索不仅对下游任务至关重要，而且对于通过检索增强生成（RAG）开发稳健的时间序列基础模型也是必要的。然而，尽管需求增加，时间序列检索仍然很大程度上未被充分探索。现有的方法往往缺乏语义基础，难以对齐异构模态，并且处理多通道信号的能力有限。

**方法:** 为了解决这一差距，研究者们提出了TRACE，一种能够在对齐的文本上下文中固定时间序列嵌入的通用多模态检索器。TRACE支持细粒度的通道级别对齐，并使用硬负例挖掘来促进语义上有意义的检索。它支持灵活的跨模态检索模式，包括文本到时间序列和时间序列到文本，有效地将语言描述与复杂的时间模式联系起来。

**结果:** 通过检索语义相关的配对，TRACE为下游模型提供了丰富的信息上下文，从而提高了预测准确性和可解释性。除了作为一个静态检索引擎外，TRACE还充当一个强大的独立编码器，通过对特定任务进行轻量级调整来完善上下文感知表示，同时保持强大的跨模态对齐。这些表现在下游预测和分类任务上达到了最先进的性能。

**结论:** 广泛的实验表明了TRACE在不同领域的双重效用，既作为下游应用的有效编码器，又作为一般用途的检索器来增强时间序列模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRACE%3A+Grounding+Time+Series+in+Context+for+Multimodal+Embedding+and+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09114，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09114&send_immediately=true&force_search=false)

**原文摘要:** The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [27] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi, Hugo Koubbi, Theodor Misiakiewicz, Nathan Srebro*

**主要类别:** cs.LG

**AI概要:** 该论文研究了单指数模型的学习问题，提出球谐函数作为自然基底比Hermite多项式更适合处理这个问题，并且基于此提出了两种估计器，分别在样本复杂度或运行时间上达到最优。


<details>
  <summary>更多</summary>
  
**动机:** 先前的工作表明，在高斯输入下，恢复权重向量的统计和计算复杂性由链接函数的Hermite展开决定。本文提出了一种新的观点：认为‘球谐函数’而非‘Hermite多项式’是解决这一问题的自然基底，因为它们捕捉到了问题固有的‘旋转对称性’。

**方法:** 作者们基于球谐函数的观点，描述了在任意球面对称输入分布下学习单指数模型的复杂性，并引入了两类估计器——基于张量展开和在线SGD——它们分别实现了最优样本复杂度或最优运行时间。

**结果:** 研究表明，当应用于高斯输入时，不仅恢复并澄清了现有的结果，而且还揭示了以前被忽视的新现象。同时，文章指出可能不存在同时实现两者最优的一般估计器。

**结论:** 通过采用球谐函数作为分析工具，该研究为单指数模型提供了更深入的理解，并且对于不同类型的输入分布都有效。此外，它还展示了针对此类问题设计的估计器的性能界限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+single-index+models+via+harmonic+decomposition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09887，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09887&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [28] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu, Xiangxiang Weng*

**主要类别:** cs.LG

**AI概要:** 本文探讨了概率矩阵分解中的贝叶斯推理方法，通过比较马尔可夫链蒙特卡洛(MCMC)和变分推理(VI)两种近似后验分布的方法，评估了它们在MovieLens数据集上的性能。实验结果显示VI收敛速度更快，而MCMC提供了更准确的后验估计。


<details>
  <summary>更多</summary>
  
**动机:** 传统的矩阵分解技术被广泛应用于推荐系统中，但概率矩阵分解(PMF)通过引入潜在因素的概率分布来量化不确定性。然而，由于高维积分的存在，计算后验分布变得不可行。

**方法:** 为了解决这个问题，研究采用了两种贝叶斯推理方法：马尔可夫链蒙特卡洛（MCMC）和变分推理（VI），用以近似后验分布。

**结果:** 实验结果表明，变分推理（VI）具有更快的收敛速度，而马尔可夫链蒙特卡洛（MCMC）则提供了更加精确的后验估计。

**结论:** 该研究表明，在处理概率矩阵分解时，变分推理能够提供快速收敛的优势，而马尔可夫链蒙特卡洛则在准确性方面表现更好。这意味着根据实际需求选择合适的方法是至关重要的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Probabilistic+Matrix+Factorization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09928，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09928&send_immediately=true&force_search=false)

**原文摘要:** Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [29] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt, Max Ruiz Luyten, Thomas Pouplin, Mihaela van der Schaar*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的大语言模型（LLM）代理框架，通过上下文学习、原子事实增强和递归前瞻搜索来提高规划能力。该框架允许代理在线改进其理解和决策，无需权重更新即可利用经验优化行为。实验表明，这种代理在复杂的交互任务中表现出更好的性能和适应性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法在复杂交互环境中需要大量的指导或广泛的交互历史才能有效执行，并且难以适应新信息或在没有微调的情况下高效地利用过去的经验进行多步推理。

**方法:** 引入了一种新的LLM代理框架，该框架通过上下文学习增强规划能力，同时借助于原子事实的增强和递回前瞻搜索。代理从互动轨迹中学习提取任务关键的“原子事实”，这些事实动态地增强了提供给基于LLM组件的提示。规划是通过深度限制的前瞻搜索完成的，其中LLM模拟潜在的轨迹并评估结果，在累积的事实和交互历史指导下进行。

**结果:** 实证上，代理在具有挑战性的交互任务中展示了改进的表现和适应性，随着经验积累，它在诸如TextFrozenLake和ALFWorld等任务中表现出了更优的行为。

**结论:** 所提出的LLM代理框架能够在线改进其理解与决策过程，利用经历改善行为而不需要权重更新。理论动机将性能联系到基于事实抽象的质量和LLM模拟准确性上。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+LLM+Agent+Planning+with+In-Context+Learning+via+Atomic+Fact+Augmentation+and+Lookahead+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09171，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09171&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [30] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu, Rui Ai, Han Zhong, Xiaoyu Chen, Liwei Wang, Zhaoran Wang, Zhuoran Yang*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在信息不对称和知识迁移的背景下，如何通过非独立同分布动作来学习混淆变量，并提出了一种有效的算法，该算法能够在在线策略交互模型中以紧致的样本复杂度$O(1/\epsilon^2)$学习到$\epsilon$-最优策略。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决多智能体系统中存在的信息不对称问题，以及由此带来的由于混杂变量导致的复杂性。同时，考虑到将知识从容易获取数据的环境转移到目标环境中去的知识可移植性挑战。

**方法:** 提出了一种样本高效的算法，旨在准确地识别信息不对称下的系统动态，并有效地处理强化学习中的知识转移难题。

**结果:** 所提出的方法能够以$O(1/\epsilon^2)$的紧凑样本复杂度学习到$\epsilon$-最优策略。

**结论:** 通过非独立同分布的动作，可以在需要知识迁移的情况下了解混杂因素，并且提出的算法在在线策略互动模型下对于学习$\epsilon$-最优策略具有良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Sample+Complexity+of+Online+Strategic+Decision+Making+with+Information+Asymmetry+and+Knowledge+Transportability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09940，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09940&send_immediately=true&force_search=false)

**原文摘要:** Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [31] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad, Yangyue Wang, Harshvardhan Sikka*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为MultiNet的开源基准和软件生态系统，用于严格评估和调整跨视觉、语言及动作领域的模型，并提供了一个包含超过1.3万亿个标记的综合数据集。


<details>
  <summary>更多</summary>
  
**动机:** 为了推动多模态行动模型的发展，整合视觉理解、语言理解和动作生成，作者们开发了MultiNet来作为标准化评估协议以及相关数据、模型和评估工具的开源平台。

**方法:** 创建了MultiNet，一个开源的基准测试和软件生态系统，它包括了对视觉-语言模型（VLMs）和视觉-语言-动作模型（VLAs）的标准评估协议，并提供了涵盖多种任务的大规模综合数据集。

**结果:** MultiNet提供了一整套工具包和评估框架，已被后续研究用来探讨VLA泛化能力的局限性。

**结论:** MultiNet为研究人员提供了一个强大的工具，用以评估和发展能够处理视觉、语言和动作任务的通用智能系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MultiNet%3A+An+Open-Source+Software+Toolkit+%5C%26+Benchmark+Suite+for+the+Evaluation+and+Adaptation+of+Multimodal+Action+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09172，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09172&send_immediately=true&force_search=false)

**原文摘要:** Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [32] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper, Rohan Wadhawan, John Michael Giorgi, Chenhao Tan, Davis Liang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为CuriosiTree的策略，它是一种基于启发式的测试时策略，用于大型语言模型中的零样本信息获取。该策略通过贪婪树搜索来估计每个行动的预期信息增益，并根据预期的信息增益和相关成本之间的平衡来选择行动。在临床诊断模拟中的实证验证表明，CuriosiTree能够有效地整合异构信息源，并且比基线行动选择策略更能在选择有助于准确诊断的行为序列方面表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 决策者往往缺乏做出自信决策所需的信息，在这种情况下，决策者可以通过咨询知识渊博的权威人士或进行实验来获取必要的信息。然而，不同的信息获取手段有不同的成本，这就提出了一个挑战，即如何选择既具信息价值又经济实惠的行动。

**方法:** 研究者们提出了一种名为CuriosiTree的方法，这是一种基于启发式的测试时策略，适用于大型语言模型（LLMs）中的零样本信息获取。CuriosiTree利用贪婪树搜索来估算每个动作的预期信息收益，并根据预期的信息收益与关联成本之间的平衡来战略性地选择动作。

**结果:** 在临床诊断模拟中的实证研究表明，CuriosiTree 能够实现异构信息来源的成本效益整合，并且在选择能促成准确诊断的动作序列方面优于基线动作选择策略。

**结论:** CuriosiTree 提供了一个有效的框架，用于在大型语言模型中进行零样本信息获取，它能够在保持成本效益的同时，促进信息的高效收集。此外，它还展示了在特定领域如临床诊断中，对于提高决策质量方面的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Curious+Language+Model%3A+Strategic+Test-Time+Information+Acquisition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09173&send_immediately=true&force_search=false)

**原文摘要:** Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [33] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu, Dan Wu, Yixin Zhu, Ying Nian Wu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的骨干网络FNF和架构DBD，专为时空建模设计，能够统一处理时域和频域信息，并通过11个公开基准数据集的实证评估展示了其在多变量长期时间序列预测中的最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前方法主要从自然语言处理或计算机视觉领域借用骨干网络，未能充分解决时间序列的独特属性（如周期性），缺乏专门针对时间序列特性的骨干网络。

**方法:** 引入FNF作为骨干网络以及DBD作为架构，其中FNF能够在单一骨干内统一局部时域和全局频域的信息处理，并且自然地扩展到空间建模；而DBD根据信息瓶颈理论提供了优于现有统一或顺序架构的梯度流和表示能力。

**结果:** 在跨越五个领域的11个公共基准数据集上进行了实证评估，证明了该方法在一致的超参数设置下达到了最先进性能。

**结论:** 研究结果表明，通过适当设计神经网络架构可以捕捉时间序列的内在特性，无需依赖辅助技术即可实现卓越表现，这可能对科学和工业应用中的时间序列建模产生变革影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multivariate+Long-term+Time+Series+Forecasting+with+Fourier+Neural+Filter，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09174，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09174&send_immediately=true&force_search=false)

**原文摘要:** Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [34] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu, Devin White, Evelyn Rose, Vernon Lawhern, Nicholas R Waytowich, Yongcan Cao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的强化学习方法，该方法通过同时考虑多种任务来模仿人类的决策过程，并利用无奖励环境中的评分推断出一个奖励函数。实验结果表明，该方法优于现有的基于评分的RL方法，有时甚至超过了传统的RL方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前的从人类反馈中进行强化学习（RLHF）的方法通常通过孤立的任务（如分类或回归）来简化人类推理的过程，而实际上人类在做决策时会整合多种策略。

**方法:** 提出了一种新型的强化强学习方法，它能够通过结合多个任务来模拟人类决策过程。该方法在没有明确奖励的情况下使用人类评分来推测奖励函数，并引入了可学习权重以平衡分类和回归模型的贡献。

**结果:** 通过合成的人类评分数据进行了几项实验，结果表明所提出的方法始终优于现有的基于评分的RL方法，在某些情况下还超过了传统RL方法的表现。

**结论:** 提出的这种新方法能够更好地捕捉人类决策中的不确定性，并且允许模型自适应地强调不同的策略，从而更有效地与用户的目标保持一致。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Task+Reward+Learning+from+Human+Ratings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09183&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [35] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh, Jyotikrishna Dass*

**主要类别:** cs.LG

**AI概要:** 本文提出了FLoRIST，一种联邦微调框架，通过在服务器上对堆叠的本地适配器分别进行奇异值分解，实现了无需高昂通信或计算开销的数学精确聚合。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦低秩适应方法在平衡通信效率、模型准确性和计算成本方面存在挑战，特别是在异构客户端之间。

**方法:** FLoRIST利用一个高效的分解流程，在紧凑的中间空间内表示从本地LoRA积累的信息，并引入了可调节的奇异值阈值来选择最优秩，以构建一对全局低秩适配器供所有客户端共享。

**结果:** 广泛的实证评估表明，无论是在同构还是异构环境下，FLoRIST都能在优越的通信效率和竞争性性能之间取得最佳平衡。

**结论:** FLoRIST为联邦学习中的大语言模型提供了参数高效的微调解决方案，同时保持了良好的通信效率和模型准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FLoRIST%3A+Singular+Value+Thresholding+for+Efficient+and+Accurate+Federated+Fine-Tuning+of+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09199，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09199&send_immediately=true&force_search=false)

**原文摘要:** Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [36] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang, Karthik Duraisamy*

**主要类别:** cs.LG

**AI概要:** LaDCast是一个基于潜在扩散的全球中程集合预报框架，它在学习到的潜在空间中生成逐小时集合预报。通过自编码器和基于变换器的扩散模型，LaDCast能够高效地处理高维ERA5再分析数据，并引入了地球球面几何的考虑、双流注意力机制以及季节性模式捕捉等特性。相比现有的数值天气预报和机器学习方法，LaDCast在追踪罕见极端事件如飓风方面表现更优，同时大幅度减少了存储与计算需求。


<details>
  <summary>更多</summary>
  
**动机:** 准确的概率天气预报需要高度精确性和有效的不确定性量化，这对现有的集合数值天气预测（NWP）和近期的机器学习方法构成了挑战。为了克服这些难题，研究者们开发了LaDCast，一种新的全球潜在扩散框架，旨在提高中程天气预报的质量。

**方法:** LaDCast利用自编码器将高维度的ERA5再分析场压缩为紧凑表示形式，在此之后，一个基于变换器的扩散模型会在任意小时初始化下产生序列化的潜在更新。该模型还结合了几何旋转位置嵌入（GeoRoPE）、双流注意力机制及正弦时间嵌入来分别解决地球球形几何、条件效率以及季节性模式捕获的问题。

**结果:** LaDCast在确定性和概率性技能上接近欧洲中期天气预报中心IFS-ENS的表现，而且在没有使用任何显式扰动的情况下，对于像飓风这样的罕见极端事件的轨迹跟踪显示出更好的性能。此外，通过在潜在空间中运作，LaDCast极大地减少了所需的存储空间和计算量，这使得实时千米级分辨率的天气预报变得可行。

**结论:** LaDCast展示了一种实用的方法，可以在保持高准确性的同时显著降低计算成本和存储要求，为实现高分辨率实时天气预报提供了一个有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LaDCast%3A+A+Latent+Diffusion+Model+for+Medium-Range+Ensemble+Weather+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09193&send_immediately=true&force_search=false)

**原文摘要:** Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [37] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu, Xinqi Wang, Simon Shaolei Du*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的任务，即从离线强化学习数据集中对轨迹进行聚类，其中每个簇中心代表生成其轨迹的策略。为了解决这个问题，作者提出了策略引导的K-均值（PG-Kmeans）和中心吸引自动编码器（CAAE）。理论证明了PG-Kmeans在有限步数内的收敛性，并指出了离线轨迹聚类的关键挑战。实验结果表明，两种方法都能有效地将轨迹划分为有意义的簇。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于通过离线强化学习数据集中的轨迹聚类来识别出生成这些轨迹的不同策略。这样可以为离线RL及其他领域提供一种基于策略的轨迹聚类框架。

**方法:** 论文提出了两种算法：Policy-Guided K-means (PG-Kmeans) 和 Centroid-Attracted Autoencoder (CAAE)。前者通过迭代训练行为克隆策略并根据策略生成概率分配轨迹；后者类似于VQ-VAE框架，通过指导轨迹的潜在表示向特定码本条目附近移动以实现聚类。

**结果:** 实验验证了所提方法的有效性，包括在广泛使用的D4RL数据集和自定义GridWorld环境上的测试。结果显示PG-Kmeans和CAAE都能够有效地区分轨迹并形成有意义的聚类。

**结论:** 该研究不仅提供了两种有效的轨迹聚类方法，还揭示了离线轨迹聚类中因策略冲突导致的最佳解模糊性的关键问题。此外，它为基于策略的轨迹聚类奠定了基础，具有广泛的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Policy-Based+Trajectory+Clustering+in+Offline+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09202，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09202&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [38] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

**主要类别:** cs.LG

**AI概要:** 本文探讨了用于总结Transformer嵌入模型输出的池化方法的设计，发现传统的AvgPool、MaxPool和ClsToken方法在信噪比波动时容易性能崩溃，并提出了一种基于注意力机制的自适应池化方法，该方法能在任何信噪比下近似信号最优向量量化器。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机源于强化学习和视觉应用领域，旨在解决输入向量中只有一部分对下游任务有用（信号），而其余是干扰（噪声）的问题。

**方法:** 将池化视为矢量量化，目的是最小化信号损失。通过理论分析证明了基于注意力机制的自适应池化方法可以在任何信噪比条件下逼近信号最优的矢量量化器，并且给出了误差范围。

**结果:** 监督实验验证了理论结果，在合成数据集上隔离了信噪比问题，然后推广到具有噪声观测的标准关系推理、多智能体强化学习和视觉基准测试，其中带有自适应池化的变压器在不同任务中表现出更好的鲁棒性。

**结论:** 基于注意力的自适应池化方法相较于传统方法如AvgPool、MaxPool和ClsToken能够更好地处理输入中的信噪比变化，从而为下游任务提供更稳健的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Noise+Attenuation+via+Adaptive+Pooling+of+Transformer+Outputs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09215&send_immediately=true&force_search=false)

**原文摘要:** We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [39] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo, David B. Emerson, Amandeep Singh, Veronica Chatrath, Marcelo Lotif, Ravi Theja, Alex Cheung, Izuki Matsubi*

**主要类别:** cs.LG

**AI概要:** 本文介绍了FedRAG，一个用于在集中式和联邦式架构中微调检索增强生成(RAG)系统的框架，支持最前沿的微调方法，并与现代RAG生态系统深度整合。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于改进检索增强生成（RAG）系统，通过微调检索器和生成器模型来提高其性能，并且提供了一个可以跨越集中式和联邦式架构进行微调的解决方案。

**方法:** 提出了一种名为FedRAG的新框架，该框架能够支持最先进的微调技术，并且允许从集中式训练平滑过渡到联邦式训练任务。

**结果:** FedRAG为RAG系统的开发提供了简单直观的接口，并且很好地融入了当前的RAG生态系统，弥补了现有工具中的一个重要空白。

**结论:** FedRAG作为新的框架，增强了RAG系统的能力，使得无论是集中式还是联邦式的设置下都能够实现高效的模型微调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedRAG%3A+A+Framework+for+Fine-Tuning+Retrieval-Augmented+Generation+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09200，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09200&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [40] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella, Joshua B. Evans, Özgür Şimşek, Anders Jonsson*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种仅从状态轨迹学习马尔可夫决策过程的状态表示框架，不需奖励信号或代理执行的动作。通过学习最小动作距离（MAD），提供了一个密集且几何上意义明确的进展度量，支持自监督学习方法，并在多种环境中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 作者希望开发一种新的状态表示框架，它能够仅基于状态轨迹来学习，而不需要依赖于奖励信号或具体执行的动作。这样的框架可以为环境提供一个基础度量，帮助实现目标条件下的强化学习和奖励塑造。

**方法:** 提出了学习最小动作距离（MAD）的方法，这是一种衡量状态之间转换所需最少动作数的指标。采用自监督学习方法创建一个嵌入空间，在该空间中嵌入状态对之间的距离对应于它们的MAD值，同时支持对称和非对称近似。

**结果:** 实证结果表明，所提出的方法能够在具有确定性和随机动态、离散和连续状态空间以及噪声观测的各种环境中有效学习准确的MAD表示，并且在表示质量方面明显优于现有状态表示方法。

**结论:** 提出的基于MAD的学习框架不仅能够有效地从不同的环境设置中学习到精确的状态表示，而且相比现有的状态表示方法，它在表示质量上有了显著的提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+The+Minimum+Action+Distance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09276&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [41] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani, Kseniya Solovyeva, David Danks, Vince Calhoun, Sergey Plis*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，通过考虑欠采样的影响来从时间序列数据中学习图形因果结构，并使用答案集编程(ASP)优化可能的解决方案。实验表明该方法在模拟和实际脑连接数据上优于现有方法，并且对于不同欠采样率具有鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 从时间序列数据中学习图形因果结构时遇到的主要挑战是测量频率与系统的因果时间尺度不匹配，这会导致信息丢失并产生多个可能的底层因果图。

**方法:** 研究采用约束优化方法，特别是答案集编程(ASP)，来找到最优解集合。ASP能够识别最有可能的底层图，并提供一个可能图的等价类供专家选择。此外，利用图论进一步减少可能解的集合，从而比传统方法更快地得到更小、更准确的答案集合。

**结果:** 所提方法在模拟数据和经验结构脑连接上的验证显示了其相对于已有方法的优势，在这些实验中平均F1分数提高了12%。此外，在从欠采样的时间序列数据重建因果图方面取得了最先进的结果。

**结论:** 提出的方法不仅提高了从欠采样的时间序列数据中重构因果图的精度和召回率，而且在现实模拟中的不同欠采样程度下表现出了鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Graph+Recovery+in+Neuroimaging+through+Answer+Set+Programming，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09286，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09286&send_immediately=true&force_search=false)

**原文摘要:** Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [42] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson, Kevin Chung, Youngsoo Choi*

**主要类别:** cs.LG

**AI概要:** 提出了一个多阶段潜在空间动力学识别(mLaSDI)方法，通过分阶段训练多个自编码器来逐步修正前一阶段的误差，从而在复杂或高频情况下提高数据重建精度和减少预测误差。


<details>
  <summary>更多</summary>
  
**动机:** 原有的Latent Space Dynamics Identification (LaSDI) 框架虽然能够为许多问题提供有效的降阶模型(ROMs)，但在处理复杂或高频情况时，难以同时精确地重建训练数据并满足潜在空间中的动力学约束条件。

**方法:** 研究者提出了一种新的多阶段潜在空间动力学识别（mLaSDI）方法，该方法按阶段顺序训练几个自编码器，每个自编码器学习修正前一阶段产生的错误。

**结果:** 与传统的LaSDI相比，采用小型自编码器的mLaSDI不仅降低了预测和重构误差，还减少了训练时间。

**结论:** mLaSDI 通过使用多个小规模自编码器分阶段训练，提高了对于复杂或高频状态下的数据重建准确性，并且在降低预测误差的同时也缩短了训练所需的时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是mLaSDI%3A+Multi-stage+latent+space+dynamics+identification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09207&send_immediately=true&force_search=false)

**原文摘要:** Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [43] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao, Hanlin Gu, Xin Yang, Bingjun Wei, Haoyang Liang, Xiangkun Wang, Tianrui Li*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的持续学习(CL)视角，强调了有意遗忘的重要性，并引入了一个名为ErrorEraser的插件来消除由于数据偏差引起错误记忆，从而提高新旧任务的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习方法忽略了现实世界数据中的偏见，导致模型学习虚假的相关性，这些相关性在任务间传递并被放大。这同时降低了CL保留和转移知识的能力。

**方法:** ErrorEraser是一种通用插件，由错误识别和错误擦除两个模块组成。错误识别模块无需先验知识就能学习任务数据在特征空间的概率密度分布；错误擦除模块通过改变代表性异常样本的决策空间来确保只擦除错误的知识。此外还设计了一种增量式特征分布学习策略以减少下游任务中错误识别时的资源开销。

**结果:** 广泛的实验结果表明，ErrorEraser显著减轻了数据偏见的负面影响，在三种类型的CL方法上达到了更高的准确性和更低的遗忘率。

**结论:** 研究提出了一个创新的观点，即持续学习不仅需要防止遗忘，还需要有意识地遗忘。ErrorEraser插件有效地解决了因数据偏见而导致的学习问题，提高了对新旧任务的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ErrorEraser%3A+Unlearning+Data+Bias+for+Improved+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09347，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09347&send_immediately=true&force_search=false)

**原文摘要:** Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [44] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren, Yue Xing, Yingqian Cui, Charu C. Aggarwal, Hui Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于意图导向的新分类法来理解大语言模型的去学习过程，并围绕这一分类法做出了三项主要贡献：重新审视了现有去除方法的功能行为，调研了现有的评估策略并提出了改进方向，强调了实际应用中面临的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 当前关于大语言模型去学习的方法往往根据技术特性进行分类，而忽视了一个更基础的维度：去学习背后的意图，即是否真正移除内部知识或仅仅是抑制其行为影响。

**方法:** 提出了一种新的分类法，该分类法基于意图导向的角度对去学习方法进行了归类；通过这种分类法，作者重审了最近的研究发现、调查了现存的评估策略，并指出了实际部署中的难题。

**结果:** 得出了许多去除方法可能在功能上表现为抑制而非真正去除的结论；确定了现有度量标准和基准测试的局限性，并为开发更加可靠且符合意图的评估方法指明了方向；强调了可扩展性和支持连续去学习等现实挑战。

**结论:** 这项工作提供了一个全面的框架来理解和推进生成式AI中的去学习研究，旨在支持未来的研究并指导有关数据移除与隐私的政策决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SoK%3A+Machine+Unlearning+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09227，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09227&send_immediately=true&force_search=false)

**原文摘要:** Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [45] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu, Jing Liu, Chengfang Li, Rui Xi, Wenchao Li, Liang Cao, Jin Wang, Laurence T. Yang, Junsong Yuan, Wei Zhou*

**主要类别:** cs.LG

**AI概要:** 该综述文章探讨了扩散模型在异常检测和生成中的应用，强调了两者间的协同关系，并提出了一个详细的分类体系来分析这些方法的优势与局限性。此外，还讨论了包括可扩展性和计算效率在内的关键挑战，并概述了未来的研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 随着深度学习技术的进步，特别是扩散模型的出现，为无监督异常检测提供了一个强大的框架。然而现有的调查往往将异常检测和生成视为两个独立的问题。本文旨在揭示这两者之间的内在协同关系，以促进更有效的异常检测解决方案的发展。

**方法:** 通过理论基础和实际应用案例的教程式分析，对使用扩散模型进行异常检测与生成（ADGDM）的方法进行了全面回顾。建立了一个详细的分类体系，基于异常评分机制、条件策略及架构设计等方面对ADGDM方法进行了归类。

**结果:** 展示了扩散模型如何通过生成技术直接解决异常数据稀缺问题，同时检测方法又能反过来提高生成的真实性和相关性。指出了当前面临的关键挑战如可扩展性和计算效率等，并提出了未来研究可能的方向。

**结论:** 综上所述，本篇综述不仅综合了最近关于利用扩散模型进行异常检测和生成的研究进展，而且也提出了一些开放的研究问题，目的是为了指导研究人员和实践者开发出跨多种应用场景下的创新异常检测解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Anomaly+Detection+and+Generation+with+Diffusion+Models%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09368，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09368&send_immediately=true&force_search=false)

**原文摘要:** Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [46] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark, Daniel Strömbergsson, Chang Liu, Marcus Liwicki, Fredrik Sandin*

**主要类别:** cs.LG

**AI概要:** 本研究提出了MindRAG框架，该框架结合了多模态检索增强生成与专为条件监测（CM）数据设计的向量存储结构，旨在减少误报、提高故障严重性估计，并提供可解释的界面。


<details>
  <summary>更多</summary>
  
**动机:** 当前计算机化维护系统虽然能够有效检测和分类故障，但在故障严重性估计和维护决策上仍依赖于人工分析。现有系统的自动分析和决策过程存在不确定性且误报率高，导致工作负荷增加和效率降低。

**方法:** 提出了一种名为MindRAG的模块化框架，该框架将基于大语言模型（LLM）的推理代理与条件监测工作流程相结合，利用多模态检索增强生成（RAG）技术以及专为CM数据设计的新颖向量存储结构。此外，该框架还利用现有的注释和维护工单作为监督学习协议中的标签替代品。

**结果:** 初步结果表明，MindRAG能够为更高效地管理警报提供有意义的决策支持，从而提高了CM系统的可解释性。

**结论:** 通过整合大型语言模型推理代理与条件监测流程，MindRAG有助于解决分析师和行业需求，包括减少误报、加强故障严重程度评估、改进决策支持及提供易于理解的人机交互界面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent-based+Condition+Monitoring+Assistance+with+Multimodal+Industrial+Database+Retrieval+Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09247，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09247&send_immediately=true&force_search=false)

**原文摘要:** Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [47] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang, Yu Xia, Yi-Feng Wu, Yuwei Hu, Yuhui Chen, Qing-Guo Chen, Xiaogang Xu, Xiangyu Wu, Hao Lu, Yanqing Ma, Shiyin Lu, Qifeng Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，即位置偏好优化（LPO），它利用位置数据来优化图形用户界面中的交互偏好，并通过信息熵预测交互位置，同时引入了基于物理距离的动态位置奖励函数。实验表明，LPO在离线基准测试和现实世界的在线评估中均达到了领先水平。


<details>
  <summary>更多</summary>
  
**动机:** 现有的监督微调方法在处理图形用户界面（GUI）的空间定位时存在局限性，尤其是在准确感知位置数据方面表现不佳。此外，强化学习等现有策略往往不能有效评估位置准确性，限制了它们的实用性。因此，研究者们提出了位置偏好优化（LPO）来解决这些问题。

**方法:** 研究人员开发了一种名为位置偏好优化（LPO）的新方法，该方法使用信息熵来预测富含信息区域的交互位置，并且基于物理距离引入了一个动态位置奖励函数，以反映不同交互位置的重要性。此外，还借助了组相对偏好优化（GRPO）来促进对GUI环境的广泛探索。

**结果:** 全面的实验显示，LPO在离线基准测试以及实际在线评估中都表现出色，达成了最先进（SOTA）的结果。

**结论:** 位置偏好优化（LPO）是一种能够显著提高GUI环境中交互精度的方法，它通过优化交互位置并考虑到位置的重要性，从而超越了现有的技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LPO%3A+Towards+Accurate+GUI+Agent+Interaction+via+Location+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09373，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09373&send_immediately=true&force_search=false)

**原文摘要:** The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [48] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong, Haotian Lu, Wenzhuo Zheng, Fan Zhang, Shuangjia Zheng, Ning Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出了EnerBridge-DPO，一种新的逆折叠框架，它通过结合马尔可夫桥和直接偏好优化来生成低能量、高稳定性的蛋白质序列，并且能够准确预测不同序列之间的ΔΔG值。


<details>
  <summary>更多</summary>
  
**动机:** 当前深度学习方法主要通过最大化序列恢复率来进行训练，往往忽视了生成序列的能量问题。设计具有最优能量稳定性的蛋白质序列是蛋白质逆折叠中的一个关键挑战。

**方法:** 提出了一种名为EnerBridge-DPO的新框架，该框架将马尔可夫桥与基于能量偏好的直接偏好优化（DPO）相结合，从信息丰富的先验序列开始优化，引入了显式的能量约束损失以增强DPO的能量驱动特性。

**结果:** 评估表明，EnerBridge-DPO可以设计出能量更低的蛋白质复合体序列，同时保持与最先进模型相媲美的序列恢复率，并能准确预测各种序列间的ΔΔG值。

**结论:** EnerBridge-DPO作为一种创新的逆折叠方法，在保证序列恢复率的同时有效降低了蛋白质序列的能量，为蛋白质设计提供了新的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EnerBridge-DPO%3A+Energy-Guided+Protein+Inverse+Folding+with+Markov+Bridges+and+Direct+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09496&send_immediately=true&force_search=false)

**原文摘要:** Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [49] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis, Sebastian Lee, Claudia Clopath, Will Dabney*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种基于认知不确定性的优先经验回放方法，以减少价值估计中的噪声干扰，并在Atari游戏套件上优于分位数回归深度Q学习基准。


<details>
  <summary>更多</summary>
  
**动机:** 传统的基于时间差误差的选择转换方式容易偏向于噪声转换，即使价值估计接近目标均值时也是如此。这类似于探索文献中假设的噪声电视问题，其中由探索引导的代理会因为将噪声误认为新颖性而陷入困境。为了解决这一问题，研究者提出使用认知不确定性估计来指导重放缓冲区中转换的优先级排序。

**方法:** 研究者首先在两个表格玩具模型（一个多臂老虎机任务和一个噪声网格世界）中展示了认知不确定性优先回放的好处。然后，他们在Atari游戏套件上评估了他们的优先级排序方案。

**结果:** 通过使用认知不确定性优先的经验回放，在Atari游戏套件上的表现超过了分位数回归深度Q学习基准。

**结论:** 利用认知不确定性来指导经验回放中的优先级排序能够有效减少由于随机过程引起的样本噪声，从而提高强化学习代理的学习效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+Prioritized+Experience+Replay，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09270，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09270&send_immediately=true&force_search=false)

**原文摘要:** Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [50] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom, Paul R. Schrater*

**主要类别:** cs.LG

**AI概要:** 本文介绍了选项核贝尔曼方程(OKBEs)，用于一种新的无奖励马尔可夫决策过程。OKBEs构建并优化一个称为状态-时间选项核(STOK)的预测映射，以最大化完成目标的概率同时避免违反约束。STOKs具有组合性、模块化和可解释性，并且能够支持长期规划和内在动机，适用于动态高维世界模型。


<details>
  <summary>更多</summary>
  
**动机:** 传统的基于价值函数的方法在处理复杂高维问题时存在局限性，特别是当需要进行长期规划以及保持策略的组合性、模块化和可解释性时。为了解决这些问题，研究者提出了一个新的方法，即选项核贝尔曼方程(OKBEs)，它直接构造并优化状态-时间选项核(STOK)来提高解决复杂任务的能力。

**方法:** 研究者引入了选项核贝尔曼方程（OKBEs），它们不依赖于价值函数而是直接构建和优化一个名为状态-时间选项核（STOK）的预测图，以增加达到目标的概率同时防止违反约束条件。STOKs是复合的、模块化的，并且可以被解读为从开始到结束的转换核，这些特性允许STOKs通过Chapman-Kolmogorov方程进行组合，以便对多个策略做出时空预测，同时也能够有效地表示和计算高维度的情况。

**结果:** 研究结果表明，OKBEs和STOKs使得代理能够快速合成元策略，跨多个任务重用规划表征，并使用赋能作为内在激励函数来证明目标的合理性。此外，STOKs记录了语义上可解释的目标成功与约束违反事件的概率，这有助于正式验证。

**结论:** OKBEs促进了组合性、模块化和可解释性的特点，支持可验证的长期规划和可扩展到动态高维世界模型的内在动机。相比之下，奖励最大化可能与上述属性相冲突。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Unified+Theory+of+Compositionality%2C+Modularity%2C+and+Interpretability+in+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09499，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09499&send_immediately=true&force_search=false)

**原文摘要:** We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [51] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho, Minju Jo, Kookjin Lee, Noseong Park*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的网络架构，用于从测量中提取周期性模式，并利用这些信息来表示信号，从而提高泛化能力和外推性能。通过综合实验验证了该方法的有效性，包括学习微分方程的周期解、时间序列插补和真实数据集上的预测。


<details>
  <summary>更多</summary>
  
**动机:** 基于坐标MLP的学习方法在处理具有周期性质的真实信号时存在过拟合和泛化能力有限的问题，导致外推表现不佳。为了解决这些问题，特别是当底层真实信号展示出空间或时间上的周期性特征时，研究旨在改进模型的泛化及外推性能。

**方法:** 提出了一个新颖的网络结构，能够从数据测量中抽取周期性的模式，并将这些周期信息用来表示信号，以增强模型对未知数据的泛化能力并改善外推效果。

**结果:** 通过一系列全面的实验表明，所提方法在学习微分方程的周期解以及进行时间序列插补和实际数据集上的预测方面是有效的。

**结论:** 新提出的网络架构能够有效地从数据中提取并利用周期性信息，这有助于提升连续神经表示学习的泛化与外推性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Functions+for+Learning+Periodic+Signal，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09526，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09526&send_immediately=true&force_search=false)

**原文摘要:** As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [52] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen, Yiyang Liu, Mattia Prosperi, Krishna Vaddiparti, Robert L Cook, Jiang Bian, Yi Guo, Yonghui Wu*

**主要类别:** cs.LG

**AI概要:** 该研究运用自然语言处理技术对电子健康记录进行主题建模，揭示了HIV感染者的污名化、社会因素及行为状况，为理解和改进HIV感染者的情况提供了新的视角。


<details>
  <summary>更多</summary>
  
**动机:** 本研究旨在通过应用自然语言处理方法于美国东南部一个大型综合卫生系统的大量电子健康记录（EHR）临床笔记，来表征寻求护理的艾滋病患者（PLWHs）中的污名维度、社会及相关的行为情况。

**方法:** 研究者从UF Health IDR中识别出9,140名PLWH队列，并使用潜在狄利克雷分配（LDA）进行主题建模分析，以揭示污名维度、社会及相关行为情况。领域专家创建了与HIV相关污名关键词的种子列表，并采用滚雪球策略迭代审阅笔记，直到达到饱和。为了确定更多目标主题，测试了三种基于关键词的过滤策略。通过词频分析突出显示每个主题相关的常见术语。此外，还进行了子组间主题变化分析，以检查不同年龄和性别特定人口统计学特征之间的差异。

**结果:** 在包含至少一个关键词的句子上进行的主题建模揭示了与HIV相关污名、社会以及相关行为情况的一系列主题，包括“心理健康关注与污名”、“社会支持与参与”、“有限的医疗保健获取与重病”、“治疗拒绝与隔离”等。跨年龄子组的主题变化分析揭示了差异。

**结论:** 从EHR临床记录中提取和理解与HIV相关的污名维度、社会以及相关行为情况，能够实现可扩展且高效的时间评估，克服传统问卷调查的局限性，并改善患者的治疗结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Topic+Modeling+Analysis+of+Stigma+Dimensions%2C+Social%2C+and+Related+Behavioral+Circumstances+in+Clinical+Notes+Among+Patients+with+HIV，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09279，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09279&send_immediately=true&force_search=false)

**原文摘要:** Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [53] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang, Zhenhua Liu, Jiaheng Wei, Xuanwu Yin, Dong Li, Emad Barsoum*

**主要类别:** cs.LG

**AI概要:** 本文提出了Athena-PRM，一种多模态过程奖励模型，用于评估解决复杂推理问题时每一步的奖励分数。通过利用弱和强完成者之间的预测一致性作为识别可靠过程标签的标准，该模型能以较少样本高效生成高质量的过程标签数据，并在多个基准测试中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 开发高性能的过程奖励模型（PRMs）通常需要大量时间和资金投入，尤其是因为需要对推理步骤进行逐级标注。传统的自动化标注方法如蒙特卡洛估计往往产生嘈杂的标签并导致高昂的计算成本。

**方法:** 为了解决上述问题，作者提出了一种基于弱与强完成者之间预测一致性的新方法来识别可靠的步骤标签，并据此构建了Athena-PRM模型。此外还开发了两种策略：ORM初始化和负样本上采样，以进一步提高模型性能。

**结果:** 实验结果表明，在仅使用5,000个样本的情况下，Athena-PRM在多种场景和基准测试中都展现出了卓越的有效性。特别是在采用Qwen2.5-VL-7B作为策略模型时，它在WeMath上的表现提升了10.2分，在MathVista上提升了7.1分；同时也在VisualProcessBench上达到了新的最先进水平。

**结论:** Athena-PRM不仅能够有效地减少获取高质量过程标签所需的成本，而且还能显著提升不同应用场景下的推理任务性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Athena%3A+Enhancing+Multimodal+Reasoning+with+Data-efficient+Process+Reward+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09532&send_immediately=true&force_search=false)

**原文摘要:** We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [54] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura, Karim Tit, Laura Bussi, Eleonora Giunchiglia, Maxime Cordy*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为TRIDENT的算法，该算法可以在推理时保证生成的结果满足线性时态逻辑（LTLf）所表达的时间约束，并且不需要重新训练模型。它将LTLf公式编译为确定有限自动机（DFA），并指导一个受约束的束搜索变体，在每个解码步骤中屏蔽会导致约束违规的转换，同时根据模型的概率和DFA的接受结构动态重排剩余路径。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型和其他神经架构在各种生成和分类任务上取得了令人印象深刻的结果，但它们本质上无法确保其输出满足时间约束，如可以用线性时态逻辑表示的约束。

**方法:** TRIDENT是一种通用且与模型无关的推理时间算法，它通过将LTLf公式编译成确定有限自动机（DFA）来工作，这个DFA用来引导受限版本的束搜索。在每个解码步骤中，会遮蔽掉导致约束违反的转移，而剩余路径则基于模型的概率和DFA的接受结构进行动态重新排序。

**结果:** TRIDENT能够保证产生的序列满足给定的LTLf约束，并且实证表明TRIDENT还提高了输出质量。在两个不同的任务中验证了这种方法：时间限制的图像流分类和受控文本生成。在这两种情况下，TRIDENT都达到了完美的约束满足度，并且与最先进方法相比显示出了更高的效率和高质量标准指标。

**结论:** TRIDENT提供了一个有效的方法来确保LLMs和其他神经网络架构的输出遵守复杂的时序约束，无需重新训练，并且能够在保持或提高输出质量的同时实现这一目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRIDENT%3A+Temporally+Restricted+Inference+via+DFA-Enhanced+Neural+Traversal，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09701&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [55] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的双状态线性注意力机制（DSLA）和一个在线自适应蒸馏框架SERVE，旨在提高长输入下大语言模型的效率和准确性。实验表明，与Llama2-7B相比，SERVE在推理速度上提高了2.3倍，并且在下游任务中保持了相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）虽然擅长通过自注意力捕捉全局token依赖关系，但面对长输入时存在计算和内存成本过高的问题。现有的次二次方法如线性注意力虽能降低成本，却因过分强调最近的tokens而降低精度。

**方法:** 提出了一种新颖的设计，即双状态线性注意力（DSLA），它维护两个专门的隐藏状态，一个用于保存历史上下文，另一个用于跟踪最近信息，从而减轻线性注意力架构中的短程偏差。此外，还介绍了一个名为SERVE的在线自适应蒸馏框架，该框架根据基于敏感性的层排序，在推理时逐步用DSLA层替换Transformer层。SERVE采用链式微调策略以保证每个新转换的DSLA层与之前替换的层保持一致，确保整体质量。

**结果:** 在常识推理、长上下文问答和文本摘要等任务上的广泛评估表明，SERVE比Llama2-7B快2.3倍，比混合Zamba-7B快3.0倍，同时在下游任务中保持了可比较的表现。消融研究表明，DSLA的双重状态能够捕捉到全局和局部依赖关系，解决了先前线性注意力中历史tokens代表性不足的问题。

**结论:** 通过引入DSLA及其伴随的SERVE框架，研究者们成功地为处理长输入的大语言模型提供了一种既高效又准确的新方案，这不仅提高了推理速度，也保持了模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On-the-Fly+Adaptive+Distillation+of+Transformer+to+Dual-State+Linear+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09316，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09316&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [56] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

**主要类别:** cs.LG

**AI概要:** 本文挑战了需要非标准空间域来实现长期稳定天气预报的假设，提出了一种名为AtmosMJ的深度卷积网络，它直接在标准经纬度网格上运行，并通过一种新的门控残差融合机制来保证模型稳定性。该模型能够生成长达约500天的稳定且物理上合理的预报，在10天预报准确率方面与Pangu-Weather和GraphCast等模型相当，同时训练成本极低。


<details>
  <summary>更多</summary>
  
**动机:** 现有的长期自回归天气预报模型通常依赖于将输入数据转换到球谐函数或HEALPix网格等非标准空间域，以确保物理一致性和长期稳定性。本文旨在探索是否可以在标准的经纬度网格上达到类似的长期预报性能。

**方法:** 提出了AtmosMJ，这是一种深层卷积网络，可以直接处理ERA5数据而无需进行球面重映射。该模型引入了一种新颖的门控残差融合（GRF）机制，可以自适应地调节特征更新，防止长时间递归模拟过程中误差累积。

**结果:** AtmosMJ能够在大约500天内产生稳定且符合物理规律的预测。在定量评估中，它的10天预报精度与Pangu-Weather和GraphCast等模型相竞争，并且仅需在V100 GPU上花费5.7天的训练时间。

**结论:** 研究结果表明，通过有效的架构设计而不是采用非标准的数据表示方法，可以成为解锁稳定且计算效率高的长期天气预测的关键。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AtmosMJ%3A+Revisiting+Gating+Mechanism+for+AI+Weather+Forecasting+Beyond+the+Year+Scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09733&send_immediately=true&force_search=false)

**原文摘要:** The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [57] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song, Ramith Hettiarachchi, Chuan Li, Jianwen Xie, Lei Li*

**主要类别:** cs.LG

**AI概要:** 本文介绍了InstructPro，一种能够根据自然语言指令设计具有特定功能（如与配体结合）的蛋白质生成模型。它使用了大规模数据集InstructProBench进行训练，并在多个基准测试中优于其他强大的基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 蛋白质与特定配体的结合在生物学和化学领域有着广泛的应用，但现有的AI模型通常依赖于稀缺且昂贵的蛋白质-配体复合物数据。相比之下，关于蛋白质-配体相互作用的人工整理文本描述数量庞大，这为开发基于自然语言指导的新模型提供了机会。

**方法:** 研究者们提出了InstructPro，一种基于自然语言指令来设计具备特定功能蛋白质的生成模型。该模型通过一个名为InstructProBench的大规模数据集来进行训练，该数据集包含超过950万个三元组（功能描述、配体公式、蛋白质序列）。

**结果:** 实验结果表明，InstructPro的两个版本（10亿参数和30亿参数）均优于包括ProGen2、ESM3及Pinal在内的强大基线模型。特别是，在中等置信度下InstructPro-1B达到了最高的对接成功率(81.52%)以及最低的平均根均方差(RMSD, 4.026 Å)；而InstructPro-3B则进一步将RMSD降至2.527 Å。

**结论:** InstructPro展示了其依据给定的功能描述和SMILES格式的配体公式生成符合要求的蛋白质序列的能力，尤其是在减少生成结构与真实结构之间的差异方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Natural+Language+Guided+Ligand-Binding+Protein+Design，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09332&send_immediately=true&force_search=false)

**原文摘要:** Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [58] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca, Peini Liu, Jordi Guitart, Rodrigo M Carrillo-Larco, Ajay Dholakia, David Ellison*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于大型语言模型（LLM）的认知架构，用于机器学习模型的监控。该架构通过特征工程原则来提高监控输出的可解释性，并通过重构、分解和编译三个步骤来模拟特征工程。这种结合了特征工程驱动规划和选择性LLM使用的方法，在多个领域中与各种基线相比，实现了显著更高的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的机器学习模型监控方法往往产生冗长且难以解读的结果，这阻碍了有效的决策制定。

**方法:** 作者提出了一种新的认知架构，该架构利用大型语言模型（LLMs）作为基础，并应用特征工程技术来改进监控结果的可解释性。该架构包括一个决策程序模块，它通过重构、分解和编译三个关键步骤来模拟特征工程。

**结果:** 实验表明，所提方法在多个领域中相比于不同基线有着显著更高的准确性。

**结论:** 通过将特征工程驱动的规划与有选择性的LLM使用相结合，研究者开发出一种能够提供高度可解释性和可行见解的强大决策支持系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature+Engineering+for+Agents%3A+An+Adaptive+Cognitive+Architecture+for+Interpretable+ML+Monitoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09742&send_immediately=true&force_search=false)

**原文摘要:** Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [59] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma, Takayuki Nishio*

**主要类别:** cs.LG

**AI概要:** 本文提出了Load-aware Tram-FL，一种考虑了计算和通信负载以减少去中心化联邦学习中总训练时间的Tram-FL扩展。通过引入方差约束来促进非IID分布下的均衡数据利用，并通过目标函数最小化总体训练延迟。仿真结果表明该方法相比基线方法能显著减少训练时间和加速收敛。


<details>
  <summary>更多</summary>
  
**动机:** 为了在去中心化联邦学习环境中减少总的训练时间，同时考虑到计算和通信负载的影响。

**方法:** 提出了一种名为Load-aware Tram-FL的方法，它将调度问题表述为全局优化任务，并将其分解为节点级别的子问题来解决。此外，通过引入方差约束来处理非IID数据分布的问题。

**结果:** 在MNIST和CIFAR-10数据集上的模拟实验显示，Load-aware Tram-FL相比于其他基线方法能够显著降低训练时间并加快收敛速度。

**结论:** Load-aware Tram-FL能够在去中心化的联邦学习中有效减少训练时间，同时对于非IID数据分布情况也能保持良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Load-Aware+Training+Scheduling+for+Model+Circulation-based+Decentralized+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09769，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09769&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [60] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov, Alexander Yuhay, Alexey Zaytsev*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个针对连续依赖数据的对比自监督学习的新理论框架，包括两种新的相似性度量方法，并通过实验证明了其在处理时空依赖性问题上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自监督学习（SSL）方法在处理具有时间或时空依赖性的数据时表现不佳，因为它们通常假设样本之间是语义独立的，这与依赖数据中复杂的关联关系不符。

**方法:** 提出了一种专为连续依赖数据设计的对比SSL理论框架，定义了两种对象间可能的‘真实相似性度量’——硬接近性和软接近性。基于此，导出了一个能够同时适应这两种接近性的估计相似性矩阵，从而引入了依赖感知的损失函数。

**结果:** 提出的Dependent TS2Vec方法在标准UEA和UCR基准测试中分别提高了4.17%和2.08%的准确率，在涉及复杂时空模式的干旱分类任务上，ROC-AUC得分提高了7%。

**结论:** 该研究证明了所提出的理论支持的损失函数对于捕捉时空依赖性非常有效，新方法在多个依赖数据任务上优于当前方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+theoretical+framework+for+self-supervised+contrastive+learning+for+continuous+dependent+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09785，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09785&send_immediately=true&force_search=false)

**原文摘要:** Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [61] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals, Vasilis Belis, Elias F. Combarro, Eduard Alarcón, Sofia Vallecorsa, Michele Grossi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种指导图压缩（GGC）框架，它使用图自动编码器来减少节点数量和节点特征的维度，从而提高下游分类任务性能，并且可以与量子或经典分类器一起使用。在高能物理中的喷射标记任务上测试了该框架，结果表明GGC优于单独使用自动编码器预处理步骤以及基线经典GNN分类器，并有助于在实际数据集上测试新的QGNN假设。


<details>
  <summary>更多</summary>
  
**动机:** 图神经网络（GNNs）在处理图结构数据方面有效，但在处理大型图时会遇到内存需求高和GPU上稀疏矩阵操作效率低的问题。量子计算为解决这些问题提供了有希望的方向，并激发了新的算法方法。特别是，最近的文献探讨了量子图神经网络（QGNNs）。然而，当前的量子硬件限制了可有效编码的数据维度。现有方法要么手动简化数据集，要么使用人工图数据集。

**方法:** 引入了指导图压缩（GGC）框架，该框架利用图自动编码器来同时减少节点的数量和节点特征的维度。这种压缩是为了增强下游分类任务的性能，并且可以应用于量子或经典分类器。

**结果:** GGC框架在Jet Tagging任务上进行了评估，这是一个区分由夸克引起的粒子喷射和由胶子引起的粒子喷射的重要分类问题。GGC的表现超过了将自动编码器作为独立预处理步骤使用的情况，也超过了经典的GNN分类器基线。此外，GGC还促进了新颖QGNN方案在现实数据集上的测试。

**结论:** 指导图压缩（GGC）框架通过减少图的规模和特征维度，在保持甚至提高分类性能的同时，使量子图神经网络能够在真实世界的数据集上进行测试。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Guided+Graph+Compression+for+Quantum+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09862，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09862&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [62] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman, Ilija Trajkovic, Julia Kaltenborn, Francis Pelletier, Alex Archibald, Yaniv Gurwicz, Peer Nowack, David Rolnick, Julien Boussard*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于因果表示学习的可解释气候模型模拟器，该模拟器能够通过物理信息方法和贝叶斯滤波器来准确学习气候动态，并在合成数据集及两个广泛应用的气候模型数据上验证了其各组件的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的气候变化模型使用复杂的耦合方程系统来模拟地球系统的物理过程，这些模拟计算成本高昂，限制了我们对气候变化的预测及其原因和影响的分析。机器学习有潜力快速模拟气候模型的数据，但现有的方法无法纳入基于物理信息的因果关系。

**方法:** 开发了一个基于因果表示学习的可解释气候模型模拟器，其中包含一个用于长期自回归模拟稳定的贝叶斯滤波器。

**结果:** 所提出的模拟器能够准确地学习气候动态，并且在现实的合成数据集以及来自两个广泛使用的气候模型的数据中展示了每个组件的重要性。

**结论:** 通过结合物理信息和因果关系，所开发的气候模型模拟器提供了一种新的方式来提高气候预测的效率和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causal+Climate+Emulation+with+Bayesian+Filtering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09891，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09891&send_immediately=true&force_search=false)

**原文摘要:** Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [63] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng, Tianming Yang*

**主要类别:** cs.LG

**AI概要:** 本文揭示了扩散蒸馏的局限性，并提出使用独立的GAN目标可以克服这些问题，将扩散模型转换为高效的一步生成器。通过冻结大部分参数的微调预训练模型，只需少量图像即可获得强大的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前扩散蒸馏技术存在需要大量训练且学生模型表现下降的问题。尽管结合GAN目标可能有所帮助，但其背后的机制尚未明确。

**方法:** 研究者首先识别出蒸馏的一个关键限制：教师和学生模型之间的步长和参数数量不匹配导致它们收敛到不同的局部最小值。进一步证明，一个独立的GAN目标无需依赖蒸馏损失便足以克服这一限制，并能够将扩散模型转变为高效的单步生成器。基于此发现，提出了扩散训练可被视为一种生成预训练形式的观点。此外，通过对预训练模型进行微调（其中85%的参数被冻结），仅用0.2M张图片就实现了强劲的表现，并在使用5M张图片时接近SOTA结果。

**结果:** 实验表明，通过GAN目标对预训练模型进行轻量级微调，即使只用少量数据也能达到很好的生成效果。频率域分析进一步解释了一步生成能力的获得。

**结论:** 这项工作提供了关于扩散训练的新视角，强调了它作为强有力的生成预训练过程的角色，这可以成为构建高效一步生成模型的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Diffusion+Models%3A+From+Generative+Pre-training+to+One-Step+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09376，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09376&send_immediately=true&force_search=false)

**原文摘要:** Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [64] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu, Yuchao Lin, Xuan Zhang, Xiaofeng Qian, Shuiwang Ji*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的有效网络QHNetV2，该网络通过SO(2)等变操作和局部框架内的特征更新来实现全局SO(3)等变性，从而加速哈密顿矩阵预测。实验表明该模型在广泛的分子结构和轨迹上具有优越的性能和强大的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了加速电子结构计算中的哈密顿矩阵预测，同时避免昂贵的SO(3) Clebsch-Gordan张量积计算。

**方法:** 设计了名为QHNetV2的新网络，该网络引入了新的高效且功能强大的SO(2)等变操作，并在SO(2)局部框架内执行所有非对角特征更新和消息传递，以及在每个节点上的连续SO(2)张量积以融合节点特征。

**结果:** 广泛的实验表明，所提出的模型在QH9和MD17大型数据集上对于多种分子结构和轨迹都展示了卓越的性能，显示出其强大的泛化能力。

**结论:** SO(2)局部框架上的SO(2)操作为可扩展且对称意识的电子结构学习提供了一个有前景的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Prediction+of+SO%283%29-Equivariant+Hamiltonian+Matrices+via+SO%282%29+Local+Frames，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09398，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09398&send_immediately=true&force_search=false)

**原文摘要:** We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [65] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu, Kai Li, Junliang Xing, Yifan Zhang, Jian Cheng*

**主要类别:** cs.LG

**AI概要:** 本文提出了进化增强机制（EAM），一种结合了深度强化学习的学习效率和遗传算法全局搜索能力的框架，以解决组合优化问题。通过理论分析及基准测试证明了该方法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决组合优化问题中的离散结构和指数级解空间所带来的挑战，同时克服深度强化学习探索有限以及易陷入局部最优的问题，并结合遗传算法全球探索能力强但样本效率低、计算密集的特点。

**方法:** 提出了一种名为进化增强机制(EAM)的通用即插即用框架，该框架能够将深度强化学习(DRL)的学习效率与遗传算法(GA)的全局搜索能力相结合。EAM通过从学习策略生成解决方案，并利用领域特定的遗传操作如交叉和变异来改进这些方案。

**结果:** 在包括TSP、CVRP、PCTSP和OP在内的基准问题上进行了广泛测试，结果表明EAM相较于其他竞争基线显著提高了解决方案质量和训练效率。此外，还提供了关于演化解分布与策略分布之间KL散度上限的理论分析。

**结论:** EAM作为一种模型无关的方法，可以无缝集成到最先进的DRL求解器中，它不仅增强了探索还加速了收敛过程，从而有效提升了组合优化问题的求解性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synergizing+Reinforcement+Learning+and+Genetic+Algorithms+for+Neural+Combinatorial+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09404，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09404&send_immediately=true&force_search=false)

**原文摘要:** Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [66] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui, Shuiwang Ji*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种通过因果意识后训练（CAPT）来减少大型语言模型（LLMs）在预训练过程中获得的虚假相关性的方法，从而提高模型的泛化能力。实验表明，使用CAPT微调的3B规模的语言模型可以在仅使用100个同分布微调样本的情况下，在同分布和异分布任务上超越传统SFT和更大规模的LLMs。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型经常在处理异分布（OOD）样本时失败，因为它们在预训练期间学习到了虚假的相关性。为了缓解这个问题，研究者们提出了因果意识后训练（CAPT）的方法来减少这些虚假相关性，并增强模型的泛化能力。

**方法:** 通过将一个带有偏见的预测分解为两个无偏见的步骤——事件估计和事件干预，研究者实现了因果意识后训练（CAPT）。该方法旨在减少LLMs在预训练中产生的偏见，同时避免引入额外的微调偏见。

**结果:** 实验结果表明，在形式化的因果推理基准CLadder和逻辑推理数据集PrOntoQA上，采用CAPT进行微调的3B级语言模型能够只用100个同分布微调样本就在同分布和异分布任务上优于传统的SFT和更大的LLMs。

**结论:** 因果意识后训练（CAPT）被证明是一种有效且样本效率高的方法，可以用于减少大型语言模型中的虚假相关性，进而提高模型对同分布及异分布任务的泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Spurious+Correlations+in+LLMs+via+Causality-Aware+Post-Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09433，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09433&send_immediately=true&force_search=false)

**原文摘要:** While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [67] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye, Tao Sun, Qing Ling*

**主要类别:** cs.LG

**AI概要:** 本文对无攻击和拜占庭容错的去中心化学习进行了细致的泛化误差分析，考虑了异构数据并在较宽松的假设条件下进行。研究揭示了数据异质性、模型初始化和随机梯度噪声对泛化误差的影响，并指出恶意代理执行的拜占庭攻击对泛化误差有显著影响，且该负面影响与样本量无关。


<details>
  <summary>更多</summary>
  
**动机:** 去中心化学习在信号和信息处理领域受到广泛关注，但其泛化误差相对较少被探索。了解泛化误差对于评估训练模型在实际应用中的性能至关重要。

**方法:** 文章采用细粒度的泛化误差分析方法，既考虑了无攻击的情况也涵盖了拜占庭容错机制下的去中心化学习，同时使用了异构数据并基于较为温和的假设条件。

**结果:** 研究表明，数据异质性、模型初始化以及随机梯度噪声等因素会对去中心化学习的泛化误差产生影响；此外，发现拜占庭攻击对泛化误差有着显著影响，这种负面影响与数据异质性相关联而与样本数量无关。

**结论:** 通过理论分析和数值实验验证了提出的泛化误差分析结果，为理解去中心化学习算法的泛化能力提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+Error+Analysis+for+Attack-Free+and+Byzantine-Resilient+Decentralized+Learning+with+Data+Heterogeneity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09438&send_immediately=true&force_search=false)

**原文摘要:** Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [68] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts, Kyle Mylonakis, Sidhartha Roy, Kaan Kale*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为彩色玻璃变换(Stained Glass Transform)的技术，该技术可以为大型语言模型(LLM)的输入提供隐私保护，同时保持模型的实用性。通过将词嵌入转换成一种学习到的、随机的且序列依赖的形式，理论上可以确保数据隐私性，并且通过与高斯混合模型互信息理论的联系，提供了后验隐私估计。


<details>
  <summary>更多</summary>
  
**动机:** 由于AI计算基础设施的所有权成本高昂以及大规模语言模型稳健服务的挑战，导致了托管式Model-as-a-Service部署的激增。即使企业选择内部部署，计算基础设施也通常由多个团队共享以最大化投资回报。在两种情况下，所部署的模型仅对明文数据进行操作，因此企业数据所有者必须允许他们的数据以明文形式出现在共享或多租户计算基础设施上。这使得拥有私密或敏感数据的数据所有者在使用这类部署时犹豫不决或受到限制。

**方法:** 研究者们引入了彩色玻璃变换（Stained Glass Transform），这是一种学习得来的、随机的并且依赖于序列的大型语言模型词嵌入转换方法。这种方法从信息论的角度为LLM输入提供隐私保护，同时保留模型的效用。此外，还特别地将一类彩色玻璃变换与高斯混合模型的互信息理论相联系。

**结果:** 研究者们基于互信息计算出后验隐私估计，并通过令牌级别的隐私度量和标准LLM性能基准测试验证了经过变换的嵌入实例的隐私性和实用性。

**结论:** 这项工作介绍了一种新的方法来解决在多租户环境中使用大型语言模型时的数据隐私问题。通过应用彩色玻璃变换，能够在保证模型性能的同时增加数据隐私保护。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Obfuscations+Of+LLM+Embedding+Sequences%3A+Stained+Glass+Transform，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09452，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09452&send_immediately=true&force_search=false)

**原文摘要:** The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [69] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu, Defu Lian, Xiaolong Chen, Xu Huang, Jin Chen, Enhong Chen*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的损失函数形式，即RG²和RG×损失，通过泰勒展开从Softmax损失导出。这些新方法与ALS优化结合使用，不仅保证了泛化能力，还提高了收敛速度，在实际数据集上的评估表明其排名性能与Softmax损失相当或更优，并且大大加速了收敛过程。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Softmax（SM）损失在列表排序中表现出色，但在大规模对象空间的应用中存在显著的计算开销和可扩展性限制。为了解决这个问题，研究者们希望开发出能够直接与排名指标对齐的新损失公式。

**方法:** 研究者提出了两种新的损失公式：Ranking-Generalizable平方（RG²）损失和Ranking-Generalizable交互（RG×）损失，它们都是通过对Softmax损失进行泰勒展开得到的。此外，这些提出的RG损失与高效的交替最小二乘（ALS）优化方法相结合，提供了泛化的保障和收敛速率分析。

**结果:** 实证评估显示，所提出的方法在真实世界的数据集上实现了与Softmax损失相当甚至更好的排名表现，同时显著加快了收敛速度。

**结论:** 该框架为相似性学习领域提供了理论见解和实用高效的工具，适用于需要平衡排名质量和计算效率的各种任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NDCG-Consistent+Softmax+Approximation+with+Accelerated+Convergence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09454，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09454&send_immediately=true&force_search=false)

**原文摘要:** Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [70] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang, Rémi Munos*

**主要类别:** cs.LG

**AI概要:** 本文指出了在为大型语言模型（LLM）的强化学习训练中实现KL散度梯度估计时的一些常见陷阱，并展示了正确实施KL梯度的方法。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在指出并纠正当前开源项目和论文中对于KL散度梯度估计实现上的一些错误做法，特别是那些试图通过区分KL估计作为损失函数来最小化KL散度的做法。

**方法:** 首先说明直接通过KL估计作为损失函数进行区分是不正确的；其次，指出某些实现没有考虑到估计问题的序列性特征，从而只能产生部分梯度。通过表格示例和LLM实验来展示这些问题的影响，并给出正确的KL梯度实现方法。

**结果:** 展示了不当实现KL散度梯度估计可能导致的问题，包括无法得到期望的KL梯度以及只计算出部分梯度等。

**结论:** 强调了在LLM的RL训练过程中正确实现KL散度梯度估计的重要性，并提供了避免常见陷阱的指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+a+few+pitfalls+in+KL+divergence+gradient+estimation+for+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09477&send_immediately=true&force_search=false)

**原文摘要:** We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [71] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang, Du Yin, Hao Xue, Flora Salim*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的框架STOAT，用于时空因果时间序列（STC-TS）的概率预测。该方法通过整合空间关系矩阵来扩展因果推理方法，从而能够估计具有空间信息的因果效应，并使用深度概率模型处理潜在序列以估计分布参数，允许校准不确定性建模。实验表明，STOAT在关键指标上优于现有先进模型，特别是在具有强空间依赖性的区域。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法通常独立地模拟空间和时间动态，并忽视了因果驱动的概率预测，这限制了它们的预测能力。为了解决这个问题，研究提出了一个新框架STOAT，旨在提高STC-TS中的预测性能。

**方法:** STOAT框架通过引入空间关系矩阵来捕捉区域间的相互依赖性，如邻近或连通性，并利用这些信息进行因果效应评估。接着，它运用深层概率模型来估算潜在序列中分布的参数，支持多输出分布（例如高斯、学生t-分布、拉普拉斯）以捕捉特定于区域的变化性。

**结果:** 在六个不同国家的COVID-19数据集上的实验显示，STOAT在关键度量上优于当前先进的概率预测模型，尤其在那些具有强烈空间相关性的地区表现突出。

**结论:** 通过结合因果推断与地理空间概率预测，STOAT提供了一个通用框架，适用于复杂的时空任务，比如流行病管理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STOAT%3A+Spatial-Temporal+Probabilistic+Causal+Inference+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09544&send_immediately=true&force_search=false)

**原文摘要:** Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [72] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary, Wassim Uddin Mondal, Laxmidhar Behera*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Meta Offline-Online Reinforcement Learning (MOORL)的混合框架，该框架结合了离线和在线强化学习的优点，以实现高效且可扩展的学习。MOORL通过引入一个元策略无缝地适应离线和在线轨迹，从而在利用离线数据进行稳健初始化的同时，使用在线交互驱动有效的探索。实验表明MOORL在多个基准测试中表现出色，优于现有的离线和混合RL基线方法，并且计算开销小，具有实际应用潜力。


<details>
  <summary>更多</summary>
  
**动机:** 深度强化学习（DRL）中的样本效率和探索问题仍然是复杂领域中的重要挑战。虽然离线RL允许代理从静态预收集的数据集中学习最优策略，但它受到诸如分布外动作等问题的限制，这会影响策略性能和泛化能力。

**方法:** 提出了Meta Offline-Online Reinforcement Learning (MOORL)，这是一种将离线和在线强化学习统一起来的混合框架，旨在提高学习效率和可扩展性。MOORL采用了一个能够跨越离线与在线轨迹自适应的元策略，使得代理能够在利用离线数据进行稳健初始化的同时，也通过在线互动来促进高效的探索。

**结果:** 理论分析表明，这种混合方法通过有效地结合离线和在线数据的互补优势增强了探索。此外，实验证明MOORL可以在不增加额外复杂度的情况下学习到稳定的Q函数。在D4RL和V-D4RL基准测试的28项任务上进行了广泛的实验，结果表明MOORL相对于最先进离线及混合RL基线的一致改进。

**结论:** MOORL作为一种新的混合型强化学习框架，它在处理离线和在线数据方面表现出良好的性能，不仅提高了样本效率，而且减少了计算负担，显示出在实际应用场景中的巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MOORL%3A+A+Framework+for+Integrating+Offline-Online+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09574&send_immediately=true&force_search=false)

**原文摘要:** Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [73] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler, Lukas Kuhn, Florian Buettner*

**主要类别:** cs.LG

**AI概要:** 本文全面探讨了基础模型在不同分布下的校准行为，揭示了这些模型内部预测时的不自信问题以及在校准技术应用下的响应性，并指出架构和训练创新对校准有复杂的影响。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于确保深度神经网络在高风险应用场景中的安全部署，解决其系统性的过度自信问题，特别是当数据分布发生变化时。文章还特别关注了如ConvNeXt、EVA 和 BEiT 等基础模型的校准特性，这是之前较少被探索的一个方面。

**方法:** 作者通过实证分析方法来考察基础模型在标准分布内和分布偏移情况下的校准表现。他们也评估了后处理校准技术对改善基础模型在标准分布内预测不自信的有效性。

**结果:** 研究发现基础模型对于标准分布内的预测往往表现出不自信，导致更高的校准误差；而在分布偏移情况下校准有所改善。此外，虽然这些模型在标准分布设置下对后处理校准技术非常敏感，但在严重的分布偏移情况下，此类技术的效果变得不太可靠，有时甚至产生反效果。

**结论:** 研究结论指出了架构与训练创新对模型校准影响的复杂性和非单调性，这挑战了持续改进的一贯说法，并强调了理解这些影响的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Overconfidence%3A+Foundation+Models+Redefine+Calibration+in+Deep+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09593，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09593&send_immediately=true&force_search=false)

**原文摘要:** Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [74] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin, Hailin Wang, Jingyao Hou, Jianjun Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了两种快速准确的随机算法用于低秩张量逼近，并开发了一个新的广义非凸建模框架，以适应大规模张量恢复。此外还研究了统一的非凸模型和高效的优化算法，特别适用于未量化和量化的高阶张量恢复任务。实验结果表明，所提方法在实际性、有效性和优越性方面优于一些现有技术。


<details>
  <summary>更多</summary>
  
**动机:** 现有的张量恢复方法未能识别张量规模变化对其结构特性的影响，并且在处理大规模高阶张量数据时面临过高的计算成本。

**方法:** 借助Krylov子空间迭代、块Lanczos双对角化过程以及随机投影策略，设计了两个针对低秩张量逼近问题的快速精确随机算法；建立了一种新颖的广义非凸建模框架，该框架利用新的正则化范式来实现大规模张量的有洞察力的先验表示；进一步研究了几种典型高阶张量恢复任务的新统一非凸模型及高效优化算法。

**结果:** 对于不同的大规模张量进行了广泛的实验，结果展示了所提出的方法比某些最先进方法具有更好的实用性和有效性。

**结论:** 通过引入随机化LRTA方案，本文提出的算法能够有效地处理大规模张量数据，并在多个典型的高阶张量恢复任务中展现出色的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+Large-Scale+Regularized+High-Order+Tensor+Recovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09594，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09594&send_immediately=true&force_search=false)

**原文摘要:** Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [75] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo, Huan Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了SparseSSM，一种无需训练的剪枝框架，专门针对状态空间架构进行了经典最优脑外科医生（OBS）框架的扩展。通过分层算法计算近似二阶显著性分数，并结合组件敏感性分析指导前馈网络（FFN）剪枝，该方法可以轻松推广到半结构化和结构化稀疏性。实验表明，在不进行微调的情况下剪枝50%的SSM权重而不会损失零样本准确性，成为基于Mamba的大规模语言模型的当前最佳剪枝算法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的一步到位剪枝方法专为注意力块设计，未能考虑到选择性状态空间模块（SSM）核心的时间共享和离散状态转移矩阵。这导致了在诸如Mamba这样的状态空间语言模型中，尽管其允许线性复杂度推理并达到Transformer的质量，但仍有数十亿参数阻碍了实际部署。

**方法:** 引入了SparseSSM，这是一个无需训练的剪枝框架，它将经典的最优脑外科医生（OBS）框架扩展到了状态空间架构。提出的分层算法包括：(i) 导出一个近似的二阶显著性评分，该评分聚合了跨时间步长的Hessian迹信息；(ii) 结合了一个组件敏感性分析以指导前馈网络（FFN）剪枝；(iii) 可以很容易地扩展到半结构化和结构化稀疏性。

**结果:** 实验证明，可以在不需要微调的情况下剪枝掉50%的SSM权重，同时保持零样本准确率不变。这使得SparseSSM成为了基于Mamba的大规模语言模型的当前最佳剪枝算法。

**结论:** 研究者们开发了一种名为SparseSSM的新剪枝框架，能够有效地减少Mamba等状态空间语言模型中的参数数量，同时不影响性能。这一发现对于提高这类模型的可部署性和效率具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SparseSSM%3A+Efficient+Selective+Structured+State+Space+Models+Can+Be+Pruned+in+One-Shot，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09613&send_immediately=true&force_search=false)

**原文摘要:** State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [76] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina, Dmitry Shirokov*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于几何（Clifford）代数的新等变神经网络架构GLGENN，该网络对所有伪正交变换具有等变性，并且通过一种权重共享参数化技术减少了可优化参数的数量，在基准测试任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 为了开发一种新的等变量神经网络架构，该架构能够处理向量空间中的旋转和反射等伪正本变换，并且对于任何非退化或退化的对称双线性形式都保持等变性。

**方法:** 设计并实现了广义Lipschitz群等变神经网络（GLGENN），这种网络利用了几何代数的基本结构和运算，并引入了一种考虑这些特性的权重共享参数化技术。

**结果:** GLGENN在几个基准的等变任务上优于或者与竞争对手相匹配，同时使用了显著更少的可优化参数。

**结论:** GLGENN是一种有效的等变神经网络架构，它能够减少过拟合的风险，并且在性能上可以与现有的等变模型竞争甚至超越它们。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GLGENN%3A+A+Novel+Parameter-Light+Equivariant+Neural+Networks+Architecture+Based+on+Clifford+Geometric+Algebras，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09625，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09625&send_immediately=true&force_search=false)

**原文摘要:** We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [77] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens, Alberto Gutierrez, Jordi Torres, Josep. Ll Berral, Anisa Halimi, Kieran Fraser*

**主要类别:** cs.LG

**AI概要:** 本文探讨了大型语言模型通过上下文学习生成合成表格数据时，上下文示例中存在的统计偏差如何传播到合成数据中，并且引入了一个对抗性场景来展示恶意参与者如何通过部分上下文示例向合成数据集中注入偏差，从而影响下游分类器的公平性。


<details>
  <summary>更多</summary>
  
**动机:** 在现实世界的应用中，用于增强数据的数据往往是噪声大且人口统计数据有偏的。先前的工作通常假设可以获得无偏的上下文示例子集，但这些假设可能不成立。因此，研究者们希望了解上下文示例中的统计偏差如何影响合成数据的分布，并探索这种偏差是否会导致下游任务的不公平。

**方法:** 通过对不同偏差程度的上下文示例进行系统的研究，以及设计一个对抗性的场景，其中恶意贡献者能够通过一小部分上下文示例将偏差注入合成数据集，从而评估对目标和受保护子群体的下游分类器公平性的影响。

**结果:** 研究表明即使是很小的上下文偏差也会导致全局统计失真。此外，在对抗性场景下，恶意贡献者确实可以通过控制一部分上下文示例来损害特定子群体的利益，这表明基于LLM的数据生成管道存在新的漏洞。

**结论:** 依赖于敏感领域内上下文提示的LLM数据生成流程存在着与统计偏差相关的脆弱性，这可能会被利用来破坏下游分类器对于特定子群体的公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是In-Context+Bias+Propagation+in+LLM-Based+Tabular+Data+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09630&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [78] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng, Ziyue Lin, Pengxin Guo, Yuyin Zhou, Feifei Wang, Liangqiong Qu*

**主要类别:** cs.LG

**AI概要:** 本文提出了FedVLMBench，这是一个用于视觉-语言模型联邦微调的系统性基准测试。它涵盖了两种主流VLM架构、四种微调策略、五种联邦学习算法以及六个跨领域的多模态数据集，并通过大量实验揭示了VLM架构、微调策略、数据异质性和多任务联邦优化之间的关键见解。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉-语言模型（VLMs）虽然在跨模态理解和生成方面表现出色，但大多依赖于集中式训练，这在医疗等对隐私要求严格的领域中带来了挑战。尽管联邦学习（FL）已被引入以解决这些隐私问题，但目前缺乏一个全面的基准来评估联邦微调策略、模型架构和任务泛化能力。

**方法:** 研究者构建了一个名为FedVLMBench的基准，它整合了两种主流VLM架构（基于编码器的和无编码器的）、四种微调策略、五种联邦学习算法及多个跨域多模态数据集。

**结果:** 实验结果表明，对于基于编码器的VLMs，在联邦学习中使用2层多层感知机（MLP）连接器并同时调整连接器和大型语言模型是最佳配置。此外，当前的联邦学习方法对视觉为中心的任务比文本为中心的任务更加敏感于数据异质性。

**结论:** FedVLMBench为研究社区提供了必要的工具、数据集和实证指导，提供了一个标准化平台以推进隐私保护的多模态基础模型的联邦训练。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedVLMBench%3A+Benchmarking+Federated+Fine-Tuning+of+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09638，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09638&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [79] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül, Stefanos Tziampazis, Nasser Jazdi, Michael Weyrich*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为SyncFed的时间感知联邦学习框架，通过显式同步和时间戳来量化陈旧性，并在聚合过程中应用基于时间信息的加权，从而提高了模型准确性及信息的新鲜度。


<details>
  <summary>更多</summary>
  
**动机:** 随着联邦学习（FL）扩展到更大、更分散的环境中，网络引起的延迟、时钟不同步以及客户端更新的可变性给训练的一致性带来了挑战。这些因素可能导致贡献不一致，从而破坏了模型的可靠性和收敛性。现有的方法如陈旧意识聚合和模型版本控制虽然能够启发式地处理滞后的更新，但缺乏量化陈旧性的机制，特别是在对延迟敏感和跨区域部署中。

**方法:** 本文介绍了SyncFed，一种时间感知的联邦学习框架，该框架采用显式同步和时间戳来在整个系统中建立一个共同的时间参考。基于网络时间协议(NTP)交换的时间戳数值上量化了陈旧性，使服务器能够推理客户更新的相对新鲜度，并在聚合过程中应用基于时间的信息权重。

**结果:** 在地理分布测试平台上的实证评估表明，在SyncFed下，全局模型在一个稳定的时间上下文中演化，与缺乏时间语义的基于轮次的基线相比，提高了准确性和信息新鲜度。

**结论:** 通过引入SyncFed，本文提供了一种解决联邦学习中由于网络延迟等因素导致的更新不同步问题的有效方法，实现了更好的模型性能和数据时效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SyncFed%3A+Time-Aware+Federated+Learning+through+Explicit+Timestamping+and+Synchronization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09660，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09660&send_immediately=true&force_search=false)

**原文摘要:** As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [80] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi, Davide Leo, Davide Carbone*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为WAFFLE的算法，该算法在联邦学习中利用小波散射变换(WST)或傅里叶变换生成压缩表示来检测异常客户端，无需访问原始数据，并且在基准数据集上的实验表明，与现有的FL异常检测算法相比，该方法提高了检测准确性和下游分类性能。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习中的异常或损坏客户端（例如传感器故障或数据分布非代表性）可能会显著降低模型性能。如何在不访问原始数据的情况下检测这些客户端是一个关键挑战。

**方法:** 提出了WAFFLE算法，它使用本地计算的小波散射变换(WST)或傅里叶变换产生的压缩表示来标记恶意客户端。两种方法都提供了低维度、任务无关的嵌入，适合无监督的客户端分离。一个轻量级的检测器通过对公共数据集进行蒸馏训练，以最小的通信和计算开销执行标记。

**结果:** 实验结果表明，与现有的联邦学习异常检测算法相比，所提方法提高了检测准确性及后续分类性能。

**结论:** WAFFLE算法提供了一种有效的预训练替代方案，用于检测联邦学习环境下的异常客户端，其中WST由于其不可逆性和对局部变形的稳定性而特别适用于联邦场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wavelet+Scattering+Transform+and+Fourier+Representation+for+Offline+Detection+of+Malicious+Clients+in+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09674，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09674&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [81] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta, Pietro Liò*

**主要类别:** cs.LG

**AI概要:** 本文介绍了Wasserstein超图神经网络，它将节点和超边邻域视为分布，并使用Sliced Wasserstein Pooling来聚合信息。该方法能够保持分布的几何特性，从而在节点分类任务上取得了显著的优势。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于图表示学习在过去十年中已成为主流，但通过超图表示高阶关系正在迅速获得关注。当前大多数超图神经网络模型仍然借鉴了图的方法，通常将聚合简化为基本的池化操作。为了克服这一局限性，本文提出了一个新模型，旨在更好地捕捉分布的几何属性。

**方法:** 提出了一种名为Wasserstein超图神经网络的新模型，该模型将节点和超边邻域视为概率分布，并利用Sliced Wasserstein Pooling来聚集信息。与传统的均值或求和聚合器相比，这种方法能够保留分布的形状和扩展等几何属性。

**结果:** 实验结果表明，在超图环境下应用Wasserstein池化对于节点分类任务具有显著的好处，在多个真实世界数据集上达到了最佳性能。

**结论:** Wasserstein超图神经网络通过采用Sliced Wasserstein Pooling作为信息聚合手段，能够有效地保存节点和超边邻域的几何性质，这使得所学得的嵌入能够反映一个超边分布转变为另一个分布的难易程度。此方法在实际应用中展示了其优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wasserstein+Hypergraph+Neural+Network，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09682，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09682&send_immediately=true&force_search=false)

**原文摘要:** The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [82] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas, Georgios Paraskevopoulos, Alexandros Potamianos*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的神经网络架构——自动压缩网络（ACNs），通过用从每一层到输出的长前馈连接替换传统的短残差连接，实现了信息的有机压缩。这种设计不仅增强了早期层的表征质量，还揭示了深层中的潜在冗余。ACNs表现出更好的噪声鲁棒性、在低数据设置下的性能提升、改进的迁移学习能力以及减轻灾难性遗忘的能力。实验结果表明，在保持准确性的前提下，该方法可减少高达18%的灾难性遗忘，并实现30-80%的架构压缩。此外，将ACNs与传统剪枝技术结合使用时，能够比传统架构获得更优的稀疏性-性能权衡。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度神经网络在多个领域取得了显著的成功，但增加网络深度往往会导致计算冗余，而不会相应地提高表征质量。基于此，研究者旨在探索一种新架构来解决这一问题。

**方法:** 研究者引入了一种名为自动压缩网络（Auto-Compressing Networks, ACNs）的新架构变体，它利用从每个层到输出的加法长前馈连接替代了传统的短残差连接。通过这种方式，网络能够在训练过程中自然地压缩信息，即所谓的“自压缩”特性。

**结果:** 实验结果显示，ACNs相比残差网络具有更强的噪声鲁棒性、在少量数据场景下的优越表现、改进后的迁移学习能力和减轻了灾难性遗忘的问题。此外，该架构还能在保持精度的同时减少高达18%的灾难性遗忘，并实现30-80%的结构压缩。当与传统修剪技术相结合时，ACNs能提供比常规架构更好的稀疏度-性能平衡。

**结论:** 研究表明，ACNs是一种实用的方法，用于开发能够根据任务复杂度自动调整其计算量的有效神经架构，同时学习更加鲁棒的表征。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Auto-Compressing+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09714，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09714&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [83] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang, Zeyang Zhang, Linxin Xiao, Haibo Chen, Chendi Ge, Wenwu Zhu*

**主要类别:** cs.LG

**AI概要:** 本文探讨了多模态图大语言模型（MG-LLM）在统一和泛化不同类型多模态图数据和任务方面的潜力，并提出了MG-LLM的五个关键特性，分析了主要挑战、回顾了相关工作，并指出了未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态图学习方法通常针对特定图数据和任务从零开始训练，无法跨多种多模态图数据和任务进行泛化。

**方法:** 提出了一种多模态图数据、任务和模型的统一框架，发现了多模态图中固有的多粒度和多尺度特性，并阐述了MG-LLM的五个关键期望特性。

**结果:** 论文总结了现有相关的多模态图数据集，以支持模型训练，并认为该文可以为MG-LLM的研究进展做出贡献，以便在多模态图数据和任务上实现泛化。

**结论:** 文章为多模态图大语言模型(MG-LLM)的发展提供了基础，指出了其在处理多模态图数据和任务时的潜在优势及未来研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Multi-modal+Graph+Large+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09738，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09738&send_immediately=true&force_search=false)

**原文摘要:** Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [84] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He, Chaozhuo Li, Peng Tang, Litian Zhang, Sen Su*

**主要类别:** cs.LG

**AI概要:** 本文介绍了针对本地私有图学习协议的首次数据投毒攻击，揭示了这种攻击如何通过注入虚假用户、操纵链接和发送精心设计的数据来破坏图学习的实用性，并探讨了防御策略的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管本地私有图学习协议在保护用户隐私方面表现出色，但它们可能受到数据投毒攻击的影响，这威胁到隐私保护图学习框架的稳健性和安全性。

**方法:** 研究者提出了一种新的数据投毒攻击方法，该方法涉及向协议中注入虚假用户，建立与真实用户的连接，并向服务器发送特定构造的数据。

**结果:** 实验结果表明，所提出的攻击对私人图学习的实用性造成了显著损害。此外，虽然已经探索了几种防御措施，但其有效性有限。

**结论:** 本文的研究强调了开发更强大的防御机制以应对针对本地私有图学习协议的数据投毒攻击的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Devil%27s+Hand%3A+Data+Poisoning+Attacks+to+Locally+Private+Graph+Learning+Protocols，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09803&send_immediately=true&force_search=false)

**原文摘要:** Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [85] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong, Alfred Hero*

**主要类别:** cs.LG

**AI概要:** 本文介绍了ProjNCE，一种新的损失函数，它统一了监督对比和自监督对比学习目标，并且证明了它是互信息的有效下界。通过实验表明ProjNCE在多个数据集上优于SupCon和标准交叉熵训练。


<details>
  <summary>更多</summary>
  
**动机:** 尽管自监督对比学习（SSCL）已经从多角度被研究，但监督对比（SupCon）方法在这方面得到的关注较少。InfoNCE是SSCL中使用的已知形成互信息（MI）的下界，而SupCon与MI之间的关系尚未探索。

**方法:** 引入了一种名为ProjNCE的新损失函数，这是InfoNCE损失的推广，通过结合投影函数和负对调整项来统一监督和自监督对比学习的目标。还探讨了基于质心的类嵌入在SupCon中的应用。

**结果:** 理论证明了ProjNCE构成了有效的MI边界，并提供了更大的灵活性来选择类嵌入的投影策略。广泛的实验表明，ProjNCE在多个数据集和设置上始终优于SupCon和标准交叉熵训练。

**结论:** 这项工作通过两个互补的角度——互信息解释和投影设计——改进了SupCon，并在SupCon作为基础对比目标时提供广泛适用的改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalizing+Supervised+Contrastive+learning%3A+A+Projection+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09810&send_immediately=true&force_search=false)

**原文摘要:** Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [86] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia, Benjamin Schiffer, Serena Wang, Shirley Zhang*

**主要类别:** cs.LG

**AI概要:** 本文通过社会选择理论的概念，提出了两种评价指标子集选择的代表性概念：位置代表性与位置比例性，并探讨了在最坏情况下保证这两种性质所需的最小指标数量，同时提供了实际案例研究。


<details>
  <summary>更多</summary>
  
**动机:** 在大型语言模型（LLM）评估中，从所有可能的指标中选择一个子集是一个常见问题。由于效率或可解释性的原因，通常需要选择一组‘代表性’的指标，但‘代表性’这一概念往往没有明确的定义。

**方法:** 论文使用社会选择理论的思想来形式化两个用于选择评价指标子集的代表性概念：位置代表性和位置比例性。对于位置代表性，确保每个选项在每个位置截断点上都有足够的代表性；而对于位置比例性，则是确保没有选项在任何位置上被不成比例地过度或不足代表超过一个小误差。此外，还研究了允许对必须被代表的度量组提供额外输入的每种属性的一般形式。

**结果:** 作者证明了在最坏的情况下，为了保证这两种性质中的任意一种所需要的最小度量数量的上下界。

**结论:** 该工作不仅为度量子集的选择提供了理论基础，而且还通过LLM评估和医院质量评估的实际案例研究将理论联系到实践中，展示了这些新概念如何帮助解决实际问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Metritocracy%3A+Representative+Metrics+for+Lite+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09813&send_immediately=true&force_search=false)

**原文摘要:** A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [87] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo, Sören Becker, Niki Kilbertus*

**主要类别:** cs.LG

**AI概要:** 本文探讨了稀疏线性常微分方程（ODE）的可识别性问题，表明在实际相关的稀疏条件下，与稠密矩阵不同，稀疏系统以一定概率不可识别，并提供了该概率的下界。此外，通过实证研究显示，这种理论上的不可识别性也在现有估计线性ODE的方法中显现出来，提示我们需要重新思考数据驱动的动力系统建模能够达到什么样的预期。


<details>
  <summary>更多</summary>
  
**动机:** 动力系统建模是自然科学和生命科学领域科学研究的核心支柱之一。随着越来越多的动力系统模型从数据中学习而来，可识别性成为了一个极其重要的概念。对于那些不能从数据中被识别出来的系统，我们无法对其在新的条件和输入下的行为做出保证，也无法对可能的控制机制进行把握。尽管大家知道“线性常微分方程几乎肯定可以从单一轨迹中被识别”，但这一结论仅适用于稠密矩阵。而现实中很多生物、社会和物理系统自然呈现出稀疏特性，因此，探索稀疏体系的可识别性具有重要意义。

**方法:** 本文通过数学分析来刻画稀疏线性ODE的可识别性特征，证明了在实用相关稀疏条件下，稀疏系统有正的概率是不可识别的，并为这个概率提供了下界。同时，作者们还采用实证方法研究了这种理论上不可识别性如何体现在最先进的用于从数据估计线性ODE的方法中。

**结果:** 研究表明，在实际相关的稀疏度条件下，稀疏线性ODE存在一定的概率是不可识别的，并且给出了不可识别性的概率下限。此外，实证结果证实了这种不可识别性同样存在于当前先进的数据驱动线性ODE估计方法中。这意味着理论上的限制不会因为归纳偏置或优化动态而得到解决。

**结论:** 研究发现呼吁重新考虑数据驱动的动力系统建模所能达到的预期，并允许对学习到的线性ODE的信任程度进行定量评估。这表明稀疏系统的不可识别性是一个需要认真对待的问题，它不仅影响着理论层面的理解，也对实践中应用这些模型时所作决策的质量提出了挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Identifiability+Challenges+in+Sparse+Linear+Ordinary+Differential+Equations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09816，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09816&send_immediately=true&force_search=false)

**原文摘要:** Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [88] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani, Sonia Ben Mokhtar, Pierre-Edouard Portier, Elod Egyed-Zsigmond, Diana Nurbakova*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为WoLA（Worker Label Alignement Loss）的加权损失函数，该方法能够在数据异构的情况下对齐诚实工作者的梯度，从而有助于识别拜占庭参与者的恶意梯度。这种方法在异构环境下显著优于现有的最先进方法，并提供了理论分析和实证结果。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习(FL)允许多个数据持有者在不与外部方共享训练数据的情况下协作训练机器学习模型，但从安全角度来看，它面临着拜占庭参与者可能贡献有毒梯度而损害模型收敛性的威胁。虽然有抵抗拜占庭故障的策略存在，但在异构设置中诚实梯度之间的差异可能会很大，使得难以区分诚实异常值和拜占庭异常值。

**方法:** 研究者引入了Worker Label Alignment Loss (WoLA)，这是一种加权损失函数，旨在即使在数据异构的情况下也能对齐诚实工作者的梯度，从而促进拜占庭梯度的识别。

**结果:** 通过使用WoLA方法，在异构环境中能够显著优于当前最先进的方法。

**结论:** 通过提供理论见解和实证证据，本文证明了WoLA方法的有效性，表明它可以提高联邦学习系统在面对拜占庭攻击时的安全性和鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weighted+Loss+Methods+for+Robust+Federated+Learning+under+Data+Heterogeneity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09824，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09824&send_immediately=true&force_search=false)

**原文摘要:** Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [89] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey, Wasim Arif, Rakhesh Singh Kshetrimayum*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种基于微波谐振传感器和机器学习的油品分类方法，通过提取油品介电性质引起的传感器响应特征来区分不同的油品。实验结果表明使用随机森林分类器可以达到99.41%的高精度，适用于工业环境中的实时油品识别。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于开发一种非破坏性、低功耗且适合于工业实时应用的方法来根据油品的介电特性对其进行分类。

**方法:** 采用微波谐振传感器收集不同油样的介电响应，并从这些响应中提取显著特征。然后利用多种机器学习分类器对这些特征进行训练和评估，以确定它们在区分油品种类方面的能力。

**结果:** 实验结果显示，使用随机森林分类器时，所提方法能够实现99.41%的高分类准确率，表明其在自动化油品识别上具有强大的潜力。

**结论:** 该系统以其紧凑的设计、高效性和高性能，为工业环境中快速可靠的油品表征提供了一个可行的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Machine+Learning-Based+Classification+of+Oils+Using+Dielectric+Properties+and+Microwave+Resonant+Sensing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09867，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09867&send_immediately=true&force_search=false)

**原文摘要:** This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [90] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso, Silvija Kokalj-Filipovic, Yagna Kaasaragadda*

**主要类别:** cs.LG

**AI概要:** 研究了VQVAE在高信噪比射频数据点受到对抗攻击时的攻击抑制特性，通过对比不同类型的对抗攻击对分类准确性的影响，并评估了VQVAE重构的数据点在分类器上的表现。结果显示VQVAE能够显著降低攻击的有效性，并通过多种方法和度量标准比较了有无攻击情况下的VQVAE潜在空间的概率分布。


<details>
  <summary>更多</summary>
  
**动机:** 探索VQVAE对于数字调制波形在遭受针对幅度调制的对抗攻击时是否具有攻击抑制能力，以及如何影响分类准确性和潜在空间特征。

**方法:** 设计了一个将数字无线电波形映射到离散潜在空间的VQVAE模型；创建了保留相位信息的对抗性攻击和不保留相位信息的攻击；测试了原始数据训练的分类器上这些对抗样本的分类准确性；评估了VQVAE重构后的对抗数据点在分类器上的性能；比较了被攻击数据、其重构及原始数据在I/Q平面上的图表；使用多种方法和指标比较了在有无攻击情况下VQVAE潜在空间的概率分布。

**结果:** VQVAE显著降低了对抗攻击的有效性；通过I/Q平面图展示了被攻击数据与其重构之间的差异；观察到了随着攻击强度变化，离散空间表现出有助于检测攻击的有趣属性。

**结论:** VQVAE可以有效地抑制针对高信噪比RF数据点的对抗攻击效果，同时通过分析潜在空间概率分布的变化为检测此类攻击提供了可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+look+at+adversarial+attacks+on+radio+waveforms+from+discrete+latent+space，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09896，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09896&send_immediately=true&force_search=false)

**原文摘要:** Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [91] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise, Vijeth Hebbar, Riya Shah, Cedric Langbort*

**主要类别:** cs.LG

**AI概要:** 本文讨论了一种新的可解释强化学习方法，称为多样化近优替代方案（DNA），该方法通过局部奖励塑形来求解具有保证ε-最优的不同策略，以产生在欧几里得空间中定性多样化的轨迹。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决强化学习中的可解释性问题，提出了一种名为多样化近优替代方案（DNA）的方法，旨在为路径规划代理提供一系列合理的“选项”，并优化策略以生成在欧几里得空间中定性不同的轨迹。

**方法:** DNA方法利用基于价值函数的策略，并在马尔可夫决策过程中对连续轨迹的代理应用局部修改的Q学习问题中的奖励塑形技术，从而找到有保障的ε-最优且各不相同的策略。

**结果:** 实验表明，DNA能够成功返回定性不同的策略，这些策略构成了有意义不同的“选项”。此外，文中还简要比较了随机优化领域质量多样性方面的相关方法。

**结论:** 除了增强可解释性外，这项工作还为强化学习中的探索和自适应规划开辟了新的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%22What+are+my+options%3F%22%3A+Explaining+RL+Agents+with+Diverse+Near-Optimal+Alternatives+%28Extended%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09901，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09901&send_immediately=true&force_search=false)

**原文摘要:** In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [92] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang, James Joshi, Ashish Kundu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的针对机器遗忘（MU）的隐私攻击Apollo，该攻击仅基于未学习模型的标签输出来推断数据样本是否已被移除，并且在对目标模型访问权限较少的情况下仍能实现较高的准确率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的针对MU的隐私推理攻击依赖于较弱的威胁模型，即假设攻击者可以访问未学习模型和原始模型，这限制了它们在实际场景中的可行性。为了克服这一局限性，作者提出了一个新的、更为严格的威胁模型下的攻击方法。

**方法:** 作者设计了一个名为Apollo的后验标签唯一成员推理攻击，该攻击仅利用未学习模型提供的标签输出，以确定某个数据样本是否已经被从训练集中删除。

**结果:** 实验结果表明，与先前需要更多访问权限的攻击相比，Apollo攻击即使在严格条件下也能达到相对较高的精度来判断样本是否被遗忘。

**结论:** 研究证明了Apollo攻击的有效性，它能够在不直接访问模型内部信息的情况下，通过观察模型的输出标签来推断出哪些数据点已经被从训练数据中移除，从而为MU的安全性带来了新的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Apollo%3A+A+Posteriori+Label-Only+Membership+Inference+Attack+Towards+Machine+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09923&send_immediately=true&force_search=false)

**原文摘要:** Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [93] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu, Tong Zhang, Ehsan Pajouheshgar, Sabine Süsstrunk*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的特征蒸馏范式CaDistill，通过使用Canonical LAtent Representations (CLAReps)来提取核心类知识，从而在训练过程中提高学生的对抗鲁棒性和泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 条件扩散模型（CDMs）虽然在生成任务中表现出色，但其建模能力导致类别定义特征与不相关上下文纠缠在一起，这给提取稳健且可解释的表示带来了挑战。

**方法:** 研究者们识别出Canonical LAtent Representations (CLAReps)，这些潜在代码在保持关键分类信息的同时去除了非鉴别性信号。利用CLAReps，开发了基于扩散的特征蒸馏范式CaDistill，在这种模式下，学生能够访问完整的训练集，而作为教师角色的CDM仅通过CLAReps传递核心类知识。

**结果:** 经过训练后，学生模型达到了强大的对抗鲁棒性和泛化能力，并且更加关注于类别信号而非虚假背景线索。

**结论:** 研究表明，CDMs不仅可以作为图像生成器，还可以作为紧凑、可解释的教师促进稳健的表示学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Canonical+Latent+Representations+in+Conditional+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09955，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09955&send_immediately=true&force_search=false)

**原文摘要:** Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [94] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang, Yuwei An, Hongyi Liu, Tianqi Chen, Beidi Chen*

**主要类别:** cs.LG

**AI概要:** 介绍了Multiverse，一种新的生成模型，它通过MapReduce范式实现原生并行生成，并且在3小时微调后，性能与领先的同规模自回归大语言模型相当。


<details>
  <summary>更多</summary>
  
**动机:** 受到自回归大语言模型（AR-LLMs）在顺序生成中常表现出隐式并行性的启发，研究者们希望开发出能够实现自然并行生成的新模型。

**方法:** 创建了名为Multiverse的生成模型，该模型采用MapReduce模式，分为三个阶段：任务分解的映射阶段、并行子任务执行的过程阶段以及无损结果合成的归约阶段。为了从前沿的AR-LLMs快速无缝迁移，研究人员对数据、算法和系统进行了共同设计。

**结果:** 经过3小时使用1K示例进行微调后，Multiverse-32B成为了唯一开源的非自回归模型，其表现与相同规模的领先AR-LLMs相当，AIME24 & 25分数分别为54%和46%。此外，在预算控制实验中，Multiverse-32B在相同的上下文长度下平均超过AR-LLMs 1.87%，并且在不同批量大小上实现了高达2倍的速度提升。

**结论:** Multiverse模型展示了出色的可扩展性和实际效率增益，超过了现有的AR-LLMs，并且整个生态系统已经开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiverse%3A+Your+Language+Models+Secretly+Decide+How+to+Parallelize+and+Merge+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09991，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09991&send_immediately=true&force_search=false)

**原文摘要:** Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


### [95] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao, Johannes Zenn, Zhen Liu, Weiyang Liu, Robert Bamler, Bernhard Schölkopf*

**主要类别:** cs.LG

**AI概要:** 论文介绍了一种名为口头拒绝采样(VRS)的方法，该方法通过让大型语言模型(LLMs)用自然语言接受或拒绝提议的样本，从而减少伯努利分布中的采样偏差。VRS在不直接接触模型内部机制的情况下，利用经典概率工具来提高LLM工作流程的可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型能够准确地使用自然语言描述概率分布，但在从这些分布中生成忠实样本方面仍然存在困难。这种知识与采样之间的差距限制了它们在需要可靠随机性的任务中的应用，如蒙特卡洛方法、基于代理的模拟和随机决策。

**方法:** 研究者们提出了一种称为口头拒绝采样（Verbalized Rejection Sampling, VRS）的新方法，它是经典拒绝采样的自然语言版本，通过提示LLM对提出的样本进行推理并决定接受或拒绝。

**结果:** 尽管VRS依赖于相同的伯努利机制，但它显著减少了跨模型的采样偏差，并且理论分析表明，在温和假设下，VRS优于直接采样，其改进归功于算法本身及提示设计。

**结论:** 更广泛地说，研究结果展示了如何将经典的概率工具转化为语言形式，并嵌入到LLM的工作流中以提高可靠性，而无需访问模型内部或进行大量提示工程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flipping+Against+All+Odds%3A+Reducing+LLM+Coin+Flip+Bias+via+Verbalized+Rejection+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09998，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09998&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [96] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai, Zhenghao Peng, Bolei Zhou*

**主要类别:** cs.AI

**AI概要:** 提出了一种自适应干预机制（AIM），这是一种新的机器人门控交互式模仿学习算法，它能够通过代理Q函数来模仿人类干预规则，并根据代理和人类动作之间的一致性调整干预请求。实验表明，与基于不确定性的基线Thrifty-DAgger相比，AIM在减少专家监控努力、提高学习效率以及识别安全关键状态方面有显著优势。


<details>
  <summary>更多</summary>
  
**动机:** 当前的交互式模仿学习方法对人类监督者提出了高认知要求。为了减轻这种负担，研究者希望开发一种能更有效率地请求人类演示的新算法。

**方法:** 研究者提出了自适应干预机制（AIM），该机制使用代理Q函数来模仿人类干预规则，并根据代理与人类行为的一致性来调整何时请求人类干预。

**结果:** 实验结果表明，AIM可以显著减少专家在连续和离散控制任务中的监控工作量。与基于不确定性的基线方法Thrifty-DAgger相比，AIM在降低人类接管成本和提升学习效率上实现了40%的改进。此外，AIM能够有效地识别出需要专家协助的安全关键状态，从而收集到更高质量的人类演示数据并减少了所需的整体专家数据和环境互动。

**结论:** AIM提供了一个有效的解决方案，用于减少交互式模仿学习中人类监督者的认知负担，并提高了学习过程的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robot-Gated+Interactive+Imitation+Learning+with+Adaptive+Intervention+Mechanism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09176，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09176&send_immediately=true&force_search=false)

**原文摘要:** Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [97] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus, A. Lawsen*

**主要类别:** cs.AI

**AI概要:** 论文指出Shojaee等人关于大型推理模型在超过特定复杂度阈值的规划谜题上出现'准确性崩溃'的研究结果主要反映了实验设计的局限性而非根本性的推理失败。通过修正实验设计中的问题，多个模型在之前被认为完全失败的汉诺塔实例上表现出高准确性。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在揭示Shojaee等人研究中大型推理模型表现不佳的原因可能在于实验设计上的不足，而非模型本身的推理能力缺陷。

**方法:** 分析了原研究中的三个关键问题：超出模型输出令牌限制、评估框架不能正确区分推理失败与实际约束、以及河渡难题中存在数学上无解的情况。并提出通过请求生成函数而不是详尽的动作列表来控制这些问题。

**结果:** 初步实验表明，在修正上述实验设计问题后，多个模型能够准确解决先前报告为完全失败的汉诺塔问题实例。

**结论:** 研究表明，仔细设计实验对于正确评价AI推理能力至关重要；Shojaee等人的结论可能由于实验设计的问题而被误解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comment+on+The+Illusion+of+Thinking%3A+Understanding+the+Strengths+and+Limitations+of+Reasoning+Models+via+the+Lens+of+Problem+Complexity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09250，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09250&send_immediately=true&force_search=false)

**原文摘要:** Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [98] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, Zhengyu He*

**主要类别:** cs.AI

**AI概要:** 提出了Ming-Omni，一个能够处理图像、文本、音频和视频的统一多模态模型，并且在语音和图像生成方面表现出色。它使用专门的编码器从不同模态中提取tokens，然后由MoE架构Ling处理。该设计允许单一模型高效地处理和融合多模态输入，无需单独模型或任务特定微调。Ming-Omni还支持通过高级音频解码器实现自然声音的语音生成，以及通过Ming-Lite-Uni实现高质量图像生成。实验结果表明Ming-Omni为所有模态的统一感知和生成提供了一个强大的解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 创建一个能够处理多种类型数据（如图像、文本、音频和视频）并擅长生成语音和图像的统一多模态模型，以避免需要独立的模型或者针对特定任务进行微调的情况。

**方法:** 采用专门的编码器来从不同的模态中提取tokens，并利用名为Ling的MoE架构进行处理，该架构配备了新提出的模态特定路由器。为了实现音频和图像生成，集成了先进的音频解码器和用于高质量图像生成的Ming-Lite-Uni。

**结果:** 实验结果显示，Ming-Omni能够在所有模态上提供统一的感知和生成能力。这是已知的第一个开源模型，其在模态支持方面与GPT-4o相匹配。

**结论:** Ming-Omni是一个多功能的统一多模态模型，可以处理包括图像、文本、音频和视频在内的多种数据类型，并且在语音和图像生成方面表现卓越。它通过集成专业组件解决了传统多模态模型的一些局限性，提供了强大的跨模态解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ming-Omni%3A+A+Unified+Multimodal+Model+for+Perception+and+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09344&send_immediately=true&force_search=false)

**原文摘要:** We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [99] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/abs/2506.09390)
*Kehan Zheng, Jinfeng Zhou, Hongning Wang*

**主要类别:** cs.AI

**AI概要:** 本研究通过让大型语言模型（LLMs）参与经典策略游戏，如石头-剪刀-布和囚徒困境，来评估它们是否表现出与人类相似的有限理性。结果表明，尽管LLMs能够模仿一些典型的人类启发式行为，但它们在适应环境动态变化方面表现较弱，并且对对手建模不够灵活。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探索大型语言模型在战略决策环境中是否像人类一样偏离完全理性，以及它们在模仿人类有限理性方面的程度。

**方法:** 采用实验范式直接改编自行为博弈论研究，选取了两个经过充分研究的战略游戏：石头-剪刀-布和囚徒困境，以此来考察大型语言模型的行为。

**结果:** 大型语言模型再现了一些熟悉的人类启发式，例如基于结果的战略转换和对未来互动可能性增加的合作，但是它们更加僵硬地应用这些规则，并且对于游戏环境中的动态变化表现出较弱的敏感性。

**结论:** 当前的大型语言模型仅捕捉到了部分类似人类的有限理性形式，并强调需要开发训练方法以促进更灵活的对手建模和更强的情境意识。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Nash+Equilibrium%3A+Bounded+Rationality+of+LLMs+and+humans+in+Strategic+Decision-making，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09390&send_immediately=true&force_search=false)

**原文摘要:** Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [100] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Chunyu Miao, Dongyuan Li, Aiwei Liu, Yue Zhou, Yankai Chen, Weizhi Zhang, Yangning Li, Liancheng Fang, Renhe Jiang, Philip S. Yu*

**主要类别:** cs.AI

**AI概要:** 本文质疑了构建完全自主AI代理的路径，提出了一种基于大语言模型的人类-代理系统（LLM-HAS），其中AI与人类协作而非取代人类。通过保持人类参与来提供指导、回答问题和控制，这些系统可以更值得信赖和适应性强。文章通过医疗保健、金融和软件开发的例子展示了人机合作如何比AI单独工作更好地处理复杂任务，并讨论了构建协作系统的挑战及其实用解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 鉴于当前大型语言模型的进步导致许多研究者专注于创建完全自主的人工智能代理，本文旨在探讨这种趋势是否正确，因为全自主系统在可靠性、透明度以及理解人类实际需求方面仍存在问题。

**方法:** 作者提出了一个不同的方法：基于大语言模型的人类-代理系统（LLM-HAS），在这个框架下人工智能与人类协同工作。文中还列举了医疗、金融和软件开发等领域的案例来展示人机协作的优势。

**结果:** 研究表明，当人类参与到AI系统中时，可以提高系统的可信度和适应性，并且能够更好地完成复杂的任务。此外，文章还指出了实现这类协作系统所面临的挑战，并提供了实际的解决策略。

**结论:** 论文认为，人工智能的进步不应以系统独立性的提升为衡量标准，而应看其与人类合作的能力。未来最有希望的人工智能方向不是取代人类角色，而是通过有意义的合作增强人类能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Call+for+Collaborative+Intelligence%3A+Why+Human-Agent+Systems+Should+Precede+AI+Autonomy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09420，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09420&send_immediately=true&force_search=false)

**原文摘要:** Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [101] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)
*Jaesik Yoon, Hyeonseo Cho, Yoshua Bengio, Sungjin Ahn*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为Fast-MCTD的方法，该方法通过并行MCTD和平滑MCTD技术改进了原有的蒙特卡洛树扩散（MCTD）算法，在保持原有优势的同时显著提高了速度和可扩展性。实验表明，Fast-MCTD相比标准MCTD实现了高达100倍的速度提升，并且在某些任务中甚至超过了Diffuser的推理速度。


<details>
  <summary>更多</summary>
  
**动机:** 虽然蒙特卡洛树扩散（MCTD）为复杂的规划问题提供了最先进的表现，但其计算开销较大，因为树搜索的顺序性质以及迭代去噪的成本。为了提高效率，同时保持MCTD的优势，提出了更高效的Fast-MCTD变体。

**方法:** Fast-MCTD引入了两种技术：Parallel MCTD允许通过延迟树更新和冗余感知选择来实现并行展开；Sparse MCTD则通过轨迹粗化减少展开长度。

**结果:** 实验结果显示，Fast-MCTD与标准MCTD相比可达至100倍的速度提升，并且在一些任务上的推理速度上还优于Diffuser。

**结论:** Fast-MCTD被定位为一种实用且可扩展的解决方案，适用于基于扩散模型的推理时推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast+Monte+Carlo+Tree+Diffusion%3A+100x+Speedup+via+Parallel+Sparse+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09498，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09498&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [102] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu, Jiajun Chai, Sicheng Li, Yuqian Fu, Yuanheng Zhu, Dongbin Zhao*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于微调的大型语言模型（LLM）代理DipLLM，它能够学习外交游戏中的均衡策略。通过使用自回归因子分解框架简化多单位行动分配任务，并且只需要Cicero模型所需数据的1.5%，就能超越其表现。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法依赖于大量计算资源生成游戏数据来训练AI系统，而大型语言模型提供了一个有希望的替代方案，能够在相对较小规模的微调下取得良好的性能。然而，在外交游戏中应用这些模型仍然具有挑战性，因为可能的动作组合呈指数增长，玩家之间的战略互动也十分复杂。

**方法:** 作者提出了DipLLM，这是一种经过微调的基于大型语言模型的代理，旨在学习适用于外交游戏的均衡策略。该方法采用自回归因子分解框架，将复杂的多单位动作分配任务转化为一系列单元级决策。

**结果:** 研究结果表明，通过仅使用达到当前最佳Cicero模型所需数据量的1.5%进行微调，DipLLM不仅在性能上超过了Cicero，还展示了微调后的大型语言模型处理多人游戏中复杂策略决策的潜力。

**结论:** 本研究表明，微调后的大型语言模型如DipLLM可以有效地解决多人游戏中的复杂策略决策问题，即使是在像外交这样需要深度合作与竞争的游戏里。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DipLLM%3A+Fine-Tuning+LLM+for+Strategic+Decision-making+in+Diplomacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09655，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09655&send_immediately=true&force_search=false)

**原文摘要:** Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [103] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/abs/2506.09656)
*Wei Zeng, Hengshu Zhu, Chuan Qin, Han Wu, Yihang Cheng, Sirui Zhang, Xiaowei Jin, Yinuo Shen, Zhenxing Wang, Feimin Zhong, Hui Xiong*

**主要类别:** cs.AI

**AI概要:** 本文综述了特定应用场景下的智能体系统中的价值对齐问题，结合大型模型驱动的人工智能进展与社会治理需求，涵盖了价值原则、智能体系统应用场景以及智能体价值对齐评估，并探讨了智能体系统中多智能体间的价值协调，最后提出了该领域的几个潜在研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI范式的不断进化，研究重点转向了复杂环境中的多智能体自主决策和任务协作。由于大型语言模型的进步及其应用的多样化和复杂化，导致了情境性和系统性风险的增加，这使得对于确保AI智能体的目标、偏好和行为与人类价值观和社会规范保持一致的价值对齐问题得到了极大的关注。

**方法:** 本论文通过回顾特定应用场景下智能体系统中的价值对齐，从宏观、中观到微观层面组织价值原则，采用由一般到具体的视角分类并回顾了智能体系统应用场景，并系统地考察了用于价值对齐评估的数据集及相关价值对齐方法。此外，还深入探讨了智能体系统内多个智能体之间的价值协调问题。

**结果:** 文章整理了不同层次的价值原则，归纳总结了智能体系统在各种应用场景中的表现，评估了当前价值对齐的方法，并讨论了多智能体系统中实现价值协调的问题。

**结论:** 基于现有研究，文章提出了未来研究方向，包括进一步探索适用于多智能体系统的价值对齐机制，以及开发新的评价标准和技术以促进人工智能与社会规范的有效融合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Application-Driven+Value+Alignment+in+Agentic+AI+Systems%3A+Survey+and+Perspectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09656，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09656&send_immediately=true&force_search=false)

**原文摘要:** The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [104] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed, Uljad Berdica, Martha Elliott, Danijela Horak, Jakob N. Foerster*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为意图分解生成（IFG）的方法，通过将采样过程分为两个阶段来增加大型语言模型输出的多样性：首先抽取语义密集的意图，如摘要或关键词；然后根据原始提示和该意图生成最终响应。这种方法提高了多样性和推理任务表现，并且在不同任务中保持了生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的方法通常只在词元级别上增加多样性，导致推理问题探索不足以及对话代理乏味重复。

**方法:** 提出了意图分解生成（IFG），将抽样过程分解为两个阶段。第一阶段，抽取一个语义密集的意图，比如摘要或者关键词。第二阶段，在给定初始提示和第一阶段意图的情况下，生成最终的回答。同时，研究发现促使模型在每一步链式思考前明确其意图对推理任务是有益的。

**结果:** 该方法在多种任务中展现了有效性，包括提高数学和代码任务中的pass@k和从验证者反馈中学习强化学习的表现。对于指令调优，结合直接偏好优化增加了对话多样性而没有牺牲奖励。此外，还使用新收集的数据集证明了在保持生成质量的同时提高了多样性。

**结论:** 提供了一个简单有效的方法来增加大语言模型样本多样性，同时维持性能。此方法可以通过改变提示并在生成过程中调整温度来实现，易于集成到许多算法中以获得各种应用的收益。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Intent+Factored+Generation%3A+Unleashing+the+Diversity+in+Your+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09659，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09659&send_immediately=true&force_search=false)

**原文摘要:** Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [105] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/abs/2506.09977)
*Stylianos Loukas Vasileiou, Antonio Rago, Maria Vanina Martinez, William Yeoh*

**主要类别:** cs.AI

**AI概要:** 该论文通过三个用户研究发现，人们在面对矛盾信息时倾向于基于解释的信念修正，这种偏好并不总是符合经典信念改变理论。这些发现对旨在模拟人类推理或与人交互的人工智能系统有重要影响，建议这些系统应考虑基于解释、可能非最小化的信念修正机制，以更好地与人类认知过程相一致。


<details>
  <summary>更多</summary>
  
**动机:** 理解人类如何根据新信息更新自己的信念对于开发能够有效建模并因此与人类推理保持一致的人工智能系统至关重要。然而，虽然理论上的信念修正框架依赖于一套原则来确定这些操作是如何执行的，但来自认知心理学的经验证据表明，当面对冲突信息时，人们可能会遵循不同的模式。

**方法:** 进行了三项全面的用户研究，展示人们在遇到不一致的信息时，无论是提供给他们的还是自己形成的解释，都始终偏好基于解释的修正，这导致了他们信念系统的改变，而这些改变不一定被经典的信念改变理论所捕捉到。

**结果:** 实验系统地调查了人们如何用解释来修正他们的信念，揭示出在不同类型的场景下对看似非最小化修正的强烈偏好。

**结论:** 研究结果表明，为了更好地与人类的认知过程相一致，设计用来模拟人类推理或与人类互动的人工智能系统应该考虑到基于解释的、可能是非最小化的信念修正操作者。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Do+People+Revise+Inconsistent+Beliefs%3F+Examining+Belief+Revision+in+Humans+with+User+Studies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09977，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09977&send_immediately=true&force_search=false)

**原文摘要:** Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [106] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种自我监督学习方法，结合大规模互联网视频数据和少量机器人交互数据来训练模型，使其能够理解、预测并在物理世界中进行规划。预训练的V-JEPA 2模型在运动理解和人类行为预测上取得了很好的效果，并且通过与大型语言模型对齐，在多个视频问答任务上达到了领先水平。此外，通过使用不到62小时的未标记机器人视频后训练得到的V-JEPA 2-AC模型，可以实现零样本情况下对物体的拾取和放置操作。


<details>
  <summary>更多</summary>
  
**动机:** 现代人工智能面临的一大挑战是通过观察来学习理解世界并采取行动。本文旨在探索一种自我监督的方法，利用互联网规模的视频数据和少量的互动数据（机器人轨迹）来开发能够理解、预测和规划物理世界的模型。

**方法:** 研究者首先在一个包含超过100万小时网络视频的数据集上预训练了一个无动作的联合嵌入预测架构——V-JEPA 2。然后，将V-JEPA 2与一个大型语言模型对齐以增强其能力。最后，他们还展示了如何通过对来自Droid数据集的不到62小时未经标注的机器人视频进行后训练，来应用自我监督学习于机器人规划任务。

**结果:** V-JEPA 2在运动理解（Something-Something v2数据集上的top-1准确率为77.3%）和人类行为预期（Epic-Kitchens-100数据集上的recall-at-5为39.7%）方面表现出色；当与大语言模型结合时，在多项视频问答任务中也展示出了最新的性能指标。此外，V-JEPA 2-AC模型能够在两个不同的实验室环境中部署Franka机械臂，并基于图像目标执行物体抓取和放置的任务。

**结论:** 这项研究表明，从网络规模的数据和少量的机器人交互数据中进行自我监督学习，可以产生一个能够在现实世界中进行规划的世界模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是V-JEPA+2%3A+Self-Supervised+Video+Models+Enable+Understanding%2C+Prediction+and+Planning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09985，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09985&send_immediately=true&force_search=false)

**原文摘要:** A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [107] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta, Amos Storkey, Mirella Lapata*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种元学习方法，通过使用从任务相关图像特征中提取的固定软提示集来在大型多模态模型（LMMs）中诱导少量样本能力，并引入了一个注意力映射模块以促进这一过程。实验表明该方法在视觉问答任务中的表现优于现有的上下文学习和其他提示调优方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型多模态模型（LMMs）在执行新任务时依赖于上下文学习（ICL），但特别是在较小规模的LMMs中，ICL的表现不稳定且不总是随着示例数量增加而单调改进。研究者认为这是由于LMM被图像嵌入中对下游任务不必要的额外信息所困扰。

**方法:** 提出一种元学习方法，利用从任务相关图像特征提炼出的一组固定的软提示，这些软提示可以在测试时用少量样本来适应。为此，还引入了一个易于与流行的LLaVA v1.5架构集成的注意力映射模块，它与软提示一起学习，允许在低数据条件下仅通过几次梯度更新实现任务适应。

**结果:** 在VL-ICL基准上的评估显示，即使在图像受到干扰的情况下，本方法也始终优于ICL和相关的提示调优方法，改善了跨视觉问答任务的任务诱导和推理。

**结论:** 该研究提供了一种新的方式，通过元学习和特定设计的软提示来提高大型多模态模型在少量样本情况下的性能，证明了其对于增强LMMs处理视觉问答等任务的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta-Adaptive+Prompt+Distillation+for+Few-Shot+Visual+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.06905，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06905&send_immediately=true&force_search=false)

**原文摘要:** Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [108] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, Navid Azizan*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种通过分位数回归进行校准的方法来改进过程奖励模型（PRM）的准确性，并基于此引入了实例自适应缩放框架，该框架能够根据部分推理轨迹估计正确答案的概率动态调整推理预算。实验表明，所提方法降低了校准误差，实现了有效的自适应缩放策略，在减少推理成本的同时保持了最终答案的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 观察到即使是最先进的过程奖励模型也可能存在校准不良和过度估计成功概率的问题。

**方法:** 采用分位数回归技术对过程奖励模型输出进行校准；基于校准后的成功估计值及其置信边界提出了一个实例自适应缩放框架，该框架可根据每个问题实例和推理步骤动态调整推理预算。

**结果:** (i) 提出的过程奖励模型校准方法达到了较小的校准误差，优于基线方法。(ii) 校准对于实现有效的自适应缩放至关重要。(iii) 所提出的IAS策略在维持最终答案准确性的前提下减少了推理成本，正如预期那样，对于更加确信的问题使用了较少的计算资源。

**结论:** 通过校准过程奖励模型并利用其实例自适应缩放能力，可以在不牺牲性能的情况下降低大型语言模型推理时的成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Know+What+You+Don%27t+Know%3A+Uncertainty+Calibration+of+Process+Reward+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09338，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09338&send_immediately=true&force_search=false)

**原文摘要:** Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [109] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra, Philippe Roudot*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种结合自注意力机制和贝叶斯滤波的混合跟踪框架，以解决在噪声和杂乱场景中多粒子跟踪的问题。


<details>
  <summary>更多</summary>
  
**动机:** 在噪声和杂乱场景中进行多粒子跟踪存在挑战性，因为轨迹假设的数量会随着粒子数量和帧数的增加而呈超指数增长。虽然Transformer架构显著提高了对高组合负载的鲁棒性，但在呈现较少轨迹假设的情况下，其表现仍不及传统的贝叶斯滤波方法。

**方法:** 作者引入了一种混合跟踪框架，该框架结合了自注意力学习粒子行为基本表示的能力与贝叶斯滤波的可靠性和可解释性。通过将轨迹到检测的关联问题转化为标签预测问题，并使用Transformer编码器来推断跨帧检测之间的软关联，从而修剪假设集，使得在贝叶斯滤波框架下能够高效地进行多粒子跟踪。

**结果:** 所提方法展示了改进的跟踪准确性和对虚假检测的鲁棒性，为高度杂乱环境下的多粒子跟踪提供了方案。

**结论:** 结合了自注意力机制与贝叶斯滤波的混合跟踪框架提供了一种有效的解决方案，能够在保持对复杂场景鲁棒性的同时提高多粒子系统的跟踪精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention-Bayesian+Hybrid+Approach+to+Modular+Multiple+Particle+Tracking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09441&send_immediately=true&force_search=false)

**原文摘要:** Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [110] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan, Jinchi Lv, Ao Sun, Yurou Wang*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种基于大型语言模型的方法LLM-CPI，通过结合高频在线文本数据来改进消费者价格指数（CPI）的预测。


<details>
  <summary>更多</summary>
  
**动机:** 现有的CPI预测方法主要依赖于低频、基于调查的数据。随着大型语言模型的进步，利用高频在线文本数据进行更准确的CPI预测成为可能。

**方法:** 研究者从中国流行的社交网络收集大量高频在线文本，并使用如ChatGPT和训练过的BERT模型为与通货膨胀相关的帖子构建连续的通胀标签。接着提取在线文本嵌入，并开发了一个联合时间序列框架，将月度CPI数据与LLM生成的日度CPI代理变量相结合。

**结果:** 通过模拟和实际数据示例证明了LLM-CPI在有限样本表现及实际应用中的优势，并建立了该方法的渐近性质以及提供了两种形式的预测区间。

**结论:** 提出的LLM-CPI方法能够有效提升CPI预测的准确性，为经济领域的CPI预测开辟了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Powered+CPI+Prediction+Inference+with+Online+Text+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09516，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09516&send_immediately=true&force_search=false)

**原文摘要:** Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [111] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce, Roi Naveiro, David Ríos Insua*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种设计针对贝叶斯预测模型的最佳逃避攻击的一般方法论，并研究了两种对抗性目标：扰动特定点预测和改变整个后验预测分布。


<details>
  <summary>更多</summary>
  
**动机:** 尽管对抗性机器学习领域的大多数研究都集中在研究经典设置下预测模型对逃避或中毒攻击的弱点，但贝叶斯预测模型对攻击的脆弱性仍然未被充分探索。

**方法:** 对于扰动特定点预测和改变整个后验预测分布这两种情况，作者提出了新颖的基于梯度的攻击方法，并在各种计算环境中研究了这些攻击的实现与特性。

**结果:** 论文中提出的攻击方法为评估贝叶斯预测模型的安全性和鲁棒性提供了新的视角。

**结论:** 本文的方法为理解贝叶斯预测模型在面对精心设计的对抗性攻击时的行为提供了重要的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evasion+Attacks+Against+Bayesian+Predictive+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09640，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09640&send_immediately=true&force_search=false)

**原文摘要:** There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [112] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso, Simone Rossi, Giulio Franzese, Markus Heinonen, Maurizio Filippone*

**主要类别:** stat.ML

**AI概要:** 本文探讨了深度学习中的预测不确定性是否遵循与模型性能相似的缩放律，并通过实验证明了在视觉和语言任务中，随着数据集大小和模型大小的变化，存在关于预测不确定性的缩放律。


<details>
  <summary>更多</summary>
  
**动机:** 受最近发现的深度学习模型性能遵循可预测的趋势的启发，研究者们探索了过参数化情况下出现的现象，并且想要了解类似的缩放律是否也适用于深度学习中的预测不确定性。

**方法:** 研究者们通过实验的方法，在视觉和语言任务上观察了对于不同数据集大小和模型大小，预测不确定性（包括分布内和分布外）的度量是否存在缩放律。他们使用了流行的近似贝叶斯推理和集成方法来估计预测不确定性。

**结果:** 研究表明，确实存在着与数据集大小和模型大小相关的预测不确定性的缩放律。这些规律为理解贝叶斯方法在大数据条件下的作用提供了强有力的证据，表明即使有大量的数据，认知不确定性也不能被忽略。

**结论:** 这项工作不仅展示了预测不确定性缩放律的优雅之处，还证明了将不确定性推断到更大规模的数据或模型上的实际效用，并且有力地反驳了对贝叶斯方法在大数据时代必要性的怀疑论点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Laws+for+Uncertainty+in+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09648，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09648&send_immediately=true&force_search=false)

**原文摘要:** Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [113] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan, Elen Vardanyan, Arnak Dalalyan*

**主要类别:** stat.ML

**AI概要:** 本文研究了去噪扩散概率模型(DDPMs)对于噪声评分估计的鲁棒性，提供了实证证据，并且在Wasserstein-2距离上建立了有限样本保证，其收敛速度优于先前的结果，并且达到了已知的高斯情况下的最优速率。


<details>
  <summary>更多</summary>
  
**动机:** 生成建模的目标是根据有限的例子集合来产生符合未知目标分布的新随机例子。作为主要方法之一，去噪扩散概率模型(DDPMs)通过一个由估计得分函数驱动的扩散过程映射布朗运动来构造这些例子。研究旨在探索DDPMs对噪声评分估计的鲁棒性，并提供理论上的保证。

**方法:** 研究首先提供了实证证据，表明DDPMs对评分评估中的常数方差噪声具有鲁棒性。然后，在Wasserstein-2距离上建立了有限样本的保证，这体现了两个关键特征：（i）它们表征并量化了DDPMs对噪声评分估计的鲁棒性；（ii）它们实现了比之前结果更快的收敛率。

**结果:** 研究表明，DDPMs确实对噪声评分估计表现出鲁棒性，并且在Wasserstein-2距离上获得了比以往结果更快的收敛率。此外，观察到得到的收敛率与高斯情况下的收敛率相匹配，暗示了它们的最优性。

**结论:** 结论指出，DDPMs不仅对噪声评分估计具有鲁棒性，而且在Wasserstein-2距离上达到了最优的收敛率，这为生成建模领域提供了重要的理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+the+Quality+of+Denoising+Diffusion+Models+in+Wasserstein+Distance%3A+Noisy+Score+and+Optimal+Bounds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09681，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09681&send_immediately=true&force_search=false)

**原文摘要:** Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [114] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon, Julien Straubhaar, Philippe Renard*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的方法，将岩溶网络表示为图，并应用图生成模型（深度学习技术）来捕捉岩溶环境的复杂性。通过使用图递归神经网络（GraphRNN）和图去噪扩散概率模型（G-DDPM）相结合的方法，可以模拟出逼真的岩溶网络，有助于研究流体流动等物理过程的行为。


<details>
  <summary>更多</summary>
  
**动机:** 由于在各种地质和水文地质背景下长时间发生的物理化学过程的复杂性，离散岩溶网络的模拟是一个重要的挑战。这些复杂的相互作用导致了多种岩溶网络模式，每一种都与特定的水文地质条件紧密相关。

**方法:** 该方法将岩溶网络视为图，并采用图生成模型（如深度学习技术）来理解岩溶环境的复杂性。节点保留空间信息和属性，边则表示节点之间的连接。生成过程分为两个主要步骤：首先，利用图递归神经网络（GraphRNN）学习岩溶网络的拓扑分布；其次，对图上的去噪扩散概率模型（G-DDPM）进行训练以学习节点特征。

**结果:** 所提出的方法能够根据实际世界中的岩溶网络数据生成子图，并且通过几何和拓扑度量与数据库中的实际子图进行比较。结果表明，该方法可以跨不同类型的地层随机模拟离散岩溶网络。

**结论:** 本研究开发了一种基于图生成模型的新型方法来模拟岩溶网络，这种方法不仅能够生成逼真的岩溶网络结构，还能帮助科学家们更好地理解并预测岩溶环境下的物理过程行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Deep+Generative+Model+for+the+Simulation+of+Discrete+Karst+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09832，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09832&send_immediately=true&force_search=false)

**原文摘要:** The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>
