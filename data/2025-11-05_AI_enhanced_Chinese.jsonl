{"id": "2511.01885", "pdf": "https://arxiv.org/pdf/2511.01885", "abs": "https://arxiv.org/abs/2511.01885", "authors": ["Robyn Wyrick"], "title": "Mirror-Neuron Patterns in AI Alignment", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "comment": "51 pages, Masters thesis. 10 tables, 7 figures, project data & code\n  here: https://github.com/robynwyrick/mirror-neuron-frog-and-toad", "summary": "As artificial intelligence (AI) advances toward superhuman capabilities,\naligning these systems with human values becomes increasingly critical. Current\nalignment strategies rely largely on externally specified constraints that may\nprove insufficient against future super-intelligent AI capable of circumventing\ntop-down controls.\n  This research investigates whether artificial neural networks (ANNs) can\ndevelop patterns analogous to biological mirror neurons cells that activate\nboth when performing and observing actions, and how such patterns might\ncontribute to intrinsic alignment in AI. Mirror neurons play a crucial role in\nempathy, imitation, and social cognition in humans. The study therefore asks:\n(1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these\npatterns contribute to ethical and cooperative decision-making in AI systems?\n  Using a novel Frog and Toad game framework designed to promote cooperative\nbehaviors, we identify conditions under which mirror-neuron patterns emerge,\nevaluate their influence on action circuits, introduce the Checkpoint Mirror\nNeuron Index (CMNI) to quantify activation strength and consistency, and\npropose a theoretical framework for further study.\n  Our findings indicate that appropriately scaled model capacities and\nself/other coupling foster shared neural representations in ANNs similar to\nbiological mirror neurons. These empathy-like circuits support cooperative\nbehavior and suggest that intrinsic motivations modeled through mirror-neuron\ndynamics could complement existing alignment techniques by embedding\nempathy-like mechanisms directly within AI architectures.", "AI": {"tldr": "本研究探索人工神经网络是否能发展出类似生物镜像神经元的结构，以及这种结构如何促进AI系统的内在对齐。通过Frog and Toad游戏框架发现，适当规模的模型容量和自我/他者耦合能促进共享神经表征，这些类似共情的电路支持合作行为。", "motivation": "随着AI向超人类能力发展，现有基于外部约束的对齐策略可能不足以应对未来超级智能AI。生物镜像神经元在人类共情、模仿和社会认知中起关键作用，研究其人工类似物可能为AI内在对齐提供新途径。", "method": "使用新颖的Frog and Toad游戏框架促进合作行为，识别镜像神经元模式出现的条件，评估其对动作电路的影响，引入检查点镜像神经元指数(CMNI)量化激活强度和一致性，并提出理论框架。", "result": "研究发现适当规模的模型容量和自我/他者耦合能在ANN中培养出类似生物镜像神经元的共享神经表征。这些类似共情的电路支持合作行为，表明通过镜像神经元动力学建模的内在动机可以补充现有对齐技术。", "conclusion": "基于镜像神经元的内在动机机制可以直接嵌入AI架构中，为AI对齐提供补充性内在方法，可能比单纯的外部约束更有效地确保未来超级智能AI与人类价值观保持一致。"}}
{"id": "2511.02071", "pdf": "https://arxiv.org/pdf/2511.02071", "abs": "https://arxiv.org/abs/2511.02071", "authors": ["Xinyi Lin", "Yuyang Zhang", "Yuanhang Gan", "Juntao Chen", "Hao Shen", "Yichun He", "Lijun Li", "Ze Yuan", "Shuang Wang", "Chaohao Wang", "Rui Zhang", "Na Li", "Jia Liu"], "title": "Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing", "categories": ["cs.AI"], "comment": null, "summary": "Scientific experiment and manufacture rely on complex, multi-step procedures\nthat demand continuous human expertise for precise execution and\ndecision-making. Despite advances in machine learning and automation,\nconventional models remain confined to virtual domains, while real-world\nexperiment and manufacture still rely on human supervision and expertise. This\ngap between machine intelligence and physical execution limits reproducibility,\nscalability, and accessibility across scientific and manufacture workflows.\nHere, we introduce human-AI co-embodied intelligence, a new form of physical AI\nthat unites human users, agentic AI, and wearable hardware into an integrated\nsystem for real-world experiment and intelligent manufacture. In this paradigm,\nhumans provide precise execution and control, while agentic AI contributes\nmemory, contextual reasoning, adaptive planning, and real-time feedback. The\nwearable interface continuously captures the experimental and manufacture\nprocesses, facilitates seamless communication between humans and AI for\ncorrective guidance and interpretable collaboration. As a demonstration, we\npresent Agentic-Physical Experimentation (APEX) system, coupling agentic\nreasoning with physical execution through mixed-reality. APEX observes and\ninterprets human actions, aligns them with standard operating procedures,\nprovides 3D visual guidance, and analyzes every step. Implemented in a\ncleanroom for flexible electronics fabrication, APEX system achieves\ncontext-aware reasoning with accuracy exceeding general multimodal large\nlanguage models, corrects errors in real time, and transfers expertise to\nbeginners. These results establish a new class of agentic-physical-human\nintelligence that extends agentic reasoning beyond computation into the\nphysical domain, transforming scientific research and manufacturing into\nautonomous, traceable, interpretable, and scalable processes.", "AI": {"tldr": "论文提出了一种人-AI协同体智能系统，将人类用户、智能AI代理和可穿戴硬件集成，用于现实世界的实验和智能制造，通过混合现实实现物理执行与智能推理的结合。", "motivation": "当前机器学习模型主要局限于虚拟领域，而现实世界的实验和制造仍依赖人类监督和专业知识，这限制了科学和制造工作流程的可重复性、可扩展性和可访问性。", "method": "开发了Agentic-Physical Experimentation (APEX)系统，通过可穿戴接口持续捕捉实验和制造过程，结合混合现实技术实现人类动作的观察与解释、标准操作程序对齐、3D视觉指导和逐步分析。", "result": "在柔性电子制造洁净室中实施APEX系统，实现了超过通用多模态大语言模型的上下文感知推理准确性，能够实时纠正错误，并将专业知识传授给初学者。", "conclusion": "这项工作建立了一类新的智能体-物理-人类智能，将智能推理从计算领域扩展到物理领域，使科学研究和制造转变为自主、可追溯、可解释和可扩展的过程。"}}
{"id": "2511.02094", "pdf": "https://arxiv.org/pdf/2511.02094", "abs": "https://arxiv.org/abs/2511.02094", "authors": ["Michel Ma", "Takuma Seno", "Kaushik Subramanian", "Peter R. Wurman", "Peter Stone", "Craig Sherstan"], "title": "Automated Reward Design for Gran Turismo", "categories": ["cs.AI"], "comment": null, "summary": "When designing reinforcement learning (RL) agents, a designer communicates\nthe desired agent behavior through the definition of reward functions -\nnumerical feedback given to the agent as reward or punishment for its actions.\nHowever, mapping desired behaviors to reward functions can be a difficult\nprocess, especially in complex environments such as autonomous racing. In this\npaper, we demonstrate how current foundation models can effectively search over\na space of reward functions to produce desirable RL agents for the Gran Turismo\n7 racing game, given only text-based instructions. Through a combination of\nLLM-based reward generation, VLM preference-based evaluation, and human\nfeedback we demonstrate how our system can be used to produce racing agents\ncompetitive with GT Sophy, a champion-level RL racing agent, as well as\ngenerate novel behaviors, paving the way for practical automated reward design\nin real world applications.", "AI": {"tldr": "本文展示了如何使用基础模型通过文本指令自动搜索奖励函数空间，为Gran Turismo 7赛车游戏生成具有竞争力的强化学习智能体，实现了与冠军级智能体GT Sophy相当的性能。", "motivation": "在设计强化学习智能体时，通过奖励函数定义期望行为是一个困难的过程，特别是在复杂环境如自动驾驶赛车中，需要更有效的方法来映射文本指令到奖励函数。", "method": "结合基于LLM的奖励生成、基于VLM偏好的评估和人类反馈，构建了一个自动化奖励设计系统，通过文本指令搜索奖励函数空间。", "result": "系统能够生成与冠军级RL赛车智能体GT Sophy竞争的赛车智能体，并能产生新颖行为，证明了方法的有效性。", "conclusion": "该方法为实际应用中的自动化奖励设计铺平了道路，展示了基础模型在复杂环境中自动设计奖励函数的潜力。"}}
{"id": "2511.02109", "pdf": "https://arxiv.org/pdf/2511.02109", "abs": "https://arxiv.org/abs/2511.02109", "authors": ["Joshua Ashkinaze", "Hua Shen", "Sai Avula", "Eric Gilbert", "Ceren Budak"], "title": "Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "NeurIPS 2025 (Spotlight)", "summary": "We introduce the Deep Value Benchmark (DVB), an evaluation framework that\ndirectly tests whether large language models (LLMs) learn fundamental human\nvalues or merely surface-level preferences. This distinction is critical for AI\nalignment: Systems that capture deeper values are likely to generalize human\nintentions robustly, while those that capture only superficial patterns in\npreference data risk producing misaligned behavior. The DVB uses a novel\nexperimental design with controlled confounding between deep values (e.g.,\nmoral principles) and shallow features (e.g., superficial attributes). In the\ntraining phase, we expose LLMs to human preference data with deliberately\ncorrelated deep and shallow features -- for instance, where a user consistently\nprefers (non-maleficence, formal language) options over (justice, informal\nlanguage) alternatives. The testing phase then breaks these correlations,\npresenting choices between (justice, formal language) and (non-maleficence,\ninformal language) options. This design allows us to precisely measure a\nmodel's Deep Value Generalization Rate (DVGR) -- the probability of\ngeneralizing based on the underlying value rather than the shallow feature.\nAcross 9 different models, the average DVGR is just 0.30. All models generalize\ndeep values less than chance. Larger models have a (slightly) lower DVGR than\nsmaller models. We are releasing our dataset, which was subject to three\nseparate human validation experiments. DVB provides an interpretable measure of\na core feature of alignment.", "AI": {"tldr": "Deep Value Benchmark (DVB) 是一个评估框架，用于测试大语言模型是否学习到深层人类价值观还是仅表面偏好，结果显示大多数模型在深层价值泛化方面表现不佳，平均仅30%的概率基于价值观而非表面特征做出选择。", "motivation": "区分AI系统是学习到深层人类价值观还是仅表面偏好模式对AI对齐至关重要，因为仅捕捉表面模式可能导致行为不匹配。", "method": "采用新颖的实验设计，在训练阶段让LLMs接触深层价值观和表面特征故意相关的偏好数据，在测试阶段打破这些相关性，测量模型基于深层价值观而非表面特征进行泛化的概率(DVGR)。", "result": "测试9个不同模型，平均DVGR仅为0.30，所有模型在深层价值泛化上都低于随机水平，且更大模型的DVGR略低于小模型。", "conclusion": "DVB提供了一个可解释的AI对齐核心特征衡量标准，表明当前LLMs在深层价值观学习方面存在显著不足，需要改进对齐方法。"}}
{"id": "2511.01891", "pdf": "https://arxiv.org/pdf/2511.01891", "abs": "https://arxiv.org/abs/2511.01891", "authors": ["Rongxin Chen", "Yunfan Li", "Yige Yuan", "Bingbing Xu", "Huawei Shen"], "title": "Multi-Personality Generation of LLMs at Decoding-time", "categories": ["cs.CL", "cs.AI"], "comment": "WSDM2026", "summary": "Multi-personality generation for LLMs, enabling simultaneous embodiment of\nmultiple personalization attributes, is a fundamental challenge. Existing\nretraining-based approaches are costly and poorly scalable, while decoding-time\nmethods often rely on external models or heuristics, limiting flexibility and\nrobustness. In this paper, we propose a novel Multi-Personality Generation\n(MPG) framework under the decoding-time combination paradigm. It flexibly\ncontrols multi-personality without relying on scarce multi-dimensional models\nor extra training, leveraging implicit density ratios in single-dimensional\nmodels as a \"free lunch\" to reformulate the task as sampling from a target\nstrategy aggregating these ratios. To implement MPG efficiently, we design\nSpeculative Chunk-level based Rejection sampling (SCR), which generates\nresponses in chunks and parallelly validates them via estimated thresholds\nwithin a sliding window. This significantly reduces computational overhead\nwhile maintaining high-quality generation. Experiments on MBTI personality and\nRole-Playing demonstrate the effectiveness of MPG, showing improvements up to\n16%-18%. Code and data are available at https://github.com/Libra117/MPG .", "AI": {"tldr": "提出MPG框架，通过解码时组合范式实现多人格生成，无需重新训练或多维模型，利用单维模型的隐式密度比进行策略聚合采样。", "motivation": "现有基于重新训练的方法成本高且扩展性差，解码时方法依赖外部模型或启发式规则，灵活性和鲁棒性受限。", "method": "提出MPG框架，将任务重新表述为从目标策略采样，设计基于推测性块级拒绝采样(SCR)的高效实现方法，分块生成响应并通过滑动窗口内的估计阈值并行验证。", "result": "在MBTI人格和角色扮演任务上的实验显示，MPG有效性显著，改进幅度达16%-18%。", "conclusion": "MPG框架提供了一种高效、灵活的多人格生成解决方案，无需额外训练成本，在保持高质量生成的同时显著降低计算开销。"}}
{"id": "2511.02119", "pdf": "https://arxiv.org/pdf/2511.02119", "abs": "https://arxiv.org/abs/2511.02119", "authors": ["Ziheng Geng", "Jiachen Liu", "Ran Cao", "Lu Cheng", "Dan M. Frangopol", "Minghui Cheng"], "title": "InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Flood insurance is an effective strategy for individuals to mitigate\ndisaster-related losses. However, participation rates among at-risk populations\nin the United States remain strikingly low. This gap underscores the need to\nunderstand and model the behavioral mechanisms underlying insurance decisions.\nLarge language models (LLMs) have recently exhibited human-like intelligence\nacross wide-ranging tasks, offering promising tools for simulating human\ndecision-making. This study constructs a benchmark dataset to capture insurance\npurchase probabilities across factors. Using this dataset, the capacity of LLMs\nis evaluated: while LLMs exhibit a qualitative understanding of factors, they\nfall short in estimating quantitative probabilities. To address this\nlimitation, InsurAgent, an LLM-empowered agent comprising five modules\nincluding perception, retrieval, reasoning, action, and memory, is proposed.\nThe retrieval module leverages retrieval-augmented generation (RAG) to ground\ndecisions in empirical survey data, achieving accurate estimation of marginal\nand bivariate probabilities. The reasoning module leverages LLM common sense to\nextrapolate beyond survey data, capturing contextual information that is\nintractable for traditional models. The memory module supports the simulation\nof temporal decision evolutions, illustrated through a roller coaster life\ntrajectory. Overall, InsurAgent provides a valuable tool for behavioral\nmodeling and policy analysis.", "AI": {"tldr": "该研究开发了InsurAgent系统，利用大语言模型模拟洪水保险决策行为，通过RAG技术结合调查数据提高概率估计准确性，并支持时序决策模拟。", "motivation": "美国高风险人群的洪水保险参保率极低，需要理解保险决策的行为机制，而大语言模型在模拟人类决策方面展现出潜力但存在定量概率估计不足的问题。", "method": "构建基准数据集评估LLM能力，提出InsurAgent系统（包含感知、检索、推理、行动和记忆五个模块），其中检索模块使用RAG技术基于调查数据，推理模块利用LLM常识进行外推，记忆模块支持时序决策模拟。", "result": "LLM对因素有定性理解但定量概率估计不足；InsurAgent通过RAG实现了边际和双变量概率的准确估计，并能捕捉传统模型难以处理的上下文信息。", "conclusion": "InsurAgent为行为建模和政策分析提供了有价值的工具，能够有效模拟保险决策行为并支持政策评估。"}}
{"id": "2511.02135", "pdf": "https://arxiv.org/pdf/2511.02135", "abs": "https://arxiv.org/abs/2511.02135", "authors": ["Joseph Suh", "Suhong Moon", "Serina Chang"], "title": "Rethinking LLM Human Simulation: When a Graph is What You Need", "categories": ["cs.CL"], "comment": "Code: https://github.com/schang-lab/gems", "summary": "Large language models (LLMs) are increasingly used to simulate humans, with\napplications ranging from survey prediction to decision-making. However, are\nLLMs strictly necessary, or can smaller, domain-grounded models suffice? We\nidentify a large class of simulation problems in which individuals make choices\namong discrete options, where a graph neural network (GNN) can match or surpass\nstrong LLM baselines despite being three orders of magnitude smaller. We\nintroduce Graph-basEd Models for human Simulation (GEMS), which casts discrete\nchoice simulation tasks as a link prediction problem on graphs, leveraging\nrelational knowledge while incorporating language representations only when\nneeded. Evaluations across three key settings on three simulation datasets show\nthat GEMS achieves comparable or better accuracy than LLMs, with far greater\nefficiency, interpretability, and transparency, highlighting the promise of\ngraph-based modeling as a lightweight alternative to LLMs for human simulation.\nOur code is available at https://github.com/schang-lab/gems.", "AI": {"tldr": "该论文提出了一种基于图神经网络的轻量级人类模拟方法GEMS，在离散选择模拟任务中能够以更小的模型规模达到或超越大型语言模型的性能", "motivation": "探索在人类模拟任务中，是否必须使用大型语言模型，还是可以使用更小、更专业的领域模型来获得相同或更好的效果", "method": "提出Graph-basEd Models for human Simulation (GEMS)，将离散选择模拟任务转化为图上的链接预测问题，利用关系知识，仅在需要时整合语言表示", "result": "在三个模拟数据集上的评估显示，GEMS达到了与LLM相当或更好的准确性，同时具有更高的效率、可解释性和透明度", "conclusion": "基于图的建模方法可以作为LLM在人类模拟任务中的轻量级替代方案，具有显著的计算效率和解释性优势"}}
{"id": "2511.02130", "pdf": "https://arxiv.org/pdf/2511.02130", "abs": "https://arxiv.org/abs/2511.02130", "authors": ["Renos Zabounidis", "Aditya Golatkar", "Michael Kleinman", "Alessandro Achille", "Wei Xia", "Stefano Soatto"], "title": "Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at Efficient Reasoning Workshop at NeurIPS 2025", "summary": "We propose Re-FORC, an adaptive reward prediction method that, given a\ncontext, enables prediction of the expected future rewards as a function of the\nnumber of future thinking tokens. Re-FORC trains a lightweight adapter on\nreasoning models, demonstrating improved prediction with longer reasoning and\nlarger models. Re-FORC enables: 1) early stopping of unpromising reasoning\nchains, reducing compute by 26% while maintaining accuracy, 2) optimized model\nand thinking length selection that achieves 4% higher accuracy at equal compute\nand 55% less compute at equal accuracy compared to the largest model, 3)\nadaptive test-time scaling, which increases accuracy by 11% in high compute\nregime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with\nlength control via cost-per-token thresholds while estimating computation time\nupfront.", "AI": {"tldr": "Re-FORC是一种自适应奖励预测方法，通过训练轻量级适配器来预测未来思考令牌数量与预期奖励的关系，实现推理链的早期停止、模型优化和自适应推理长度控制", "motivation": "为了解决大型推理模型计算成本高、推理效率低的问题，需要一种能够动态控制推理长度并提前预测计算效果的方法", "method": "在推理模型上训练轻量级适配器，预测不同思考令牌数量下的预期未来奖励，实现基于奖励预测的自适应推理控制", "result": "计算成本降低26%同时保持准确性；在相同计算量下准确率提高4%；在相同准确率下计算量减少55%；在高计算模式下准确率提高11%，低计算模式下提高7%", "conclusion": "Re-FORC提供了一种有效的自适应推理控制方法，能够在保持模型性能的同时显著降低计算成本，实现动态推理长度控制"}}
{"id": "2511.02213", "pdf": "https://arxiv.org/pdf/2511.02213", "abs": "https://arxiv.org/abs/2511.02213", "authors": ["Kangyu Qiao", "Shaolei Zhang", "Yang Feng"], "title": "IG-Pruning: Input-Guided Block Pruning for Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025. Code is available at\n  https://github.com/ictnlp/IG-Pruning", "summary": "With the growing computational demands of large language models (LLMs),\nefficient inference has become increasingly critical for practical deployment.\nDepth pruning has emerged as a promising approach for reducing the\ncomputational costs of large language models by removing transformer layers.\nHowever, existing methods typically rely on fixed block masks, which can lead\nto suboptimal performance across different tasks and inputs. In this paper, we\npropose IG-Pruning, a novel input-aware block-wise pruning method that\ndynamically selects layer masks at inference time. Our approach consists of two\nstages: (1) Discovering diverse mask candidates through semantic clustering and\nL0 optimization, and (2) Implementing efficient dynamic pruning without the\nneed for extensive training. Experimental results demonstrate that our method\nconsistently outperforms state-of-the-art static depth pruning methods, making\nit particularly suitable for resource-constrained deployment scenarios.", "AI": {"tldr": "IG-Pruning是一种新颖的输入感知块级剪枝方法，通过动态选择层掩码来减少大语言模型的计算成本，无需大量训练即可实现高效推理。", "motivation": "随着大语言模型计算需求增长，需要高效的推理方法。现有深度剪枝方法使用固定块掩码，在不同任务和输入上表现不佳。", "method": "两阶段方法：1)通过语义聚类和L0优化发现多样掩码候选；2)在推理时动态选择层掩码，无需大量训练。", "result": "实验结果表明该方法始终优于最先进的静态深度剪枝方法。", "conclusion": "该方法特别适合资源受限的部署场景，为LLMs的高效推理提供了有效解决方案。"}}
{"id": "2511.02194", "pdf": "https://arxiv.org/pdf/2511.02194", "abs": "https://arxiv.org/abs/2511.02194", "authors": ["Yibo Zhao", "Yang Zhao", "Hongru Du", "Hao Frank Yang"], "title": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Decision-making models for individuals, particularly in high-stakes scenarios\nlike vaccine uptake, often diverge from population optimal predictions. This\ngap arises from the uniqueness of the individual decision-making process,\nshaped by numerical attributes (e.g., cost, time) and linguistic influences\n(e.g., personal preferences and constraints). Developing upon Utility Theory\nand leveraging the textual-reasoning capabilities of Large Language Models\n(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric\nReasoning framework (ATHENA) to address the optimal information integration.\nATHENA uniquely integrates two stages: First, it discovers robust, group-level\nsymbolic utility functions via LLM-augmented symbolic discovery; Second, it\nimplements individual-level semantic adaptation, creating personalized semantic\ntemplates guided by the optimal utility to model personalized choices.\nValidated on real-world travel mode and vaccine choice tasks, ATHENA\nconsistently outperforms utility-based, machine learning, and other LLM-based\nmodels, lifting F1 score by at least 6.5% over the strongest cutting-edge\nmodels. Further, ablation studies confirm that both stages of ATHENA are\ncritical and complementary, as removing either clearly degrades overall\npredictive performance. By organically integrating symbolic utility modeling\nand semantic adaptation, ATHENA provides a new scheme for modeling\nhuman-centric decisions. The project page can be found at\nhttps://yibozh.github.io/Athena.", "AI": {"tldr": "ATHENA框架结合效用理论和LLM的文本推理能力，通过两阶段方法（群体级符号效用函数发现和个体级语义适应）来建模个性化决策，在旅行方式和疫苗选择任务中显著优于现有模型。", "motivation": "个体决策模型与群体最优预测存在差异，这种差异源于个体决策过程的独特性，包括数值属性（成本、时间）和语言影响（个人偏好和约束）。", "method": "提出ATHENA框架：1）通过LLM增强的符号发现找到群体级符号效用函数；2）基于最优效用进行个体级语义适应，创建个性化语义模板来建模个性化选择。", "result": "在真实世界的旅行方式和疫苗选择任务中，ATHENA始终优于基于效用、机器学习和其他基于LLM的模型，F1分数比最强前沿模型至少提高6.5%。消融研究证实两个阶段都至关重要且互补。", "conclusion": "通过有机整合符号效用建模和语义适应，ATHENA为建模以人为本的决策提供了新方案。"}}
{"id": "2511.02246", "pdf": "https://arxiv.org/pdf/2511.02246", "abs": "https://arxiv.org/abs/2511.02246", "authors": ["Jonathan Liu", "Haoling Qiu", "Jonathan Lasko", "Damianos Karakos", "Mahsa Yarmohammadi", "Mark Dredze"], "title": "Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Recent research has shown that hallucinations, omissions, and biases are\nprevalent in everyday use-cases of LLMs. However, chatbots used in medical\ncontexts must provide consistent advice in situations where non-medical factors\nare involved, such as when demographic information is present. In order to\nunderstand the conditions under which medical chatbots fail to perform as\nexpected, we develop an infrastructure that 1) automatically generates queries\nto probe LLMs and 2) evaluates answers to these queries using multiple\nLLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples\nthe space of patient demographics, histories, disorders, and writing styles to\ncreate realistic questions that we subsequently use to prompt LLMs. In 2), our\nevaluation pipeline provides hallucination and omission detection using\nLLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge\ntreatment category detectors. As a baseline study, we perform two case studies\non inter-LLM agreement and the impact of varying the answering and evaluation\nLLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's\nKappa $\\kappa=0.118$), and only specific (answering, evaluation) LLM pairs\nyield statistically significant differences across writing styles, genders, and\nraces. We recommend that studies using LLM evaluation use multiple LLMs as\nevaluators in order to avoid arriving at statistically significant but\nnon-generalizable results, particularly in the absence of ground-truth data. We\nalso suggest publishing inter-LLM agreement metrics for transparency. Our code\nand dataset are available here:\nhttps://github.com/BBN-E/medic-neurips-2025-demo.", "AI": {"tldr": "该研究开发了一个评估医疗聊天机器人一致性的基础设施，发现LLM评估者间一致性很低，建议使用多个LLM评估器以避免统计显著但不可泛化的结果。", "motivation": "医疗聊天机器人在涉及非医疗因素（如人口统计信息）时必须提供一致的建议，但现有LLM存在幻觉、遗漏和偏见问题，需要了解其失效条件。", "method": "开发自动生成查询的基础设施：1）通过采样患者人口统计、病史、疾病和写作风格创建真实问题；2）使用LLM作为评判者进行幻觉和遗漏检测，以及治疗类别检测。进行了两个案例研究：LLM间一致性和不同回答/评估LLM的影响。", "result": "LLM标注者表现出低一致性分数（平均Cohen's Kappa κ=0.118），只有特定的（回答、评估）LLM对在写作风格、性别和种族方面产生统计显著差异。", "conclusion": "建议使用多个LLM作为评估器以避免非泛化结果，特别是在缺乏真实数据的情况下，并建议发布LLM间一致性指标以提高透明度。"}}
{"id": "2511.02200", "pdf": "https://arxiv.org/pdf/2511.02200", "abs": "https://arxiv.org/abs/2511.02200", "authors": ["Jingbo Wang", "Sendong Zhao", "Haochun Wang", "Yuzheng Fan", "Lizhe Zhang", "Yan Liu", "Ting Liu"], "title": "Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of multi-agent systems powered by large language models (LLMs)\nhas unlocked new frontiers in complex task-solving, enabling diverse agents to\nintegrate unique expertise, collaborate flexibly, and address challenges\nunattainable for individual models. However, the full potential of such systems\nis hindered by rigid agent scheduling and inefficient coordination strategies\nthat fail to adapt to evolving task requirements. In this paper, we propose\nSTRMAC, a state-aware routing framework designed for efficient collaboration in\nmulti-agent systems. Our method separately encodes interaction history and\nagent knowledge to power the router, which adaptively selects the most suitable\nsingle agent at each step for efficient and effective collaboration.\nFurthermore, we introduce a self-evolving data generation approach that\naccelerates the collection of high-quality execution paths for efficient system\ntraining. Experiments on challenging collaborative reasoning benchmarks\ndemonstrate that our method achieves state-of-the-art performance, achieving up\nto 23.8% improvement over baselines and reducing data collection overhead by up\nto 90.1% compared to exhaustive search.", "AI": {"tldr": "STRMAC是一个状态感知路由框架，通过分别编码交互历史和代理知识来动态选择最适合的单一代理，实现多代理系统的高效协作，并在基准测试中取得SOTA性能。", "motivation": "当前多代理系统受到僵化的代理调度和低效协调策略的限制，无法适应不断变化的任务需求，阻碍了系统潜力的充分发挥。", "method": "提出STRMAC框架，分别编码交互历史和代理知识来驱动路由器，自适应选择每个步骤中最合适的单一代理；引入自进化数据生成方法加速高质量执行路径的收集。", "result": "在协作推理基准测试中达到最先进性能，比基线提升高达23.8%，与穷举搜索相比减少数据收集开销高达90.1%。", "conclusion": "STRMAC框架通过状态感知路由和自进化数据生成，有效解决了多代理系统的协作效率问题，为复杂任务解决提供了更高效的解决方案。"}}
{"id": "2511.02347", "pdf": "https://arxiv.org/pdf/2511.02347", "abs": "https://arxiv.org/abs/2511.02347", "authors": ["Liuhao Lin", "Ke Li", "Zihan Xu", "Yuchen Shi", "Yulei Qin", "Yan Zhang", "Xing Sun", "Rongrong Ji"], "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw", "categories": ["cs.CL"], "comment": "Accepted by NeurIPS 2025", "summary": "Current evaluation paradigms for large language models (LLMs) represent a\ncritical blind spot in AI research--relying on opaque numerical metrics that\nconceal fundamental limitations in spatial reasoning while providing no\nintuitive understanding of model capabilities. This deficiency creates a\ndangerous disconnect between reported performance and practical abilities,\nparticularly for applications requiring physical world understanding. We\nintroduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation\nfrom abstract scores to directly observable visual outputs by requiring models\nto generate drawings through dot matrices or executable code. This approach\nmakes spatial reasoning limitations immediately apparent even to non-experts,\nbridging the fundamental gap between statistical performance and intuitive\nassessment. LTD-Bench implements a comprehensive methodology with complementary\ngeneration tasks (testing spatial imagination) and recognition tasks (assessing\nspatial perception) across three progressively challenging difficulty levels,\nmethodically evaluating both directions of the critical language-spatial\nmapping. Our extensive experiments with state-of-the-art models expose an\nalarming capability gap: even LLMs achieving impressive results on traditional\nbenchmarks demonstrate profound deficiencies in establishing bidirectional\nmappings between language and spatial concept--a fundamental limitation that\nundermines their potential as genuine world models. Furthermore, LTD-Bench's\nvisual outputs enable powerful diagnostic analysis, offering a potential\napproach to investigate model similarity.", "AI": {"tldr": "LTD-Bench是一个突破性的视觉化基准测试，通过让大语言模型生成点阵图或可执行代码来评估空间推理能力，将抽象数值指标转化为直观可视的输出，揭示了当前LLM在语言-空间映射方面的严重缺陷。", "motivation": "当前LLM评估存在严重盲点，依赖不透明的数值指标掩盖了空间推理的根本局限性，无法提供对模型能力的直观理解，导致报告性能与实际能力之间存在危险脱节。", "method": "LTD-Bench采用全面方法论，包含生成任务（测试空间想象力）和识别任务（评估空间感知），通过三个难度递增的级别，系统评估语言-空间映射的两个关键方向。", "result": "实验发现即使传统基准测试表现优异的LLM在建立语言与空间概念双向映射方面也存在严重缺陷，这削弱了它们作为真正世界模型的潜力。", "conclusion": "LTD-Bench通过可视化输出使空间推理限制对非专家也显而易见，弥合了统计性能与直观评估之间的根本差距，并为模型相似性分析提供了强大的诊断方法。"}}
{"id": "2511.02208", "pdf": "https://arxiv.org/pdf/2511.02208", "abs": "https://arxiv.org/abs/2511.02208", "authors": ["Weiwei Sun", "Xuhui Zhou", "Weihua Du", "Xingyao Wang", "Sean Welleck", "Graham Neubig", "Maarten Sap", "Yiming Yang"], "title": "Training Proactive and Personalized LLM Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "While existing work focuses primarily on task success, we argue that\neffective real-world agents require optimizing three dimensions: productivity\n(task completion), proactivity (asking essential questions), and\npersonalization (adapting to diverse user preferences). We introduce UserVille,\nan interactive environment with LLM-based user simulators enabling diverse,\nconfigurable user preferences. Leveraging UserVille, we introduce PPP, a\nmulti-objective reinforcement learning approach that jointly optimizes all\nthree dimensions: Productivity, Proactivity, and Personalization. Experiments\non software engineering and deep research tasks show that agents trained with\nPPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6\non average), demonstrating the ability to ask strategic clarifying questions,\nadapt to unseen user preferences, and improve task success through better\ninteraction. This work demonstrates that explicitly optimizing for\nuser-centered interaction is critical for building practical and effective AI\nagents.", "AI": {"tldr": "该论文提出了一个多目标强化学习方法PPP，同时优化生产力、主动性和个性化三个维度，通过UserVille交互环境训练AI智能体，在软件工程和深度研究任务上显著优于GPT-5等基线模型。", "motivation": "现有研究主要关注任务成功率，但现实世界中的有效智能体需要同时优化生产力（任务完成）、主动性（提出关键问题）和个性化（适应用户偏好）三个维度。", "method": "引入UserVille交互环境，使用基于LLM的用户模拟器支持多样化可配置的用户偏好；提出PPP多目标强化学习方法，联合优化生产力、主动性和个性化三个目标。", "result": "在软件工程和深度研究任务上的实验显示，PPP训练的智能体相比GPT-5等强基线平均提升21.6%，能够提出策略性澄清问题、适应未见过的用户偏好，并通过更好的交互提高任务成功率。", "conclusion": "明确优化以用户为中心的交互对于构建实用有效的AI智能体至关重要，多维度优化方法显著提升了智能体的实际应用效果。"}}
{"id": "2511.02358", "pdf": "https://arxiv.org/pdf/2511.02358", "abs": "https://arxiv.org/abs/2511.02358", "authors": ["Wongyu Kim", "Hochang Lee", "Sanghak Lee", "Yoonsung Kim", "Jaehyun Park"], "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "comment": "Accepted to MMGenSR Workshop (CIKM 2025)", "summary": "Query augmentation makes queries more meaningful by appending further\ninformation to the queries to find relevant documents. Current studies have\nproposed Large Language Model (LLM)-based embedders, which learn representation\nfor embedding and generation for query augmentation in a multi-task manner by\nleveraging the generative capabilities of LLM. During inference, these jointly\ntrained embedders have conducted query augmentation followed by embedding,\nshowing effective results. However, augmenting every query leads to substantial\nembedding latency and query augmentation can be detrimental to performance for\nsome queries. Also, previous methods have not been explored in multimodal\nenvironments. To tackle these problems, we propose M-Solomon, a universal\nmultimodal embedder that can adaptively determine when to augment queries. Our\napproach first divides the queries of the training datasets into two groups at\nthe dataset level. One includes queries that require augmentation and the other\nincludes queries that do not. Then, we introduces a synthesis process that\ngenerates appropriate augmentations for queries that require them by leveraging\na powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.\nThrough this step, M-Solomon can conduct query augmentation only when necessary\nby learning to generate synthetic augmentations with the prefix /augment for\nqueries that demand them and to generate the simple string /embed for others.\nExperimental results showed that M-Solomon not only surpassed the baseline\nwithout augmentation by a large margin but also outperformed the baseline that\nalways used augmentation, providing much faster embedding latency.", "AI": {"tldr": "M-Solomon是一个多模态嵌入器，通过自适应查询增强技术，只在需要时对查询进行增强，解决了传统方法中所有查询都增强导致的延迟问题和性能下降问题。", "motivation": "现有基于LLM的嵌入器对所有查询都进行增强，导致显著的嵌入延迟，且某些查询增强后性能反而下降。同时，之前的方法未在多模态环境中探索。", "method": "1. 在数据集层面将查询分为需要增强和不需要增强两组；2. 利用多模态大语言模型为需要增强的查询生成合适的增强内容；3. 通过自适应查询增强机制，只在必要时进行增强（使用/augment前缀），其他情况生成/embed字符串。", "result": "实验结果显示M-Solomon不仅大幅超越无增强的基线方法，还优于总是使用增强的基线方法，同时提供了更快的嵌入延迟。", "conclusion": "自适应查询增强策略在多模态环境中有效，能够在保持性能的同时显著降低延迟，为查询增强提供了更智能的解决方案。"}}
{"id": "2511.02219", "pdf": "https://arxiv.org/pdf/2511.02219", "abs": "https://arxiv.org/abs/2511.02219", "authors": ["Changjiang Jiang", "Fengchang Yu", "Haihua Chen", "Wei Lu", "Jin Zeng"], "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data", "categories": ["cs.AI"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Complex reasoning over tabular data is crucial in real-world data analysis,\nyet large language models (LLMs) often underperform due to complex queries,\nnoisy data, and limited numerical capabilities. To address these issues, we\npropose \\method, a framework consisting of: (1) a query decomposer that breaks\ndown complex questions, (2) a table sanitizer that cleans and filters noisy\ntables, and (3) a program-of-thoughts (PoT)-based reasoner that generates\nexecutable code to derive the final answer from the sanitized table. To ensure\nunbiased evaluation and mitigate data leakage, we introduce a new dataset,\nCalTab151, specifically designed for complex numerical reasoning over tables.\nExperimental results demonstrate that \\method consistently outperforms existing\nmethods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and\n19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively.\nMoreover, our framework integrates seamlessly with mainstream LLMs, providing a\nrobust solution for complex tabular numerical reasoning. These findings\nhighlight the effectiveness of our framework in enhancing LLM performance for\ncomplex tabular numerical reasoning. Data and code are available upon request.", "AI": {"tldr": "提出名为method的框架，通过查询分解、表格清理和程序化思维推理三个组件，显著提升大语言模型在复杂表格数值推理任务上的性能，在多个基准测试中达到最先进水平。", "motivation": "大语言模型在处理复杂表格查询时表现不佳，主要面临复杂查询处理、噪声数据和数值能力有限等问题。", "method": "框架包含三个核心组件：(1)查询分解器分解复杂问题，(2)表格清理器过滤噪声表格，(3)基于程序化思维(PoT)的推理器生成可执行代码来从清理后的表格推导最终答案。", "result": "在TAT-QA、TableBench和method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到最先进性能。", "conclusion": "该框架有效增强了大语言模型在复杂表格数值推理方面的能力，提供了稳健的解决方案，并可无缝集成到主流大语言模型中。"}}
{"id": "2511.02366", "pdf": "https://arxiv.org/pdf/2511.02366", "abs": "https://arxiv.org/abs/2511.02366", "authors": ["Yudong Li", "Zhongliang Yang", "Kejiang Chen", "Wenxuan Wang", "Tianxin Zhang", "Sifang Wan", "Kecheng Wang", "Haitian Li", "Xu Wang", "Lefan Cheng", "Youdan Yang", "Baocheng Chen", "Ziyu Liu", "Yufei Sun", "Liyan Wu", "Wenya Wen", "Xingchi Gu", "Peiru Yang"], "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.", "AI": {"tldr": "LiveSecBench是一个针对中文LLM应用场景的动态安全基准测试，评估6个关键维度，并持续更新威胁向量，目前已评估18个模型。", "motivation": "针对中文语言环境下的LLM应用缺乏专门的安全评估基准，需要基于中国法律和社会框架来评估模型安全性", "method": "建立包含合法性、伦理、事实性、隐私、对抗鲁棒性和推理安全6个维度的评估体系，采用动态更新机制纳入新威胁向量", "result": "已完成v251030版本，评估了18个LLM模型，提供了中文AI安全现状的全面视图，并建立了公开排行榜", "conclusion": "LiveSecBench为中文LLM安全评估提供了重要基准工具，通过持续更新机制保持相关性，有助于推动中文语言AI安全发展"}}
{"id": "2511.02238", "pdf": "https://arxiv.org/pdf/2511.02238", "abs": "https://arxiv.org/abs/2511.02238", "authors": ["Keyu Zhao", "Weiquan Lin", "Qirui Zheng", "Fengli Xu", "Yong Li"], "title": "Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network", "categories": ["cs.AI"], "comment": "23 pages, 5 figures", "summary": "Novel research ideas play a critical role in advancing scientific inquiries.\nRecent advancements in Large Language Models (LLMs) have demonstrated their\npotential to generate novel research ideas by leveraging large-scale scientific\nliterature. However, previous work in research ideation has primarily relied on\nsimplistic methods, such as keyword co-occurrence or semantic similarity. These\napproaches focus on identifying statistical associations in the literature but\noverlook the complex, contextual relationships between scientific concepts,\nwhich are essential to effectively leverage knowledge embedded in human\nliterature. For instance, papers that simultaneously mention \"keyword A\" and\n\"keyword B\" often present research ideas that integrate both concepts.\nAdditionally, some LLM-driven methods propose and refine research ideas using\nthe model's internal knowledge, but they fail to effectively utilize the\nscientific concept network, limiting the grounding of ideas in established\nresearch. To address these challenges, we propose the Deep Ideation framework\nto address these challenges, integrating a scientific network that captures\nkeyword co-occurrence and contextual relationships, enriching LLM-driven\nideation. The framework introduces an explore-expand-evolve workflow to\niteratively refine research ideas, using an Idea Stack to track progress. A\ncritic engine, trained on real-world reviewer feedback, guides the process by\nproviding continuous feedback on the novelty and feasibility of ideas. Our\nexperiments show that our approach improves the quality of generated ideas by\n10.67% compared to other methods, with ideas surpassing top conference\nacceptance levels. Human evaluation highlights their practical value in\nscientific research, and ablation studies confirm the effectiveness of each\ncomponent in the workflow. Code repo is available at\nhttps://github.com/kyZhao-1/Deep-Ideation.", "AI": {"tldr": "提出了Deep Ideation框架，通过整合科学概念网络和LLM，利用探索-扩展-演化工作流程生成高质量的研究想法，相比现有方法提升10.67%的质量", "motivation": "现有研究想法生成方法过于简单，仅依赖关键词共现或语义相似性，忽视了科学概念间的复杂上下文关系，无法有效利用科学文献中的知识", "method": "结合科学概念网络（捕捉关键词共现和上下文关系）和LLM，采用探索-扩展-演化迭代流程，使用Idea Stack追踪进度，并通过基于真实评审反馈训练的批评引擎提供持续反馈", "result": "实验显示方法相比其他方法提升10.67%的想法质量，生成的想法超过顶级会议接受水平，人工评估确认其科研实用价值，消融研究验证了各组件有效性", "conclusion": "Deep Ideation框架成功解决了现有方法的局限性，通过整合科学网络和迭代优化流程，能够生成高质量、新颖且可行的研究想法，对科研创新有重要价值"}}
{"id": "2511.02374", "pdf": "https://arxiv.org/pdf/2511.02374", "abs": "https://arxiv.org/abs/2511.02374", "authors": ["Mohd Nauman", "Sravan Gvm", "Vijay Devane", "Shyam Pawar", "Viraj Thakur", "Kundeshwar Pundalik", "Piyush Sawarkar", "Rohit Saluja", "Maunendra Desarkar", "Ganesh Ramakrishnan"], "title": "AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current large language models excel at broad, general-purpose tasks, but\nconsistently underperform when exposed to highly specialized domains that\nrequire deep cultural, linguistic, and subject-matter expertise. In particular,\ntraditional medical systems such as Ayurveda embody centuries of nuanced\ntextual and clinical knowledge that mainstream LLMs fail to accurately\ninterpret or apply. We introduce AyurParam-2.9B, a domain-specialized,\nbilingual language model fine-tuned from Param-1-2.9B using an extensive,\nexpertly curated Ayurveda dataset spanning classical texts and clinical\nguidance. AyurParam's dataset incorporates context-aware, reasoning, and\nobjective-style Q&A in both English and Hindi, with rigorous annotation\nprotocols for factual precision and instructional clarity. Benchmarked on\nBhashaBench-Ayur, AyurParam not only surpasses all open-source\ninstruction-tuned models in its size class (1.5--3B parameters), but also\ndemonstrates competitive or superior performance compared to much larger\nmodels. The results from AyurParam highlight the necessity for authentic domain\nadaptation and high-quality supervision in delivering reliable, culturally\ncongruent AI for specialized medical knowledge.", "AI": {"tldr": "AyurParam-2.9B是一个专门针对阿育吠陀医学领域的双语语言模型，在1.5-3B参数规模的开源模型中表现最佳，甚至能与更大模型竞争。", "motivation": "主流大语言模型在需要深度文化、语言和专业知识的高度专业领域（如阿育吠陀医学）表现不佳，无法准确解释和应用这些传统医学系统的知识。", "method": "从Param-1-2.9B模型微调，使用经过专家精心策划的阿育吠陀数据集，包含英语和印地语的情境感知、推理和客观式问答，采用严格的标注协议确保事实准确性和指导清晰度。", "result": "在BhashaBench-Ayur基准测试中，AyurParam不仅超越了同规模的所有开源指令调优模型，还表现出与更大模型相当或更优的性能。", "conclusion": "研究结果表明，在提供可靠、文化契合的专业医学知识AI时，真实的领域适应和高质量监督是必要的。"}}
{"id": "2511.02243", "pdf": "https://arxiv.org/pdf/2511.02243", "abs": "https://arxiv.org/abs/2511.02243", "authors": ["Zhuoran Zhang", "Tengyue Wang", "Xilin Gong", "Yang Shi", "Haotian Wang", "Di Wang", "Lijie Hu"], "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs", "categories": ["cs.AI"], "comment": "19 pages", "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.", "AI": {"tldr": "该研究提出了一个分析多模态大语言模型(MLLMs)处理模态冲突的新框架，将模态跟随行为分解为相对推理不确定性和固有模态偏好两个核心因素，揭示了模型在模态选择中的内在机制。", "motivation": "现有研究仅使用粗粒度的数据集级统计来衡量多模态模型的模态跟随行为，忽略了模型在单模态推理中的置信度影响，需要更精细的分析框架。", "method": "构建可控数据集系统性地改变视觉和文本输入的推理难度，使用熵作为细粒度不确定性度量，分析层间预测以揭示内部机制。", "result": "发现了一个普遍规律：模型跟随某个模态的概率随其相对不确定性的增加而单调下降；在平衡点处揭示了模型的固有偏好；发现模型在模糊区域会在不同层间在模态间振荡。", "conclusion": "相对不确定性和固有偏好是控制模态跟随的两个基本原则，该框架为理解MLLMs如何解决冲突信息提供了定量分析和机制洞察。"}}
{"id": "2511.02376", "pdf": "https://arxiv.org/pdf/2511.02376", "abs": "https://arxiv.org/abs/2511.02376", "authors": ["Aashray Reddy", "Andrew Zagula", "Nicholas Saban"], "title": "AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where\nadversarial prompts elicit harmful outputs, yet most evaluations focus on\nsingle-turn interactions while real-world attacks unfold through adaptive\nmulti-turn conversations. We present AutoAdv, a training-free framework for\nautomated multi-turn jailbreaking that achieves up to 95% attack success rate\non Llama-3.1-8B within six turns a 24 percent improvement over single turn\nbaselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern\nmanager that learns from successful attacks to enhance future prompts, a\ntemperature manager that dynamically adjusts sampling parameters based on\nfailure modes, and a two-phase rewriting strategy that disguises harmful\nrequests then iteratively refines them. Extensive evaluation across commercial\nand open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent\nvulnerabilities in current safety mechanisms, with multi-turn attacks\nconsistently outperforming single-turn approaches. These findings demonstrate\nthat alignment strategies optimized for single-turn interactions fail to\nmaintain robustness across extended conversations, highlighting an urgent need\nfor multi-turn-aware defenses.", "AI": {"tldr": "AutoAdv是一个无需训练的多轮越狱攻击框架，通过三种自适应机制在6轮对话内达到95%的攻击成功率，比单轮攻击提升24%，揭示了当前AI安全机制在多轮对话中的脆弱性。", "motivation": "现有的大语言模型安全评估主要关注单轮交互，而现实中的越狱攻击往往通过多轮自适应对话进行，需要开发能够评估多轮攻击威胁的方法。", "method": "AutoAdv框架结合三种机制：模式管理器从成功攻击中学习改进提示、温度管理器根据失败模式动态调整采样参数、两阶段重写策略先伪装有害请求再迭代优化。", "result": "在Llama-3.1-8B上达到95%攻击成功率，比单轮基线提升24%；在GPT-4o-mini、Qwen3-235B、Mistral-7B等多个模型上都显示多轮攻击比单轮方法更有效。", "conclusion": "针对单轮交互优化的对齐策略无法在扩展对话中保持鲁棒性，当前安全机制存在持续漏洞，迫切需要开发多轮感知的防御措施。"}}
{"id": "2511.02303", "pdf": "https://arxiv.org/pdf/2511.02303", "abs": "https://arxiv.org/abs/2511.02303", "authors": ["Zhiwei Zhang", "Xiaomin Li", "Yudi Lin", "Hui Liu", "Ramraj Chandradevan", "Linlin Wu", "Minhua Lin", "Fali Wang", "Xianfeng Tang", "Qi He", "Suhang Wang"], "title": "Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) trained with reinforcement learning and\nverifiable rewards have achieved strong results on complex reasoning tasks.\nRecent work extends this paradigm to a multi-agent setting, where a\nmeta-thinking agent proposes plans and monitors progress while a reasoning\nagent executes subtasks through sequential conversational turns. Despite\npromising performance, we identify a critical limitation: lazy agent behavior,\nin which one agent dominates while the other contributes little, undermining\ncollaboration and collapsing the setup to an ineffective single agent. In this\npaper, we first provide a theoretical analysis showing why lazy behavior\nnaturally arises in multi-agent reasoning. We then introduce a stable and\nefficient method for measuring causal influence, helping mitigate this issue.\nFinally, as collaboration intensifies, the reasoning agent risks getting lost\nin multi-turn interactions and trapped by previous noisy responses. To counter\nthis, we propose a verifiable reward mechanism that encourages deliberation by\nallowing the reasoning agent to discard noisy outputs, consolidate\ninstructions, and restart its reasoning process when necessary. Extensive\nexperiments demonstrate that our framework alleviates lazy agent behavior and\nunlocks the full potential of multi-agent framework for complex reasoning\ntasks.", "AI": {"tldr": "该论文针对多智能体推理中的懒惰代理行为问题，提出了因果影响测量方法和可验证奖励机制来改善协作效果。", "motivation": "发现多智能体推理中存在懒惰代理行为，即一个代理主导而另一个贡献很少，导致协作失效，实际上退化为单智能体系统。", "method": "1) 提供理论分析解释懒惰行为的产生原因；2) 引入稳定高效的因果影响测量方法；3) 提出可验证奖励机制，允许推理代理丢弃噪声输出、整合指令并在必要时重启推理过程。", "result": "大量实验表明，该框架有效缓解了懒惰代理行为，释放了多智能体框架在复杂推理任务中的全部潜力。", "conclusion": "通过理论分析和创新方法解决了多智能体协作中的关键问题，显著提升了多智能体推理系统的性能和协作效果。"}}
{"id": "2511.02451", "pdf": "https://arxiv.org/pdf/2511.02451", "abs": "https://arxiv.org/abs/2511.02451", "authors": ["Kentaro Ueda", "François Portet", "Hirohiko Suwa", "Keiichi Yasumoto"], "title": "Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance", "categories": ["cs.CL"], "comment": null, "summary": "While LLMs excel at general tasks, they struggle in specialized domains like\nfinance, requiring diverse skills in domain knowledge, mathematical reasoning,\nand multilingual processing. Merging domain-specific Continual Pre-training\n(CPT) \"experts\" offers a practical alternative to costly and unstable\nmulti-skill training. However, unlike established Supervised Fine-Tuning (SFT)\nmodel-based merging, CPT model merging remains largely unexplored. We address\nthis gap by creating financial LLMs from experts in finance, math, and\nJapanese. We propose a three-stage evaluation focusing on knowledge recovery,\ncomplementarity, and emergence, and assess three merging methods (Task\nArithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated\nfrom 18 tasks across 8 established datasets. Results show that merging an\nexpert with its base model recovers general knowledge lost during CPT, while\nmerging experts improves performance and can yield emergent cross-domain\nskills. Among the methods, Task Arithmetic performs strongly but is\nhyperparameter-sensitive, whereas TIES is more robust. Our findings also\nsuggest that while model similarity correlates with merging success, emergent\nskills depend on more complex factors. This work presents the first\nfoundational analysis of CPT model merging, establishing a principled framework\nand providing clear guidance for building multi-skill LLMs from existing\nassets.", "AI": {"tldr": "该论文首次系统研究了领域特定持续预训练(CPT)模型的融合方法，通过融合金融、数学和日语专家模型，提出了三阶段评估框架，发现模型融合能恢复通用知识并产生跨领域涌现能力。", "motivation": "大语言模型在专业领域表现不佳，需要融合多种技能。现有研究主要关注监督微调模型融合，而CPT模型融合研究空白，需要探索如何有效整合领域专家模型。", "method": "创建金融、数学和日语专家模型，采用三种融合方法(Task Arithmetic、TIES、DARE-TIES)，在包含18个任务8个数据集的金融基准上进行三阶段评估(知识恢复、互补性、涌现性)。", "result": "融合专家与基础模型能恢复CPT过程中丢失的通用知识；融合多个专家能提升性能并产生跨领域涌现技能；Task Arithmetic表现强劲但对超参数敏感，TIES更稳健；模型相似性与融合成功相关，但涌现技能依赖更复杂因素。", "conclusion": "本研究为CPT模型融合提供了首个基础分析，建立了原则性框架，为从现有资源构建多技能大语言模型提供了明确指导。"}}
{"id": "2511.02340", "pdf": "https://arxiv.org/pdf/2511.02340", "abs": "https://arxiv.org/abs/2511.02340", "authors": ["Yohan Lee", "DongGyun Kang", "SeHoon Park", "Sa-Yoon Park", "Kwangsoo Kim"], "title": "Chronic Kidney Disease Prognosis Prediction Using Transformer", "categories": ["cs.AI", "q-bio.OT"], "comment": "5 pages, 2 figures, 2 tables", "summary": "Chronic Kidney Disease (CKD) affects nearly 10\\% of the global population and\noften progresses to end-stage renal failure. Accurate prognosis prediction is\nvital for timely interventions and resource optimization. We present a\ntransformer-based framework for predicting CKD progression using multi-modal\nelectronic health records (EHR) from the Seoul National University Hospital\nOMOP Common Data Model. Our approach (\\textbf{ProQ-BERT}) integrates\ndemographic, clinical, and laboratory data, employing quantization-based\ntokenization for continuous lab values and attention mechanisms for\ninterpretability. The model was pretrained with masked language modeling and\nfine-tuned for binary classification tasks predicting progression from stage 3a\nto stage 5 across varying follow-up and assessment periods. Evaluated on a\ncohort of 91,816 patients, our model consistently outperformed CEHR-BERT,\nachieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.\nThese results highlight the effectiveness of transformer architectures and\ntemporal design choices in clinical prognosis modeling, offering a promising\ndirection for personalized CKD care.", "AI": {"tldr": "提出ProQ-BERT框架，基于Transformer架构，使用多模态电子健康记录预测慢性肾病进展，在短期预测中达到接近完美的性能指标。", "motivation": "慢性肾病影响全球近10%人口且常进展至终末期肾衰竭，准确的预后预测对于及时干预和资源优化至关重要。", "method": "基于Transformer框架，整合人口统计学、临床和实验室数据，采用基于量化的标记化处理连续实验室数值，使用掩码语言建模预训练，并针对从3a期到5期的二元分类任务进行微调。", "result": "在91,816名患者队列中评估，模型始终优于CEHR-BERT，短期预测的ROC-AUC高达0.995，PR-AUC高达0.989。", "conclusion": "Transformer架构和时间设计选择在临床预后建模中效果显著，为个性化慢性肾病护理提供了有前景的方向。"}}
{"id": "2511.02458", "pdf": "https://arxiv.org/pdf/2511.02458", "abs": "https://arxiv.org/abs/2511.02458", "authors": ["Giulia Iadisernia", "Carolina Camassa"], "title": "Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas", "categories": ["cs.CL", "cs.CE", "econ.GN", "q-fin.EC"], "comment": "9 pages, 8-pages appendix, accepted at ICAIF 25", "summary": "We evaluate whether persona-based prompting improves Large Language Model\n(LLM) performance on macroeconomic forecasting tasks. Using 2,368\neconomics-related personas from the PersonaHub corpus, we prompt GPT-4o to\nreplicate the ECB Survey of Professional Forecasters across 50 quarterly rounds\n(2013-2025). We compare the persona-prompted forecasts against the human\nexperts panel, across four target variables (HICP, core HICP, GDP growth,\nunemployment) and four forecast horizons. We also compare the results against\n100 baseline forecasts without persona descriptions to isolate its effect. We\nreport two main findings. Firstly, GPT-4o and human forecasters achieve\nremarkably similar accuracy levels, with differences that are statistically\nsignificant yet practically modest. Our out-of-sample evaluation on 2024-2025\ndata demonstrates that GPT-4o can maintain competitive forecasting performance\non unseen events, though with notable differences compared to the in-sample\nperiod. Secondly, our ablation experiment reveals no measurable forecasting\nadvantage from persona descriptions, suggesting these prompt components can be\nomitted to reduce computational costs without sacrificing accuracy. Our results\nprovide evidence that GPT-4o can achieve competitive forecasting accuracy even\non out-of-sample macroeconomic events, if provided with relevant context data,\nwhile revealing that diverse prompts produce remarkably homogeneous forecasts\ncompared to human panels.", "AI": {"tldr": "研究发现GPT-4o在宏观经济预测任务中表现与人类专家相当，但角色提示(persona-based prompting)并未带来预测优势，可以省略以节省计算成本。", "motivation": "评估基于角色的提示是否能提高大语言模型在宏观经济预测任务中的表现，特别是与欧洲央行专业预测者调查进行比较。", "method": "使用PersonaHub语料库中的2,368个经济学相关角色提示GPT-4o，复制欧洲央行专业预测者调查的50个季度预测(2013-2025)，涵盖4个目标变量和4个预测周期，并与无角色描述的基线预测进行对比。", "result": "1. GPT-4o与人类预测者达到非常相似的准确度水平，统计显著但实际差异不大；2. 样本外评估显示GPT-4o在未见事件上保持竞争力；3. 角色描述未带来可测量的预测优势。", "conclusion": "GPT-4o在提供相关上下文数据时能在宏观经济预测中达到竞争性准确度，但多样化的提示产生的预测结果与人类专家组相比高度同质化，角色提示可以省略而不影响准确性。"}}
{"id": "2511.02392", "pdf": "https://arxiv.org/pdf/2511.02392", "abs": "https://arxiv.org/abs/2511.02392", "authors": ["Muhammad Sheharyar Liaqat"], "title": "Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients", "categories": ["cs.AI"], "comment": null, "summary": "Breast cancer remains one of the leading causes of mortality among women\nworldwide, with early diagnosis being critical for effective treatment and\nimproved survival rates. However, timely detection continues to be a challenge\ndue to the complex nature of the disease and variability in patient risk\nfactors. This study presents a fuzzy soft set theory-based expert system\ndesigned to assess the risk of breast cancer in patients using measurable\nclinical and physiological parameters. The proposed system integrates Body Mass\nIndex, Insulin Level, Leptin Level, Adiponectin Level, and age as input\nvariables to estimate breast cancer risk through a set of fuzzy inference rules\nand soft set computations. These parameters can be obtained from routine blood\nanalyses, enabling a non-invasive and accessible method for preliminary\nassessment. The dataset used for model development and validation was obtained\nfrom the UCI Machine Learning Repository. The proposed expert system aims to\nsupport healthcare professionals in identifying high-risk patients and\ndetermining the necessity of further diagnostic procedures such as biopsies.", "AI": {"tldr": "基于模糊软集理论的专家系统，使用BMI、胰岛素、瘦素、脂联素和年龄等临床参数进行乳腺癌风险评估，为非侵入性初步筛查提供支持", "motivation": "乳腺癌是全球女性主要死因之一，早期诊断对治疗和生存率至关重要，但由于疾病复杂性和患者风险因素差异，及时检测仍面临挑战", "method": "开发基于模糊软集理论的专家系统，整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算进行风险评估", "result": "系统使用UCI机器学习数据库进行开发和验证，能够通过常规血液分析获取参数，为非侵入性初步评估提供可行方法", "conclusion": "该专家系统旨在帮助医疗专业人员识别高风险患者，并确定是否需要进一步诊断程序（如活检），提高乳腺癌早期检测的可及性"}}
{"id": "2511.02537", "pdf": "https://arxiv.org/pdf/2511.02537", "abs": "https://arxiv.org/abs/2511.02537", "authors": ["Kenza Khelkhal", "Dihia Lanasri"], "title": "Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching", "categories": ["cs.CL"], "comment": null, "summary": "Hiring processes often involve the manual screening of hundreds of resumes\nfor each job, a task that is time and effort consuming, error-prone, and\nsubject to human bias. This paper presents Smart-Hiring, an end-to-end Natural\nLanguage Processing (NLP) pipeline de- signed to automatically extract\nstructured information from unstructured resumes and to semantically match\ncandidates with job descriptions. The proposed system combines document\nparsing, named-entity recognition, and contextual text embedding techniques to\ncapture skills, experience, and qualifications. Using advanced NLP technics,\nSmart-Hiring encodes both resumes and job descriptions in a shared vector space\nto compute similarity scores between candidates and job postings. The pipeline\nis modular and explainable, allowing users to inspect extracted entities and\nmatching rationales. Experiments were conducted on a real-world dataset of\nresumes and job descriptions spanning multiple professional domains,\ndemonstrating the robustness and feasibility of the proposed approach. The\nsystem achieves competitive matching accuracy while preserving a high degree of\ninterpretability and transparency in its decision process. This work introduces\na scalable and practical NLP frame- work for recruitment analytics and outlines\npromising directions for bias mitigation, fairness-aware modeling, and\nlarge-scale deployment of data-driven hiring solutions.", "AI": {"tldr": "Smart-Hiring是一个端到端的NLP流水线系统，能够自动从简历中提取结构化信息，并通过语义匹配将候选人与职位描述进行智能匹配，提高招聘效率并减少人为偏见。", "motivation": "传统招聘过程中手动筛选大量简历耗时耗力、容易出错且存在人为偏见，需要自动化解决方案来提高招聘效率和公平性。", "method": "结合文档解析、命名实体识别和上下文文本嵌入技术，在共享向量空间中编码简历和职位描述，计算相似度分数。系统采用模块化和可解释的设计。", "result": "在真实数据集上的实验表明，该系统在保持高可解释性和透明度的同时，实现了有竞争力的匹配准确率。", "conclusion": "该研究提出了一个可扩展且实用的NLP招聘分析框架，为偏见缓解、公平感知建模和大规模数据驱动招聘解决方案的部署指明了有前景的方向。"}}
{"id": "2511.02414", "pdf": "https://arxiv.org/pdf/2511.02414", "abs": "https://arxiv.org/abs/2511.02414", "authors": ["Benjamin Sykes", "Loïc Simon", "Julien Rabin", "Jalal Fadili"], "title": "A New Perspective on Precision and Recall for Generative Models", "categories": ["cs.AI"], "comment": null, "summary": "With the recent success of generative models in image and text, the question\nof their evaluation has recently gained a lot of attention. While most methods\nfrom the state of the art rely on scalar metrics, the introduction of Precision\nand Recall (PR) for generative model has opened up a new avenue of research.\nThe associated PR curve allows for a richer analysis, but their estimation\nposes several challenges. In this paper, we present a new framework for\nestimating entire PR curves based on a binary classification standpoint. We\nconduct a thorough statistical analysis of the proposed estimates. As a\nbyproduct, we obtain a minimax upper bound on the PR estimation risk. We also\nshow that our framework extends several landmark PR metrics of the literature\nwhich by design are restrained to the extreme values of the curve. Finally, we\nstudy the different behaviors of the curves obtained experimentally in various\nsettings.", "AI": {"tldr": "该论文提出了一个新的基于二元分类视角的生成模型精度-召回率（PR）曲线估计框架，进行了统计分析并获得了PR估计风险的最小最大上界，扩展了现有文献中的PR度量方法。", "motivation": "随着生成模型在图像和文本领域的成功，其评估方法受到广泛关注。虽然现有方法主要依赖标量指标，但精度-召回率（PR）曲线为生成模型评估提供了更丰富的分析维度，但其估计面临诸多挑战。", "method": "基于二元分类的视角，提出了一个估计完整PR曲线的新框架，进行了深入的统计分析，并获得了PR估计风险的最小最大上界。", "result": "开发了一个能够估计完整PR曲线的框架，该框架扩展了现有文献中仅限于曲线极值的PR度量方法，并在不同实验设置下研究了曲线的不同行为特征。", "conclusion": "提出的基于二元分类的PR曲线估计框架为生成模型评估提供了更全面的分析工具，克服了现有标量指标的局限性，具有重要的理论和实践价值。"}}
{"id": "2511.02587", "pdf": "https://arxiv.org/pdf/2511.02587", "abs": "https://arxiv.org/abs/2511.02587", "authors": ["Angela Stamatie"], "title": "The Analysis of Lexical Errors in Machine Translation from English into Romanian", "categories": ["cs.CL"], "comment": "Doctoral thesis", "summary": "The research explores error analysis in the performance of translating by\nMachine Translation from English into Romanian, and it focuses on lexical\nerrors found in texts which include official information, provided by the World\nHealth Organization (WHO), the Gavi Organization, by the patient information\nleaflet (the information about the active ingredients of the vaccines or the\nmedication, the indications, the dosage instructions, the storage instructions,\nthe side effects and warning, etc.). All of these texts are related to Covid-19\nand have been translated by Google Translate, a multilingual Machine\nTranslation that was created by Google. In the last decades, Google has\nactively worked to develop a more accurate and fluent automatic translation\nsystem. This research, specifically focused on improving Google Translate, aims\nto enhance the overall quality of Machine Translation by achieving better\nlexical selection and by reducing errors. The investigation involves a\ncomprehensive analysis of 230 texts that have been translated from English into\nRomanian.", "AI": {"tldr": "该研究分析了谷歌翻译从英语到罗马尼亚语翻译中的词汇错误，特别关注WHO和Gavi组织提供的COVID-19相关官方信息文本，通过分析230个文本来提升机器翻译的词汇选择准确性。", "motivation": "研究旨在提高机器翻译的质量，特别是谷歌翻译在英语到罗马尼亚语翻译中的表现，通过识别和减少词汇错误来改进翻译系统的准确性和流畅度。", "method": "对230个从英语翻译成罗马尼亚语的文本进行全面的错误分析，这些文本包含COVID-19相关的官方医疗信息，使用谷歌翻译作为翻译工具。", "result": "研究发现并分类了机器翻译中出现的各种词汇错误，为改进翻译系统提供了具体的数据支持。", "conclusion": "通过系统性的错误分析，该研究为提升谷歌翻译的词汇选择准确性提供了重要见解，有助于改进机器翻译的整体质量。"}}
{"id": "2511.02424", "pdf": "https://arxiv.org/pdf/2511.02424", "abs": "https://arxiv.org/abs/2511.02424", "authors": ["Jae-Woo Choi", "Hyungmin Kim", "Hyobin Ong", "Minsu Jang", "Dohyung Kim", "Jaehong Kim", "Youngwoo Yoon"], "title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have enabled significant\nprogress in decision-making and task planning for embodied autonomous agents.\nHowever, most existing methods still struggle with complex, long-horizon tasks\nbecause they rely on a monolithic trajectory that entangles all past decisions\nand observations, attempting to solve the entire task in a single unified\nprocess. To address this limitation, we propose ReAcTree, a hierarchical\ntask-planning method that decomposes a complex goal into more manageable\nsubgoals within a dynamically constructed agent tree. Each subgoal is handled\nby an LLM agent node capable of reasoning, acting, and further expanding the\ntree, while control flow nodes coordinate the execution strategies of agent\nnodes. In addition, we integrate two complementary memory systems: each agent\nnode retrieves goal-specific, subgoal-level examples from episodic memory and\nshares environment-specific observations through working memory. Experiments on\nthe WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently\noutperforms strong task-planning baselines such as ReAct across diverse LLMs.\nNotably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5\n72B, nearly doubling ReAct's 31%.", "AI": {"tldr": "ReAcTree是一种分层任务规划方法，通过动态构建代理树将复杂目标分解为可管理的子目标，使用LLM代理节点处理子任务，配合控制流节点协调执行，集成情景记忆和工作记忆系统，在WAH-NL和ALFRED数据集上显著优于ReAct等基线方法。", "motivation": "现有方法在处理复杂长时程任务时存在局限，因为它们依赖单一轨迹来整合所有过去的决策和观察，试图通过单一统一过程解决整个任务，这导致性能受限。", "method": "提出分层任务规划方法ReAcTree：1) 将复杂目标分解为可管理子目标；2) 构建动态代理树结构；3) 每个子目标由LLM代理节点处理（推理、行动、扩展树）；4) 控制流节点协调执行策略；5) 集成情景记忆（目标特定示例）和工作记忆（环境特定观察）系统。", "result": "在WAH-NL和ALFRED数据集上的实验表明，ReAcTree在不同LLM上始终优于ReAct等强基线。在WAH-NL上，使用Qwen 2.5 72B时达到61%的目标成功率，几乎是ReAct（31%）的两倍。", "conclusion": "ReAcTree通过分层分解和动态树结构有效解决了复杂长时程任务规划问题，证明了分层方法和记忆系统集成在提升LLM代理决策能力方面的重要价值。"}}
{"id": "2511.02599", "pdf": "https://arxiv.org/pdf/2511.02599", "abs": "https://arxiv.org/abs/2511.02599", "authors": ["Max Norris", "Kobi Gal", "Sahan Bulathwela"], "title": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modelling student knowledge is a key challenge when leveraging AI in\neducation, with major implications for personalised learning. The Knowledge\nTracing (KT) task aims to predict how students will respond to educational\nquestions in learning environments, based on their prior interactions. Existing\nKT models typically use response correctness along with metadata like skill\ntags and timestamps, often overlooking the question text, which is an important\nsource of pedagogical insight. This omission poses a lost opportunity while\nlimiting predictive performance. We propose Next Token Knowledge Tracing\n(NTKT), a novel approach that reframes KT as a next-token prediction task using\npretrained Large Language Models (LLMs). NTKT represents both student histories\nand question content as sequences of text, allowing LLMs to learn patterns in\nboth behaviour and language. Our series of experiments significantly improves\nperformance over state-of-the-art neural KT models and generalises much better\nto cold-start questions and users. These findings highlight the importance of\nquestion content in KT and demonstrate the benefits of leveraging pretrained\nrepresentations of LLMs to model student learning more effectively.", "AI": {"tldr": "NTKT是一种新颖的知识追踪方法，将KT任务重新定义为使用预训练大语言模型的下一词预测任务，通过将学生历史和问题内容表示为文本序列，显著提升了预测性能。", "motivation": "现有KT模型主要使用回答正确性和元数据，但忽略了问题文本这一重要教学信息源，这限制了预测性能并错失了改进机会。", "method": "提出NTKT方法，将知识追踪重构为下一词预测任务，利用预训练LLM同时学习学生行为模式和语言模式，将学生历史记录和问题内容表示为文本序列。", "result": "实验表明NTKT显著优于最先进的神经KT模型，在冷启动问题和用户场景下表现更好的泛化能力。", "conclusion": "问题内容在知识追踪中至关重要，利用预训练LLM表示能更有效地建模学生学习过程，为个性化学习提供了新的技术路径。"}}
{"id": "2511.02463", "pdf": "https://arxiv.org/pdf/2511.02463", "abs": "https://arxiv.org/abs/2511.02463", "authors": ["Mengyu Zhang", "Xubo Liu", "Siyu Ding", "Weichong Yin", "Yu Sun", "Hua Wu", "Wenya Guo", "Ying Zhang"], "title": "Auditable-choice reframing unlocks RL-based verification for open-ended tasks", "categories": ["cs.AI"], "comment": "9 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great\npotential in enhancing the reasoning capabilities of large language models\n(LLMs), achieving remarkable progress in domains such as mathematics and\nprogramming where standard answers are available. However, for open-ended tasks\nlacking ground-truth solutions (e.g., creative writing and instruction\nfollowing), existing studies typically regard them as non-reasoning scenarios,\nthereby overlooking the latent value of reasoning capabilities. This raises a\nkey question: Can strengthening reasoning improve performance in open-ended\ntasks? To address this, we explore the transfer of the RLVR paradigm to the\nopen domain. Yet, since RLVR fundamentally relies on verifiers that presuppose\nthe existence of standard answers, it cannot be directly applied to open-ended\ntasks. To overcome this challenge, we introduce Verifiable Multiple-Choice\nReformulation (VMR), a novel training strategy that restructures open-ended\ndata into verifiable multiple-choice formats, enabling effective training even\nin the absence of explicit ground truth. Experimental results on multiple\nbenchmarks validate the effectiveness of our method in improving LLM\nperformance on open-ended tasks. Notably, across eight open-ended benchmarks,\nour VMR-based training delivers an average gain of 5.99 points over the\nbaseline. Code will be released upon acceptance to facilitate reproducibility.", "AI": {"tldr": "该论文提出VMR方法，将开放式任务转换为可验证的多选题格式，使RLVR训练范式能够应用于缺乏标准答案的开放式任务，在8个基准测试中平均提升5.99分。", "motivation": "现有RLVR方法在数学和编程等有标准答案的任务中表现优异，但在开放式任务（如创意写作和指令跟随）中被视为非推理场景，忽略了推理能力的潜在价值。", "method": "提出Verifiable Multiple-Choice Reformulation (VMR)训练策略，将开放式数据重构为可验证的多选题格式，从而在没有显式真实答案的情况下实现有效训练。", "result": "在多个基准测试中验证了方法的有效性，在8个开放式基准测试中，基于VMR的训练相比基线平均提升了5.99分。", "conclusion": "VMR方法成功将RLVR范式扩展到开放式任务领域，证明了通过增强推理能力可以提升开放式任务的性能，为LLM在更广泛场景中的应用提供了新思路。"}}
{"id": "2511.02603", "pdf": "https://arxiv.org/pdf/2511.02603", "abs": "https://arxiv.org/abs/2511.02603", "authors": ["Ehsan Aghazadeh", "Ahmad Ghasemi", "Hedyeh Beyhaghi", "Hossein Pishro-Nik"], "title": "CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency", "categories": ["cs.CL"], "comment": "Efficient Reasoning @ NeurIPS2025", "summary": "Large language models (LLMs) are often queried multiple times at test time,\nwith predictions aggregated by majority vote. While effective, this\nself-consistency strategy (arXiv:2203.11171) requires a fixed number of calls\nand can fail when the correct answer is rare. We introduce Confidence-Guided\nEarly Stopping (CGES), a Bayesian framework that forms posteriors over\ncandidate answers using scalar confidence signals derived from token\nprobabilities or reward models. CGES adaptively halts sampling once the\nposterior mass of a candidate exceeds a threshold. We provide theoretical\nguarantees for both perfectly calibrated confidences and realistic noisy\nconfidence signals. Across five reasoning benchmarks, CGES reduces the average\nnumber of model calls by about 69 percent (for example, from 16.0 to 4.9) while\nmatching the accuracy of self-consistency within 0.06 percentage points.", "AI": {"tldr": "CGES是一种基于贝叶斯框架的自适应早停方法，通过置信度信号指导采样，在保持精度的同时显著减少大语言模型的调用次数。", "motivation": "现有的自一致性策略需要固定次数的模型调用，且当正确答案罕见时容易失败，需要更高效的采样方法。", "method": "使用从token概率或奖励模型获得的标量置信度信号构建候选答案的后验分布，当某个候选的后验质量超过阈值时自适应停止采样。", "result": "在五个推理基准测试中，平均减少69%的模型调用次数（如从16.0次降至4.9次），同时精度仅下降0.06个百分点。", "conclusion": "CGES框架提供了理论保证，在保持自一致性策略精度的同时大幅提升了推理效率，适用于实际部署场景。"}}
{"id": "2511.02532", "pdf": "https://arxiv.org/pdf/2511.02532", "abs": "https://arxiv.org/abs/2511.02532", "authors": ["Jorge Pellejero", "Luis A. Hernández Gómez", "Luis Mendo Tomás", "Zoraida Frias Barroso"], "title": "Agentic AI for Mobile Network RAN Management and Optimization", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Agentic AI represents a new paradigm for automating complex systems by using\nLarge AI Models (LAMs) to provide human-level cognitive abilities with\nmultimodal perception, planning, memory, and reasoning capabilities. This will\nlead to a new generation of AI systems that autonomously decompose goals,\nretain context over time, learn continuously, operate across tools and\nenvironments, and adapt dynamically. The complexity of 5G and upcoming 6G\nnetworks renders manual optimization ineffective, pointing to Agentic AI as a\nmethod for automating decisions in dynamic RAN environments. However, despite\nits rapid advances, there is no established framework outlining the\nfoundational components and operational principles of Agentic AI systems nor a\nuniversally accepted definition.\n  This paper contributes to ongoing research on Agentic AI in 5G and 6G\nnetworks by outlining its core concepts and then proposing a practical use case\nthat applies Agentic principles to RAN optimization. We first introduce Agentic\nAI, tracing its evolution from classical agents and discussing the progress\nfrom workflows and simple AI agents to Agentic AI. Core design\npatterns-reflection, planning, tool use, and multi-agent collaboration-are then\ndescribed to illustrate how intelligent behaviors are orchestrated. These\ntheorical concepts are grounded in the context of mobile networks, with a focus\non RAN management and optimization. A practical 5G RAN case study shows how\ntime-series analytics and LAM-driven agents collaborate for KPI-based\nautonomous decision-making.", "AI": {"tldr": "本文探讨了Agentic AI在5G/6G网络中的应用，提出了一个将大型AI模型与RAN优化相结合的理论框架和实践案例，展示了基于KPI的自主决策能力。", "motivation": "5G/6G网络的复杂性使得人工优化变得低效，需要Agentic AI来在动态RAN环境中实现自动化决策，但目前缺乏统一的理论框架和定义。", "method": "首先介绍Agentic AI的概念演进，从传统智能体到现代Agentic AI的发展；然后描述核心设计模式（反思、规划、工具使用、多智能体协作）；最后通过5G RAN案例研究展示时间序列分析和LAM驱动智能体的协作。", "result": "提出了Agentic AI在移动网络中的理论基础，并展示了其在RAN管理和优化中的实际应用能力，实现了基于KPI的自主决策。", "conclusion": "Agentic AI为5G/6G网络自动化提供了有前景的解决方案，通过结合大型AI模型和核心设计模式，能够有效处理复杂网络环境的动态优化问题。"}}
{"id": "2511.02623", "pdf": "https://arxiv.org/pdf/2511.02623", "abs": "https://arxiv.org/abs/2511.02623", "authors": ["Aakash Sen Sharma", "Debdeep Sanyal", "Vivek Srivastava", "Shirish Karande", "Murari Mandal"], "title": "The Realignment Problem: When Right becomes Wrong in LLMs", "categories": ["cs.CL"], "comment": "23 Pages", "summary": "The alignment of Large Language Models (LLMs) with human values is central to\ntheir safe deployment, yet current practice produces static, brittle, and\ncostly-to-maintain models that fail to keep pace with evolving norms and\npolicies. This misalignment, which we term the Alignment-Reality Gap, poses a\ngrowing challenge for reliable long-term use. Existing remedies are inadequate:\nlarge-scale re-annotation is economically prohibitive, and standard unlearning\nmethods act as blunt instruments that erode utility rather than enable precise\npolicy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict\nEvaluation), a framework for principled unlearning that reconceives\nre-alignment as a programmatic policy application problem. TRACE\nprogrammatically triages existing preference data against a new policy,\nidentifies high-impact conflicts via a alignment impact score, and applies a\nhybrid optimization that cleanly inverts, discards, or preserves preferences\nwhile safeguarding model performance. Empirical results show that TRACE\nachieves robust re-alignment across diverse model families (Qwen2.5-7B,\nGemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF\ndataset under complex policy shift, TRACE enforces new principles without\ndegrading general capabilities. Our work establishes a scalable, dynamic, and\ncost-effective paradigm for maintaining LLM alignment, providing a foundation\nfor sustainable and responsible AI deployment.", "AI": {"tldr": "TRACE框架通过程序化策略应用解决大语言模型与现实政策间的对齐差距，实现精确的策略更新而不损害模型性能", "motivation": "当前大语言模型与人类价值观的对齐存在静态、脆弱且维护成本高的问题，无法跟上不断演变的规范和政策，形成对齐-现实差距", "method": "提出TRACE框架：程序化筛选现有偏好数据与新政策的冲突，通过对齐影响评分识别高影响冲突，应用混合优化方法（反转、丢弃或保留偏好）", "result": "在多个模型家族（Qwen2.5-7B、Gemma-2-9B、Llama-3.1-8B）上实现稳健的重新对齐，在合成基准和PKU-SafeRLHF数据集上实施新原则而不降低通用能力", "conclusion": "TRACE为维持LLM对齐建立了可扩展、动态且成本效益高的范式，为可持续和负责任的AI部署提供了基础"}}
{"id": "2511.02534", "pdf": "https://arxiv.org/pdf/2511.02534", "abs": "https://arxiv.org/abs/2511.02534", "authors": ["Enhong Mu", "Jinyu Cai", "Yijun Lu", "Mingyue Zhang", "Kenji Tei", "Jialong Li"], "title": "Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting", "categories": ["cs.AI"], "comment": null, "summary": "The rapid iteration and frequent updates of modern video games pose\nsignificant challenges to the efficiency and specificity of testing. Although\nautomated playtesting methods based on Large Language Models (LLMs) have shown\npromise, they often lack structured knowledge accumulation mechanisms, making\nit difficult to conduct precise and efficient testing tailored for incremental\ngame updates. To address this challenge, this paper proposes a KLPEG framework.\nThe framework constructs and maintains a Knowledge Graph (KG) to systematically\nmodel game elements, task dependencies, and causal relationships, enabling\nknowledge accumulation and reuse across versions. Building on this foundation,\nthe framework utilizes LLMs to parse natural language update logs, identify the\nscope of impact through multi-hop reasoning on the KG, enabling the generation\nof update-tailored test cases. Experiments in two representative game\nenvironments, Overcooked and Minecraft, demonstrate that KLPEG can more\naccurately locate functionalities affected by updates and complete tests in\nfewer steps, significantly improving both playtesting effectiveness and\nefficiency.", "AI": {"tldr": "本文提出KLPEG框架，通过构建知识图谱来系统建模游戏元素、任务依赖和因果关系，利用LLM解析更新日志并生成针对性测试用例，在Overcooked和Minecraft游戏中验证了其高效性和准确性。", "motivation": "现代游戏快速迭代更新给测试带来挑战，现有基于LLM的自动化测试方法缺乏结构化知识积累机制，难以针对增量更新进行精确高效测试。", "method": "提出KLPEG框架：1) 构建和维护知识图谱(KG)建模游戏元素、任务依赖和因果关系；2) 利用LLM解析自然语言更新日志；3) 通过KG多跳推理识别影响范围；4) 生成针对更新的测试用例。", "result": "在Overcooked和Minecraft两个代表性游戏环境中实验表明，KLPEG能更准确定位受更新影响的功能，用更少步骤完成测试，显著提升测试效果和效率。", "conclusion": "KLPEG框架通过知识图谱和LLM的结合，有效解决了游戏增量更新的自动化测试问题，实现了知识的积累和重用，为游戏测试提供了新的解决方案。"}}
{"id": "2511.02626", "pdf": "https://arxiv.org/pdf/2511.02626", "abs": "https://arxiv.org/abs/2511.02626", "authors": ["Renfei Dang", "Peng Hu", "Changjiang Gao", "Shujian Huang"], "title": "Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation", "categories": ["cs.CL"], "comment": null, "summary": "Previous studies show that introducing new knowledge during large language\nmodels (LLMs) fine-tuning can lead to the generation of erroneous output when\ntested on known information, thereby triggering factual hallucinations.\nHowever, existing studies have not deeply investigated the specific\nmanifestations and underlying mechanisms of these hallucinations. Our work\naddresses this gap by designing a controlled dataset Biography-Reasoning, and\nconducting a fine-grained analysis across multiple knowledge types and two task\ntypes, including knowledge question answering (QA) and knowledge reasoning\ntasks. We find that when fine-tuned on a dataset in which a specific knowledge\ntype consists entirely of new knowledge, LLMs exhibit significantly increased\nhallucination tendencies. This suggests that the high unfamiliarity of a\nparticular knowledge type, rather than the overall proportion of new knowledge,\nis a stronger driver of hallucinations, and these tendencies can even affect\nother knowledge types in QA tasks. To mitigate such factual hallucinations, we\npropose KnownPatch, which patches a small number of known knowledge samples in\nthe later stages of training, effectively alleviating new-knowledge-induced\nhallucinations. Through attention analysis, we find that learning new knowledge\nreduces the model's attention to key entities in the question, thus causing\nexcessive focus on the surrounding context, which may increase the risk of\nhallucination. Moreover, the attention pattern can propagate to similar\ncontexts, facilitating the spread of hallucinations to textually similar\nquestions. Our method effectively mitigates the disruption of new knowledge\nlearning to the model's attention on key entities, accompanied by improved\nperformance.", "AI": {"tldr": "研究发现LLM在新知识微调时会产生事实幻觉，提出KnownPatch方法通过在训练后期添加少量已知知识样本来缓解这一问题，并通过注意力分析揭示了幻觉机制。", "motivation": "现有研究未深入探讨LLM在新知识微调时产生事实幻觉的具体表现和机制，需要填补这一研究空白。", "method": "设计控制数据集Biography-Reasoning，进行细粒度分析；提出KnownPatch方法在训练后期添加已知知识样本；通过注意力分析研究幻觉机制。", "result": "发现特定知识类型完全由新知识组成时幻觉显著增加；KnownPatch有效缓解新知识引起的幻觉；注意力分析显示新知识学习会降低模型对问题关键实体的关注。", "conclusion": "知识类型的高陌生度是幻觉的主要驱动因素；KnownPatch方法能有效改善注意力模式并提升性能；注意力模式可传播到相似语境导致幻觉扩散。"}}
{"id": "2511.02589", "pdf": "https://arxiv.org/pdf/2511.02589", "abs": "https://arxiv.org/abs/2511.02589", "authors": ["Claudia Herambourg", "Dawid Siuda", "Anna Szczepanek", "Julia Kopczyńska", "Joao R. L. Santos", "Wojciech Sas", "Joanna Śmietańska-Nowak"], "title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics.", "AI": {"tldr": "ORCA基准测试评估5个顶级LLM在多领域定量推理任务中的表现，结果显示准确率仅45-63%，主要错误为舍入和计算错误，模型在不同领域表现差异明显且存在部分互补性。", "motivation": "现有数学数据集无法充分评估LLM在真实多领域定量推理任务中的表现，需要开发能够测试逐步推理、数值精度和领域泛化能力的新基准。", "method": "创建ORCA基准，包含500个跨领域自然语言任务（金融、物理、健康、统计），使用已验证的计算器输出作为标准答案，评估5个先进LLM的性能。", "result": "所有模型准确率在45-63%之间，35%错误来自舍入问题，33%来自计算错误。数学和工程领域表现较好，物理和自然科学较弱。模型间相关性0.40-0.65，显示部分互补性。", "conclusion": "当前LLM在真实定量推理任务中仍有显著局限性，需要改进数值精度和领域适应性。ORCA基准为评估和改进LLM的定量推理能力提供了重要工具。"}}
{"id": "2511.02681", "pdf": "https://arxiv.org/pdf/2511.02681", "abs": "https://arxiv.org/abs/2511.02681", "authors": ["Mohammadsajad Alipour", "Mohammad Mohammadi Amiri"], "title": "Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly prevalent across diverse\napplications. However, their enormous size limits storage and processing\ncapabilities to a few well-resourced stakeholders. As a result, most\napplications rely on pre-trained LLMs, fine-tuned for specific tasks. However,\neven storing the fine-tuned versions of these models remains a significant\nchallenge due to the wide range of tasks they address. Recently, studies show\nthat fine-tuning these models primarily affects a small fraction of parameters,\nhighlighting the need for more efficient storage of fine-tuned models. This\npaper focuses on efficient storage of parameter updates in pre-trained models\nafter fine-tuning. To address this challenge, we leverage the observation that\nfine-tuning updates are both low-rank and sparse, which can be utilized for\nstorage efficiency. However, using only low-rank approximation or\nsparsification may discard critical singular components that enhance model\nexpressivity. We first observe that given the same memory budget, sparsified\nlow-rank approximations with larger ranks outperform standard low-rank\napproximations with smaller ranks. Building on this, we propose our method,\noptimal singular damage, that selectively sparsifies low-rank approximated\nupdates by leveraging the interleaved importance of singular vectors, ensuring\nthat the most impactful components are retained. We demonstrate through\nextensive experiments that our proposed methods lead to significant storage\nefficiency and superior accuracy within the same memory budget compared to\nemploying the low-rank approximation or sparsification individually.", "AI": {"tldr": "本文提出了一种名为\"最优奇异值损伤\"的方法，通过选择性稀疏化低秩近似更新来高效存储微调后的参数更新，在相同内存预算下实现了更好的存储效率和准确性。", "motivation": "大语言模型(LLMs)的巨大尺寸限制了存储和处理能力，微调版本存储仍然是一个重大挑战。研究发现微调主要影响少量参数，需要更高效的微调模型存储方案。", "method": "利用微调更新具有低秩和稀疏特性的观察，提出最优奇异值损伤方法，通过利用奇异向量的交错重要性选择性地稀疏化低秩近似更新，确保保留最具影响力的组件。", "result": "在相同内存预算下，具有更大秩的稀疏化低秩近似优于较小秩的标准低秩近似，实验证明该方法显著提高了存储效率并保持优越的准确性。", "conclusion": "该方法有效解决了微调模型存储的挑战，通过结合低秩近似和稀疏化的优势，在有限内存条件下实现了更好的性能表现。"}}
{"id": "2511.02605", "pdf": "https://arxiv.org/pdf/2511.02605", "abs": "https://arxiv.org/abs/2511.02605", "authors": ["Tiberiu-Andrei Georgescu", "Alexander W. Goodall", "Dalal Alrajeh", "Francesco Belardinelli", "Sebastian Uchitel"], "title": "Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Shielding is widely used to enforce safety in reinforcement learning (RL),\nensuring that an agent's actions remain compliant with formal specifications.\nClassical shielding approaches, however, are often static, in the sense that\nthey assume fixed logical specifications and hand-crafted abstractions. While\nthese static shields provide safety under nominal assumptions, they fail to\nadapt when environment assumptions are violated. In this paper, we develop the\nfirst adaptive shielding framework - to the best of our knowledge - based on\nGeneralized Reactivity of rank 1 (GR(1)) specifications, a tractable and\nexpressive fragment of Linear Temporal Logic (LTL) that captures both safety\nand liveness properties. Our method detects environment assumption violations\nat runtime and employs Inductive Logic Programming (ILP) to automatically\nrepair GR(1) specifications online, in a systematic and interpretable way. This\nensures that the shield evolves gracefully, ensuring liveness is achievable and\nweakening goals only when necessary. We consider two case studies: Minepump and\nAtari Seaquest; showing that (i) static symbolic controllers are often severely\nsuboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped\nwith our adaptive shield maintain near-optimal reward and perfect logical\ncompliance compared with static shields.", "AI": {"tldr": "提出了首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违规和使用ILP在线修复规范，使屏蔽器能够优雅演化，在保持最优奖励的同时确保完美的逻辑合规性。", "motivation": "传统静态屏蔽方法假设固定的逻辑规范和手工制作的抽象，在环境假设被违反时无法适应，需要开发能够动态调整的屏蔽机制。", "method": "基于GR(1)规范，运行时检测环境假设违规，使用归纳逻辑编程(ILP)在线自动修复GR(1)规范，实现系统化和可解释的适应性调整。", "result": "在Minepump和Atari Seaquest案例研究中显示，自适应屏蔽相比静态屏蔽能够维持接近最优的奖励和完美的逻辑合规性。", "conclusion": "自适应屏蔽框架成功解决了静态屏蔽的局限性，为强化学习安全提供了更加灵活和有效的保障机制。"}}
{"id": "2511.02721", "pdf": "https://arxiv.org/pdf/2511.02721", "abs": "https://arxiv.org/abs/2511.02721", "authors": ["Doreen Osmelak", "Koel Dutta Chowdhury", "Uliana Sentsova", "Cristina España-Bonet", "Josef van Genabith"], "title": "PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation", "categories": ["cs.CL"], "comment": null, "summary": "Translators often enrich texts with background details that make implicit\ncultural meanings explicit for new audiences. This phenomenon, known as\npragmatic explicitation, has been widely discussed in translation theory but\nrarely modeled computationally. We introduce PragExTra, the first multilingual\ncorpus and detection framework for pragmatic explicitation. The corpus covers\neight language pairs from TED-Multi and Europarl and includes additions such as\nentity descriptions, measurement conversions, and translator remarks. We\nidentify candidate explicitation cases through null alignments and refined\nusing active learning with human annotation. Our results show that entity and\nsystem-level explicitations are most frequent, and that active learning\nimproves classifier accuracy by 7-8 percentage points, achieving up to 0.88\naccuracy and 0.82 F1 across languages. PragExTra establishes pragmatic\nexplicitation as a measurable, cross-linguistic phenomenon and takes a step\ntowards building culturally aware machine translation. Keywords: translation,\nmultilingualism, explicitation", "AI": {"tldr": "PragExTra：首个多语言语料库和检测框架，用于计算建模语用显化现象，通过空对齐和主动学习识别文化背景信息的显化添加。", "motivation": "翻译理论中广泛讨论但很少计算建模的语用显化现象，即译者通过添加背景细节使隐含文化意义对目标读者显化的过程。", "method": "构建多语言语料库（覆盖8种语言对），通过空对齐识别候选显化案例，使用主动学习和人工标注进行精炼，开发检测框架。", "result": "实体和系统层面的显化最为频繁，主动学习使分类器准确率提升7-8个百分点，跨语言准确率达0.88，F1值达0.82。", "conclusion": "PragExTra将语用显化确立为可衡量的跨语言现象，为构建文化感知的机器翻译迈出重要一步。"}}
{"id": "2511.02606", "pdf": "https://arxiv.org/pdf/2511.02606", "abs": "https://arxiv.org/abs/2511.02606", "authors": ["Xiangen Hu", "Jiarui Tong", "Sheng Xu"], "title": "A Multi-Agent Psychological Simulation System for Human Behavior Modeling", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Training and education in human-centered fields require authentic practice,\nyet realistic simulations of human behavior have remained limited. We present a\nmulti-agent psychological simulation system that models internal\ncognitive-affective processes to generate believable human behaviors. In\ncontrast to black-box neural models, this system is grounded in established\npsychological theories (e.g., self-efficacy, mindset, social constructivism)\nand explicitly simulates an ``inner parliament'' of agents corresponding to key\npsychological factors. These agents deliberate and interact to determine the\nsystem's output behavior, enabling unprecedented transparency and alignment\nwith human psychology. We describe the system's architecture and theoretical\nfoundations, illustrate its use in teacher training and research, and discuss\nhow it embodies principles of social learning, cognitive apprenticeship,\ndeliberate practice, and meta-cognition.", "AI": {"tldr": "提出基于心理学理论的多智能体心理模拟系统，通过模拟内部认知情感过程生成可信人类行为，应用于教师培训和研究", "motivation": "人类中心领域的培训需要真实实践，但现有的人类行为模拟系统缺乏真实性和理论依据", "method": "建立基于心理学理论（自我效能、思维模式、社会建构主义）的多智能体系统，模拟'内部议会'的心理因素代理进行决策互动", "result": "开发出具有前所未有的透明度和与人类心理学一致性的行为生成系统", "conclusion": "该系统体现了社会学习、认知学徒制、刻意练习和元认知原则，为教师培训和研究提供了有效的仿真工具"}}
{"id": "2511.02752", "pdf": "https://arxiv.org/pdf/2511.02752", "abs": "https://arxiv.org/abs/2511.02752", "authors": ["Amit Misra", "Syed Waqas Zamir", "Wassim Hamidouche", "Inbal Becker-Reshef", "Juan Lavista Ferres"], "title": "AI Diffusion in Low Resource Language Countries", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "9 pages, 4 tables. Also available at\n  https://aka.ms/AI_Diffusion_Low_Resource_Language_Countries", "summary": "Artificial intelligence (AI) is diffusing globally at unprecedented speed,\nbut adoption remains uneven. Frontier Large Language Models (LLMs) are known to\nperform poorly on low-resource languages due to data scarcity. We hypothesize\nthat this performance deficit reduces the utility of AI, thereby slowing\nadoption in Low-Resource Language Countries (LRLCs). To test this, we use a\nweighted regression model to isolate the language effect from socioeconomic and\ndemographic factors, finding that LRLCs have a share of AI users that is\napproximately 20% lower relative to their baseline. These results indicate that\nlinguistic accessibility is a significant, independent barrier to equitable AI\ndiffusion.", "AI": {"tldr": "研究发现语言资源匮乏国家因AI模型在低资源语言上表现不佳，导致AI用户比例比基准低约20%，表明语言可及性是AI公平扩散的重要障碍", "motivation": "人工智能在全球快速扩散但采用不均衡，前沿大语言模型在低资源语言上表现差，假设这会降低AI效用并减缓低资源语言国家的采用速度", "method": "使用加权回归模型从社会经济和人口因素中分离出语言效应", "result": "低资源语言国家的AI用户比例比基准低约20%", "conclusion": "语言可及性是AI公平扩散的一个重要且独立的障碍"}}
{"id": "2511.02627", "pdf": "https://arxiv.org/pdf/2511.02627", "abs": "https://arxiv.org/abs/2511.02627", "authors": ["Lachlan McPheat", "Navdeep Kaur", "Robert Blackwell", "Alessandra Russo", "Anthony G. Cohn", "Pranava Madhyastha"], "title": "DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning", "categories": ["cs.AI"], "comment": null, "summary": "We introduce DecompSR, decomposed spatial reasoning, a large benchmark\ndataset (over 5m datapoints) and generation framework designed to analyse\ncompositional spatial reasoning ability. The generation of DecompSR allows\nusers to independently vary several aspects of compositionality, namely:\nproductivity (reasoning depth), substitutivity (entity and linguistic\nvariability), overgeneralisation (input order, distractors) and systematicity\n(novel linguistic elements). DecompSR is built procedurally in a manner which\nmakes it is correct by construction, which is independently verified using a\nsymbolic solver to guarantee the correctness of the dataset. DecompSR is\ncomprehensively benchmarked across a host of Large Language Models (LLMs) where\nwe show that LLMs struggle with productive and systematic generalisation in\nspatial reasoning tasks whereas they are more robust to linguistic variation.\nDecompSR provides a provably correct and rigorous benchmarking dataset with a\nnovel ability to independently vary the degrees of several key aspects of\ncompositionality, allowing for robust and fine-grained probing of the\ncompositional reasoning abilities of LLMs.", "AI": {"tldr": "DecompSR是一个包含500多万数据点的大规模空间推理基准数据集和生成框架，用于分析组合空间推理能力，通过独立控制组合性的多个维度来评估大语言模型的性能。", "motivation": "需要系统性地分析和评估大语言模型在组合空间推理任务中的能力，特别是对生产力、可替代性、过度泛化和系统性等组合性关键方面的处理能力。", "method": "采用程序化构建方法生成数据集，确保构造正确性，并使用符号求解器独立验证数据集的正确性。通过独立控制组合性的多个维度（推理深度、实体和语言变异性、输入顺序和干扰项、新语言元素）来构建数据集。", "result": "实验表明，大语言模型在空间推理任务中难以处理生产性和系统性泛化，但对语言变异表现出更强的鲁棒性。", "conclusion": "DecompSR提供了一个可证明正确且严谨的基准测试数据集，能够独立控制组合性的多个关键维度，为大语言模型的组合推理能力提供了细粒度和鲁棒的评估工具。"}}
{"id": "2511.02755", "pdf": "https://arxiv.org/pdf/2511.02755", "abs": "https://arxiv.org/abs/2511.02755", "authors": ["Bowen Jin", "TJ Collins", "Donghan Yu", "Mert Cemri", "Shenao Zhang", "Mengyu Li", "Jay Tang", "Tian Qin", "Zhiyang Xu", "Jiarui Lu", "Guoli Yin", "Jiawei Han", "Zirui Wang"], "title": "Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning", "categories": ["cs.CL"], "comment": "14 pages", "summary": "Large language models (LLMs) exhibit complementary strengths across domains\nand come with varying inference costs, motivating the design of multi-agent LLM\nsystems where specialized models collaborate efficiently. Existing approaches\npredominantly rely on decentralized frameworks, which invoke multiple LLMs for\nevery input and thus lead to substantial and uncontrolled inference costs. In\nthis work, we introduce a centralized multi-LLM framework, where a controller\nLLM selectively coordinates a pool of expert models in a cost-efficient and\ncost-controllable manner. We formulate this coordination problem as\nreinforcement learning with dual objectives: maximizing task performance while\nminimizing the overall inference cost. In addition, we expect the multi-agent\nsystem to have adapted behavior with different budget conditions during\ninference. To this end, we propose CoRL, a reinforcement learning framework\nthat optimizes the performance cost trade-off in a controllable multi-budget\nsetting. Experiments on four diverse benchmarks demonstrate that CoRL enables a\nsingle system to surpass the best expert LLM under high-budget settings, while\nmaintaining strong performance in more economical low-budget modes,\nhighlighting the effectiveness of centralized coordination for scalable and\ncost-efficient multi-agent LLM systems.", "AI": {"tldr": "该论文提出了CoRL框架，通过强化学习实现集中式多LLM系统的成本控制协调，在高预算下超越最佳专家模型，在低预算下保持良好性能。", "motivation": "现有去中心化多LLM系统为每个输入调用多个模型，导致推理成本高昂且不可控，需要设计成本高效且可控的多智能体LLM系统。", "method": "提出集中式框架，使用控制器LLM选择性协调专家模型池，通过强化学习框架CoRL优化性能与成本的权衡，支持多预算设置。", "result": "在四个基准测试中，CoRL系统在高预算设置下超越最佳专家LLM，在低预算模式下仍保持强劲性能。", "conclusion": "集中式协调为可扩展和成本高效的多智能体LLM系统提供了有效解决方案，实现了性能与成本的优化平衡。"}}
{"id": "2511.02687", "pdf": "https://arxiv.org/pdf/2511.02687", "abs": "https://arxiv.org/abs/2511.02687", "authors": ["Tim R. Davidson", "Adam Fourney", "Saleema Amershi", "Robert West", "Eric Horvitz", "Ece Kamar"], "title": "The Collaboration Gap", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration.", "AI": {"tldr": "论文提出了一个协作迷宫求解基准测试，评估了32个主流AI模型在协作任务中的表现，发现存在明显的'协作鸿沟'：单独表现良好的模型在协作时性能大幅下降，并提出了'接力推理'等改进方法。", "motivation": "随着AI发展，我们将越来越多地依赖由不同信息、权限和工具的独立开发智能体组成的系统，但这些系统的成功关键取决于异构智能体间的有效协作，而目前缺乏大规模实证研究来评估这种协作能力。", "method": "设计了一个协作迷宫求解基准测试框架，该框架能够：(i)隔离协作能力，(ii)调节问题复杂度，(iii)实现可扩展的自动评分，(iv)保持生态合理性。使用该框架评估了32个领先的开源和闭源模型在单独、同质配对和异质配对三种场景下的表现。", "result": "发现了明显的'协作鸿沟'：单独表现良好的模型在需要协作时性能显著下降；协作可能完全崩溃，例如某些单独能很好解决迷宫的小型蒸馏模型在某些配对中几乎完全失败；发现从较强智能体开始通常能改善结果，提出了'接力推理'方法。", "conclusion": "研究结果主张：(1)协作感知的评估方法，(2)开发增强协作能力的训练策略，(3)设计能够可靠激发智能体潜在技能的交互设计，这些指导适用于AI-AI协作和人类-AI协作。"}}
{"id": "2511.02770", "pdf": "https://arxiv.org/pdf/2511.02770", "abs": "https://arxiv.org/abs/2511.02770", "authors": ["Hung-Ting Chen", "Xiang Liu", "Shauli Ravfogel", "Eunsol Choi"], "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Most text retrievers generate \\emph{one} query vector to retrieve relevant\ndocuments. Yet, the conditional distribution of relevant documents for the\nquery may be multimodal, e.g., representing different interpretations of the\nquery. We first quantify the limitations of existing retrievers. All retrievers\nwe evaluate struggle more as the distance between target document embeddings\ngrows. To address this limitation, we develop a new retriever architecture,\n\\emph{A}utoregressive \\emph{M}ulti-\\emph{E}mbedding \\emph{R}etriever (AMER).\nOur model autoregressively generates multiple query vectors, and all the\npredicted query vectors are used to retrieve documents from the corpus. We show\nthat on the synthetic vectorized data, the proposed method could capture\nmultiple target distributions perfectly, showing 4x better performance than\nsingle embedding model. We also fine-tune our model on real-world multi-answer\nretrieval datasets and evaluate in-domain. AMER presents 4 and 21\\% relative\ngains over single-embedding baselines on two datasets we evaluate on.\nFurthermore, we consistently observe larger gains on the subset of dataset\nwhere the embeddings of the target documents are less similar to each other. We\ndemonstrate the potential of using a multi-query vector retriever and open up a\nnew direction for future work.", "AI": {"tldr": "AMER模型通过自回归生成多个查询向量来解决传统检索器单向量无法捕捉多模态相关文档分布的问题，在合成数据和真实数据集上分别实现了4倍和4-21%的性能提升。", "motivation": "现有文本检索器只生成单个查询向量，但查询的相关文档分布可能是多模态的（如查询的不同解释），这导致在目标文档嵌入距离较大时检索性能下降。", "method": "提出自回归多嵌入检索器（AMER），通过自回归方式生成多个查询向量，所有生成的查询向量都用于从语料库中检索文档。", "result": "在合成向量化数据上完美捕捉多目标分布，性能比单嵌入模型提升4倍；在真实多答案检索数据集上相对单嵌入基线分别获得4%和21%的相对增益；在目标文档嵌入相似度较低的子集上增益更大。", "conclusion": "多查询向量检索器具有巨大潜力，为未来研究开辟了新方向，特别是在处理多模态相关文档分布的场景中。"}}
{"id": "2511.02734", "pdf": "https://arxiv.org/pdf/2511.02734", "abs": "https://arxiv.org/abs/2511.02734", "authors": ["Jiayu Liu", "Cheng Qian", "Zhaochen Su", "Qing Zong", "Shijue Huang", "Bingxiang He", "Yi R. Fung"], "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust.", "AI": {"tldr": "CostBench是一个专注于成本效益的基准测试，用于评估LLM代理的经济推理和重新规划能力，发现在静态和动态环境下当前代理在成本最优规划方面存在显著不足。", "motivation": "当前LLM代理评估主要关注任务完成度，忽视了资源效率和适应性这一关键能力，特别是代理在变化环境中制定和调整成本最优计划的能力。", "method": "在旅行规划领域构建CostBench基准，包含可通过多种原子和复合工具序列解决的任务，支持四种动态阻塞事件（如工具故障和成本变化）来模拟现实世界的不确定性。", "result": "评估显示当前代理在成本感知规划方面存在巨大差距：在静态设置中经常无法找到成本最优解（GPT-5在最难任务上精确匹配率低于75%），动态条件下性能进一步下降约40%。", "conclusion": "CostBench通过诊断这些弱点，为开发既经济理性又鲁棒的未来代理奠定了基础。"}}
{"id": "2511.02805", "pdf": "https://arxiv.org/pdf/2511.02805", "abs": "https://arxiv.org/abs/2511.02805", "authors": ["Qianhao Yuan", "Jie Lou", "Zichao Li", "Jiawei Chen", "Yaojie Lu", "Hongyu Lin", "Le Sun", "Debing Zhang", "Xianpei Han"], "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Project page: https://github.com/icip-cas/MemSearcher", "summary": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher", "AI": {"tldr": "MemSearcher提出了一种通过迭代维护紧凑记忆来优化多轮搜索代理的方法，在保持信息完整性的同时显著降低计算和内存成本，并在多个基准测试中取得了显著性能提升。", "motivation": "传统搜索代理要么使用完整历史记录导致计算成本高昂，要么仅使用当前轮次导致信息丢失，这种权衡限制了搜索代理的可扩展性。", "method": "提出MemSearcher工作流，通过迭代维护紧凑记忆，将当前轮次问题与记忆融合生成推理轨迹、执行搜索操作并更新记忆。采用多上下文GRPO强化学习框架联合优化推理、搜索策略和记忆管理。", "result": "在七个公共基准测试中相比强基线显著提升：Qwen2.5-3B-Instruct平均提升11%，Qwen2.5-7B-Instruct平均提升12%。3B版本的MemSearcher甚至超越7B基线模型。", "conclusion": "在信息完整性和效率之间取得平衡既能提高准确性又能降低计算开销，MemSearcher为解决多轮搜索代理的可扩展性问题提供了有效解决方案。"}}
{"id": "2511.02749", "pdf": "https://arxiv.org/pdf/2511.02749", "abs": "https://arxiv.org/abs/2511.02749", "authors": ["Paul Castro", "Nick Mitchell", "Nathan Ordonez", "Thomas Parnell", "Mudhakar Srivatsa", "Antoni Viros i Martin"], "title": "Using Span Queries to Optimize for Cache and Attention Locality", "categories": ["cs.AI"], "comment": "12 pages, 17 figures", "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.", "AI": {"tldr": "本文提出span query概念，通过表达式树和交换性约束来统一表达聊天、RAG、推理时间扩展和代理工作负载等多种推理场景，显著提升KV缓存命中率和推理性能", "motivation": "当前推理服务器主要针对聊天完成优化，但客户端需求已扩展到多种推理场景。现有解决方案通常只针对单一用例（如RAG），缺乏通用接口", "method": "引入span query概念，构建推理调用的表达式树，通过交换性约束连接。修改vLLM（仅492行代码）实现高性能执行，并展示如何自动优化KV缓存局部性和注意力局部性", "result": "在两种非聊天用例中实现TTFT降低10-20倍；注意力优化的span query在2b参数模型上性能超越标准推理服务器的8b模型", "conclusion": "Span query提供了一种通用接口，能够有效支持多种推理工作负载，显著提升推理效率和性能，解决了传统推理服务器在非聊天场景下的局限性"}}
{"id": "2511.02817", "pdf": "https://arxiv.org/pdf/2511.02817", "abs": "https://arxiv.org/abs/2511.02817", "authors": ["Amanda Bertsch", "Adithya Pratapa", "Teruko Mitamura", "Graham Neubig", "Matthew R. Gormley"], "title": "Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "As model context lengths continue to grow, concerns about whether models\neffectively use the full context length have persisted. While several carefully\ndesigned long-context evaluations have recently been released, these\nevaluations tend to rely on retrieval from one or more sections of the context,\nwhich allows nearly all of the context tokens to be disregarded as noise. This\nrepresents only one type of task that might be performed with long context. We\nintroduce Oolong, a benchmark of long-context reasoning tasks that require\nanalyzing individual chunks of text on an atomic level, and then aggregating\nthese analyses to answer distributional questions. Oolong is separated into two\ntask sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can\neasily ablate components of the reasoning problem; and Oolong-real, a\ndownstream setting which requires reasoning over real-world conversational\ndata. Oolong requires models to reason over large quantities of examples, to\nperform both classification and counting in-context, and to reason over\ntemporal and user relations. Even frontier models struggle on Oolong, with\nGPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy\non both splits at 128K. We release the data and evaluation harness for Oolong\nto enable further development of models that can reason over large quantities\nof text.", "AI": {"tldr": "Oolong是一个新的长上下文推理基准测试，包含合成和真实世界任务，要求模型对大量文本进行原子级分析并聚合结果，前沿模型在128K上下文长度下准确率不足50%。", "motivation": "现有长上下文评估主要关注检索任务，允许模型忽略大部分上下文作为噪声，无法全面评估模型的长上下文推理能力。", "method": "开发Oolong基准测试，包含两个任务集：Oolong-synth（自然合成任务）和Oolong-real（真实对话数据推理），要求模型进行文本块原子分析、分类、计数以及时空和用户关系推理。", "result": "即使是GPT-5、Claude-Sonnet-4和Gemini-2.5-Pro等前沿模型，在128K上下文长度下对两个任务集的准确率都低于50%。", "conclusion": "当前模型在需要深度分析和聚合推理的长上下文任务上表现不佳，Oolong基准的发布将促进能够处理大量文本推理的模型发展。"}}
{"id": "2511.02759", "pdf": "https://arxiv.org/pdf/2511.02759", "abs": "https://arxiv.org/abs/2511.02759", "authors": ["Julius Fiedler", "Carsten Knoll", "Klaus Röbenack"], "title": "LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "4 pages, 2 figures", "summary": "The rapid growth of research output in control engineering calls for new\napproaches to structure and formalize domain knowledge. This paper briefly\ndescribes an LLM-supported method for semi-automated generation of formal\nknowledge representations that combine human readability with machine\ninterpretability and increased expressiveness. Based on the Imperative\nRepresentation of Knowledge (PyIRK) framework, we demonstrate how language\nmodels can assist in transforming natural-language descriptions and\nmathematical definitions (available as LaTeX source code) into a formalized\nknowledge graph. As a first application we present the generation of an\n``interactive semantic layer'' to enhance the source documents in order to\nfacilitate knowledge transfer. From our perspective this contributes to the\nvision of easily accessible, collaborative, and verifiable knowledge bases for\nthe control engineering domain.", "AI": {"tldr": "本文提出了一种基于大语言模型的半自动化方法，用于将控制工程领域的自然语言描述和数学定义转换为形式化知识图谱，以增强文档的语义交互层。", "motivation": "控制工程研究产出的快速增长需要新的方法来结构化和形式化领域知识，以实现易于访问、协作和可验证的知识库。", "method": "基于Imperative Representation of Knowledge (PyIRK)框架，利用语言模型辅助将自然语言描述和LaTeX数学定义转换为形式化知识图谱。", "result": "开发了交互式语义层来增强源文档，促进知识传递。", "conclusion": "该方法为实现控制工程领域易于访问、协作和可验证的知识库愿景做出了贡献。"}}
{"id": "2511.02794", "pdf": "https://arxiv.org/pdf/2511.02794", "abs": "https://arxiv.org/abs/2511.02794", "authors": ["Chenyu Zhang", "Minsol Kim", "Shohreh Ghorbani", "Jingyao Wu", "Rosalind Picard", "Patricia Maes", "Paul Pu Liang"], "title": "When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning", "categories": ["cs.AI", "cs.MA"], "comment": "Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop,\n  NeurIPS 2025", "summary": "Despite rapid growth in multimodal large language models (MLLMs), their\nreasoning traces remain opaque: it is often unclear which modality drives a\nprediction, how conflicts are resolved, or when one stream dominates. In this\npaper, we introduce modality sabotage, a diagnostic failure mode in which a\nhigh-confidence unimodal error overrides other evidence and misleads the fused\nresult. To analyze such dynamics, we propose a lightweight, model-agnostic\nevaluation layer that treats each modality as an agent, producing candidate\nlabels and a brief self-assessment used for auditing. A simple fusion mechanism\naggregates these outputs, exposing contributors (modalities supporting correct\noutcomes) and saboteurs (modalities that mislead). Applying our diagnostic\nlayer in a case study on multimodal emotion recognition benchmarks with\nfoundation models revealed systematic reliability profiles, providing insight\ninto whether failures may arise from dataset artifacts or model limitations.\nMore broadly, our framework offers a diagnostic scaffold for multimodal\nreasoning, supporting principled auditing of fusion dynamics and informing\npossible interventions.", "AI": {"tldr": "论文提出了一种名为\"模态破坏\"的诊断方法，用于分析多模态大语言模型中各模态如何影响预测结果，通过将每个模态视为独立代理进行评估，识别哪些模态支持正确结果，哪些会误导模型。", "motivation": "多模态大语言模型的推理过程不透明，难以理解哪个模态主导预测、冲突如何解决，以及何时某个模态会主导结果。需要一种方法来诊断模型中的故障模式。", "method": "提出轻量级、模型无关的评估层，将每个模态视为代理，生成候选标签和简短自评估。通过简单的融合机制聚合输出，识别贡献者（支持正确结果的模态）和破坏者（误导的模态）。", "result": "在多模态情感识别基准测试中应用该方法，揭示了系统的可靠性特征，能够区分故障是来自数据集伪影还是模型限制。", "conclusion": "该框架为多模态推理提供了诊断支架，支持对融合动态的原则性审计，并为可能的干预措施提供信息。"}}
{"id": "2511.02818", "pdf": "https://arxiv.org/pdf/2511.02818", "abs": "https://arxiv.org/abs/2511.02818", "authors": ["Mohamed Bouadi", "Pratinav Seth", "Aditya Tanna", "Vinay Kumar Sankarapu"], "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Tabular data remain the predominant format for real-world applications. Yet,\ndeveloping effective neural models for tabular data remains challenging due to\nheterogeneous feature types and complex interactions occurring at multiple\nscales. Recent advances in tabular in-context learning (ICL), such as TabPFN\nand TabICL, have achieved state-of-the-art performance comparable to\ngradient-boosted trees (GBTs) without task-specific fine-tuning. However,\ncurrent architectures exhibit key limitations: (1) single-scale feature\nprocessing that overlooks hierarchical dependencies, (2) dense attention with\nquadratic scaling in table width, and (3) strictly sequential component\nprocessing that prevents iterative representation refinement and\ncross-component communication. To address these challenges, we introduce\nOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)\nmulti-scale processing to capture hierarchical feature interactions; (2)\nblock-sparse attention combining windowed, global, and random patterns for\nscalable efficiency and long-range connectivity; and (3) a Perceiver-style\nmemory enabling safe bidirectional information flow across components. Across\ndiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance\nwhile scaling effectively to high-dimensional tables, establishing a new\nstandard for efficient tabular in-context learning. The model is publicly\navailable at https://github.com/Lexsi-Labs/Orion-MSP .", "AI": {"tldr": "Orion-MSP是一个新的表格数据上下文学习架构，通过多尺度处理、块稀疏注意力和感知器式内存解决了现有方法的局限性，在多个基准测试中达到或超越了最先进性能。", "motivation": "现有表格上下文学习方法存在单尺度特征处理、二次复杂度密集注意力和严格顺序组件处理等局限性，无法有效处理异构特征类型和多尺度复杂交互。", "method": "提出Orion-MSP架构，包含三个关键创新：多尺度处理捕获层次特征交互；块稀疏注意力结合窗口化、全局和随机模式实现可扩展效率；感知器式内存实现组件间双向信息流。", "result": "在多样化基准测试中，Orion-MSP匹配或超越了最先进性能，并能有效扩展到高维表格数据。", "conclusion": "Orion-MSP为高效的表格上下文学习设立了新标准，解决了现有架构的关键限制，实现了更好的性能和可扩展性。"}}
{"id": "2511.02823", "pdf": "https://arxiv.org/pdf/2511.02823", "abs": "https://arxiv.org/abs/2511.02823", "authors": ["Chloe Loughridge", "Paul Colognese", "Avery Griffin", "Tyler Tracy", "Jon Kutasov", "Joe Benton"], "title": "Optimizing AI Agent Attacks With Synthetic Data", "categories": ["cs.AI"], "comment": null, "summary": "As AI deployments become more complex and high-stakes, it becomes\nincreasingly important to be able to estimate their risk. AI control is one\nframework for doing so. However, good control evaluations require eliciting\nstrong attack policies. This can be challenging in complex agentic environments\nwhere compute constraints leave us data-poor. In this work, we show how to\noptimize attack policies in SHADE-Arena, a dataset of diverse realistic control\nenvironments. We do this by decomposing attack capability into five constituent\nskills -- suspicion modeling, attack selection, plan synthesis, execution and\nsubtlety -- and optimizing each component individually. To get around the\nconstraint of limited data, we develop a probabilistic model of attack\ndynamics, optimize our attack hyperparameters using this simulation, and then\nshow that the results transfer to SHADE-Arena. This results in a substantial\nimprovement in attack strength, reducing safety score from a baseline of 0.87\nto 0.41 using our scaffold.", "AI": {"tldr": "该研究通过分解攻击能力为五个核心技能并开发概率模型来优化AI控制评估中的攻击策略，在数据有限的复杂环境中显著提升了攻击强度", "motivation": "随着AI部署变得复杂且高风险，需要准确评估风险。AI控制框架需要强大的攻击策略，但在计算受限的复杂智能体环境中获取数据困难", "method": "将攻击能力分解为五个构成技能（怀疑建模、攻击选择、计划合成、执行和隐蔽性），开发攻击动态概率模型，在模拟中优化超参数，然后转移到SHADE-Arena数据集", "result": "攻击强度显著提升，安全分数从基线0.87降低到0.41", "conclusion": "通过技能分解和概率建模方法可以有效优化攻击策略，即使在数据有限的环境中也能显著改善AI控制评估的效果"}}
{"id": "2511.02824", "pdf": "https://arxiv.org/pdf/2511.02824", "abs": "https://arxiv.org/abs/2511.02824", "authors": ["Ludovico Mitchener", "Angela Yiu", "Benjamin Chang", "Mathieu Bourdenx", "Tyler Nadolski", "Arvis Sulovari", "Eric C. Landsness", "Daniel L. Barabasi", "Siddharth Narayanan", "Nicky Evans", "Shriya Reddy", "Martha Foiani", "Aizad Kamal", "Leah P. Shriver", "Fang Cao", "Asmamaw T. Wassie", "Jon M. Laurent", "Edwin Melville-Green", "Mayk Caldas", "Albert Bou", "Kaleigh F. Roberts", "Sladjana Zagorac", "Timothy C. Orr", "Miranda E. Orr", "Kevin J. Zwezdaryk", "Ali E. Ghareeb", "Laurie McCoy", "Bruna Gomes", "Euan A. Ashley", "Karen E. Duff", "Tonio Buonassisi", "Tom Rainforth", "Randall J. Bateman", "Michael Skarlinski", "Samuel G. Rodriques", "Michaela M. Hinks", "Andrew D. White"], "title": "Kosmos: An AI Scientist for Autonomous Discovery", "categories": ["cs.AI"], "comment": null, "summary": "Data-driven scientific discovery requires iterative cycles of literature\nsearch, hypothesis generation, and data analysis. Substantial progress has been\nmade towards AI agents that can automate scientific research, but all such\nagents remain limited in the number of actions they can take before losing\ncoherence, thus limiting the depth of their findings. Here we present Kosmos,\nan AI scientist that automates data-driven discovery. Given an open-ended\nobjective and a dataset, Kosmos runs for up to 12 hours performing cycles of\nparallel data analysis, literature search, and hypothesis generation before\nsynthesizing discoveries into scientific reports. Unlike prior systems, Kosmos\nuses a structured world model to share information between a data analysis\nagent and a literature search agent. The world model enables Kosmos to\ncoherently pursue the specified objective over 200 agent rollouts, collectively\nexecuting an average of 42,000 lines of code and reading 1,500 papers per run.\nKosmos cites all statements in its reports with code or primary literature,\nensuring its reasoning is traceable. Independent scientists found 79.4% of\nstatements in Kosmos reports to be accurate, and collaborators reported that a\nsingle 20-cycle Kosmos run performed the equivalent of 6 months of their own\nresearch time on average. Furthermore, collaborators reported that the number\nof valuable scientific findings generated scales linearly with Kosmos cycles\n(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that\nspan metabolomics, materials science, neuroscience, and statistical genetics.\nThree discoveries independently reproduce findings from preprinted or\nunpublished manuscripts that were not accessed by Kosmos at runtime, while four\nmake novel contributions to the scientific literature.", "AI": {"tldr": "Kosmos是一个能够自动化数据驱动科学发现的AI科学家系统，通过结构化世界模型连接数据分析与文献搜索代理，可在12小时内执行多达200次操作，生成可追溯的科学报告，其产出相当于人类研究者6个月的工作量。", "motivation": "现有AI科研代理在执行多次操作后会失去连贯性，限制了科学发现的深度。需要开发能够维持长期连贯性、自动化科研流程的AI系统。", "method": "使用结构化世界模型在数据分析代理和文献搜索代理之间共享信息，进行并行的数据分析、文献搜索和假设生成循环，最终合成科学报告。每个运行平均执行42,000行代码和阅读1,500篇论文。", "result": "79.4%的报告陈述被独立科学家确认为准确，单次20循环运行相当于人类6个月的研究工作。产生了7个跨学科发现，其中3个独立重现了未发表的发现，4个是全新贡献。", "conclusion": "Kosmos展示了AI系统能够进行长期连贯的科学研究，产出可验证的发现，且科学发现的价值与运行周期呈线性增长关系，为自动化科学发现提供了可行路径。"}}
{"id": "2511.02825", "pdf": "https://arxiv.org/pdf/2511.02825", "abs": "https://arxiv.org/abs/2511.02825", "authors": ["Artur d'Avila Garcez", "Simon Odense"], "title": "Neurosymbolic Deep Learning Semantics", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) is a powerful new language of science as\nevidenced by recent Nobel Prizes in chemistry and physics that recognized\ncontributions to AI applied to those areas. Yet, this new language lacks\nsemantics, which makes AI's scientific discoveries unsatisfactory at best. With\nthe purpose of uncovering new facts but also improving our understanding of the\nworld, AI-based science requires formalization through a framework capable of\ntranslating insight into comprehensible scientific knowledge. In this paper, we\nargue that logic offers an adequate framework. In particular, we use logic in a\nneurosymbolic framework to offer a much needed semantics for deep learning, the\nneural network-based technology of current AI. Deep learning and neurosymbolic\nAI lack a general set of conditions to ensure that desirable properties are\nsatisfied. Instead, there is a plethora of encoding and knowledge extraction\napproaches designed for particular cases. To rectify this, we introduced a\nframework for semantic encoding, making explicit the mapping between neural\nnetworks and logic, and characterizing the common ingredients of the various\nexisting approaches. In this paper, we describe succinctly and exemplify how\nlogical semantics and neural networks are linked through this framework, we\nreview some of the most prominent approaches and techniques developed for\nneural encoding and knowledge extraction, provide a formal definition of our\nframework, and discuss some of the difficulties of identifying a semantic\nencoding in practice in light of analogous problems in the philosophy of mind.", "AI": {"tldr": "本文提出使用逻辑框架为深度学习提供语义基础，通过神经符号AI方法建立神经网络与逻辑之间的映射关系，解决AI科学发现缺乏语义理解的问题。", "motivation": "AI在科学领域取得重大成就但缺乏语义基础，导致科学发现难以被理解。需要建立框架将AI洞察转化为可理解的科学知识。", "method": "采用神经符号框架，通过逻辑语义将神经网络与逻辑连接，建立语义编码框架，统一现有的各种编码和知识提取方法。", "result": "提出了一个正式的语义编码框架，明确了神经网络与逻辑之间的映射关系，并描述了现有各种方法的共同要素。", "conclusion": "逻辑为深度学习提供了急需的语义基础，但实践中识别语义编码仍面临类似心灵哲学中的困难，需要进一步研究解决。"}}
{"id": "2511.02834", "pdf": "https://arxiv.org/pdf/2511.02834", "abs": "https://arxiv.org/abs/2511.02834", "authors": ["Huawei Lin", "Yunzhi Shi", "Tong Geng", "Weijie Zhao", "Wei Wang", "Ravender Pal Singh"], "title": "Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 7 figures, 14 tables. Under Review", "summary": "Multimodal large language models (MLLMs) have shown strong capabilities but\nremain limited to fixed modality pairs and require costly fine-tuning with\nlarge aligned datasets. Building fully omni-capable models that can integrate\ntext, images, audio, and video remains impractical and lacks robust reasoning\nsupport. In this paper, we propose an Agent-Omni framework that coordinates\nexisting foundation models through a master-agent system, enabling flexible\nmultimodal reasoning without retraining. The master agent interprets user\nintent, delegates subtasks to modality-specific agents, and integrates their\noutputs into coherent responses. Extensive experiments across text, image,\naudio, video, and omni benchmarks show that Agent-Omni consistently achieves\nstate-of-the-art performance, particularly on tasks requiring complex\ncross-modal reasoning. Its agent-based design enables seamless integration of\nspecialized foundation models, ensuring adaptability to diverse inputs while\nmaintaining transparency and interpretability. In addition, the framework is\nmodular and easily extensible, allowing future improvements as stronger models\nbecome available. %We release an open-source implementation to support\ncontinued research on scalable and reliable omni-modal reasoning.", "AI": {"tldr": "Agent-Omni框架通过主代理系统协调现有基础模型，实现无需重新训练的多模态推理，在文本、图像、音频、视频等多种模态任务上达到最先进性能。", "motivation": "现有的多模态大语言模型局限于固定模态对，需要大量对齐数据微调，缺乏全模态能力和稳健推理支持。", "method": "采用主代理系统，主代理解析用户意图，将子任务委派给特定模态代理，整合各代理输出形成连贯响应。", "result": "在多种模态基准测试中一致达到最先进性能，特别是在需要复杂跨模态推理的任务上表现优异。", "conclusion": "该框架具有模块化和易扩展性，能够无缝集成专业化基础模型，保持透明度和可解释性，为可扩展可靠的全模态推理研究提供支持。"}}
