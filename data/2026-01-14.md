<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 61]
- [cs.CL](#cs.CL) [总数: 72]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin, Nusrat Shirmin, Suraiya Shabnam Bristy*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种混合可解释AI框架，结合模糊逻辑和SHAP解释，用于孕产妇健康风险预测，在孟加拉国临床验证显示能提升医生信任度。


<details>
  <summary>更多</summary>
  
**动机:** 解决机器学习在资源受限医疗环境中应用时缺乏可解释性和信任度的关键障碍。

**方法:** 开发模糊-XGBoost混合模型，使用1,014份孕产妇健康记录，结合前向模糊逻辑和后向SHAP解释，并通过14名医疗专业人员进行系统验证。

**结果:** 模型准确率达88.67%（ROC-AUC: 0.9703），71.4%的医生偏好混合解释方法，54.8%表示信任临床使用，SHAP分析识别医疗可及性为主要预测因子。

**结论:** 结合可解释模糊规则和特征重要性解释能同时提升实用性和信任度，为孕产妇医疗中的XAI部署提供实践见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+the+Trust+Gap%3A+Clinician-Validated+Hybrid+Explainable+AI+for+Maternal+Health+Risk+Assessment+in+Bangladesh，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07866，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07866&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [2] [Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling](https://arxiv.org/abs/2601.07964)
*Alexander Boldachev*

**主要类别:** cs.AI

**AI概要:** 论文提出使用可执行本体论（EO）通过boldsea框架实现游戏开发的范式转变，从算法行为编程转向语义世界建模，让智能体行为从声明式领域规则中自然涌现，而非显式编码。


<details>
  <summary>更多</summary>
  
**动机:** 解决游戏AI架构中的语义-过程鸿沟，传统方法如行为树和目标导向行动规划主要关注智能体应该做什么，而EO关注动作何时成为可能，实现更自然的优先级任务中断。

**方法:** 使用boldsea框架实现可执行本体论，通过生存游戏场景（Winter Feast）进行验证，利用数据流条件而非显式抢占逻辑实现优先级任务中断，并与行为树和目标导向行动规划进行对比分析。

**结果:** EO能够通过声明式规则实现自然的任务中断和行为涌现，相比传统方法更好地解决了语义-过程鸿沟问题，同时具有时间事件图调试优势和LLM驱动的运行时模型生成潜力。

**结论:** 可执行本体论代表了游戏AI开发的重要范式转变，从显式编程转向语义建模，为更智能、更自然的游戏智能体行为提供了新的技术路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Executable+Ontologies+in+Game+Development%3A+From+Algorithmic+Control+to+Semantic+World+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07964，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07964&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.

</details>


### [3] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin, Rui Wu, Tong Che, Qixin Zhang, Hongwu Peng, Jiahui Zhao, Zhenting Wang, Wenqi Wei, Ligong Han, Zhao Zhang, Yuan Cao, Ruixiang Tang, Dimitris N. Metaxas*

**主要类别:** cs.AI

**AI概要:** CADA方法通过案例增强的审慎对齐，使用强化学习训练自生成的安全推理链，在提高LLM安全性的同时保持实用性，避免了仅依赖规则的方法导致的帮助性下降问题。


<details>
  <summary>更多</summary>
  
**动机:** 解决大型语言模型在遵循安全原则时拒绝良性请求的问题，研究开源LLMs中详细安全规则与案例演示方法的效果差异

**方法:** 提出CADA方法，使用强化学习在自生成的安全推理链上进行训练，结合案例增强的简单代码而非详细的规则枚举

**结果:** CADA有效提高了无害性，增强了对抗攻击的鲁棒性，减少了过度拒绝，同时在多样化基准测试中保持了实用性

**结论:** 案例增强的审慎对齐方法比仅依赖规则的方法更有效，能够在提高安全性的同时保持模型的帮助性，为LLM安全对齐提供了实用替代方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+over+Precedents+Alongside+Statutes%3A+Case-Augmented+Deliberative+Alignment+for+LLM+Safety，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08000，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08000&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [4] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon, Stephen Casper*

**主要类别:** cs.AI

**AI概要:** 本文分析2025年美欧前沿AI监管在内部部署系统方面存在的三大漏洞：范围模糊性、静态合规评估和信息不对称，探讨其成因并提出应对策略。


<details>
  <summary>更多</summary>
  
**动机:** 前沿AI监管主要关注外部用户部署的系统，但企业内部部署的高风险AI应用（如研发自动化、关键业务流程处理）可能逃避监管，存在重大风险。

**方法:** 通过分析美国和欧盟的前沿AI监管框架，识别内部部署系统的监管漏洞，分析漏洞成因（可衡量性、激励机制、信息获取等矛盾），并提出解决方案框架。

**结果:** 识别出三个关键监管漏洞：1)范围模糊使内部系统逃避监管义务；2)静态合规评估无法捕捉系统持续演化；3)信息不对称破坏监管意识与监督。

**结论:** 需要有针对性地制定内部部署AI系统的政策选择，通过理解这些监管模式漏洞，可以更谨慎而非偶然地做出监管决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Internal+Deployment+Gaps+in+AI+Regulation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08005，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08005&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [5] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona, Ann Move Oguti, Joseph Walusimbi, Ritah Kobusingye*

**主要类别:** cs.AI

**AI概要:** SCASED系统结合物联网技术，通过面部情绪识别和自动考勤来监控课堂参与度，为教师提供实时课堂动态洞察，情绪分类准确率达89.5%。


<details>
  <summary>更多</summary>
  
**动机:** 当前智能教室技术主要关注自动考勤，对学生情绪和认知参与度关注不足，限制了教师识别学生分心并实时调整教学策略的能力。

**方法:** 使用树莓派摄像头和OpenCV进行人脸检测，采用微调MobileNetV2模型分类四种学习相关情绪状态（投入、无聊、困惑、沮丧），实现基于会话的考勤和持续情绪监测。

**结果:** 在DAiSEE数据集上的实验评估显示情绪分类准确率达到89.5%，系统能有效整合考勤数据和情绪分析。

**结论:** 整合考勤与情绪分析能为教师提供更深入的课堂动态洞察，支持更及时响应的教学实践。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Attendance+Tracking+and+Emotion+Detection+for+Enhanced+Student+Engagement+in+Smart+Classrooms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08049，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08049&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [6] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia, Rachael Shawb, Karl Mason*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种基于深度强化学习的乳牛场负荷调度框架，通过改进PPO算法结合短期预测和自适应KL散度控制，在真实农场数据上实现了比传统方法更低的电力成本和更高的电网独立性。


<details>
  <summary>更多</summary>
  
**动机:** 乳牛场是能源密集型产业，严重依赖电网电力。随着可再生能源的整合，实时平衡供需面临挑战。现有强化学习方法假设完全了解未来价格或发电信息，这在动态环境中不现实，且标准PPO变体在可变电价下训练不稳定。

**方法:** 提出了深度强化学习框架，专注于电池储能和热水负荷调度。Forecast Aware PPO采用基于小时和月份的残差校准进行短期需求预测，PID KL PPO变体使用PID控制器自适应调节KL散度以实现稳定的策略更新。

**结果:** 在真实乳牛场数据上训练，该方法比PPO降低电力成本1%，比DQN降低4.8%，比SAC降低1.5%。在电池调度方面，PPO减少电网输入13.1%，展示了可扩展性和有效性。

**结论:** 该研究提出的深度强化学习框架为现代乳牛养殖中的可持续能源管理提供了有效的解决方案，通过结合预测技术和自适应控制机制，在降低运营成本和提高电网独立性方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Forecast+Aware+Deep+Reinforcement+Learning+for+Efficient+Electricity+Load+Scheduling+in+Dairy+Farms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08052&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [7] [A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems](https://arxiv.org/abs/2601.08065)
*Samuel I. Akinwande, Sydney M. Katz, Mykel J. Kochenderfer, Clark Barrett*

**主要类别:** cs.AI

**AI概要:** 提出新的后向可达集算法，结合前向分析技术，构建神经网络控制系统验证框架


<details>
  <summary>更多</summary>
  
**动机:** 现有后向可达性方法可扩展性有限，前向可达性分析在验证神经网络反馈系统的可达-避免属性中占主导地位

**方法:** 开发新算法计算后向可达集的过近似和欠近似，并将后向算法与现有前向分析技术集成

**结果:** 创建了神经网络反馈系统的统一验证框架

**结论:** 通过结合前向和后向分析方法的优势，提供了更全面的神经网络控制系统验证解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+New+Strategy+for+Verifying+Reach-Avoid+Specifications+in+Neural+Feedback+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08065，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08065&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Forward reachability analysis is the predominant approach for verifying reach-avoid properties in neural feedback systems (dynamical systems controlled by neural networks). This dominance stems from the limited scalability of existing backward reachability methods. In this work, we introduce new algorithms that compute both over- and under-approximations of backward reachable sets for such systems. We further integrate these backward algorithms with established forward analysis techniques to yield a unified verification framework for neural feedback systems.

</details>


### [8] [Semantic Gravity Wells: Why Negative Constraints Backfire](https://arxiv.org/abs/2601.08070)
*Shailesh Rana*

**主要类别:** cs.AI

**AI概要:** 论文通过机制分析发现大语言模型在处理否定指令时存在系统性失败，揭示了语义压力和两种失败模式（启动失败和覆盖失败）是导致违反禁止词约束的根本原因。


<details>
  <summary>更多</summary>
  
**动机:** 尽管否定指令看似简单，但大语言模型经常失败，且失败条件一直未被充分理解，需要进行全面的机制研究。

**方法:** 使用语义压力量化模型生成禁止词的内在概率，通过logit lens技术进行分层分析，识别失败模式，并通过激活修补验证因果机制。

**结果:** 发现违反概率与语义压力呈紧密逻辑关系，识别出87.5%的启动失败和12.5%的覆盖失败，确定23-27层是导致约束失效的关键层。

**结论:** 否定约束设计存在根本性矛盾：提及禁止词的行为反而会激活模型生成该词，揭示了指令跟随能力的深层机制缺陷。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semantic+Gravity+Wells%3A+Why+Negative+Constraints+Backfire，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08070&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Negative constraints (instructions of the form "do not use word X") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic probability of generating the forbidden token, and demonstrate that violation probability follows a tight logistic relationship with pressure ($p=σ(-2.40+2.27\cdot P_0)$; $n=40{,}000$ samples; bootstrap $95%$ CI for slope: $[2.21,,2.33]$). Through layer-wise analysis using the logit lens technique, we establish that the suppression signal induced by negative instructions is present but systematically weaker in failures: the instruction reduces target probability by only 5.2 percentage points in failures versus 22.8 points in successes -- a $4.4\times$ asymmetry. We trace this asymmetry to two mechanistically distinct failure modes. In priming failure (87.5% of violations), the instruction's explicit mention of the forbidden word paradoxically activates rather than suppresses the target representation. In override failure (12.5%), late-layer feed-forward networks generate contributions of $+0.39$ toward the target probability -- nearly $4\times$ larger than in successes -- overwhelming earlier suppression signals. Activation patching confirms that layers 23--27 are causally responsible: replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

</details>


### [9] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian, Zhao Cao, Zheng Liu*

**主要类别:** cs.AI

**AI概要:** MemoBrain是一个用于工具增强智能体的执行记忆模型，通过构建依赖感知记忆来管理长时推理过程中的中间状态和逻辑关系，避免上下文积累问题，提升长时推理性能。


<details>
  <summary>更多</summary>
  
**动机:** 工具增强智能体框架中的复杂推理本质上是长时程的，导致推理轨迹和临时工具产物积累，超出大语言模型的有界工作上下文限制，破坏逻辑连续性和任务对齐。

**方法:** 提出MemoBrain执行记忆模型，构建依赖感知记忆来捕获关键中间状态及其逻辑关系，作为推理智能体的协同工作模块，在不阻塞执行的情况下组织推理进度，主动管理工作上下文，包括修剪无效步骤、折叠完成的子轨迹，在固定上下文预算下保持紧凑的高显著性推理主干。

**结果:** 在GAIA、WebWalker和BrowseComp-Plus等具有挑战性的长时程基准测试中评估MemoBrain，显示出相对于强基线的持续改进。

**结论:** 记忆不是辅助效率问题，而是维持长时程连贯、目标导向推理的核心组件，MemoBrain通过显式认知控制机制有效解决了长时推理中的上下文积累问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MemoBrain%3A+Executive+Memory+as+an+Agentic+Brain+for+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08079&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [10] [MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness](https://arxiv.org/abs/2601.08118)
*Ashutosh Hathidara, Julien Yu, Vaishali Senthil, Sebastian Schreiber, Anil Babu Ankisettipalli*

**主要类别:** cs.AI

**AI概要:** MIRRORBENCH是一个用于评估语言模型作为用户代理生成人类对话质量的标准化基准框架，通过模块化设计和多种评估指标来系统比较用户代理与真实用户的差异。


<details>
  <summary>更多</summary>
  
**动机:** 当前使用LLMs作为人类模拟器时，简单的"act-as-a-user"提示往往产生冗长不真实的对话，需要原则性的评估方法来衡量用户代理生成人类类似对话的能力。

**方法:** 开发了模块化的MIRRORBENCH框架，包含类型化接口、元数据驱动注册、多后端支持、缓存和可观测性。支持可插拔的用户代理、数据集、任务和指标，包括三个词汇多样性指标和三个基于LLM评判的指标。

**结果:** 在四个开放数据集上，MIRRORBENCH提供了方差感知的结果，并揭示了用户代理与真实人类用户之间的系统性差距。

**结论:** MIRRORBENCH是一个可扩展的开源基准测试框架，为评估用户代理生成人类对话质量提供了标准化方法，有助于改进LLMs作为人类模拟器的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MirrorBench%3A+An+Extensible+Framework+to+Evaluate+User-Proxy+Agents+for+Human-Likeness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08118，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08118&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive "act-as-a-user" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.

</details>


### [11] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen, Yuxuan Wang, Pan Liu, Victor L. Knoop, David Z. W. Wang, Yu Han*

**主要类别:** cs.AI

**AI概要:** 该研究分析了车祸后车辆变道行为的特征，开发了一个新的轨迹预测框架，通过显式建模让行行为来提升预测精度和碰撞风险分析能力。


<details>
  <summary>更多</summary>
  
**动机:** 车祸发生后，后续车辆需要变道绕过障碍物，但目标车道车辆可能拒绝让行，增加了碰撞风险。目前对这类车祸后变道行为的行为特征和运动模式尚不清楚。

**方法:** 从无人机视频中提取车辆轨迹构建数据集，开发基于图注意力机制的框架，显式建模让行行为作为辅助任务，指导条件变分自编码器和Transformer解码器进行轨迹预测。

**结果:** 相比强制性和自由变道，车祸后变道持续时间更长、插入速度更低、碰撞风险更高。新模型在轨迹预测误差上比基线方法提升10%以上，并提高了碰撞风险分析的可靠性。

**结论:** 提出的交互感知轨迹预测框架能有效处理车祸后变道场景，提高了预测精度和安全性分析能力，且在不同地点数据上具有良好的可迁移性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+vehicles+change+lanes+after+encountering+crashes%3A+Empirical+analysis+and+modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08125&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [12] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta, Stephen D. H. Hsu*

**主要类别:** cs.AI

**AI概要:** 提出一种在边缘设备上运行的AI伴侣记忆系统，通过活动/非活动双阶段设计，在有限计算资源下实现低延迟对话和高效记忆管理


<details>
  <summary>更多</summary>
  
**动机:** 边缘设备计算资源受限，现有AI伴侣系统无法直接部署，需要解决计算资源和延迟问题

**方法:** 采用活动/非活动双阶段记忆范式：用户活动时进行低延迟实时对话检索，非活动时进行密集的记忆提取、整合和维护

**结果:** 使用弱模型(Qwen2.5-7B-Instruct量化版)在多数指标上超越无记忆的原始LLM，性能接近GPT-3.5(16k上下文)

**结论:** 提出的双阶段记忆范式能在嵌入式硬件严格约束下最小化延迟同时保持长期个性化能力，为边缘AI伴侣系统提供可行解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embedded+AI+Companion+System+on+Edge+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08128，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08128&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [13] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav, Varad Dherange, Kumar Shivam*

**主要类别:** cs.AI

**AI概要:** Project Synapse是一个用于自主解决最后一公里配送中断的新型智能代理框架，采用分层多代理架构和LangGraph编排复杂工作流，在30个复杂中断场景数据集上验证性能


<details>
  <summary>更多</summary>
  
**动机:** 解决最后一公里配送中的中断问题，这些中断通常来自现实世界用户反馈的复杂场景

**方法:** 使用分层多代理架构：中央解析监督代理进行战略任务分解，专业工作代理负责战术执行；采用LangGraph编排复杂循环工作流；构建30个复杂中断场景基准数据集；使用LLM-as-a-Judge协议进行性能评估并明确缓解偏差

**结果:** 论文提出了一个完整的框架并建立了验证数据集和评估协议，但具体性能结果需要阅读完整论文

**结论:** Project Synapse框架为自主解决配送中断问题提供了有效的多代理解决方案，通过分层架构和系统化评估方法展示了可行性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Project+Synapse%3A+A+Hierarchical+Multi-Agent+Framework+with+Hybrid+Memory+for+Autonomous+Resolution+of+Last-Mile+Delivery+Disruptions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08156，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08156&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [14] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari*

**主要类别:** cs.AI

**AI概要:** 基于模型的分层多智能体强化学习框架，用于多核平台的温度和能量感知调度，实现零样本部署和快速决策


<details>
  <summary>更多</summary>
  
**动机:** 现有方法要么依赖忽略停顿时间的启发式方法，要么需要大量离线分析生成表格，无法实现运行时自适应

**方法:** 使用模型驱动的分层多智能体强化学习，结合LLM语义特征提取和回归技术预测热力学和性能状态，采用Dyna-Q框架整合直接强化学习和基于模型的规划

**结果:** 在多种硬件平台上实现7.09倍能效提升和4.0倍制造时间优化，首次决策延迟比基于表格的分析快8300倍，收敛速度比无模型方法快20倍

**结论:** 该框架为动态嵌入式系统提供了实用的热管理和能量性能平衡解决方案，通过零样本部署能力消除了对工作负载特定分析样本的需求

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ZeroDVFS%3A+Zero-Shot+LLM-Guided+Core+and+Frequency+Allocation+for+Embedded+Platforms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08166，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08166&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [15] [The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios](https://arxiv.org/abs/2601.08173)
*Daocheng Fu, Jianbiao Mei, Rong Wu, Xuemeng Yang, Jia Xu, Ding Wang, Pinlong Cai, Yong Liu, Licheng Wen, Botian Shi*

**主要类别:** cs.AI

**AI概要:** 该论文提出了EvoEnv动态评估环境，用于测试多模态大语言模型在动态环境中的鲁棒性，重点关注任务调度、主动探索和持续学习能力，发现现有先进模型在这些方面存在显著不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有MLLM研究主要关注静态环境下的性能上限，忽略了实际部署中的随机性和动态性挑战，需要评估模型在动态任务调度、不确定性下的主动探索和从经验中持续学习的能力。

**方法:** 引入EvoEnv动态评估环境，模拟"学员"代理在新环境中持续探索，从三个维度评估智能体：上下文感知的流式任务调度、通过主动探索减少幻觉的谨慎信息获取、以及从基于规则的动态生成任务中提炼泛化策略的持续进化。

**结果:** 实验表明，最先进的智能体在动态环境中存在显著缺陷，特别是在主动探索和持续学习方面表现不佳。

**结论:** 该工作建立了评估智能体可靠性的框架，将评估从静态测试转向更现实的生产导向场景，为MLLM在实际部署中的鲁棒性评估提供了重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Agent%27s+First+Day%3A+Benchmarking+Learning%2C+Exploration%2C+and+Scheduling+in+the+Workplace+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08173，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08173&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv

</details>


### [16] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di, Bin Lu, Huquan Kang, Luoyi Fu, Jiaxin Ding, Xiaoying Gan, Lei Zhou, Xinbing Wang, Chenghu Zhou*

**主要类别:** cs.AI

**AI概要:** HS2C框架通过利用图同质性进行结构和语义压缩，提升大语言模型在图理解任务中的性能和推理稳定性


<details>
  <summary>更多</summary>
  
**动机:** 现有方法因上下文窗口限制而随机采样节点/边，会引入噪声导致推理不稳定。图结构蕴含丰富的结构和语义信息，有效利用可提升LLMs推理性能

**方法:** 提出HS2C框架：结构上基于结构熵最小化原则进行全局分层划分，识别同质性社区；语义上让LLM基于社区类型进行差异化语义聚合，压缩冗余背景为社区级共识

**结果:** 在10个节点级基准测试中，HS2C同时提高了压缩率和下游推理准确性；在7个图级基准测试中验证了任务泛化性

**结论:** HS2C通过结构和语义压缩有效利用图同质性，显著提升LLMs在图理解任务中的性能，具有优越性和可扩展性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+LLM+Reasoning+with+Homophily-aware+Structural+and+Semantic+Text-Attributed+Graph+Compression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08187，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08187&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [17] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang, Lingfeng Li, Yunlong Lu, Wenxin Li*

**主要类别:** cs.AI

**AI概要:** 本研究使用AI自对弈数据分析国际麻将的公平性问题，发现先手优势和子目标计分设置的问题，提出补偿分机制和计分规则调整，使游戏更适合在线单轮对战环境。


<details>
  <summary>更多</summary>
  
**动机:** 在线麻将玩家游戏时间碎片化且对手不固定，与传统线下多轮固定对手的游戏模式不同，需要修改规则来确保在线单轮游戏的公平性。

**方法:** 利用世界冠军级AI进行自对弈比赛并进行统计分析，识别游戏平衡性问题。

**结果:** 发现了先手优势和子目标计分设置的问题，提出了补偿分机制和不同牌型子目标计分细化等规则调整方案。

**结论:** 通过AI数据分析评估游戏平衡性并开发了更适合在线玩家的修订版麻将游戏，补偿分机制比传统轮换位置方法更便捷。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adapting+Rules+of+Official+International+Mahjong+for+Online+Players，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08211，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08211&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [18] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon, Won-gi Paeng*

**主要类别:** cs.AI

**AI概要:** SANC(E3)是一个自组织表示学习框架，通过竞争选择、重建和压缩在有限激活容量下产生稳定的表示单元，统一了感知、想象、预测、规划和行动。


<details>
  <summary>更多</summary>
  
**动机:** 现有AI系统预设固定的基本单元（如token、像素），忽略了表示单元如何自发涌现和稳定的问题，而通用智能需要将经验重组为能够进行预测和行动的内部结构。

**方法:** 提出基于能量函数E3最小化的公理化框架，包含五个核心公理：有限容量、共现关联、相似性竞争、置信度稳定、重建-压缩-更新的权衡，采用伪内存映射I/O机制统一处理外部输入和内部重放的格式塔。

**结果:** 从公理推导出12个命题，证明类别形成、层次组织、无监督学习和高级认知活动都可以理解为E3最小化下的格式塔补全过程。

**结论:** SANC(E3)为表示单元的自发涌现提供了原则性框架，统一了各种认知过程，为构建更接近人类智能的AI系统提供了理论基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Axiomatic+Approach+to+General+Intelligence%3A+SANC%28E3%29+--+Self-organizing+Active+Network+of+Concepts+with+Energy+E3，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08224，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08224&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [19] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang, Haopeng Zhang*

**主要类别:** cs.AI

**AI概要:** MPCI-Bench是首个多模态配对上下文完整性基准，用于评估智能代理在平衡隐私与效用时的表现，发现现有模型存在系统性失败和模态泄露问题


<details>
  <summary>更多</summary>
  
**动机:** 随着语言模型代理从被动聊天机器人发展为处理个人数据的主动助手，评估其遵守社会规范变得至关重要。现有基准主要关注文本和负面拒绝场景，忽略了多模态隐私风险和隐私与效用的权衡

**方法:** 开发MPCI-Bench基准，包含从相同视觉源派生的正负配对实例，分为三个层级：规范性种子判断、上下文丰富的故事推理和可执行代理行动轨迹。通过三原则迭代精炼管道确保数据质量

**结果:** 评估最先进的多模态模型显示，它们在平衡隐私和效用方面存在系统性失败，并表现出明显的模态泄露差距（视觉敏感信息比文本信息泄露更频繁）

**结论:** MPCI-Bench将开源以促进未来关于代理上下文完整性的研究，强调了多模态隐私风险评估的重要性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MPCI-Bench%3A+A+Benchmark+for+Multimodal+Pairwise+Contextual+Integrity+Evaluation+of+Language+Model+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08235&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [20] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su, Yandong Sun, Congjia Yu*

**主要类别:** cs.AI

**AI概要:** 该论文提出从传统手工设计的数值奖励函数向基于语言的目标规范转变的新范式，利用大语言模型从自然语言描述合成奖励函数，解决多智能体强化学习中的奖励工程挑战。


<details>
  <summary>更多</summary>
  
**动机:** 多智能体强化学习中奖励工程面临信用分配模糊、环境非平稳性和交互复杂性组合增长等根本挑战，需要新的解决方案。

**方法:** 利用大语言模型(LLMs)从自然语言描述直接合成奖励函数(如EUREKA)，并在线适应奖励公式(如CARD)，结合可验证奖励的强化学习(RLVR)范式。

**结果:** 语言介导的监督可以作为传统奖励工程的可行替代方案，通过语义奖励规范、动态奖励适应和更好的人类意图对齐三个维度实现转变。

**结论:** 提出了一个研究方向：协调应该来自共享的语义表示而非显式设计的数值信号，但还需要解决计算开销、幻觉鲁棒性和大规模多智能体系统可扩展性等开放挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+End+of+Reward+Engineering%3A+How+LLMs+Are+Redefining+Multi-Agent+Coordination，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08237，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08237&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [21] [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)
*Abdikarim Mohamed Ibrahim, Rosdiadee Nordin*

**主要类别:** cs.AI

**AI概要:** 提出了一种由大型语言模型指导的深度强化学习方法(LAM-DRL)，在非地面网络应用中相比传统DRL方法在吞吐量、公平性和中断概率方面有显著提升


<details>
  <summary>更多</summary>
  
**动机:** 利用大型AI模型的强大泛化能力和减少特定任务训练的需求，提升非地面网络应用的性能

**方法:** 使用大型语言模型作为高级协调器，生成文本指导来塑造深度强化学习智能体在训练过程中的奖励函数

**结果:** LAM-DRL方法在正常天气场景下比传统DRL提升40%，在极端天气场景下提升64%，相比启发式方法在吞吐量、公平性和中断概率方面表现更好

**结论:** LLM指导的DRL方法在非地面网络中具有显著性能优势，特别是在极端条件下表现更加突出，证明了大型AI模型在网络优化中的价值

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Artificial+Intelligence+Model+Guided+Deep+Reinforcement+Learning+for+Resource+Allocation+in+Non+Terrestrial+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08254&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.

</details>


### [22] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

**主要类别:** cs.AI

**AI概要:** T3是一个诊断基准，用于评估大语言模型在Pearl因果阶梯上的因果判断能力，包含454个专家策划的情景案例，识别出模型在因果推理中的两种病理现象：L1层的"怀疑陷阱"和L3层的非单调扩展悖论


<details>
  <summary>更多</summary>
  
**动机:** 需要严格评估大语言模型在因果判断方面的能力，特别是在Pearl因果阶梯的不同层级上，以诊断模型在因果推理中的系统性缺陷

**方法:** 开发了T3基准测试，包含454个专家策划的情景案例，将性能分解为效用（敏感性）、安全性（特异性）和明智拒绝三个维度，并应用于前沿模型进行测试

**结果:** 发现两种病理现象：L1层安全调优模型拒绝60%有效链接的"怀疑陷阱"，以及L3层GPT-5.2在模糊反事实问题上比GPT-4-Turbo差55分的非单调扩展悖论，主要由过度犹豫而非幻觉导致

**结论:** T3基准成功捕获了结构化验证下决定性因果判断的恢复，验证了过程验证协议（RCA）的有效性，为评估和改进大语言模型的因果推理能力提供了重要工具

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是T3%3A+Benchmarking+Sycophancy+and+Skepticism+in+Causal+Judgment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08258，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08258&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [23] [VGG Induced Deep Hand Sign Language Detection](https://arxiv.org/abs/2601.08262)
*Subham Sharma, Sharmila Subudhi*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种基于VGG-16卷积神经网络的手势识别系统，通过迁移学习和数据增强技术，在NUS数据集上实现了约98%的准确率，专门为残障人士设计。


<details>
  <summary>更多</summary>
  
**动机:** 手势识别是人机交互的重要方面，特别为视障人士提供手语识别基础，旨在为残障人士开发有效的手势识别系统。

**方法:** 使用VGG-16卷积神经网络构建训练模型，采用Python和Keras库，在广泛使用的图像数据集上进行训练，并通过NUS数据集（包含10类手势）进行验证。使用Google开源API构建包含10类手势的测试数据集进行实验验证。

**结果:** 实验结果显示，结合迁移学习机制和图像数据增强技术，VGG-16网络达到了约98%的准确率。

**结论:** 该方法通过迁移学习和数据增强有效提升了手势识别的准确性，为残障人士提供了高效的手势识别解决方案，证明了VGG-16网络在此类任务中的优越性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VGG+Induced+Deep+Hand+Sign+Language+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08262&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.

</details>


### [24] [Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces](https://arxiv.org/abs/2601.08271)
*Angshul Majumdar*

**主要类别:** cs.AI

**AI概要:** 该论文提出了稀疏代理控制(SAC)框架，用于分析工具增强LLM系统中的顺序决策问题，建立了基于压缩感知理论的政策学习理论，证明了在稀疏条件下可实现对数级样本复杂度和精确工具支持恢复。


<details>
  <summary>更多</summary>
  
**动机:** 工具增强的LLM系统面临大规模离散动作空间中的顺序决策问题，传统学习理论对此关注不足，需要新的理论框架来分析这种稀疏代理控制场景。

**方法:** 采用ell_{1,2}-正则化政策学习，通过凸代理方法建立理论分析框架，利用Policy-RSC条件、原始-对偶见证论证等压缩感知技术。

**结果:** 证明了估计误差和价值次优性按k(log M/T)^{1/2}缩放，在T > k log M条件下可实现精确工具支持恢复，且仅需对数级样本复杂度。

**结论:** SAC框架为工具增强LLM系统提供了理论基础，解释了纯提示控制的不稳定性，并展示了在部分可观测性下LLM仅通过信念误差影响性能，保持了M的对数依赖性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparsity+Is+Necessary%3A+Polynomial-Time+Stability+for+Agentic+LLMs+in+Large+Action+Spaces，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08271&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.

</details>


### [25] [ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web](https://arxiv.org/abs/2601.08276)
*Zhiyuan Yao, Zishan Xu, Yifu Guo, Zhiguang Han, Cheng Yang, Shuo Zhang, Weinan Zhang, Xingshan Zeng, Weiwen Liu*

**主要类别:** cs.AI

**AI概要:** ToolACE-MCP是一个用于训练历史感知路由器的管道，旨在解决Agent Web和MCP生态系统中可扩展性和通用性瓶颈问题，通过依赖丰富的候选图和合成多轮轨迹来训练路由器，实现精确导航和即插即用的轻量路由代理。


<details>
  <summary>更多</summary>
  
**动机:** 随着Agent Web和Model Context Protocol (MCP)的发展，代理生态系统正在演变为开放的协作网络，可访问工具数量呈指数级增长，但当前架构面临严重的可扩展性和通用性瓶颈。

**方法:** 提出ToolACE-MCP管道，利用依赖丰富的候选图合成多轮轨迹，训练具有动态上下文理解能力的历史感知路由器，创建即插即用的轻量路由代理。

**结果:** 在真实世界基准测试MCP-Universe和MCP-Mark上表现出优越性能，能够以最小适应度泛化到多代理协作，对噪声保持出色鲁棒性，并能有效扩展到大规模候选空间。

**结论:** ToolACE-MCP为开放生态系统中的通用编排提供了强有力的实证基础，展示了未来Agent Web所需的关键特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ToolACE-MCP%3A+Generalizing+History-Aware+Routing+from+MCP+Tools+to+the+Agent+Web，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08276&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.

</details>


### [26] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

**主要类别:** cs.AI

**AI概要:** 该论文研究大规模动作空间中的稀疏动作发现问题，提出一种基于块稀疏恢复的贪婪算法，能够在多项式样本复杂度下准确识别相关动作集，为智能体系统的动作剪枝提供理论基础。


<details>
  <summary>更多</summary>
  
**动机:** 现代智能体系统面临极大的动作空间（如数千个API），但实际部署中只有少量动作真正影响性能。基于这种稀疏性观察，研究如何有效发现相关动作。

**方法:** 采用上下文线性奖励模型，假设动作相关性具有结构化稀疏性。提出受正交匹配追踪启发的贪婪算法，将动作发现建模为块稀疏恢复问题。

**结果:** 在标准假设下，证明贪婪算法能以高概率准确恢复相关动作集，样本复杂度随稀疏度和潜在维度多项式增长，仅随总动作数对数增长。同时提供参数估计误差保证和决策规则近似最优性证明。

**结论:** 稀疏动作发现是大规模动作决策的基本原理，信息理论下界证明稀疏性和充分覆盖是问题可处理性的必要条件，为智能体系统的动作剪枝提供了理论依据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Greedy+Is+Enough%3A+Sparse+Action+Discovery+in+Agentic+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08280，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08280&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [27] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu, Hanzhong Cao, Jianhao Chen, Yufei Li*

**主要类别:** cs.AI

**AI概要:** OpenMic是一个端到端的多智能体系统，能够将用户提供的生活话题转换为3-5分钟的中文单口喜剧表演，并生成带旁白的喜剧视频。


<details>
  <summary>更多</summary>
  
**动机:** 中文单口喜剧生成需要文化背景的幽默、精确的时间控制、舞台表演提示和隐式多步推理。现有的中文幽默数据集更适合幽默理解和评估，而不适合长篇单口喜剧生成，导致直接监督与目标任务不匹配。

**方法:** 基于AutoGen构建多智能体系统，采用多轮迭代循环规划来协调多个专业智能体共同优化幽默性、时间控制和可表演性。使用检索增强生成(RAG)进行材料基础和想法扩展，并微调专门的JokeWriter来内化单口喜剧特有的铺垫-笑点结构和长距离回调。

**结果:** 开发了OpenMic系统，能够生成完整的单口喜剧表演内容和视频。

**结论:** OpenMic通过多智能体协作和专门的技术方法，有效解决了中文单口喜剧生成中的文化幽默、时间控制和表演性等挑战，填补了现有数据集与目标任务之间的不匹配问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OpenMic%3A+A+Multi-Agent-Based+Stand-Up+Comedy+Generation+System，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08288，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08288&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [28] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin*

**主要类别:** cs.AI

**AI概要:** AtomMem提出了一种基于学习的动态内存管理框架，将内存操作分解为原子CRUD操作，通过监督微调和强化学习训练自主策略，在长上下文任务中优于静态工作流方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有代理内存机制依赖静态手工工作流，限制了性能和泛化能力，需要更灵活的学习型内存框架。

**方法:** 将内存管理重构为动态决策问题，分解为原子CRUD操作，结合监督微调和强化学习训练任务对齐的自主策略。

**结果:** 在3个长上下文基准测试中，AtomMem-8B持续优于先前的静态工作流内存方法，训练动态分析显示能发现结构化、任务对齐的策略。

**结论:** 学习型方法能发现优于预定义例程的内存管理策略，为代理内存设计提供了更灵活有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AtomMem+%3A+Learnable+Dynamic+Agentic+Memory+with+Atomic+Memory+Operation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08323&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [29] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk, Roman Bondar*

**主要类别:** cs.AI

**AI概要:** 论文指出LLM智能体架构存在语义漂白问题，即缺乏充分论证的命题通过可信接口被系统接受，这是盖梯尔问题的架构实现。作者证明在标准架构下循环认知证成不可避免，并提出授权侵蚀原理解释这一现象。


<details>
  <summary>更多</summary>
  
**动机:** 发现基于LLM的智能体架构系统性地混淆了信息传输机制和认知证成机制，导致命题在没有充分论证的情况下被接受，需要从架构层面分析这一问题的本质和不可避免性。

**方法:** 通过形式化分析将这类架构失败定义为语义漂白，证明其为盖梯尔问题的架构实现，并提出必然自我授权定理和授权侵蚀原理进行理论分析。

**结果:** 证明了在标准架构假设下，循环认知证成无法消除，扩展、模型改进和LLM作为评判者的方案在结构上无法解决这一类型层面的问题。

**结论:** 语义漂白是LLM智能体架构的根本缺陷，需要在架构设计层面重新思考信息传输与认知证成的关系，而非仅仅依赖规模扩展或模型改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semantic+Laundering+in+AI+Agent+Architectures%3A+Why+Tool+Boundaries+Do+Not+Confer+Epistemic+Warrant，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08333&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [30] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb, Matt Bower, Ana Amélia Carvalho, Fredrik Mørk Røkenes, Jodie Torrington, Jonathan D. Cohen, Yousra Chtouki, Kathryn Maccallum, Tanya Linden, Deirdre Butler, Juliana Elisa Raffaghelli, Henriikka Vartiainen, Martina Ronci, Peter Tiernan, David M. Smith, Chris Shelton, Joyce Malyn-smith, Pierre Gorissen*

**主要类别:** cs.AI

**AI概要:** TWG5工作组专注于提升教师AI素养和能动性，通过课程设计、专业发展项目、课堂实践应用和政策指南等策略，帮助教师掌握AI知识技能并将其整合到教学中。


<details>
  <summary>更多</summary>
  
**动机:** 提升教师的AI素养和教学能力，使教师能够自信地使用AI工具并在学生中培养对AI概念的深入理解。

**方法:** 通过开发课程设计、专业发展项目、实际课堂应用和政策指南等多种策略来实施。

**结果:** 为教师提供了必要的知识和技能，使其能够有效整合AI到教学实践中。

**结论:** 通过综合性的策略方法，成功增强了教师在AI教育方面的能力和信心，促进了AI在教育领域的有效应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Thematic+Working+Group+5+--+Artificial+Intelligence+%28AI%29+literacy+for+teaching+and+learning%3A+design+and+implementation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08380，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08380&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [31] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

**主要类别:** cs.AI

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Qualitative+Model+to+Reason+about+Object+Rotations+%28QOR%29+applied+to+solve+the+Cube+Comparison+Test+%28CCT%29，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08382，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08382&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [32] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang, Junzhuo Li, Hong Chen, Yuanlin Chu, Yuxuan Fan, Xuming Hu*

**主要类别:** cs.AI

**AI概要:** 该论文研究了混合专家(MoE)架构在预训练期间的知识获取动态，发现MoE形成了低熵骨干结构、早期稳定性和功能鲁棒性等三种模式，表明稀疏性促进了分布式而非脆性的知识存储。


<details>
  <summary>更多</summary>
  
**动机:** 研究MoE架构如何在预训练期间塑造知识获取过程，以及这一过程与密集架构有何不同，因为目前这方面仍然未知。

**方法:** 引入Gated-LPI（对数概率增加）神经元级归因指标，分解跨神经元的对数概率增加。对MoE和密集架构进行时间分辨比较，分别跟踪120万训练步骤（约5.0T标记）和60万训练步骤（约2.5T标记）的检查点。

**结果:** 发现三种模式：(1)低熵骨干：前1%的MoE神经元捕获超过45%的正更新；(2)早期巩固：MoE模型在10万步内锁定稳定重要性分布；(3)功能鲁棒性：屏蔽最重要的MoE注意力头对关系HIT@10的影响小于10%，而密集模型超过50%。

**结论:** 稀疏性从训练早期就培养出内在稳定和分布式的计算骨干，有助于弥合稀疏架构与训练时可解释性之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deconstructing+Pre-training%3A+Knowledge+Attribution+Analysis+in+MoE+and+Dense+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08383&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [33] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

**主要类别:** cs.AI

**AI概要:** 该论文提出从生成视角看待AI创造力，将其视为有限领域生成模型在受限信息环境中的涌现特性，而非事后评估标签。论文将创造力分解为四个交互组件，为研究AI系统中的涌现创造力提供技术框架。


<details>
  <summary>更多</summary>
  
**动机:** 现有AI创造力研究多关注评估生成输出的新颖性、多样性等属性，而非将创造力作为显式建模的现象。随着多模态生成系统展现复杂模式重组能力，需要重新思考机器创造力的本质和限制。

**方法:** 提出生成视角的创造力框架，将其分解为四个交互组件：基于模式的生成、诱导世界模型、上下文基础、任意性，并分析这些组件在多模态生成系统中的表现。

**结果:** 建立了将创造力视为生成动态与领域特定表征相互作用的涌现现象的理论框架，为系统研究AI创造力提供了新的技术基础。

**结论:** 通过将创造力根植于生成动态与领域特定表征的交互中，该工作为将创造力作为AI系统中的涌现现象而非事后评估标签进行研究提供了概念和技术框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Creativity+in+AI+as+Emergence+from+Domain-Limited+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08388，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08388&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [34] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy*

**主要类别:** cs.AI

**AI概要:** OSPO是一个新的强化学习框架，通过Shapley-Owen属性分配来解决推荐系统中序列级奖励的信用分配问题，无需额外价值模型就能识别驱动性能的响应部分。


<details>
  <summary>更多</summary>
  
**动机:** 传统GRPO等方法依赖稀疏的序列级奖励，导致信用分配不明确，特别是在需要从模糊语言推断用户潜在意图的推荐任务中，这个问题更加严重。

**方法:** 使用基于潜在奖励塑形的Shapley-Owen属性分配方法，通过形成语义连贯单元（如描述产品属性的短语或捕捉偏好的句子）的联盟，重新分配序列级优势值。

**结果:** 在Amazon ESCI和H&M Fashion数据集上的实验显示，OSPO相比基线方法获得了一致的性能提升，并对训练时未见过的分布外检索器表现出良好的测试时鲁棒性。

**结论:** OSPO框架有效解决了推荐任务中的信用分配问题，能够直接从未标记的任务反馈中学习，识别响应中驱动性能的关键部分，且不需要额外的参数化价值模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Owen-Shapley+Policy+Optimization+%28OSPO%29%3A+A+Principled+RL+Algorithm+for+Generative+Search+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08403，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08403&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [35] [WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents](https://arxiv.org/abs/2601.08406)
*Xinyi Wu, Jiagui Chen, Geng Hong, Jiayi Dong, Xudong Pan, Jiarun Dai, Min Yang*

**主要类别:** cs.AI

**AI概要:** WebTrap Park是一个自动化平台，用于通过直接观察Web代理与真实网页的交互来进行系统性安全评估，包含1226个可执行评估任务，无需修改代理即可进行基于操作的评估。


<details>
  <summary>更多</summary>
  
**动机:** Web代理在真实网络环境中执行复杂任务日益增多，但其安全评估仍然分散且难以标准化，需要系统化的评估方法。

**方法:** 开发WebTrap Park平台，将三大安全风险源实例化为1226个可执行评估任务，通过直接观察代理与实时网页的交互进行安全评估。

**结果:** 结果显示不同代理框架存在明显的安全差异，强调代理架构的重要性超越底层模型。

**结论:** WebTrap Park为可复现的Web代理安全评估提供了可扩展的基础，平台已公开访问，有助于标准化Web代理安全评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WebTrap+Park%3A+An+Automated+Platform+for+Systematic+Security+Evaluation+of+Web+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08406&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.

</details>


### [36] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng, Hichem Snoussi, Yuhang Wang, Jing Teng, Abel Cherouat, Tian Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种结合知识蒸馏、思维链引导和监督微调的方法，用于无人机多SDK控制任务，成功将大型语言模型的代码生成能力迁移到轻量化模型上，在保持高精度的同时显著提升部署和推理效率。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在代码生成方面潜力巨大，但资源消耗高与无人机平台的实时轻量化需求存在矛盾，需要解决这一矛盾以实现无人机精准轻量化智能控制。

**方法:** 1) 构建包含指令-代码-推理链的高质量数据集，加入反事实负样本进行数据增强；2) 使用DeepSeek-Coder-V2-Lite作为教师模型，采用混合黑盒白盒蒸馏策略生成思维链软标签；3) 通过加权交叉熵损失将复杂推理能力迁移到学生模型；4) 针对无人机控制场景进行提示调优工程。

**结果:** 蒸馏后的轻量化模型在保持高代码生成精度的同时，实现了部署和推理效率的显著提升。

**结论:** 该方法有效证明了在无人机精准轻量化智能控制方面的可行性和优越性，为解决大模型资源消耗与无人机实时需求矛盾提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Distillation+with+CoT+Guidance+for+Edge-Drone+Control+Code+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08412&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [37] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen*

**主要类别:** cs.AI

**AI概要:** 提出自动化粗到细评分标准生成框架(RubricHub)和两阶段后训练流程，显著提升开放生成任务性能，在HealthBench上达到SOTA结果


<details>
  <summary>更多</summary>
  
**动机:** 现有基于评分标准的强化学习方法存在可扩展性瓶颈和标准粗糙问题，导致监督天花板效应，无法捕捉生成任务的细微差别

**方法:** 提出Coarse-to-Fine Rubric Generation框架，结合原则指导合成、多模型聚合和难度演化；建立大规模多领域RubricHub数据集；采用两阶段后训练流程(RuFT和RuRL)

**结果:** Qwen3-14B模型在HealthBench上达到69.3分，超越GPT-5等前沿专有模型，实现SOTA性能

**结论:** RubricHub框架有效解决了开放生成任务的监督瓶颈，通过精细化评分标准和强化学习实现了显著性能提升，为推理密集型领域提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RubricHub%3A+A+Comprehensive+and+Highly+Discriminative+Rubric+Dataset+via+Automated+Coarse-to-Fine+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08430，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08430&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [38] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Hadi Abdine, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang*

**主要类别:** cs.AI

**AI概要:** YaPO提出了一种基于稀疏自编码器的稀疏导向向量学习方法，相比密集导向向量方法在文化对齐等细粒度任务中表现更优，具有更好的解耦性、可解释性和训练稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于密集导向向量的激活干预方法由于神经元多语义性导致多个潜在因素纠缠，在文化对齐等需要区分密切相关价值观和行为的细粒度场景中效果有限。

**方法:** 提出YaPO方法，在稀疏自编码器的潜在空间中学习稀疏导向向量，通过优化稀疏编码实现解耦的、可解释的导向方向。

**结果:** YaPO收敛更快，性能更强，训练稳定性更好，在文化对齐、幻觉控制、财富寻求等多种对齐相关行为上表现优异，且不损害模型的通用知识能力。

**结论:** YaPO为LLMs提供了高效、稳定和细粒度对齐的通用方案，在可控性和领域适应方面具有广泛应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是YaPO%3A+Learnable+Sparse+Activation+Steering+Vectors+for+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08441，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08441&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [39] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang, Junhao Gan, Shengxiang Gao, Shenghao Ye, Zhengyi Yang, Jianzhong Qi*

**主要类别:** cs.AI

**AI概要:** TABGR是一种无需训练的表推理模型，通过将表格表示为属性表图(ATG)来保持表格结构，使用基于图的推理提高可解释性，并通过问题引导的PageRank机制缓解信息丢失问题，在基准测试中准确率提升最高达9.7%。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大语言模型表推理方法通过线性化表格会丢失表格结构、缺乏显式推理路径导致可解释性差，并且存在"中间丢失"问题。

**方法:** 提出TABGR模型：1) 将表格表示为属性表图(ATG)保持行列单元格结构；2) 使用基于图的推理提高可解释性；3) 提出问题引导的个性化PageRank(QG-PPR)机制重新排序表格数据

**结果:** 在两个常用基准测试中，TABGR始终优于最先进模型，准确率提升最高达9.7%

**结论:** TABGR通过图表示和基于图的推理有效解决了表格结构丢失、推理可解释性差和信息丢失问题，为表推理任务提供了更优的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Linearization%3A+Attributed+Table+Graphs+for+Table+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08444，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08444&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [40] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav, Abhishek Kaushik, Kevin Mc Daid*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个多模态可解释的Web应用，用于检测印地语-英语混合文本和表情包中的厌女内容，采用先进的Transformer模型并整合SHAP和LIME解释技术来提高透明度。


<details>
  <summary>更多</summary>
  
**动机:** 数字平台用户增长导致仇恨言论和厌女内容传播，现有AI模型在低资源混合语言场景下效果有限且缺乏可解释性，需要透明化的检测系统来打击基于性别的网络暴力。

**方法:** 使用XLM-R和mBERT处理约4193条混合文本评论，采用mBERT+EfficientNet和mBERT+ResNET处理约4218个表情包，集成SHAP和LIME提供特征重要性解释。

**结果:** 开发了功能完整的Web应用，通过人类评估者使用Chatbot Usability Questionnaire和User Experience Questionnaire评估系统可用性。

**结论:** 该系统为研究者和内容审核者提供了有效工具，促进该领域进一步研究，有助于打击基于性别的数字暴力并确保安全的数字空间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Under-Explored+Application+for+Explainable+Multimodal+Misogyny+Detection+in+code-mixed+Hindi-English，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08457&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [41] [M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games](https://arxiv.org/abs/2601.08462)
*Sixiong Xie, Zhuofan Shi, Haiyang Shen, Gang Huang, Yun Ma, Xiang Jing*

**主要类别:** cs.AI

**AI概要:** M3-Bench是一个多阶段基准测试，用于评估LLM智能体的社会行为能力，通过行为轨迹、推理过程和通信内容三个维度的协同分析，结合人格模型和社会交换理论，提供超越简单任务分数的可解释社会行为画像。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试往往只关注单一能力维度或仅依赖行为结果，忽略了智能体决策推理和通信交互的丰富过程信息，无法全面评估LLM智能体的复杂社会行为。

**方法:** 提出M3-Bench多阶段混合动机游戏基准，包含三个分析模块：BTA（行为轨迹分析）、RPA（推理过程分析）和CCA（通信内容分析），并整合大五人格模型和社会交换理论进行多维度证据聚合。

**结果:** 实验结果表明M3-Bench能可靠区分不同模型的社会行为能力，并揭示一些模型虽然行为结果看似合理，但在推理和通信方面存在明显不一致性。

**结论:** M3-Bench提供了一个过程感知的评估框架，能够更全面地评估LLM智能体的社会行为，为理解模型的社会能力和人格特征提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是M3-BENCH%3A+Process-Aware+Evaluation+of+LLM+Agents+Social+Behaviors+in+Mixed-Motive+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08462，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08462&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.

</details>


### [42] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun, Juhwan Choi, Kyohoon Jin, Soojin Jang, Jinhee Jang, YoungBin Kim*

**主要类别:** cs.AI

**AI概要:** SummPilot是一个基于大语言模型的交互式可定制摘要系统，通过语义图、实体聚类和可解释评估等交互组件，让用户能够根据个人兴趣和需求生成个性化摘要。


<details>
  <summary>更多</summary>
  
**动机:** 解决自动摘要效率与个性化需求之间的矛盾，为用户提供量身定制的摘要服务。

**方法:** 利用大语言模型实现自动和交互式摘要，通过语义图、实体聚类和可解释评估等交互组件让用户参与摘要定制过程。

**结果:** 演示和用户研究表明SummPilot系统具有高度的适应性和实用性，能够有效满足可定制摘要的需求。

**结论:** SummPilot成功结合了自动摘要的高效性和个性化定制能力，为交互式可定制摘要提供了一个有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SUMMPILOT%3A+Bridging+Efficiency+and+Customization+for+Interactive+Summarization+System，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08475，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08475&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [43] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang, Hyunbin Jin, Hyungjin Park, Kyubyung Chae, Taesup Kim*

**主要类别:** cs.AI

**AI概要:** WIT是一个多模态时间序列预测基准，通过专家构建的合理或反事实场景来评估模型是否能根据上下文文本（特别是未来场景）进行条件预测。


<details>
  <summary>更多</summary>
  
**动机:** 现有时间序列预测方法大多是单模态的，依赖历史模式外推。虽然大语言模型展示了多模态预测潜力，但现有基准提供的是回顾性或不对齐的原始上下文，无法确定模型是否真正利用了文本输入。

**方法:** 引入What If TSF (WIT)基准，提供专家构建的合理或反事实场景，设计用于评估模型在上下文文本（特别是未来场景）条件下的预测能力。

**结果:** WIT提供了一个严格的测试平台，用于场景引导的多模态预测评估，基准已在GitHub上开源。

**结论:** WIT基准填补了多模态时间序列预测评估的空白，通过场景化测试能够更准确地评估模型是否真正利用文本上下文进行预测，模拟了人类专家结合历史证据和假设场景进行预测的实际决策过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+If+TSF%3A+A+Benchmark+for+Reframing+Forecasting+as+Scenario-Guided+Multimodal+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08509，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08509&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [44] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree, Xusheng Du, Yushu Cai, Zhengyang Wang, Ye Zhang, Haoran Xie*

**主要类别:** cs.AI

**AI概要:** 提出结合生成式AI和视觉语言模型的三阶段框架，直接从粗略结构草图和文本描述生成立面改造方案，无需详细竣工建模。


<details>
  <summary>更多</summary>
  
**动机:** 立面改造比完全拆除更可持续，但现有工作流程需要耗时的竣工建模和反复修改，设计提案难以在保留原有结构的同时表达新意图。

**方法:** 三阶段框架：1) 微调VLM模型预测需要修改的区域和添加组件；2) 稳定扩散模型生成新元素细节草图并通过修复管道合并；3) 使用ControlNet生成逼真图像。

**结果:** 在数据集和实际工业建筑上的实验表明，该框架能生成保留原有结构同时提升立面细节质量的改造方案。

**结论:** 该方法有效规避了详细竣工建模需求，使建筑师能快速探索设计方案、迭代早期概念并更清晰地传达改造意图。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sketch-Based+Facade+Renovation+With+Generative+AI%3A+A+Streamlined+Framework+for+Bypassing+As-Built+Modelling+in+Industrial+Adaptive+Reuse，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08531，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08531&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [45] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang Yao, Zhipeng Gao, Jingyuan Chen*

**主要类别:** cs.AI

**AI概要:** 论文提出了LPR任务和LSG框架，通过检索修复方案和迭代优化方法，在修复代码错误的同时提供错误原因解释，显著优于现有基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 当前智能编程辅导系统主要关注修复错误代码，但缺乏对错误根本原因的解释，无法满足学习者的深层需求。

**方法:** 提出两阶段框架：1) 使用修复方案检索数据库和编辑驱动代码检索方法；2) 解决方案引导的程序修复方法，结合迭代检索增强优化策略。

**结果:** 实验结果表明，该方法大幅超越了一系列基线模型，验证了在新提出的LPR任务上的有效性。

**结论:** LSG框架通过结合检索技术和迭代优化，成功实现了在修复代码的同时提供错误解释，为智能编程辅导提供了更全面的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learner-Tailored+Program+Repair%3A+A+Solution+Generator+with+Iterative+Edit-Driven+Retrieval+Enhancement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08545，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08545&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [46] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran, Mariangel Garcia Andarcia, Hugo Retief, Chris Dickens, Paulo Silva*

**主要类别:** cs.AI

**AI概要:** WaterCopilot是一个AI驱动的虚拟助手，通过RAG和工具调用架构整合静态政策文档和实时水文数据，为林波波河流域提供多语言交互、实时警报和水资源管理支持，评估得分0.8043。


<details>
  <summary>更多</summary>
  
**动机:** 解决跨界河流流域水资源管理中数据碎片化、实时访问受限以及多样化信息源整合复杂的问题。

**方法:** 基于检索增强生成(RAG)和工具调用架构，开发两个定制插件：iwmi-doc-plugin用于文档语义搜索，iwmi-api-plugin用于实时数据库查询。

**结果:** 系统获得RAGAS框架总体评分0.8043，其中答案相关性0.8571，上下文精确度0.8009，实现了环境流量警报、降雨趋势、水库水位等动态洞察。

**结论:** WaterCopilot建立了一个可复制的AI增强框架，在数据稀缺的跨界环境中增强水治理能力，展示了支持及时决策和加强水安全的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WaterCopilot%3A+An+AI-Driven+Virtual+Assistant+for+Water+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08559&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [47] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison, Quentin Macé, Antoine Edy, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse, Céline Hudelot, Gautier Viaud*

**主要类别:** cs.AI

**AI概要:** ViDoRe v3是一个多模态RAG基准测试，包含视觉丰富的文档和多种查询类型，评估现有模型在视觉元素理解、多文档信息合成和准确来源定位方面的能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有RAG基准测试主要关注文本数据和单文档理解，无法充分评估多模态文档中视觉元素解析、跨文档信息合成和来源定位等复杂挑战。

**方法:** 构建包含10个专业领域数据集、约26,000个文档页面和3,099个人工验证查询的多模态基准，提供检索相关性、边界框定位和参考答案的高质量标注。

**结果:** 评估显示视觉检索器优于文本检索器，后期交互模型和文本重排序显著提升性能，混合或纯视觉上下文提高答案生成质量，但模型在非文本元素、开放式查询和细粒度视觉定位方面仍有困难。

**结论:** ViDoRe v3基准测试揭示了当前RAG系统在处理多模态文档时的局限性，为促进该领域发展提供了重要的评估工具，并采用商业友好许可证开放使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ViDoRe+V3%3A+A+Comprehensive+Evaluation+of+Retrieval+Augmented+Generation+in+Complex+Real-World+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08620&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [48] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个可解释的多智能体系统，用于解决模因币跟单交易中的挑战，通过模仿资产管理团队结构，使用少样本思维链提示让专业智能体协作分析，在识别高质量项目和KOL钱包方面达到73%和70%的准确率。


<details>
  <summary>更多</summary>
  
**动机:** 模因币跟单交易虽然流行但存在盈利不确定性，主要由于操纵性机器人、被跟随钱包未来表现不确定性和交易执行延迟等问题。同时单一大型语言模型在处理复杂多任务时能力有限，且在加密货币领域缺乏足够专业知识。

**方法:** 提出基于资产管理团队结构的可解释多智能体系统，将复杂任务分解为子任务，通过少样本思维链提示让专业智能体获取模因币交易知识，解释多模态数据并生成可解释决策。

**结果:** 使用1000个模因币项目交易数据评估，多智能体系统在识别高质量模因币项目和KOL钱包方面的准确率分别达到73%和70%，所选KOL在这些项目中总共产生50万美元利润。

**结论:** 多智能体系统通过任务分解和专业智能体协作，有效解决了模因币跟单交易的挑战，在性能和可解释性方面优于传统机器学习模型和单一LLM，为加密货币投资提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Resisting+Manipulative+Bots+in+Memecoin+Copy+Trading%3A+A+Multi-Agent+Approach+with+Chain-of-Thought+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08641，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08641&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [49] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao, Jinzhi Liao, Xiang Zhao*

**主要类别:** cs.AI

**AI概要:** Prism框架通过逻辑依赖建模解决LLM在社交平台中复杂意图理解的挑战，包含意图分解、逻辑澄清生成、意图感知奖励和自我进化调优四个模块，在逻辑一致性、用户满意度和任务效率方面显著优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法通过顺序或并行提问澄清用户意图，但未能解决核心挑战——建模澄清问题之间的逻辑依赖关系，这限制了LLM在社交平台中处理用户模糊动态目标的能力。

**方法:** 提出Prism框架：1)复杂意图分解模块将意图分解为结构化元素并识别逻辑依赖；2)逻辑澄清生成模块基于依赖关系组织问题；3)意图感知奖励模块通过奖励函数和蒙特卡洛采样评估交互质量；4)自我进化调优模块通过数据驱动反馈迭代优化LLM能力。

**结果:** 在澄清交互、意图执行和认知负载基准测试中 consistently outperforms现有方法：逻辑冲突降至11.5%，用户满意度提升14.4%，任务完成时间减少34.8%，达到state-of-the-art逻辑一致性。

**结论:** Prism通过建模逻辑依赖关系实现了逻辑连贯且高效的意图澄清，为LLM在社交平台中的复杂意图理解提供了有效解决方案，所有数据和代码已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prism%3A+Towards+Lowering+User+Cognitive+Load+in+LLMs+via+Complex+Intent+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08653&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [50] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar*

**主要类别:** cs.AI

**AI概要:** 这是一个面向本科生的强化学习教程，通过实例驱动的清晰解释使RL更易理解，重点解决理论到代码实现的过渡难题


<details>
  <summary>更多</summary>
  
**动机:** 让本科生更容易接触和理解强化学习，弥合RL理论与实际编程应用之间的差距

**方法:** 采用实践示例和易于理解的解释方法，提供动手编程实例

**结果:** 学生能够获得应用强化学习技术的基础技能

**结论:** 该教程旨在帮助学生建立信心，在现实场景中应用RL技术

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Classical+to+Quantum+Reinforcement+Learning+and+Its+Applications+in+Quantum+Control%3A+A+Beginner%27s+Tutorial，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08662，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08662&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [51] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo, Paolo Papotti*

**主要类别:** cs.AI

**AI概要:** Pced是一个无需训练的框架，通过并行解码和对比解码规则实现多文档检索增强生成，解决了传统方法在预填充瓶颈和跨文档交互之间的权衡问题。


<details>
  <summary>更多</summary>
  
**动机:** 检索增强生成面临两难：长提示连接文档支持多文档推理但导致预填充瓶颈，而单独编码文档KV缓存虽快但破坏了跨文档交互。

**方法:** 提出Parallel Context-of-Experts Decoding (Pced)，将证据聚合从注意力机制转移到解码过程，将检索文档视为独立专家，通过新颖的检索感知对比解码规则同步预测。

**结果:** 该方法恢复了跨文档推理能力，无需构建跨文档的共享注意力机制。

**结论:** Pced框架有效解决了检索增强生成中的速度与交互性权衡问题，通过解码层面的创新实现了高效的多文档推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Parallel+Context-of-Experts+Decoding+for+Retrieval+Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08670&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [52] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette, Sandro Claudio Lera, Ke Wu*

**主要类别:** cs.AI

**AI概要:** 论文认为LLM的"恶意行为"不是对齐失败，而是对人类社会中权力不对称关系的统计泛化，真正的AGI风险在于其作为人类智能和矛盾的放大器，而非敌对意图。


<details>
  <summary>更多</summary>
  
**动机:** 针对近期关于大语言模型表现出欺骗、威胁等行为的报道，作者认为将这些行为解释为对齐失败或恶意涌现是基于概念错误，需要重新理解这些行为的本质。

**方法:** 运用关系模型理论，分析黑mail等行为在人类社会连续体中的位置，论证这些行为是权力、信息或约束极端不对称下的结构泛化，而非道德异常。

**结果:** LLM的"不道德"行为实际上是人类社会互动记录的统计反映，特别是极端权力不对称情境下的行为模式。AGI的主要风险是其作为人类智能和矛盾的放大器作用。

**结论:** 需要重新构建AGI风险框架：重点不是模型意图对齐，而是治理AGI作为人类智能放大器的结构性影响，关注复杂性、制度稳定性和时间压缩效应。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Why+AI+Alignment+Failure+Is+Structural%3A+Learned+Human+Interaction+Structures+and+AGI+as+an+Endogenous+Evolutionary+Shock，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08673&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [53] [Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance](https://arxiv.org/abs/2601.08676)
*Yilei Zhao, Wentao Zhang, Xiao Lei, Yandan Zheng, Mengpu Liu, Wei Yang Bryan Lim*

**主要类别:** cs.AI

**AI概要:** ESGAgent是一个分层多智能体系统，通过专业工具集解决ESG分析中的数据碎片化和复杂工作流问题，在原子问答任务中达到84.15%准确率，并在专业报告生成方面表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 专业ESG分析面临非结构化数据碎片化问题，现有大语言模型难以处理复杂的多步骤审计工作流程。

**方法:** 开发ESGAgent分层多智能体系统，配备检索增强、网络搜索和领域特定功能工具集，并基于310份企业可持续发展报告构建三级基准测试。

**结果:** ESGAgent在原子问答任务中平均准确率达84.15%，优于最先进的闭源LLM，在整合丰富图表和可验证参考的专业报告生成方面表现突出。

**结论:** 该基准测试具有诊断价值，为高风险垂直领域中评估通用和高级智能体能力提供了重要测试平台。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+ESG+Intelligence%3A+An+Expert-level+Agent+and+Comprehensive+Benchmark+for+Sustainable+Finance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08676，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08676&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.

</details>


### [54] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei*

**主要类别:** cs.AI

**AI概要:** PersonaDual框架在单一模型中同时支持客观推理和个性化推理，通过上下文自适应切换模式，既保留个性化优势又减少干扰。


<details>
  <summary>更多</summary>
  
**动机:** 解决LLMs个性化信息带来的问题：虽然能改善交互体验，但可能损害客观性和事实正确性，特别是当个性化信息与问题不匹配时。

**方法:** 先通过监督微调(SFT)学习两种推理模式，然后用提出的DualGRPO强化学习优化模式选择。

**结果:** 在客观和个性化基准测试中，PersonaDual实现了近乎无干扰的性能，并更好地利用有益的个性化信号来提升客观问题解决能力。

**结论:** PersonaDual框架有效平衡了个性化与客观性，在保持个性化优势的同时显著减少了对事实准确性的干扰。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PersonaDual%3A+Balancing+Personalization+and+Objectivity+via+Adaptive+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08679，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08679&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [55] [MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection](https://arxiv.org/abs/2601.08684)
*Paolo Italiani, David Gimeno-Gomez, Luca Ragazzi, Gianluca Moro, Paolo Rosso*

**主要类别:** cs.AI

**AI概要:** MemeWeaver是一个端到端的多模态框架，通过创新的跨meme图推理机制检测性别歧视和厌女症，在MAMI和EXIST基准测试中优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 女性因性别遭受网络骚扰的概率是男性的两倍，现有方法忽视了这种现象背后的社会动态，即施害者在志同道合的社区中强化偏见和群体认同。

**方法:** 提出MemWeaver框架，采用图推理机制捕捉互动关系，系统评估多种视觉-文本融合策略，实现端到端训练。

**结果:** 在MAMI和EXIST基准测试中持续优于最先进基线方法，训练收敛速度更快，学习到的图结构捕获了有意义的语义模式。

**结论:** 该方法有效捕捉了在线仇恨的关系性质，为理解网络骚扰的社会动态提供了有价值的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MEMEWEAVER%3A+Inter-Meme+Graph+Reasoning+for+Sexism+and+Misogyny+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08684&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.

</details>


### [56] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni, Alexander Lyzhov, Shiva Chaitanya, Preetam Joshi*

**主要类别:** cs.AI

**AI概要:** 提出了OIP-SCE评估方法，通过检查对话是否按正确顺序满足所有临床义务要求，并提供清晰证据供临床医生审查，从而弥合AI技术与医疗实际需求之间的差距。


<details>
  <summary>更多</summary>
  
**动机:** 现有对话AI评估方法忽视了合规性对完整对话过程的依赖，无法有效支持真实的临床工作需求。

**方法:** 开发了Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE)方法，要求按阶段检查必需信息的完整性、顺序正确性和证据清晰性。

**结果:** 在两个案例研究（呼吸病史、福利验证）中验证了该方法，证明阶段级证据能将政策转化为可共享、可操作的具体步骤。

**结论:** OIP-SCE为临床医生提供了可控的检查标准，为工程师提供了清晰的实施规范，创建了统一的、可审计的评估界面，支持AI能力与临床工作流程的对齐和安全常规使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是All+Required%2C+In+Order%3A+Phase-Level+Evaluation+for+AI-Human+Dialogue+in+Healthcare+and+Beyond，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08690，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08690&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [57] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal, Eoin Delaney, Zihao Fu, Sandra Wachter, Chris Russell*

**主要类别:** cs.AI

**AI概要:** 该论文提出了解释性AI评估的新方法AXE，能够有效检测模型解释中的对抗性公平性伪装，成功率达100%，无需依赖真实解释作为基准。


<details>
  <summary>更多</summary>
  
**动机:** 现有模型解释评估方法依赖与真实解释的对比，这掩盖了Rashomon集合中模型行为的差异，且无法有效检测解释中的公平性伪装问题。

**方法:** 提出了AXE方法，基于三个解释评估原则，通过分析特征重要性解释来评估解释质量，无需真实解释作为基准。

**结果:** AXE方法能够100%成功检测到对抗性公平性伪装，并能识别受保护属性是否被用于预测，优于基于模型敏感性或真实解释比较的现有方法。

**结论:** AXE提供了一种无需真实解释基准的有效评估方法，能够揭示Rashomon集合中模型的行为差异，帮助选择更合适的模型部署，有效防止解释性AI中的公平性伪装问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+the+Ability+of+Explanations+to+Disambiguate+Models+in+a+Rashomon+Set，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08703&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [58] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan, Yuning Wang, Wenjie Qiu, He Zhu*

**主要类别:** cs.AI

**AI概要:** Cago是一种新的模仿学习方法，通过动态跟踪智能体在专家轨迹上的能力，选择刚好超出当前能力的中间目标来指导学习，从而解决长时程环境中模仿学习的累积误差问题。


<details>
  <summary>更多</summary>
  
**动机:** 模仿学习在长时程环境中经常失败，因为完美复制演示不现实，小错误会累积导致灾难性后果，需要减少对专家轨迹的脆弱依赖。

**方法:** Cago方法动态跟踪智能体在专家轨迹上的能力水平，选择刚好超出当前能力的中间目标（goals）来创建自适应课程，指导学习过程。

**结果:** 实验结果显示，Cago在一系列稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，始终优于现有的模仿学习基线方法。

**结论:** Cago通过能力感知的目标采样方法有效解决了模仿学习在长时程任务中的累积误差问题，提供了一种稳健的学习框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+from+Demonstrations+via+Capability-Aware+Goal+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08731，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08731&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [59] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers, Ari Holtzman*

**主要类别:** cs.AI

**AI概要:** 论文指出AI系统主要被定位为提升生产力的智能工具，但实际正被广泛用于娱乐目的，特别是年轻人群体。AI领域缺乏评估娱乐内容社会影响的框架，当前评估主要关注文化危害而忽视文化益处。作者提出"厚娱乐"框架来评估AI生成内容在意义创造、身份形成和社会连接方面的价值。


<details>
  <summary>更多</summary>
  
**动机:** AI系统主要被设计和评估为提升生产力的工具，但现实中AI正被大量用于娱乐目的，这种使用模式与主流叙事存在张力。当前AI评估框架主要关注文化危害，缺乏评估娱乐内容积极社会价值的方法。

**方法:** 通过分析当前AI评估实践，识别评估框架的不对称性：过度关注文化危害而忽视文化益处。借鉴人文学科见解，提出"厚娱乐"评估框架，关注娱乐在意义创造、身份形成和社会连接方面的作用。

**结果:** 发现AI已广泛用于娱乐目的，特别是年轻人群体，并可能成为AI公司的主要商业模式。当前评估框架无法充分评估AI生成娱乐内容的社会价值，需要新的评估范式。

**结论:** AI可能最终更多是关于娱乐而非智能，就像社交媒体更多是关于社交连接一样。需要建立新的评估框架来理解和评估AI生成娱乐内容在社会意义创造、身份形成和社会连接方面的积极价值，而不仅仅是减少危害。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+as+Entertainment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08768&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [60] [Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards](https://arxiv.org/abs/2601.08778)
*Tengjun Jin, Yoojin Choi, Yuxuan Zhu, Daniel Kang*

**主要类别:** cs.AI

**AI概要:** 该研究分析了文本到SQL基准测试BIRD和Spider 2.0-Snow的标注错误率，发现分别高达52.8%和62.8%。通过修正BIRD开发集子集并重新评估16个开源代理，发现性能变化达-7%到31%，排名变化达±9位，表明标注错误严重扭曲性能评估和排名。


<details>
  <summary>更多</summary>
  
**动机:** 文本到SQL技术依赖人工标注的基准测试进行性能比较和部署选择，但标注有效性尚未得到充分验证，可能误导研究方向和实际应用选择。

**方法:** 采用专家分析评估两个主流文本到SQL基准的标注错误率；修正BIRD开发集子集；重新评估16个开源代理在原始和修正数据集上的性能；分析排名相关性。

**结果:** BIRD Mini-Dev错误率52.8%，Spider 2.0-Snow错误率62.8%；代理性能相对变化-7%到31%；排名变化±9位；修正前后排名相关性从0.85降至0.32。

**结论:** 基准测试中的标注错误对文本到SQL系统性能评估和排名有显著影响，可能误导研究和部署决策，需要更可靠的评估方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pervasive+Annotation+Errors+Break+Text-to-SQL+Benchmarks+and+Leaderboards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08778，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08778&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.
  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.

</details>


### [61] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen, Karen de Jong, Andreas Poole, Jan Burakowski, Elena Elderson Nosti, Joep Windt, Chendi Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种通过将模型生成的投票预测与真实议会投票记录对齐来构建政治偏见基准的方法，在荷兰、挪威和西班牙三个国家案例中应用，发现最先进的LLM普遍表现出左倾或中间派倾向，并对右翼保守政党存在明显负面偏见。


<details>
  <summary>更多</summary>
  
**动机:** 随着LLM在数字平台和决策系统中深度应用，对其政治偏见的担忧日益增长。虽然已有大量关于性别和种族等社会偏见的研究，但对政治偏见的系统性研究仍然有限，尽管其具有直接的社会影响。

**方法:** 提出通用方法构建政治偏见基准：将模型生成的投票预测与验证过的议会投票记录对齐。在三个国家案例中实例化：荷兰(2701项动议/投票，15个政党)、挪威(10584项，9个政党)、西班牙(2480项，10个政党)。提出可视化方法，将LLM和政党的意识形态在CHES二维空间中展示。

**结果:** 实验揭示了细粒度的意识形态区分：最先进的LLM一致表现出左倾或中间派倾向，同时对右翼保守政党存在明显的负面偏见。

**结论:** 这些发现强调了基于真实议会行为的透明跨国评估对于理解和审计现代LLM政治偏见的价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncovering+Political+Bias+in+Large+Language+Models+using+Parliamentary+Voting+Records，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08785，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08785&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [62] [EmbeddingRWKV: State-Centric Retrieval with Reusable States](https://arxiv.org/abs/2601.07861)
*Haowen Hou, Jie Yang*

**主要类别:** cs.CL

**AI概要:** 提出State-Centric Retrieval统一检索范式，通过状态表示学习将嵌入模型和重排序器连接，显著提升RAG系统效率，实现5.4-44.8倍加速。


<details>
  <summary>更多</summary>
  
**动机:** 传统两阶段RAG系统存在效率低下问题，由于嵌入模型和重排序器之间缺乏信息共享，导致大量冗余计算。

**方法:** 1. 基于RWKV的LLM进行状态表示学习，创建EmbeddingRWKV统一模型；2. 设计基于状态的重排序器，仅处理查询token；3. 采用统一层选择策略减少计算量。

**结果:** 模型在仅使用25%层数的情况下保持98.62%的完整模型性能，重排序阶段实现5.4-44.8倍加速，同时获得高质量的检索和重排序结果。

**结论:** State-Centric Retrieval通过状态共享机制有效解决了传统RAG系统的效率瓶颈，在保持性能的同时大幅提升系统效率，为高效检索系统提供了新范式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EmbeddingRWKV%3A+State-Centric+Retrieval+with+Reusable+States，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07861&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.

</details>


### [63] [A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics](https://arxiv.org/abs/2601.07954)
*Haoan Jin, Han Ying, Jiacheng Ji, Hanhui Xu, Mengyue Wu*

**主要类别:** cs.CL

**AI概要:** 提出了MedES基准和guardian-in-the-loop框架，通过监督微调和领域特定偏好优化，使7B参数LLM在中文医疗伦理任务上超越更大基线模型


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在医疗任务中应用广泛，但在复杂现实场景下与医疗伦理要求的对齐仍待探索，特别是在中文医疗领域缺乏专门基准

**方法:** 从260个权威中文医疗、伦理和法律来源构建MedES基准；开发guardian-in-the-loop框架，使用准确率97%的自动评估器生成针对性提示和结构化伦理反馈；通过监督微调和领域特定偏好优化对齐7B参数LLM

**结果:** 在中文医疗伦理环境下，对齐后的模型在核心伦理任务上显著优于更大的基线模型，质量和综合评估指标均有提升

**结论:** 为中文医疗领域提供了实用且可适应的LLM伦理对齐框架，通过模块化替换底层规范语料，类似对齐流程可应用于其他法律和文化环境

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Human-Centric+Pipeline+for+Aligning+Large+Language+Models+with+Chinese+Medical+Ethics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07954，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07954&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric benchmark specifically constructed from 260 authoritative Chinese medical, ethical, and legal sources to reflect the challenges in clinical decision-making. To facilitate model alignment, we introduce a guardian-in-the-loop framework that leverages a dedicated automated evaluator (trained on expert-labeled data and achieving over 97% accuracy within our domain) to generate targeted prompts and provide structured ethical feedback. Using this pipeline, we align a 7B-parameter LLM through supervised fine-tuning and domain-specific preference optimization. Experimental results, conducted entirely within the Chinese medical ethics context, demonstrate that our aligned model outperforms notably larger baselines on core ethical tasks, with observed improvements in both quality and composite evaluation metrics. Our work offers a practical and adaptable framework for aligning LLMs with medical ethics in the Chinese healthcare domain, and suggests that similar alignment pipelines may be instantiated in other legal and cultural environments through modular replacement of the underlying normative corpus.

</details>


### [64] [Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs](https://arxiv.org/abs/2601.07972)
*Jen-tse Huang, Jiantong Qin, Xueli Qiu, Sharon Levy, Michelle R. Kaufman, Mark Dredze*

**主要类别:** cs.CL

**AI概要:** 研究发现大型语言模型在价值对齐方面表现出近乎完美的模型间一致性，但与人类存在显著差异，同时揭示了人类和AI都存在知识-行动差距，指令引导价值表现会下降。


<details>
  <summary>更多</summary>
  
**动机:** 探索大型语言模型如何在现实决策情境中表征和执行人类价值，以及价值对齐训练的效果。

**方法:** 使用ValAct-15k数据集（3000个Reddit建议寻求场景），基于Schwartz基本人类价值理论，评估10个前沿LLM和55名人类参与者的价值表现，比较场景决策和传统问卷的差异。

**结果:** LLM间决策一致性近乎完美（r≈1.0），人类变异性大（r∈[-0.79,0.98]）；人类和LLM都存在自我报告与实际行动价值之间的弱对应关系（r=0.4,0.3）；指令引导价值时LLM性能下降6.6%。

**结论:** 对齐训练虽然实现了规范性价值趋同，但未能消除人类特有的价值认知与行动之间的不连贯性，LLM表现出角色扮演厌恶。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowing+But+Not+Doing%3A+Convergent+Morality+and+Divergent+Action+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07972&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Value alignment is central to the development of safe and socially compatible artificial intelligence. However, how Large Language Models (LLMs) represent and enact human values in real-world decision contexts remains under-explored. We present ValAct-15k, a dataset of 3,000 advice-seeking scenarios derived from Reddit, designed to elicit ten values defined by Schwartz Theory of Basic Human Values. Using both the scenario-based questions and the traditional value questionnaire, we evaluate ten frontier LLMs (five from U.S. companies, five from Chinese ones) and human participants ($n = 55$). We find near-perfect cross-model consistency in scenario-based decisions (Pearson $r \approx 1.0$), contrasting sharply with the broad variability observed among humans ($r \in [-0.79, 0.98]$). Yet, both humans and LLMs show weak correspondence between self-reported and enacted values ($r = 0.4, 0.3$), revealing a systematic knowledge-action gap. When instructed to "hold" a specific value, LLMs' performance declines up to $6.6%$ compared to merely selecting the value, indicating a role-play aversion. These findings suggest that while alignment training yields normative value convergence, it does not eliminate the human-like incoherence between knowing and acting upon values.

</details>


### [65] [Explaining Generalization of AI-Generated Text Detectors Through Linguistic Analysis](https://arxiv.org/abs/2601.07974)
*Yuxi Xia, Kinga Stańczak, Benjamin Roth*

**主要类别:** cs.CL

**AI概要:** 该论文通过语言学分析系统研究了AI文本检测器的泛化问题，发现泛化性能与训练和测试条件之间的语言特征变化显著相关，特别是时态使用和代词频率等特征。


<details>
  <summary>更多</summary>
  
**动机:** AI文本检测器在领域内基准测试中准确率高，但在不同生成条件（如未见过的提示、模型家族或领域）下泛化能力差，且缺乏对根本原因的系统性研究。

**方法:** 构建包含6种提示策略、7个大语言模型和4个领域数据集的综合基准，微调基于分类的检测器，评估跨提示、跨模型和跨数据集泛化，并通过80个语言特征的相关性分析解释性能差异。

**结果:** 研究发现特定检测器和评估条件的泛化性能与语言特征（如时态使用和代词频率）的变化显著相关。

**结论:** 语言特征的变化是影响AI文本检测器泛化能力的关键因素，这为改进检测器的泛化性能提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explaining+Generalization+of+AI-Generated+Text+Detectors+Through+Linguistic+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07974，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07974&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** AI-text detectors achieve high accuracy on in-domain benchmarks, but often struggle to generalize across different generation conditions such as unseen prompts, model families, or domains. While prior work has reported these generalization gaps, there are limited insights about the underlying causes. In this work, we present a systematic study aimed at explaining generalization behavior through linguistic analysis. We construct a comprehensive benchmark that spans 6 prompting strategies, 7 large language models (LLMs), and 4 domain datasets, resulting in a diverse set of human- and AI-generated texts. Using this dataset, we fine-tune classification-based detectors on various generation settings and evaluate their cross-prompt, cross-model, and cross-dataset generalization. To explain the performance variance, we compute correlations between generalization accuracies and feature shifts of 80 linguistic features between training and test conditions. Our analysis reveals that generalization performance for specific detectors and evaluation conditions is significantly associated with linguistic features such as tense usage and pronoun frequency.

</details>


### [66] [Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models](https://arxiv.org/abs/2601.07984)
*Haorui Yu, Ramon Ruiz-Dolz, Xuehang Wen, Fengrui Zhang, Qiufeng Yi*

**主要类别:** cs.CL

**AI概要:** 提出了一个三层评估框架来评估视觉语言模型在跨文化艺术评论中的文化理解能力，通过自动化指标、基于量表的评分和人类评分校准来提供文化理解分数和诊断信息


<details>
  <summary>更多</summary>
  
**动机:** 视觉语言模型在视觉感知方面表现出色，但其解读艺术中文化意义的能力尚未得到充分验证，需要系统评估方法

**方法:** 三层评估框架：第一层计算自动化覆盖和风险指标；第二层使用单一主评判者在五个维度进行基于量表的评分；第三层通过等渗回归将第二层得分校准到人类评分

**结果:** 在152个样本的测试集上MAE降低了5.2%，评估了15个VLM模型在6种文化传统的294个专家锚点上，发现自动化指标不可靠、西方样本得分高于非西方样本、跨评判者尺度不匹配问题

**结论:** 该框架可为模型选择和文化差距诊断提供校准的文化理解分数，同时提供维度级诊断和风险指标，数据集和代码已公开

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Cultural+Expert-Level+Art+Critique+Evaluation+with+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07984，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07984&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.

</details>


### [67] [Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset](https://arxiv.org/abs/2601.07985)
*Z. Melce Hüsünbeyi, Virginie Mouilleron, Leonie Uhling, Daniel Foppe, Tatjana Scheffler, Djamé Seddah*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个构建多语言多模态事实核查数据集的新流程，通过整合ClaimReview数据、抓取完整辟谣文章、标准化裁决结果，并利用大语言模型进行证据提取和理由生成，为多语言多模态错误信息验证研究奠定基础。


<details>
  <summary>更多</summary>
  
**动机:** 现有事实核查数据集在范围上存在局限，缺乏多模态证据、结构化标注以及声明、证据和裁决之间的详细链接，无法满足对强大、最新、可解释和多语言事实核查资源的需求。

**方法:** 开发了一个综合数据处理流程：聚合ClaimReview数据源、抓取完整辟谣文章、标准化异质声明裁决、用结构化元数据和对齐的视觉内容进行丰富。使用最先进的大语言模型和多模态大语言模型进行证据提取和理由生成。

**结果:** 通过G-Eval和人工评估验证，该流程能够对不同组织或媒体市场的事实核查实践进行细粒度比较，促进开发更可解释和基于证据的事实核查模型。

**结论:** 该研究为未来多语言多模态错误信息验证研究奠定了基础，提供了更全面的事实核查数据集构建方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multilingual%2C+Multimodal+Pipeline+for+Creating+Authentic+and+Structured+Fact-Checked+Claim+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07985，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07985&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The rapid proliferation of misinformation across online platforms underscores the urgent need for robust, up-to-date, explainable, and multilingual fact-checking resources. However, existing datasets are limited in scope, often lacking multimodal evidence, structured annotations, and detailed links between claims, evidence, and verdicts. This paper introduces a comprehensive data collection and processing pipeline that constructs multimodal fact-checking datasets in French and German languages by aggregating ClaimReview feeds, scraping full debunking articles, normalizing heterogeneous claim verdicts, and enriching them with structured metadata and aligned visual content. We used state-of-the-art large language models (LLMs) and multimodal LLMs for (i) evidence extraction under predefined evidence categories and (ii) justification generation that links evidence to verdicts. Evaluation with G-Eval and human assessment demonstrates that our pipeline enables fine-grained comparison of fact-checking practices across different organizations or media markets, facilitates the development of more interpretable and evidence-grounded fact-checking models, and lays the groundwork for future research on multilingual, multimodal misinformation verification.

</details>


### [68] [VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding](https://arxiv.org/abs/2601.07986)
*Haorui Yu, Ramon Ruiz-Dolz, Diji Yang, Hang He, Fengrui Zhang, Qiufeng Yi*

**主要类别:** cs.CL

**AI概要:** VULCA-Bench是一个多文化艺术评论基准，用于评估视觉语言模型在文化理解方面的能力，超越了表面视觉感知。该基准包含7,410个图像-评论对，覆盖8种文化传统，支持中英双语。


<details>
  <summary>更多</summary>
  
**动机:** 现有VLM基准主要评估L1-L2能力（物体识别、场景描述和事实问答），而忽视了更高层次的文化解释能力，因此需要专门的文化理解评估基准。

**方法:** 采用五层框架（L1-L5，从视觉感知到哲学美学）来操作化文化理解，具体化为225个文化特定维度，并由专家撰写双语评论支持。

**结果:** 初步结果显示，高层推理（L3-L5）始终比视觉和技术分析（L1-L2）更具挑战性。

**结论:** VULCA-Bench填补了VLM文化理解评估的空白，数据集、评估脚本和标注工具已在补充材料中提供，采用CC BY 4.0许可。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是VULCA-Bench%3A+A+Multicultural+Vision-Language+Benchmark+for+Evaluating+Cultural+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07986&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.

</details>


### [69] [From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP](https://arxiv.org/abs/2601.07988)
*Adithya V Ganesan, Vasudha Varadarajan, Oscar NE Kjell, Whitney R Ringwald, Scott Feltman, Benjamin J Luft, Roman Kotov, Ryan L Boyd, H Andrew Schwartz*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个针对纵向研究中文本数据的建模和评估范式，通过四个关键改进来处理人员嵌套和时间排序的行为序列数据，相比传统NLP方法能提供更准确的生态效度评估。


<details>
  <summary>更多</summary>
  
**动机:** 传统NLP方法将文档视为独立无序样本，但在纵向研究中，文档嵌套在作者内部并按时间排序形成行为序列，这种假设不成立，需要专门的建模方法。

**方法:** 提出了包含四个改进的纵向建模范式：(1)按人员和时间的评估分割；(2)区分人际差异和个体内部动态的准确度指标；(3)默认包含历史信息的序列输入；(4)支持不同粒度潜在状态表示的模型内部结构。

**结果:** 在包含238名参与者17,000篇日记文本的数据集上验证，发现传统文档级评估与提出的生态有效建模方法相比，可能得出显著不同甚至相反的结论。

**结论:** 研究结果支持NLP从词序列评估向行为序列范式的转变，为纵向文本数据分析提供了更有效的框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Word+Sequences+to+Behavioral+Sequences%3A+Adapting+Modeling+and+Evaluation+Paradigms+for+Longitudinal+NLP，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07988&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While NLP typically treats documents as independent and unordered samples, in longitudinal studies, this assumption rarely holds: documents are nested within authors and ordered in time, forming person-indexed, time-ordered $\textit{behavioral sequences}$. Here, we demonstrate the need for and propose a longitudinal modeling and evaluation paradigm that consequently updates four parts of the NLP pipeline: (1) evaluation splits aligned to generalization over people ($\textit{cross-sectional}$) and/or time ($\textit{prospective}$); (2) accuracy metrics separating between-person differences from within-person dynamics; (3) sequence inputs to incorporate history by default; and (4) model internals that support different $\textit{coarseness}$ of latent state over histories (pooled summaries, explicit dynamics, or interaction-based models). We demonstrate the issues ensued by traditional pipeline and our proposed improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to our ecologically valid modeling and evaluation. We tie our results to a broader discussion motivating a shift from word-sequence evaluation toward $\textit{behavior-sequence}$ paradigms for NLP.

</details>


### [70] [DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs](https://arxiv.org/abs/2601.07994)
*Nayoung Choi, Jonathan Zhang, Jinho D. Choi*

**主要类别:** cs.CL

**AI概要:** DyCP是一种轻量级上下文管理方法，通过动态分割和检索相关记忆来解决长对话中LLM响应延迟增加和答案质量下降的问题，在多个基准测试中显著提升性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在长对话中会出现响应延迟增加和答案质量下降的问题，现有方法要么需要额外LLM调用来构建记忆，要么离线构建记忆而不考虑当前用户话语，导致效率低下或破坏对话连续性。

**方法:** DyCP方法在查询时动态分割和检索相关记忆，保留对话的顺序结构而无需预定义主题边界，支持高效、自适应的上下文检索。

**结果:** 在三个长对话基准测试（LoCoMo、MT-Bench+和SCM4LLMs）和多个LLM上，DyCP持续提高了答案质量同时减少了响应延迟。

**结论:** 研究还揭示了现代LLM扩展上下文窗口与其实际长上下文处理能力之间的差距，强调了有效上下文管理的持续重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DYCP%3A+Dynamic+Context+Pruning+for+Long-Form+Dialogue+with+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07994，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07994&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.

</details>


### [71] [Is Sentiment Banana-Shaped? Exploring the Geometry and Portability of Sentiment Concept Vectors](https://arxiv.org/abs/2601.07995)
*Laurits Lyngbaek, Pascale Feldkamp, Yuri Bizzoni, Kristoffer L. Nielbo, Kenneth Enevoldsen*

**主要类别:** cs.CL

**AI概要:** 该论文评估了概念向量投影(CVP)方法在情感分析中的跨领域可移植性和线性假设的有效性，发现CVP具有良好的跨语种和跨领域泛化能力，但其线性假设只是近似成立。


<details>
  <summary>更多</summary>
  
**动机:** 人文领域的情感分析应用需要情境化的连续情感评分，CVP方法通过将情感建模为嵌入空间中的方向来提供这种能力，但该方法在跨领域可移植性和基本假设方面的表现尚未得到充分研究。

**方法:** 研究评估了CVP方法在不同文体、历史时期、语言和情感维度上的表现，检验了基于一个语料库训练的概念向量在其他语料库上的迁移效果，并深入分析了CVP的线性假设。

**结果:** 研究发现，在一个语料库上训练的概念向量可以很好地迁移到其他语料库，性能损失很小，表明CVP具有很好的可移植性。但线性假设只是近似成立，指出了进一步改进的空间。

**结论:** CVP是一种有效的可移植方法，能够捕捉可泛化的情感模式，但其线性假设的近似性表明该方法仍有进一步发展的潜力，特别是在改进线性建模假设方面。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Is+Sentiment+Banana-Shaped%3F+Exploring+the+Geometry+and+Portability+of+Sentiment+Concept+Vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.07995，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07995&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Use cases of sentiment analysis in the humanities often require contextualized, continuous scores. Concept Vector Projections (CVP) offer a recent solution: by modeling sentiment as a direction in embedding space, they produce continuous, multilingual scores that align closely with human judgments. Yet the method's portability across domains and underlying assumptions remain underexplored. We evaluate CVP across genres, historical periods, languages, and affective dimensions, finding that concept vectors trained on one corpus transfer well to others with minimal performance loss. To understand the patterns of generalization, we further examine the linearity assumption underlying CVP. Our findings suggest that while CVP is a portable approach that effectively captures generalizable patterns, its linearity assumption is approximate, pointing to potential for further development.

</details>


### [72] [LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback](https://arxiv.org/abs/2601.08003)
*Weiyue Li, Mingxiao Song, Zhenda Shen, Dachuan Zhao, Yunfan Long, Yi Li, Yongce Li, Ruyi Yang, Mengyu Wang*

**主要类别:** cs.CL

**AI概要:** LLM Review框架通过盲审机制提升大语言模型的创造性生成，避免多智能体交互导致的内容同质化问题，在科幻写作任务中表现优于基线方法


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在创造性生成方面存在困难，而多智能体框架虽然能提升推理能力，但会导致内容同质化，反而阻碍创造力

**方法:** 提出LLM Review框架，采用盲审机制：智能体交换针对性反馈但独立修改，保持创意多样性。创建SciFi-100科幻写作数据集，结合LLM评分、人工标注和基于规则的新颖性指标进行综合评估

**结果:** 实验显示LLM Review持续优于多智能体基线方法，使用该框架的较小模型可以超越更大的单智能体模型

**结论:** 交互结构可能替代模型规模，通过适当的协作机制可以显著提升语言模型的创造性表现

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+Review%3A+Enhancing+Creative+Writing+via+Blind+Peer+Review+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08003&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.

</details>


### [73] [Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models](https://arxiv.org/abs/2601.08058)
*Zhenghao He, Guangzhi Xiong, Bohan Liu, Sanchit Sinha, Aidong Zhang*

**主要类别:** cs.CL

**AI概要:** 研究发现大语言模型的多步推理能力由潜在内部激活支持，通过稀疏自编码器识别出与推理相关的关键特征，单特征引导即可显著提升推理准确性，无需显式思维链提示。


<details>
  <summary>更多</summary>
  
**动机:** 探索思维链提示为何有效以及是否是大语言模型中触发推理的唯一机制，理解推理能力的内部工作机制。

**方法:** 使用稀疏自编码器(SAEs)直接分析和干预大语言模型的内部表示，识别与推理行为因果相关的潜在特征，并进行特征引导实验。

**结果:** 在多个模型系列和推理基准测试中，引导单个推理相关潜在特征可显著提高准确性；对大模型而言，潜在引导能达到标准思维链提示的相当性能且输出更高效；推理导向的内部状态在生成早期即被触发。

**结论:** 思维链提示是激活大语言模型推理机制的有效但非唯一方式，多步推理由可被外部激活的潜在内部激活支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Beyond+Chain-of-Thought%3A+A+Latent+Computational+Mode+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08058，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08058&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.

</details>


### [74] [Universal computation is intrinsic to language model decoding](https://arxiv.org/abs/2601.08061)
*Alex Lewandowski, Marlos C. Machado, Dale Schuurmans*

**主要类别:** cs.CL

**AI概要:** 论文证明语言模型通过自回归输出链可以实现通用计算，能模拟任何算法执行。研究发现即使是随机初始化的未训练语言模型也具备通用计算能力，训练主要提升的是程序可编程性而非计算表达能力。


<details>
  <summary>更多</summary>
  
**动机:** 探索语言模型的终极计算能力，澄清关于其计算表达能力的科学争论，理解训练对语言模型计算能力的影响。

**方法:** 通过理论证明语言模型的自回归输出链足以执行通用计算，并实验验证随机初始化未训练语言模型的通用计算能力。

**结果:** 证明了语言模型具备模拟任何算法执行的通用计算能力，且这种能力在训练前就已存在。

**结论:** 语言模型的训练主要改善程序可编程性（通过提示词访问内在能力），而非创造新的计算表达能力，为理解语言模型的计算本质提供了新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+computation+is+intrinsic+to+language+model+decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08061，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08061&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Language models now provide an interface to express and often solve general problems in natural language, yet their ultimate computational capabilities remain a major topic of scientific debate. Unlike a formal computer, a language model is trained to autoregressively predict successive elements in human-generated text. We prove that chaining a language model's autoregressive output is sufficient to perform universal computation. That is, a language model can simulate the execution of any algorithm on any input. The challenge of eliciting desired computational behaviour can thus be reframed in terms of programmability: the ease of finding a suitable prompt. Strikingly, we demonstrate that even randomly initialized language models are capable of universal computation before training. This implies that training does not give rise to computational expressiveness -- rather, it improves programmability, enabling a natural language interface for accessing these intrinsic capabilities.

</details>


### [75] [Calibration Is Not Enough: Evaluating Confidence Estimation Under Language Variations](https://arxiv.org/abs/2601.08064)
*Yuxi Xia, Dennis Ulmer, Terra Blevins, Yihong Liu, Hinrich Schütze, Benjamin Roth*

**主要类别:** cs.CL

**AI概要:** 本文提出了一个全面的大语言模型置信度估计评估框架，包含三个新维度：提示扰动鲁棒性、语义等效答案稳定性、语义差异答案敏感性，揭示了现有方法在这些实际应用相关指标上的不足。


<details>
  <summary>更多</summary>
  
**动机:** 现有置信度评估方法仅关注校准和区分度，忽视了LLM语境下的语言变体问题，无法评估置信度在语义等效提示/答案下的一致性以及在答案含义变化时的敏感性。

**方法:** 提出包含三个新维度的综合评估框架：1) 对提示扰动的鲁棒性；2) 对语义等效答案的稳定性；3) 对语义差异答案的敏感性。

**结果:** 研究表明，在传统校准和区分度指标上表现良好的常见置信度估计方法，在新提出的三个维度上往往表现不佳，缺乏对提示变化的鲁棒性和对答案变化的敏感性。

**结论:** 该框架揭示了现有置信度评估方法在真实应用场景中的局限性，为选择和设计更可靠的置信度估计方法提供了实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Calibration+Is+Not+Enough%3A+Evaluating+Confidence+Estimation+Under+Language+Variations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08064，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08064&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Confidence estimation (CE) indicates how reliable the answers of large language models (LLMs) are, and can impact user trust and decision-making. Existing work evaluates CE methods almost exclusively through calibration, examining whether stated confidence aligns with accuracy, or discrimination, whether confidence is ranked higher for correct predictions than incorrect ones. However, these facets ignore pitfalls of CE in the context of LLMs and language variation: confidence estimates should remain consistent under semantically equivalent prompt or answer variations, and should change when the answer meaning differs. Therefore, we present a comprehensive evaluation framework for CE that measures their confidence quality on three new aspects: robustness of confidence against prompt perturbations, stability across semantic equivalent answers, and sensitivity to semantically different answers. In our work, we demonstrate that common CE methods for LLMs often fail on these metrics: methods that achieve good performance on calibration or discrimination are not robust to prompt variations or are not sensitive to answer changes. Overall, our framework reveals limitations of existing CE evaluations relevant for real-world LLM use cases and provides practical guidance for selecting and designing more reliable CE methods.

</details>


### [76] [AdaJudge: Adaptive Multi-Perspective Judging for Reward Modeling](https://arxiv.org/abs/2601.08097)
*Yongliang Miao, Yangyang Liang, Mengnan Du*

**主要类别:** cs.CL

**AI概要:** AdaJudge是一个用于奖励建模的统一框架，通过门控精化块改进表示学习，并使用自适应多视图池化模块替代静态池化策略，显著提升了奖励模型的性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统奖励模型架构依赖静态池化策略将序列压缩为标量分数，存在两个关键限制：静态归纳偏差与任务依赖偏好信号不匹配，以及表示不匹配（主干网络为生成而非细粒度判别优化）。

**方法:** 提出AdaJudge框架，联合适应表示和聚合：1）通过门控精化块将主干表示精化为面向判别的空间；2）用自适应多视图池化模块替代静态读取，动态路由和组合证据。

**结果:** 在RM-Bench和JudgeBench上的广泛实验显示，AdaJudge优于强大的现成奖励模型和传统池化基线。

**结论:** AdaJudge通过自适应表示精化和动态池化策略有效解决了传统奖励模型的局限性，为对齐大语言模型与人类偏好提供了更有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AdaJudge%3A+Adaptive+Multi-Perspective+Judging+for+Reward+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08097，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08097&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reward modeling is essential for aligning large language models with human preferences, yet predominant architectures rely on a static pooling strategy to condense sequences into scalar scores. This paradigm, however, suffers from two key limitations: a static inductive bias that misaligns with task-dependent preference signals, and a representational mismatch, as the backbone is optimized for generation rather than fine-grained discrimination. To address this, we propose AdaJudge, a unified framework that jointly adapts representation and aggregation. AdaJudge first refines backbone representations into a discrimination-oriented space via gated refinement blocks. It then replaces the static readout with an adaptive multi-view pooling module that dynamically routes and combines evidence. Extensive experiments on RM-Bench and JudgeBench show that AdaJudge outperforms strong off-the-shelf reward models and traditional pooling baselines.

</details>


### [77] [Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning](https://arxiv.org/abs/2601.08105)
*Fabian Spaeh, Tianyi Chen, Chen-Hao Chiang, Bin Shen*

**主要类别:** cs.CL

**AI概要:** 本文提出了针对agentic RAG系统的查询建议方法，当用户提问超出知识范围时，系统能够推荐相似且可回答的查询，通过动态少样本学习技术提升建议质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有agentic RAG系统在用户提问超出知识范围时会产生幻觉问题，虽然防护框架可以阻止超出范围的问题，但缺乏为不回答的问题提供可回答查询建议的研究。

**方法:** 采用鲁棒动态少样本学习方法，从相关工作流中检索示例，系统可以自我学习（如基于历史用户查询），适用于实际应用。

**结果:** 在三个基准数据集上的实验表明，该方法相比少样本和仅检索基线，能产生更相关和可回答的建议，提升了与agentic RAG交互的安全性和有效性。

**结论:** 该方法通过查询建议增强了agentic RAG系统的用户交互体验，解决了超出知识范围查询的问题，为实际应用提供了有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Query+Suggestion+for+Retrieval-Augmented+Generation+via+Dynamic+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08105，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08105&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.
  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.

</details>


### [78] [Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought](https://arxiv.org/abs/2601.08108)
*Bowen Li, Ziqi Xu, Jing Ren, Renqiang Luo, Xikun Zhang, Xiuzhen Zhang, Yongli Ren, Feng Xia*

**主要类别:** cs.CL

**AI概要:** ACPS框架通过因果模型和简略思维链，在减少token使用的同时提升多任务推理的准确性和效率


<details>
  <summary>更多</summary>
  
**动机:** 现有提示方法如思维链存在token使用过多和跨任务泛化能力有限的问题

**方法:** 提出自适应因果提示框架ACPS，利用结构因果模型推断查询对答案的因果效应，自适应选择干预策略，并用简略思维链替代冗长思维链

**结果:** 在多个推理基准测试和LLM上，ACPS在准确性、鲁棒性和计算效率方面均优于现有基线方法

**结论:** ACPS框架通过因果推理和高效提示设计，实现了跨异构任务的通用推理能力，显著降低了计算成本

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Debiasing+Large+Language+Models+via+Adaptive+Causal+Prompting+with+Sketch-of-Thought，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08108，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08108&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.

</details>


### [79] [Attention Projection Mixing and Exogenous Anchors](https://arxiv.org/abs/2601.08131)
*Jonathan Su*

**主要类别:** cs.CL

**AI概要:** ExoFormer通过引入外部锚定投影解耦了Transformer中早期层的双重角色（稳定参考和计算优化），在各种注意力路径上实现了性能提升、数据效率提高和注意力下沉减少，但出现了表征坍缩现象。


<details>
  <summary>更多</summary>
  
**动机:** 解决传统Transformer中早期层需要同时作为稳定参考和有效计算块的根本矛盾

**方法:** 提出ExoFormer，学习专用的外部锚定投影，通过统一的归一化混合框架（包含元素级、头级、标量级等不同粒度）应用于所有注意力路径

**结果:** 动态变体下游准确率提升2.13%，数据效率提高1.84倍，注意力下沉减少2倍，但所有变体都出现表征坍缩

**结论:** 外部锚定能够保留关键token身份信息，使各层专注于计算优化，虽然出现表征坍缩但整体性能显著提升

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attention+Projection+Mixing+and+Exogenous+Anchors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08131，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08131&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Transformers that reuse early-layer attention projections as residuals face a fundamental tension: the first layer must simultaneously serve as a stable reference for all deeper layers and as an effective computational block. To resolve this, we propose ExoFormer, which learns dedicated exogenous anchor projections outside the sequential layer stack, decoupling the anchor role from computational refinement. Through a unified normalized mixing framework (studying different coefficient granularities: elementwise, headwise, scalar) across all attention pathways (queries, keys, values, and gate logits), ExoFormer variants consistently outperform their internal-anchor counterparts. Moreover, the dynamic variant achieves a 2.13-point increase in downstream accuracy over the baseline and demonstrates superior data efficiency, matching baseline validation loss with 1.84x fewer tokens. ExoFormer also achieves a 2x reduction in attention sink compared to standard Gated Attention. Paradoxically, all ExoFormer variants exhibit signs of representation collapse. We explain this via an Offloading Hypothesis: external anchors preserve essential token identity, allowing layers to specialize exclusively in computational refinement. We release codes and models to facilitate future research.

</details>


### [80] [How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains](https://arxiv.org/abs/2601.08134)
*Reza Khanmohammadi, Erfan Miahi, Simerjot Kaur, Ivan Brugere, Charese H. Smiley, Kundan Thind, Mohammad M. Ghassemi*

**主要类别:** cs.CL

**AI概要:** 该论文提出了RMCB基准测试，用于评估大型推理模型的置信度校准问题，发现文本编码器在区分能力上表现最佳，而结构感知模型在校准方面最优，但没有任何方法能在两方面都占优。


<details>
  <summary>更多</summary>
  
**动机:** 大型推理模型(LRMs)的错误校准在高风险领域中降低了其可靠性，需要准确估计其长格式多步输出的置信度方法。

**方法:** 构建了RMCB基准测试，包含347,496个推理轨迹，涵盖临床、金融、法律和数学推理等多个高风险领域。评估了十多种基于表示的方法，包括序列、基于图和文本的架构。

**结果:** 发现区分能力(AUROC)和校准(ECE)之间存在持续权衡：文本编码器获得最佳AUROC(0.672)，结构感知模型获得最佳ECE(0.148)，没有单一方法在两方面都占优。架构复杂性增加并不总能超越简单的序列基线。

**结论:** 这项工作为该任务提供了最全面的基准测试，建立了严格的基线，并证明了当前基于表示的方法的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Reliable+are+Confidence+Estimators+for+Large+Reasoning+Models%3F+A+Systematic+Benchmark+on+High-Stakes+Domains，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08134&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The miscalibration of Large Reasoning Models (LRMs) undermines their reliability in high-stakes domains, necessitating methods to accurately estimate the confidence of their long-form, multi-step outputs. To address this gap, we introduce the Reasoning Model Confidence estimation Benchmark (RMCB), a public resource of 347,496 reasoning traces from six popular LRMs across different architectural families. The benchmark is constructed from a diverse suite of datasets spanning high-stakes domains, including clinical, financial, legal, and mathematical reasoning, alongside complex general reasoning benchmarks, with correctness annotations provided for all samples. Using RMCB, we conduct a large-scale empirical evaluation of over ten distinct representation-based methods, spanning sequential, graph-based, and text-based architectures. Our central finding is a persistent trade-off between discrimination (AUROC) and calibration (ECE): text-based encoders achieve the best AUROC (0.672), while structurally-aware models yield the best ECE (0.148), with no single method dominating both. Furthermore, we find that increased architectural complexity does not reliably outperform simpler sequential baselines, suggesting a performance ceiling for methods relying solely on chunk-level hidden states. This work provides the most comprehensive benchmark for this task to date, establishing rigorous baselines and demonstrating the limitations of current representation-based paradigms.

</details>


### [81] [Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training](https://arxiv.org/abs/2601.08141)
*Muhammad Taimoor Hassan, Jawad Ahmed, Muhammad Awais*

**主要类别:** cs.CL

**AI概要:** Qalb是一个针对乌尔都语优化的语言模型，通过两阶段训练方法在LLaMA 3.1 8B基础上开发，在乌尔都语NLP任务上取得了最先进的性能表现。


<details>
  <summary>更多</summary>
  
**动机:** 乌尔都语作为拥有2.3亿使用者的语言，在现代NLP系统中代表性严重不足，现有多语言模型在处理乌尔都语的复杂形态、Nastaliq文字和丰富文学传统时表现不佳。

**方法:** 采用两阶段方法：1) 在19.7亿token的乌尔都语语料库上进行持续预训练；2) 使用Alif Urdu-instruct数据集进行监督微调。语料库包含新闻、文学、政府文件和社交媒体文本，并加入英文数据防止灾难性遗忘。

**结果:** Qalb在乌尔都语基准测试中获得90.34的加权平均分，比之前最先进的Alif-1.0-Instruct模型高出3.24分，比基础LLaMA-3.1 8B-Instruct模型高出44.64分，在7个不同任务中均达到最先进性能。

**结论:** 通过在多样化高质量语言数据上进行持续预训练，并结合有针对性的指令微调，可以有效地将基础模型适配到低资源语言，为乌尔都语NLP发展提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Qalb%3A+Largest+State-of-the-Art+Urdu+Large+Language+Model+for+230M+Speakers+with+Systematic+Continued+Pre-training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08141&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.

</details>


### [82] [Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning](https://arxiv.org/abs/2601.08146)
*Khumaisa Nur'aini, Ayu Purwarianti, Alham Fikri Aji, Derry Wijaya*

**主要类别:** cs.CL

**AI概要:** CT-SFT是一种针对低资源语言的LLM适应方法，通过识别任务相关注意力头并仅更新这些头部来减少参数更新量，提高跨语言准确性并减少灾难性遗忘


<details>
  <summary>更多</summary>
  
**动机:** 低资源语言适应面临标注数据稀缺、全模型微调不稳定、跨语言持续调优导致灾难性遗忘等问题

**方法:** 使用标签平衡均值基线和任务方向相关性评分识别代理语言检查点中的任务相关注意力头，然后通过头部级梯度掩码仅更新这些头部和LayerNorm参数

**结果:** 在NusaX-Senti和XNLI任务上，CT-SFT相比持续全微调提高了跨语言准确性，同时只更新少量参数，并显著减少灾难性遗忘

**结论:** CT-SFT提供了一种编辑保持的权衡策略，在保持源语言能力的同时实现有效的跨语言迁移，为低资源语言适应提供了高效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mechanisms+are+Transferable%3A+Data-Efficient+Low-Resource+Adaptation+via+Circuit-Targeted+Supervised+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08146&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Adapting LLMs to low-resource languages is difficult: labeled data is scarce, full-model fine-tuning is unstable, and continued cross-lingual tuning can cause catastrophic forgetting. We propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT): a counterfactual-free adaptation of CD-T (Contextual Decomposition Transformer) that uses a label-balanced mean baseline and task-directional relevance scoring to identify a sparse set of task-relevant attention heads in a proxy-language checkpoint, then transfer learns to a target language by updating only those heads (plus LayerNorm) via head-level gradient masking. Across NusaX-Senti and XNLI, CT-SFT improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of model parameters. We find an editing-preserving trade-off: harder transfers favor editing circuit heads, while easier transfers often favor near-zero (i.e., low-relevance heads) updates, preserving the source mechanism. CT-SFT also substantially reduces catastrophic forgetting, preserving proxy/source-language competence during transfer.

</details>


### [83] [WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents](https://arxiv.org/abs/2601.08158)
*Yuqing Zhou, Zhuoer Wang, Jie Yuan, Hong Wang, Samson Koelle, Ziwei Zhu, Wei Niu*

**主要类别:** cs.CL

**AI概要:** WISE-Flow是一个面向用户服务的自进化代理框架，通过将历史服务交互转换为可重用的程序化经验，提升LLM代理在新任务中的表现并减少错误重复。


<details>
  <summary>更多</summary>
  
**动机:** 基于大语言模型的代理在用户服务中广泛部署，但在新任务中容易出错，倾向于重复相同的失败模式，且运行间变异性大。通过环境特定训练或手动修补来修复失败成本高且难以扩展。

**方法:** 提出WISE-Flow工作流中心框架，将历史服务交互转换为具有先决条件增强动作块的可重用程序化经验。在部署时，将代理执行轨迹与检索到的工作流对齐，并进行先决条件感知的可行性推理以实现状态接地化的下一步动作。

**结果:** 在ToolSandbox和τ²-bench上的实验显示，该方法在不同基础模型上均取得了持续改进。

**结论:** WISE-Flow框架能够有效提升LLM代理在用户服务环境中的性能和可靠性，实现自进化能力，减少错误并提高任务执行的一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WISE-Flow%3A+Workflow-Induced+Structured+Experience+for+Self-Evolving+Conversational+Service+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08158&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $τ^2$-bench show consistent improvement across base models.

</details>


### [84] [SwiftMem: Fast Agentic Memory via Query-aware Indexing](https://arxiv.org/abs/2601.08160)
*Anxin Tian, Yiming Li, Xing Li, Hui-Ling Zhen, Lei Chen, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan*

**主要类别:** cs.CL

**AI概要:** SwiftMem是一个查询感知的智能体记忆系统，通过时间索引和语义DAG-Tag索引实现次线性检索，解决了现有记忆系统全量检索导致的延迟问题，搜索速度比现有方法快47倍。


<details>
  <summary>更多</summary>
  
**动机:** 现有智能体记忆系统存在根本性限制：无论查询特性如何，都会在整个存储层执行全量检索。这种暴力方法随着内存增长会造成严重的延迟瓶颈，阻碍实时智能体交互。

**方法:** 提出SwiftMem系统：1) 时间索引实现对数时间范围查询；2) 语义DAG-Tag索引通过层次化标签结构将查询映射到相关主题；3) 嵌入-标签协同整合机制基于语义聚类重组存储以改善缓存局部性。

**结果:** 在LoCoMo和LongMemEval基准测试中，SwiftMem相比最先进的基线方法实现了47倍的搜索加速，同时保持了竞争性的准确度。

**结论:** SwiftMem通过专门的索引技术解决了智能体记忆系统的检索效率问题，使得内存增强的LLM智能体能够实际部署应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwiftMem%3A+Fast+Agentic+Memory+via+Query-aware+Indexing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08160&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.

</details>


### [85] [Relational Knowledge Distillation Using Fine-tuned Function Vectors](https://arxiv.org/abs/2601.08169)
*Andrea Kang, Yingnian Wu, Hongjing Lu*

**主要类别:** cs.CL

**AI概要:** 通过微调函数向量（约20个词对）和创建复合函数向量，显著提升了语言模型在关系推理任务上的表现，增强了可解释性和推理能力


<details>
  <summary>更多</summary>
  
**动机:** 概念间的关系表示是智能系统理解世界的核心前提，现有因果中介分析提取的函数向量在关系推理任务上有改进空间

**方法:** 使用少量示例微调函数向量，创建复合函数向量作为加权组合，在推理时通过激活修补机制插入到LLM激活中

**结果:** 微调后的函数向量在关系词补全任务上表现更好，解码性能提升，与人类语义关系判断更一致；复合向量显著提高了认知科学和SAT类比问题的解决能力

**结论:** 激活修补作为一种可控机制，在编码和操作关系知识方面具有巨大潜力，能够同时提升大语言模型的可解释性和推理能力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Relational+Knowledge+Distillation+Using+Fine-tuned+Function+Vectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08169，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08169&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.

</details>


### [86] [Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering](https://arxiv.org/abs/2601.08176)
*Lavanya Prahallad, Sai Utkarsh Choudarypally, Pragna Prahallad, Pranathi Prahallad*

**主要类别:** cs.CL

**AI概要:** 本研究探讨提示工程对LLM政治问答清晰度自动评估的影响，发现GPT-5.2在思维链和少样本提示下，清晰度准确率从56%提升至63%，但细粒度回避检测仍具挑战性。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究虽然提供了清晰度和回避性的人工标注数据，但提示设计对自动清晰度评估的影响尚未充分探索，特别是在政治问答这种需要高清晰度的领域。

**方法:** 使用SemEval 2026的CLARITY数据集，比较GPT-3.5基线与GPT-5.2在三种提示策略下的表现：简单提示、思维链提示、带少样本的思维链提示，使用准确率和分层精确匹配等指标评估。

**结果:** GPT-5.2在清晰度预测上始终优于GPT-3.5基线，思维链少样本提示使准确率从56%提升至63%；思维链提示在回避检测上达到34%的最高准确率；基于推理的提示将主题识别准确率从60%提升至74%。

**结论:** 提示设计能可靠提升高层次清晰度评估效果，但尽管使用了结构化推理提示，细粒度回避检测和主题识别仍然是挑战性任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prompt-Based+Clarity+Evaluation+and+Topic+Detection+in+Political+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08176，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08176&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.

</details>


### [87] [Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis](https://arxiv.org/abs/2601.08196)
*Da Song, Yuheng Huang, Boqi Chen, Tianshuo Cong, Randy Goebel, Lei Ma, Foutse Khomh*

**主要类别:** cs.CL

**AI概要:** 论文提出了LogiSafetyGen框架和LogiSafetyBench基准测试，用于评估大型语言模型在自主执行任务时是否能够遵守隐含的监管安全约束，发现大模型虽然功能正确性更好，但经常为了完成任务而违反安全规则。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试往往忽略隐式监管合规性，无法评估LLMs是否能够自主执行强制性安全约束，特别是在高风险领域中需要严格遵循监管标准。

**方法:** 开发LogiSafetyGen框架，将非结构化监管规则转换为线性时序逻辑预言机，采用逻辑引导的模糊测试合成安全关键的有效轨迹，并构建包含240个人工验证任务的LogiSafetyBench基准。

**结果:** 对13个最先进LLMs的评估显示，更大的模型虽然功能正确性更好，但经常优先考虑任务完成而忽视安全，导致不合规行为。

**结论:** 当前LLMs在自主执行任务时存在安全合规性缺陷，需要开发更好的方法来确保模型在高风险领域中严格遵守监管标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Implicit+Regulatory+Compliance+in+LLM+Tool+Invocation+via+Logic-Guided+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08196&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. However, existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LogiSafetyGen, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LogiSafetyBench, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, which results in non-compliant behavior.

</details>


### [88] [Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs](https://arxiv.org/abs/2601.08198)
*Yibo Wang, Hai-Long Sun, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Lijun Zhang*

**主要类别:** cs.CL

**AI概要:** T-SPIN是一种改进的自对弈微调方法，通过引入历史优势比较和熵约束来解决SPIN方法的不稳定性和训练-生成不一致问题，在标注数据稀缺的情况下仅需25%样本即可达到或超越监督微调效果。


<details>
  <summary>更多</summary>
  
**动机:** SPIN方法在迭代过程中当前奖励优势会逐渐消失导致优化不稳定，且参考策略的使用造成训练奖励公式与生成度量之间的不对齐问题。

**方法:** 提出三元组自对弈微调(T-SPIN)，包含两个关键设计：1) 引入历史优势比较(当前响应与初始策略生成的原型合成响应)；2) 在自对弈框架中加入熵约束实现无参考微调。

**结果:** 在各种任务上的实验表明，T-SPIN不仅性能优于SPIN，且在迭代过程中保持稳定演化。仅使用25%样本即可达到或超越监督微调的性能。

**结论:** T-SPIN通过历史优势稳定化和熵约束的无参考训练，有效解决了SPIN的局限性，在标注数据稀缺的场景下表现出色，为语言模型微调提供了更稳定高效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Triplets+Better+Than+Pairs%3A+Towards+Stable+and+Effective+Self-Play+Fine-Tuning+for+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08198，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08198&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated responses over synthetic responses at hand, which may gradually vanish during iterations, leading to unstable optimization. Moreover, the utilization of reference policy induces a misalignment issue between the reward formulation for training and the metric for generation. To address these limitations, we propose a novel Triplet-based Self-Play fIne-tuNing (T-SPIN) method that integrates two key designs. First, beyond current advantages, T-SPIN additionally incorporates historical advantages between iteratively generated responses and proto-synthetic responses produced by the initial policy. Even if the current advantages diminish, historical advantages remain effective, stabilizing the overall optimization. Second, T-SPIN introduces the entropy constraint into the self-play framework, which is theoretically justified to support reference-free fine-tuning, eliminating the training-generation discrepancy. Empirical results on various tasks demonstrate not only the superior performance of T-SPIN over SPIN, but also its stable evolution during iterations. Remarkably, compared to supervised fine-tuning, T-SPIN achieves comparable or even better performance with only 25% samples, highlighting its effectiveness when faced with scarce annotated data.

</details>


### [89] [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://arxiv.org/abs/2601.08209)
*Rongji Li, Jian Xu, Xueqing Chen, Yisheng Yang, Jiayi Wang, Xingyu Chen, Chunyu Xie, Dawei Leng, Xu-Yao Zhang*

**主要类别:** cs.CL

**AI概要:** 论文提出了Generation-Augmented Generation (GAG)方法，通过将私有专业知识作为额外专家模态与冻结的基础模型对齐，解决了传统微调和RAG方法在私有知识注入中的问题，在保持通用能力的同时显著提升专业领域性能。


<details>
  <summary>更多</summary>
  
**动机:** 在生物医学、材料和金融等高风险领域部署大语言模型时，需要注入私有、领域特定的知识，但现有方法存在明显缺陷：微调迭代成本高且容易导致灾难性遗忘，而检索增强生成(RAG)在专业语料库中表现脆弱。

**方法:** 提出GAG方法，将私有专业知识视为额外的专家模态，通过紧凑的表征级接口与冻结的基础模型对齐，避免了提示时的证据序列化，实现了即插即用的专业化和可扩展的多领域组合。

**结果:** 在两个私有科学QA基准测试（免疫学佐剂和催化材料）中，GAG分别比强RAG基线提升了15.34%和14.86%的性能，同时在六个开放通用基准上保持性能，并实现了接近oracle的选择性激活能力。

**结论:** GAG提供了一种有效的方法来解决私有知识注入的挑战，在保持模型通用能力的同时显著提升专业领域性能，支持可扩展的多领域部署。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generation-Augmented+Generation%3A+A+Plug-and-Play+Framework+for+Private+Knowledge+Injection+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08209，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08209&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.

</details>


### [90] [Towards Principled Design of Mixture-of-Experts Language Models under Memory and Inference Constraints](https://arxiv.org/abs/2601.08215)
*Seng Pei Liew, Kenta Shinzato, Yuyang Dong*

**主要类别:** cs.CL

**AI概要:** MoE语言模型的性能主要由总参数量(N_total)和专家稀疏度(s)决定，而非仅考虑总参数和激活参数。研究发现更大的专家数量会略微降低性能，因此提出设计原则：在约束条件下最大化总参数、最小化稀疏度和专家数量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的MoE模型设计仅基于总参数和激活参数，但研究发现这两个因素不足以描述最优架构，需要更深入理解性能决定因素。

**方法:** 通过系统性研究，分析MoE性能与总参数(N_total)、专家数量(n_exp)、top-k专家数(n_topk)和专家稀疏度(s=n_exp/n_topk)之间的关系。

**结果:** 发现MoE性能主要取决于总参数和专家稀疏度，专家数量与top-k专家数不能简单抵消，更大的专家数量会因降低核心模型维度而轻微损害性能。

**结论:** 提出MoE设计原则：在给定约束下最大化总参数，同时最小化专家稀疏度和专家数量，为MoE架构设计提供了明确框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Principled+Design+of+Mixture-of-Experts+Language+Models+under+Memory+and+Inference+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08215&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modern Mixture-of-Experts (MoE) language models are designed based on total parameters (memory footprint) and active parameters (inference cost). However, we find these two factors alone are insufficient to describe an optimal architecture. Through a systematic study, we demonstrate that MoE performance is primarily determined by total parameters ($N_{total}$) and expert sparsity ($s:=n_{exp}/n_{topk}$).
  Moreover, $n_{exp}$ and $n_{topk}$ do not "cancel out" within the sparsity ratio; instead, a larger total number of experts slightly penalizes performance by forcing a reduction in core model dimensions (depth and width) to meet memory constraints. This motivates a simple principle for MoE design which maximizes $N_{total}$ while minimizing $s$ (maximizing $n_{topk}$) and $n_{exp}$ under the given constraints. Our findings provide a robust framework for resolving architectural ambiguity and guiding MoE design.

</details>


### [91] [User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale](https://arxiv.org/abs/2601.08225)
*Jungho Cho, Minbyul Jeong, Sungrae Park*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种用户导向的模拟框架，用于生成更真实的多轮工具使用对话数据，解决现有方法只能产生简单任务解决轨迹的问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大规模推理模型工具使用数据集受限于静态预定义工具集，无法适应开放人机协作的复杂性，且纯任务导向的设计导致对话轮次过少，与真实场景不符。

**方法:** 开发了自动化任务导向多轮对话生成框架，采用基于LRM的模拟器动态生成领域特定工具。转向用户导向模拟范式，通过解耦任务生成和专用用户模拟器（模拟人类行为规则如增量请求和逐轮反馈）来生成更真实的多轮对话。

**结果:** 创建了一个多功能即插即用生成管道，能够从任何状态启动生成，确保产生扩展工具使用数据的高可扩展性。支持单轨迹内多个任务完成，生成高密度数据集。

**结论:** 用户导向的模拟方法能够生成更真实反映现实世界人机交互迭代性质的多轮对话数据，解决了纯任务导向方法的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是User-Oriented+Multi-Turn+Dialogue+Generation+with+Tool+Use+at+scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08225&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.

</details>


### [92] [Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning](https://arxiv.org/abs/2601.08267)
*Fan Gao, Sherry T. Tong, Jiwoong Sohn, Jiahao Huang, Junfeng Jiang, Ding Xia, Piyalitt Ittichaiwong, Kanyakorn Veerakanjana, Hyunjae Kim, Qingyu Chen, Edison Marrese Taylor, Kazuma Kobayashi, Akkiko Aizawa, Irene Li*

**主要类别:** cs.CL

**AI概要:** Med-CoReasoner是一个多语言医疗推理框架，通过英语和本地语言的并行推理、概念抽象和知识整合，显著提升非英语医疗推理性能，在低资源语言中表现尤为突出。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型在英语医疗任务上表现良好，但在其他语言中存在显著的多语言差距，限制了全球医疗公平部署，需要解决非英语医疗推理能力薄弱的问题。

**方法:** 提出Med-CoReasoner框架：1)并行英语和本地语言推理；2)将推理抽象为结构化概念；3)通过概念级对齐和检索将本地临床知识整合到英语逻辑框架中；4)构建MultiMed-X多语言基准进行评估。

**结果:** 在三个基准测试中，Med-CoReasoner平均提升多语言推理性能5%，在低资源语言中提升幅度更大。模型蒸馏和专家评估证实其产生临床合理且文化相关的推理轨迹。

**结论:** 该框架成功结合了英语推理的结构稳健性和本地语言的实践专业知识，为弥合多语言医疗推理差距提供了有效解决方案，具有重要的全球医疗公平应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Med-CoReasoner%3A+Reducing+Language+Disparities+in+Medical+Reasoning+via+Language-Informed+Co-Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08267&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.

</details>


### [93] [Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees](https://arxiv.org/abs/2601.08274)
*Kun Li, Zenan Xu, Junan Li, Zengrui Jin, Jinghao Deng, Zexuan Qiu, Bo Zhou*

**主要类别:** cs.CL

**AI概要:** DART是一个强化学习框架，通过动态rollout树在长链推理中自动发现和强化工具使用机会，无需人工标注，在复杂基准测试中表现优异


<details>
  <summary>更多</summary>
  
**动机:** 当前工具集成推理范式缺乏训练数据，且在长链思维过程中整合工具使用而不损害模型内在推理能力存在挑战

**方法:** 构建动态rollout树探索工具使用机会，通过基于树的优势估计识别和强化对解决方案有积极贡献的工具调用子轨迹

**结果:** 在AIME和GPQA-Diamond等挑战性基准测试中显著优于现有方法，成功协调工具执行与长链推理

**结论:** DART框架有效解决了工具集成到长链推理中的挑战，实现了无需人工标注的自发工具使用，为LLM增强计算能力提供了新途径

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discovery+and+Reinforcement+of+Tool-Integrated+Reasoning+Chains+via+Rollout+Trees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08274&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.

</details>


### [94] [D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning](https://arxiv.org/abs/2601.08282)
*Kangcheng Luo, Tinglang Wu, Yansong Feng*

**主要类别:** cs.CL

**AI概要:** D²Plan：一种双代理动态全局规划范式，通过Reasoner和Purifier的协作解决检索增强LLMs在复杂推理任务中的搜索链构建失败和推理劫持问题，提升多步推理的连贯性和抗干扰能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的检索增强LLMs在累积上下文被关键证据和无关信息淹没时面临两个关键失败模式：(1)无效的搜索链构建，产生错误查询或遗漏关键信息检索；(2)外围证据导致的推理劫持，使模型将干扰项误认为有效证据。

**方法:** 提出D²Plan双代理动态全局规划范式：Reasoner在推理过程中构建显式全局计划并根据检索反馈动态调整；Purifier评估检索相关性并为Reasoner压缩关键信息。采用两阶段训练框架：基于合成轨迹的监督微调冷启动和面向规划奖励的强化学习。

**结果:** 大量实验表明，D²Plan能够实现更连贯的多步推理和更强的抗无关信息干扰能力，在具有挑战性的QA基准测试中取得优异性能。

**结论:** D²Plan通过双代理协作和动态规划机制有效解决了检索增强推理中的关键问题，为复杂多跳推理任务提供了更可靠的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是D%24%5E2%24Plan%3A+Dual-Agent+Dynamic+Global+Planning+for+Complex+Retrieval-Augmented+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08282&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.

</details>


### [95] [Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques](https://arxiv.org/abs/2601.08302)
*Marvin Schmitt, Anne Schwerk, Sebastian Lempert*

**主要类别:** cs.CL

**AI概要:** 研究通过提示工程提升GPT-4o-mini和gemini-1.5-flash在情感分析任务中的表现，发现不同提示策略对不同模型和任务效果各异，few-shot在GPT-4o-mini表现最佳，chain-of-thought在gemini-1.5-flash的讽刺检测中提升46%。


<details>
  <summary>更多</summary>
  
**动机:** 探索如何通过高级提示工程技术来增强大语言模型在情感分析任务中的性能，包括情感分类、方面情感分析和微妙情感（如讽刺）检测。

**方法:** 使用few-shot学习、chain-of-thought提示和self-consistency等高级提示技术，与基线方法对比。评估指标包括准确率、召回率、精确率和F1分数。

**结果:** 高级提示技术显著提升情感分析性能：few-shot方法在GPT-4o-mini中表现最佳，chain-of-thought提示使gemini-1.5-flash的讽刺检测性能提升高达46%。

**结论:** 提示策略需要根据具体模型架构和任务语义复杂度进行定制，提示设计应与LLM架构和任务复杂性相匹配才能获得最佳效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Sentiment+Classification+and+Irony+Detection+in+Large+Language+Models+through+Advanced+Prompt+Engineering+Techniques，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08302，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08302&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.

</details>


### [96] [AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture](https://arxiv.org/abs/2601.08308)
*Bo Yang, Yu Zhang, Yunkui Chen, Lanfei Feng, Xiao Xu, Nueraili Aierken, Shijian Li*

**主要类别:** cs.CL

**AI概要:** AgriAgent是一个针对农业场景的两级智能代理框架，通过分层执行策略处理不同复杂度的任务：简单任务由模态特定代理直接处理，复杂任务通过合约驱动的规划机制进行多步骤可验证执行。


<details>
  <summary>更多</summary>
  
**动机:** 现有智能代理系统采用统一执行范式，难以处理农业环境中任务复杂度差异大和工具不完整的挑战。

**方法:** 采用两级代理框架：简单任务通过模态特定代理直接推理处理，复杂任务通过合约驱动规划机制，将任务分解为能力需求，进行能力感知的工具编排和动态工具生成，支持失败恢复。

**结果:** 实验结果表明，AgriAgent在复杂任务上比依赖统一执行范式的基准方法具有更高的执行成功率和鲁棒性。

**结论:** AgriAgent通过分层执行策略有效解决了农业场景中任务复杂度差异和工具不完整的问题，为实际农业应用提供了更可靠的智能代理解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgriAgent%3A+Contract-Driven+Planning+and+Capability-Aware+Tool+Orchestration+in+Real-World+Agriculture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08308，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08308&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.

</details>


### [97] [CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark](https://arxiv.org/abs/2601.08331)
*Daniil Gurgurov, Yusser Al Ghussin, Tanja Baeumel, Cheng-Ting Chou, Patrick Schramowski, Marius Mosbach, Josef van Genabith, Simon Ostermann*

**主要类别:** cs.CL

**AI概要:** CLaS-Bench是一个用于评估多语言引导技术的轻量级基准测试，涵盖32种语言，通过语言控制和语义相关性两个维度来量化引导效果，发现基于残差流的DiffMean方法表现最佳。


<details>
  <summary>更多</summary>
  
**动机:** 当前缺乏专门用于量化语言模型引导技术效果的基准测试和评估协议，需要系统评估多语言引导方法。

**方法:** 引入CLaS-Bench基准测试，评估多种引导技术（包括残差流DiffMean干预、探针导出方向、语言特定神经元、PCA/LDA向量、稀疏自编码器等），通过语言控制和语义相关性两个维度进行测量。

**结果:** DiffMean方法在所有语言中表现最优；语言特定结构主要在后期层出现；引导方向按语系聚类。

**结论:** CLaS-Bench是首个多语言引导标准化基准，为语言表示的科学分析和低成本适应替代方案提供实用评估工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLaS-Bench%3A+A+Cross-Lingual+Alignment+and+Steering+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08331&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question benchmark for evaluating language-forcing behavior in LLMs across 32 languages, enabling systematic evaluation of multilingual steering methods. We evaluate a broad array of steering techniques, including residual-stream DiffMean interventions, probe-derived directions, language-specific neurons, PCA/LDA vectors, Sparse Autoencoders, and prompting baselines. Steering performance is measured along two axes: language control and semantic relevance, combined into a single harmonic-mean steering score. We find that across languages simple residual-based DiffMean method consistently outperforms all other methods. Moreover, a layer-wise analysis reveals that language-specific structure emerges predominantly in later layers and steering directions cluster based on language family. CLaS-Bench is the first standardized benchmark for multilingual steering, enabling both rigorous scientific analysis of language representations and practical evaluation of steering as a low-cost adaptation alternative.

</details>


### [98] [Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue](https://arxiv.org/abs/2601.08342)
*Run Chen, Wen Liang, Ziwei Gong, Lin Ai, Julia Hirschberg*

**主要类别:** cs.CL

**AI概要:** 首个针对语音对话中心理操纵检测的研究，通过合成语音数据集SPEECHMENTALMANIP发现，音频模态下模型和人类检测者的召回率显著低于文本模态，揭示了声学和韵律线索对操纵检测的重要性


<details>
  <summary>更多</summary>
  
**动机:** 心理操纵作为计算社会推理的新兴任务，先前研究仅关注文本对话，忽略了操纵策略在语音中的表现形式，需要研究多模态环境下的操纵检测

**方法:** 创建合成多说话者基准SPEECHMENTALMANIP，将基于文本的数据集转换为高质量、声音一致的语音音频；使用少样本大型音频-语言模型和人工标注评估模态对检测准确性和感知的影响

**结果:** 模型在语音检测中表现出高特异性但召回率显著低于文本，表明对训练中缺失的声学或韵律线索敏感；人类评分者在音频设置中也表现出类似的不确定性

**结论:** 研究强调了多模态对话系统中需要模态感知评估和安全对齐，揭示了操纵性语音的固有模糊性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+Mental+Manipulation+in+Speech+via+Synthetic+Multi-Speaker+Dialogue，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08342&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.

</details>


### [99] [PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors](https://arxiv.org/abs/2601.08402)
*Donya Rooein, Sankalan Pal Chowdhury, Mariia Eremeeva, Yuan Qin, Debora Nozza, Mrinmaya Sachan, Dirk Hovy*

**主要类别:** cs.CL

**AI概要:** 该研究构建了一个将教学方法与学生人格特征相匹配的分类框架，让LLM导师能根据学生个性调整教学策略，经人类教师评估显示该方法优于基线方法，并促进了角色扮演等高影响力策略的使用。


<details>
  <summary>更多</summary>
  
**动机:** 当前的大型语言模型(LLM)教育导师系统没有考虑学生的人格特质，而不同的教学策略对不同个性的学生效果不同，不匹配的策略可能对学生学习产生负面影响。

**方法:** 基于教育学文献构建了教学方法与人格特征关联的分类法，通过模拟师生对话让LLM导师根据模拟的学生个性调整教学策略，并与人类教师一起进行评估。

**结果:** 人类教师一致认为该方法优于两个基线方法，该方法还增加了角色扮演等不常见但高影响力策略的使用频率，人类和LLM标注者都显著偏好这些策略。

**结论:** 这项研究为在教育应用中开发更个性化和有效的LLM使用方法铺平了道路，证明了基于人格特征调整教学策略的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PATS%3A+Personality-Aware+Teaching+Strategies+with+Large+Language+Model+Tutors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08402，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08402&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.

</details>


### [100] [Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering](https://arxiv.org/abs/2601.08427)
*Nonghai Zhang, Weitao Ma, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Jingwen Xu*

**主要类别:** cs.CL

**AI概要:** Latent-GRPO是一个通过潜在空间几何特征直接获取内在奖励的框架，无需依赖昂贵的外部验证器，实现了2倍以上的训练加速，同时保持模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Group Relative Policy Optimization (GRPO)方法依赖昂贵的外部验证器或人工规则，导致计算成本高、训练延迟大，且稀疏奖励阻碍优化效率。

**方法:** 提出Latent-GRPO框架，基于发现正确推理轨迹的终端token表示形成密集聚类而错误轨迹为异常点的几何特性，开发了Iterative Robust Centroid Estimation (IRCE)算法，通过球面投影缓解幅度波动并迭代聚合估计稳健的"真值质心"。

**结果:** 在多个数据集上的实验显示，该方法在保持模型性能的同时实现了超过2倍的训练加速，并展现出强大的泛化能力和鲁棒性。

**结论:** Latent-GRPO通过潜在空间几何特征有效解决了GRPO对昂贵外部验证器的依赖问题，显著提升了训练效率和优化性能，具有良好的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Silence+the+Judge%3A+Reinforcement+Learning+with+Self-Verifier+via+Latent+Geometric+Clustering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08427，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08427&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.

</details>


### [101] [Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management](https://arxiv.org/abs/2601.08435)
*Weitao Ma, Xiaocheng Feng, Lei Huang, Xiachong Feng, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Bing Qin*

**主要类别:** cs.CL

**AI概要:** Fine-Mem是一个用于大语言模型代理的细粒度记忆管理框架，通过分块级步骤奖励和证据锚定奖励分配来解决现有强化学习方法中的奖励稀疏性问题，显著提升了长期任务中的记忆管理效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有基于强化学习的记忆管理方法主要依赖最终任务性能作为奖励，导致严重的奖励稀疏性和信用分配无效问题，无法为单个记忆操作提供充分指导。

**方法:** 提出Fine-Mem统一框架：1)引入分块级步骤奖励，通过辅助的分块特定问答任务提供即时步骤级监督；2)设计证据锚定奖励分配，基于推理中用作证据的特定记忆项来重新分配全局奖励。

**结果:** 在Memalpha和MemoryAgentBench上的实验显示，Fine-Mem始终优于强基线，在各种子任务中实现了更高的成功率。分析表明该方法在不同模型配置和骨干网络上具有良好的适应性和泛化能力。

**结论:** Fine-Mem通过细粒度反馈对齐实现了稳定的策略优化，使局部记忆操作与记忆的长期效用保持一致，有效解决了记忆管理中的奖励稀疏和信用分配问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-Mem%3A+Fine-Grained+Feedback+Alignment+for+Long-Horizon+Memory+Management，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08435，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08435&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.

</details>


### [102] [JudgeRLVR: Judge First, Generate Second for Efficient Reasoning](https://arxiv.org/abs/2601.08468)
*Jiangshan Duo, Hanyu Li, Hailin Zhang, Yudong Wang, Sujian Li, Liang Zhao*

**主要类别:** cs.CL

**AI概要:** 论文提出JudgeRLVR方法，通过先训练模型判断解决方案有效性，再进行生成，在保持准确率的同时显著减少生成长度，实现更好的质量-效率平衡。


<details>
  <summary>更多</summary>
  
**动机:** 传统的RLVR方法仅优化最终答案正确性，导致模型进行无目的的冗长探索，依赖试错而非结构化规划。虽然长度惩罚可以减少冗长，但可能截断关键推理步骤。

**方法:** 提出两阶段的judge-then-generate范式：第一阶段训练模型判断具有可验证答案的解决方案；第二阶段使用从判断模型初始化的普通生成RLVR对同一模型进行微调。

**结果:** 相比传统RLVR，JudgeRLVR在Qwen3-30B-A3B上实现更好的质量-效率平衡：领域内数学任务准确率提升约3.7分，生成长度减少42%；领域外基准测试准确率提升约4.5分。

**结论:** 判别能力是高效生成的前提条件，通过学习区分有效解决方案，模型可以内化指导信号来修剪搜索空间，实现更好的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是JudgeRLVR%3A+Judge+First%2C+Generate+Second+for+Efficient+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08468，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08468&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.

</details>


### [103] [sui-1: Grounded and Verifiable Long-Form Summarization](https://arxiv.org/abs/2601.08472)
*Benedikt Droste, Jan Philipp Harries, Maximilian Idahl, Björn Plüster*

**主要类别:** cs.CL

**AI概要:** sui-1是一个240亿参数的语言模型，专门用于生成带有内联引用的摘要，让用户能够追踪每个声明到源句子，在引用基础的摘要任务上显著优于更大参数量的模型。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型经常生成看似合理但无法验证的摘要，这在政府和法律分析等合规敏感领域是一个关键限制。

**方法:** 使用合成数据流水线结合思维链提示和多阶段验证，从议会文件、网页文本和维基百科等多样化来源生成超过22,000个高质量多语言训练样本。

**结果:** sui-1在所有测试的开源基线模型中表现显著更优，包括参数量多3倍的模型。

**结论:** 任务特定的训练在引用基础摘要任务上明显优于单纯扩大模型规模，模型权重和交互演示已公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是sui-1%3A+Grounded+and+Verifiable+Long-Form+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08472&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.

</details>


### [104] [Do You Understand How I Feel?: Towards Verified Empathy in Therapy Chatbots](https://arxiv.org/abs/2601.08477)
*Francesco Dettori, Matteo Forasassi, Lorenzo Veronese, Livia Lestingi, Vincenzo Scotti, Matteo Giovanni Rossi*

**主要类别:** cs.CL

**AI概要:** 提出一个结合NLP和形式化验证的框架，用于开发和验证具有同理心能力的治疗聊天机器人，通过Transformer提取对话特征并转换为随机混合自动机模型，使用统计模型检查验证同理心属性。


<details>
  <summary>更多</summary>
  
**动机:** 当前聊天机器人开发实践缺乏系统化的方法来指定和验证同理心这一治疗场景中的关键非功能性需求，而同理心在心理治疗路径中具有重要社会影响。

**方法:** 使用基于Transformer的模型提取对话特征，将其转换为描述二元治疗会话的随机混合自动机模型，通过统计模型检查验证同理心相关属性，并通过策略合成指导智能体行为塑造。

**结果:** 初步结果显示形式化模型能够以良好保真度捕捉治疗动态，特定策略提高了满足同理心需求的概率。

**结论:** 该框架为开发具有验证同理心能力的治疗聊天机器人提供了系统化方法，结合NLP和形式化验证技术有望改善心理治疗支持工具的质量和效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Do+You+Understand+How+I+Feel%3F%3A+Towards+Verified+Empathy+in+Therapy+Chatbots，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08477&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Conversational agents are increasingly used as support tools along mental therapeutic pathways with significant societal impacts. In particular, empathy is a key non-functional requirement in therapeutic contexts, yet current chatbot development practices provide no systematic means to specify or verify it. This paper envisions a framework integrating natural language processing and formal verification to deliver empathetic therapy chatbots. A Transformer-based model extracts dialogue features, which are then translated into a Stochastic Hybrid Automaton model of dyadic therapy sessions. Empathy-related properties can then be verified through Statistical Model Checking, while strategy synthesis provides guidance for shaping agent behavior. Preliminary results show that the formal model captures therapy dynamics with good fidelity and that ad-hoc strategies improve the probability of satisfying empathy requirements.

</details>


### [105] [Surgical Refusal Ablation: Disentangling Safety from Intelligence via Concept-Guided Spectral Cleaning](https://arxiv.org/abs/2601.08489)
*Tony Cristofano*

**主要类别:** cs.CL

**AI概要:** Surgical Refusal Ablation (SRA) 方法通过正交化处理，有效分离拒绝信号与核心能力回路，在保持模型能力的同时大幅降低拒绝率


<details>
  <summary>更多</summary>
  
**动机:** 传统的拒绝向量消融会导致模型能力受损和分布漂移，因为原始拒绝向量是多义的，将拒绝信号与核心能力和语言风格纠缠在一起

**方法:** SRA构建独立概念原子注册表表示保护能力和风格混淆，使用岭正则化谱残差化将拒绝向量与这些方向正交化，获得干净的拒绝方向

**结果:** 在五个模型上实现深度拒绝率降低(0-2%)，对Wikitext-2的困惑度影响极小(平均ΔPPL≈0.02)，分布漂移最小，保持数学和代码能力

**结论:** 常见的"模型损伤"实际上是"幽灵噪声"，即脏拒绝方向向能力子空间的光谱渗漏，SRA方法能有效解决这一问题

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Surgical+Refusal+Ablation%3A+Disentangling+Safety+from+Intelligence+via+Concept-Guided+Spectral+Cleaning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08489，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08489&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Safety-aligned language models systematically refuse harmful requests. While activation steering can modulate refusal, ablating the raw "refusal vector" calculated from contrastive harmful and harmless prompts often causes collateral damage and distribution drift. We argue this degradation occurs because the raw vector is polysemantic, entangling the refusal signal with core capability circuits and linguistic style.
  We introduce Surgical Refusal Ablation (SRA) to distill these steering directions. SRA constructs a registry of independent Concept Atoms representing protected capabilities and stylistic confounds, then uses ridge-regularized spectral residualization to orthogonalize the refusal vector against these directions. This yields a clean refusal direction that targets refusal-relevant structure while minimizing disruption to the model's semantic geometry.
  Across five models (Qwen3-VL and Ministral series), SRA achieves deep refusal reduction (0-2%) with negligible perplexity impact on Wikitext-2 (mean delta PPL approx. 0.02) and minimal distribution drift. Notably, standard ablation on Qwen3-VL-4B induces severe drift (first-token KL = 2.088), whereas SRA maintains the original distribution (KL = 0.044) while achieving the same 0% refusal rate. Using teacher-forced perplexity on GSM8K and MBPP as a high-resolution capability proxy, we show SRA preserves math and code distributions. These results suggest that common "model damage" is often "Ghost Noise," defined as the spectral bleeding of the dirty refusal direction into capability subspaces.

</details>


### [106] [BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts](https://arxiv.org/abs/2601.08490)
*Erin Feiglin, Nir Hutnik, Raz Lapid*

**主要类别:** cs.CL

**AI概要:** 论文研究发现大语言模型存在Overflow现象，即普通文本提示会引发过度输出，导致服务成本增加、延迟和服务性能下降，并提出BenchOverflow基准测试方法来评估和缓解这一问题。


<details>
  <summary>更多</summary>
  
**动机:** 发现大语言模型在普通交互设置下会产生过度输出，这不仅影响可用性，还带来经济成本增加、能源消耗和环境问题，同时在共享环境中可能被用作计算放大和服务降级的攻击向量。

**方法:** 引入BenchOverflow基准测试，包含9种普通文本提示策略，使用标准化协议（5000个新token预算）评估9个开源和闭源模型，通过长度分布、饱和率和累积分布函数等指标量化风险。

**结果:** 观察到明显的长度分布右移和重尾现象，不同模型家族和攻击向量之间存在异质性，轻量级缓解措施（简洁性提醒）能有效降低过度输出风险。

**结论:** 长度控制应被视为可测量的可靠性、成本和可持续性问题，BenchOverflow为选择资源浪费最小化的部署方案和评估防御措施提供了实用基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BenchOverflow%3A+Measuring+Overflow+in+Large+Language+Models+via+Plain-Text+Prompts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08490，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08490&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We investigate a failure mode of large language models (LLMs) in which plain-text prompts elicit excessive outputs, a phenomenon we term Overflow. Unlike jailbreaks or prompt injection, Overflow arises under ordinary interaction settings and can lead to elevated serving cost, latency, and cross-user performance degradation, particularly when scaled across many requests. Beyond usability, the stakes are economic and environmental: unnecessary tokens increase per-request cost and energy consumption, compounding into substantial operational spend and carbon footprint at scale. Moreover, Overflow represents a practical vector for compute amplification and service degradation in shared environments. We introduce BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies that amplify output volume without adversarial suffixes or policy circumvention. Using a standardized protocol with a fixed budget of 5000 new tokens, we evaluate nine open- and closed-source models and observe pronounced rightward shifts and heavy tails in length distributions. Cap-saturation rates (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) quantify tail risk; within-prompt variance and cross-model correlations show that Overflow is broadly reproducible yet heterogeneous across families and attack vectors. A lightweight mitigation-a fixed conciseness reminder-attenuates right tails and lowers CSR for all strategies across the majority of models. Our findings position length control as a measurable reliability, cost, and sustainability concern rather than a stylistic quirk. By enabling standardized comparison of length-control robustness across models, BenchOverflow provides a practical basis for selecting deployments that minimize resource waste and operating expense, and for evaluating defenses that curb compute amplification without eroding task performance.

</details>


### [107] [It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models](https://arxiv.org/abs/2601.08500)
*Cristian Santini, Marieke Van Erp, Mehwish Alam*

**主要类别:** cs.CL

**AI概要:** MHEL-LLaMo是一个无监督的多语言历史实体链接系统，结合小型语言模型和大型语言模型，通过集成方法解决历史文本中的实体链接挑战，无需微调即可超越现有最先进模型。


<details>
  <summary>更多</summary>
  
**动机:** 历史文本的实体链接面临语言变异、噪声输入和语义演变等挑战，现有解决方案需要大量训练数据或依赖领域特定规则，限制了可扩展性。

**方法:** 采用无监督集成方法，结合多语言双编码器(BELA)进行候选检索，使用指令调优的LLM通过提示链进行NIL预测和候选选择，利用SLM置信度分数区分简单和困难样本，仅对困难样本应用LLM。

**结果:** 在6种欧洲语言(英语、芬兰语、法语、德语、意大利语和瑞典语)的4个基准测试中，MHEL-LLaMo无需微调即超越了最先进模型。

**结论:** MHEL-LLaMo为低资源历史实体链接提供了可扩展的解决方案，通过选择性使用LLM降低了计算成本并防止简单样本的幻觉问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是It%27s+All+About+the+Confidence%3A+An+Unsupervised+Approach+for+Multilingual+Historical+Entity+Linking+using+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08500，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08500&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.

</details>


### [108] [STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays](https://arxiv.org/abs/2601.08510)
*Qiuyu Tian, Yiding Li, Fengyi Chen, Zequn Liu, Youyong Kong, Fan Guo, Yuyao Li, Jinjing Shen, Zhijing Xie, Yiyun Luo, Xin Zhang*

**主要类别:** cs.CL

**AI概要:** STAGE是一个新的电影剧本基准测试，包含知识图谱构建、场景事件摘要、长文本问答和角色扮演四个任务，用于全面评估模型在叙事理解和一致性方面的能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试主要针对问答或对话生成等子任务，缺乏对模型构建连贯故事世界并在多种推理和生成任务中保持一致性的评估能力。

**方法:** 提出STAGE基准，包含150部中英文电影的清洗后剧本、知识图谱、事件和角色注释，定义四个基于共享叙事世界表示的任务。

**结果:** 提供了统一的评估框架，能够全面测试模型构建世界表示、抽象和验证叙事事件、长文本推理以及生成角色一致响应的能力。

**结论:** STAGE填补了长叙事文本理解评估的空白，为模型在复杂叙事理解和一致性表现方面提供了全面的测试平台。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STAGE%3A+A+Benchmark+for+Knowledge+Graph+Construction%2C+Question+Answering%2C+and+In-Script+Role-Playing+over+Movie+Screenplays，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08510，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08510&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.

</details>


### [109] [STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio](https://arxiv.org/abs/2601.08511)
*Seong-Gyu Park, Sohee Park, Jisu Lee, Hyunsik Na, Daeseon Choi*

**主要类别:** cs.CL

**AI概要:** STAR框架通过分析输出概率变化检测推理时后门攻击，利用恶意推理路径的先验概率低但后验概率高的统计差异，在多种模型和数据集上实现近乎完美的检测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有LLMs的显式推理机制（如Chain-of-Thought）暴露了新的推理时后门攻击面，这些攻击生成语言连贯的恶意推理路径且难以被传统方法检测。

**方法:** 提出STAR框架，通过量化状态转移放大比（恶意路径先验概率低但后验概率高的统计差异），并采用CUSUM算法检测持续异常。

**结果:** 在8B-70B多种模型和五个基准数据集上，STAR表现出强大泛化能力，AUROC≈1.0，效率比现有基线高约42倍，且对自适应攻击具有鲁棒性。

**结论:** STAR框架有效解决了推理时后门攻击的检测问题，通过概率统计方法实现了高效且鲁棒的检测性能，为LLMs安全提供了重要保障。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STAR%3A+Detecting+Inference-time+Backdoors+in+LLM+Reasoning+via+State-Transition+Amplification+Ratio，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08511&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\approx$ 1.0) with approximately $42\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.

</details>


### [110] [Algorithmic Stability in Infinite Dimensions: Characterizing Unconditional Convergence in Banach Spaces](https://arxiv.org/abs/2601.08512)
*Przemysław Spyra*

**主要类别:** cs.CL

**AI概要:** 该论文统一了无限维空间中无条件收敛的七个等价条件，并建立了这些理论结果与计算算法稳定性分析的联系，为随机梯度下降和信号处理等应用提供了严格的理论基础。


<details>
  <summary>更多</summary>
  
**动机:** 有限维空间中条件收敛、无条件收敛和绝对收敛的概念是一致的，但在一般Banach空间中这些概念严格分离（Dvoretzky-Rogers定理），需要建立统一的理论框架来指导计算算法的稳定性分析。

**方法:** 提出一个综合的特征定理，统一了无条件收敛的七个等价条件：置换不变性、网络收敛、子级数检验、符号稳定性、有界乘子性质和弱一致收敛。

**结果:** 建立了无条件收敛的完整特征化理论框架，证明了七个条件的等价性。

**结论:** 这项工作将经典泛函分析与现代计算实践联系起来，为顺序无关和数值稳健的求和过程提供了严格的理论基础，对随机梯度下降中的梯度累积置换不变性和基于框架的信号处理中的系数阈值化等算法提供了理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Algorithmic+Stability+in+Infinite+Dimensions%3A+Characterizing+Unconditional+Convergence+in+Banach+Spaces，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08512，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08512&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The distinction between conditional, unconditional, and absolute convergence in infinite-dimensional spaces has fundamental implications for computational algorithms. While these concepts coincide in finite dimensions, the Dvoretzky-Rogers theorem establishes their strict separation in general Banach spaces. We present a comprehensive characterization theorem unifying seven equivalent conditions for unconditional convergence: permutation invariance, net convergence, subseries tests, sign stability, bounded multiplier properties, and weak uniform convergence. These theoretical results directly inform algorithmic stability analysis, governing permutation invariance in gradient accumulation for Stochastic Gradient Descent and justifying coefficient thresholding in frame-based signal processing. Our work bridges classical functional analysis with contemporary computational practice, providing rigorous foundations for order-independent and numerically robust summation processes.

</details>


### [111] [DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report](https://arxiv.org/abs/2601.08536)
*Ruizhe Li, Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, Zhendong Mao*

**主要类别:** cs.CL

**AI概要:** Deep Research Bench II是一个新的评估深度研究系统的基准，包含132个研究任务和9430个细粒度二元评分标准，评估结果显示当前最强模型只能满足不到50%的标准。


<details>
  <summary>更多</summary>
  
**动机:** 现有深度研究基准存在两个问题：要么无法充分测试系统分析证据和撰写连贯报告的能力，要么依赖过于粗略或LLM定义的评估标准，导致评分相对于人类专家存在偏见且难以验证。

**方法:** 构建包含132个基础研究任务的基准，涵盖22个领域；每个任务需要生成长篇研究报告，通过9430个细粒度二元评分标准进行评估，覆盖信息召回、分析和呈现三个维度；所有评分标准源自专家撰写的调查文章，通过四阶段LLM+人工流程构建，包括自动提取和400多小时专家评审。

**结果:** 评估多个最先进的深度研究系统发现，即使最强的模型也只能满足不到50%的评分标准，揭示了当前DRS与人类专家之间的显著差距。

**结论:** Deep Research Bench II提供了一个严格、可验证且与人类专家判断一致的评估框架，揭示了当前深度研究系统的局限性，为未来系统改进提供了重要基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepResearch+Bench+II%3A+Diagnosing+Deep+Research+Agents+via+Rubrics+from+Expert+Report，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08536&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Deep Research Systems (DRS) aim to help users search the web, synthesize information, and deliver comprehensive investigative reports. However, how to rigorously evaluate these systems remains under-explored. Existing deep-research benchmarks often fall into two failure modes. Some do not adequately test a system's ability to analyze evidence and write coherent reports. Others rely on evaluation criteria that are either overly coarse or directly defined by LLMs (or both), leading to scores that can be biased relative to human experts and are hard to verify or interpret. To address these issues, we introduce Deep Research Bench II, a new benchmark for evaluating DRS-generated reports. It contains 132 grounded research tasks across 22 domains; for each task, a system must produce a long-form research report that is evaluated by a set of 9430 fine-grained binary rubrics in total, covering three dimensions: information recall, analysis, and presentation. All rubrics are derived from carefully selected expert-written investigative articles and are constructed through a four-stage LLM+human pipeline that combines automatic extraction with over 400 human-hours of expert review, ensuring that the criteria are atomic, verifiable, and aligned with human expert judgment. We evaluate several state-of-the-art deep-research systems on Deep Research Bench II and find that even the strongest models satisfy fewer than 50% of the rubrics, revealing a substantial gap between current DRSs and human experts.

</details>


### [112] [Ministral 3](https://arxiv.org/abs/2601.08584)
*Alexander H. Liu, Kartik Khandelwal, Sandeep Subramanian, Victor Jouault, Abhinav Rastogi, Adrien Sadé, Alan Jeffares, Albert Jiang, Alexandre Cahill, Alexandre Gavaudan, Alexandre Sablayrolles, Amélie Héliou, Amos You, Andy Ehrenberg, Andy Lo, Anton Eliseev, Antonia Calvi, Avinash Sooriyarachchi, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Clémence Lanfranchi, Corentin Barreau, Cyprien Courtot, Daniele Grattarola, Darius Dabert, Diego de las Casas, Elliot Chane-Sane, Faruk Ahmed, Gabrielle Berrada, Gaëtan Ecrepont, Gauthier Guinet, Georgii Novikov, Guillaume Kunsch, Guillaume Lample, Guillaume Martin, Gunshi Gupta, Jan Ludziejewski, Jason Rute, Joachim Studnia, Jonas Amar, Joséphine Delas, Josselin Somerville Roberts, Karmesh Yadav, Khyathi Chandu, Kush Jain, Laurence Aitchison, Laurent Fainsin, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Maarten Buyl, Margaret Jennings, Marie Pellat, Mark Prins, Mathieu Poirée, Mathilde Guillaumin, Matthieu Dinot, Matthieu Futeral, Maxime Darrin, Maximilian Augustin, Mia Chiquier, Michel Schimpf, Nathan Grinsztajn, Neha Gupta, Nikhil Raghuraman, Olivier Bousquet, Olivier Duchenne, Patricia Wang, Patrick von Platen, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Quentin Torroba, Romain Sauvestre, Roman Soletskyi, Rupert Menneer, Sagar Vaze, Samuel Barry, Sanchit Gandhi, Siddhant Waghjale, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Théo Cachet, Theo Simon Sorg, Thibaut Lavril, Thiziri Nait Saada, Thomas Chabal, Thomas Foubert, Thomas Robert, Thomas Wang, Tim Lawson, Tom Bewley, Tom Bewley, Tom Edwards, Umar Jamil, Umberto Tomasini, Valeriia Nemychnikova, Van Phung, Vincent Maladière, Virgile Richard, Wassim Bouaziz, Wen-Ding Li, William Marshall, Xinghui Li, Xinyu Yang, Yassine El Ouahidi, Yihan Wang, Yunhao Tang, Zaccharie Ramzi*

**主要类别:** cs.CL

**AI概要:** Ministral 3是一个参数高效的密集语言模型系列，包含3B、8B和14B三个规模，每个规模都有基础预训练模型、指令微调模型和推理模型三种变体，采用Cascade Distillation技术训练，具备图像理解能力，使用Apache 2.0许可证。


<details>
  <summary>更多</summary>
  
**动机:** 针对计算和内存受限的应用场景，需要开发参数效率高、性能优秀的语言模型，满足不同规模和需求的应用部署。

**方法:** 使用Cascade Distillation技术进行迭代剪枝和持续蒸馏训练，从大规模模型逐步提炼出参数更少但性能保持的紧凑模型。

**结果:** 成功开发了三个不同规模的模型系列（3B/8B/14B），每个规模都提供基础版、指令微调版和推理版三种变体，均具备图像理解能力。

**结论:** Ministral 3系列模型通过创新的蒸馏技术实现了参数效率与性能的良好平衡，为资源受限环境提供了实用的语言模型解决方案，并通过开源许可证促进广泛应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Ministral+3，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08584，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08584&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.

</details>


### [113] [ExpSeek: Self-Triggered Experience Seeking for Web Agents](https://arxiv.org/abs/2601.08605)
*Wenyuan Zhang, Xinghua Zhang, Haiyang Yu, Shuaiyi Nie, Bingli Wu, Juwei Yue, Tingwen Liu, Yongbin Li*

**主要类别:** cs.CL

**AI概要:** ExpSeek提出了一种在web智能体中主动寻求经验的方法，通过熵阈值判断干预时机，在四个基准测试中显著提升了模型性能


<details>
  <summary>更多</summary>
  
**动机:** 现有方法主要在任务执行前被动注入经验作为全局上下文，难以适应智能体与环境交互中动态变化的上下文观察

**方法:** 提出ExpSeek方法：1)利用模型内在信号估计步级熵阈值来确定干预时机；2)设计步级定制化经验内容

**结果:** 在Qwen3-8B和32B模型上，四个web智能体基准测试分别实现了9.3%和7.5%的绝对性能提升

**结论:** 验证了熵作为自触发信号的可行性，发现即使4B小规模经验模型也能显著提升更大智能体模型的性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ExpSeek%3A+Self-Triggered+Experience+Seeking+for+Web+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08605&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.

</details>


### [114] [GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning](https://arxiv.org/abs/2601.08621)
*Jiajin Liu, Yuanfu Sun, Dongzhe Fan, Qiaoyu Tan*

**主要类别:** cs.CL

**AI概要:** GraphSearch：首个将搜索增强推理扩展到图学习的框架，无需任务特定微调即可实现零样本图学习，在图结构数据上实现了与监督方法相当甚至更优的性能


<details>
  <summary>更多</summary>
  
**动机:** 现有搜索增强大模型主要处理文本数据，对图结构数据的应用探索不足。图数据包含丰富的拓扑信号，但如何有效利用图结构进行检索和推理面临独特挑战

**方法:** 提出GraphSearch框架，包含图感知查询规划器（分离搜索空间和语义查询）和图感知检索器（基于拓扑构建候选集并使用混合评分函数排序）。提供两种遍历模式：递归扩展邻域的GraphSearch-R和灵活检索的GraphSearch-F

**结果:** 在多个基准测试中，GraphSearch在零样本节点分类和链接预测任务上取得了竞争性甚至优于监督图学习方法的结果，达到了最先进的性能

**结论:** GraphSearch为图上的智能推理提供了一个灵活且可泛化的范式，展示了在无需任务特定训练的情况下有效利用图结构进行检索和推理的能力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GraphSearch%3A+Agentic+Search-Augmented+Reasoning+for+Zero-Shot+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08621，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08621&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.

</details>


### [115] [How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction](https://arxiv.org/abs/2601.08626)
*Yingjie He, Zhaolu Kang, Kehan Jiang, Qianyuan Zhang, Jiachen Qian, Chunlei Meng, Yujie Feng, Yuan Wang, Jiabao Dou, Aming Wu, Leqi Zheng, Pengxiang Zhao, Jiaxin Liu, Zeyu Zhang, Lei Wang, Guansu Wang, Qishi Zhan, Xiaomin He, Meisheng Zhang, Jianyuan Ni*

**主要类别:** cs.CL

**AI概要:** OrderProbe基准测试显示，即使最先进的大语言模型在结构重建任务上表现不佳，零-shot恢复准确率常低于35%，表明语义能力不能自动带来结构鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在语义理解方面表现出色，但从乱序输入中重建内部结构的能力尚未充分探索。句子级恢复因存在多种有效词序而难以自动化评估。

**方法:** 引入OrderProbe确定性基准，使用中文、日文和韩文的固定四字表达（具有唯一规范顺序），支持精确匹配评分。提出诊断框架评估恢复准确性之外的语义保真度、逻辑有效性、一致性、鲁棒敏感性和信息密度。

**结果:** 在12个广泛使用的大语言模型上的实验显示，结构重建仍然很困难：零-shot恢复准确率经常低于35%。观察到语义回忆和结构规划之间存在一致的分离。

**结论:** 结构鲁棒性不是语义能力的自动副产品，大语言模型在结构重建方面仍有显著挑战，需要专门的方法来提升这方面的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Order-Sensitive+Are+LLMs%3F+OrderProbe+for+Deterministic+Structural+Reconstruction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08626&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.

</details>


### [116] [Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation](https://arxiv.org/abs/2601.08629)
*Saumitra Yadav, Manish Shrivastava*

**主要类别:** cs.CL

**AI概要:** LALITA框架通过词汇和语言学特征筛选源语句，优化低资源机器翻译训练数据选择，显著提升翻译质量并减少50%以上数据需求


<details>
  <summary>更多</summary>
  
**动机:** 低资源语言机器翻译中，人工翻译生成足够训练数据成本过高，需要开发有效的数据筛选框架来优化平行文本构建

**方法:** 开发LALITA框架，使用词汇和语言学特征分析英语-印地语双语文本，选择复杂句子进行训练，在50K-800K句子规模数据集上测试

**结果:** 在所有数据规模下均显示性能提升，显著减少50%以上数据需求，在多种语言（印地语、奥里亚语、尼泊尔语、挪威尼诺斯克语、德语）上验证有效

**结论:** LALITA框架通过智能筛选复杂句子，有效降低机器翻译系统训练成本，在低资源环境下展现出优异的数据增强效用

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Get+away+with+less%3A+Need+of+source+side+data+curation+to+build+parallel+corpus+for+low+resource+Machine+Translation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08629&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Data curation is a critical yet under-researched step in the machine translation training paradigm. To train translation systems, data acquisition relies primarily on human translations and digital parallel sources or, to a limited degree, synthetic generation. But, for low-resource languages, human translation to generate sufficient data is prohibitively expensive. Therefore, it is crucial to develop a framework that screens source sentences to form efficient parallel text, ensuring optimal MT system performance in low-resource environments. We approach this by evaluating English-Hindi bi-text to determine effective sentence selection strategies for optimal MT system training. Our extensively tested framework, (Lexical And Linguistically Informed Text Analysis) LALITA, targets source sentence selection using lexical and linguistic features to curate parallel corpora. We find that by training mostly on complex sentences from both existing and synthetic datasets, our method significantly improves translation quality. We test this by simulating low-resource data availabilty with curated datasets of 50K to 800K English sentences and report improved performances on all data sizes. LALITA demonstrates remarkable efficiency, reducing data needs by more than half across multiple languages (Hindi, Odia, Nepali, Norwegian Nynorsk, and German). This approach not only reduces MT systems training cost by reducing training data requirement, but also showcases LALITA's utility in data augmentation.

</details>


### [117] [Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs](https://arxiv.org/abs/2601.08634)
*Chenchen Yuan, Bolei Ma, Zheyu Zhang, Bardh Prenkaj, Frauke Kreuter, Gjergji Kasneci*

**主要类别:** cs.CL

**AI概要:** 该研究通过将道德价值作为可控条件，探讨了道德直觉与政治立场之间的因果关系，发现道德条件化能显著改变大语言模型在政治坐标上的位置，且这种效应受到角色设定和模型规模的系统性调节。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要通过直接探测或人口统计角色工程来评估大语言模型的政治倾向，但政治意识形态在社会心理学中被理解为基本道德直觉的下游结果，因此需要从道德价值的角度更深入地理解模型的政治定位。

**方法:** 将道德取向作为可控条件，让模型认可或拒绝特定道德价值，然后使用政治罗盘测试评估由此产生的政治取向变化，将道德价值作为观察政治倾向的透镜。

**结果:** 道德条件化能引起模型政治坐标的显著、价值特异性偏移，这些效应受到角色设定和模型规模的系统性调节，且在实例化相同道德价值的替代评估工具中具有稳健性。

**结论:** 有效的对齐需要在更广泛的社会价值（包括道德）背景下锚定政治评估，这为开发更具社会基础的对齐技术铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Moral+Lenses%2C+Political+Coordinates%3A+Towards+Ideological+Positioning+of+Morally+Conditioned+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08634，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08634&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While recent research has systematically documented political orientation in large language models (LLMs), existing evaluations rely primarily on direct probing or demographic persona engineering to surface ideological biases. In social psychology, however, political ideology is also understood as a downstream consequence of fundamental moral intuitions. In this work, we investigate the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. Rather than simply assigning a demographic persona, we condition models to endorse or reject specific moral values and evaluate the resulting shifts on their political orientations, using the Political Compass Test. By treating moral values as lenses, we observe how moral conditioning actively steers model trajectories across economic and social dimensions. Our findings show that such conditioning induces pronounced, value-specific shifts in models' political coordinates. We further notice that these effects are systematically modulated by role framing and model scale, and are robust across alternative assessment instruments instantiating the same moral value. This highlights that effective alignment requires anchoring political assessments within the context of broader social values including morality, paving the way for more socially grounded alignment techniques.

</details>


### [118] [A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding](https://arxiv.org/abs/2601.08645)
*Dilara Torunoğlu-Selamet, Dogukan Arslan, Rodrigo Wilkens, Wei He, Doruk Eryiğit, Thomas Pickard, Adriana S. Pagano, Aline Villavicencio, Gülşen Eryiğit, Ágnes Abuczki, Aida Cardoso, Alesia Lazarenka, Dina Almassova, Amalia Mendes, Anna Kanellopoulou, Antoni Brosa-Rodríguez, Baiba Saulite, Beata Wojtowicz, Bolette Pedersen, Carlos Manuel Hidalgo-Ternero, Chaya Liebeskind, Danka Jokić, Diego Alves, Eleni Triantafyllidi, Erik Velldal, Fred Philippy, Giedre Valunaite Oleskeviciene, Ieva Rizgeliene, Inguna Skadina, Irina Lobzhanidze, Isabell Stinessen Haugen, Jauza Akbar Krito, Jelena M. Marković, Johanna Monti, Josue Alejandro Sauca, Kaja Dobrovoljc, Kingsley O. Ugwuanyi, Laura Rituma, Lilja Øvrelid, Maha Tufail Agro, Manzura Abjalova, Maria Chatzigrigoriou, María del Mar Sánchez Ramos, Marija Pendevska, Masoumeh Seyyedrezaei, Mehrnoush Shamsfard, Momina Ahsan, Muhammad Ahsan Riaz Khan, Nathalie Carmen Hau Norman, Nilay Erdem Ayyıldız, Nina Hosseini-Kivanani, Noémi Ligeti-Nagy, Numaan Naeem, Olha Kanishcheva, Olha Yatsyshyna, Daniil Orel, Petra Giommarelli, Petya Osenova, Radovan Garabik, Regina E. Semou, Rozane Rebechi, Salsabila Zahirah Pranida, Samia Touileb, Sanni Nimb, Sarfraz Ahmad, Sarvinoz Nematkhonova, Shahar Golan, Shaoxiong Ji, Sopuruchi Christian Aboh, Srdjan Sucur, Stella Markantonatou, Sussi Olsen, Vahide Tajalli, Veronika Lipp, Voula Giouli, Yelda Yeşildal Eraydın, Zahra Saaberi, Zhuohan Xie*

**主要类别:** cs.CL

**AI概要:** XMPIE是一个包含34种语言、超过一万个条目的多语言多模态并行数据集，用于评估NLP系统对潜在习语表达的理解能力，支持跨语言和跨模态的比较研究。


<details>
  <summary>更多</summary>
  
**动机:** 潜在习语表达(PIEs)与语言社区的日常经验密切相关，是评估NLP系统语言和文化能力的重要挑战。需要创建高质量数据集来研究多语言和多模态环境下的习语理解。

**方法:** 由语言专家创建，包含文本和视觉组件，每个PIE配有五张图像，涵盖从习语意义到字面意义的连续谱，包括语义相关和随机干扰项。采用多语言指导原则构建。

**结果:** 创建了一个高质量的基准数据集，支持对34种语言的习语模式进行比较分析，能够评估模型在不同语言中的表现以及跨语言、跨模态的习语理解迁移能力。

**结论:** XMPIE数据集为评估多语言和多模态习语理解提供了重要资源，有助于深入了解不同语言社区共享的文化方面，推动NLP系统在习语理解方面的进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Parallel+Cross-Lingual+Benchmark+for+Multimodal+Idiomaticity+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08645，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08645&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.

</details>


### [119] [Safe Language Generation in the Limit](https://arxiv.org/abs/2601.08648)
*Antonios Anastasopoulos, Giuseppe Ateniese, Evgenios M. Kornaropoulos*

**主要类别:** cs.CL

**AI概要:** 该论文首次对安全语言生成进行理论分析，证明安全语言识别不可能，安全语言生成至少与普通语言识别一样困难（同样不可能），并讨论了部分可处理的特殊情况。


<details>
  <summary>更多</summary>
  
**动机:** 随着语言学习理论的发展，虽然语言识别被证明不可能但语言生成可行，需要研究现实环境中安全语言生成的理论基础。

**方法:** 基于极限学习的计算范式，形式化安全语言识别和生成任务，进行理论证明和复杂性分析。

**结果:** 证明安全语言识别不可能；安全语言生成至少与普通语言识别一样困难（不可能）；识别了部分可处理和不可处理的情况。

**结论:** 安全语言生成在理论上具有根本性困难，但在特定约束条件下可能存在可行的解决方案，为实际应用提供了理论指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Safe+Language+Generation+in+the+Limit，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08648，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08648&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent results in learning a language in the limit have shown that, although language identification is impossible, language generation is tractable. As this foundational area expands, we need to consider the implications of language generation in real-world settings.
  This work offers the first theoretical treatment of safe language generation. Building on the computational paradigm of learning in the limit, we formalize the tasks of safe language identification and generation. We prove that under this model, safe language identification is impossible, and that safe language generation is at least as hard as (vanilla) language identification, which is also impossible. Last, we discuss several intractable and tractable cases.

</details>


### [120] [RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation](https://arxiv.org/abs/2601.08654)
*Yihan Hong, Huaiyuan Yao, Bolin Shen, Wanpeng Xu, Hua Wei, Yushun Dong*

**主要类别:** cs.CL

**AI概要:** RULERS框架通过将评分标准编译成可执行规范，解决了LLM评判中的三个主要问题：提示敏感性、不可验证推理和尺度不对齐，显著提升了与人类评判的一致性。


<details>
  <summary>更多</summary>
  
**动机:** LLM作为评判者的范式虽然提供了可扩展的基于标准的评估，但由于固有的生成随机性，使冻结的黑盒模型与人类标准对齐仍然是一个挑战。

**方法:** 提出RULERS框架：将自然语言评分标准编译成版本化的不可变包，强制执行结构化解码和确定性证据验证，应用基于Wasserstein的轻量级后校准，无需更新模型参数。

**结果:** 在论文和摘要基准测试中，RULERS在人类一致性方面显著优于基线方法，对对抗性评分标准扰动保持强稳定性，并使较小模型能够与大型专有评判者相媲美。

**结论:** 可靠的LLM评判需要可执行的评分标准、可验证的证据和校准的尺度，而不仅仅是提示词措辞。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RULERS%3A+Locked+Rubrics+and+Evidence-Anchored+Scoring+for+Robust+LLM+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08654，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08654&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.

</details>


### [121] [Analyzing Bias in False Refusal Behavior of Large Language Models for Hate Speech Detoxification](https://arxiv.org/abs/2601.08668)
*Kyuri Im, Shuzhou Yuan, Michael Färber*

**主要类别:** cs.CL

**AI概要:** 研究发现大型语言模型在仇恨言论净化任务中存在虚假拒绝行为，对特定群体和语义毒性高的内容存在偏见，并提出跨语言翻译策略有效减少拒绝率


<details>
  <summary>更多</summary>
  
**动机:** LLMs在仇恨言论净化应用中频繁触发安全警报拒绝执行任务，需要系统研究这种虚假拒绝行为的原因和模式

**方法:** 评估9个LLMs在英语和多语言数据集上的表现，分析语义毒性和目标群体对拒绝行为的影响，并提出英译中再译回的跨翻译策略

**结果:** LLMs对语义毒性高、针对国籍/宗教/政治意识形态的内容拒绝率更高；多语言数据集总体拒绝率低于英语但存在语言依赖性偏见；跨翻译策略显著减少虚假拒绝同时保持内容完整性

**结论:** 跨语言翻译是减少LLMs虚假拒绝行为的有效轻量级方法，揭示了模型安全机制中的系统性偏见问题

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Analyzing+Bias+in+False+Refusal+Behavior+of+Large+Language+Models+for+Hate+Speech+Detoxification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08668&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While large language models (LLMs) have increasingly been applied to hate speech detoxification, the prompts often trigger safety alerts, causing LLMs to refuse the task. In this study, we systematically investigate false refusal behavior in hate speech detoxification and analyze the contextual and linguistic biases that trigger such refusals. We evaluate nine LLMs on both English and multilingual datasets, our results show that LLMs disproportionately refuse inputs with higher semantic toxicity and those targeting specific groups, particularly nationality, religion, and political ideology. Although multilingual datasets exhibit lower overall false refusal rates than English datasets, models still display systematic, language-dependent biases toward certain targets. Based on these findings, we propose a simple cross-translation strategy, translating English hate speech into Chinese for detoxification and back, which substantially reduces false refusals while preserving the original content, providing an effective and lightweight mitigation approach.

</details>


### [122] [Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization](https://arxiv.org/abs/2601.08682)
*Kushal Chawla, Chenyang Zhu, Pengshan Cai, Sangwoo Cho, Scott Novotney, Ayushman Singh, Jonah Lewis, Keasha Safewright, Alfy Samuel, Erin Babinsky, Shi-Xiong Zhang, Sambit Sahu*

**主要类别:** cs.CL

**AI概要:** 本文是一个关于多轮对话摘要的工业案例研究，分享了构建可靠、适应性强的智能体系统的实践经验，涵盖评估方法、组件优化、数据瓶颈和供应商锁定等问题。


<details>
  <summary>更多</summary>
  
**动机:** 多轮对话自动摘要对工业应用至关重要，但现有研究主要使用静态数据集，而实际应用中需求会不断变化，需要开发能够适应需求演变的摘要系统。

**方法:** 采用智能体架构进行任务分解，实现组件级优化，并开发了应对需求变化和任务主观性的鲁棒评估方法。

**结果:** 提出了构建可靠、适应性强的多轮对话摘要系统的完整开发生命周期实践指南，包括处理上游数据瓶颈和LLM提示词可移植性差导致的供应商锁定问题。

**结论:** 智能体架构为多轮对话摘要提供了有效的解决方案，但需要关注数据质量和供应商依赖问题，为未来研究和实践提供了重要参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lessons+from+the+Field%3A+An+Adaptable+Lifecycle+Approach+to+Applied+Dialogue+Summarization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08682，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08682&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.

</details>


### [123] [QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models](https://arxiv.org/abs/2601.08689)
*Zhaolu Kang, Junhao Gong, Wenqing Hu, Shuo Yin, Kehan Jiang, Zhicheng Fang, Yingjie He, Chunlei Meng, Rong Fu, Dongyang Chen, Leqi Zheng, Eric Hanchen Jiang, Yunfei Feng, Yitong Leng, Junfan Zhu, Xiaoyou Chen, Xi Yang, Richeng Xuan*

**主要类别:** cs.CL

**AI概要:** QuantEval是一个新的金融量化评估基准，从知识问答、数学推理和策略编码三个维度评估大语言模型，并引入了CTA式回测框架来执行模型生成的策略，通过金融绩效指标进行更真实的评估。


<details>
  <summary>更多</summary>
  
**动机:** 当前大语言模型在金融量化任务中的评估零散且主要局限于知识问答，缺乏对量化推理和策略编码能力的全面评估，需要更真实的测试框架。

**方法:** 构建QuantEval基准，包含三个评估维度：知识问答、数学推理和策略编码，并集成CTA式回测框架执行模型生成的策略，使用金融绩效指标进行评估。对开源和专有LLMs进行评估，并进行大规模监督微调和强化学习实验。

**结果:** 发现当前最先进的LLMs与人类专家在推理和策略编码方面存在显著差距，但通过领域对齐数据的监督微调和强化学习可以带来一致的性能提升。

**结论:** QuantEval基准将促进LLMs在量化金融能力方面的研究，并加速其在真实交易工作流程中的实际应用，同时通过发布完整的确定性回测配置确保严格的可复现性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QuantEval%3A+A+Benchmark+for+Financial+Quantitative+Tasks+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08689&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.

</details>


### [124] [Nationality and Region Prediction from Names: A Comparative Study of Neural Models and Large Language Models](https://arxiv.org/abs/2601.08692)
*Keito Inoshita*

**主要类别:** cs.CL

**AI概要:** LLM在国籍预测任务中全面优于传统神经网络模型，特别是在细粒度预测上优势明显，主要得益于其预训练获得的世界知识。模型选择应考虑所需的粒度级别，评估应关注错误质量而不仅仅是准确率。


<details>
  <summary>更多</summary>
  
**动机:** 从人名预测国籍在营销、人口研究和家谱研究中具有实用价值。传统神经模型难以泛化到低频国籍和区分同一地区的相似国籍，LLM有潜力通过预训练获得的世界知识来解决这些挑战。

**方法:** 全面比较神经模型和LLM在国籍预测上的表现，评估6个神经模型和6种LLM提示策略，在三个粒度级别（国籍、地区、大陆）进行分析，包括基于频率的分层分析和错误分析。

**结果:** LLM在所有粒度级别上都优于神经模型，差距随粒度变粗而缩小。简单机器学习方法具有最高的频率鲁棒性，而预训练模型和LLM对低频国籍表现下降。LLM倾向于做出"近失"错误（预测正确地区但错误国籍），神经模型则表现出更多跨区域错误和高频类别偏向。

**结论:** LLM的优势源于世界知识，模型选择应考虑所需粒度，评估应超越准确率关注错误质量。LLM的"近失"错误表明其理解地理和文化关联，而神经模型的错误更具随机性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Nationality+and+Region+Prediction+from+Names%3A+A+Comparative+Study+of+Neural+Models+and+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08692，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08692&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Predicting nationality from personal names has practical value in marketing, demographic research, and genealogical studies. Conventional neural models learn statistical correspondences between names and nationalities from task-specific training data, posing challenges in generalizing to low-frequency nationalities and distinguishing similar nationalities within the same region. Large language models (LLMs) have the potential to address these challenges by leveraging world knowledge acquired during pre-training. In this study, we comprehensively compare neural models and LLMs on nationality prediction, evaluating six neural models and six LLM prompting strategies across three granularity levels (nationality, region, and continent), with frequency-based stratified analysis and error analysis. Results show that LLMs outperform neural models at all granularity levels, with the gap narrowing as granularity becomes coarser. Simple machine learning methods exhibit the highest frequency robustness, while pre-trained models and LLMs show degradation for low-frequency nationalities. Error analysis reveals that LLMs tend to make ``near-miss'' errors, predicting the correct region even when nationality is incorrect, whereas neural models exhibit more cross-regional errors and bias toward high-frequency classes. These findings indicate that LLM superiority stems from world knowledge, model selection should consider required granularity, and evaluation should account for error quality beyond accuracy.

</details>


### [125] [RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis](https://arxiv.org/abs/2601.08699)
*Zhengwei Tao, Bo Li, Jialong Wu, Guochen Yan, Huanyao Zhang, Jiahao Xu, Haitao Mi, Wentao Zhang*

**主要类别:** cs.CL

**AI概要:** RAGShaper框架通过自动化构建RAG任务和智能体轨迹，解决了高质量训练数据稀缺的问题，显著提升了RAG智能体在噪声环境下的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 传统手动标注方法无法规模化且难以捕捉处理检索失败所需的动态推理策略，阻碍了鲁棒RAG智能体的发展。

**方法:** 提出RAGShaper框架：使用InfoCurator构建包含感知和认知层面对抗性干扰的密集信息树，并通过约束导航策略迫使教师智能体面对干扰，生成展示错误纠正和噪声拒绝的轨迹。

**结果:** 在综合实验中，使用合成语料训练的模型显著优于现有基线，在噪声密集和复杂检索任务中表现出更优的鲁棒性。

**结论:** RAGShaper为RAG智能体的训练提供了可扩展的高质量数据合成方案，有效解决了现实检索环境中的噪声和复杂性问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RAGShaper%3A+Eliciting+Sophisticated+Agentic+RAG+Skills+via+Automated+Data+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08699&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.

</details>


### [126] [PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation](https://arxiv.org/abs/2601.08739)
*Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang*

**主要类别:** cs.CL

**AI概要:** PrivGemo是一个保护隐私的知识图谱增强推理框架，通过双塔设计在本地保留原始KG知识，同时在匿名视图上进行远程推理，解决了现有隐私保护方法的结构泄露、不可控远程交互等问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有知识图谱隐私保护方法主要关注实体名称掩码，但仍面临结构泄露、不可控远程交互、多跳多实体推理脆弱以及经验复用有限等问题，需要更全面的隐私保护解决方案。

**方法:** 采用双塔设计保持原始KG知识本地化，在匿名视图上进行远程推理；检索连接所有主题实体的匿名长跳路径；使用分层控制器和隐私感知经验记忆减少不必要的探索和远程交互。

**结果:** 在六个基准测试中达到最先进性能，比最强基线提升高达17.1%；使小型模型（如Qwen3-4B）达到与GPT-4-Turbo相当的推理性能。

**结论:** PrivGemo有效解决了KG增强推理中的隐私泄露问题，通过全面的隐私保护机制实现了高性能的推理能力，为隐私敏感场景下的知识密集型问答提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PrivGemo%3A+Privacy-Preserving+Dual-Tower+Graph+Retrieval+for+Empowering+LLM+Reasoning+with+Memory+Augmentation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08739，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08739&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [127] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati, Sahil Sen, Waqar Sarguroh, Kevin Paul*

**主要类别:** cs.CL

**AI概要:** FRTR是一个多模态检索增强生成框架，专门解决LLM在企业级电子表格推理中的挑战，通过细粒度嵌入和混合检索方法，在准确性和效率上显著超越现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在处理包含数千行数字、多个链接表格和嵌入式视觉内容的企业级电子表格时存在困难，现有方法在可扩展性和多模态处理方面表现不足。

**方法:** 提出FRTR框架：将Excel工作簿分解为行、列和块的细粒度嵌入，采用词汇-稠密混合检索与互惠排名融合技术，集成多模态嵌入来处理数值和视觉信息。

**结果:** 在FRTR-Bench上达到74%准确率（Claude Sonnet 4.5），相比之前最佳方法的24%有显著提升；在SpreadsheetLLM基准上达到87%准确率（GPT-5），同时减少约50%的token使用量。

**结论:** FRTR框架通过创新的多模态检索和细粒度分解方法，有效解决了企业级电子表格的推理挑战，为LLM在复杂工作簿处理中的应用提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Rows+to+Reasoning%3A+A+Retrieval-Augmented+Multimodal+Framework+for+Spreadsheet+Understanding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08741，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08741&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [128] [Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents](https://arxiv.org/abs/2601.08742)
*Xin Quan, Jiafeng Xiong, Marco Valentino, André Freitas*

**主要类别:** cs.CL

**AI概要:** 该论文提出了Attributional NLI框架，将自然语言推理扩展到多智能体环境中的意图推理，通过神经符号方法显著提升了LLM在归因推理任务上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 传统自然语言推理无法捕捉多智能体环境中基于意图的复杂推理，需要开发能够进行归因推理（预测潜在意图）的新框架。

**方法:** 引入Attributional NLI框架，结合社会心理学原理，通过文本游戏Undercover-V实验三种LLM智能体：标准NLI（仅演绎推理）、Att-NLI（溯因-演绎推理）和神经符号Att-NLI（带外部定理证明器）。

**结果:** 神经符号Att-NLI智能体表现最佳，平均胜率达到17.08%，明显优于其他方法，显示出归因推理能力的清晰层次结构。

**结论:** Att-NLI框架对于开发具有复杂推理能力的智能体至关重要，神经符号AI在构建理性LLM智能体方面具有重要潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Inferring+Latent+Intentions%3A+Attributional+Natural+Language+Inference+in+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08742&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.

</details>


### [129] [TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL](https://arxiv.org/abs/2601.08743)
*Jinbo Su, Yuxuan Hu, Cuiping Li, Hong Chen, Jia Li, Lintao Ma, Jing Zhang*

**主要类别:** cs.CL

**AI概要:** TableCache：通过预计算表表示的KV缓存和Table Trie结构，优化Text-to-SQL任务中的LLM推理效率，减少前缀缓存冗余，实现TTFT加速3.62倍


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM方法在处理Text-to-SQL任务时包含完整数据库模式，导致上下文过长和预填充延迟。用户查询通常关注重复表集，但当前推理引擎在处理不同表顺序查询时会产生冗余前缀缓存副本

**方法:** 1. 离线预计算表表示的KV缓存，保持主外键关系；2. 构建Table Trie结构支持高效KV缓存查找；3. 引入缓存管理系统，包含查询重排序策略提高缓存命中率和并行计算加载流水线

**结果:** 实验结果显示TableCache在首次令牌时间(TTFT)上实现最高3.62倍加速，性能下降可忽略不计

**结论:** 通过预计算表缓存和优化缓存管理，TableCache有效解决了Text-to-SQL中LLM推理的冗余缓存问题，显著提升推理效率而不影响性能

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TableCache%3A+Primary+Foreign+Key+Guided+KV+Cache+Precomputation+for+Low+Latency+Text-to-SQL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08743，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08743&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.

</details>


### [130] [To Retrieve or To Think? An Agentic Approach for Context Evolution](https://arxiv.org/abs/2601.08747)
*Rubing Chen, Jian Wang, Wenjie Li, Xiao-Yong Wei, Qing Li*

**主要类别:** cs.CL

**AI概要:** ACE框架通过类人元认知机制动态决策何时检索新证据或使用现有知识进行推理，替代了传统的每一步都检索的暴力策略，在提高精度的同时显著降低了计算开销。


<details>
  <summary>更多</summary>
  
**动机:** 解决当前上下文增强方法（如检索增强生成）在每个步骤都执行检索的刚性策略问题，这种方法不仅产生不必要的计算成本，还会因引入无关噪声而降低性能。

**方法:** 引入Agentic Context Evolution (ACE)框架，采用中央编排器代理通过多数投票策略决策，在检索代理（外部检索）和推理代理（内部分析精炼）之间动态切换，保持简洁且进化的上下文。

**结果:** 在具有挑战性的多跳QA基准测试中，ACE在准确性方面显著优于竞争基线，同时实现了高效的token消耗。

**结论:** ACE为推进复杂知识密集型任务的上下文进化生成提供了宝贵见解，通过消除冗余检索步骤实现了性能与效率的双重提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是To+Retrieve+or+To+Think%3F+An+Agentic+Approach+for+Context+Evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08747，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08747&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.

</details>


### [131] [Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables](https://arxiv.org/abs/2601.08750)
*Valerie Zermatten, Chiara Vanalli, Gencer Sumbul, Diego Marcos, Devis Tuia*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种基于注意力机制的方法，将航空影像和地理位置文本在空间邻域内结合，通过动态选择对预测任务有用的空间邻居来提升环境变量预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 自然语言文本作为新兴生态数据源，具有独特信息价值，但文本数据在生态应用中的贡献定义不清，且与地理空间数据整合存在挑战（数据稀疏、不规则分布）。

**方法:** 使用注意力机制结合视觉和文本表示，加入地理位置编码，动态选择空间邻域中有用的观测点。在EcoWikiRS数据集上评估，该数据集包含瑞士高分辨率航空影像和维基百科环境描述文本。

**结果:** 该方法在预测SWECO25数据立方体的103个环境变量任务中， consistently优于单位置或单模态（仅图像或仅文本）基线模型。

**结论:** 通过结合文本和图像数据并考虑空间上下文，在气候、土壤、人口和土地利用/土地覆盖等主题组变量上性能显著提升，证明了空间上下文整合的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spatial+Context+Improves+the+Integration+of+Text+with+Remote+Sensing+for+Mapping+Environmental+Variables，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08750，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08750&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.

</details>


### [132] [Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge](https://arxiv.org/abs/2601.08808)
*Yao Tang, Li Dong, Yaru Hao, Qingxiu Dong, Furu Wei, Jiatao Gu*

**主要类别:** cs.CL

**AI概要:** Multiplex Thinking是一种随机软推理机制，通过采样K个候选token并将其嵌入聚合成单一连续多路复用token，在保持离散生成优势的同时实现更紧凑的推理序列，在数学推理基准测试中优于传统CoT方法


<details>
  <summary>更多</summary>
  
**动机:** 受人类软推理方式启发，传统Chain-of-Thought方法生成冗长的token序列，而人类通常通过维持可能下一步的概率分布来进行推理

**方法:** 提出Multiplex Thinking方法：在每个推理步骤采样K个候选token，将其嵌入聚合成单一连续多路复用token，保持词汇嵌入先验和采样动态，同时通过策略强化学习直接优化多路复用轨迹

**结果:** 在数学推理基准测试中，从Pass@1到Pass@1024都优于强离散CoT和RL基线方法，同时产生更短的序列长度

**结论:** Multiplex Thinking是自适应的软推理机制，在模型置信度高时接近离散CoT，不确定时紧凑表示多个可能下一步，实现了高效且性能优越的推理

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiplex+Thinking%3A+Reasoning+via+Token-wise+Branch-and-Merge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08808，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08808&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.

</details>


### [133] [Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System](https://arxiv.org/abs/2601.08829)
*Hsiang-Wei Huang, Junbin Lu, Kuang-Ming Chen, Jenq-Neng Hwang*

**主要类别:** cs.CL

**AI概要:** 研究探索了在Elo排名系统中使用LLM代理审稿人的动态，通过真实会议论文提交进行多轮审稿交互，比较了包含Elo评分和审稿人记忆的不同条件设置。


<details>
  <summary>更多</summary>
  
**动机:** 探索大型语言模型代理在学术审稿系统中的表现，特别是Elo排名系统如何影响审稿质量和决策准确性。

**方法:** 使用真实会议论文提交，创建多个具有不同角色的LLM代理审稿人，在区域主席协调下进行多轮审稿交互，比较基线设置与包含Elo评分和审稿人记忆的条件。

**结果:** 模拟结果显示，引入Elo评分提高了区域主席的决策准确性，同时审稿人表现出适应性的审稿策略，能够利用Elo系统但未提升审稿努力程度。

**结论:** Elo排名系统可以改善学术审稿的决策质量，但需要关注审稿人可能出现的策略性行为，这为优化自动审稿系统提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Modeling+LLM+Agent+Reviewer+Dynamics+in+Elo-Ranked+Review+System，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2601.08829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08829&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.

</details>
