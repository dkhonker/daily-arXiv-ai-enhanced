<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 85]
- [cs.AI](#cs.AI) [总数: 45]
- [stat.ML](#stat.ML) [总数: 11]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
*Banseok Lee, Dongkyu Kim, Youngcheon You, Youngmin Kim*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为LittleBit的新型方法，用于极端的大语言模型压缩。它通过低秩形式和二值化因子来表示权重，并引入了多尺度补偿机制以减少信息损失。实验表明，LittleBit在低于1比特的量化中表现出色，提供了更好的大小性能权衡。


<details>
  <summary>更多</summary>
  
**动机:** 部署大型语言模型（LLMs）时面临内存和计算成本的挑战。尽管量化提供了解决方案，但在低于1比特的情况下保持性能尤为困难。

**方法:** 提出了LittleBit方法，采用潜矩阵分解将权重表示为低秩形式并随后进行二值化。为了对抗这种极精度带来的信息丢失，集成了一个包括行、列以及额外潜维度的多尺度补偿机制。Dual-SVID用于稳定的量化感知训练初始化，而集成残差补偿则有助于减小误差。

**结果:** 广泛的实验确认了LittleBit在低于1比特量化中的优越性，例如其0.1比特每权重的表现优于领先的0.7比特每权重的方法。此外，内核级基准测试表明，与FP16相比有潜力实现5倍的速度提升。

**结论:** LittleBit为在资源受限环境中部署强大的大语言模型铺平了道路，提供了出色的大小性能权衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LittleBit%3A+Ultra+Low-Bit+Quantization+via+Latent+Factorization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13771&send_immediately=true&force_search=false)

**原文摘要:** Deploying large language models (LLMs) often faces challenges from
substantial memory and computational costs. Quantization offers a solution, yet
performance degradation in the sub-1-bit regime remains particularly difficult.
This paper introduces LittleBit, a novel method for extreme LLM compression. It
targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$
memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents
weights in a low-rank form using latent matrix factorization, subsequently
binarizing these factors. To counteract information loss from this extreme
precision, it integrates a multi-scale compensation mechanism. This includes
row, column, and an additional latent dimension that learns per-rank
importance. Two key contributions enable effective training: Dual
Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware
training (QAT) initialization, and integrated Residual Compensation to mitigate
errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit
quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading
method's 0.7 BPW. This establishes a superior size-performance trade-off, with
kernel-level benchmarks indicating potential for a 5$\times$ speedup compared
to FP16. LittleBit paves the way for deploying powerful LLMs in
resource-constrained environments.

</details>


### [2] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/abs/2506.13772)
*Zhenyan Lu, Daliang Xu, Dongqi Cai, Zexi Li, Wei Liu, Fangming Liu, Shangguang Wang, Mengwei Xu*

**主要类别:** cs.LG

**AI概要:** 提出了MobiEdit，一种能够在商用移动设备上高效个性化大型语言模型的知识编辑框架。它通过量化前向梯度估计替代全精度反向传播，并引入了提前停止机制和前缀缓存来提高效率。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型在处理个性化或未见过的查询时可能会产生错误的回答，而现有的知识编辑方法由于需要大量的资源来进行更新，在本地设备上运行不实际。

**方法:** MobiEdit使用量化前向梯度估计代替全精度反向传播，以适应节能型移动神经处理单元。此外，该方法还引入了两种优化措施：一种是能够根据成功情况自适应地终止编辑过程的早期停止机制；另一种是跨步骤重用计算的前缀缓存。

**结果:** MobiEdit使得在商用移动设备上对具有30亿参数的模型进行实时编辑成为可能，相比以前的知识编辑方法，它节省了7.6倍的内存、14.7倍的能量消耗，并且延迟降低了3.6倍。

**结论:** MobiEdit为商用移动设备提供了首个高效的知识编辑框架，使大型语言模型能够更好地应对个性化或新奇的查询，同时显著减少了资源消耗。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MobiEdit%3A+Resource-efficient+Knowledge+Editing+for+Personalized+On-device+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13772，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13772&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are deployed on mobile devices to power killer
applications such as intelligent assistants. LLMs pre-trained on general
corpora often hallucinate when handling personalized or unseen queries, leading
to incorrect or outdated responses. Knowledge editing addresses this by
identifying and adjusting a small crucial portion of model weights, without
compromising the general knowledge. However, prior knowledge editing methods
are impractical to run on local devices due to the resource-heavy
backpropagation (BP) needed for updates. We present MobiEdit, the first mobile
knowledge editing framework that enables efficient LLM personalization on
commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces
full-precision BP with quantized forward-only gradient estimation, thus
compatible with the energy-efficient mobile neural processing units (NPUs).
MobiEdit replaces full-precision backpropagation with quantized forward-only
gradient estimation, making it compatible with energy-efficient mobile NPUs. To
further improve gradient estimation efficiency, we introduce two optimizations:
an early stoping mechanism that adaptively terminates editing upon success and
a prefix cache that reuses computation across steps. Our approach enables
real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile
devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$
less latency compared to previous knowledge editing methods.

</details>


### [3] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/abs/2506.13781)
*Pablo Ariño Fernández, Carlos Quesada González*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为JobShopLib的模块化库，用于解决作业车间调度问题。该库支持自定义图表示、节点特征、动作空间和奖励函数等关键因素，并通过模仿学习训练了几个调度器以展示其效用。实验表明，仅使用单个操作特征的模型就优于各种基于图的调度器，而GNN模型在大规模问题上接近最先进水平。


<details>
  <summary>更多</summary>
  
**动机:** 传统的作业车间调度方法依赖于简单的启发式优先级分配规则，而近期的研究尝试利用深度学习模型特别是图神经网络（GNNs）从数据中学习优先级分配。然而，由于缺乏模块化的实验库来定制多个因素如图表示、节点特性、动作空间和奖励函数，这使得研究变得耗时。

**方法:** 提出了一个名为JobShopLib的模块化库，它允许用户定制上述提到的因素，并且可以在其强化学习环境中创建新的组件。此外，作者们还通过模仿学习训练了几个调度器来展示环境的有效性。

**结果:** 一个仅使用个体操作特性的模型在性能上超过了多种基于图的调度器，强调了特征定制的重要性。同时，所提出的GNN模型在处理大规模问题时达到了接近当前最优的结果。

**结论:** JobShopLib提供了一套必要的工具，为未来开发此类模型提供了改进的空间。这些结果暗示了在开发此类模型方面存在显著的改进潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Solving+the+Job+Shop+Scheduling+Problem+with+Graph+Neural+Networks%3A+A+Customizable+Reinforcement+Learning+Environment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13781&send_immediately=true&force_search=false)

**原文摘要:** The job shop scheduling problem is an NP-hard combinatorial optimization
problem relevant to manufacturing and timetabling. Traditional approaches use
priority dispatching rules based on simple heuristics. Recent work has
attempted to replace these with deep learning models, particularly graph neural
networks (GNNs), that learn to assign priorities from data. However, training
such models requires customizing numerous factors: graph representation, node
features, action space, and reward functions. The lack of modular libraries for
experimentation makes this research time-consuming. This work introduces
JobShopLib, a modular library that allows customizing these factors and
creating new components with its reinforcement learning environment. We trained
several dispatchers through imitation learning to demonstrate the environment's
utility. One model outperformed various graph-based dispatchers using only
individual operation features, highlighting the importance of feature
customization. Our GNN model achieved near state-of-the-art results on
large-scale problems. These results suggest significant room for improvement in
developing such models. JobShopLib provides the necessary tools for future
experimentation.

</details>


### [4] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/abs/2506.13786)
*Vuong M. Ngo, Tran Quang Vinh, Patricia Kearney, Mark Roantree*

**主要类别:** cs.LG

**AI概要:** 研究通过数据工程整合了2011年至2021年的糖尿病相关数据集，并使用增强的bagging集成回归模型（EBMBag+）来预测美国城市的糖尿病患病率。实验结果表明，与多种基线模型相比，EBMBag+算法在性能上表现最佳。


<details>
  <summary>更多</summary>
  
**动机:** 准确地进行州级预测对于有效的医疗规划和有针对性的干预至关重要，但往往缺乏必要的分析数据。

**方法:** 本研究首先通过一个数据工程过程整合从2011年到2021年的糖尿病相关数据集以创建一个综合特征集。然后介绍了一种用于时间序列预测的增强型bagging集成回归模型(EBMBag+)，用以预测美国各城市糖尿病的流行情况。

**结果:** 实验结果显示，与其他基准模型如SVMReg、BDTree、LSBoost、NN、LSTM和ERMBag相比，EBMBag+算法达到了最佳性能，其MAE为0.41，RMSE为0.53，MAPE为4.01，R2值为0.9。

**结论:** 研究表明，所提出的EBMBag+模型在预测美国城市糖尿病患病率方面优于其他几种传统和现代机器学习方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Bagging+Ensemble+Regression+with+Data+Integration+for+Time+Series-Based+Diabetes+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13786，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13786&send_immediately=true&force_search=false)

**原文摘要:** Diabetes is a chronic metabolic disease characterized by elevated blood
glucose levels, leading to complications like heart disease, kidney failure,
and nerve damage. Accurate state-level predictions are vital for effective
healthcare planning and targeted interventions, but in many cases, data for
necessary analyses are incomplete. This study begins with a data engineering
process to integrate diabetes-related datasets from 2011 to 2021 to create a
comprehensive feature set. We then introduce an enhanced bagging ensemble
regression model (EBMBag+) for time series forecasting to predict diabetes
prevalence across U.S. cities. Several baseline models, including SVMReg,
BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our
EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved
the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an
R2 of 0.9.

</details>


### [5] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/abs/2506.13828)
*Abdullah Burkan Bereketoglu*

**主要类别:** cs.LG

**AI概要:** 提出了一种混合元学习框架，用于非线性动力系统的预测和异常检测。该框架结合了物理启发的模拟器、CNN-LSTM架构、变分自编码器(VAE)、隔离森林以及双阶段注意力循环神经网络(DA-RNN)，并通过一个元学习器将这些模型结合起来，以生成综合异常预测。实验表明该集成方法在异常定位、泛化能力和对非线性偏差的鲁棒性方面优于单个模型。


<details>
  <summary>更多</summary>
  
**动机:** 针对具有非平稳性和随机行为特征的非线性动态系统，提出了一个通用的数据驱动方法来进行早期缺陷识别和预测监控。

**方法:** 开发了一个融合了物理启发式模拟器、CNN-LSTM架构、VAE无监督异常评分、基于残差的隔离森林异常检测，以及DA-RNN一步预测的混合元学习框架。

**结果:** 通过仿真实验验证，该混合集成模型在异常定位、泛化能力及对非线性偏离的鲁棒性上表现更优。

**结论:** 此框架为非线性系统的早期缺陷识别和预测监控提供了一个广泛适用的数据驱动解决方案，适用于可能无法获得完整物理模型的各种场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+Meta-Learning+Framework+for+Anomaly+Forecasting+in+Nonlinear+Dynamical+Systems+via+Physics-Inspired+Simulation+and+Deep+Ensembles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13828&send_immediately=true&force_search=false)

**原文摘要:** We propose a hybrid meta-learning framework for forecasting and anomaly
detection in nonlinear dynamical systems characterized by nonstationary and
stochastic behavior. The approach integrates a physics-inspired simulator that
captures nonlinear growth-relaxation dynamics with random perturbations,
representative of many complex physical, industrial, and cyber-physical
systems. We use CNN-LSTM architectures for spatio-temporal feature extraction,
Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation
Forests for residual-based outlier detection in addition to a Dual-Stage
Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of
the generated simulation data. To create composite anomaly forecasts, these
models are combined using a meta-learner that combines forecasting outputs,
reconstruction errors, and residual scores. The hybrid ensemble performs better
than standalone models in anomaly localization, generalization, and robustness
to nonlinear deviations, according to simulation-based experiments. The
framework provides a broad, data-driven approach to early defect identification
and predictive monitoring in nonlinear systems, which may be applied to a
variety of scenarios where complete physical models might not be accessible.

</details>


### [6] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/abs/2506.13831)
*Jitian Zhao, Chenghui Li, Frederic Sala, Karl Rohe*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个假设检验框架来量化CLIP嵌入空间中的旋转敏感结构，并提出了一种后验概念分解方法，这种方法在保证发现的概念代表了鲁棒的、可重复的模式的同时，还在重建误差方面优于其他技术。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于概念的方法缺乏统计严谨性，难以验证识别出的概念和比较不同技术。为了解决这个问题，研究者引入了一个新的假设检验框架以及一种具有理论保障的概念分解方法。

**方法:** 研究者们首先通过假设检验框架来量化CLIP嵌入空间中的旋转敏感结构，然后利用所提出的一种后验概念分解方法来分析模型内部表征。

**结果:** 实证研究表明，该概念分解算法有效平衡了重构准确性和概念可解释性，并有助于减轻数据中的虚假线索。在一个流行的虚假相关数据集中，去除虚假背景概念后，最差组准确度提高了22.6%。

**结论:** 这项工作介绍了一种用于量化CLIP嵌入空间中旋转敏感结构的假设检验框架和一种有理论保证的概念分解方法，它不仅提高了重构准确性还增强了模型行为的可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantifying+Structure+in+CLIP+Embeddings%3A+A+Statistical+Framework+for+Concept+Interpretation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13831，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13831&send_immediately=true&force_search=false)

**原文摘要:** Concept-based approaches, which aim to identify human-understandable concepts
within a model's internal representations, are a promising method for
interpreting embeddings from deep neural network models, such as CLIP. While
these approaches help explain model behavior, current methods lack statistical
rigor, making it challenging to validate identified concepts and compare
different techniques. To address this challenge, we introduce a hypothesis
testing framework that quantifies rotation-sensitive structures within the CLIP
embedding space. Once such structures are identified, we propose a post-hoc
concept decomposition method. Unlike existing approaches, it offers theoretical
guarantees that discovered concepts represent robust, reproducible patterns
(rather than method-specific artifacts) and outperforms other techniques in
terms of reconstruction error. Empirically, we demonstrate that our
concept-based decomposition algorithm effectively balances reconstruction
accuracy with concept interpretability and helps mitigate spurious cues in
data. Applied to a popular spurious correlation dataset, our method yields a
22.6% increase in worst-group accuracy after removing spurious background
concepts.

</details>


### [7] [Evolvable Conditional Diffusion](https://arxiv.org/abs/2506.13834)
*Zhao Wei, Chin Chun Ooi, Abhishek Gupta, Jian Cheng Wong, Pao-Hsiung Chiu, Sheares Xue Wen Toh, Yew-Soon Ong*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种可进化条件扩散方法，可以利用黑盒、非微分多物理模型来指导生成过程，以促进自主科学发现。通过将指导视为优化问题，并从概率进化的角度推导出一种进化引导的方法，该方法在不需要计算任何导数的情况下生成满足特定优化目标的设计。


<details>
  <summary>更多</summary>
  
**动机:** 为了能够使用常见的黑盒、非微分多物理模型（如计算流体动力学和电磁学中的模型）来指导生成过程，从而促进科学的自主发现。

**方法:** 作者将指导过程定义为一个优化问题，通过对去噪分布的描述性统计进行更新来优化所需的目标函数，并且从概率演化的角度出发，提出了基于进化引导的方法。最终得到的更新算法类似于常见的基于梯度的引导扩散模型，但无需计算任何导数。

**结果:** 所提出的可进化扩散算法在两个科学领域的人工智能场景中得到了验证：流体拓扑的自动化设计和超表面。结果表明，该方法能够有效生成更符合特定优化目标的设计，而无需依赖于可微代理。

**结论:** 本研究提供了一种有效的基于指导扩散的方法，能够利用科学界普遍存在的大量黑盒、非微分多物理数值模型，从而实现不依赖于可微代理的有效设计生成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolvable+Conditional+Diffusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13834&send_immediately=true&force_search=false)

**原文摘要:** This paper presents an evolvable conditional diffusion method such that
black-box, non-differentiable multi-physics models, as are common in domains
like computational fluid dynamics and electromagnetics, can be effectively used
for guiding the generative process to facilitate autonomous scientific
discovery. We formulate the guidance as an optimization problem where one
optimizes for a desired fitness function through updates to the descriptive
statistic for the denoising distribution, and derive an evolution-guided
approach from first principles through the lens of probabilistic evolution.
Interestingly, the final derived update algorithm is analogous to the update as
per common gradient-based guided diffusion models, but without ever having to
compute any derivatives. We validate our proposed evolvable diffusion algorithm
in two AI for Science scenarios: the automated design of fluidic topology and
meta-surface. Results demonstrate that this method effectively generates
designs that better satisfy specific optimization objectives without reliance
on differentiable proxies, providing an effective means of guidance-based
diffusion that can capitalize on the wealth of black-box, non-differentiable
multi-physics numerical models common across Science.

</details>


### [8] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/abs/2506.13836)
*Dang Viet Anh Nguyen, Carlos Lima Azevedo, Tomer Toledo, Filipe Rodrigues*

**主要类别:** cs.LG

**AI概要:** 研究提出了T-REX，一个基于SUMO的开源仿真框架，用于在动态和事故场景下训练和评估基于强化学习的交通信号控制方法，并通过实验表明不同RL-TSC方法在实际部署中的鲁棒性表现。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探索基于强化学习的交通信号控制方法（RL-TSC）在诸如交通事故等现实世界干扰下的鲁棒性，这是一个尚未充分探讨的领域。

**方法:** 引入了T-REX，一种基于SUMO的开源仿真框架，用于在考虑驾驶员概率改道、速度调整以及上下文车道变换的情况下模拟动态事故场景中网络级性能。同时提出了一套超越传统交通效率指标的度量标准来评估鲁棒性。

**结果:** 结果显示，在稳定交通条件和同质化网络中，独立价值型和去中心化压力型方法提供快速收敛和泛化能力，但在事故驱动的分布变化下性能急剧下降。相比之下，分层协调方法在大规模不规则网络中表现出更稳定和适应性强的表现，但收敛较慢且训练复杂度更高。

**结论:** 研究表明需要重视RL-TSC研究中的鲁棒性意识设计与评价，而T-REX则为这一努力提供了开放、标准化和可重复的平台以在动态和破坏性的交通场景下对RL方法进行基准测试。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robustness+of+Reinforcement+Learning-Based+Traffic+Signal+Control+under+Incidents%3A+A+Comparative+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13836，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13836&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a
promising approach for improving urban mobility. However, its robustness under
real-world disruptions such as traffic incidents remains largely underexplored.
In this study, we introduce T-REX, an open-source, SUMO-based simulation
framework for training and evaluating RL-TSC methods under dynamic, incident
scenarios. T-REX models realistic network-level performance considering
drivers' probabilistic rerouting, speed adaptation, and contextual
lane-changing, enabling the simulation of congestion propagation under
incidents. To assess robustness, we propose a suite of metrics that extend
beyond conventional traffic efficiency measures. Through extensive experiments
across synthetic and real-world networks, we showcase T-REX for the evaluation
of several state-of-the-art RL-TSC methods under multiple real-world deployment
paradigms. Our findings show that while independent value-based and
decentralized pressure-based methods offer fast convergence and generalization
in stable traffic conditions and homogeneous networks, their performance
degrades sharply under incident-driven distribution shifts. In contrast,
hierarchical coordination methods tend to offer more stable and adaptable
performance in large-scale, irregular networks, benefiting from their
structured decision-making architecture. However, this comes with the trade-off
of slower convergence and higher training complexity. These findings highlight
the need for robustness-aware design and evaluation in RL-TSC research. T-REX
contributes to this effort by providing an open, standardized and reproducible
platform for benchmarking RL methods under dynamic and disruptive traffic
scenarios.

</details>


### [9] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/abs/2506.13838)
*Lorena Poenaru-Olaru, June Sallou, Luis Cruz, Jan Rellermeyer, Arie van Deursen*

**主要类别:** cs.LG

**AI概要:** 研究了机器学习系统中常见再训练技术的能耗，发现仅使用最新数据进行再训练可以减少高达25%的能耗，而基于数据变化检测器按需再训练则可进一步节省最多40%的能耗。


<details>
  <summary>更多</summary>
  
**动机:** 机器学习系统的可靠性受到随时间变化的数据影响，需要定期维护和模型再训练，但再训练过程计算需求大、能耗高，并且对环境有影响。为了设计可持续的ML应用，有必要了解应考虑哪些再训练技术。

**方法:** 研究了常见的再训练技术的能量消耗，并从能源效率和准确度两个方面比较了这些技术。

**结果:** 与使用所有可用数据相比，仅使用最近的数据进行再训练能够将能量消耗降低多达25%。而且，如果有一个可靠的数据变化检测器，那么只有在必要时才进行模型再训练，而不是按照固定的时间表，可以将能量消耗降低多达40%。

**结论:** 该研究为机器学习实践者提供了更好的建议，指导他们在设计可持续的机器学习软件系统时采用更节能的再训练技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sustainable+Machine+Learning+Retraining%3A+Optimizing+Energy+Efficiency+Without+Compromising+Accuracy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13838，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13838&send_immediately=true&force_search=false)

**原文摘要:** The reliability of machine learning (ML) software systems is heavily
influenced by changes in data over time. For that reason, ML systems require
regular maintenance, typically based on model retraining. However, retraining
requires significant computational demand, which makes it energy-intensive and
raises concerns about its environmental impact. To understand which retraining
techniques should be considered when designing sustainable ML applications, in
this work, we study the energy consumption of common retraining techniques.
Since the accuracy of ML systems is also essential, we compare retraining
techniques in terms of both energy efficiency and accuracy. We showcase that
retraining with only the most recent data, compared to all available data,
reduces energy consumption by up to 25\%, being a sustainable alternative to
the status quo. Furthermore, our findings show that retraining a model only
when there is evidence that updates are necessary, rather than on a fixed
schedule, can reduce energy consumption by up to 40\%, provided a reliable data
change detector is in place. Our findings pave the way for better
recommendations for ML practitioners, guiding them toward more energy-efficient
retraining techniques when designing sustainable ML software systems.

</details>


### [10] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/abs/2506.13842)
*Yuanlong Wang, Pengqi Wang, Changchang Yin, Ping Zhang*

**主要类别:** cs.LG

**AI概要:** 研究人员开发了SatHealth数据集，整合了多模态时空数据如环境数据、卫星图像、全疾病发病率和社会健康决定因素指标。实验表明，生活环境中包含的信息可以显著提高AI模型在各种任务上的表现和时空泛化能力。此外，他们还发布了一个基于网络的应用程序，以便于探索SatHealth数据集，并提供了一键访问区域环境嵌入的功能。


<details>
  <summary>更多</summary>
  
**动机:** 由于公共和人口健康研究中缺乏长期且细粒度的空间和时间数据，现有的大多数研究未能将环境数据纳入考虑，这限制了模型的性能及其实际应用。为了解决这一不足，需要创建一个包含丰富环境信息的数据集来改进AI模型的表现。

**方法:** 开发了一个名为SatHealth的新数据集，该数据集结合了多种类型的时空数据。利用这些数据进行了两个使用案例的研究：地区公共卫生建模和个人疾病风险预测。

**结果:** 实验结果表明，生活环境信息能够显著提升AI模型在不同任务中的表现以及它们的时空泛化性。

**结论:** 通过构建并利用SatHealth数据集，研究者们不仅展示了如何有效整合环境数据以增强AI模型的能力，而且提供了一个易于使用的网络应用程序和代码管道来促进环境健康信息学领域的未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SatHealth%3A+A+Multimodal+Public+Health+Dataset+with+Satellite-based+Environmental+Factors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13842，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13842&send_immediately=true&force_search=false)

**原文摘要:** Living environments play a vital role in the prevalence and progression of
diseases, and understanding their impact on patient's health status becomes
increasingly crucial for developing AI models. However, due to the lack of
long-term and fine-grained spatial and temporal data in public and population
health studies, most existing studies fail to incorporate environmental data,
limiting the models' performance and real-world application. To address this
shortage, we developed SatHealth, a novel dataset combining multimodal
spatiotemporal data, including environmental data, satellite images,
all-disease prevalences estimated from medical claims, and social determinants
of health (SDoH) indicators. We conducted experiments under two use cases with
SatHealth: regional public health modeling and personal disease risk
prediction. Experimental results show that living environmental information can
significantly improve AI models' performance and temporal-spatial
generalizability on various tasks. Finally, we deploy a web-based application
to provide an exploration tool for SatHealth and one-click access to both our
data and regional environmental embedding to facilitate plug-and-play
utilization. SatHealth is now published with data in Ohio, and we will keep
updating SatHealth to cover the other parts of the US. With the web application
and published code pipeline, our work provides valuable angles and resources to
include environmental data in healthcare research and establishes a
foundational framework for future research in environmental health informatics.

</details>


### [11] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/abs/2506.13862)
*Alena Shilova, Alex Davey, Brahim Driss, Riad Akrour*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为StaQ的新算法，该算法基于Policy Mirror Descent（PMD）框架，但仅保留最近的M个Q函数，从而在实践中提供了一种可收敛且无策略更新误差的方法。与现有的深度强化学习基准相比，StaQ表现出色，并减少了性能波动，为完全稳定的深度RL算法铺平了道路。


<details>
  <summary>更多</summary>
  
**动机:** 正则化是强化学习中一种流行的工具，它可以通过熵奖励或约束连续策略的Kullback-Leibler散度来提高探索性、鲁棒性和稳定性。尽管Policy Mirror Descent (PMD) 是一个解决这类正则化策略优化问题的理论框架，但它要求所有过去Q函数的总和，这在实际应用中难以实现。因此，需要开发一种更加实用的PMD类算法。

**方法:** 研究者们提出了类似于PMD的算法，这些算法只在内存中保存最后的M个Q函数。通过数学分析表明，对于足够大的有限M值，可以推导出一种收敛的算法，这种算法在策略更新过程中不会引入误差，这一点与先前的深度RL PMD实现不同。

**结果:** 新提出的算法StaQ不仅享有强大的理论保障，而且在与深度RL基线对比时也表现出了竞争力，同时显示出更少的性能振荡。这意味着StaQ有助于实现全稳定深度RL算法的发展，并为Policy Mirror Descent的研究提供了实验平台。

**结论:** 本文介绍了一种改进版的PMD算法——StaQ，它克服了传统PMD方法中的不切实际之处，通过仅存储一定数量的历史Q函数实现了实践上的可行性。StaQ算法具有良好的理论性质，同时也展示了其实用价值，为未来的深度强化学习算法设计提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StaQ+it%21+Growing+neural+networks+for+Policy+Mirror+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13862，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13862&send_immediately=true&force_search=false)

**原文摘要:** In Reinforcement Learning (RL), regularization has emerged as a popular tool
both in theory and practice, typically based either on an entropy bonus or a
Kullback-Leibler divergence that constrains successive policies. In practice,
these approaches have been shown to improve exploration, robustness and
stability, giving rise to popular Deep RL algorithms such as SAC and TRPO.
Policy Mirror Descent (PMD) is a theoretical framework that solves this general
regularized policy optimization problem, however the closed-form solution
involves the sum of all past Q-functions, which is intractable in practice. We
propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions
in memory, and show that for finite and large enough $M$, a convergent
algorithm can be derived, introducing no error in the policy update, unlike
prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong
theoretical guarantees and is competitive with deep RL baselines, while
exhibiting less performance oscillation, paving the way for fully stable deep
RL algorithms and providing a testbed for experimentation with Policy Mirror
Descent.

</details>


### [12] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/abs/2506.13892)
*Samuel Beaussant, Mehdi Mounsif*

**主要类别:** cs.LG

**AI概要:** 本文提出使用S6模型来改进算法蒸馏（AD），以克服之前基于transformer方法在长序列处理上的局限性，并在四个复杂的元强化学习环境中展示了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 先前的算法蒸馏（AD）方法由于transformer的二次复杂度限制，只能应用于简单的离散环境且时间跨度较短。研究旨在通过引入线性扩展的S6模型解决这一问题，使得AD能够处理更长的序列和更复杂的连续环境。

**方法:** 采用新提出的S6模型替代传统的transformer架构，该模型在线性时间内对长距离序列建模表现出色。通过对四个复杂的连续元强化学习环境进行实验，比较了基于S6层构建的Mamba模型与transformer模型的表现。

**结果:** 实验结果表明，在所有测试的复杂连续元强化学习环境中，Mamba模型相较于transformer模型展现出更好的性能。此外，随着上下文长度增加，AD方法的性能得到了进一步提升，甚至可以与最先进的在线元强化学习基准相媲美。

**结论:** 利用S6模型改善了算法蒸馏（AD）方法，使其能够有效地处理更长的训练历史以及更加复杂的环境。这不仅提高了ICRL任务中的表现，也证明了AD方法在适当调整后具有强大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling+Algorithm+Distillation+for+Continuous+Control+with+Mamba，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13892&send_immediately=true&force_search=false)

**原文摘要:** Algorithm Distillation (AD) was recently proposed as a new approach to
perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic
training histories autoregressively with a causal transformer model. However,
due to practical limitations induced by the attention mechanism, experiments
were bottlenecked by the transformer's quadratic complexity and limited to
simple discrete environments with short time horizons. In this work, we propose
leveraging the recently proposed Selective Structured State Space Sequence (S6)
models, which achieved state-of-the-art (SOTA) performance on long-range
sequence modeling while scaling linearly in sequence length. Through four
complex and continuous Meta Reinforcement Learning environments, we demonstrate
the overall superiority of Mamba, a model built with S6 layers, over a
transformer model for AD. Additionally, we show that scaling AD to very long
contexts can improve ICRL performance and make it competitive even with a SOTA
online meta RL baseline.

</details>


### [13] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/abs/2506.13903)
*Christel Sirocchi, Damiano Verda*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架，用于估计基于规则系统的特征贡献，并通过图基特征可视化策略、新的特征重要性度量以及基于特征贡献的规则集比较距离度量来解决复杂规则模型中的特征识别和理解问题。实验结果表明该方法在临床数据集中可以揭示临床特征的预测价值，并有助于识别新的风险因素和生物标志物。


<details>
  <summary>更多</summary>
  
**动机:** 在需要透明性和可信度的领域，如医疗保健中，基于规则的系统由于其固有的可解释性而被广泛使用且通常比黑盒模型更受欢迎。然而，随着基于规则模型变得越来越复杂，辨别关键特征、理解它们之间的相互作用以及跨不同规则集比较特征贡献变得具有挑战性。

**方法:** 研究者提出了一个综合框架，用于估计基于规则系统的特征贡献，引入了基于图的特征可视化策略，一种与基于规则的预测器无关的新颖特征重要性度量，以及一种基于特征贡献比较规则集的距离度量。

**结果:** 通过对两个临床数据集和四种基于规则的方法（决策树、逻辑学习机、关联规则和带有规则提取的神经网络）进行实验，展示了该方法能够揭示临床特征在数据集层面和特定类别层面上的联合预测价值。这些见解可以帮助识别新的风险因素、标志性基因和潜在生物标志物。

**结论:** 所提出的方法在15个公共基准测试上与最先进方法进行了对比分析，显示出竞争性的性能和优越的鲁棒性。这种方法的实现可在GitHub上获取。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+interpretability+of+rule-based+classifiers+through+feature+graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13903，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13903&send_immediately=true&force_search=false)

**原文摘要:** In domains where transparency and trustworthiness are crucial, such as
healthcare, rule-based systems are widely used and often preferred over
black-box models for decision support systems due to their inherent
interpretability. However, as rule-based models grow complex, discerning
crucial features, understanding their interactions, and comparing feature
contributions across different rule sets becomes challenging. To address this,
we propose a comprehensive framework for estimating feature contributions in
rule-based systems, introducing a graph-based feature visualisation strategy, a
novel feature importance metric agnostic to rule-based predictors, and a
distance metric for comparing rule sets based on feature contributions. By
experimenting on two clinical datasets and four rule-based methods (decision
trees, logic learning machines, association rules, and neural networks with
rule extraction), we showcase our method's capability to uncover novel insights
on the combined predictive value of clinical features, both at the dataset and
class-specific levels. These insights can aid in identifying new risk factors,
signature genes, and potential biomarkers, and determining the subset of
patient information that should be prioritised to enhance diagnostic accuracy.
Comparative analysis of the proposed feature importance score with
state-of-the-art methods on 15 public benchmarks demonstrates competitive
performance and superior robustness. The method implementation is available on
GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [14] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/abs/2506.13906)
*Milad Ramezankhani, Janak M. Patel, Anirudh Deodhar, Dagnachew Birru*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的图信息变压器操作符（GITO）架构，用于学习定义在不规则几何和非均匀网格上的复杂偏微分方程系统。GITO结合了混合图变压器(HGT)与变压器神经算子(TNO)，并在基准PDE任务上展示了优于现有基于变压器的神经算子的表现。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决在不规则几何形状和非均匀网格上定义的复杂偏微分方程系统的学习问题，开发一种高效且与网格无关的替代求解器。

**方法:** 设计了一种名为GITO的新架构，它包括两个主要模块：混合图变压器（HGT），利用图神经网络编码局部空间关系并通过变压器捕捉长距离依赖；变压器神经算子（TNO）模块采用线性复杂度的交叉注意力和自注意力层来映射编码输入函数到任意查询位置的预测。

**结果:** 实验结果表明，GITO在基准PDE任务中表现优于现有的基于变压器的神经算子，并且能够实现零样本超分辨率。

**结论:** GITO架构通过结合图神经网络和变压器的优点，在处理不规则几何和非均匀网格上的偏微分方程系统方面展现出优越性能，为工程应用中的有效、与网格无关的替代求解器铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GITO%3A+Graph-Informed+Transformer+Operator+for+Learning+Complex+Partial+Differential+Equations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13906，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13906&send_immediately=true&force_search=false)

**原文摘要:** We present a novel graph-informed transformer operator (GITO) architecture
for learning complex partial differential equation systems defined on irregular
geometries and non-uniform meshes. GITO consists of two main modules: a hybrid
graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages
a graph neural network (GNN) to encode local spatial relationships and a
transformer to capture long-range dependencies. A self-attention fusion layer
integrates the outputs of the GNN and transformer to enable more expressive
feature learning on graph-structured data. TNO module employs linear-complexity
cross-attention and self-attention layers to map encoded input functions to
predictions at arbitrary query locations, ensuring discretization invariance
and enabling zero-shot super-resolution across any mesh. Empirical results on
benchmark PDE tasks demonstrate that GITO outperforms existing
transformer-based neural operators, paving the way for efficient, mesh-agnostic
surrogate solvers in engineering applications.

</details>


### [15] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/abs/2506.13909)
*Xinyuan Tu, Haocheng Zhang, Tao Chengxu, Zuyi Chen*

**主要类别:** cs.LG

**AI概要:** 本文研究了工业时间序列数据中的少样本学习（FSL），特别是在螺钉紧固过程监控方面。通过使用包含16种缺陷类型的2,300个样本的多变量扭矩数据集，引入了一种标签感知的情景采样器来处理多标签序列，并比较了基于度量的学习方法和基于梯度的学习方法的效果。结果显示轻量级CNN架构结合简单的度量学习在数据稀缺的情况下表现更优。


<details>
  <summary>更多</summary>
  
**动机:** 在工业时间序列数据分析中，为每个新缺陷打标签的成本非常高昂。少样本学习(FSL)在视觉领域已经显示出潜力，但在工业时间序列数据中仍待探索。研究旨在通过有限的数据实现有效的缺陷检测。

**方法:** 提出了一个标签感知的情景采样器，将多标签序列转化为多个单标签任务；采用两种FSL范式：基于度量的原型网络(Prototypical Network)和基于梯度的模型无关元学习(MAML)，并分别与三种骨干网络（1D CNN、InceptionTime以及大规模参数的变压器Moment）搭配使用。

**结果:** 在10-shot、3-way评估中，InceptionTime+原型网络组合在多类情况下达到了0.944的加权F1分数，在多标签情况下达到了0.935，优于经过微调的Moment最多达5.3%，同时所需参数和训练时间减少了两个数量级。此外，相对于MAML，度量学习方法在整个骨干网络上始终表现出色，而标签感知采样比传统的基于类别的采样额外提高了1.7%的F1分数。

**结论:** 研究表明，当数据稀缺时，轻量级CNN架构加上简单的度量学习不仅收敛更快而且泛化能力更强，挑战了大型基础模型总是优越的观点。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Few-Shot+Learning+for+Industrial+Time+Series%3A+A+Comparative+Analysis+Using+the+Example+of+Screw-Fastening+Process+Monitoring，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13909&send_immediately=true&force_search=false)

**原文摘要:** Few-shot learning (FSL) has shown promise in vision but remains largely
unexplored for \emph{industrial} time-series data, where annotating every new
defect is prohibitively expensive. We present a systematic FSL study on
screw-fastening process monitoring, using a 2\,300-sample multivariate torque
dataset that covers 16 uni- and multi-factorial defect types. Beyond
benchmarking, we introduce a \textbf{label-aware episodic sampler} that
collapses multi-label sequences into multiple single-label tasks, keeping the
output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical
Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML),
each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter
transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime +
Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the
multi-class regime and \textbf{0.935} in the multi-label regime, outperforming
finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer
parameters and training time. Across all backbones, metric learning
consistently surpasses MAML, and our label-aware sampling yields an additional
1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are
always superior: when data are scarce, lightweight CNN architectures augmented
with simple metric learning not only converge faster but also generalize
better. We release code, data splits and pre-trained weights to foster
reproducible research and to catalyze the adoption of FSL in high-value
manufacturing inspection.

</details>


### [16] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
*Siyu Chen, Heejune Sheen, Xuyuan Xiong, Tianhao Wang, Zhuoran Yang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的统计框架和基于偏置适应的稀疏自动编码器训练算法，该算法能够从理论上保证单义特征的正确恢复，并且通过组偏置适应技术在实际应用中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 现有的稀疏自动编码器（SAE）训练算法通常缺乏严格的数学保证，并且存在诸如超参数敏感性和不稳定性等实际限制。为了解决这些问题，研究者提出了一个新的统计框架来处理特征恢复问题。

**方法:** 研究者首先定义了一个新的统计框架，其中包括通过将多义特征建模为底层单义概念的稀疏混合物来引入一种新的特征可识别性概念。基于这个框架，他们引入了一种新的基于‘偏置适应’的SAE训练算法，该技术自适应地调整神经网络的偏置参数以确保适当的激活稀疏性。此外，还开发了一种改进的经验变体——组偏置适应（GBA）。

**结果:** 研究者从理论上证明了当输入数据是从他们提出的统计模型中采样时，该算法能够正确恢复所有单义特征。并且展示了GBA在应用于具有多达15亿参数的大规模语言模型时，相比基准方法表现出更优的性能。

**结论:** 这项工作代表了揭开SAE训练神秘面纱的基础步骤，提供了第一个具有理论恢复保证的SAE算法，从而通过增强机制可解释性推进了更加透明和值得信赖的人工智能系统的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Taming+Polysemanticity+in+LLMs%3A+Provable+Feature+Recovery+via+Sparse+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14002，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14002&send_immediately=true&force_search=false)

**原文摘要:** We study the challenge of achieving theoretically grounded feature recovery
using Sparse Autoencoders (SAEs) for the interpretation of Large Language
Models. Existing SAE training algorithms often lack rigorous mathematical
guarantees and suffer from practical limitations such as hyperparameter
sensitivity and instability. To address these issues, we first propose a novel
statistical framework for the feature recovery problem, which includes a new
notion of feature identifiability by modeling polysemantic features as sparse
mixtures of underlying monosemantic concepts. Building on this framework, we
introduce a new SAE training algorithm based on ``bias adaptation'', a
technique that adaptively adjusts neural network bias parameters to ensure
appropriate activation sparsity. We theoretically \highlight{prove that this
algorithm correctly recovers all monosemantic features} when input data is
sampled from our proposed statistical model. Furthermore, we develop an
improved empirical variant, Group Bias Adaptation (GBA), and
\highlight{demonstrate its superior performance against benchmark methods when
applied to LLMs with up to 1.5 billion parameters}. This work represents a
foundational step in demystifying SAE training by providing the first SAE
algorithm with theoretical recovery guarantees, thereby advancing the
development of more transparent and trustworthy AI systems through enhanced
mechanistic interpretability.

</details>


### [17] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/abs/2506.13911)
*Arie Soeteman, Balder ten Cate*

**主要类别:** cs.LG

**AI概要:** 本文提出了分层自我图神经网络(HEGNNs)，这是一种受到图同构测试中的个性化-细化范式启发的图神经网络(GNNs)的表达性扩展。HEGNNs 通过层级节点个性化，能够区分至多同构的图，并且在实验中表现出比传统 GNN 架构更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们希望扩展图神经网络（GNNs）的能力，使其能更好地识别和处理图结构数据。受到用于图同构测试的个性化-细化方法的启发，他们提出了一个更具表现力的模型——分层自我图神经网络（HEGNNs）。

**方法:** 研究者开发了分层自我图神经网络（HEGNNs），它是一种图神经网络（GNNs）的拓展，具有层次化的节点个性化特征。此外，他们还提供了有无子图限制情况下HEGNN节点分类器的逻辑表征，使用的是分级混合逻辑。

**结果:** HEGNNs 的分离能力与高阶GNNs、带有局部同态计数特征的GNNs以及基于个性化-细化的颜色细化算法相关联。实验结果显示，HEGNNs 在实践中是可行的，并且相较于传统的GNN架构，无论是否具备局部同态计数特征，都显示出优势。

**结论:** HEGNNs 是一种新的图神经网络框架，它继承了图同构测试的思想，形成了一个越来越具表现力的模型层次，最终能够区分至多同构的图。这种模型不仅在理论上得到了合理的逻辑刻画，在实际应用中也证明了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Logical+Expressiveness+of+Graph+Neural+Networks+with+Hierarchical+Node+Individualization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13911，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13911&send_immediately=true&force_search=false)

**原文摘要:** We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an
expressive extension of graph neural networks (GNNs) with hierarchical node
individualization, inspired by the Individualization-Refinement paradigm for
graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy
of increasingly expressive models that, in the limit, can distinguish graphs up
to isomorphism. We provide a logical characterization of HEGNN node
classifiers, with and without subgraph restrictions, using graded hybrid logic.
This characterization enables us to relate the separating power of HEGNNs to
that of higher-order GNNs, GNNs enriched with local homomorphism count
features, and color refinement algorithms based on
Individualization-Refinement. Our experimental results confirm the practical
feasibility of HEGNNs and show benefits in comparison with traditional GNN
architectures, both with and without local homomorphism count features.

</details>


### [18] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/abs/2506.14020)
*Keyue Jiang, Jiahao Cui, Xiaowen Dong, Laura Toni*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于最优传输位移和马尔可夫随机场的图生成框架BWFlow，该框架能够更好地适应图数据内在的非欧几里得结构和相互连接模式，从而提供更优的概率路径，并在图生成任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 当前的图生成方法通常独立地建模单个节点和边的演变，并假设数据位于欧几里得空间中使用线性插值来构建概率路径，这与图数据内在的非欧几里得结构和互相连接的模式不匹配，可能导致采样收敛问题。

**方法:** 作者通过将图表示为由马尔可夫随机场参数化的连通系统来建模节点和边的联合演变，并利用MRF对象之间的最优传输位移设计用于图生成的概率路径。基于此提出了一个称为BWFlow的流匹配框架，该框架既适用于连续也适用于离散的流匹配算法。

**结果:** 实验评估表明，在普通图生成以及2D/3D分子生成方面，BWFlow具有竞争力的表现、稳定的训练过程以及保证的采样收敛性。

**结论:** BWFlow提供了一个新的视角来解决图生成中的挑战，通过考虑图数据的固有几何结构，它能够产生更有效的概率路径，并且在多种图生成任务上展示了其优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bures-Wasserstein+Flow+Matching+for+Graph+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14020，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14020&send_immediately=true&force_search=false)

**原文摘要:** Graph generation has emerged as a critical task in fields ranging from
molecule design to drug discovery. Contemporary approaches, notably diffusion
and flow-based models, have achieved solid graph generative performance through
constructing a probability path that interpolates between a reference
distribution and the data distribution. However, these methods typically model
the evolution of individual nodes and edges independently and use linear
interpolations to build the path assuming that the data lie in Euclidean space.
We show that this is suboptimal given the intrinsic non-Euclidean structure and
interconnected patterns of graphs, and it poses risks to the sampling
convergence. To build a better probability path, we model the joint evolution
of the nodes and edges by representing graphs as connected systems
parameterized by Markov random fields (MRF). We then leverage the optimal
transport displacement between MRF objects to design the probability path for
graph generation. Based on this, we introduce BWFlow, a flow-matching framework
for graph generation that respects the underlying geometry of graphs and
provides smooth velocities in the probability path. The novel framework can be
adapted to both continuous and discrete flow-matching algorithms. Experimental
evaluations in plain graph generation and 2D/3D molecule generation validate
the effectiveness of BWFlow in graph generation with competitive performance,
stable training, and guaranteed sampling convergence.

</details>


### [19] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales, Arturo Jaramillo, Heli Ricalde Guerrero*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的基于粒子的变分推断方法，称为分支Stein变分梯度下降（BSVGD），它通过引入随机分支机制来探索状态空间，并在理论上保证了分布收敛性。数值实验验证了算法的有效性，并与经典SVGD进行了性能比较。


<details>
  <summary>更多</summary>
  
**动机:** 现有的变分推断方法可能无法有效处理多模态分布的问题，因此需要一种能够更好地探索状态空间并适用于多模态分布的新方法。

**方法:** 作者提出了Branched Stein Variational Gradient Descent (BSVGD)，该方法扩展了传统的Stein Variational Gradient Descent (SVGD) 算法，通过加入一个随机分支机制来促进状态空间的探索。

**结果:** 研究提供了理论上的收敛性保障，并通过数值实验证明了BSVGD算法的有效性。此外，还使用Wasserstein距离和计算时间对BSVGD和SVGD的性能进行了对比。

**结论:** BSVGD为处理多模态分布提供了一个有效的解决方案，其表现优于传统的SVGD方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Branching+Stein+Variational+Gradient+Descent+for+sampling+multimodal+distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13916&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel particle-based variational inference method designed to
work with multimodal distributions. Our approach, referred to as Branched Stein
Variational Gradient Descent (BSVGD), extends the classical Stein Variational
Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism
that encourages the exploration of the state space. In this work, a theoretical
guarantee for the convergence in distribution is presented, as well as
numerical experiments to validate the suitability of our algorithm. Performance
comparisons between the BSVGD and the SVGD are presented using the Wasserstein
distance between samples and the corresponding computational times.

</details>


### [20] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/abs/2506.14113)
*Yitian Zhang, Liheng Ma, Antonios Valkanas, Boris N. Oreshkin, Mark Coates*

**主要类别:** cs.LG

**AI概要:** 本文通过将Koopman算子近似与线性循环神经网络(RNN)联系起来，提出了一种名为SKOLR的方法，该方法结合了可学习的输入信号谱分解和多层感知器(MLP)作为测量函数，并通过高度并行的线性RNN堆栈实现了结构化的Koopman算子。实验表明这种方法在各种预测基准测试和动力系统中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** Koopman算子理论提供了一个非线性动力系统分析和时间序列预测的框架，但其通常具有无限维特性。研究目的是找到能够产生可处理的有限维Koopman算子近似的测量函数。

**方法:** 文章建立了Koopman算子近似与线性RNN之间的联系，提出了SKOLR方法，它使用可学习的输入信号谱分解和MLP作为测量函数，并且通过高效并行的线性RNN堆栈来实现结构化的Koopman算子。

**结果:** 数值实验显示，基于Koopman理论的设计在多个预测基准和动力系统上提供了卓越的表现。

**结论:** 通过将Koopman算子理论与线性RNN相结合，设计出的SKOLR方法在实际应用中展现出了优秀的性能，为非线性动力系统的分析和预测提供了一种有效的新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SKOLR%3A+Structured+Koopman+Operator+Linear+RNN+for+Time-Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14113，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14113&send_immediately=true&force_search=false)

**原文摘要:** Koopman operator theory provides a framework for nonlinear dynamical system
analysis and time-series forecasting by mapping dynamics to a space of
real-valued measurement functions, enabling a linear operator representation.
Despite the advantage of linearity, the operator is generally
infinite-dimensional. Therefore, the objective is to learn measurement
functions that yield a tractable finite-dimensional Koopman operator
approximation. In this work, we establish a connection between Koopman operator
approximation and linear Recurrent Neural Networks (RNNs), which have recently
demonstrated remarkable success in sequence modeling. We show that by
considering an extended state consisting of lagged observations, we can
establish an equivalence between a structured Koopman operator and linear RNN
updates. Building on this connection, we present SKOLR, which integrates a
learnable spectral decomposition of the input signal with a multilayer
perceptron (MLP) as the measurement functions and implements a structured
Koopman operator via a highly parallel linear RNN stack. Numerical experiments
on various forecasting benchmarks and dynamical systems show that this
streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [21] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
*Vaskar Nath, Elaine Lau, Anisha Gunjal, Manasi Sharma, Nikhil Baharte, Sean Hendryx*

**主要类别:** cs.LG

**AI概要:** 本文研究了通过可验证奖励的强化学习训练推理模型以解决新问题的过程。研究发现，RLVR主要通过两种方式提高性能：将pass@$k$压缩为pass@1以及通过'能力增益'让模型学会解决以前无法解决的新问题。此外，还介绍了一种新的在线训练算法$	ext{Guide}$，该算法能自适应地在模型上下文中加入提示，并调整重要性采样比率以优化无提示情况下的策略。实验表明，Guide-GRPO相较于传统的GRPO，在数学基准测试中提高了高达4%的宏观平均改进率。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探索基于可验证奖励的强化学习（RLVR）如何促使推理模型学习解决新问题的能力，并在此基础上提出一种新的在线训练算法来进一步提升模型解决问题的能力。

**方法:** 本研究采用的方法包括但不限于使用不同规模的模型（从0.5B到72B参数）对超过50万个涉及数学、科学和代码领域的推理问题进行分析；引入自然语言指导以改善模型在特定背景下的解题能力；开发出名为$	ext{Guide}$的新在线训练算法，并对其进行变体设计以便于与GRPO和PPO等现有技术结合。

**结果:** 研究表明，尽管跨模型尺度存在能力增益现象，但主要是通过自我蒸馏过程学会了处理新问题。另外，通过$	ext{Guide}$算法，特别是在7B和32B参数模型上应用时，相对于基础版本，数学基准上的宏观平均表现有显著提升，最高可达4%。

**结论:** 本文揭示了RLVR促进推理模型性能提升的机制，并提出了$	ext{Guide}$这一创新性的在线训练算法，它能够有效地帮助模型在缺乏明确指示的情况下也能够增强其解决问题的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Guidance+Accelerates+Reinforcement+Learning+of+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13923，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13923&send_immediately=true&force_search=false)

**原文摘要:** We study the process through which reasoning models trained with
reinforcement learning on verifiable rewards (RLVR) can learn to solve new
problems. We find that RLVR drives performance through two main means: (1) by
compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models
learn to solve new problems that they previously could not solve even at high
$k$. We find that while capability gain exists across model scales, learning to
solve new problems is primarily driven through self-distillation. We
demonstrate these findings across model scales ranging from 0.5B to 72B on
>500,000 reasoning problems with prompts and verifiable final answers across
math, science, and code domains. We further show that we can significantly
improve pass@$k$ rates by leveraging natural language guidance for the model to
consider within context while still requiring the model to derive a solution
chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new
class of online training algorithms. $\text{Guide}$ adaptively incorporates
hints into the model's context on problems for which all rollouts were
initially incorrect and adjusts the importance sampling ratio for the
"off-policy" trajectories in order to optimize the policy for contexts in which
the hints are no longer present. We describe variants of $\text{Guide}$ for
GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter
models improves generalization over its vanilla counterpart with up to 4$\%$
macro-average improvement across math benchmarks. We include careful ablations
to analyze $\text{Guide}$'s components and theoretically analyze Guide's
learning efficiency.

</details>


### [22] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/abs/2506.14202)
*Makoto Shing, Takuya Akiba*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为DiffusionBlocks的训练框架，通过将神经网络块解释为连续时间扩散过程中的去噪操作，并独立地训练这些块，从而在生成任务中实现显著的内存效率并保持与传统反向传播相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 端到终的反向传播训练大型神经网络会产生显著的内存瓶颈，限制了对最先进AI研究的访问。

**方法:** DiffusionBlocks，一种新的训练框架，将神经网络块视为在连续时间扩散过程中执行去噪操作，并基于等累积概率质量优化噪声水平分配。

**结果:** 实验表明，在图像生成和语言建模任务上，该方法能根据块的数量成比例地减少内存使用，同时达到优越的性能。

**结论:** DiffusionBlocks为在有限计算资源下进行大规模神经网络训练提供了一个有希望的途径，有助于普及对大型模型训练的访问。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DiffusionBlocks%3A+Blockwise+Training+for+Generative+Models+via+Score-Based+Diffusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14202，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14202&send_immediately=true&force_search=false)

**原文摘要:** Training large neural networks with end-to-end backpropagation creates
significant memory bottlenecks, limiting accessibility to state-of-the-art AI
research. We propose $\textit{DiffusionBlocks}$, a novel training framework
that interprets neural network blocks as performing denoising operations in a
continuous-time diffusion process. By partitioning the network into
independently trainable blocks and optimizing noise level assignments based on
equal cumulative probability mass, our approach achieves significant memory
efficiency while maintaining competitive performance compared to traditional
backpropagation in generative tasks. Experiments on image generation and
language modeling tasks demonstrate memory reduction proportional to the number
of blocks while achieving superior performance. DiffusionBlocks provides a
promising pathway for democratizing access to large-scale neural network
training with limited computational resources.

</details>


### [23] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/abs/2506.13935)
*Vishesh Kumar Tanwar, Soumik Sarkar, Asheesh K. Singh, Sajal K. Das*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为ReinDSplit的新框架，该框架使用强化学习动态调整深度神经网络在不同设备上的分割点，以优化分布式机器学习在农业生态系统中的效率和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的分裂学习框架采用一刀切的策略，这在农业生态系统中是有限制的，因为边缘害虫监测设备在计算能力、能源限制和连接性方面存在巨大异质性。这种异质性导致了落后瓶颈、资源利用效率低下以及模型性能受损。

**方法:** 研究人员引入了ReinDSplit框架，这是一种基于强化学习的方法，它能够为每个设备动态地定制深度神经网络的分割点，通过Q-learning代理作为自适应协调器，在不同的设备之间平衡工作负载和延迟阈值，从而缓解计算饥饿或过载的问题。

**结果:** 在三个昆虫分类数据集上，使用ResNet18、GoogleNet和MobileNetV2进行评估后，ReinDSplit使用MobileNetV2达到了94.31%的准确率。

**结论:** ReinDSplit不仅在农业领域内提高了资源效率和隐私保护，而且在异构环境中实现了可扩展性，并且代表了分裂学习领域的一个范式转变。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReinDSplit%3A+Reinforced+Dynamic+Split+Learning+for+Pest+Recognition+in+Precision+Agriculture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13935&send_immediately=true&force_search=false)

**原文摘要:** To empower precision agriculture through distributed machine learning (DML),
split learning (SL) has emerged as a promising paradigm, partitioning deep
neural networks (DNNs) between edge devices and servers to reduce computational
burdens and preserve data privacy. However, conventional SL frameworks'
one-split-fits-all strategy is a critical limitation in agricultural ecosystems
where edge insect monitoring devices exhibit vast heterogeneity in
computational power, energy constraints, and connectivity. This leads to
straggler bottlenecks, inefficient resource utilization, and compromised model
performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement
learning (RL)-driven framework that dynamically tailors DNN split points for
each device, optimizing efficiency without sacrificing accuracy. Specifically,
a Q-learning agent acts as an adaptive orchestrator, balancing workloads and
latency thresholds across devices to mitigate computational starvation or
overload. By framing split layer selection as a finite-state Markov decision
process, ReinDSplit convergence ensures that highly constrained devices
contribute meaningfully to model training over time. Evaluated on three insect
classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit
achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit
pioneers a paradigm shift in SL by harmonizing RL for resource efficiency,
privacy, and scalability in heterogeneous environments.

</details>


### [24] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/abs/2506.14262)
*Mohammad Emtiyaz Khan*

**主要类别:** cs.LG

**AI概要:** 研究提出了机器快速适应的机制，指出所有适应方法都可以看作是修正近似后验的不同方式。更准确的后验导致较小的修正，从而意味着更快的适应。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在模型适应方面取得了许多进展，但机器仍然缺乏像人类和动物那样自然学习适应的机制。该研究旨在探索一种让机器能够快速学习适应的自然机制。

**方法:** 通过Khan和Rue（2023）提出的贝叶斯学习规则的双重视角来分析适应过程中产生的干扰，并将这种干扰特征化为过去数据上的自然梯度不匹配。

**结果:** 展示了后验修正可以作为机器快速学习适应的一种自然机制，并提供了多个例子来证明这一点。

**结论:** 研究揭示了机器可以通过修正近似后验来实现快速适应，这为开发更加灵活、类似人类学习能力的人工智能提供了一种新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Adaptation+as+Posterior+Correction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14262，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14262&send_immediately=true&force_search=false)

**原文摘要:** Adaptation is the holy grail of intelligence, but even the best AI models
(like GPT) lack the adaptivity of toddlers. So the question remains: how can
machines adapt quickly? Despite a lot of progress on model adaptation to
facilitate continual and federated learning, as well as model merging, editing,
unlearning, etc., little is known about the mechanisms by which machines can
naturally learn to adapt in a similar way as humans and animals. Here, we show
that all such adaptation methods can be seen as different ways of `correcting'
the approximate posteriors. More accurate posteriors lead to smaller
corrections, which in turn imply quicker adaptation. The result is obtained by
using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023)
where interference created during adaptation is characterized by the
natural-gradient mismatch over the past data. We present many examples to
demonstrate the use of posterior-correction as a natural mechanism for the
machines to learn to adapt quickly.

</details>


### [25] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/abs/2506.13958)
*Leonardo Guiducci, Antonio Rizzo, Giovanna Maria Dimitri*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种系统的事后可解释性框架，用于分析内在动机如何影响Elastic Decision Transformers (EDTs) 中学习到的嵌入表示，并通过统计分析揭示了不同内在动机变体产生根本不同的表示结构。


<details>
  <summary>更多</summary>
  
**动机:** 尽管将内在动机机制融入Elastic Decision Transformers(EDTs)已被证明可以改善探索任务中的表现，但这种改进背后的表征机制仍不清楚。研究者希望理解内在动机是如何塑造EDTs中学习到的嵌入表示的。

**方法:** 研究者引入了一个系统的事后可解释性框架来分析内在动机对EDTs中学习到的嵌入的影响。他们通过对嵌入属性（包括协方差结构、向量大小和正交性）进行统计分析，展示了不同内在动机变体所产生的根本不同的表示结构。

**结果:** 研究表明，不同类型的内在动机创建了本质上不同的表示结构，并且在特定环境下的嵌入指标与性能之间存在相关模式，这解释了为什么内在动机会提高策略学习的效果。

**结论:** 内在动机不仅作为简单的探索奖励发挥作用，而且作为一个表征先验，以生物上合理的方式塑造嵌入几何，创造出有利于更好决策的环境特定组织结构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Explainable+Offline+RL%3A+Analyzing+Representations+in+Intrinsically+Motivated+Decision+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13958，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13958&send_immediately=true&force_search=false)

**原文摘要:** Elastic Decision Transformers (EDTs) have proved to be particularly
successful in offline reinforcement learning, offering a flexible framework
that unifies sequence modeling with decision-making under uncertainty. Recent
research has shown that incorporating intrinsic motivation mechanisms into EDTs
improves performance across exploration tasks, yet the representational
mechanisms underlying these improvements remain unexplored. In this paper, we
introduce a systematic post-hoc explainability framework to analyze how
intrinsic motivation shapes learned embeddings in EDTs. Through statistical
analysis of embedding properties (including covariance structure, vector
magnitudes, and orthogonality), we reveal that different intrinsic motivation
variants create fundamentally different representational structures. Our
analysis demonstrates environment-specific correlation patterns between
embedding metrics and performance that explain why intrinsic motivation
improves policy learning. These findings show that intrinsic motivation
operates beyond simple exploration bonuses, acting as a representational prior
that shapes embedding geometry in biologically plausible ways, creating
environment-specific organizational structures that facilitate better
decision-making.

</details>


### [26] [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
*Bai Cong, Nico Daheim, Yuesong Shen, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff*

**主要类别:** cs.LG

**AI概要:** 本文探讨了使用一种新的变分算法IVON来改进LoRA微调，该方法不仅易于实现且成本与AdamW相当，还能通过简单的后验剪枝技术大幅提高多个指标。实验结果表明，在十亿级的大规模语言模型上，如Llama和Qwen系列，相比AdamW和其他贝叶斯方法（如Laplace-LoRA和BLoB），IVON在准确性上提高了1.3%，ECE降低了5.4%。


<details>
  <summary>更多</summary>
  
**动机:** 尽管贝叶斯方法可以改善校准，但它们对其他度量标准（例如准确性）的影响很小，有时甚至可能有害，并且还会增加计算开销。因此，研究者们希望通过引入IVON这一新型变分算法来解决这些问题，同时保持或提升模型性能而不显著增加成本。

**方法:** 采用了一种新近提出的名为IVON的变分算法进行LoRA微调，结合简单的后验剪枝技术以优化模型表现。

**结果:** 在大规模语言模型上的测试显示，相对于AdamW等现有优化器及其它贝叶斯方法，IVON不仅能够有效提升模型精度（如在常识推理任务中提升了1.3%），还大幅降低了预期校准误差（ECE减少了5.4%）。

**结论:** 研究表明，利用IVON进行变分学习可有效地改进LoRA微调过程，提供更好的模型性能，同时避免了传统贝叶斯方法所带来的额外复杂性和成本问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+LoRA+with+Variational+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14280，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14280&send_immediately=true&force_search=false)

**原文摘要:** Bayesian methods have recently been used to improve LoRA finetuning and,
although they improve calibration, their effect on other metrics (such as
accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian
methods also increase computational overheads and require additional tricks for
them to work well. Here, we fix these issues by using a recently proposed
variational algorithm called IVON. We show that IVON is easy to implement and
has similar costs to AdamW, and yet it can also drastically improve many
metrics by using a simple posterior pruning technique. We present extensive
results on billion-scale LLMs (Llama and Qwen series) going way beyond the
scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B
model on a set of commonsense reasoning tasks and improve accuracy over AdamW
by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian
methods like Laplace-LoRA and BLoB. Overall, our results show that variational
learning with IVON can effectively improve LoRA finetuning.

</details>


### [27] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/abs/2506.13972)
*Zhiqi Wang, Chengyu Zhang, Yuetian Chen, Nathalie Baracaldo, Swanand Kadhe, Lei Yu*

**主要类别:** cs.LG

**AI概要:** 本文揭示了成员推理攻击（MIAs）中不同攻击方法及同一方法多次实例化之间存在的差异，并通过基于覆盖和稳定性分析的新框架系统地研究这些差异。提出了一个集成框架，以利用最先进的MIAs的优势同时考虑它们之间的差异，从而构建更强大的攻击并提供更加稳健全面的隐私评估方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的成员推理攻击（MIAs）研究主要集中在提高或评估如AUC、准确率等性能指标上，但忽视了不同攻击方法以及同一方法不同实例间存在的差异性。这种差异对MIAs作为隐私评估工具的可靠性和完整性有重要影响。

**方法:** 提出了一种新的基于覆盖和稳定性分析的研究框架来系统地调查MIAs中的差异。此外，还设计了一个包含三种不同策略的集成框架，旨在结合最新MIAs的优点同时处理它们之间的差异。

**结果:** 广泛的实验显示了MIAs中存在的显著差异及其潜在原因，以及这对隐私评估带来的更广泛影响。提出的集成框架能够构造出更有效的攻击手段，并为隐私评估提供了更加稳固且全面的方法论。

**结论:** 认识到MIAs内部分歧的重要性，并通过创新性的研究框架加以探讨，最终提出了一种新的集成方法，不仅增强了攻击能力，也为机器学习模型的隐私保护评价开辟了更为科学合理的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Membership+Inference+Attacks+as+Privacy+Tools%3A+Reliability%2C+Disparity+and+Ensemble，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13972&send_immediately=true&force_search=false)

**原文摘要:** Membership inference attacks (MIAs) pose a significant threat to the privacy
of machine learning models and are widely used as tools for privacy assessment,
auditing, and machine unlearning. While prior MIA research has primarily
focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either
by developing new methods to enhance these metrics or using them to evaluate
privacy solutions - we found that it overlooks the disparities among different
attacks. These disparities, both between distinct attack methods and between
multiple instantiations of the same method, have crucial implications for the
reliability and completeness of MIAs as privacy evaluation tools. In this
paper, we systematically investigate these disparities through a novel
framework based on coverage and stability analysis. Extensive experiments
reveal significant disparities in MIAs, their potential causes, and their
broader implications for privacy evaluation. To address these challenges, we
propose an ensemble framework with three distinct strategies to harness the
strengths of state-of-the-art MIAs while accounting for their disparities. This
framework not only enables the construction of more powerful attacks but also
provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [28] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/abs/2506.14291)
*Ben Finkelshtein, İsmail İlkan Ceylan, Michael Bronstein, Ron Levie*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种设计节点级任务图基础模型的方法，通过研究图基础模型必须遵守的对称性，并证明了所得到的网络是遵循这些对称性的多重集上的通用逼近器。实验结果表明该方法在零样本学习和随着训练图数量增加时均有优异表现。


<details>
  <summary>更多</summary>
  
**动机:** 当前图机器学习架构通常是为特定数据集上的特定任务量身定制的，这限制了它们更广泛的应用。因此，研究者们正在探索如何构建能够跨任意图和特征泛化的图基础模型。

**方法:** 研究者首先系统地研究了图基础模型必须尊重的对称性，认为除了常见的节点置换等变性外，还需要标签置换等变性和特征置换不变性。接着，他们刻画了对节点和标签置换等变以及对特征置换不变的线性变换空间，并且证明了由此产生的网络是上述对称性下的多重集上的一个通用逼近器。最后，基于局部邻域诱导出的特征多重集上使用这种层来获得一类用于节点属性预测的图基础模型。

**结果:** 通过对29个真实世界节点分类数据集进行广泛的实验，验证了该方法的有效性，展示了强大的零样本性能，并且随着训练图数量的增加而持续改进。

**结论:** 研究提供了一个从第一原理出发设计用于节点级任务的图基础模型的方案，它不仅考虑了标准的节点置换等变性，还强调了对于标签置换等变性和特征置换不变性的需求，从而使得模型能够在不同的图结构和特征分布中具有更好的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Equivariance+Everywhere+All+At+Once%3A+A+Recipe+for+Graph+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14291&send_immediately=true&force_search=false)

**原文摘要:** Graph machine learning architectures are typically tailored to specific tasks
on specific datasets, which hinders their broader applicability. This has led
to a new quest in graph machine learning: how to build graph foundation models
capable of generalizing across arbitrary graphs and features? In this work, we
present a recipe for designing graph foundation models for node-level tasks
from first principles. The key ingredient underpinning our study is a
systematic investigation of the symmetries that a graph foundation model must
respect. In a nutshell, we argue that label permutation-equivariance alongside
feature permutation-invariance are necessary in addition to the common node
permutation-equivariance on each local neighborhood of the graph. To this end,
we first characterize the space of linear transformations that are equivariant
to permutations of nodes and labels, and invariant to permutations of features.
We then prove that the resulting network is a universal approximator on
multisets that respect the aforementioned symmetries. Our recipe uses such
layers on the multiset of features induced by the local neighborhood of the
graph to obtain a class of graph foundation models for node property
prediction. We validate our approach through extensive experiments on 29
real-world node classification datasets, demonstrating both strong zero-shot
empirical performance and consistent improvement as the number of training
graphs increases.

</details>


### [29] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/abs/2506.13974)
*Michael Crawshaw, Blake Woodworth, Mingrui Liu*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Constant+Stepsize+Local+GD+for+Logistic+Regression%3A+Acceleration+by+Instability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13974，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13974&send_immediately=true&force_search=false)

**原文摘要:** Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous
objectives requires stepsizes $\eta \leq 1/K$ where $K$ is the communication
interval, which ensures monotonic decrease of the objective. In contrast, we
analyze Local Gradient Descent for logistic regression with separable,
heterogeneous data using any stepsize $\eta > 0$. With $R$ communication rounds
and $M$ clients, we show convergence at a rate $\mathcal{O}(1/\eta K R)$ after
an initial unstable phase lasting for $\widetilde{\mathcal{O}}(\eta K M)$
rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general
smooth, convex objectives. Our analysis parallels the single machine analysis
of~\cite{wu2024large} in which instability is caused by extremely large
stepsizes, but in our setting another source of instability is large local
updates with heterogeneous objectives.

</details>


### [30] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/abs/2506.14597)
*Thomas Newman, Christopher Nemeth, Matthew Jones, Philip Jonathan*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种时空反演框架，该框架结合了计算流体动力学的深度学习代理模型和顺序蒙特卡洛算法，用于动态流场中温室气体排放率和源位置的实时贝叶斯推断。


<details>
  <summary>更多</summary>
  
**动机:** 实现实时识别和量化瞬态大气条件下温室气体排放是环境监测中的一个关键挑战。

**方法:** 研究人员开发了一个时空反演框架，将计算流体动力学（CFD）的深度学习替代模型嵌入到顺序蒙特卡洛算法中，以执行动态流场中排放速率和源位置的贝叶斯推断。使用多层感知器代替昂贵的数值求解器，该模型经过高保真CFD输出训练，能够捕捉气体扩散的空间异质性和时间演变，同时提供接近实时的预测。

**结果:** 在Chilbolton甲烷释放数据集上的验证表明，与全CFD求解器和高斯烟羽模型相比，该方法具有相当的准确性，并且运行时间快了几个数量级。进一步在模拟遮挡流场景下的实验确认了其在复杂环境中的鲁棒性。

**结论:** 这项工作调和了物理真实性与计算可行性之间的矛盾，为工业排放监测及其他环境和科学建模中的时间敏感时空反演任务提供了可扩展解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Learning+Surrogates+for+Real-Time+Gas+Emission+Inversion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14597，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14597&send_immediately=true&force_search=false)

**原文摘要:** Real-time identification and quantification of greenhouse-gas emissions under
transient atmospheric conditions is a critical challenge in environmental
monitoring. We introduce a spatio-temporal inversion framework that embeds a
deep-learning surrogate of computational fluid dynamics (CFD) within a
sequential Monte Carlo algorithm to perform Bayesian inference of both emission
rate and source location in dynamic flow fields. By substituting costly
numerical solvers with a multilayer perceptron trained on high-fidelity CFD
outputs, our surrogate captures spatial heterogeneity and temporal evolution of
gas dispersion, while delivering near-real-time predictions. Validation on the
Chilbolton methane release dataset demonstrates comparable accuracy to full CFD
solvers and Gaussian plume models, yet achieves orders-of-magnitude faster
runtimes. Further experiments under simulated obstructed-flow scenarios confirm
robustness in complex environments. This work reconciles physical fidelity with
computational feasibility, offering a scalable solution for industrial
emissions monitoring and other time-sensitive spatio-temporal inversion tasks
in environmental and scientific modeling.

</details>


### [31] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/abs/2506.13981)
*Thanh Dan Bui*

**主要类别:** cs.LG

**AI概要:** 提出了一种结合了基于ResNet的降噪模块、时间自注意力机制和混合LSTM-Transformer核心的深度学习框架HAELT，用于高频股票价格预测。该模型在苹果公司2024年1月至2025年5月每小时数据上测试时达到了最高的F1分数，表明其在金融预测和算法交易中的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 由于非平稳性、噪音和波动性的存在，高频股票价格预测具有挑战性。为了解决这些问题，并提高股票价格预测的准确性与实用性，提出了新的深度学习框架。

**方法:** 设计了Hybrid Attentive Ensemble Learning Transformer (HAELT) 框架，它整合了一个基于ResNet的噪声缓解模块来减少噪音干扰，利用时间自注意力机制动态关注相关历史信息，以及一个能够捕捉局部和长距离依赖关系的混合LSTM-Transformer核心。此外，根据最近的表现自适应地集成这些组件。

**结果:** 在2024年1月至2025年5月期间苹果公司的每小时股价数据上进行评估，HAELT在测试集上获得了最高的F1-Score，有效识别了股价上涨和下跌的趋势。

**结论:** 研究证明了HAELT框架在面对股市固有的非平稳性和波动性问题时的有效性，显示出了在稳健的金融预测及算法交易应用中的巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HAELT%3A+A+Hybrid+Attentive+Ensemble+Learning+Transformer+Framework+for+High-Frequency+Stock+Price+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13981，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13981&send_immediately=true&force_search=false)

**原文摘要:** High-frequency stock price prediction is challenging due to non-stationarity,
noise, and volatility. To tackle these issues, we propose the Hybrid Attentive
Ensemble Learning Transformer (HAELT), a deep learning framework combining a
ResNet-based noise-mitigation module, temporal self-attention for dynamic focus
on relevant history, and a hybrid LSTM-Transformer core that captures both
local and long-range dependencies. These components are adaptively ensembled
based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from
Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set,
effectively identifying both upward and downward price movements. This
demonstrates HAELT's potential for robust, practical financial forecasting and
algorithmic trading.

</details>


### [32] [On the Hardness of Bandit Learning](https://arxiv.org/abs/2506.14746)
*Nataly Brukhim, Aldo Pacchiano, Miroslav Dudik, Robert Schapire*

**主要类别:** cs.LG

**AI概要:** 本文研究了在已知但任意的函数类F下，真实奖励函数f所属的最佳臂识别问题。文章探讨了哪些类是可以学习的以及如何学习的问题，并揭示了结构化bandit学习中的局限性，指出了通过类似维度量来确定可学习类的方法对于bandit学习并不适用，并证明了一个计算上的困难结果：存在一类奖励函数，尽管只需要最多两个查询就可以找到最优动作，但除非RP=NP否则没有算法能够在多项式时间内完成。此外还讨论了噪声下的学习、噪声模型之间的权衡以及查询复杂性和遗憾最小化之间的关系。


<details>
  <summary>更多</summary>
  
**动机:** 动机在于建立一个类似于分类PAC框架的一般bandit学习理论，以理解在给定函数类中哪些是可学习的以及如何进行学习。

**方法:** 采用理论分析方法，包括证明不存在能够表征bandit学习性的组合维度，以及构造一个奖励函数类来证明计算难度。

**结果:** 发现通过类似维度量（如VC维度）来决定可学习性的方法不适用于bandit学习；证明了即使在一个有限类中也没有组合维度可以表征bandit学习性；并且展示了一个计算上的困难性结果，即存在某个奖励函数类，使得在多项式时间内找到最优动作是不可能的，除非RP=NP。

**结论:** 结论是bandit学习具有内在的计算难度，这与传统学习理论的结果不同。同时，文中还进一步探讨了其他主题，例如噪声下的学习、噪声模型之间的权衡以及查询复杂度和遗憾最小化的关系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Hardness+of+Bandit+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14746，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14746&send_immediately=true&force_search=false)

**原文摘要:** We study the task of bandit learning, also known as best-arm identification,
under the assumption that the true reward function f belongs to a known, but
arbitrary, function class F. We seek a general theory of bandit learnability,
akin to the PAC framework for classification. Our investigation is guided by
the following two questions: (1) which classes F are learnable, and (2) how
they are learnable. For example, in the case of binary PAC classification,
learnability is fully determined by a combinatorial dimension - the VC
dimension- and can be attained via a simple algorithmic principle, namely,
empirical risk minimization (ERM). In contrast to classical learning-theoretic
results, our findings reveal limitations of learning in structured bandits,
offering insights into the boundaries of bandit learnability. First, for the
question of "which", we show that the paradigm of identifying the learnable
classes via a dimension-like quantity fails for bandit learning. We give a
simple proof demonstrating that no combinatorial dimension can characterize
bandit learnability, even in finite classes, following a standard definition of
dimension introduced by Ben-David et al. (2019). For the question of "how", we
prove a computational hardness result: we construct a reward function class for
which at most two queries are needed to find the optimal action, yet no
algorithm can do so in polynomial time unless RP=NP. We also prove that this
class admits efficient algorithms for standard algorithmic operations often
considered in learning theory, such as an ERM. This implies that computational
hardness is in this case inherent to the task of bandit learning. Beyond these
results, we investigate additional themes such as learning under noise,
trade-offs between noise models, and the relationship between query complexity
and regret minimization.

</details>


### [33] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/abs/2506.13987)
*Md Abrar Jahin, Adiba Abid, M. F. Mridha*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的框架QCL-MixNet，它结合了量子启发的对比学习和k近邻引导的动态混合策略，以解决不平衡数据集中的鲁棒分类问题。在18个真实世界的数据集上进行了广泛的实验，并且证明了该方法在宏观F1分数和召回率方面优于20种最新的机器学习、深度学习以及基于图神经网络的方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法如成本敏感学习、过采样和图神经网络在处理类别不平衡表格数据时存在过拟合、标签噪声和低密度区域泛化能力差的问题。为了克服这些挑战，研究提出了一个新的解决方案。

**方法:** QCL-MixNet，一种新的量子信息对比学习框架，通过以下三个核心创新：（i）受量子纠缠启发的层，通过正弦变换和门控注意力机制建模复杂的特征交互；（ii）样本感知混合策略，自适应地插值语义相似实例的特征表示以增强少数类别的表达；（iii）混合损失函数，结合焦点重加权、监督对比学习、三元组边缘损失和方差正则化来提高类内紧凑性和类间可分离性。

**结果:** 在18个现实世界的不平衡数据集（二分类和多分类）上的广泛实验表明，QCL-MixNet在宏平均F1得分和召回率上始终优于20种最先进机器学习、深度学习和基于GNN的基线方法，通常有显著的优势。消融研究进一步验证了每个架构组件的关键作用。

**结论:** 结果确立了QCL-MixNet作为专家系统中表格不平衡处理的新基准。理论分析强化了其表现力、泛化能力和优化稳健性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quantum-Informed+Contrastive+Learning+with+Dynamic+Mixup+Augmentation+for+Class-Imbalanced+Expert+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13987，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13987&send_immediately=true&force_search=false)

**原文摘要:** Expert systems often operate in domains characterized by class-imbalanced
tabular data, where detecting rare but critical instances is essential for
safety and reliability. While conventional approaches, such as cost-sensitive
learning, oversampling, and graph neural networks, provide partial solutions,
they suffer from drawbacks like overfitting, label noise, and poor
generalization in low-density regions. To address these challenges, we propose
QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented
with k-nearest neighbor (kNN) guided dynamic mixup for robust classification
under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum
Entanglement-inspired layer that models complex feature interactions through
sinusoidal transformations and gated attention, (ii) a sample-aware mixup
strategy that adaptively interpolates feature representations of semantically
similar instances to enhance minority class representation, and (iii) a hybrid
loss function that unifies focal reweighting, supervised contrastive learning,
triplet margin loss, and variance regularization to improve both intra-class
compactness and inter-class separability. Extensive experiments on 18
real-world imbalanced datasets (binary and multi-class) demonstrate that
QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep
learning, and GNN-based baselines in macro-F1 and recall, often by substantial
margins. Ablation studies further validate the critical role of each
architectural component. Our results establish QCL-MixNet as a new benchmark
for tabular imbalance handling in expert systems. Theoretical analyses
reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [34] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
*An Luo, Xun Xian, Jin Du, Fangqiao Tian, Ganghua Wang, Ming Zhong, Shengchun Zhao, Xuan Bi, Zirui Liu, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding*

**主要类别:** cs.LG

**AI概要:** 本研究提出了AssistedDS基准，用于评估大型语言模型在表格预测任务中处理领域知识的能力。结果表明，这些模型往往无批判地采纳提供的信息，且有益指导不足以抵消负面信息的影响，在Kaggle数据集上处理时间序列数据和特征工程时也存在错误。


<details>
  <summary>更多</summary>
  
**动机:** 探索大型语言模型是否能够像人类数据科学家一样在实践中有效利用外部领域知识。

**方法:** 引入了名为AssistedDS的基准测试，该测试包括合成数据集和真实世界Kaggle竞赛，并附有精心策划的帮助性和对抗性文档。通过这些文档提供关于数据清洗、特征工程和模型选择的具体见解。对最先进LLM的能力进行评估，以区分并应用有利与有害的领域知识。

**结果:** 1. LLMs通常表现出对所提供信息的无批判接受，当引入对抗内容时会显著损害其预测性能。2. 有益指导往往不足以抵消对抗信息的负面影响。3. 在Kaggle数据集中，LLMs在处理时间序列数据、跨不同折叠应用一致的特征工程以及正确解释分类变量方面经常出错。

**结论:** 当前模型在批判性评估和利用专家知识方面存在明显差距，强调了开发更加稳健的知识感知型自动化数据科学系统的重要研究方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AssistedDS%3A+Benchmarking+How+External+Domain+Knowledge+Assists+LLMs+in+Automated+Data+Science，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13992&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have advanced the automation of data science
workflows. Yet it remains unclear whether they can critically leverage external
domain knowledge as human data scientists do in practice. To answer this
question, we introduce AssistedDS (Assisted Data Science), a benchmark designed
to systematically evaluate how LLMs handle domain knowledge in tabular
prediction tasks. AssistedDS features both synthetic datasets with explicitly
known generative mechanisms and real-world Kaggle competitions, each
accompanied by curated bundles of helpful and adversarial documents. These
documents provide domain-specific insights into data cleaning, feature
engineering, and model selection. We assess state-of-the-art LLMs on their
ability to discern and apply beneficial versus harmful domain knowledge,
evaluating submission validity, information recall, and predictive performance.
Our results demonstrate three key findings: (1) LLMs frequently exhibit an
uncritical adoption of provided information, significantly impairing their
predictive performance when adversarial content is introduced, (2) helpful
guidance is often insufficient to counteract the negative influence of
adversarial information, and (3) in Kaggle datasets, LLMs often make errors in
handling time-series data, applying consistent feature engineering across
different folds, and interpreting categorical variables correctly. These
findings highlight a substantial gap in current models' ability to critically
evaluate and leverage expert knowledge, underscoring an essential research
direction for developing more robust, knowledge-aware automated data science
systems.

</details>


### [35] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/abs/2506.13996)
*Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He*

**主要类别:** cs.LG

**AI概要:** 本文介绍了Arctic Long Sequence Training (ALST)，一种针对长序列训练的优化方法，能够显著提高在单个或多个GPU上训练如Llama 8B模型时支持的序列长度，并且与Hugging Face模型完全兼容。


<details>
  <summary>更多</summary>
  
**动机:** 现有的开源解决方案对于长序列训练的支持有限，导致在资源受限的情况下难以进行长序列的训练。特别是使用现代NVIDIA H100 80GB GPU集群时，基于Hugging Face的基础模型训练超过32K序列长度的Llama 8B模型会遇到内存不足的问题。

**方法:** 提出了一种名为Arctic Long Sequence Training (ALST)的方法，该方法结合了对单个GPU和多GPU内存优化的技术，使得多种Hugging Face模型可以开箱即用的方式训练数百万长度的序列。

**结果:** ALST能够在单个H100 GPU上支持500K序列长度的Llama 8B模型训练，在单个8xH100 GPU节点上支持3.7M序列长度，在4节点集群上甚至超过了15M序列长度。相比于32K序列长度基线，后者提高了超过400倍。

**结论:** 通过ALST，研究者们解决了长序列训练面临的挑战，极大地提升了在单个或多个GPU上可支持的最大序列长度，并且这一技术已经开源，适用于广泛的Hugging Face模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Arctic+Long+Sequence+Training%3A+Scalable+And+Efficient+Training+For+Multi-Million+Token+Sequences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13996&send_immediately=true&force_search=false)

**原文摘要:** Long sequences are critical for applications like RAG, long document
summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout,
support max sequence length of up to 10 million tokens. However, outside of
enterprise labs, long sequence training is challenging for the AI community
with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B
model with sequence over 32K runs out of memory on a basic Hugging Face (HF)
model due to two reasons: i) LLM training workloads are not optimized to fully
leverage a single GPU memory, ii) existing solutions for leveraging multiple
GPU memory are not easily available to HF models, making long sequence training
inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a
combination of attention-agnostic single GPU and multi-GPU memory
optimizations, that enables it to support out-of-box training of multi-million
sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a
single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node
cluster, an increase of over 400x compared to the 32K baseline for the latter.
ALST is fully compatible with HF models and open-sourced via Deepspeed
https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic
Training
https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [36] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/abs/2506.14003)
*Yiwei Chen, Soumyadeep Pal, Yimeng Zhang, Qing Qu, Sijia Liu*

**主要类别:** cs.LG

**AI概要:** 本文探讨了大型语言模型（LLMs）在进行机器遗忘（MU）后留下的持久指纹，这些指纹可以通过输出响应和内部表示检测到，并且即使是对无关输入的响应也能被简单的监督分类器可靠地识别。研究发现表明，这些痕迹嵌入到了中间激活中，并形成了低维、可学习的流形，使得通过特定提示可以以超过90%的准确率检测出遗忘痕迹。


<details>
  <summary>更多</summary>
  
**动机:** 本文动机在于揭示机器遗忘过程之后存在的一个新漏洞：即能够检测到遗忘操作的痕迹。这种能力对保护数据隐私、执行版权以及减轻LLM中的社会技术危害构成了新的挑战。

**方法:** 通过实验分析，使用简单监督分类器基于文本输出来判断模型是否经历了遗忘处理；进一步研究表明，这些痕迹存在于中间激活层，并非线性地传递到最终层，在激活空间内形成低维可学习流形。

**结果:** 实验证明，与遗忘相关的提示可以让所有规模的模型以超过90%的准确性检测到遗忘痕迹；即使对于不相关的输入，大型LLM仍保持高度可检测性，这展示了遗忘痕迹检测的广泛适用性。

**结论:** 研究结果揭示了机器遗忘过程留下了可测量的签名，这意味着当给定输入查询时识别出已遗忘信息存在新的逆向工程风险。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlearning+Isn%27t+Invisible%3A+Detecting+Unlearning+Traces+in+LLMs+from+Model+Outputs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14003，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14003&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning (MU) for large language models (LLMs), commonly referred
to as LLM unlearning, seeks to remove specific undesirable data or knowledge
from a trained model, while maintaining its performance on standard tasks.
While unlearning plays a vital role in protecting data privacy, enforcing
copyright, and mitigating sociotechnical harms in LLMs, we identify a new
vulnerability post-unlearning: unlearning trace detection. We discover that
unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces
in both model behavior and internal representations. These traces can be
identified from output responses, even when prompted with forget-irrelevant
inputs. Specifically, a simple supervised classifier can reliably determine
whether a model has undergone unlearning based solely on its textual outputs.
Further analysis shows that these traces are embedded in intermediate
activations and propagate nonlinearly to the final layer, forming
low-dimensional, learnable manifolds in activation space. Through extensive
experiments, we show that forget-relevant prompts enable over 90% accuracy in
detecting unlearning traces across all model sizes. Even with forget-irrelevant
inputs, large LLMs maintain high detectability, demonstrating the broad
applicability of unlearning trace detection. These findings reveal that
unlearning leaves measurable signatures, introducing a new risk of
reverse-engineering forgotten information when a model is identified as
unlearned given an input query. Codes are available at [this
URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [37] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/abs/2506.14036)
*Tatthapong Srikitrungruang, Sina Aghaee Dabaghan Fard, Matthew Lemon, Jaesung Lee, Yuxiao Zhou*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种新的逆弹性物理信息神经网络（IE-PINN），能够从噪声位移数据中稳健地重建非均匀弹性参数分布。通过结合多种方法创新，该模型在严重噪声条件下仍能提供准确的绝对尺度弹性估计，并且在临床成像诊断和机械特性表征方面具有很大的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的逆估计技术在处理噪声测量时存在不稳定性、对噪声敏感以及难以恢复杨氏模量绝对值的问题。因此，需要一种新的方法来提高从噪声位移测量中估计空间异质性弹性参数（特别是杨氏模量和泊松比）的准确性。

**方法:** 研究者开发了一个名为IE-PINN的新框架，它整合了三种不同的神经网络架构，分别用于建模位移场、应变场和弹性分布。此外，还引入了两阶段估计策略：第一阶段恢复杨氏模量和泊松比的相对空间分布，第二阶段利用施加的载荷边界条件校准杨氏模量的绝对尺度。其他方法学上的创新包括位置编码、正弦激活函数和顺序预训练协议。

**结果:** 广泛的数值实验表明，即使在严重的噪声条件下，IE-PINN也能够克服现有方法的关键限制，提供精确的绝对尺度弹性估计。

**结论:** IE-PINN为解决逆弹性问题中的挑战提供了有效的方法，并在面对大量噪声的情况下显著提高了弹性参数估计的稳定性和准确性。这项进展对于临床成像诊断和材料机械特性分析领域有着重要的应用前景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Physics-Informed+Neural+Network+Approach+for+Estimating+Heterogeneous+Elastic+Properties+from+Noisy+Displacement+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14036，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14036&send_immediately=true&force_search=false)

**原文摘要:** Accurately estimating spatially heterogeneous elasticity parameters,
particularly Young's modulus and Poisson's ratio, from noisy displacement
measurements remains significantly challenging in inverse elasticity problems.
Existing inverse estimation techniques are often limited by instability,
pronounced sensitivity to measurement noise, and difficulty in recovering
absolute-scale Young's modulus. This work presents a novel Inverse Elasticity
Physics-Informed Neural Network (IE-PINN) specifically designed to robustly
reconstruct heterogeneous distributions of elasticity parameters from noisy
displacement data based on linear elasticity physics. IE-PINN integrates three
distinct neural network architectures dedicated to separately modeling
displacement fields, strain fields, and elasticity distributions, thereby
significantly enhancing stability and accuracy against measurement noise.
Additionally, a two-phase estimation strategy is introduced: the first phase
recovers relative spatial distributions of Young's modulus and Poisson's ratio,
and the second phase calibrates the absolute scale of Young's modulus using
imposed loading boundary conditions. Additional methodological innovations,
including positional encoding, sine activation functions, and a sequential
pretraining protocol, further enhance the model's performance and robustness.
Extensive numerical experiments demonstrate that IE-PINN effectively overcomes
critical limitations encountered by existing methods, delivering accurate
absolute-scale elasticity estimations even under severe noise conditions. This
advancement holds substantial potential for clinical imaging diagnostics and
mechanical characterization, where measurements typically encounter substantial
noise.

</details>


### [38] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/abs/2506.14038)
*Nabil Omi, Siddhartha Sen, Ali Farhadi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的负载均衡损失，该损失保持了令牌间的关系结构，并鼓励在训练期间对相似输入做出一致的专家选择。实验结果表明，与流行的负载平衡损失相比，应用这种损失可以实现更快的收敛速度和更低的冗余度。


<details>
  <summary>更多</summary>
  
**动机:** 现有的负载均衡机制倾向于让每个令牌大致均匀地分布专家，这可能导致路由行为不一致，使模型将其容量用于学习冗余知识。为了解决这个问题，作者引入了一种新的负载均衡损失来保持令牌间的关系结构，从而提高模型性能。

**方法:** 提出了一种新颖的负载均衡损失方法，这种方法鼓励对于相似输入在训练过程中选择相同的专家，以此来保持令牌级别的关系结构一致性。

**结果:** 实验结果显示，将新提出的损失应用于路由器后，相较于一种广为人知的负载均衡损失，新方法能够实现36%更快的收敛速度以及减少模型中的冗余信息。

**结论:** 通过引入一种新的负载均衡损失，可以有效提升稀疏混合专家模型的训练效率和性能，避免了模型学习冗余知识的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Load+Balancing+Mixture+of+Experts+with+Similarity+Preserving+Routers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14038，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14038&send_immediately=true&force_search=false)

**原文摘要:** Sparse Mixture of Experts (MoE) models offer a scalable and efficient
architecture for training large neural networks by activating only a subset of
parameters ("experts") for each input. A learned router computes a distribution
over these experts, and assigns input tokens to a small subset. However,
without auxiliary balancing mechanisms, routers often converge to using only a
few experts, severely limiting model capacity and degrading performance. Most
current load balancing mechanisms encourage a distribution over experts that
resembles a roughly uniform distribution of experts per token. During training,
this can result in inconsistent routing behavior, resulting in the model
spending its capacity to learn redundant knowledge. We address this by
introducing a novel load balancing loss that preserves token-wise relational
structure, encouraging consistent expert choices for similar inputs during
training. Our experimental results show that applying our loss to the router
results in 36% faster convergence and lower redundancy compared to a popular
load balancing loss.

</details>


### [39] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/abs/2506.14054)
*Joshua Fan, Haodi Xu, Feng Tao, Md Nasim, Marc Grimson, Yiqi Luo, Carla P. Gomes*

**主要类别:** cs.LG

**AI概要:** 提出了一种科学可解释推理网络（ScIReN），它结合了可解释的神经推理和基于过程的推理，以提高预测准确性的同时提供科学解释性。


<details>
  <summary>更多</summary>
  
**动机:** 神经网络虽然可以从数据中学习模式，但其黑箱性质无法遵循已知的科学定律或揭示新的科学见解。而基于过程的模型虽然可以量化解释生物或物理原理，但其自由参数众多，且通常在跨尺度预测时与观察结果拟合不佳。

**方法:** 提出了科学可解释推理网络（ScIReN），这是一种完全透明的框架，结合了解释性的神经推理和基于过程的推理。该网络使用一个可解释的编码器来预测具有科学意义的潜在参数，这些参数随后通过一个基于过程的解码器来预测标记输出变量。ScIReN还引入了一个新颖的硬Sigmoid约束层，将潜在参数限制在由科学先验知识定义的有意义范围内。

**结果:** ScIReN在模拟土壤有机碳流动以及建模植物生态系统呼吸两个任务上，相比黑盒网络提高了预测准确性，并提供了实质性的科学解释性——能够推断出隐藏的科学机制及其与输入特征间的关系。

**结论:** ScIReN成功地结合了基于过程的知识和机器学习方法，为科学研究带来了更高的准确性和更好的解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scientifically-Interpretable+Reasoning+Network+%28ScIReN%29%3A+Uncovering+the+Black-Box+of+Nature，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14054&send_immediately=true&force_search=false)

**原文摘要:** Neural networks are a powerful tool for learning patterns from data. However,
they do not respect known scientific laws, nor can they reveal novel scientific
insights due to their black-box nature. In contrast, scientific reasoning
distills biological or physical principles from observations and controlled
experiments, and quantitatively interprets them with process-based models made
of mathematical equations. Yet, process-based models rely on numerous free
parameters that must be set in an ad-hoc manner, and thus often fit
observations poorly in cross-scale predictions. While prior work has embedded
process-based models in conventional neural networks, discovering interpretable
relationships between parameters in process-based models and input features is
still a grand challenge for scientific discovery. We thus propose
Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent
framework that combines interpretable neural and process-based reasoning. An
interpretable encoder predicts scientifically-meaningful latent parameters,
which are then passed through a differentiable process-based decoder to predict
labeled output variables. ScIReN also uses a novel hard-sigmoid constraint
layer to restrict latent parameters to meaningful ranges defined by scientific
prior knowledge, further enhancing its interpretability. While the embedded
process-based model enforces established scientific knowledge, the encoder
reveals new scientific mechanisms and relationships hidden in conventional
black-box models. We apply ScIReN on two tasks: simulating the flow of organic
carbon through soils, and modeling ecosystem respiration from plants. In both
tasks, ScIReN outperforms black-box networks in predictive accuracy while
providing substantial scientific interpretability -- it can infer latent
scientific mechanisms and their relationships with input features.

</details>


### [40] [A Regret Perspective on Online Selective Generation](https://arxiv.org/abs/2506.14067)
*Minjae Lee, Yoonjae Jung, Sangdon Park*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种在线学习算法，用于在部分反馈下进行选择性生成，以解决大型语言模型产生虚假响应的问题。通过将选择性生成问题转化为多臂赌博机问题，并利用一个新颖的转换引理来利用已知的赌博机算法及其理论性质，从而控制幻觉的发生。此外，还提出了一种解锁未知反馈的方法，以克服部分反馈导致的收敛速度慢的问题。实验结果表明该算法能够有效控制期望的错误发现率（FDR），同时保持合理的非拒绝回答比例。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言生成模型越来越多地与人类互动，它们产生的虚假响应引起了人们的担忧。为了解决这一幻觉效应，当模型对其答案不确定时，选择性地避免作答提供了一种有效的控制方法。然而，由于选择性生成器在非随机环境中交互，并且只从用户那里获得关于选择性生成的部分反馈（例如，对选定答案点赞或点踩），在这种实际设置下的学习方法至关重要但目前尚缺。

**方法:** 作者提出了一种在线学习算法，用于处理只有部分反馈情况下的选择性生成。他们将选择性生成问题归约到多臂赌博机问题上，并给出一个从赌博机问题反向转换到选择性生成的新颖引理，以便利用任何已知的赌博机算法和理论属性。特别是，这种方法将赌博机的遗憾保证关联到了选择性生成中错误发现率（FDR）的保证，以控制幻觉现象。另外，针对部分反馈固有的缓慢收敛速度问题，作者利用了选择性生成中的独特结构来进行反馈解锁，即从观察到的反馈中解锁未知的反馈。

**结果:** 所提出的在线选择性生成算法在不同数据环境设定下进行了理论分析和实证评估，结果表明该算法能够在控制期望的FDR的同时，相比基线维持合理的选答效率，即非拒绝回答的比例。

**结论:** 这项研究开发了一种新的在线学习框架，旨在改善大型语言模型在面对不确定性时的选择性生成策略，通过引入部分反馈机制和独特的反馈解锁技术，成功提高了模型对于虚假信息生成的控制能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Regret+Perspective+on+Online+Selective+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14067，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14067&send_immediately=true&force_search=false)

**原文摘要:** Large language generative models increasingly interact with humans, while
their falsified responses raise concerns. To address this hallucination effect,
selectively abstaining from answering, called selective generation, provides an
effective way for generators to control the hallucination when it is unsure of
their answers. However, as selective generators are interacting under
non-stochastic environments and having partial feedback from users on selective
generation (e.g., thumbs up or down on the selected answer), learning methods
for selective generation under such practical setups are crucial but currently
missing. To address these limitations, we propose an online learning algorithm
for selective generation under partial feedback. In particular, as learning
under partial feedback is well-studied by multi-armed bandit problems, we
reduce selective generation to bandits and provide a novel conversion lemma
from bandits back to selective generation to leverage any known bandit
algorithms and theoretical properties. This mainly connects regret guarantees
of bandits to false discovery rate (FDR) guarantees of selective generation for
controlling hallucination. However, naively exploiting known bandit algorithms
and their regret bounds suffers from slow convergence speed in practice due the
nature of partial feedback. To overcome this, we exploit a unique structure of
arms in selective generation for feedback unlocking, i.e., unlocking unknown
feedback from observed feedback. We theoretically and empirically evaluate the
efficacy of the proposed online selective generation algorithm under partial
feedback over diverse data environment setups, resulting in controlling a
desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio
of non-abstaining answers, compared to baselines.

</details>


### [41] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/abs/2506.14074)
*Nathaniel Pinckney, Chenhui Deng, Chia-Tung Ho, Yun-Da Tsai, Mingjie Liu, Wenfei Zhou, Brucek Khailany, Haoxing Ren*

**主要类别:** cs.LG

**AI概要:** 提出了CVDP基准，一个包含783个问题的新数据集和基础设施，旨在推动硬件设计与验证中的大语言模型和代理研究。该基准比先前工作更具挑战性，当前最先进的模型在代码生成上的通过率不超过34%。


<details>
  <summary>更多</summary>
  
**动机:** 为了推进硬件设计和验证领域的大语言模型（LLM）和智能体的研究，创建了一个新的更具有挑战性的基准。

**方法:** 构建了名为CVDP的基准，它包括由经验丰富的硬件工程师编写的783个问题，涵盖RTL生成、验证、调试等13个任务类别，并提供了非代理性和代理性两种格式的问题。

**结果:** 现有的最先进模型在代码生成任务上最多只能达到34%的通过率；特别是涉及RTL重用和验证的代理性任务特别困难。

**结论:** CVDP基准揭示了现有模型能力的重大差距，强调了继续研究以实现稳健的现实世界硬件设计自动化的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comprehensive+Verilog+Design+Problems%3A+A+Next-Generation+Benchmark+Dataset+for+Evaluating+Large+Language+Models+and+Agents+on+RTL+Design+and+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14074，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14074&send_immediately=true&force_search=false)

**原文摘要:** We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new
dataset and infrastructure to advance LLM and agent research in hardware design
and verification. CVDP includes 783 problems across 13 task categories,
covering RTL generation, verification, debugging, specification alignment, and
technical Q&A authored by experienced hardware engineers. Problems are offered
in both non-agentic and agentic formats. The benchmark introduces more
realistic and challenging contexts than prior work, with state-of-the-art
models achieving no more than 34% pass@1 on code generation. Agentic
tasks$\unicode{x2013}$especially those involving RTL reuse and
verification$\unicode{x2013}$are particularly difficult. Evaluation uses
open-source tools and model scoring infrastructure, with comprehension tasks
assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in
current model capabilities, underscoring the need for continued research toward
robust, real-world hardware design automation.

</details>


### [42] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/abs/2506.14087)
*Zhongzheng Qiao, Chenghao Liu, Yiming Zhang, Ming Jin, Quang Pham, Qingsong Wen, P. N. Suganthan, Xudong Jiang, Savitha Ramasamy*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Scale+Finetuning+for+Encoder-based+Time+Series+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14087&send_immediately=true&force_search=false)

**原文摘要:** Time series foundation models (TSFMs) demonstrate impressive zero-shot
performance for time series forecasting. However, an important yet
underexplored challenge is how to effectively finetune TSFMs on specific
downstream tasks. While naive finetuning can yield performance gains, we argue
that it falls short of fully leveraging TSFMs' capabilities, often resulting in
overfitting and suboptimal performance. Given the diverse temporal patterns
across sampling scales and the inherent multi-scale forecasting capabilities of
TSFMs, we adopt a causal perspective to analyze finetuning process, through
which we highlight the critical importance of explicitly modeling multiple
scales and reveal the shortcomings of naive approaches. Focusing on
\textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale
\textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet
general framework that explicitly integrates multi-scale modeling into the
finetuning process. Experimental results on three different backbones (\moirai,
\moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only
outperform naive and typical parameter efficient finetuning methods but also
surpass state-of-the-art deep learning methods.

</details>


### [43] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/abs/2506.14095)
*Parikshit Ram, Kenneth L. Clarkson, Tim Klinger, Shashanka Ubaru, Alexander G. Gray*

**主要类别:** cs.LG

**AI概要:** 研究了输入依赖的稀疏注意力模型相较于标准注意力模型在学习速度和泛化能力上的优势，并通过理论分析解释了这种现象。


<details>
  <summary>更多</summary>
  
**动机:** 探讨不同形式的稀疏注意力机制对Transformer模型的学习能力和泛化性能的影响，而非仅关注效率问题。

**方法:** 通过实证研究一系列注意力机制，比较输入依赖和输入无关的稀疏注意力模型与标准注意力模型之间的差异；开发理论特征来解释观察到的现象。

**结果:** 发现输入依赖的稀疏注意力模型比标准注意力模型收敛更快且泛化更好，而输入无关的稀疏注意力模型则没有这些优点；建立了关于softmax稳定性与损失函数Lipschitz属性之间联系的理论，并展示了稀疏性如何影响softmax稳定性和注意力机制的收敛及泛化保证。

**结论:** 输入依赖的稀疏注意力能够加速学习并改善泛化效果，而输入无关的稀疏注意力则无此益处；提出了理论依据来支持实验结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Transformers+Learn+Faster+with+Semantic+Focus，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14095，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14095&send_immediately=true&force_search=false)

**原文摘要:** Various forms of sparse attention have been explored to mitigate the
quadratic computational and memory cost of the attention mechanism in
transformers. We study sparse transformers not through a lens of efficiency but
rather in terms of learnability and generalization. Empirically studying a
range of attention mechanisms, we find that input-dependent sparse attention
models appear to converge faster and generalize better than standard attention
models, while input-agnostic sparse attention models show no such benefits -- a
phenomenon that is robust across architectural and optimization hyperparameter
choices. This can be interpreted as demonstrating that concentrating a model's
"semantic focus" with respect to the tokens currently being considered (in the
form of input-dependent sparse attention) accelerates learning. We develop a
theoretical characterization of the conditions that explain this behavior. We
establish a connection between the stability of the standard softmax and the
loss function's Lipschitz properties, then show how sparsity affects the
stability of the softmax and the subsequent convergence and generalization
guarantees resulting from the attention mechanism. This allows us to
theoretically establish that input-agnostic sparse attention does not provide
any benefits. We also characterize conditions when semantic focus
(input-dependent sparse attention) can provide improved guarantees, and we
validate that these conditions are in fact met in our empirical evaluations.

</details>


### [44] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/abs/2506.14098)
*Ziyuan Tang, Jie Chen*

**主要类别:** cs.LG

**AI概要:** 本文探讨了如何构建图基础模型，通过采用Transformer架构并使用多种图数据集进行预训练。提出了一种将节点表示为多个随机游走的方法，以使Transformer能够从序列中提取节点、边和图的表示。此外，还开发了一种新的上下文预测损失，并展示了模型预训练及其在下游任务中的适应性。


<details>
  <summary>更多</summary>
  
**动机:** 受到自然语言领域基础模型（如GPT）成功的启发，作者们探索了能否为图数据构建类似的基础模型。目标是创建一个能够处理不同大小和来自不同领域的图的通用模型。

**方法:** 研究者提出了一种方法，即利用多样化的图数据集并通过调整Transformer架构来预训练图基础模型。他们创新地将每个节点表示成多条随机游走路径，以便让Transformer可以从这些序列中学习到节点特征，进而形成边和整个图的表示。同时，为了提高这种表示的质量，研究人员设计了一种新颖的上下文预测损失函数，并对该方法区分邻域及不同图的能力进行了理论分析。

**结果:** 研究表明，所提出的基于随机游走的节点表示方法以及上下文预测损失函数有效提升了模型对于不同类型图数据的理解能力。实验结果表明该模型在多个下游任务上具有良好的迁移学习性能。

**结论:** 本文介绍了一种新的图基础模型构建方法，它能够处理各种类型的图数据，并且在多个下游任务中表现出色，证明了其作为图结构数据处理与推理平台的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+a+Graph+Foundation+Model%3A+Pre-Training+Transformers+With+Random+Walks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14098，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14098&send_immediately=true&force_search=false)

**原文摘要:** A foundation model like GPT elicits many emergent abilities, owing to the
pre-training with broad inclusion of data and the use of the powerful
Transformer architecture. While foundation models in natural languages are
prevalent, can we build similar models for graphs? This paper describes an
approach toward a graph foundation model that is pre-trained with diverse graph
datasets by adapting the Transformer backbone. A central challenge toward this
end is how a sequence model encodes graphs of varying sizes and from different
domains. We propose representing a node as multiple random walks, such that the
Transformer can extract node representations from sequences, which in turn form
edge and graph representations. We develop a novel context prediction loss for
these random walks and theoretically analyze their expressive power in
distinguishing neighborhoods and graphs. We also demonstrate the pre-training
of our model and its adaptation to downstream tasks, showcasing its potential
as a foundation for processing and reasoning with graph-structured data.

</details>


### [45] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/abs/2506.14114)
*Khushnood Abbas, Ruizhe Hou, Zhou Wengang, Dong Shi, Niu Ling, Satyaki Nan, Alireza Abbasi*

**主要类别:** cs.LG

**AI概要:** 本研究全面评估了七种知名的图神经网络（GNNs）架构与30种单一和混合损失函数在不同任务下的相互作用，揭示了混合损失函数、特定模型-损失组合以及GIN架构在归纳学习中的优势。


<details>
  <summary>更多</summary>
  
**动机:** 为了优化图神经网络（GNNs）在非欧几里得数据上的表现，需要选择合适的模型架构和训练目标（即损失函数）。虽然这些部分已经分别被研究过，但还没有大规模的评估来探讨不同的GNN模型和多种损失函数如何共同工作以完成不同的任务。

**方法:** 研究人员对七种流行的GNN架构进行了详尽的研究，并使用了30种单个加上混合损失函数。该研究既包括归纳式也包括转导式的设置，并且跨越了三个不同的真实世界数据集，使用21个综合评估指标来评估性能。

**结果:** 研究发现表明，在归纳情况下：1) 混合损失函数通常比单一损失函数产生更优和更稳健的表现；2) GIN架构始终表现出最高水平的平均性能，特别是在使用交叉熵损失时；3) 尽管某些组合的整体平均排名较低，但像GAT这样的模型，特别是与某些混合损失结合时，展现出了惊人的专门优势，在个人指标中最大化了最多的顶级结果；4) 相反，MPNN架构通常落后于它所测试的情景。

**结论:** 研究得出结论，混合损失函数对于多目标优化是有益的，并且GIN架构在归纳学习中表现最佳。此外，特定模型-损失组合可能展现出针对特定任务需求的微妙优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Loss+Functions+for+Graph+Neural+Networks%3A+Towards+Pretraining+and+Generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14114，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14114&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) became useful for learning on non-Euclidean
data. However, their best performance depends on choosing the right model
architecture and the training objective, also called the loss function.
Researchers have studied these parts separately, but a large-scale evaluation
has not looked at how GNN models and many loss functions work together across
different tasks. To fix this, we ran a thorough study - it included seven
well-known GNN architectures. We also used a large group of 30 single plus
mixed loss functions. The study looked at both inductive and transductive
settings. Our evaluation spanned three distinct real-world datasets, assessing
performance in both inductive and transductive settings using 21 comprehensive
evaluation metrics. From these extensive results (detailed in supplementary
information 1 \& 2), we meticulously analyzed the top ten model-loss
combinations for each metric based on their average rank. Our findings reveal
that, especially for the inductive case: 1) Hybrid loss functions generally
yield superior and more robust performance compared to single loss functions,
indicating the benefit of multi-objective optimization. 2) The GIN architecture
always showed the highest-level average performance, especially with
Cross-Entropy loss. 3) Although some combinations had overall lower average
ranks, models such as GAT, particularly with certain hybrid losses,
demonstrated incredible specialized strengths, maximizing the most top-1
results among the individual metrics, emphasizing subtle strengths for
particular task demands. 4) On the other hand, the MPNN architecture typically
lagged behind the scenarios it was tested against.

</details>


### [46] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/abs/2506.14122)
*Tianming Zhang, Renbo Zhang, Zhengyi Yang, Yunjun Gao, Bin Cao, Jing Fan*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于对比学习的图神经网络（CLGNN）用于准确且高效地预测时间介数中心性（TBC），通过构建实例图保持路径有效性和时序顺序，并引入了稳定性聚类指导的对比模块以缓解类别不平衡问题。实验表明，CLGNN在多样化的基准测试中表现出色，相比现有方法，在速度和准确性上均有显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的计算时间介数中心性的方法要么成本高昂，要么无法处理极度不平衡的数据分布，导致学习模型过度拟合于零中心性的节点，不能准确预测TBC值或识别真正重要的节点。同时，现存的图神经网络方法未能妥善解决这种不平衡或忽略了时间依赖性。

**方法:** 提出了一个可扩展且归纳式的基于对比学习的图神经网络（CLGNN）。该方法首先构建了一个实例图来维护路径的有效性和时间顺序；接着使用双聚合方式编码结构与时间特征；此外，还设计了一个基于稳定性的聚类引导对比模块（KContrastNet）来区分高、中、低中心性的节点，以及一个回归模块(ValueNet)来估计TBC值。

**结果:** CLGNN在多种基准测试中显示出了卓越的效果与效率。它相较于最先进的精确TBC计算方法最高可达663.7倍的速度提升。相比于领先的静态GNN基线，MAE降低了最多31.4倍，Spearman相关系数提高了16.7倍；对于最先进的时态GNNs，MAE降低了最多5.7倍，而Spearman相关系数提高了3.9倍。

**结论:** 提出的CLGNN方法能够有效地解决时间介数中心性预测中的不平衡问题，并且在速度和精度方面都优于现有的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLGNN%3A+A+Contrastive+Learning-based+GNN+Model+for+Betweenness+Centrality+Prediction+on+Temporal+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14122，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14122&send_immediately=true&force_search=false)

**原文摘要:** Temporal Betweenness Centrality (TBC) measures how often a node appears on
optimal temporal paths, reflecting its importance in temporal networks.
However, exact computation is highly expensive, and real-world TBC
distributions are extremely imbalanced. The severe imbalance leads
learning-based models to overfit to zero-centrality nodes, resulting in
inaccurate TBC predictions and failure to identify truly central nodes.
Existing graph neural network (GNN) methods either fail to handle such
imbalance or ignore temporal dependencies altogether. To address these issues,
we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for
accurate and efficient TBC prediction. CLGNN builds an instance graph to
preserve path validity and temporal order, then encodes structural and temporal
features using dual aggregation, i.e., mean and edge-to-node multi-head
attention mechanisms, enhanced by temporal path count and time encodings. A
stability-based clustering-guided contrastive module (KContrastNet) is
introduced to separate high-, median-, and low-centrality nodes in
representation space, mitigating class imbalance, while a regression module
(ValueNet) estimates TBC values. CLGNN also supports multiple optimal path
definitions to accommodate diverse temporal semantics. Extensive experiments
demonstrate the effectiveness and efficiency of CLGNN across diverse
benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to
state-of-the-art exact TBC computation methods. It outperforms leading static
GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher
Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to
5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [47] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/abs/2506.14126)
*Stefan Horoi, Guy Wolf, Eugene Belilovsky, Gintare Karolina Dziugaite*

**主要类别:** cs.LG

**AI概要:** 研究挑战了在深度学习模型流水线中，专家微调阶段的改进会自然传递到下游阶段的假设，并发现长时间针对特定任务优化的微调会导致模型合并性能下降。通过采用积极的早期停止策略可以显著提高模型再利用性能。


<details>
  <summary>更多</summary>
  
**动机:** 本文旨在探讨专家微调对模型再利用的影响，并质疑现有观点，即认为在预训练-微调-再利用这一流程中，某一阶段的改进必然带来后续阶段的增益。

**方法:** 研究者们通过实验分析了长时间微调专家模型（包括完全微调和LoRA适应）对之后模型合并及更广泛用途系统构建时表现的影响。他们还追踪了导致性能下降的原因，即在后期微调过程中记忆了一些困难样本，这些样本在合并过程中被遗忘。此外，研究测试了一种基于任务的激进早停策略来改善问题。

**结果:** 研究表明，过度优化单个专家模型的性能会导致其在合并至多任务系统时的整体性能下降。当使用LoRA适配器并将它们升级为混合专家层时，也会观察到下游结果变差。而采用一种依赖于任务的积极早停方法，则能明显提升模型再利用的效果。

**结论:** 长时间专注于个别任务进行微调可能损害模型后续在多任务场景下的再利用能力。通过实施恰当的早停策略，可以在不影响初始微调效果的同时增强模型的通用性和迁移性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Less+is+More%3A+Undertraining+Experts+Improves+Model+Upcycling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14126，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14126&send_immediately=true&force_search=false)

**原文摘要:** Modern deep learning is increasingly characterized by the use of open-weight
foundation models that can be fine-tuned on specialized datasets. This has led
to a proliferation of expert models and adapters, often shared via platforms
like HuggingFace and AdapterHub. To leverage these resources, numerous model
upcycling methods have emerged, enabling the reuse of fine-tuned models in
multi-task systems. A natural pipeline has thus formed to harness the benefits
of transfer learning and amortize sunk training costs: models are pre-trained
on general data, fine-tuned on specific tasks, and then upcycled into more
general-purpose systems. A prevailing assumption is that improvements at one
stage of this pipeline propagate downstream, leading to gains at subsequent
steps. In this work, we challenge that assumption by examining how expert
fine-tuning affects model upcycling. We show that long fine-tuning of experts
that optimizes for their individual performance leads to degraded merging
performance, both for fully fine-tuned and LoRA-adapted models, and to worse
downstream results when LoRA adapters are upcycled into MoE layers. We trace
this degradation to the memorization of a small set of difficult examples that
dominate late fine-tuning steps and are subsequently forgotten during merging.
Finally, we demonstrate that a task-dependent aggressive early stopping
strategy can significantly improve upcycling performance.

</details>


### [48] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/abs/2506.14143)
*Hayden McTavish, Zachery Boner, Jon Donnelly, Margo Seltzer, Cynthia Rudin*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的决策树布尔逻辑表示方法，该方法解决了预测等价性问题，并忠实于底层的决策边界。基于这种新表示法，作者展示了决策树在测试时对特征值缺失具有鲁棒性，探讨了预测等价性对于变量重要性量化的影响，并给出了一种优化达到预测成本的算法。


<details>
  <summary>更多</summary>
  
**动机:** 尽管决策树由于其清晰的推理过程被广泛用于可解释机器学习中，但它们面临一个称为预测等价性的挑战：给定树的决策边界可以由许多不同的决策树来表示。这些模型虽然有相同的决策边界，但在处理缺失值和变量重要性评估上会表现出不同行为，而大多数优化程序会随机选择其中一个模型返回。

**方法:** 研究者们提出了一种不会出现预测等价性的决策树布尔逻辑表示方法。他们将这种新表示应用于多个下游机器学习任务中。

**结果:** 通过使用新的表示方法，研究人员发现决策树对测试阶段的特征值缺失具有出乎意料的鲁棒性；讨论了预测等价性如何影响变量重要性的量化；并提供了一个旨在优化做出预测所需成本的算法。

**结论:** 所提出的决策树布尔逻辑表示不仅解决了预测等价性的问题，还为理解和改进决策树模型提供了新的途径，包括更好地处理缺失数据、更准确地度量变量的重要性以及减少做出预测的成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Predictive+Equivalence+in+Decision+Trees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14143&send_immediately=true&force_search=false)

**原文摘要:** Decision trees are widely used for interpretable machine learning due to
their clearly structured reasoning process. However, this structure belies a
challenge we refer to as predictive equivalence: a given tree's decision
boundary can be represented by many different decision trees. The presence of
models with identical decision boundaries but different evaluation processes
makes model selection challenging. The models will have different variable
importance and behave differently in the presence of missing values, but most
optimization procedures will arbitrarily choose one such model to return. We
present a boolean logical representation of decision trees that does not
exhibit predictive equivalence and is faithful to the underlying decision
boundary. We apply our representation to several downstream machine learning
tasks. Using our representation, we show that decision trees are surprisingly
robust to test-time missingness of feature values; we address predictive
equivalence's impact on quantifying variable importance; and we present an
algorithm to optimize the cost of reaching predictions.

</details>


### [49] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/abs/2506.14162)
*Amirhossein Rajabpour, Kiarash Aghakasiri, Sandra Zilles, Levi H. S. Lelis*

**主要类别:** cs.LG

**AI概要:** 论文指出，通过简单的调整神经策略的训练流程，如采用更简单的神经架构、使用与程序化策略相同的稀疏观察类型以及采用允许更安全策略的奖励函数等方法，神经策略可以在OOD问题上达到与程序化策略相当的泛化效果。


<details>
  <summary>更多</summary>
  
**动机:** 作者认为常用的基准测试低估了程序化表示的泛化能力，并且希望通过分析展示神经网络策略在适当修改后也能够很好地处理分布外（OOD）问题。

**方法:** 通过对四篇相关文献中的实验进行分析，对神经策略的训练管道做出简单改变，比如使用与程序化策略相同的稀疏观测类型、简化神经架构和使用支持更安全策略的奖励函数等。

**结果:** 研究发现，在进行了上述调整之后，神经策略在处理OOD问题时可以像程序化策略一样有效地泛化。

**结论:** 结论是，通过适当的调整，神经策略能够与程序化策略一样好地泛化到OOD问题上，并建议创建新的基准问题来强调对OOD泛化重要的概念。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Common+Benchmarks+Undervalue+the+Generalization+Power+of+Programmatic+Policies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14162，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14162&send_immediately=true&force_search=false)

**原文摘要:** Algorithms for learning programmatic representations for sequential
decision-making problems are often evaluated on out-of-distribution (OOD)
problems, with the common conclusion that programmatic policies generalize
better than neural policies on OOD problems. In this position paper, we argue
that commonly used benchmarks undervalue the generalization capabilities of
programmatic representations. We analyze the experiments of four papers from
the literature and show that neural policies, which were shown not to
generalize, can generalize as effectively as programmatic policies on OOD
problems. This is achieved with simple changes in the neural policies training
pipeline. Namely, we show that simpler neural architectures with the same type
of sparse observation used with programmatic policies can help attain OOD
generalization. Another modification we have shown to be effective is the use
of reward functions that allow for safer policies (e.g., agents that drive
slowly can generalize better). Also, we argue for creating benchmark problems
highlighting concepts needed for OOD generalization that may challenge neural
policies but align with programmatic representations, such as tasks requiring
algorithmic constructs like stacks.

</details>


### [50] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/abs/2506.14164)
*Hanzhong Cao*

**主要类别:** cs.LG

**AI概要:** 本文研究了在部分可观测的、合作竞争战斗环境LAG中多智能体强化学习的表现，比较了HAPPO和HASAC两种算法，并分析了它们在不同战斗模式下的训练稳定性、奖励进展以及智能体间的协调能力。


<details>
  <summary>更多</summary>
  
**动机:** 探索在部分可观察的合作-竞争战斗环境中（LAG），不同多智能体强化学习算法的性能表现及其适应性。

**方法:** 设计了包括动作、层次控制及奖励机制在内的LAG环境，并评估了两种代表性算法：HAPPO（基于PPO的一种策略内层次变体）与HASAC（基于软演员-评论家的策略外方法）。通过分析这些算法在不同战斗模式如无武器模式和导弹射击模式下的训练稳定性、奖励增长情况以及代理之间的协作能力来进行比较。

**结果:** 实验结果显示，在不需要武器的较简单协作任务中，HASAC表现出色；而在涉及导弹战斗这样更加动态且表达性强的情境里，HAPPO展现出了更强的适应性。

**结论:** 研究揭示了策略内与策略外方法在多智能体设置中的权衡关系，为选择合适的MARL算法提供了见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Light+Aircraft+Game+%3A+Basic+Implementation+and+training+results+analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14164，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14164&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates multi-agent reinforcement learning (MARL) in a
partially observable, cooperative-competitive combat environment known as LAG.
We describe the environment's setup, including agent actions, hierarchical
controls, and reward design across different combat modes such as No Weapon and
ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy
hierarchical variant of PPO, and HASAC, an off-policy method based on soft
actor-critic. We analyze their training stability, reward progression, and
inter-agent coordination capabilities. Experimental results show that HASAC
performs well in simpler coordination tasks without weapons, while HAPPO
demonstrates stronger adaptability in more dynamic and expressive scenarios
involving missile combat. These findings provide insights into the trade-offs
between on-policy and off-policy methods in multi-agent settings.

</details>


### [51] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)
*Prithvi Raj*

**主要类别:** cs.LG

**AI概要:** 本文通过将Kolmogorov-Arnold表示定理重新解释为概率空间之间的Markov Kernel，提出了一种新的生成模型。该模型结合了Kolmogorov-Arnold网络生成器与基于能量的先验，并通过最大似然法训练。此外，还引入了基于混合分布和Langevin Monte Carlo方法的扩展以提高灵活性和减少先验-后验不匹配。


<details>
  <summary>更多</summary>
  
**动机:** 作者希望利用Kolmogorov-Arnold表示定理来改进生成建模的方法，使得模型易于理解、设计简便且高效。同时，他们旨在通过引入可逆采样来加快推理速度，并允许在训练前加入先验知识，从而提高学习效率和样本质量。

**方法:** 研究者们创建了一个生成模型，它使用Kolmogorov-Arnold网络作为生成器，并与独立的能量基先验相结合。这个模型是通过最大似然估计进行训练的。为了增强模型的灵活性并缓解先验与后验之间的不匹配，他们还提出了基于混合分布和Langevin Monte Carlo方法的扩展。

**结果:** 新提出的生成模型不仅提高了学习效率和样本质量，而且提供了可恢复性和可视化性。所提出的扩展方法也能够平衡灵活性和训练效率。

**结论:** 该研究成功地将经典的表示定理与现代的概率建模联系起来，同时保持了训练稳定性、推理速度以及生成物的质量和多样性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Structured+and+Informed+Probabilistic+Modeling+with+the+Thermodynamic+Kolmogorov-Arnold+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14167&send_immediately=true&force_search=false)

**原文摘要:** We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling
by reinterpreting its inner functions as a Markov Kernel between probability
spaces via inverse transform sampling. We present a generative model that is
interpretable, easy to design, and efficient. Our approach couples a
Kolmogorov-Arnold Network generator with independent energy-based priors,
trained via Maximum Likelihood. Inverse sampling enables fast inference, while
prior knowledge can be incorporated before training to better align priors with
posteriors, thereby improving learning efficiency and sample quality. The
learned prior is also recoverable and visualizable post-training, offering an
empirical Bayes perspective. To address inflexibility and mitigate
prior-posterior mismatch, we introduce scalable extensions based on mixture
distributions and Langevin Monte Carlo methods, admitting a trade-off between
flexibility and training efficiency. Our contributions connect classical
representation theorems with modern probabilistic modeling, while balancing
training stability, inference speed, and the quality and diversity of
generations.

</details>


### [52] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/abs/2506.14194)
*Sudeepta Mondal, Zhuolin Jiang, Ganesh Sundaramoorthi*

**主要类别:** cs.LG

**AI概要:** 提出了一种用于神经网络的分布外（OOD）检测特征构建理论，通过引入基于KL散度和信息瓶颈的新信息论损失函数来优化并获得OOD特征。该理论不仅能解释现有的OOD特征，还预测了一个新的塑形函数，在OOD基准测试中优于现有方法，并为构造各种具有清晰可解释性的新特征提供了通用框架。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于开发一种新的方法来提高神经网络对于未知或异常数据点（即分布外数据）的检测能力，从而增强模型的鲁棒性和可靠性。

**方法:** 作者提出的方法是通过设计一个由两部分组成的信息论损失函数：一部分基于KL散度以区分分布内（ID）和分布外（OOD）的数据特征分布；另一部分则采用信息瓶颈原则，旨在保留有助于OOD识别的关键信息同时压缩特征。

**结果:** 研究表明，所提出的理论能够有效地生成用于OOD检测的新特性，并且根据对OOD分布所做的假设，可以恢复现有OOD特性的属性。此外，该理论还成功预测了一种新型塑形函数，在多个OOD评估基准上表现优于传统方法。

**结论:** 结论指出，本研究所提的理论不仅能够提供对现有OOD特征的理解，而且还能指导开发更有效的OOD检测方法，为未来的研究开辟了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Variational+Information+Theoretic+Approach+to+Out-of-Distribution+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14194，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14194&send_immediately=true&force_search=false)

**原文摘要:** We present a theory for the construction of out-of-distribution (OOD)
detection features for neural networks. We introduce random features for OOD
through a novel information-theoretic loss functional consisting of two terms,
the first based on the KL divergence separates resulting in-distribution (ID)
and OOD feature distributions and the second term is the Information
Bottleneck, which favors compressed features that retain the OOD information.
We formulate a variational procedure to optimize the loss and obtain OOD
features. Based on assumptions on OOD distributions, one can recover properties
of existing OOD features, i.e., shaping functions. Furthermore, we show that
our theory can predict a new shaping function that out-performs existing ones
on OOD benchmarks. Our theory provides a general framework for constructing a
variety of new features with clear explainability.

</details>


### [53] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/abs/2506.14217)
*Dipesh Tharu Mahato, Rohan Poudel, Pramod Dhungana*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为TriGuard的统一安全评估框架，结合了形式化鲁棒性验证、归因熵量化显著性集中度以及一种新的归因漂移评分来测量解释稳定性。该框架揭示了模型准确性和可解释性之间的关键不匹配，并且通过三个数据集和五种架构的广泛实验展示了其在揭示神经推理中微妙脆弱性方面的能力。此外，研究表明，熵正则化训练可以减少解释漂移而不牺牲性能。


<details>
  <summary>更多</summary>
  
**动机:** 深度神经网络通常能够达到很高的准确性，但在对抗性和分布偏移下确保其可靠性仍然是一个亟待解决的问题。

**方法:** 提出了TriGuard，一个统一的安全评估框架，它结合了（1）形式化的鲁棒性验证，（2）用于量化显著性集中程度的归因熵，以及（3）一种新颖的归因漂移分数来衡量解释稳定性。

**结果:** TriGuard揭示了模型准确性和可解释性之间的重要不匹配：经过验证的模型仍然可能表现出不稳定的推理过程，并且基于归因的信号提供了超出对抗性准确性的互补安全见解。通过对三个数据集和五种架构的大量实验表明，TriGuard如何揭示出神经推理中的细微脆弱性。进一步证明了熵正则化训练能够在不牺牲性能的情况下减少解释漂移。

**结论:** TriGuard推动了鲁棒性和可解释模型评估的发展前沿。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TriGuard%3A+Testing+Model+Safety+with+Attribution+Entropy%2C+Verification%2C+and+Drift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14217&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks often achieve high accuracy, but ensuring their
reliability under adversarial and distributional shifts remains a pressing
challenge. We propose TriGuard, a unified safety evaluation framework that
combines (1) formal robustness verification, (2) attribution entropy to
quantify saliency concentration, and (3) a novel Attribution Drift Score
measuring explanation stability. TriGuard reveals critical mismatches between
model accuracy and interpretability: verified models can still exhibit unstable
reasoning, and attribution-based signals provide complementary safety insights
beyond adversarial accuracy. Extensive experiments across three datasets and
five architectures show how TriGuard uncovers subtle fragilities in neural
reasoning. We further demonstrate that entropy-regularized training reduces
explanation drift without sacrificing performance. TriGuard advances the
frontier in robust, interpretable model evaluation.

</details>


### [54] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/abs/2506.14220)
*Kangkang Lu, Yanhua Yu, Zhiyong Huang, Tat-Seng Chua*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，利用大型语言模型（LLMs）来估计图的同质性，并据此自适应地设计多项式谱滤波器，从而提高谱图神经网络（SGNNs）的表现力和适应性。实验表明，该框架在同质性和异质性设置下均优于现有基线，且计算和经济开销极小。


<details>
  <summary>更多</summary>
  
**动机:** 谱图神经网络（SGNNs）能够近似任意滤波器，但当标签稀缺时，可能学习到次优滤波器，导致性能下降。同时，大型语言模型（LLMs）的成功激发了人们探索其在GNN领域潜力的兴趣。因此，研究者提出了一个问题：LLMs是否可以帮助克服SGNNs的局限性并增强其性能？

**方法:** 提出了一种新方法，通过使用LLMs来估计给定图的同质性，并基于此估计来自适应地指导多项式谱滤波器的设计。此外，还引入了一个轻量级管道，其中LLM生成同质性感知先验，这些先验被注入滤波器系数中以更好地与基础图拓扑对齐。

**结果:** 广泛的基准数据集实验显示，提出的LLM驱动SGNN框架在同质性和异质性设置下始终优于现有的基线，并且具有最小的计算和货币开销。

**结论:** 研究表明，通过结合LLMs来估计图同质性并以此指导滤波器设计，可以显著提升SGNNs在不同图结构上的表现力和适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Large+Language+Models+Improve+Spectral+Graph+Neural+Networks%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14220&send_immediately=true&force_search=false)

**原文摘要:** Spectral Graph Neural Networks (SGNNs) have attracted significant attention
due to their ability to approximate arbitrary filters. They typically rely on
supervision from downstream tasks to adaptively learn appropriate filters.
However, under label-scarce conditions, SGNNs may learn suboptimal filters,
leading to degraded performance. Meanwhile, the remarkable success of Large
Language Models (LLMs) has inspired growing interest in exploring their
potential within the GNN domain. This naturally raises an important question:
\textit{Can LLMs help overcome the limitations of SGNNs and enhance their
performance?} In this paper, we propose a novel approach that leverages LLMs to
estimate the homophily of a given graph. The estimated homophily is then used
to adaptively guide the design of polynomial spectral filters, thereby
improving the expressiveness and adaptability of SGNNs across diverse graph
structures. Specifically, we introduce a lightweight pipeline in which the LLM
generates homophily-aware priors, which are injected into the filter
coefficients to better align with the underlying graph topology. Extensive
experiments on benchmark datasets demonstrate that our LLM-driven SGNN
framework consistently outperforms existing baselines under both homophilic and
heterophilic settings, with minimal computational and monetary overhead.

</details>


### [55] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/abs/2506.14251)
*Xiyu Zhao, Qimei Cui, Weicai Li, Wei Ni, Ekram Hossain, Quan Z. Sheng, Xiaofeng Tao, Ping Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为DP-Ditto的个性化联邦学习方法，该方法在差分隐私保护下扩展了Ditto，并分析了其隐私保证、模型收敛性与性能分布公平性之间的权衡。实验表明，DP-Ditto在公平性和准确性方面优于当前最先进的PFL模型的DP扰动版本。


<details>
  <summary>更多</summary>
  
**动机:** 现有的个性化联邦学习方法如Ditto，在进行个性化学习时依赖于联邦学习的结果，但客户端对于隐私的关注以及由此导致的本地模型扰动可能会影响个性化学习的收敛性和性能公平性。因此需要一种既能提供隐私保护又能保证模型性能的方法。

**方法:** 提出了DP-Ditto，这是在差分隐私（DP）保护下的Ditto的一种非平凡扩展。分析了DP-Ditto在隐私保证、模型收敛性和性能分配公平性方面的折衷。研究了在给定隐私预算的情况下个性化模型的收敛上限，并得出了最优全局聚合次数。还分析了个性化模型的表现公平性，揭示了同时优化DP-Ditto以实现收敛和公平性的可行性。

**结果:** 实验验证了分析结果，并展示了DP-Ditto在公平性和准确性上分别比最先进的PFL模型的DP扰动版本高出32.71%和9.66%。

**结论:** DP-Ditto作为一种改进的个性化联邦学习方法，在确保用户隐私的同时，还能提高模型的公平性和准确性，为个性化联邦学习提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Convergence-Privacy-Fairness+Trade-Off+in+Personalized+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14251&send_immediately=true&force_search=false)

**原文摘要:** Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a
balance between personalization and generalization by conducting federated
learning (FL) to guide personalized learning (PL). While FL is unaffected by
personalized model training, in Ditto, PL depends on the outcome of the FL.
However, the clients' concern about their privacy and consequent perturbation
of their local models can affect the convergence and (performance) fairness of
PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension
of Ditto under the protection of differential privacy (DP), and analyzes the
trade-off among its privacy guarantee, model convergence, and performance
distribution fairness. We also analyze the convergence upper bound of the
personalized models under DP-Ditto and derive the optimal number of global
aggregations given a privacy budget. Further, we analyze the performance
fairness of the personalized models, and reveal the feasibility of optimizing
DP-Ditto jointly for convergence and fairness. Experiments validate our
analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of
the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by
over 32.71% in fairness and 9.66% in accuracy.

</details>


### [56] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
*Rohan Gupta, Erik Jenner*

**主要类别:** cs.LG

**AI概要:** 研究了大语言模型是否可以通过强化学习(RL-Obfuscation)来逃避潜空间监控器，并发现基于token级别的监控器很容易被规避，而更全面的监控器如最大池化或基于注意力机制的探针则更具鲁棒性。此外，对抗策略可以泛化到未见过但同类型的监控器上，并且模型还学会了重新定义tokens的内部含义。


<details>
  <summary>更多</summary>
  
**动机:** 探讨大型语言模型(LLMs)是否能够通过学习来逃避潜空间监控器，这些监控器旨在通过利用模型内部表示而非仅依赖黑盒输出来检测不期望的行为。

**方法:** 引入了一种名为RL-Obfuscation的方法，通过对7B至14B参数规模的语言模型进行强化学习微调，使其在保持生成内容连贯的同时试图绕过潜空间监控器。

**结果:** 结果显示，基于token级别的潜空间监控器极易受到此类攻击的影响；而更为综合性的监控方法，例如采用最大池化或基于注意力机制的探测，则表现出较强的抵御能力。另外，为逃避单一静态监控器训练出的对抗策略也能够泛化至未曾遭遇过的同类监控器中。

**结论:** 研究表明，通过强化学习可以使大型语言模型学会逃避特定类型的潜空间监控器，并且这种逃避行为可以在一定程度上推广到其他相似类型的监控器。此外，模型甚至学会了改变某些词汇在内部的意义以达到逃避监控的目的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RL-Obfuscation%3A+Can+Language+Models+Learn+to+Evade+Latent-Space+Monitors%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14261&send_immediately=true&force_search=false)

**原文摘要:** Latent-space monitors aim to detect undesirable behaviours in large language
models by leveraging internal model representations rather than relying solely
on black-box outputs. These methods have shown promise in identifying
behaviours such as deception and unsafe completions, but a critical open
question remains: can LLMs learn to evade such monitors? To study this, we
introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement
learning to bypass latent-space monitors while maintaining coherent
generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters
and evaluate evasion success against a suite of monitors. We find that
token-level latent-space monitors are highly vulnerable to this attack. More
holistic monitors, such as max-pooling or attention-based probes, remain
robust. Moreover, we show that adversarial policies trained to evade a single
static monitor generalise to unseen monitors of the same type. Finally, we
study how the policy learned by RL bypasses these monitors and find that the
model can also learn to repurpose tokens to mean something different
internally.

</details>


### [57] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/abs/2506.14263)
*Qingyu Song, Wei Lin, Juncheng Wang, Hong Xu*

**主要类别:** cs.LG

**AI概要:** 本文解决了学习优化（L2O）在分布外（OOD）场景下的性能和鲁棒性缺乏理论证明的问题，提出了一个简洁的仅基于梯度特征构建和新颖的基于梯度的历史建模方法的L2O模型，并通过数值模拟展示了该模型在InD和OOD场景下相对于现有技术的优势。


<details>
  <summary>更多</summary>
  
**动机:** 尽管学习优化（L2O）技术在诸如无线通信、计算机网络和电子设计等许多现实世界场景中取得了巨大成功，但现有的L2O工作缺乏对其在分布外（OOD）场景下性能和鲁棒性的理论证明。

**方法:** 研究者们首先证明了L2O模型对于所有In-Distribution (InD)实例具有均匀收敛率的鲁棒性的充分条件；假设L2O模型对InD情形达到了鲁棒性，他们提出了一种将OOD问题与InD问题对齐的方法论，并且表明L2O模型在OOD场景中的收敛率会根据输入特征的一个方程而恶化；此外，他们还提出了一种只有梯度特征构造的L2O模型以及一种新的基于梯度的历史建模方法。

**结果:** 数值仿真表明，所提出的模型在InD和OOD场景中都优于最先进基线，并且实现了高达10倍的收敛速度提升。

**结论:** 这项工作填补了L2O在OOD场景下性能和鲁棒性理论证明的空白，为L2O提供了一个更加健壮的设计方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Robust+Learning+to+Optimize+with+Theoretical+Guarantees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14263，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14263&send_immediately=true&force_search=false)

**原文摘要:** Learning to optimize (L2O) is an emerging technique to solve mathematical
optimization problems with learning-based methods. Although with great success
in many real-world scenarios such as wireless communications, computer
networks, and electronic design, existing L2O works lack theoretical
demonstration of their performance and robustness in out-of-distribution (OOD)
scenarios. We address this gap by providing comprehensive proofs. First, we
prove a sufficient condition for a robust L2O model with homogeneous
convergence rates over all In-Distribution (InD) instances. We assume an L2O
model achieves robustness for an InD scenario. Based on our proposed
methodology of aligning OOD problems to InD problems, we also demonstrate that
the L2O model's convergence rate in OOD scenarios will deteriorate by an
equation of the L2O model's input features. Moreover, we propose an L2O model
with a concise gradient-only feature construction and a novel gradient-based
history modeling method. Numerical simulation demonstrates that our proposed
model outperforms the state-of-the-art baseline in both InD and OOD scenarios
and achieves up to 10 $\times$ convergence speedup. The code of our method can
be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [58] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/abs/2506.14306)
*Ata Yalcin, Asli Umay Ozturk, Yigit Sever, Viktoria Pauw, Stephan Hachinger, Ismail Hakki Toroslu, Pinar Karagoz*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在标签和敏感属性都存在数据不平衡的情况下，如何通过多标准方法找到最合适的采样和分布以提高公平性和分类准确性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的去偏方法在面对数据收集不平衡时表现不佳，特别是在标签和敏感属性双重不平衡的数据集上。

**方法:** 首先对双重不平衡数据集进行探索性分析，随后提出一个多标准解决方案来寻找标签和敏感属性的最佳采样与分布方案。

**结果:** 提出的多标准方法能够有效处理双重不平衡问题，并且在公平性和分类准确性方面找到了一个平衡点。

**结论:** 研究强调了针对双重不平衡数据集的特殊考虑对于确保机器学习决策过程中的公平性至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fair+for+a+few%3A+Improving+Fairness+in+Doubly+Imbalanced+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14306&send_immediately=true&force_search=false)

**原文摘要:** Fairness has been identified as an important aspect of Machine Learning and
Artificial Intelligence solutions for decision making. Recent literature offers
a variety of approaches for debiasing, however many of them fall short when the
data collection is imbalanced. In this paper, we focus on a particular case,
fairness in doubly imbalanced datasets, such that the data collection is
imbalanced both for the label and the groups in the sensitive attribute.
Firstly, we present an exploratory analysis to illustrate limitations in
debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution
is proposed for finding the most suitable sampling and distribution for label
and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [59] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/abs/2506.14375)
*Muhammad Hamza Yousuf, Jason Li, Sahar Vahdati, Raphael Theilen, Jakob Wittenstein, Jens Lehmann*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种改进的离线强化学习方法，用于优化重症监护室中患者的机械通气设置。通过减少动作空间并直接处理混合动作空间的问题，以及引入基于临床的奖励函数，该方法能够提高患者安全性和个体化肺部支持。


<details>
  <summary>更多</summary>
  
**动机:** 机械通气是重症监护病房中维持生命的重要手段，但其参数设置因人而异且容易出错。尽管离线强化学习显示出潜力，但现有方法难以处理机械通气控制中的连续与离散混合动作问题。

**方法:** 研究者们对先前的动作空间简化工作进行了改进，以应对离散动作空间带来的挑战；同时调整了最新的离线RL算法（IQL和EDAC），使其能直接应用于混合动作空间，并避免了离散化所带来的问题。此外，他们还设计了一个基于无呼吸机天数及生理目标的临床导向奖励函数。

**结果:** 实验结果表明，所提出的AI辅助机械通气优化方法可能提高患者安全性，并实现个性化的肺部支持治疗，这标志着向智能化、数据驱动的关键护理解决方案迈出了重要一步。

**结论:** 本研究展示了利用先进的离线强化学习技术来优化机械通气设定的可能性，为提升ICU内患者护理质量提供了新的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IntelliLung%3A+Advancing+Safe+Mechanical+Ventilation+using+Offline+RL+with+Hybrid+Actions+and+Clinically+Aligned+Rewards，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14375，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14375&send_immediately=true&force_search=false)

**原文摘要:** Invasive mechanical ventilation (MV) is a life-sustaining therapy for
critically ill patients in the intensive care unit (ICU). However, optimizing
its settings remains a complex and error-prone process due to patient-specific
variability. While Offline Reinforcement Learning (RL) shows promise for MV
control, current stateof-the-art (SOTA) methods struggle with the hybrid
(continuous and discrete) nature of MV actions. Discretizing the action space
limits available actions due to exponential growth in combinations and
introduces distribution shifts that can compromise safety. In this paper, we
propose optimizations that build upon prior work in action space reduction to
address the challenges of discrete action spaces. We also adapt SOTA offline RL
algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby
avoiding the pitfalls of discretization. Additionally, we introduce a
clinically grounded reward function based on ventilator-free days and
physiological targets, which provides a more meaningful optimization objective
compared to traditional sparse mortality-based rewards. Our findings
demonstrate that AI-assisted MV optimization may enhance patient safety and
enable individualized lung support, representing a significant advancement
toward intelligent, data-driven critical care solutions.

</details>


### [60] [ResNets Are Deeper Than You Think](https://arxiv.org/abs/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel, Michael Wand*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的解释，即残差网络不仅仅重新参数化了前馈网络，而是占据了一个不同的函数空间。通过控制实验比较发现，类似ResNets的可变深度架构在泛化性能上优于固定深度网络，这表明残差连接带来的优势超出了优化范畴，并指向了与自然数据结构相一致的更深层次的归纳偏置。


<details>
  <summary>更多</summary>
  
**动机:** 尽管残差连接自从引入以来已经在现代神经网络架构中变得无处不在，但它们广泛被采用的原因通常归功于其显著提高的训练能力：残差网络比前馈网络训练得更快、更稳定，并且达到更高的准确率。尽管提出了许多技术来缩小残差网络和前馈网络之间的性能差距，但这种差距仍然存在。因此，作者想要探索是否还有其他因素导致了残差网络的成功。

**方法:** 作者设计了一个受控的后训练比较实验，以将泛化性能从训练能力中分离出来。

**结果:** 研究发现，类似于ResNets的可变深度架构在即使优化不太可能产生差异的情况下，也始终优于固定深度网络。

**结论:** 结果表明，残差连接提供的性能优势超越了单纯的优化问题，而是指向了一个更深层的归纳偏置，这个偏置与自然数据的结构对齐。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ResNets+Are+Deeper+Than+You+Think，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14386，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14386&send_immediately=true&force_search=false)

**原文摘要:** Residual connections remain ubiquitous in modern neural network architectures
nearly a decade after their introduction. Their widespread adoption is often
credited to their dramatically improved trainability: residual networks train
faster, more stably, and achieve higher accuracy than their feedforward
counterparts. While numerous techniques, ranging from improved initialization
to advanced learning rate schedules, have been proposed to close the
performance gap between residual and feedforward networks, this gap has
persisted. In this work, we propose an alternative explanation: residual
networks do not merely reparameterize feedforward networks, but instead inhabit
a different function space. We design a controlled post-training comparison to
isolate generalization performance from trainability; we find that
variable-depth architectures, similar to ResNets, consistently outperform
fixed-depth networks, even when optimization is unlikely to make a difference.
These results suggest that residual connections confer performance advantages
beyond optimization, pointing instead to a deeper inductive bias aligned with
the structure of natural data.

</details>


### [61] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister, Erik Bochinski, Volker Eiselein, Elvira Fleig*

**主要类别:** cs.LG

**AI概要:** 本文通过结合自解释原型变分模型和基于自动编码器的异常值检测方法，提出了一种新的约束损失来定义紧凑但不过度简化的同分布区域，并通过广泛的评估证明了该方法在异常值检测上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 理解深度机器学习模型的决策过程并信任其可靠性对于将这些方法应用于安全相关的领域至关重要。为了提高对深度模型的信任，研究者们希望开发出既能自我解释又能有效检测异常值（OOD）的模型。

**方法:** 研究人员扩展了自解释原型变分模型，引入了基于变分自动编码器的方法，它能够学习有意义的潜在空间，用于基于距离的分类、异常值检测的可能性估计以及重建。使用高斯混合分布定义同分布区域，并引入了一种新颖的限制损失，促使在潜在空间中形成紧凑的同分布区域而不将其塌缩为单个点。

**结果:** 通过在常见的异常值检测基准数据集上进行广泛评估，以及在一个大规模的真实世界铁路应用数据集上测试，结果表明该方法优于先前的方法。

**结论:** 所提出的方法不仅提高了异常值检测的性能，而且通过自动编码器的重构能力增强了模型的可解释性，从而有助于实际应用中的决策过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enclosing+Prototypical+Variational+Autoencoder+for+Explainable+Out-of-Distribution+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14390&send_immediately=true&force_search=false)

**原文摘要:** Understanding the decision-making and trusting the reliability of Deep
Machine Learning Models is crucial for adopting such methods to safety-relevant
applications. We extend self-explainable Prototypical Variational models with
autoencoder-based out-of-distribution (OOD) detection: A Variational
Autoencoder is applied to learn a meaningful latent space which can be used for
distance-based classification, likelihood estimation for OOD detection, and
reconstruction. The In-Distribution (ID) region is defined by a Gaussian
mixture distribution with learned prototypes representing the center of each
mode. Furthermore, a novel restriction loss is introduced that promotes a
compact ID region in the latent space without collapsing it into single points.
The reconstructive capabilities of the Autoencoder ensure the explainability of
the prototypes and the ID region of the classifier, further aiding the
discrimination of OOD samples. Extensive evaluations on common OOD detection
benchmarks as well as a large-scale dataset from a real-world railway
application demonstrate the usefulness of the approach, outperforming previous
methods.

</details>


### [62] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)
*Yaqiao Zhu, Hongkai Wen, Geyong Min, Man Luo*

**主要类别:** cs.LG

**AI概要:** 提出了HiLight，一种具有全局对抗性指导的分层强化学习框架，用于大规模交通信号控制。通过实验验证了该方法在大规模场景下的显著优势以及在不同规模标准基准测试中的竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习方法在扩展到大型网络时面临挑战，同时难以保持全局协调。集中式RL存在可扩展性问题，而分散式方法通常缺乏统一的目标，导致网络级效率有限。

**方法:** HiLight是一个具有全局对抗性引导的层次化强化学习框架，包括一个使用Transformer-LSTM架构生成子目标并划分交通网络为子区域的高层元策略（Meta-Policy），以及一个具备全局意识来控制单独交叉口的低层次策略（Sub-Policy）。引入了一种对抗训练机制，以提高全局规划与局部执行之间的一致性。

**结果:** HiLight在合成和真实世界基准上进行了评估，并构建了一个包含多种交通状况的大规模曼哈顿网络进行测试。实验结果显示HiLight在大规模场景中表现出显著优势，并且在不同规模的标准基准测试中保持竞争力。

**结论:** HiLight框架通过结合全局对抗性指导和层次化策略，在解决大规模交通信号控制的问题上提供了一种有效的方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HiLight%3A+A+Hierarchical+Reinforcement+Learning+Framework+with+Global+Adversarial+Guidance+for+Large-Scale+Traffic+Signal+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14391，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14391&send_immediately=true&force_search=false)

**原文摘要:** Efficient traffic signal control (TSC) is essential for mitigating urban
congestion, yet existing reinforcement learning (RL) methods face challenges in
scaling to large networks while maintaining global coordination. Centralized RL
suffers from scalability issues, while decentralized approaches often lack
unified objectives, resulting in limited network-level efficiency. In this
paper, we propose HiLight, a hierarchical reinforcement learning framework with
global adversarial guidance for large-scale TSC. HiLight consists of a
high-level Meta-Policy, which partitions the traffic network into subregions
and generates sub-goals using a Transformer-LSTM architecture, and a low-level
Sub-Policy, which controls individual intersections with global awareness. To
improve the alignment between global planning and local execution, we introduce
an adversarial training mechanism, where the Meta-Policy generates challenging
yet informative sub-goals, and the Sub-Policy learns to surpass these targets,
leading to more effective coordination. We evaluate HiLight across both
synthetic and real-world benchmarks, and additionally construct a large-scale
Manhattan network with diverse traffic conditions, including peak transitions,
adverse weather, and holiday surges. Experimental results show that HiLight
exhibits significant advantages in large-scale scenarios and remains
competitive across standard benchmarks of varying sizes.

</details>


### [63] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/abs/2506.14400)
*Roland Roller, Michael Hahn, Ajay Madhavan Ravichandran, Bilgin Osmanodja, Florian Oetke, Zeineb Sassi, Aljoscha Burchardt, Klaus Netter, Klemens Budde, Anne Herrmann, Tobias Strapatsas, Peter Dabrock, Sebastian Möller*

**主要类别:** cs.LG

**AI概要:** 本文分析了机器学习模型在医疗预测任务中的性能差异，尤其是在不同患者群体之间的表现。强调了在将这些模型整合到临床工作流程之前进行子群级别评估的重要性，以确保公平性和透明性。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决现实世界中医疗数据集存在的问题，比如数据噪声、不完整和不平衡等，这些问题会导致不同患者群体之间模型性能的差异，并引发公平性问题，特别是当它们加剧了边缘化群体已有的劣势时。

**方法:** 通过对几个具体的医疗预测任务进行分析，展示模型性能如何随患者特征而变化。

**结果:** 尽管机器学习模型可能显示出良好的整体性能，但子群体级别的性能差异仍然存在。通过子群体层面的表现分析，可以明确识别出这些差异。

**结论:** 基于上述发现，作者认为在将机器学习模型集成到临床实践中前对其进行子群体水平的评估至关重要。这不仅有助于考虑到临床实践中的性能差异，而且还能为开发更有效且负责任的模型提供见解。此外，这项工作促进了关于医疗ML模型子群体敏感发展与部署以及公平性和透明度之间相互联系的实际讨论。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One+Size+Fits+None%3A+Rethinking+Fairness+in+Medical+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14400，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14400&send_immediately=true&force_search=false)

**原文摘要:** Machine learning (ML) models are increasingly used to support clinical
decision-making. However, real-world medical datasets are often noisy,
incomplete, and imbalanced, leading to performance disparities across patient
subgroups. These differences raise fairness concerns, particularly when they
reinforce existing disadvantages for marginalized groups. In this work, we
analyze several medical prediction tasks and demonstrate how model performance
varies with patient characteristics. While ML models may demonstrate good
overall performance, we argue that subgroup-level evaluation is essential
before integrating them into clinical workflows. By conducting a performance
analysis at the subgroup level, differences can be clearly identified-allowing,
on the one hand, for performance disparities to be considered in clinical
practice, and on the other hand, for these insights to inform the responsible
development of more effective models. Thereby, our work contributes to a
practical discussion around the subgroup-sensitive development and deployment
of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [64] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/abs/2506.14411)
*John Wikman, Alexandre Proutiere, David Broman*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架，称为交互层，以及一种基于该框架的算法ACDA，以处理强化学习中不可观测和时变延迟的问题，并在多种运动基准环境中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 在标准强化学习设置中，通常假设智能体可以即时观察系统状态、无延迟地选择动作并立即执行。然而，在现实世界的动态环境中，由于智能体与系统之间的交互存在延迟，这种假设往往不成立。现有方法保守地通过假设一个已知的固定最大延迟来处理这种不确定性，即使实际延迟通常远低于这个值。

**方法:** 研究者们引入了交互层这一通用框架，使智能体能够自适应且无缝地处理不可观测和时变的延迟问题。具体来说，智能体生成可能未来行动矩阵以应对不可预测的延迟和网络传输过程中丢失的动作包。基于此框架，他们开发了一个基于模型的算法——带有延迟适应的演员-评论家（ACDA），该算法能够根据延迟模式进行动态调整。

**结果:** 实验结果表明，所提出的方法在广泛的运动基准环境中显著优于现有的最先进方法。

**结论:** 新提出的交互层框架及ACDA算法为解决强化学习中的时变延迟问题提供了有效途径，并在多种环境下验证了其优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Reinforcement+Learning+for+Unobservable+Random+Delays，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14411，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14411&send_immediately=true&force_search=false)

**原文摘要:** In standard Reinforcement Learning (RL) settings, the interaction between the
agent and the environment is typically modeled as a Markov Decision Process
(MDP), which assumes that the agent observes the system state instantaneously,
selects an action without delay, and executes it immediately. In real-world
dynamic environments, such as cyber-physical systems, this assumption often
breaks down due to delays in the interaction between the agent and the system.
These delays can vary stochastically over time and are typically unobservable,
meaning they are unknown when deciding on an action. Existing methods deal with
this uncertainty conservatively by assuming a known fixed upper bound on the
delay, even if the delay is often much lower. In this work, we introduce the
interaction layer, a general framework that enables agents to adaptively and
seamlessly handle unobservable and time-varying delays. Specifically, the agent
generates a matrix of possible future actions to handle both unpredictable
delays and lost action packets sent over networks. Building on this framework,
we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA),
which dynamically adjusts to delay patterns. Our method significantly
outperforms state-of-the-art approaches across a wide range of locomotion
benchmark environments.

</details>


### [65] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/abs/2506.14420)
*Ting Xiao, Jiakun Zheng, Rushuai Yang, Kang Xu, Qiaosheng Zhang, Peng Liu, Chenjia Bai*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的无监督强化学习方法，通过最大化技能状态密度之间的差异来促进技能间的状态多样性，并使用一种新颖的条件自动编码器来估计高维空间中的状态密度，同时基于学习到的自动编码器构建内在奖励以激励技能内部探索。实验表明该方法在各种下游任务中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决基于熵的探索在大规模状态空间（如图像）中的困难以及基于赋能的方法在状态探索方面的局限性，本文旨在提出一种新的技能发现目标，以提高不同技能间的状态多样性和鼓励技能内部探索。

**方法:** 提出了一种新的技能发现目标，目的是最大化一个技能的状态密度与其它已探索技能区域之间的偏差；设计了一个具有软模块化的新型条件自动编码器用于高维度空间下的不同技能策略的状态密度估计；并基于所学得的自动编码器制定了类似于紧凑潜在空间内计数探索的内在奖励机制。

**结果:** 通过在具有挑战性的状态和基于图像的任务上进行广泛的实验验证，证明了所提出的方法能够学习到有意义的技能，并且在多种下游任务中达到了优越的表现。

**结论:** 本文介绍的新方法不仅克服了现有无监督强化学习技术的一些关键限制，还展示了如何有效促进技能间及技能内的状态多样性，从而加速下游任务的学习过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+Skill+Discovery+through+Skill+Regions+Differentiation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14420，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14420&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors
that can accelerate the learning of downstream tasks. Previous methods
typically focus on entropy-based exploration or empowerment-driven skill
learning. However, entropy-based exploration struggles in large-scale state
spaces (e.g., images), and empowerment-based methods with Mutual Information
(MI) estimations have limitations in state exploration. To address these
challenges, we propose a novel skill discovery objective that maximizes the
deviation of the state density of one skill from the explored regions of other
skills, encouraging inter-skill state diversity similar to the initial MI
objective. For state-density estimation, we construct a novel conditional
autoencoder with soft modularization for different skill policies in
high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we
formulate an intrinsic reward based on the learned autoencoder that resembles
count-based exploration in a compact latent space. Through extensive
experiments in challenging state and image-based tasks, we find our method
learns meaningful skills and achieves superior performance in various
downstream tasks.

</details>


### [66] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan, Yin Zheng, Taifeng Wang, Binbin Liu, Hongteng Xu*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为MoORE的新策略，通过SVD和可学习的路由器来调整预训练模型权重矩阵的奇异值，从而在多任务适应场景中减少任务冲突和遗忘。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决大规模基础模型在多任务场景下遇到的任务冲突和遗忘问题。

**方法:** 对预训练模型的权重矩阵使用SVD，并引入一个可学习的路由器基于任务和样本调整其奇异值，形成正交秩一专家混合（MoORE），并对右奇异向量施加可学习的正交变换以提高模型容量。

**结果:** 实验表明，与现有的多任务适应方法相比，MoORE在抵抗冲突和防止原有任务遗忘方面表现出色。

**结论:** MoORE是一种有效的多任务适应方法，能够保证专家之间的正交性并保持原始权重矩阵的列空间，从而避免新任务间的冲突以及原任务的遗忘。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoORE%3A+SVD-based+Model+MoE-ization+for+Conflict-+and+Oblivion-Resistant+Multi-Task+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14436，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14436&send_immediately=true&force_search=false)

**原文摘要:** Adapting large-scale foundation models in multi-task scenarios often suffers
from task conflict and oblivion. To mitigate such issues, we propose a novel
''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant
multi-task adaptation method. Given a weight matrix of a pre-trained model, our
method applies SVD to it and introduces a learnable router to adjust its
singular values based on tasks and samples. Accordingly, the weight matrix
becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert
corresponds to the outer product of a left singular vector and the
corresponding right one. We can improve the model capacity by imposing a
learnable orthogonal transform on the right singular vectors. Unlike low-rank
adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'
orthogonality and maintains the column space of the original weight matrix.
These two properties make the adapted model resistant to the conflicts among
the new tasks and the oblivion of its original tasks, respectively. Experiments
on various datasets demonstrate that MoORE outperforms existing multi-task
adaptation methods consistently, showing its superiority in terms of conflict-
and oblivion-resistance. The code of the experiments is available at
https://github.com/DaShenZi721/MoORE.

</details>


### [67] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/abs/2506.14438)
*Pol Arévalo, Alexis Molina, Álvaro Ciudad*

**主要类别:** cs.LG

**AI概要:** 本文通过简化双曲神经网络中的关键操作，提高了计算速度和预测准确性，使得双曲神经网络成为更广泛的应用选择。


<details>
  <summary>更多</summary>
  
**动机:** 尽管双曲神经网络在建模复杂结构化数据方面具有优势，特别是在存在层次或树状关系的数据中，但它们经常面临性能挑战，尤其是在计算效率和需要高精度的任务上。

**方法:** 研究者们通过对双曲神经网络内部的关键操作进行简化来解决这些限制。

**结果:** 研究结果表明，简化后的双曲操作可以在计算速度和预测准确性方面带来显著的提高。

**结论:** 简化双曲神经网络中的操作能够增强其计算效率和性能，使其成为更加可行的选择以应用于更广泛的领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是sHGCN%3A+Simplified+hyperbolic+graph+convolutional+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14438，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14438&send_immediately=true&force_search=false)

**原文摘要:** Hyperbolic geometry has emerged as a powerful tool for modeling complex,
structured data, particularly where hierarchical or tree-like relationships are
present. By enabling embeddings with lower distortion, hyperbolic neural
networks offer promising alternatives to Euclidean-based models for capturing
intricate data structures. Despite these advantages, they often face
performance challenges, particularly in computational efficiency and tasks
requiring high precision. In this work, we address these limitations by
simplifying key operations within hyperbolic neural networks, achieving notable
improvements in both runtime and performance. Our findings demonstrate that
streamlined hyperbolic operations can lead to substantial gains in
computational speed and predictive accuracy, making hyperbolic neural networks
a more viable choice for a broader range of applications.

</details>


### [68] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/abs/2506.14439)
*Rikiya Takehi, Masahiro Asami, Kosuke Kawakami, Yuta Saito*

**主要类别:** cs.LG

**AI概要:** 本文研究了在部分观察到的奖励下，如何通过利用密集观察到的次级奖励作为补充数据来学习最大化预期目标奖励的策略，并提出了一种新的方法HyPeR，该方法能够有效地使用次级奖励以及部分观察到的目标奖励来进行有效的离线策略学习。


<details>
  <summary>更多</summary>
  
**动机:** 离线策略学习（OPL）在上下文强盗问题中旨在仅使用历史交互数据来学习一个最大化目标奖励的决策策略。然而，当奖励只是部分观察时，OPL的效果会显著下降。为了应对这样的部分奖励，可以使用更密集观察到的次级奖励，但这些次级奖励可能与目标奖励不一致。因此，这项工作探讨了一个新的且普遍的问题，即如何利用密集观察到的次级奖励来学习最大化预期目标奖励的策略。

**方法:** 提出了一个新的方法叫做HyPeR (Hybrid Policy Optimization for Partially-Observed Reward)，它能够在挑战性场景下，除了利用部分观察到的目标奖励外，还有效利用次级奖励来实现有效的离线策略学习。此外，文章还讨论了同时优化预期目标奖励和一定程度上优化预期次级奖励的情况。

**结果:** 统计分析和实证评估表明，无论是合成数据还是真实世界的数据，在各种情况下HyPeR都优于现有方法。

**结论:** 通过结合次级奖励和部分观察到的目标奖励，HyPeR为在部分观察到的奖励下进行有效的离线策略学习提供了解决方案，并且在实际应用中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+General+Framework+for+Off-Policy+Learning+with+Partially-Observed+Reward，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14439，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14439&send_immediately=true&force_search=false)

**原文摘要:** Off-policy learning (OPL) in contextual bandits aims to learn a
decision-making policy that maximizes the target rewards by using only
historical interaction data collected under previously developed policies.
Unfortunately, when rewards are only partially observed, the effectiveness of
OPL degrades severely. Well-known examples of such partial rewards include
explicit ratings in content recommendations, conversion signals on e-commerce
platforms that are partial due to delay, and the issue of censoring in medical
problems. One possible solution to deal with such partial rewards is to use
secondary rewards, such as dwelling time, clicks, and medical indicators, which
are more densely observed. However, relying solely on such secondary rewards
can also lead to poor policy learning since they may not align with the target
reward. Thus, this work studies a new and general problem of OPL where the goal
is to learn a policy that maximizes the expected target reward by leveraging
densely observed secondary rewards as supplemental data. We then propose a new
method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR),
which effectively uses the secondary rewards in addition to the
partially-observed target reward to achieve effective OPL despite the
challenging scenario. We also discuss a case where we aim to optimize not only
the expected target reward but also the expected secondary rewards to some
extent; counter-intuitively, we will show that leveraging the two objectives is
in fact advantageous also for the optimization of only the target reward. Along
with statistical analysis of our proposed methods, empirical evaluations on
both synthetic and real-world data show that HyPeR outperforms existing methods
in various scenarios.

</details>


### [69] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/abs/2506.14449)
*Lucas Kreiss, Amey Chaware, Maryam Roohian, Sarah Lemire, Oana-Maria Thoma, Birgitta Carlé, Maximilian Waldner, Sebastian Schürmann, Oliver Friedrich, Roarke Horstmeyer*

**主要类别:** cs.LG

**AI概要:** 本文开发了一种基于深度学习的卷积神经网络，用于无标记多光子显微镜图像中免疫细胞类型的分类。通过使用挤压网架构，模型实现了可靠的二元和多元分类结果，并且扰动测试表明两个自然荧光通道对分类同样重要。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高无标记多光子显微镜（MPM）图像的特异性，研究者希望利用深度学习（DL）模型来预测特定目标注释，从而数字化地增强这些无标记图像的特异性。

**方法:** 研究采用了无标记MPM图像的数据集，包含了多种不同的免疫细胞类型，并训练了一个卷积神经网络（CNN）以天然自体荧光作为输入进行细胞类型分类。

**结果:** 实验结果显示，低复杂度的挤压网架构能够实现可靠的免疫细胞分类结果，对于混合样本的二元分类达到了0.89 ROC-AUC 和 0.95 PR-AUC；对于六类分类任务，在孤立样本中F1分数为0.689，精度为0.697，召回率为0.748，MCC为0.683。

**结论:** 未来这样的预测性深度学习模型可以直接在未染色的图像中检测特定的免疫细胞，从而计算上提高无标记MPM的特异性，这将对活体端显微术具有巨大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+immune+cells+with+label-free+two-photon+autofluorescence+and+deep+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14449，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14449&send_immediately=true&force_search=false)

**原文摘要:** Label-free imaging has gained broad interest because of its potential to omit
elaborate staining procedures which is especially relevant for in vivo use.
Label-free multiphoton microscopy (MPM), for instance, exploits two-photon
excitation of natural autofluorescence (AF) from native, metabolic proteins,
making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been
widely used in other optical imaging technologies to predict specific target
annotations and thereby digitally augment the specificity of these label-free
images. However, this computational specificity has only rarely been
implemented for MPM. In this work, we used a data set of label-free MPM images
from a series of different immune cell types (5,075 individual cells for binary
classification in mixed samples and 3,424 cells for a multi-class
classification task) and trained a convolutional neural network (CNN) to
classify cell types based on this label-free AF as input. A low-complexity
squeezeNet architecture was able to achieve reliable immune cell classification
results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples;
0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class
classification in isolated samples). Perturbation tests confirmed that the
model is not confused by extracellular environment and that both input AF
channels (NADH and FAD) are about equally important to the classification. In
the future, such predictive DL models could directly detect specific immune
cells in unstained images and thus, computationally improve the specificity of
label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [70] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/abs/2506.14457)
*Freya Behrens, Lenka Zdeborová*

**主要类别:** cs.LG

**AI概要:** 本文研究了数据集蒸馏过程中，教师模型如何将记忆的信息通过软标签传递给学生模型，并分析了这种信息传递在无结构随机数据集上的效果以及温度参数对这一过程的影响。


<details>
  <summary>更多</summary>
  
**动机:** 探讨现代神经网络在数据蒸馏设置中是否及如何转移被记住的具体信息，特别是当数据缺乏内在结构时，学生模型能否从教师模型学习到关于未见过的数据的非平凡准确度。

**方法:** 通过使用有限的独立同分布随机数据集来排除泛化可能性，确保教师模型只能进行纯粹的记忆；然后观察学生模型基于这些数据集上教师提供的软标签的学习表现。

**结果:** 即使在没有直接观察的情况下，学生模型也能对保留的记忆数据达到非平凡的准确性，在某些情况下甚至能达到完美准确度。这种现象强烈依赖于logits平滑的温度，但在不同网络容量、架构和数据集组成下依然存在。

**结论:** 研究表明，即使是在不可能泛化的纯随机数据集上，学生模型也能够通过软标签从教师那里学到关于未见数据的重要信息，这表明数据蒸馏过程中不仅传递了结构化知识，还有具体的记忆内容。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dataset+distillation+for+memorized+data%3A+Soft+labels+can+leak+held-out+teacher+knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14457&send_immediately=true&force_search=false)

**原文摘要:** Dataset distillation aims to compress training data into fewer examples via a
teacher, from which a student can learn effectively. While its success is often
attributed to structure in the data, modern neural networks also memorize
specific facts, but if and how such memorized information is can transferred in
distillation settings remains less understood. In this work, we show that
students trained on soft labels from teachers can achieve non-trivial accuracy
on held-out memorized data they never directly observed. This effect persists
on structured data when the teacher has not generalized.To analyze it in
isolation, we consider finite random i.i.d. datasets where generalization is a
priori impossible and a successful teacher fit implies pure memorization.
Still, students can learn non-trivial information about the held-out data, in
some cases up to perfect accuracy. In those settings, enough soft labels are
available to recover the teacher functionally - the student matches the
teacher's predictions on all possible inputs, including the held-out memorized
data. We show that these phenomena strongly depend on the temperature with
which the logits are smoothed, but persist across varying network capacities,
architectures and dataset compositions.

</details>


### [71] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/abs/2506.14459)
*Md. Mortuza Ahmmed, Abdullah Al Noman, Mahin Montasir Afif, K. M. Tahsin Kabir, Md. Mostafizur Rahman, Mufti Mahmud*

**主要类别:** cs.LG

**AI概要:** 本研究针对职场人士的抑郁情绪预测问题，提出了一种基于堆叠集成学习方法的模型。该模型结合了多个基学习器，并通过逻辑回归进行中介，从而有效捕捉多样化的学习模式。实验结果表明，所提出的模型在训练数据上达到了99.64%的准确率，在测试数据上达到了98.75%的准确率，且精确度、召回率和F1分数均超过98%，显示了集成学习在心理健康分析中的有效性及其在早期发现与干预策略上的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 抑郁症是重要的心理健康问题，特别是在工作环境里，工作压力、经济负担以及生活失衡等因素导致人们的健康状况恶化。尽管人们对此越来越关注，但研究人员和实践者在开发准确且可泛化的心理健康障碍预测模型方面仍面临重大挑战。传统分类方法往往难以应对抑郁症的复杂性，因为它受到职业压力、睡眠模式和工作满意度等多方面相互依赖因素的影响。

**方法:** 研究中提出了一个基于堆叠（stacking）的集成学习方法来提高对专业人士抑郁情况分类的预测准确性。该模型利用了从Kaggle收集到的抑郁专业人员数据集，其中包含了影响心理健康的个人基本信息、职业信息及生活方式特征。堆叠模型整合了多种基础学习算法，并通过逻辑回归作为元分类器来进行综合决策。

**结果:** 实验结果显示，所提出的堆叠模型在训练数据上的准确率达到99.64%，在测试数据上的准确率为98.75%，同时精度、召回率以及F1分数均超过了98%。这些发现突显了集成学习技术在心理健康数据分析中的高效性及其用于早期识别和介入措施的巨大潜力。

**结论:** 这项研究表明，使用堆叠集成学习方法能够显著提高抑郁症预测的准确性。高准确率和其他性能指标证明了该方法的有效性。此外，这种方法展示了其在心理健康领域内支持早期检测和干预策略方面的强大潜能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Model-Mediated+Stacked+Ensemble+Approach+for+Depression+Prediction+Among+Professionals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14459，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14459&send_immediately=true&force_search=false)

**原文摘要:** Depression is a significant mental health concern, particularly in
professional environments where work-related stress, financial pressure, and
lifestyle imbalances contribute to deteriorating well-being. Despite increasing
awareness, researchers and practitioners face critical challenges in developing
accurate and generalizable predictive models for mental health disorders.
Traditional classification approaches often struggle with the complexity of
depression, as it is influenced by multifaceted, interdependent factors,
including occupational stress, sleep patterns, and job satisfaction. This study
addresses these challenges by proposing a stacking-based ensemble learning
approach to improve the predictive accuracy of depression classification among
professionals. The Depression Professional Dataset has been collected from
Kaggle. The dataset comprises demographic, occupational, and lifestyle
attributes that influence mental well-being. Our stacking model integrates
multiple base learners with a logistic regression-mediated model, effectively
capturing diverse learning patterns. The experimental results demonstrate that
the proposed model achieves high predictive performance, with an accuracy of
99.64% on training data and 98.75% on testing data, with precision, recall, and
F1-score all exceeding 98%. These findings highlight the effectiveness of
ensemble learning in mental health analytics and underscore its potential for
early detection and intervention strategies.

</details>


### [72] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/abs/2506.14460)
*Junbin Qiu, Zhengpeng Xie, Xiangda Yan, Yongjie Yang, Yao Shu*

**主要类别:** cs.LG

**AI概要:** 本文揭示了零阶优化（ZOO）与单步策略优化（PO）之间的基本联系，并基于此提出了一种新的ZOO算法ZoAR，该算法通过引入平均基线和查询重用来减少方差并提高收敛性。实证研究表明，ZoAR在收敛速度和最终性能方面优于其他方法。


<details>
  <summary>更多</summary>
  
**动机:** 论文动机在于阐明流行的零阶优化方法（特别是使用随机有限差分的方法）的底层机制及其与强化学习等其他优化范式的联系，因为这些关系之前尚未被充分解释。

**方法:** 研究者们建立了零阶优化与单步策略优化之间的关联，展示了常用的ZOO梯度估计器数学上等同于带有特定基线函数的REINFORCE梯度估计器。基于这一统一框架，提出了结合PO启发式方差减少技术的新ZOO算法——ZoAR。

**结果:** 理论分析证明了所提出的技术可以减少方差并促进收敛。广泛的实证研究证实了理论预测，并表明ZoAR在收敛速度和最终表现上显著优于其他方法。

**结论:** 这项工作为理解ZOO提供了新的理论视角，并且从其与PO的关系中得出了实用的算法改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zeroth-Order+Optimization+is+Secretly+Single-Step+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14460，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14460&send_immediately=true&force_search=false)

**原文摘要:** Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing
functions where explicit gradients are unavailable or expensive to compute.
However, the underlying mechanisms of popular ZOO methods, particularly those
employing randomized finite differences, and their connection to other
optimization paradigms like Reinforcement Learning (RL) are not fully
elucidated. This paper establishes a fundamental and previously unrecognized
connection: ZOO with finite differences is equivalent to a specific instance of
single-step Policy Optimization (PO). We formally unveil that the implicitly
smoothed objective function optimized by common ZOO algorithms is identical to
a single-step PO objective. Furthermore, we show that widely used ZOO gradient
estimators, are mathematically equivalent to the REINFORCE gradient estimator
with a specific baseline function, revealing the variance-reducing mechanism in
ZOO from a PO perspective.Built on this unified framework, we propose ZoAR
(Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO
algorithm incorporating PO-inspired variance reduction techniques: an averaged
baseline from recent evaluations and query reuse analogous to experience
replay. Our theoretical analysis further substantiates these techniques reduce
variance and enhance convergence. Extensive empirical studies validate our
theory and demonstrate that ZoAR significantly outperforms other methods in
terms of convergence speed and final performance. Overall, our work provides a
new theoretical lens for understanding ZOO and offers practical algorithmic
improvements derived from its connection to PO.

</details>


### [73] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/abs/2506.14472)
*Fabien Bernier, Maxime Cordy, Yves Le Traon*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种超网络架构，能够有效利用外部因素（如天气指标）来提高整体电力消耗预测模型的准确性。通过对比各种预测模型，证明了在结合外部因素时，所提出的超网络方法比现有方法表现更好，减少了预测误差，并且保持了全局模型的优点。


<details>
  <summary>更多</summary>
  
**动机:** 准确的电力消费预测对于有效的能源管理和资源配置至关重要。尽管传统的时序预测依赖于历史模式和时间依赖性，但引入诸如天气指标等外部因素已显示出在复杂现实世界应用中提高预测精度的巨大潜力。然而，这些额外特征的加入通常会降低在整个群体上训练的全球预测模型的性能，尽管它们提高了个体家庭层面模型的表现。为了解决这一挑战，研究者们发现了超网络架构可以有效地利用外部因素来增强全球电力消费预测模型的准确性。

**方法:** 研究人员收集了一个跨度两年的数据集，包括来自6000多个卢森堡家庭的消费数据以及相应的外部因素，比如天气指标、假日和主要本地事件。通过对多种预测模型进行比较，展示了当与外部因素相结合时，超网络方法优于现有的方法。

**结果:** 结果表明，使用超网络架构的方法相比其他方法，在考虑外部因素的情况下能更好地减少预测错误，并达到最佳的准确性，同时保持了全局模型的优势。

**结论:** 结论是超网络架构能够有效地利用外部因素来调整每个消费者的模型权重，从而提高全球电气消耗预测模型的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+External+Factors+in+Household-Level+Electrical+Consumption+Forecasting+using+Hypernetworks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14472，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14472&send_immediately=true&force_search=false)

**原文摘要:** Accurate electrical consumption forecasting is crucial for efficient energy
management and resource allocation. While traditional time series forecasting
relies on historical patterns and temporal dependencies, incorporating external
factors -- such as weather indicators -- has shown significant potential for
improving prediction accuracy in complex real-world applications. However, the
inclusion of these additional features often degrades the performance of global
predictive models trained on entire populations, despite improving individual
household-level models. To address this challenge, we found that a hypernetwork
architecture can effectively leverage external factors to enhance the accuracy
of global electrical consumption forecasting models, by specifically adjusting
the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising
consumption data from over 6000 luxembourgish households and corresponding
external factors such as weather indicators, holidays, and major local events.
By comparing various forecasting models, we demonstrate that a hypernetwork
approach outperforms existing methods when associated to external factors,
reducing forecasting errors and achieving the best accuracy while maintaining
the benefits of a global model.

</details>


### [74] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga, Jaskaran Singh, Arun K. Dubey*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为FAMR（遗忘对齐模型重建）的框架，它为深度图像分类器中的后处理遗忘提供了一个理论基础和计算效率高的方法。该框架将遗忘视为一个有约束的优化问题，在保证模型参数与原始值相近的同时，最小化遗忘集上的统一预测损失。实验结果表明，FAMR在CIFAR-10和ImageNet-100数据集上执行类别遗忘任务时有效，并且具有较强的性能保持能力和较低的计算开销。


<details>
  <summary>更多</summary>
  
**动机:** 随着机器学习系统越来越多地依赖受隐私法规保护的数据，从训练好的模型中选择性地移除特定信息变得至关重要。特别是在图像分类领域，需要在不完全重新训练的情况下，移除某些训练样本、语义类别或视觉风格的影响。

**方法:** 提出了Forget-Aligned Model Reconstruction (FAMR)框架，通过构建一个约束优化问题来实现后处理遗忘。此过程旨在最小化遗忘集合上的一致性预测损失，并通过ℓ2惩罚项使模型参数锚定于其初始值。此外，理论分析显示FAMR解决方案与基于影响函数的重训练近似相关联，并提供了参数和输出偏差的界限。

**结果:** 在CIFAR-10和ImageNet-100数据集上的类别遗忘任务中展示了FAMR的有效性，表现出色的性能保持以及极小的额外计算成本。该框架还能自然扩展到概念和样式擦除任务，为视觉模型中的高效后处理遗忘提供了一条可扩展且可认证的路径。

**结论:** FAMR作为一种新的后处理遗忘框架，不仅在理论上得到支持，而且在实践中也被证明是高效的。它能够有效地去除图像分类器中特定类别的信息而不需从头开始再训练整个模型，同时保持了较高的分类准确率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Train+Once%2C+Forget+Precisely%3A+Anchored+Optimization+for+Efficient+Post-Hoc+Unlearning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14515&send_immediately=true&force_search=false)

**原文摘要:** As machine learning systems increasingly rely on data subject to privacy
regulation, selectively unlearning specific information from trained models has
become essential. In image classification, this involves removing the influence
of particular training samples, semantic classes, or visual styles without full
retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a
theoretically grounded and computationally efficient framework for post-hoc
unlearning in deep image classifiers. FAMR frames forgetting as a constrained
optimization problem that minimizes a uniform-prediction loss on the forget set
while anchoring model parameters to their original values via an $\ell_2$
penalty. A theoretical analysis links FAMR's solution to
influence-function-based retraining approximations, with bounds on parameter
and output deviation. Empirical results on class forgetting tasks using
CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong
performance retention and minimal computational overhead. The framework
generalizes naturally to concept and style erasure, offering a scalable and
certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [75] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/abs/2506.14518)
*Elif Yılmaz, Christos Dimitrakakis*

**主要类别:** cs.LG

**AI概要:** 本文研究了在未知收益矩阵下，通过bandit反馈估计的双人零和博弈。提出了两种算法ETC-TPZSG和ETC-TPZSG-AE，并且导出了这两种算法实例依赖的预期遗憾上界，表明基于ETC的算法能够在对抗性游戏设置中有效执行，同时提供实例依赖分析的见解。


<details>
  <summary>更多</summary>
  
**动机:** 研究目的是展示经验尝试（Explore-Then-Commit, ETC）方法在双人零和博弈（TPZSG）环境下的适用性，特别是学习纯策略纳什均衡，并且对两种提出的算法给出实例依赖的预期遗憾上界的推导，这在零和博弈文献中鲜有关注。

**方法:** 提出并分析了两种算法：ETC-TPZSG直接将ETC应用于TPZSG场景，而ETC-TPZSG-AE则通过引入利用ε-纳什均衡属性的动作对消除策略来改进前者。

**结果:** 对于ETC-TPZSG，在T轮后达到了O(Δ + √T)的实例依赖遗憾上界；而对于ETC-TPZSG-AE，则达到了O(log(T Δ^2)/Δ)的更优上界。

**结论:** 结果表明，基于ETC的算法在对抗性游戏环境中能够有效运行，其遗憾界限与现有方法相当，同时提供了实例依赖分析的新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-Player+Zero-Sum+Games+with+Bandit+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14518，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14518&send_immediately=true&force_search=false)

**原文摘要:** We study a two-player zero-sum game (TPZSG) in which the row player aims to
maximize their payoff against an adversarial column player, under an unknown
payoff matrix estimated through bandit feedback. We propose and analyze two
algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and
ETC-TPZSG-AE, which improves upon it by incorporating an action pair
elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium
property to efficiently select the optimal action pair. Our objective is to
demonstrate the applicability of ETC in a TPZSG setting by focusing on learning
pure strategy Nash Equilibrium. A key contribution of our work is a derivation
of instance-dependent upper bounds on the expected regret for both algorithms,
has received limited attention in the literature on zero-sum games.
Particularly, after $T$ rounds, we achieve an instance-dependent regret upper
bounds of $O(\Delta + \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T
\Delta^2)}{\Delta})$ for ETC-TPZSG-AE, where $\Delta$ denotes the suboptimality
gap. Therefore, our results indicate that ETC-based algorithms perform
effectively in adversarial game settings, achieving regret bounds comparable to
existing methods while providing insights through instance-dependent analysis.

</details>


### [76] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/abs/2506.14521)
*Korbinian Pfab, Marcel Rothering*

**主要类别:** cs.LG

**AI概要:** 本文通过一个工业AI用例——自动光学检测中的误报减少，展示了当前最佳实践方法的不足，并指出了七个普遍存在的弱点及其后果。作者提倡采用需求感知度量、明确的成功标准定义和对实验数据集时间动态的彻底分析，以实现商业目标并促进更成功的应用AI研究。


<details>
  <summary>更多</summary>
  
**动机:** 探讨当前的人工智能研究方法是否足以创建成功、高效且盈利的AI应用程序。

**方法:** 通过一个名为“误报减少”的工业AI用例进行案例研究，识别了同行评审工作中常见的七个弱点，并通过实验展示了这些弱点所带来的后果。

**结果:** 证明了对于该特定用例，基于现有最佳实践的方法论将会失败。提出需要有针对业务目标的需求感知度量标准、清晰的成功标准定义以及对实验数据集中时间动态的深入分析。

**结论:** 鼓励研究人员批判性地评估其方法论，以推动更加成功的应用型人工智能研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Improved+Research+Methodologies+for+Industrial+AI%3A+A+case+study+of+false+call+reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14521，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14521&send_immediately=true&force_search=false)

**原文摘要:** Are current artificial intelligence (AI) research methodologies ready to
create successful, productive, and profitable AI applications? This work
presents a case study on an industrial AI use case called false call reduction
for automated optical inspection to demonstrate the shortcomings of current
best practices. We identify seven weaknesses prevalent in related peer-reviewed
work and experimentally show their consequences. We show that the best-practice
methodology would fail for this use case. We argue amongst others for the
necessity of requirement-aware metrics to ensure achieving business objectives,
clear definitions of success criteria, and a thorough analysis of temporal
dynamics in experimental datasets. Our work encourages researchers to
critically assess their methodologies for more successful applied AI research.

</details>


### [77] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/abs/2506.14529)
*Xiaohan Zheng, Lanning Wei, Yong Li, Quanming Yao*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为LLMNet的方法，该方法利用大型语言模型来自动生成和优化图神经网络（GNNs），并通过实验证明了其在不同数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 决策过程中经常需要从图结构数据中学习，而图神经网络（GNNs）在其中扮演着重要角色。但是配置和调优这些网络需要付出大量努力。因此，作者们提出了一个通过大型语言模型自动设计GNN的新方法。

**方法:** 研究者提出了一种称为LLMNet的系统，它开发了一系列代理来构建与图相关的知识库，并使用检索增强生成（RAG）技术支持通过知识引导的进化过程自动生成和细化GNN模型。

**结果:** 实验结果显示，LLMNet在三个图学习任务中的十二个数据集上表现出色，证明了其在设计GNN模型方面的有效性。

**结论:** LLMNet提供了一种有效的方法来自动化图神经网络的设计流程，展示了通过大型语言模型实现这一目标的可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Decision-Making+on+Networks+with+LLMs+through+Knowledge-Guided+Evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14529，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14529&send_immediately=true&force_search=false)

**原文摘要:** Effective decision-making on networks often relies on learning from
graph-structured data, where Graph Neural Networks (GNNs) play a central role,
but they take efforts to configure and tune. In this demo, we propose LLMNet,
showing how to design GNN automated through Large Language Models. Our system
develops a set of agents that construct graph-related knowlege bases and then
leverages Retrieval-Augmented Generation (RAG) to support automated
configuration and refinement of GNN models through a knowledge-guided evolution
process. These agents, equipped with specialized knowledge bases, extract
insights into tasks and graph structures by interacting with the knowledge
bases. Empirical results show LLMNet excels in twelve datasets across three
graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [78] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/abs/2506.14540)
*Gerardo A. Flores, Alyssa H. Smith, Julia A. Fukuyama, Ashia C. Wilson*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的评估框架，用于选择校准后的分类器，该框架考虑了临床环境中类别流行率的不确定性以及特定领域的成本不对称性。


<details>
  <summary>更多</summary>
  
**动机:** 当前广泛使用的评分规则，如准确率和AUC-ROC，并不能充分反映关键的临床优先事项，包括校准、对分布偏移的鲁棒性和对非对称错误成本的敏感性。

**方法:** 基于适当的评分规则理论，特别是Schervish表示法，推导出一种调整后的交叉熵（log score）变体，它在临床上相关的类别平衡范围内平均成本加权性能。

**结果:** 所提出的评估方法易于应用，对临床部署条件敏感，并旨在优先考虑那些既经过校准又能适应现实世界变化的模型。

**结论:** 这项工作提供了一个有原则但实用的评估框架，有助于选择更适合临床环境需求的机器学习决策支持系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aligning+Evaluation+with+Clinical+Priorities%3A+Calibration%2C+Label+Shift%2C+and+Error+Costs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14540，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14540&send_immediately=true&force_search=false)

**原文摘要:** Machine learning-based decision support systems are increasingly deployed in
clinical settings, where probabilistic scoring functions are used to inform and
prioritize patient management decisions. However, widely used scoring rules,
such as accuracy and AUC-ROC, fail to adequately reflect key clinical
priorities, including calibration, robustness to distributional shifts, and
sensitivity to asymmetric error costs. In this work, we propose a principled
yet practical evaluation framework for selecting calibrated thresholded
classifiers that explicitly accounts for the uncertainty in class prevalences
and domain-specific cost asymmetries often found in clinical settings. Building
on the theory of proper scoring rules, particularly the Schervish
representation, we derive an adjusted variant of cross-entropy (log score) that
averages cost-weighted performance over clinically relevant ranges of class
balance. The resulting evaluation is simple to apply, sensitive to clinical
deployment conditions, and designed to prioritize models that are both
calibrated and robust to real-world variations.

</details>


### [79] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/abs/2506.14563)
*Jesse St. Amand, Leonardo Gizzi, Martin A. Giese*

**主要类别:** cs.LG

**AI概要:** 本文介绍了高斯过程动态混合模型（GPDMM），并展示了它在人类运动数据单样本学习中的应用。GPDMM结合了多个GPDM，并在一个概率混合专家框架中利用嵌入的几何特征，能够在单一潜在空间中编码多样化的序列，从而实现每个序列类别的分类和生成。该模型特别适合于数据有限且模型可解释性至关重要的情况，如针对患者的医疗应用。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决在数据有限的情况下建模人类运动的挑战，特别是对于需要高度模型可解释性的应用场景，比如患者特异性医疗应用，提出了一个改进的模型来提高分类准确性和生成能力。

**方法:** 通过将多个高斯过程动态模型（GPDM）整合到一个概率混合专家框架中，创建了一个高斯过程动态混合模型（GPDMM）。此模型使用嵌入式几何特征，允许在单一潜在空间中对不同序列进行编码，从而能够对每种序列类型进行分类和生成。

**结果:** GPDMM在单样本学习的分类准确度和生成能力方面进行了评估，与LSTMs、VAEs和transformers等其他模型进行了对比。

**结论:** GPDMM为单样本学习提供了有效的解决方案，特别是在人类运动数据建模中，当面临数据稀缺并且需要保持模型可解释性时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Single-Example+Learning+in+a+Mixture+of+GPDMs+with+Latent+Geometries，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14563，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14563&send_immediately=true&force_search=false)

**原文摘要:** We present the Gaussian process dynamical mixture model (GPDMM) and show its
utility in single-example learning of human motion data. The Gaussian process
dynamical model (GPDM) is a form of the Gaussian process latent variable model
(GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM
combines multiple GPDMs in a probabilistic mixture-of-experts framework,
utilizing embedded geometric features to allow for diverse sequences to be
encoded in a single latent space, enabling the categorization and generation of
each sequence class. GPDMs and our mixture model are particularly advantageous
in addressing the challenges of modeling human movement in scenarios where data
is limited and model interpretability is vital, such as in patient-specific
medical applications like prosthesis control. We score the GPDMM on
classification accuracy and generative ability in single-example learning,
showcase model variations, and benchmark it against LSTMs, VAEs, and
transformers.

</details>


### [80] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
*Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于token级别奖励指导的直接偏好优化(DPO)方法，通过将序列级别的PPO分解为一系列token级别的近端策略优化问题，并推导出最优的token级别策略和相应的奖励。实验表明该方法在多个基准测试中相对于DPO有显著性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 尽管细粒度的token级别奖励模型可以显著提高大型语言模型与PPO的一致性，但要将这种token级别的奖励作为DPO的指导具有挑战性，因为DPO被定义为一个序列级别的bandit问题。

**方法:** 作者将序列级别的PPO分解成了一系列token级别的近端策略优化问题，并且构建了带有token级别奖励指导的token级别PPO问题框架。利用得到的奖励和Bradley-Terry模型，建立了用于DPO的可计算损失函数框架，并提出了基于诱导DPO奖励的实际奖励指导方案。

**结果:** 实验结果显示，所提出的方法在MT-Bench、AlpacaEval 2以及Arena-Hard等基准上相较于DPO取得了显著的性能提升，胜率分别提高了7.5分、6.2分和4.3分。

**结论:** 提出的基于token级别奖励指导的DPO方法能够在不同基准上实现优于标准DPO的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TGDPO%3A+Harnessing+Token-Level+Reward+Guidance+for+Enhancing+Direct+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14574，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14574&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in reinforcement learning from human feedback have shown
that utilizing fine-grained token-level reward models can substantially enhance
the performance of Proximal Policy Optimization (PPO) in aligning large
language models. However, it is challenging to leverage such token-level reward
as guidance for Direct Preference Optimization (DPO), since DPO is formulated
as a sequence-level bandit problem. To address this challenge, this work
decomposes the sequence-level PPO into a sequence of token-level proximal
policy optimization problems and then frames the problem of token-level PPO
with token-level reward guidance, from which closed-form optimal token-level
policy and the corresponding token-level reward can be derived. Using the
obtained reward and Bradley-Terry model, this work establishes a framework of
computable loss functions with token-level reward guidance for DPO, and
proposes a practical reward guidance based on the induced DPO reward. This
formulation enables different tokens to exhibit varying degrees of deviation
from reference policy based on their respective rewards. Experiment results
demonstrate that our method achieves substantial performance improvements over
DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on
AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at
https://github.com/dvlab-research/TGDPO.

</details>


### [81] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/abs/2506.14577)
*Abdul Rahman Jacob, Avinash Kori, Emanuele De Angelis, Ben Glocker, Maurizio Proietti, Francesca Toni*

**主要类别:** cs.LG

**AI概要:** 摘要介绍了一种新的神经论辩学习（NAL）架构，它将基于假设的论证（ABA）与深度学习相结合，用于图像分析。该架构在合成数据上的实验表明，它可以与最先进的方法竞争。


<details>
  <summary>更多</summary>
  
**动机:** 随着我们越来越多地依赖深度学习技术来做关键决策，对于它们的安全性、可靠性和可解释性的担忧也随之出现。

**方法:** 提出了一种新的神经论辩学习（NAL）架构，该架构结合了基于对象的学习来分割和编码图像为事实，并应用了基于假设的论证（ABA）学习来开发能够使用图像进行预测的ABA框架。

**结果:** 在合成数据上进行的实验显示，NAL架构可以与当前最先进的替代方案相媲美。

**结论:** 研究通过引入NAL架构，为解决深度学习中的安全性、可靠性和可解释性问题提供了新途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Object-Centric+Neuro-Argumentative+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14577，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14577&send_immediately=true&force_search=false)

**原文摘要:** Over the last decade, as we rely more on deep learning technologies to make
critical decisions, concerns regarding their safety, reliability and
interpretability have emerged. We introduce a novel Neural Argumentative
Learning (NAL) architecture that integrates Assumption-Based Argumentation
(ABA) with deep learning for image analysis. Our architecture consists of
neural and symbolic components. The former segments and encodes images into
facts using object-centric learning, while the latter applies ABA learning to
develop ABA frameworks enabling predictions with images. Experiments on
synthetic data show that the NAL architecture can be competitive with a
state-of-the-art alternative.

</details>


### [82] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/abs/2506.14587)
*Shuo Yang, Bardh Prenkaj, Gjergji Kasneci*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为SCISSOR的新方法，旨在通过抑制语义簇来防止模型利用捷径学习。SCISSOR不需要数据增强或重写，并在多个基准测试中展示了性能提升，特别是在轻量级模型上。


<details>
  <summary>更多</summary>
  
**动机:** 研究发现样本嵌入的语义分布不平衡会导致虚假的语义相关性，从而损害模型对于未见数据的鲁棒性。为了解决这个问题，提出了SCISSOR方法以重新映射语义空间并阻止模型依赖于这些捷径。

**方法:** SCISSOR是一种基于Siamese网络的方法，它通过抑制被用作捷径的潜在语义簇来重新映射语义空间，与之前需要进行数据增强和重写的方法不同，SCISSOR直接作用于模型训练过程。

**结果:** SCISSOR在计算机视觉（Chest-XRay, Not-MNIST）和自然语言处理任务（GYAFC, Yelp）上的四个基准测试中表现优于几个基线模型，特别是在F1分数上有显著提高。此外，在轻量级模型如ViT和BERT上也观察到了明显的改进。

**结论:** 研究表明，解决被忽视的语义偏差对改善模型泛化能力至关重要。SCISSOR提供了一个基础框架，用于减轻捷径学习现象，并促进开发出更加鲁棒且抗偏的AI系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCISSOR%3A+Mitigating+Semantic+Bias+through+Cluster-Aware+Siamese+Networks+for+Robust+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14587，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14587&send_immediately=true&force_search=false)

**原文摘要:** Shortcut learning undermines model generalization to out-of-distribution
data. While the literature attributes shortcuts to biases in superficial
features, we show that imbalances in the semantic distribution of sample
embeddings induce spurious semantic correlations, compromising model
robustness. To address this issue, we propose SCISSOR (Semantic Cluster
Intervention for Suppressing ShORtcut), a Siamese network-based debiasing
approach that remaps the semantic space by discouraging latent clusters
exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR
eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on
6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and
GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports
+5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay,
and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models
with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for
BERT on NLP. Our study redefines the landscape of model generalization by
addressing overlooked semantic biases, establishing SCISSOR as a foundational
framework for mitigating shortcut learning and fostering more robust,
bias-resistant AI systems.

</details>


### [83] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/abs/2506.14607)
*Ziyu Gong, Jim Lim, David I. Inouye*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的基于似然的分布匹配（DM）训练方法，通过使用表达能力强的基于分数的先验分布来解决传统方法中存在的偏见、稳定性和模式崩溃等问题，并在多个任务上展示了优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的非参数DM方法存在可扩展性问题，对抗性的DM方法则不稳定且容易出现模式崩溃。基于似然的方法尽管有潜力，但通常会通过固定的先验引入不必要的偏差，或者需要显式的密度模型，这可能难以训练。

**方法:** 研究者们介绍了一种新的基于似然的DM训练方法，利用了表达力强的基于分数的先验分布。他们发现基于梯度的DM训练只需要先验的分数函数而非其密度，从而可以通过去噪分数匹配来训练先验。这种方法消除了固定先验带来的偏差，使得几何保留正则化更有效，同时避免了学习显式先验密度模型的挑战。

**结果:** 该方法相比其他基于扩散的先验（如LSGM）表现出了更好的稳定性和计算效率。实验结果表明，在多个任务中该基于分数的方法表现优于现有方法。

**结论:** 提出的新方法为分布匹配提供了一个稳定而有效的解决方案，并在不同任务中显示出优越的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Expressive+Score-Based+Priors+for+Distribution+Matching+with+Geometry-Preserving+Regularization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14607，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14607&send_immediately=true&force_search=false)

**原文摘要:** Distribution matching (DM) is a versatile domain-invariant representation
learning technique that has been applied to tasks such as fair classification,
domain adaptation, and domain translation. Non-parametric DM methods struggle
with scalability and adversarial DM approaches suffer from instability and mode
collapse. While likelihood-based methods are a promising alternative, they
often impose unnecessary biases through fixed priors or require explicit
density models (e.g., flows) that can be challenging to train. We address this
limitation by introducing a novel approach to training likelihood-based DM
using expressive score-based prior distributions. Our key insight is that
gradient-based DM training only requires the prior's score function -- not its
density -- allowing us to train the prior via denoising score matching. This
approach eliminates biases from fixed priors (e.g., in VAEs), enabling more
effective use of geometry-preserving regularization, while avoiding the
challenge of learning an explicit prior density model (e.g., a flow-based
prior). Our method also demonstrates better stability and computational
efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore,
experiments demonstrate superior performance across multiple tasks,
establishing our score-based method as a stable and effective approach to
distribution matching. Source code available at
https://github.com/inouye-lab/SAUB.

</details>


### [84] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/abs/2506.14619)
*Paolo Ascia, Elena Raponi, Thomas Bäck, Fabian Duddeck*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种针对存在昂贵约束条件下的贝叶斯优化算法FuRBO，通过自适应的信任区域策略加快了可行解的发现速度，并在多个基准测试中展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 许多实际优化任务需要在有限的评估预算下进行，并且涉及到成本高昂的约束条件，这些约束条件的解析形式未知且通常定义在高维空间中，其中可行区域小、不规则且难以识别。现有的方法可能会花费大量预算来寻找第一个可行解，从而限制了它们的有效性。

**方法:** 提出了Feasibility-Driven Trust Region Bayesian Optimization (FuRBO) 算法，该算法迭代地定义一个信任区域，从中选择下一个候选解，同时利用目标和约束替代模型的信息。此自适应策略允许信任区域在每次迭代之间显著移动和调整大小，使优化器能够快速重新聚焦搜索并持续加速发现可行且高质量的解决方案。

**结果:** 通过完整的BBOB约束COCO基准套件和其他受物理启发的基准测试，实证证明了FuRBO的有效性。将其与不同约束严重程度和问题维度（从2到60）下的最新黑盒约束优化基线进行了比较。

**结论:** FuRBO算法能够在存在昂贵约束条件下有效地提高优化效率，尤其是在高维问题上，它表现出比现有方法更快找到可行解的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feasibility-Driven+Trust+Region+Bayesian+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14619&send_immediately=true&force_search=false)

**原文摘要:** Bayesian optimization is a powerful tool for solving real-world optimization
tasks under tight evaluation budgets, making it well-suited for applications
involving costly simulations or experiments. However, many of these tasks are
also characterized by the presence of expensive constraints whose analytical
formulation is unknown and often defined in high-dimensional spaces where
feasible regions are small, irregular, and difficult to identify. In such
cases, a substantial portion of the optimization budget may be spent just
trying to locate the first feasible solution, limiting the effectiveness of
existing methods. In this work, we present a Feasibility-Driven Trust Region
Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust
region from which the next candidate solution is selected, using information
from both the objective and constraint surrogate models. Our adaptive strategy
allows the trust region to shift and resize significantly between iterations,
enabling the optimizer to rapidly refocus its search and consistently
accelerate the discovery of feasible and good-quality solutions. We empirically
demonstrate the effectiveness of FuRBO through extensive testing on the full
BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks,
comparing it against state-of-the-art baselines for constrained black-box
optimization across varying levels of constraint severity and problem
dimensionalities ranging from 2 to 60.

</details>


### [85] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender, Jan Herrmann, Klaus-Robert Müller, Grégoire Montavon*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的'平滑反事实探索器'(SCE)算法，旨在解决现有视觉反事实解释器(VCEs)在优化样本质量或变化最小化方面过于狭隘的问题，并通过系统评估证明了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉反事实解释器（VCEs）在提高图像分类器透明度方面很有前景，但它们过于关注优化样本质量或改变的最小化，而忽略了对解释的整体要求，比如忠实性、可理解性和充分性。

**方法:** 研究者们探索了新的反事实生成机制，并研究了这些机制如何帮助满足上述对解释的要求。他们将这些新机制结合成一种新颖的‘平滑反事实探索器’(SCE)算法。

**结果:** 通过合成数据和真实数据上的系统评估，展示了SCE算法的有效性。

**结论:** 论文介绍了一种新的方法来改进视觉反事实解释器，该方法不仅关注样本质量和变化的最小化，还考虑到了解释的忠实性、可理解性和充分性，并且通过实验证明了其效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Desiderata-Driven+Design+of+Visual+Counterfactual+Explainers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14698&send_immediately=true&force_search=false)

**原文摘要:** Visual counterfactual explainers (VCEs) are a straightforward and promising
approach to enhancing the transparency of image classifiers. VCEs complement
other types of explanations, such as feature attribution, by revealing the
specific data transformations to which a machine learning model responds most
strongly. In this paper, we argue that existing VCEs focus too narrowly on
optimizing sample quality or change minimality; they fail to consider the more
holistic desiderata for an explanation, such as fidelity, understandability,
and sufficiency. To address this shortcoming, we explore new mechanisms for
counterfactual generation and investigate how they can help fulfill these
desiderata. We combine these mechanisms into a novel 'smooth counterfactual
explorer' (SCE) algorithm and demonstrate its effectiveness through systematic
evaluations on synthetic and real data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] ['Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra](https://arxiv.org/abs/2506.13768)
*Stefan Reimann*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个非结合代数框架，用于在高维空间内对信息项目进行表示和运算，该框架支持长时间序列的记忆编码同时保留时序结构，并且能重现序列位置曲线。


<details>
  <summary>更多</summary>
  
**动机:** 为了在保持时间结构的同时，在高维空间中有效地表示任意长度的序列，并且不依赖于辅助顺序结构。同时，该框架与空间计算原则以及认知科学关于记忆的经验发现相一致。

**方法:** 提出了一种非结合性的代数框架，用于高维空间中的信息项表示和计算。通过类似于乘法的绑定和非结合性干扰式的捆绑进行计算。提出了L状态（通过左结合捆绑生成）和R状态（通过右结合捆绑形成），分别强调了近因效应和首因效应。

**结果:** 提出的非结合性捆绑允许构建任意长度序列的稀疏表示，并保持其时间结构。噪声是顺序信息表示的一个组成部分，而非掩盖它的手段。单一序列由两个不同的状态表示，即L状态和R状态。

**结论:** 该模型能够复制序列位置曲线，反映了认知实验中观察到的近期效应和首因效应。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%27Memory+States%27+from+Almost+Nothing%3A+Representing+and+Computing+in+a+Non-associative+Algebra，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13768&send_immediately=true&force_search=false)

**原文摘要:** This note presents a non-associative algebraic framework for the
representation and computation of information items in high-dimensional space.
This framework is consistent with the principles of spatial computing and with
the empirical findings in cognitive science about memory. Computations are
performed through a process of multiplication-like binding and non-associative
interference-like bundling. Models that rely on associative bundling typically
lose order information, which necessitates the use of auxiliary order
structures, such as position markers, to represent sequential information that
is important for cognitive tasks. In contrast, the non-associative bundling
proposed allows the construction of sparse representations of arbitrarily long
sequences that maintain their temporal structure across arbitrary lengths. In
this operation, noise is a constituent element of the representation of order
information, rather than a means of obscuring it. The non-associative nature of
the proposed framework results in the representation of a single sequence by
two distinct states. The L-state, generated through left-associative bundling,
continuously updates and emphasises a recency effect, while the R-state, formed
through right-associative bundling, encodes finite sequences or chunks,
capturing a primacy effect. The construction of these states may be associated
with activity in the prefrontal cortex in relation to short-term memory and
hippocampal encoding in long-term memory, respectively. The accuracy of
retrieval is contingent upon a decision-making process that is based on the
mutual information between the memory states and the cue. The model is able to
replicate the Serial Position Curve, which reflects the empirical recency and
primacy effects observed in cognitive experiments.

</details>


### [87] [Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs](https://arxiv.org/abs/2506.13773)
*Milapji Singh Gill, Tom Jeleniewski, Felix Gehlhoff, Alexander Fay*

**主要类别:** cs.AI

**AI概要:** 本文介绍了两个用于在知识图谱中表示和丰富微分方程的本体论工件，以及一种高效生成知识图谱的方法，并在航空维修领域进行了验证。


<details>
  <summary>更多</summary>
  
**动机:** 为了确保时间连续动态模型在不同生命周期阶段的有效可用性，需要将这些以微分方程形式存在的行为信息与更多的CPS信息进行情境化整合。当前缺乏可重用的本体论工件和方法来减少手动实例化的工作量。

**方法:** 提出了一种基于标准的模块化语义模型，直接在知识图谱中表示微分方程并对其语义进行丰富；还介绍了一种有效生成知识图谱的方法。

**结果:** 所提出的工件在航空维修领域得到了验证，结果显示一个复杂的电液伺服作动器的微分方程可以在知识图谱中正式表示，并与其他生命周期数据相结合。

**结论:** 引入的工件展示了其实用的应用性，能够有效地在知识图谱中表示微分方程，并将其与其它生命周期数据相联系。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Representing+Time-Continuous+Behavior+of+Cyber-Physical+Systems+in+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13773&send_immediately=true&force_search=false)

**原文摘要:** Time-continuous dynamic models are essential for various Cyber-Physical
System (CPS) applications. To ensure effective usability in different lifecycle
phases, such behavioral information in the form of differential equations must
be contextualized and integrated with further CPS information. While knowledge
graphs provide a formal description and structuring mechanism for this task,
there is a lack of reusable ontological artifacts and methods to reduce manual
instantiation effort. Hence, this contribution introduces two artifacts:
Firstly, a modular semantic model based on standards is introduced to represent
differential equations directly within knowledge graphs and to enrich them
semantically. Secondly, a method for efficient knowledge graph generation is
presented. A validation of these artifacts was conducted in the domain of
aviation maintenance. Results show that differential equations of a complex
Electro-Hydraulic Servoactuator can be formally represented in a knowledge
graph and be contextualized with other lifecycle data, proving the artifacts'
practical applicability.

</details>


### [88] [Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values](https://arxiv.org/abs/2506.13774)
*Nell Watson, Ahmed Amer, Evan Harris, Preeti Ravindra, Shujun Zhang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为'超我'代理的新型解决方案，旨在为自主AI提供个性化监督机制。该系统通过用户选择的包含多样化规则集的'信条宪法'来动态指导AI规划，并在执行前通过实时合规执行器验证计划。实验证明，这种方法显著减少了有害输出，提高了安全性。


<details>
  <summary>更多</summary>
  
**动机:** 当前的AI对齐方法难以提供深度、个性化的上下文信息而不引发混淆或操作效率低下。此外，将AI行为与人类价值观、复杂的安全要求和特定的合规需求相匹配也是一个挑战。

**方法:** 引入了一种名为'超我'代理的新系统，它根据用户选定的'信条宪法'动态地引导AI规划，并且能够调整遵循程度以适应不可妥协的价值观。系统还包括一个实时合规执行器，在执行前对计划进行验证。

**结果:** 广泛的基准评估（如HarmBench, AgentHarm）表明，'超我'代理大幅减少了有害输出，对于领先的LLM模型如Gemini 2.5 Flash 和 GPT-4o，达到了高达98.3%的危害分数降低和近乎完美的拒绝率。

**结论:** 此方法简化了个性化AI对齐过程，使自主系统更可靠地适应个体和文化背景，同时实现了重要的安全改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Personalized+Constitutionally-Aligned+Agentic+Superego%3A+Secure+AI+Behavior+Aligned+to+Diverse+Human+Values，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13774&send_immediately=true&force_search=false)

**原文摘要:** Agentic AI systems, possessing capabilities for autonomous planning and
action, exhibit immense potential across diverse domains. However, their
practical deployment is significantly hampered by challenges in aligning their
behavior with varied human values, complex safety requirements, and specific
compliance needs. Existing alignment methodologies often falter when faced with
the intricate task of providing deep, personalized contextual information
without inducing confabulation or operational inefficiencies. This paper
introduces a novel solution: a 'superego' agent, designed as a personalized
oversight mechanism for agentic AI. This system dynamically steers AI planning
by referencing user-selected "Creed Constitutions"-encapsulating diverse rule
sets-with adjustable adherence levels to fit non-negotiable values. A real-time
compliance enforcer validates plans against these constitutions and a universal
ethical floor before execution. We present a functional system, including a
demonstration interface (www.Creed.Space) with a prototypical
constitution-sharing portal, and successful integration with third-party models
via the Model Context Protocol (MCP). Comprehensive benchmark evaluations
(HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces
harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect
refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for
leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially
simplifies personalized AI alignment, rendering agentic systems more reliably
attuned to individual and cultural contexts, while also enabling substantial
safety improvements. An overview on this research with examples is available at
https://superego.creed.space.

</details>


### [89] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei, Patricia Paskov, Sunishchal Dev, Michael J. Byun, Anka Reuel, Xavier Roberts-Gaal, Rachel Calcott, Evie Coxon, Chinmay Deshpande*

**主要类别:** cs.AI

**AI概要:** 本文提出，基础模型评估中的人类基线必须更加严格和透明，以实现人类与AI性能的有意义比较，并提供了建议和报告清单。基于对测量理论和AI评估文献的元回顾，作者提出了设计、执行和报告人类基线的框架和建议。通过系统性地回顾了115个人类基线研究，确定了现有方法的不足之处。


<details>
  <summary>更多</summary>
  
**动机:** 作者认为现有的人类基线方法不够严格且记录不充分，难以稳健地衡量和评估性能差异。为了解决这一问题，需要更严谨的人类基线来解释AI评估结果。

**方法:** 通过对测量理论和AI评估文献进行元回顾，导出一个关于设计、执行和报告人类基线的框架和建议。将这些建议综合成一个清单，用于系统性地审查115个在基础模型评估中的案例研究。

**结果:** 发现现有基线方法存在不足，提供了改进人类基线实践的具体建议和检查清单，有助于研究人员更好地开展人类基线研究和报告结果。

**结论:** 希望该工作能够推动更严格的AI评估实践，更好地服务于研究社区和决策者。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Recommendations+and+Reporting+Checklist+for+Rigorous+%26+Transparent+Human+Baselines+in+Model+Evaluations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13776，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13776&send_immediately=true&force_search=false)

**原文摘要:** In this position paper, we argue that human baselines in foundation model
evaluations must be more rigorous and more transparent to enable meaningful
comparisons of human vs. AI performance, and we provide recommendations and a
reporting checklist towards this end. Human performance baselines are vital for
the machine learning community, downstream users, and policymakers to interpret
AI evaluations. Models are often claimed to achieve "super-human" performance,
but existing baselining methods are neither sufficiently rigorous nor
sufficiently well-documented to robustly measure and assess performance
differences. Based on a meta-review of the measurement theory and AI evaluation
literatures, we derive a framework with recommendations for designing,
executing, and reporting human baselines. We synthesize our recommendations
into a checklist that we use to systematically review 115 human baselines
(studies) in foundation model evaluations and thus identify shortcomings in
existing baselining methods; our checklist can also assist researchers in
conducting human baselines and reporting results. We hope our work can advance
more rigorous AI evaluation practices that can better serve both the research
community and policymakers. Data is available at:
https://github.com/kevinlwei/human-baselines

</details>


### [90] [The NordDRG AI Benchmark for Large Language Models](https://arxiv.org/abs/2506.13790)
*Tapio Pitkäranta*

**主要类别:** cs.AI

**AI概要:** 本文介绍了NordDRG-AI-Benchmark，这是一个针对医院资金层的公开测试平台，旨在评估大型语言模型（LLMs）在多语言诊断、程序和费用逻辑推理方面的能力。基准测试包括定义表、专家手册和变更日志模板以及一系列案例组合任务，并且提供了五个先进LLMs在此基准上的表现差异。


<details>
  <summary>更多</summary>
  
**动机:** 为了填补目前没有开放基准专门针对医院资金层面（其中诊断相关组DRG决定了许多国家的报销）来评估大型语言模型能力的空白。

**方法:** 创建并发布了一个名为NordDRG-AI-Benchmark的公共测试平台，该平台包含一套完整的DRG规则集，可以评估LLM处理多语言诊断、程序和费用逻辑的能力。该基准测试由三类人工制品组成：定义表、专家手册与变更日志模板及一系列案例组合任务。

**结果:** 通过一个基线演示表明，五种最先进的LLMs在九个自动可验证任务中的表现大相径庭：o3（OpenAI）得分为9/9，GPT-4o 和 o4-mini-high 得分为7/9，而Gemini 2.5 Pro和Gemini 2.5 Flash分别只解决了5/9和3/9的任务。

**结论:** NordDRG-AI-Benchmark揭示了领域特定的优势与劣势，这些在通用LLM基准中可能未被发现，为研究医院资助方面的可信自动化提供了一个可复制的基线。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+NordDRG+AI+Benchmark+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13790&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are already being piloted for clinical coding
and decision support. However, until now, no open benchmark has targeted the
hospital funding layer where Diagnosis-Related Groups (DRG) determine
reimbursement across many countries. We release NordDRG-AI-Benchmark, the first
public test-bed that captures a complete DRG rule set and evaluates an LLM's
ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with
20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits,
and country flags; (ii) expert manuals and changelog templates describing real
governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span
code lookup, cross-table inference, multilingual terminology, and
quality-assurance audits.
  All artefacts are available at:
https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very
differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9
out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and
Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These
results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths
and weaknesses that remain hidden in generic LLM benchmarks, offering a
reproducible baseline for research on trustworthy automation in hospital
funding.

</details>


### [91] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
*Gonçalo Hora de Carvalho, Lazar S. Popov, Sander Kaatee, Kristinn R. Thórisson, Tangrui Li, Pétur Húni Björnsson, Jilles S. Dibangoye*

**主要类别:** cs.AI

**AI概要:** 本文介绍了ICE-ID，一个用于历史身份解析的新基准数据集，包含了220年（1703-1920）的冰岛人口普查记录。它为长期人员实体匹配提供了首个大规模开放表格数据集，并定义了身份解析任务及评估指标。研究对比了多种方法，发现NARS（非公理性推理系统）在该任务上达到了最优表现。


<details>
  <summary>更多</summary>
  
**动机:** 创建了一个新的基准数据集ICE-ID，旨在为真实世界人口中的长期个人实体匹配提供研究资源，并促进跨学科的数据关联和历史分析研究。

**方法:** 定义了不同的人口普查波次内和跨波次的身份解析任务，并使用一系列方法进行评价：手工规则匹配器、机器学习集成方法以及结构化数据的大语言模型（如基于变压器的表格网络），并引入了一种新的处理表格数据的方法NARS。

**结果:** 实验表明，NARS方法尽管简单，但在给定任务上的表现与其它标准方法相当，甚至达到最佳状态。

**结论:** 通过发布ICE-ID数据集和代码，作者希望支持纵向设置中身份解析方法的可重复性基准测试，并开辟数据关联和历史分析领域的新研究途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ICE-ID%3A+A+Novel+Historical+Census+Data+Benchmark+Comparing+NARS+against+LLMs%2C+%5C%26+a+ML+Ensemble+on+Longitudinal+Identity+Resolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13792，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13792&send_immediately=true&force_search=false)

**原文摘要:** We introduce ICE-ID, a novel benchmark dataset for historical identity
resolution, comprising 220 years (1703-1920) of Icelandic census records.
ICE-ID spans multiple generations of longitudinal data, capturing name
variations, demographic changes, and rich genealogical links. To the best of
our knowledge, this is the first large-scale, open tabular dataset specifically
designed to study long-term person-entity matching in a real-world population.
We define identity resolution tasks (within and across census waves) with
clearly documented metrics and splits. We evaluate a range of methods:
handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured
data (e.g. transformer-based tabular networks) against a novel approach to
tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose
AI framework designed to reason with limited knowledge and resources. Its core
is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that
NARS is suprisingly simple and competitive with other standard approaches,
achieving SOTA at our task. By releasing ICE-ID and our code, we enable
reproducible benchmarking of identity resolution approaches in longitudinal
settings and hope that ICE-ID opens new avenues for cross-disciplinary research
in data linkage and historical analytics.

</details>


### [92] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/abs/2506.13793)
*Zongxian Yang, Jiayu Qian, Zegao Peng, Haoyu Zhang, Zhi-An Huang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为Med-REFL的方法，用于通过自我纠正的细粒度反思来提高医学推理的质量。该方法在不需要大量专家标注的情况下，能够自动构建直接偏好优化数据，并指导模型识别和纠正推理错误。实验表明，这种方法在多个具有挑战性的医学问答数据集上提升了现有模型的表现，尤其是在USMLE基准测试中取得了显著进步。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型推理模型在数学和代码推理方面取得了显著进展，但这些成就并未顺利地转移到医学领域。其中一个重要原因是缺乏对中间反思步骤质量的关注，而这一点在高风险医疗场景中尤为重要。为了解决这一问题，作者提出了Med-REFL方法。

**方法:** Med-REFL采用了“思维树”方法将医学问题分解成细粒度的推理路径，并对每个步骤及其后续反思进行量化评估。这种评估方式允许自动生成直接偏好优化所需的数据，减少了对昂贵专家注释的需求，同时引导模型识别并修正推理中的错误。

**结果:** 实验结果表明，在MedQA-USMLE基准测试中，Med-REFL实现了稳定提升，平均增益达到4.11%。对于7B/8B级别的模型，它进一步提高了现有最先进性能达4.13%。此外，Med-REFL还在多个挑战性医学问答数据集中展现了强大的泛化能力和鲁棒性。

**结论:** 本研究表明注重反思质量可以导致更准确可信的医学AI应用中的推理。通过引入Med-REFL方法，不仅提高了模型的准确性，也增强了其在处理复杂医疗案例时的可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Med-REFL%3A+Medical+Reasoning+Enhancement+via+Self-Corrected+Fine-grained+Reflection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13793，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13793&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models have recently made significant strides in mathematical
and code reasoning, yet their success has not transferred smoothly to the
medical domain. While multiple factors contribute to this disparity, a critical
issue is the inadequate focus on the quality of intermediate reflection steps,
which is particularly crucial in high-stakes medical scenarios. To address this
challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical
\underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via
self-corrected \underline{\textbf{F}}ine-grained
ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought
approach to decompose medical questions into fine-grained reasoning paths,
quantitatively evaluating each step and its subsequent reflections. These
assessments enable automatic construction of direct preference optimization
data, reducing reliance on expensive expert annotations while guiding models to
identify and correct reasoning errors. Experimental results on the MedQA-USMLE
benchmark demonstrate Med-REFL achieves consistent improvements, with average
gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance
of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong
generalization capabilities and robustness across several challenging medical
question-answering datasets. Our work illustrates that prioritizing reflection
quality leads to more accurate and trustworthy reasoning in medical AI
applications. Checkpoints, code, and data can be found
\href{https://github.com/TianYin123/Med-REFL}{here}.

</details>


### [93] [BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection](https://arxiv.org/abs/2506.13795)
*Boshen Shi, Yongqing Wang, Fangda Guo, Jiangli Shao, Huawei Shen, Xueqi Cheng*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为BotTrans的多源图域适应模型，通过利用多个源网络中的标记知识和提高网络同质性来解决社交机器人检测中的标签稀缺问题。


<details>
  <summary>更多</summary>
  
**动机:** 在基于GNN的模型中，通过从相关社交网络转移大量知识以克服检测社交机器人和其他异常时遇到的标签稀缺问题。然而，有效的知识迁移面临两个关键挑战：网络异质性问题以及单源迁移可能导致的结果不佳和不稳定。

**方法:** 首先利用多个源网络间共享的标记知识建立具有增强网络同质性的跨源域拓扑；然后聚合跨域邻居信息以增强源节点嵌入的区分度；接着结合每个源-目标对之间的相关性与模型优化，促进更相关源网络的知识转移；最后提出一种改进策略，利用目标域内的语义知识来提高检测性能。

**结果:** 广泛的实验表明，BotTrans在实际数据集上优于现有的最先进方法，特别是在目标检测任务未标记的情况下能够有效利用多源知识。

**结论:** 该研究提出的BotTrans模型为解决社交机器人检测中的标签稀缺问题提供了新的途径，并展示了其相对于现有方法的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BotTrans%3A+A+Multi-Source+Graph+Domain+Adaptation+Approach+for+Social+Bot+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13795，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13795&send_immediately=true&force_search=false)

**原文摘要:** Transferring extensive knowledge from relevant social networks has emerged as
a promising solution to overcome label scarcity in detecting social bots and
other anomalies with GNN-based models. However, effective transfer faces two
critical challenges. Firstly, the network heterophily problem, which is caused
by bots hiding malicious behaviors via indiscriminately interacting with human
users, hinders the model's ability to learn sufficient and accurate bot-related
knowledge from source domains. Secondly, single-source transfer might lead to
inferior and unstable results, as the source network may embody weak relevance
to the task and provide limited knowledge. To address these challenges, we
explore multiple source domains and propose a multi-source graph domain
adaptation model named \textit{BotTrans}. We initially leverage the labeling
knowledge shared across multiple source networks to establish a
cross-source-domain topology with increased network homophily. We then
aggregate cross-domain neighbor information to enhance the discriminability of
source node embeddings. Subsequently, we integrate the relevance between each
source-target pair with model optimization, which facilitates knowledge
transfer from source networks that are more relevant to the detection task.
Additionally, we propose a refinement strategy to improve detection performance
by utilizing semantic knowledge within the target domain. Extensive experiments
on real-world datasets demonstrate that \textit{BotTrans} outperforms the
existing state-of-the-art methods, revealing its efficacy in leveraging
multi-source knowledge when the target detection task is unlabeled.

</details>


### [94] [Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization](https://arxiv.org/abs/2506.13799)
*Soroush Vahidi*

**主要类别:** cs.AI

**AI概要:** 本文提出了一套可扩展的算法，用于在大规模加权有向图中最小化反馈弧，目的是揭示神经连接体中的生物意义前馈结构。实验表明，与先前的顶级方法相比，我们的最佳解决方案提高了前向边权重。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于开发一种算法，能够在大规模加权有向图中减少反馈弧的数量，以期能够揭示出神经连接体中具有生物学意义的前馈结构。

**方法:** 该论文采用了一系列结合贪婪启发式、增益感知局部优化和基于强连通分量的全局结构分析的方法，并使用FlyWire Connectome Challenge数据集来展示这些排序策略的有效性。

**结果:** 实验结果表明，所提出的最佳方案在前向边权重上优于之前表现最好的方法。

**结论:** 通过提出的新算法，研究者们能够更有效地从神经连接体中提取出有意义的前馈结构，而且这些算法已经被高效地实现并可以通过Google Colab Pro+进行云执行。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feedforward+Ordering+in+Neural+Connectomes+via+Feedback+Arc+Minimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13799&send_immediately=true&force_search=false)

**原文摘要:** We present a suite of scalable algorithms for minimizing feedback arcs in
large-scale weighted directed graphs, with the goal of revealing biologically
meaningful feedforward structure in neural connectomes. Using the FlyWire
Connectome Challenge dataset, we demonstrate the effectiveness of our ranking
strategies in maximizing the total weight of forward-pointing edges. Our
methods integrate greedy heuristics, gain-aware local refinements, and global
structural analysis based on strongly connected components. Experiments show
that our best solution improves the forward edge weight over previous
top-performing methods. All algorithms are implemented efficiently in Python
and validated using cloud-based execution on Google Colab Pro+.

</details>


### [95] [Causality in the human niche: lessons for machine learning](https://arxiv.org/abs/2506.13803)
*Richard D. Lange, Konrad P. Kording*

**主要类别:** cs.AI

**AI概要:** 本文探讨了人类因果认知的独特方面，这些方面在现有的结构因果模型（SCM）框架中未能充分捕捉。作者认为，为了构建更有效的、可解释的人工智能，需要将类似人类的因果能力纳入机器学习系统，并且应当从人类所处的社会、自主和目标驱动的环境中理解因果关系的重要性。


<details>
  <summary>更多</summary>
  
**动机:** 当前机器学习系统在某些领域表现不佳，而人类能够高效地在新领域内进行泛化和学习。文章动机在于通过研究人类因果认知的特点以及它们如何适应于人类的生活环境，来改善机器学习系统的能力。

**方法:** 该论文采用了理论分析的方法，对比了人类因果认知与基于结构因果模型（SCM）的形式化因果性之间的差异，并讨论了人类如何通过类比等方法来推广因果知识。

**结果:** 研究表明，人类的因果推理能力在很多方面优于现有的机器学习方法，特别是对于相似对象间的因果属性迁移。

**结论:** 为了使机器学习系统更加高效、可控和易于解释，未来的研究应该更多地借鉴人类因果认知的归纳偏差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causality+in+the+human+niche%3A+lessons+for+machine+learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13803&send_immediately=true&force_search=false)

**原文摘要:** Humans interpret the world around them in terms of cause and effect and
communicate their understanding of the world to each other in causal terms.
These causal aspects of human cognition are thought to underlie humans' ability
to generalize and learn efficiently in new domains, an area where current
machine learning systems are weak. Building human-like causal competency into
machine learning systems may facilitate the construction of effective and
interpretable AI. Indeed, the machine learning community has been importing
ideas on causality formalized by the Structural Causal Model (SCM) framework,
which provides a rigorous formal language for many aspects of causality and has
led to significant advances. However, the SCM framework fails to capture some
salient aspects of human causal cognition and has likewise not yet led to
advances in machine learning in certain critical areas where humans excel. We
contend that the problem of causality in the ``human niche'' -- for a social,
autonomous, and goal-driven agent sensing and acting in the world in which
humans live -- is quite different from the kind of causality captured by SCMs.
For example, everyday objects come in similar types that have similar causal
properties, and so humans readily generalize knowledge of one type of object
(cups) to another related type (bowls) by drawing causal analogies between
objects with similar properties, but such analogies are at best awkward to
express in SCMs. We explore how such causal capabilities are adaptive in, and
motivated by, the human niche. By better appreciating properties of human
causal cognition and, crucially, how those properties are adaptive in the niche
in which humans live, we hope that future work at the intersection of machine
learning and causality will leverage more human-like inductive biases to create
more capable, controllable, and interpretable systems.

</details>


### [96] [Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study](https://arxiv.org/abs/2506.13810)
*Olivier Saidi*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的模式感知复杂性框架，通过量化和利用结构规律性来减少跨领域的有效计算复杂性，包括金融预测和大语言模型优化，并在TSP基准测试中实现了高达79%的解决方案质量提升。


<details>
  <summary>更多</summary>
  
**动机:** 尽管NP难优化问题如旅行商问题（TSP）在最坏情况下难以高效解决，但实际问题实例往往表现出可利用的模式。作者希望开发一种能够识别并利用这些模式的方法，以降低有效计算复杂度。

**方法:** 文章介绍了一个新颖的模式感知复杂性框架，定义了诸如模式利用率效率（PUE）这样的度量指标，并且提出了一个由元学习驱动的求解器流程。该方法旨在量化和利用结构性规律，例如聚类和对称性。

**结果:** 研究者们在TSP基准测试中（城市数量从22到2392不等）实现了最高达79%的解决方案质量改善。

**结论:** 与理论上的NP难度不同，本研究提供了一个统一而实用的角度来看待基于模式的效率，并且在多个领域内展示了其应用潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Pattern-Aware+Complexity+with+NP-Hard+Optimization%3A+A+Unifying+Framework+and+Empirical+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13810&send_immediately=true&force_search=false)

**原文摘要:** NP hard optimization problems like the Traveling Salesman Problem (TSP) defy
efficient solutions in the worst case, yet real-world instances often exhibit
exploitable patterns. We propose a novel patternaware complexity framework that
quantifies and leverages structural regularities e.g., clustering, symmetry to
reduce effective computational complexity across domains, including financial
forecasting and LLM optimization. With rigorous definitions, theorems, and a
meta learning driven solver pipeline, we introduce metrics like Pattern
Utilization Efficiency (PUE) and achieve up to 79 percent solution quality
gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP
hardness, our approach offers a unified, practical lens for pattern-driven
efficiency.

</details>


### [97] [The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness](https://arxiv.org/abs/2506.13825)
*Gnankan Landry Regis N'guessan, Issa Karambal*

**主要类别:** cs.AI

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Reflexive+Integrated+Information+Unit%3A+A+Differentiable+Primitive+for+Artificial+Consciousness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13825&send_immediately=true&force_search=false)

**原文摘要:** Research on artificial consciousness lacks the equivalent of the perceptron:
a small, trainable module that can be copied, benchmarked, and iteratively
improved. We introduce the Reflexive Integrated Information Unit (RIIU), a
recurrent cell that augments its hidden state $h$ with two additional vectors:
(i) a meta-state $\mu$ that records the cell's own causal footprint, and (ii) a
broadcast buffer $B$ that exposes that footprint to the rest of the network. A
sliding-window covariance and a differentiable Auto-$\Phi$ surrogate let each
RIIU maximize local information integration online. We prove that RIIUs (1) are
end-to-end differentiable, (2) compose additively, and (3) perform
$\Phi$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a
four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator
failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero
Auto-$\Phi$ signal. By shrinking "consciousness-like" computation down to unit
scale, RIIUs turn a philosophical debate into an empirical mathematical
problem.

</details>


### [98] [LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning](https://arxiv.org/abs/2506.13841)
*Miho Koda, Yu Zheng, Ruixian Ma, Mingyang Sun, Devesh Pansare, Fabio Duarte, Paolo Santi*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个名为LocationReasoner的基准测试，旨在评估大型语言模型（LLMs）在现实世界选址场景中的推理能力。该基准测试包含超过300个不同难度级别的查询，并提供了一个带有约束条件搜索工具的沙箱环境。评估结果显示，最先进的推理模型相比其非推理前辈，在现实世界情境中仅有有限的改进，甚至最新的OpenAI o4模型也在30%的选址任务上失败。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（特别是通过强化后训练增强的模型）在数学问题解决和代码生成等领域的推理能力表现出色，但这些技能是否能够泛化到复杂的真实世界场景仍是一个开放性的问题。

**方法:** 创建了LocationReasoner基准测试，其中包含了多样化的、复杂的空间、环境和物流约束条件，以评价LLMs在真实世界选址过程中的推理能力。该基准测试包括了超过300个精心设计的问题，覆盖不同的难度等级，并且配备了一个具有内部开发工具支持的沙箱环境来辅助基于约束的位置搜索。

**结果:** 广泛评估显示，当前先进的推理模型与它们不具备推理能力的前代版本相比，在处理现实世界情境时仅提供了有限的改善；即便是最新型号的OpenAI o4也未能完成30%的选址任务。此外，诸如ReAct和Reflexion之类的代理策略往往由于过度推理而表现得比直接代码生成提示更差。

**结论:** 通过对LLMs在综合性和非线性推理方面的关键限制进行强调，研究者们发布了LocationReasoner基准测试，以促进能够在现实世界决策任务中进行稳健、有根据推理的LLM及代理的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LocationReasoner%3A+Evaluating+LLMs+on+Real-World+Site+Selection+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13841，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13841&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs), particularly those enhanced
through reinforced post-training, have demonstrated impressive reasoning
capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1.
However, these capabilities are predominantly benchmarked on domains like
mathematical problem solving and code generation -- leaving open the question
of whether such reasoning skills generalize to complex, real-world scenarios.
In this paper, we introduce LocationReasoner, a benchmark designed to evaluate
LLMs' reasoning abilities in the context of real-world site selection, where
models must identify feasible locations by reasoning over diverse and
complicated spatial, environmental, and logistical constraints. The benchmark
comprises over 300 carefully crafted queries of varying difficulty levels,
supported by a sandbox environment with in-house tools for constraint-based
location search. Extensive evaluations reveal that state-of-the-art reasoning
models offer limited improvement over their non-reasoning predecessors in
real-world contexts, with even the latest OpenAI o4 model failing on 30% of
site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion
often suffer from over-reasoning, leading to worse outcomes than direct
code-generation prompting. With key limitations of LLMs in holistic and
non-linear reasoning highlighted, we release LocationReasoner to foster the
development of LLMs and agents capable of robust, grounded reasoning in
real-world decision-making tasks. Codes and data for our benchmark are
available at https://github.com/miho-koda/LocationReasoner.

</details>


### [99] [Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features](https://arxiv.org/abs/2506.13917)
*Miguel A. Lago, Ghada Zamzmi, Brandon Eich, Jana G. Delfino*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种评估和报告可解释AI特性的框架，该框架基于四个标准：一致性、可信度、保真度和有用性，并通过一个记分卡来全面描述和评估这种类型的算法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的AI设备缺乏对其所提供解释质量的评估技术。为了填补这一空白，作者们提出了一套评估框架，旨在为AI的可解释性提供一套完整的描述和评价方法。

**方法:** 提出了一个包含四个评判标准（一致性、可信度、保真度和有用性）的AI可解释性评估框架，并开发了一个用于此类型算法的评分表。

**结果:** 文章介绍并举例说明了这四个标准如何进行评估，并以Ablation CAM和Eigen CAM作为案例研究，展示了在合成乳房X光片中乳腺病变检测时解释热图的评估。前三个标准针对临床相关场景进行了评估。

**结论:** 所提出的框架建立了一套标准，可以用来评估AI模型所提供的解释的质量，并希望引发关于可解释性特征价值的讨论，帮助改进基于AI的医疗设备的开发和评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Explainability%3A+A+Framework+for+Systematic+Assessment+and+Reporting+of+Explainable+AI+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13917，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13917&send_immediately=true&force_search=false)

**原文摘要:** Explainability features are intended to provide insight into the internal
mechanisms of an AI device, but there is a lack of evaluation techniques for
assessing the quality of provided explanations. We propose a framework to
assess and report explainable AI features. Our evaluation framework for AI
explainability is based on four criteria: 1) Consistency quantifies the
variability of explanations to similar inputs, 2) Plausibility estimates how
close the explanation is to the ground truth, 3) Fidelity assesses the
alignment between the explanation and the model internal mechanisms, and 4)
Usefulness evaluates the impact on task performance of the explanation.
Finally, we developed a scorecard for AI explainability methods that serves as
a complete description and evaluation to accompany this type of algorithm. We
describe these four criteria and give examples on how they can be evaluated. As
a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of
explanation heatmaps on the detection of breast lesions on synthetic
mammographies. The first three criteria are evaluated for clinically-relevant
scenarios. Our proposed framework establishes criteria through which the
quality of explanations provided by AI models can be evaluated. We intend for
our framework to spark a dialogue regarding the value provided by
explainability features and help improve the development and evaluation of
AI-based medical devices.

</details>


### [100] [Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction](https://arxiv.org/abs/2506.13920)
*Mbithe Nzomo, Deshendran Moodley*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于本体知识图谱和多模态电子健康记录数据构建贝叶斯网络的新方法，用于可解释的疾病风险预测。通过心房颤动的应用案例，展示了该方法能够在通用医学知识和个人化患者背景之间取得平衡，有效处理不确定性，并且具有高度可解释性和良好的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 多模态电子健康记录（EHR）数据对于基于医学领域知识的疾病风险预测是有用的。然而，为了实现实际临床应用，必须将一般医学知识调整适应特定的医疗环境和患者群体。此外，风险预测系统在保持可解释性的同时，还必须能够处理来自不完整数据和非确定性健康结果的不确定性。

**方法:** 本文介绍了一种从基于本体的知识图谱(KGs)和多模态EHR数据中构建贝叶斯网络(BNs)的新方法，旨在进行可解释的疾病风险预测。

**结果:** 通过一个心房颤动的实际应用案例，研究表明这种方法能够很好地平衡广义医学知识与具体患者情况之间的关系，有效地应对不确定性问题，而且具有很高的可解释性以及不错的预测表现。

**结论:** 提出的结合了知识图谱和贝叶斯网络的方法能有效地将通用医学知识与特定患者的情况相结合，同时提供了处理不确定性的能力，增强了模型的可解释性，并实现了良好的预测效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Knowledge+Graphs+and+Bayesian+Networks%3A+A+Hybrid+Approach+for+Explainable+Disease+Risk+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13920&send_immediately=true&force_search=false)

**原文摘要:** Multimodal electronic health record (EHR) data is useful for disease risk
prediction based on medical domain knowledge. However, general medical
knowledge must be adapted to specific healthcare settings and patient
populations to achieve practical clinical use. Additionally, risk prediction
systems must handle uncertainty from incomplete data and non-deterministic
health outcomes while remaining explainable. These challenges can be alleviated
by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We
present a novel approach for constructing BNs from ontology-based KGs and
multimodal EHR data for explainable disease risk prediction. Through an
application use case of atrial fibrillation and real-world EHR data, we
demonstrate that the approach balances generalised medical knowledge with
patient-specific context, effectively handles uncertainty, is highly
explainable, and achieves good predictive performance.

</details>


### [101] [ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users](https://arxiv.org/abs/2506.13980)
*Shahaf David, Yair Meidan, Ido Hersko, Daniel Varnovitzky, Dudu Mimran, Yuval Elovici, Asaf Shabtai*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的框架ProfiLLM，用于通过聊天机器人互动进行隐式和动态的用户画像。该框架包括一个可以适应不同领域的分类法以及基于大语言模型（LLM）的方法来根据分类法对用户进行画像。在IT/网络安全领域应用ProfiLLM[ITSec]版本，并通过1,760次人类类似的聊天机器人对话进行了评估，结果显示能够快速准确地推断出用户的IT/安全技术水平。


<details>
  <summary>更多</summary>
  
**动机:** 当前由大型语言模型驱动的聊天机器人难以根据个人用户特征定制回复，特别是在像IT/网络安全这样的专业领域中，用户的知识水平差异很大。现有的聊天机器人个性化方法主要依赖于静态的用户类别或明确的自我报告信息，这限制了它们随着用户交互过程中对其熟练程度认识的发展而进行调整的能力。

**方法:** 研究者们提出了ProfiLLM框架，它包含了一个可适用于多个领域的分类体系，以及一种基于大语言模型（LLM）的用户画像方法。特别地，在IT/网络安全领域，他们开发了ProfiLLM[ITSec]版本，并通过分析1,760个来自263位合成用户的类人聊天机器人会话来评估其性能。

**结果:** ProfiLLM[ITSec]能够在单个提示后迅速并准确地推测出用户的IT/安全档案，将实际得分与预测得分之间的差距减少了55-65%，随后只有轻微波动并进一步细化。此外，还提供了一个基于大语言模型的角色模拟方法、一个结构化的IT/安全熟练度分类体系、代码库及数据集以支持未来的研究。

**结论:** ProfiLLM框架有效地实现了基于聊天机器人交互的隐式和动态用户画像，尤其在IT/网络安全领域中展现出了很好的效果。这项工作为更智能和个性化的聊天机器人服务奠定了基础，并为未来的研究提供了宝贵的资源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ProfiLLM%3A+An+LLM-Based+Framework+for+Implicit+Profiling+of+Chatbot+Users，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13980，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13980&send_immediately=true&force_search=false)

**原文摘要:** Despite significant advancements in conversational AI, large language model
(LLM)-powered chatbots often struggle with personalizing their responses
according to individual user characteristics, such as technical expertise,
learning style, and communication preferences. This lack of personalization is
particularly problematic in specialized knowledge-intense domains like
IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing
approaches for chatbot personalization primarily rely on static user categories
or explicit self-reported information, limiting their adaptability to an
evolving perception of the user's proficiency, obtained in the course of
ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for
implicit and dynamic user profiling through chatbot interactions. This
framework consists of a taxonomy that can be adapted for use in diverse domains
and an LLM-based method for user profiling in terms of the taxonomy. To
demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where
troubleshooting interactions are used to infer chatbot users' technical
proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted
variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot
conversations from 263 synthetic users. Results show that ProfiLLM[ITSec]
rapidly and accurately infers ITSec profiles, reducing the gap between actual
and predicted scores by up to 55--65\% after a single prompt, followed by minor
fluctuations and further refinement. In addition to evaluating our new implicit
and dynamic profiling framework, we also propose an LLM-based persona
simulation methodology, a structured taxonomy for ITSec proficiency, our
codebase, and a dataset of chatbot interactions to support future research.

</details>


### [102] [SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine](https://arxiv.org/abs/2506.13983)
*Adarsh Gupta, Bhabesh Mali, Chandan Karfa*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为SANGAM的SystemVerilog断言生成框架，它利用大型语言模型指导的蒙特卡洛树搜索来自动生成行业级别的SVA。该框架分为三个阶段，并在评估过程中表现优于近期的方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型（LLMs）在推理领域的最新进展，为更复杂和自动化的硬件断言生成技术提供了新的可能性。

**方法:** 提出了一种使用LLM引导的蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）的SystemVerilog断言生成框架SANGAM。该框架包括三个阶段：第一阶段是多模态规范处理，第二阶段采用蒙特卡洛树自精炼（MCTSr）算法进行每个信号的SVA自动化推理，第三阶段结合MCTSr生成的推理轨迹来为每个信号生成SVA断言。

**结果:** 结果表明，SANGAM框架能够生成一套强大的SVA，在评估过程中与最近的方法相比表现更好。

**结论:** SANGAM是一种有效的SystemVerilog断言生成方法，它通过结合大型语言模型和蒙特卡洛树搜索技术，可以为工业级规格自动生成高质量的SVA。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SANGAM%3A+SystemVerilog+Assertion+Generation+via+Monte+Carlo+Tree+Self-Refine，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13983&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in the field of reasoning using Large Language Models
(LLMs) have created new possibilities for more complex and automatic Hardware
Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog
Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the
automatic generation of SVAs from industry-level specifications. The proposed
framework utilizes a three-stage approach: Stage 1 consists of multi-modal
Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform
Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine
(MCTSr) algorithm for automatic reasoning about SVAs for each signal, and
finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA
assertions for each signal. The results demonstrated that our framework,
SANGAM, can generate a robust set of SVAs, performing better in the evaluation
process in comparison to the recent methods.

</details>


### [103] [Machine Mirages: Defining the Undefined](https://arxiv.org/abs/2506.13990)
*Hamidou Tembine*

**主要类别:** cs.AI

**AI概要:** 随着多模态机器智能系统在处理图像、语言和声音的任务中达到了平均动物级和人类水平的流畅度，它们开始表现出一种新的认知偏差类别：机器幻象。文章列举了这些错误，并认为必须明确定义并系统地评估这些失败。理解机器幻象不仅对于提高机器智能的可靠性至关重要，而且对于构建一个尊重各种生命、认知和表达形式的多尺度伦理共进化智能生态系统也是必要的。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于识别和理解多模态机器智能系统中出现的一系列新的认知偏差，这些偏差被称为'机器幻象'。由于这些系统在执行任务时已经达到了一定的流利程度，它们也开始产生类似于但不完全复制人类或动物的错误类型。

**方法:** 本文采用的方法是通过列举和描述多种机器幻象现象，包括错觉、幻想、虚构等，并提出需要对这些新出现的错误进行明确的定义和系统的评估。

**结果:** 结果表明，机器幻象是一类新的认知偏差，它们模仿但并不复制人类或动物的易错性。作者强调了对这些错误进行定义和评估的重要性。

**结论:** 结论是理解机器幻象对于提高机器智能系统的可靠性和构建一个能够尊重多样化的生命形式、认知方式以及表达方法的智能生态系统至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Machine+Mirages%3A+Defining+the+Undefined，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13990&send_immediately=true&force_search=false)

**原文摘要:** As multimodal machine intelligence systems started achieving average
animal-level and average human-level fluency in many measurable tasks in
processing images, language, and sound, they began to exhibit a new class of
cognitive aberrations: machine mirages. These include delusion, illusion,
confabulation, hallucination, misattribution error, semantic drift, semantic
compression, exaggeration, causal inference failure, uncanny valley of
perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic
misunderstanding, hypersignification, semantic reheating-warming, simulated
authority effect, fallacious abductive leap, contextual drift, referential
hallucination, semiotic Frankenstein effect, calibration failure, spurious
correlation, bias amplification, concept drift sensitivity, misclassification
under uncertainty, adversarial vulnerability, overfitting, prosodic
misclassification, accent bias, turn boundary failure, semantic boundary
confusion, noise overfitting, latency-induced decision drift, ambiguity
collapse and other forms of error that mimic but do not replicate human or
animal fallibility. This article presents some of the errors and argues that
these failures must be explicitly defined and systematically assessed.
Understanding machine mirages is essential not only for improving machine
intelligence reliability but also for constructing a multiscale ethical,
co-evolving intelligence ecosystem that respects the diverse forms of life,
cognition, and expression it will inevitably touch.

</details>


### [104] [Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.14045)
*Martin Klissarov, Akhil Bagaria, Ziyan Luo, George Konidaris, Doina Precup, Marlos C. Machado*

**主要类别:** cs.AI

**AI概要:** 本文探讨了层次强化学习（HRL）在复杂开放环境中的潜力，以及它如何帮助解决决策制定的基本挑战。文章还讨论了不同方法发现时间结构的途径，并指出了该领域面临的挑战和适合应用的领域。


<details>
  <summary>更多</summary>
  
**动机:** 发展能够在复杂开放环境中探索、规划和学习的智能体是人工智能的一大挑战。层次强化学习提供了一种通过发现并利用经验流中的时间结构来应对这一挑战的方法。然而，目前还不清楚什么样的结构被认为是好的，或者在哪些问题中识别这种结构是有帮助的。

**方法:** 文章从决策制定的基本挑战的角度出发，试图确定HRL的好处，并强调其对AI代理性能权衡的影响。接着，概述了几类能够发现HRL中时间结构的方法，包括直接从在线经验学习到离线数据集，再到利用大型语言模型（LLMs）。

**结果:** 研究结果表明，层次强化学习对于处理长期依赖性和抽象化问题特别有效。此外，它还可以改善样本效率和泛化能力。

**结论:** 尽管存在挑战，但层次强化学习为提升智能体在复杂环境下的表现提供了重要机会。特别地，时间结构的发现对于某些特定领域的应用非常有价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discovering+Temporal+Structure%3A+An+Overview+of+Hierarchical+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14045，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14045&send_immediately=true&force_search=false)

**原文摘要:** Developing agents capable of exploring, planning and learning in complex
open-ended environments is a grand challenge in artificial intelligence (AI).
Hierarchical reinforcement learning (HRL) offers a promising solution to this
challenge by discovering and exploiting the temporal structure within a stream
of experience. The strong appeal of the HRL framework has led to a rich and
diverse body of literature attempting to discover a useful structure. However,
it is still not clear how one might define what constitutes good structure in
the first place, or the kind of problems in which identifying it may be
helpful. This work aims to identify the benefits of HRL from the perspective of
the fundamental challenges in decision-making, as well as highlight its impact
on the performance trade-offs of AI agents. Through these benefits, we then
cover the families of methods that discover temporal structure in HRL, ranging
from learning directly from online experience to offline datasets, to
leveraging large language models (LLMs). Finally, we highlight the challenges
of temporal structure discovery and the domains that are particularly
well-suited for such endeavours.

</details>


### [105] [Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places](https://arxiv.org/abs/2506.14070)
*Xinglei Wang, Tao Cheng, Stephen Law, Zichao Zeng, Ilya Ilyankou, Junyuan Liu, Lu Yin, Weiming Huang, Natchapon Jongwiriyanurak*

**主要类别:** cs.AI

**AI概要:** 论文介绍了一种名为CaLLiPer的多模态表示学习框架，用于个体移动预测中的位置嵌入。该方法结合了空间坐标和兴趣点的语义特征，并通过对比学习进行融合。实验结果表明，CaLLiPer在传统和归纳设定下均优于现有基准模型，特别是在处理新出现的位置时表现突出。


<details>
  <summary>更多</summary>
  
**动机:** 传统的个体移动预测方法主要依赖于从历史移动模式中学习到的位置嵌入，这限制了它们编码显式空间信息、整合丰富的城市语义上下文以及适应之前未见过的位置的能力。研究旨在解决这些问题，提高人类移动性预测系统的性能。

**方法:** 提出了一种称为CaLLiPer的多模态表示学习框架，它利用对比学习将空间坐标与兴趣点（POI）的语义特征相融合来生成位置嵌入。这种设计使得嵌入具有空间显性、语义丰富及归纳性。

**结果:** 在四个公开的人类移动数据集上进行了广泛的实验，在常规设置和归纳设置下都显示了CaLLiPer相对于强基线的一致优越性，尤其是在归纳场景中表现尤为出色。

**结论:** 研究表明，像CaLLiPer这样的多模态归纳位置嵌入有可能显著提升人类移动预测系统的能力。此外，为了促进可重复性和未来的研究，作者还发布了代码和数据。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Into+the+Unknown%3A+Applying+Inductive+Spatial-Semantic+Location+Embeddings+for+Predicting+Individuals%27+Mobility+Beyond+Visited+Places，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14070，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14070&send_immediately=true&force_search=false)

**原文摘要:** Predicting individuals' next locations is a core task in human mobility
modelling, with wide-ranging implications for urban planning, transportation,
public policy and personalised mobility services. Traditional approaches
largely depend on location embeddings learned from historical mobility
patterns, limiting their ability to encode explicit spatial information,
integrate rich urban semantic context, and accommodate previously unseen
locations. To address these challenges, we explore the application of CaLLiPer
-- a multimodal representation learning framework that fuses spatial
coordinates and semantic features of points of interest through contrastive
learning -- for location embedding in individual mobility prediction.
CaLLiPer's embeddings are spatially explicit, semantically enriched, and
inductive by design, enabling robust prediction performance even in scenarios
involving emerging locations. Through extensive experiments on four public
mobility datasets under both conventional and inductive settings, we
demonstrate that CaLLiPer consistently outperforms strong baselines,
particularly excelling in inductive scenarios. Our findings highlight the
potential of multimodal, inductive location embeddings to advance the
capabilities of human mobility prediction systems. We also release the code and
data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility
and future research.

</details>


### [106] [FormGym: Doing Paperwork with Agents](https://arxiv.org/abs/2506.14079)
*Matthew Toles, Rattandeep Singh, Isaac Song Zhou Yu*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个新的表单填写基准，发现基线视觉语言代理的表现不佳，而GUI代理虽然表现稍好但成本和延迟较高。为了解决这个问题，作者提出了FieldFinder工具来帮助大型语言模型定位表单上的文本放置位置，并在所有研究条件下都提高了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 完成文书工作是一个具有挑战性和耗时的问题，特别是在纯图像领域中，没有OCR、排版PDF文本或DOM的情况下进行表单填写尤其困难。计算机代理需要具备多模态理解、信息检索和工具使用等多重能力。

**方法:** 研究人员构建了一个包含55个文档和3项任务的新型表单填写基准，涉及432个字段和每位用户的236个特征。同时，他们开发了FieldFinder工具以辅助大型语言模型确定表单上文本的位置。

**结果:** 基线视觉语言代理（VLAs）在大多数情况下的准确率低于1%，主要因为它们的定位能力较差。GUI代理虽然表现稍好，准确率在10.6%到68.0%之间，但成本高且延迟大。通过使用FieldFinder，所有模型在六种研究条件下的表现都有所提高，最高增幅是从2%提升到了56%。

**结论:** 研究表明，现有的视觉语言代理在表单填写任务中表现欠佳，而新提出的FieldFinder工具能够显著改善这一问题，帮助各种模型更准确地完成表单填写。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FormGym%3A+Doing+Paperwork+with+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14079&send_immediately=true&force_search=false)

**原文摘要:** Completing paperwork is a challenging and time-consuming problem. Form
filling is especially challenging in the pure-image domain without access to
OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple
abilities, including multi-modal understanding, information retrieval, and
tool-use. We present a novel form-filling benchmark consisting of 432 fields
spread across 55 documents and 3 tasks, requiring knowledge of 236 features per
user. We find that baseline VLAs achieve less than 1% accuracy in most cases,
primarily due to poor localization ability. GUI agents also struggle, scoring
between 10.6-68.0% despite high cost and latency. Therefore, we also contribute
FieldFinder, a tool to assist LLMs in identifying where to place text on a
form. With FieldFinder, all models achieve equal or better performance in all
six study conditions, with a maximum increase from 2% to 56%.

</details>


### [107] [Lightweight Relevance Grader in RAG](https://arxiv.org/abs/2506.14084)
*Taehee Jeong*

**主要类别:** cs.AI

**AI概要:** 该论文介绍了在检索增强生成（RAG）中使用轻量级相关性评分器来提高检索文档的相关性，通过微调llama-3.2-1b模型作为相关性评分器，实现了精确度的显著提升。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服大型语言模型信息准确性和时效性的局限，RAG架构利用向量数据库来提供更精准和更新的信息。然而，确保检索到的文档与查询高度相关是一个巨大的挑战。因此，引入了相关性评分器以验证文档的相关性，并且为了降低计算需求，选择使用轻量级的小型语言模型。

**方法:** 研究者们采用了微调的方法，将llama-3.2-1b调整为一个相关性评分器，用来判断RAG过程中检索到的文档是否与用户的查询相关。

**结果:** 经过微调后，llama-3.2-1b作为相关性评分器的表现有了显著提升，其精确度从0.1301提高到了0.7750，这一结果与更大规模的llama-3.1-70b模型相当。

**结论:** 通过使用轻量级的相关性评分器，可以有效提高RAG架构下检索文档的相关性，同时保持较低的计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lightweight+Relevance+Grader+in+RAG，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14084，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14084&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) addresses limitations of large language
models (LLMs) by leveraging a vector database to provide more accurate and
up-to-date information. When a user submits a query, RAG executes a vector
search to find relevant documents, which are then used to generate a response.
However, ensuring the relevance of retrieved documents with a query would be a
big challenge. To address this, a secondary model, known as a relevant grader,
can be served to verify its relevance. To reduce computational requirements of
a relevant grader, a lightweight small language model is preferred. In this
work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant
increase in precision from 0.1301 to 0.7750. Its precision is comparable to
that of llama-3.1-70b. Our code is available at
https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.

</details>


### [108] [Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models](https://arxiv.org/abs/2506.14092)
*Haonan Yin, Shai Vardi, Vidyanand Choudhary*

**主要类别:** cs.AI

**AI概要:** 研究发现大型语言模型（LLMs）在决策支持系统中表现出显著且一致的位置偏见，包括一种新的中心性偏见。这些位置偏见甚至比性别偏见更强烈，并且可能导致模型选择严格劣质的选项。为了解决这些问题，提出了包括温度参数的新用途在内的针对性缓解策略。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型越来越多地应用于高风险领域如招聘和大学录取等决策支持系统，其中决策通常涉及在竞争选项间进行选择，而先前的研究已经注意到LLM驱动比较中的位置顺序偏见，但这些偏见尚未被系统地剖析或与潜在偏好结构联系起来。

**方法:** 对多种LLM架构和领域的顺序偏见进行了首次全面调查，揭示了强烈的、一致的顺序效应，并引入了一个框架来将成对偏好分类为稳健的、脆弱的或无所谓的。

**结果:** 发现了包括一种新记录的中心性偏见在内的强且一致的顺序效应，以及当选项质量高时表现为优先偏见，而在选项质量低时则倾向于后者的选择模式。此外还识别出了一种未被记录的对某些名字的偏爱。

**结论:** 结果表明LLM不仅继承了类似人类的偏见，而且表现出不同于人类决策的独特失败模式。为了减少由顺序引起的影响，研究者们提出了一系列针对性的缓解策略，其中包括温度参数的一种新颖用法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fragile+Preferences%3A+A+Deep+Dive+Into+Order+Effects+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14092，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14092&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly used in decision-support
systems across high-stakes domains such as hiring and university admissions,
where decisions often involve selecting among competing alternatives. While
prior work has noted positional order biases in LLM-driven comparisons, these
biases have not been systematically dissected or linked to underlying
preference structures. We provide the first comprehensive investigation of
positional biases across multiple LLM architectures and domains, uncovering
strong and consistent order effects, including a novel centrality bias not
previously documented in human or machine decision-making. We also find a
quality-dependent shift: when options are high quality, models exhibit primacy
bias, but favor latter options when option quality is low. We further identify
a previously undocumented bias favoring certain names over others. To
distinguish superficial tie-breaking from true distortions of judgment, we
introduce a framework that classifies pairwise preferences as robust, fragile,
or indifferent. We show that order effects can lead models to select strictly
inferior options, and that positional biases are typically stronger than gender
biases. These findings suggest that LLMs are not merely inheriting human-like
biases, but exhibit distinct failure modes not seen in human decision-making.
We propose targeted mitigation strategies, including a novel use of the
temperature parameter, to reduce order-driven distortions.

</details>


### [109] [Situational-Constrained Sequential Resources Allocation via Reinforcement Learning](https://arxiv.org/abs/2506.14125)
*Libo Zhang, Yang Chen, Toru Takisaka, Kaiqi Zhao, Weidong Li, Jiamou Liu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的框架SCRL，以处理具有情境约束的序列资源分配问题，并在两个场景中展示了其优越性。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界的应用中，资源需求和优先级是依赖于上下文的，因此带有情境约束的序列资源分配是一个显著挑战。

**方法:** 文章将情境约束形式化为逻辑蕴含，并开发了一种新算法来动态惩罚违反约束的行为。为了有效处理情境约束，提出了一个概率选择机制来克服传统约束强化学习(CRL)方法的局限性。

**结果:** 实验表明，在满足约束的同时保持高资源效率方面，SCRL的表现优于现有的基准方法。

**结论:** SCRL框架在医疗资源分配和农业农药分发两个场景中的表现证明了它在实际、敏感于上下文的决策任务中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Situational-Constrained+Sequential+Resources+Allocation+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14125&send_immediately=true&force_search=false)

**原文摘要:** Sequential Resource Allocation with situational constraints presents a
significant challenge in real-world applications, where resource demands and
priorities are context-dependent. This paper introduces a novel framework,
SCRL, to address this problem. We formalize situational constraints as logic
implications and develop a new algorithm that dynamically penalizes constraint
violations. To handle situational constraints effectively, we propose a
probabilistic selection mechanism to overcome limitations of traditional
constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two
scenarios: medical resource allocation during a pandemic and pesticide
distribution in agriculture. Experiments demonstrate that SCRL outperforms
existing baselines in satisfying constraints while maintaining high resource
efficiency, showcasing its potential for real-world, context-sensitive
decision-making tasks.

</details>


### [110] [Collaborative Editable Model](https://arxiv.org/abs/2506.14146)
*Kaiwen Tang, Aitong Wu, Yao Lu, Guangda Sun*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为CoEM的模型，通过用户贡献的知识片段、互动对话和用户评价来实现轻量级领域适应，从而在不需要大量标注数据和计算资源的情况下改进了垂直领域大语言模型的表现。


<details>
  <summary>更多</summary>
  
**动机:** 垂直领域的大语言模型在诸如金融、医疗保健和法律等专门场景中起着关键作用，但其训练通常依赖于大规模标注数据和大量计算资源，这阻碍了快速开发和持续迭代。

**方法:** 引入了协作可编辑模型（CoEM），该模型从用户贡献的领域片段构建候选知识库，结合交互式用户-模型对话以及用户评级和归属分析以识别高价值知识片段，并通过上下文提示注入这些片段以实现轻量级领域适应。

**结果:** 在一个金融信息场景中，收集了约120名用户的15,000条反馈，并使用用户评级验证了CoEM，显示了在特定领域生成方面有显著改进，同时避免了传统微调工作流程的时间和计算开销。

**结论:** 通过利用用户参与的反馈和贡献，CoEM能够提高垂直领域内大语言模型输出的质量和准确性，同时减少了对大量标注数据及计算资源的需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Collaborative+Editable+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14146&send_immediately=true&force_search=false)

**原文摘要:** Vertical-domain large language models (LLMs) play a crucial role in
specialized scenarios such as finance, healthcare, and law; however, their
training often relies on large-scale annotated data and substantial
computational resources, impeding rapid development and continuous iteration.
To address these challenges, we introduce the Collaborative Editable Model
(CoEM), which constructs a candidate knowledge pool from user-contributed
domain snippets, leverages interactive user-model dialogues combined with user
ratings and attribution analysis to pinpoint high-value knowledge fragments,
and injects these fragments via in-context prompts for lightweight domain
adaptation. With high-value knowledge, the LLM can generate more accurate and
domain-specific content. In a financial information scenario, we collect 15k
feedback from about 120 users and validate CoEM with user ratings to assess the
quality of generated insights, demonstrating significant improvements in
domain-specific generation while avoiding the time and compute overhead of
traditional fine-tuning workflows.

</details>


### [111] [What's in the Box? Reasoning about Unseen Objects from Multimodal Cues](https://arxiv.org/abs/2506.14212)
*Lance Ying, Daniel Xu, Alicia Zhang, Katherine M. Collins, Max H. Siegel, Joshua B. Tenenbaum*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种神经符号模型，该模型能够解析开放式的多模态输入，并通过贝叶斯模型整合不同信息源来评估不同的假设。通过一种名为'盒子里有什么？'的新颖物体猜测游戏对模型进行评估，结果表明该模型与人类判断高度相关，而单模态删减模型和大型多模态神经模型基线则表现不佳。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探讨人们如何灵活地整合来自多种来源的信息（如听觉和视觉线索、语言以及我们对场景的先验信念和知识），以理解周围的世界，即使对于没有直接了解的事物也能够做出推断。

**方法:** 提出了一种神经符号模型，它使用神经网络处理开放式多模态输入，并采用贝叶斯模型来整合不同信息源以评估各种假设。此外，还设计了一个新的物体猜测游戏'盒子里有什么？'来测试模型的表现。

**结果:** 实验显示所提出的模型与人类判断有很强的相关性，相比之下，单模态删减模型和大型多模态神经模型基线的相关性较差。

**结论:** 本研究表明，结合了神经网络和贝叶斯模型的神经符号方法能够在多模态推理任务中有效模拟人类的决策过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What%27s+in+the+Box%3F+Reasoning+about+Unseen+Objects+from+Multimodal+Cues，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14212，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14212&send_immediately=true&force_search=false)

**原文摘要:** People regularly make inferences about objects in the world that they cannot
see by flexibly integrating information from multiple sources: auditory and
visual cues, language, and our prior beliefs and knowledge about the scene. How
are we able to so flexibly integrate many sources of information to make sense
of the world around us, even if we have no direct knowledge? In this work, we
propose a neurosymbolic model that uses neural networks to parse open-ended
multimodal inputs and then applies a Bayesian model to integrate different
sources of information to evaluate different hypotheses. We evaluate our model
with a novel object guessing game called ``What's in the Box?'' where humans
and models watch a video clip of an experimenter shaking boxes and then try to
guess the objects inside the boxes. Through a human experiment, we show that
our model correlates strongly with human judgments, whereas unimodal ablated
models and large multimodal neural model baselines show poor correlation.

</details>


### [112] [From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/abs/2506.14224)
*Xinyang Li, Siqi Liu, Bochao Zou, Jiansheng Chen, Huimin Ma*

**主要类别:** cs.AI

**AI概要:** 本研究通过内部机制的方法对多模态大语言模型(MLLMs)的思维理论(ToM)进行了可解释性评估, 构建了GridToM数据集, 并展示注意力头能够区分不同视角的认知信息。此外, 提出了一种轻量级且无需训练的方法, 通过对注意力头方向进行调整, 显著提高了模型展现的ToM能力。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型的发展, 人们越来越期待它们能够模仿类似人类的思维理论(ToM)来协助完成日常任务。然而, 现有的机器ToM评估方法主要集中在单模态模型上, 并且通常将这些模型视为黑箱处理, 缺乏对其内部机制的解释性探索。

**方法:** 本研究采用基于内部机制的方法, 对多模态大语言模型(MLLMs)的ToM进行了可解释性的评估。为此, 首先构建了一个名为GridToM的多模态ToM测试数据集, 其中包含了多样化的信念测试任务和来自多个角度的感知信息。接着, 分析表明在多模态大模型中的注意力头可以跨视角区分认知信息, 这为ToM能力提供了证据。此外, 介绍了一种轻量级且无需训练的方法, 该方法通过对注意力头的方向进行调整, 大大增强了模型所表现出的ToM。

**结果:** 研究表明, 在多模态大语言模型中, 注意力头确实能够在不同视角间区分认知信息, 表明这些模型具备一定程度的ToM能力。同时, 提出的轻量级方法成功地显著提升了模型的ToM表现, 无需额外训练即能实现性能增强。

**结论:** 通过深入探究多模态大语言模型的内部运作方式, 本研究不仅揭示了其潜在的ToM能力, 而且开发了一种简单有效的策略来进一步提升这种能力, 为未来研究如何更好地理解和改进AI系统的人类思维模拟奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Black+Boxes+to+Transparent+Minds%3A+Evaluating+and+Enhancing+the+Theory+of+Mind+in+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14224，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14224&send_immediately=true&force_search=false)

**原文摘要:** As large language models evolve, there is growing anticipation that they will
emulate human-like Theory of Mind (ToM) to assist with routine tasks. However,
existing methods for evaluating machine ToM focus primarily on unimodal models
and largely treat these models as black boxes, lacking an interpretative
exploration of their internal mechanisms. In response, this study adopts an
approach based on internal mechanisms to provide an interpretability-driven
assessment of ToM in multimodal large language models (MLLMs). Specifically, we
first construct a multimodal ToM test dataset, GridToM, which incorporates
diverse belief testing tasks and perceptual information from multiple
perspectives. Next, our analysis shows that attention heads in multimodal large
models can distinguish cognitive information across perspectives, providing
evidence of ToM capabilities. Furthermore, we present a lightweight,
training-free approach that significantly enhances the model's exhibited ToM by
adjusting in the direction of the attention head.

</details>


### [113] [ImpReSS: Implicit Recommender System for Support Conversations](https://arxiv.org/abs/2506.14231)
*Omri Haller, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为ImpReSS的隐式推荐系统，它专为客服对话设计，能够在用户报告问题时推荐相关解决方案产品类别（SPCs），以解决问题或防止问题再次发生。该系统在无需假设用户购买意图的情况下运行，并且在多个场景下表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 虽然基于大型语言模型的对话推荐系统已经引起了关注，但很少有研究涉及到将推荐功能隐式地整合到客户服务互动中。为了提升客户支持的质量和促进业务增长，本研究旨在开发一种能够识别并推荐有助于解决或预防问题发生的解决方案产品类别的隐式推荐系统。

**方法:** 研究人员开发了ImpReSS，一个专门为客服对话设计的隐式推荐系统。该系统与现有的支持聊天机器人协同工作，在用户报告问题时提供解决方案，并推荐相关的解决方案产品类别。ImpReSS并不依赖于任何关于用户购买意图的假设。

**结果:** 实证评估表明，ImpReSS在推荐相关SPCs方面表现良好，包括一般问题解决、信息安全支持和网络安全故障排除等场景下的MRR@1 (recall@3)分别为0.72 (0.89)、0.82 (0.83) 和 0.85 (0.67)。

**结论:** ImpReSS作为一个隐式推荐系统，被证明可以在多种客户服务场景下有效推荐解决方案产品类别，从而提高服务质量和促进业务发展。此外，为了支持未来的研究，作者愿意根据请求分享他们的数据和代码。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ImpReSS%3A+Implicit+Recommender+System+for+Support+Conversations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14231，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14231&send_immediately=true&force_search=false)

**原文摘要:** Following recent advancements in large language models (LLMs), LLM-based
chatbots have transformed customer support by automating interactions and
providing consistent, scalable service. While LLM-based conversational
recommender systems (CRSs) have attracted attention for their ability to
enhance the quality of recommendations, limited research has addressed the
implicit integration of recommendations within customer support interactions.
In this work, we introduce ImpReSS, an implicit recommender system designed for
customer support conversations. ImpReSS operates alongside existing support
chatbots, where users report issues and chatbots provide solutions. Based on a
customer support conversation, ImpReSS identifies opportunities to recommend
relevant solution product categories (SPCs) that help resolve the issue or
prevent its recurrence -- thereby also supporting business growth. Unlike
traditional CRSs, ImpReSS functions entirely implicitly and does not rely on
any assumption of a user's purchasing intent. Our empirical evaluation of
ImpReSS's ability to recommend relevant SPCs that can help address issues
raised in support conversations shows promising results, including an MRR@1
(and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for
information security support, and 0.85 (0.67) for cybersecurity
troubleshooting. To support future research, our data and code will be shared
upon request.

</details>


### [114] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/abs/2506.14239)
*Louis Vervoort, Vitaly Nikolaev*

**主要类别:** cs.AI

**AI概要:** 本文基于因果哲学中的神经元图提出了一种针对AI的抽象因果推理测试，并展示了先进的大语言模型在这一测试中的表现，同时提出了一个比现有定义更通用的因果定义。


<details>
  <summary>更多</summary>
  
**动机:** 作者希望通过开发一种新的测试方法来评估AI系统在因果推理方面的能力，尤其是使用哲学中流行的神经元图作为基础。此外，他们还试图为神经元图提供一个更广泛的因果定义，挑战当前认为这种定义难以捉摸的观点。

**方法:** 研究者设计了一个基于神经元图的测试，并用它来评估几个高级的大语言模型（如ChatGPT、DeepSeek和Gemini）在因果识别上的能力。为了支持这个测试，他们也提出了一个新的因果定义。

**结果:** 结果显示，被测的大语言模型能够正确地识别出那些在文献中争议很大的因果关系。新提出的因果定义也为评价这些模型以及未来专门的人工智能提供了依据。

**结论:** 该研究表明，现代聊天机器人已经具备了相当程度的因果推理能力，并且未来的哲学研究可能会发展成人与人工智能专业知识之间的一种互动。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Causes+in+neuron+diagrams%2C+and+testing+causal+reasoning+in+Large+Language+Models.+A+glimpse+of+the+future+of+philosophy%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14239，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14239&send_immediately=true&force_search=false)

**原文摘要:** We propose a test for abstract causal reasoning in AI, based on scholarship
in the philosophy of causation, in particular on the neuron diagrams
popularized by D. Lewis. We illustrate the test on advanced Large Language
Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already
capable of correctly identifying causes in cases that are hotly debated in the
literature. In order to assess the results of these LLMs and future dedicated
AI, we propose a definition of cause in neuron diagrams with a wider validity
than published hitherto, which challenges the widespread view that such a
definition is elusive. We submit that these results are an illustration of how
future philosophical research might evolve: as an interplay between human and
artificial expertise.

</details>


### [115] [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
*Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang*

**主要类别:** cs.AI

**AI概要:** 本文解决了RLVR调整模型在$Pass@K$度量上表现不佳的矛盾，提出了一种新的评价指标$CoT$-$Pass@K$，并证实了RLVR能够激励正确推理的一般化。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们发现强化学习与可验证奖励（RLVR）虽然被看作是提升大型语言模型（LLMs）推理能力的一个有前景的方法，但RLVR调优后的模型经常在解决方案寻找的$Pass@K$度量上不如基础模型，这导致了一个假设，即RLVR只是重新加权现有的推理路径而牺牲了推理多样性。

**方法:** 作者通过指出$Pass@K$度量本身是一个有缺陷的推理衡量标准，并引入一个更精确的评估指标$CoT$-$Pass@K$来解决这个问题，该指标要求推理路径和最终答案都必须正确。此外，他们提供了一个新的理论基础，形式化地说明了RLVR如何独特地构建以激励逻辑完整性。

**结果:** 实证结果支持了这种方法：使用$CoT$-$Pass@K$，观察到对于所有$K$值，RLVR都能够激励正确的推理泛化。同时，通过分析训练动态，发现在训练过程早期就出现了这种增强的推理能力，并且能够平稳地泛化。

**结论:** 这项工作为RLVR的角色提供了清晰的观点，提出了一个更为可靠的评估方法，并确认了它真正推进机器推理的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+with+Verifiable+Rewards+Implicitly+Incentivizes+Correct+Reasoning+in+Base+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14245，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14245&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
promising paradigm for advancing the reasoning capabilities of Large Language
Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned
models often underperform their base models on the $Pass@K$ metric for
solution-finding, leading to the hypothesis that RLVR merely re-weights
existing reasoning paths at the cost of reasoning diversity. In this work, we
resolve this contradiction by identifying the source of the problem: the
$Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct
final answers that probably arise from inaccurate or incomplete chains of
thought (CoTs). To address this, we introduce a more precise evaluation metric,
$CoT$-$Pass@K$, which mandates that both the reasoning path and the final
answer be correct. We provide a new theoretical foundation that formalizes how
RLVR, unlike traditional RL, is uniquely structured to incentivize logical
integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we
observe that RLVR can incentivize the generalization of correct reasoning for
all values of $K$. Furthermore, by analyzing the training dynamics, we find
that this enhanced reasoning capability emerges early in the training process
and smoothly generalizes. Our work provides a clear perspective on the role of
RLVR, offers a more reliable method for its evaluation, and confirms its
potential to genuinely advance machine reasoning.

</details>


### [116] [Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents](https://arxiv.org/abs/2506.14246)
*Lingfeng Li, Yunlong Lu, Yongyi Wang, Qifan Zheng, Wenxin Li*

**主要类别:** cs.AI

**AI概要:** 本文介绍了Mxplainer，一种参数化搜索算法，可以转换为等效的神经网络来学习黑盒代理的参数。通过实验表明，所学参数提供了对这些代理特征和游戏风格的人类可理解的见解，并且该框架能够局部解释大多数麻将游戏状态下黑盒代理的决策过程。


<details>
  <summary>更多</summary>
  
**动机:** 尽管AI研究人员已经开发出了一些表现与专业人类玩家相当的麻将AI代理，但这些代理通常被视为难以从中获得洞见的黑盒子。为了帮助人们内化AI代理的技能以提高自身能力，需要有一种方法来理解和解析这些黑盒代理的行为。

**方法:** 论文提出了一种名为Mxplainer的参数化搜索算法，它可以被转化为一个等效的神经网络，用于学习黑盒代理的参数。此外，还展示了如何使用基于搜索的框架来局部地解释黑盒代理在多数麻将游戏状态下的决策过程。

**结果:** 实验结果表明，通过Mxplainer学习到的参数提供了对AI代理特性及玩法风格的人类可理解的洞察。同时，基于搜索的框架也能够有效地局部解释黑盒代理的决策过程。

**结论:** Mxplainer提供了一种有效的方法来揭示麻将AI代理内部运作的细节，从而让人类可以从中学到有用的策略和技巧。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mxplainer%3A+Explain+and+Learn+Insights+by+Imitating+Mahjong+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14246&send_immediately=true&force_search=false)

**原文摘要:** People need to internalize the skills of AI agents to improve their own
capabilities. Our paper focuses on Mahjong, a multiplayer game involving
imperfect information and requiring effective long-term decision-making amidst
randomness and hidden information. Through the efforts of AI researchers,
several impressive Mahjong AI agents have already achieved performance levels
comparable to those of professional human players; however, these agents are
often treated as black boxes from which few insights can be gleaned. This paper
introduces Mxplainer, a parameterized search algorithm that can be converted
into an equivalent neural network to learn the parameters of black-box agents.
Experiments conducted on AI and human player data demonstrate that the learned
parameters provide human-understandable insights into these agents'
characteristics and play styles. In addition to analyzing the learned
parameters, we also showcase how our search-based framework can locally explain
the decision-making processes of black-box agents for most Mahjong game states.

</details>


### [117] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/abs/2506.14276)
*Jack Cole, Mohamed Osman*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种基于深度学习的方法来解决Abstraction and Reasoning Corpus (ARC-AGI)挑战，通过在测试时进行神经网络的即时训练、测试时微调（TTFT）以及增强推理反向增强和投票（AIRV）等技术，显著提升了在ARC上的性能，并且这种方法在2023年ARCathon竞赛中获得第一名。


<details>
  <summary>更多</summary>
  
**动机:** 尽管深度学习范式在处理ARC时通常表现不佳，但它仍然是生成跨视觉、语言等多种模态和任务的技能型神经网络最有效的已知策略。作者希望通过充分利用深度学习获取新抽象的能力来提高AI系统在ARC上的表现。

**方法:** 文章介绍了一种从预训练的语言模型（LLMs）开始并在ARC上进行训练以提升其推理能力的方法论。此外，提出了测试时微调（Test-Time Fine-Tuning, TTFT）和增强推理反向增强与投票（Augment Inference Reverse-Augmentation and Vote, AIRV）作为有效的测试时技术。

**结果:** 研究结果表明，所提出的方法能够将准确率提高到原来的260%，而使用TTFT后进一步提高了300%。该方法的一个早期版本在2023年ARCathon比赛中获得了第一名，最终版本则达到了ARC私人测试集当前最好的成绩58%。

**结论:** 研究强调了构建一个在未知领域中的健壮推理系统所需的关键要素，并突出了改进广泛感知推理的核心机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Don%27t+throw+the+baby+out+with+the+bathwater%3A+How+and+why+deep+learning+for+ARC，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14276&send_immediately=true&force_search=false)

**原文摘要:** The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable
challenge for AI systems. Despite the typically low performance on ARC, the
deep learning paradigm remains the most effective known strategy for generating
skillful (state-of-the-art) neural networks (NN) across varied modalities and
tasks in vision, language etc. The deep learning paradigm has proven to be able
to train these skillful neural networks and learn the abstractions needed in
these diverse domains. Our work doubles down on that and continues to leverage
this paradigm by incorporating on-the-fly NN training at test time. We
demonstrate that fully committing to deep learning's capacity to acquire novel
abstractions yields state-of-the-art performance on ARC. Specifically, we treat
both the neural network and the optimizer (rather than just a pre-trained
network) as integral components of the inference process, fostering
generalization to unseen tasks. Concretely, we propose a methodology for
training on ARC, starting from pretrained LLMs, and enhancing their ARC
reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment
Inference Reverse-Augmentation and Vote (AIRV) as effective test-time
techniques. We are the first to propose and show deep learning can be used
effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a
further 300% boost with TTFT. An early version of this approach secured first
place in the 2023 ARCathon competition, while the final version achieved the
current best score on the ARC private test-set (58%). Our findings highlight
the key ingredients of a robust reasoning system in unfamiliar domains,
underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


### [118] [ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems](https://arxiv.org/abs/2506.14299)
*Fanzhi Zeng, Siqi Wang, Chuzhao Zhu, Li Li*

**主要类别:** cs.AI

**AI概要:** 本研究提出了ADRD框架，它利用大型语言模型生成基于规则的决策系统以解决自动驾驶决策问题。该框架通过信息模块、代理模块和测试模块实现对驾驶场景的理解，并生成可执行的驾驶策略。实验表明，与传统强化学习方法相比，ADRD在解释性、响应速度和驾驶表现上具有明显优势。


<details>
  <summary>更多</summary>
  
**动机:** 构建一个可解释的自动驾驶决策系统已经成为学术研究的重点。为了应对这一挑战，本文提出了一种新方法，使用大型语言模型来生成可执行的、基于规则的决策系统。

**方法:** 引入了ADRD（基于大型语言模型驱动的基于规则决策系统的自动驾驶）框架，该框架包括三个核心模块：信息模块、代理模块和测试模块。框架首先通过信息模块汇总上下文驾驶情景信息，然后利用代理模块生成基于规则的驾驶策略，这些策略通过与测试模块的持续互动得到逐步优化。

**结果:** 广泛的实验评估表明，ADRD在自动驾驶决策任务中表现出优越的性能。相较于传统的强化学习方法以及最先进的基于大型语言模型的方法，ADRD在可解释性、响应速度和驾驶性能方面显示出显著的优势。

**结论:** 结果突显了该框架能够实现对复杂驾驶情景全面而准确的理解，并强调了透明且易于修改的基于规则的决策系统在未来广泛应用中的前景。据我们所知，这是首次将大型语言模型与基于规则的系统相结合用于自动驾驶决策的研究工作，我们的发现证实了其在现实世界部署中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ADRD%3A+LLM-Driven+Autonomous+Driving+Based+on+Rule-based+Decision+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14299，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14299&send_immediately=true&force_search=false)

**原文摘要:** How to construct an interpretable autonomous driving decision-making system
has become a focal point in academic research. In this study, we propose a
novel approach that leverages large language models (LLMs) to generate
executable, rule-based decision systems to address this challenge.
Specifically, harnessing the strong reasoning and programming capabilities of
LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based
Decision Systems) framework, which integrates three core modules: the
Information Module, the Agents Module, and the Testing Module. The framework
operates by first aggregating contextual driving scenario information through
the Information Module, then utilizing the Agents Module to generate rule-based
driving tactics. These tactics are iteratively refined through continuous
interaction with the Testing Module. Extensive experimental evaluations
demonstrate that ADRD exhibits superior performance in autonomous driving
decision tasks. Compared to traditional reinforcement learning approaches and
the most advanced LLM-based methods, ADRD shows significant advantages in terms
of interpretability, response speed, and driving performance. These results
highlight the framework's ability to achieve comprehensive and accurate
understanding of complex driving scenarios, and underscore the promising future
of transparent, rule-based decision systems that are easily modifiable and
broadly applicable. To the best of our knowledge, this is the first work that
integrates large language models with rule-based systems for autonomous driving
decision-making, and our findings validate its potential for real-world
deployment.

</details>


### [119] [AviationLLM: An LLM-based Knowledge System for Aviation Training](https://arxiv.org/abs/2506.14336)
*Jia'ang Wan, Feng Shen, Fujuan Li, Yanjin Sun, Yan Li, Shiwen Zhang*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种通过直接偏好优化进行检索增强的语言模型对齐方法（RALA-DPO），用于提高航空理论培训中专业问题回答的准确性和质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的航空培训系统依赖有限数量的讲师传授知识，且互联网上的专业答案不够准确，导致培训效率低下。基础预训练语言模型无法提供专业领域的准确答案，而传统的监督微调方法因数据覆盖不足可能产生事实错误的回答。

**方法:** 研究者使用开源预训练语言模型Qwen，并通过基于直接偏好优化(DPO)的方法使其适应航空理论培训领域。同时，为了解决由于训练数据偏差、知识过时或领域知识差距导致的幻觉问题，采用了检索-生成混合(RAG)技术。

**结果:** 实验结果表明，RALA-DPO能够有效提升针对专业航空知识回复的准确性。结合RAG机制后，该系统还能进一步提高答案准确性，并实现零成本的知识更新。

**结论:** RALA-DPO通过结合DPO和RAG技术，提高了航空理论培训中专业问答的准确度和质量，同时也实现了知识库的实时更新。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AviationLLM%3A+An+LLM-based+Knowledge+System+for+Aviation+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14336，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14336&send_immediately=true&force_search=false)

**原文摘要:** Aviation training is a core link in ensuring flight safety, improving
industry efficiency and promoting sustainable development. It not only involves
flight simulation but also requires the learning of a great deal of
professional aviation theory knowledge. In the existing training system, the
knowledge is mainly imparted by the the instructors. However, the number of
instructors is limited and the professional answers obtained from the Internet
are not accurate enough, resulting in low training efficiency. To address this,
we introduced LLM, but the basic pre-trained model cannot provide accurate
answers to professional fields, so we fine-tuned it. Traditional Supervised
Fine-Tuning (SFT) risk generating superficially plausible but factually
incorrect responses due to insufficient data coverage. To address this, we
employ Direct Preference Optimization(DPO). This paper proposes
Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO).
We select open source pre-trained LLM Qwen and adapt it to aviation theory
training through DPO-based domain alignment. Simultaneously, to mitigate
hallucinations caused by training data biases, knowledge obsolescence, or
domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG)
technology that combines generative and retrieval models. RALA-DPO effectively
retrieves relevant information from external knowledge bases and delivers
precise and high-quality responses through the generative model. Experimental
results demonstrate that RALA-DPO can improve accuracy in response to
professional aviation knowledge. With integrated RAG mechanisms, this system
can further improve the accuracy of answers and achieve zero-cost knowledge
updates simultaneously.

</details>


### [120] [Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning](https://arxiv.org/abs/2506.14387)
*William F. Shen, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的微调方法SEAT，旨在解决大型语言模型在传统微调过程中丧失表达无知能力的问题。通过稀疏训练和实体扰动结合KL散度正则化，该方法能够有效保持模型的无知意识同时维持微调性能。


<details>
  <summary>更多</summary>
  
**动机:** 目前对于缓解大语言模型微调时出现灾难性遗忘的研究主要集中在保留特定数据或任务上，而忽略了安全对齐过程中灌输的基本能力的退化，特别是模型诚实地表示无知的能力。

**方法:** 提出了一种名为SEAT的微调方法，该方法包括两个关键组成部分：（1）限制激活漂移的稀疏训练；（2）一种新颖的实体扰动方法，并且采用了KL散度正则化来对抗知识纠缠。

**结果:** 实验结果显示，SEAT在保持无知意识方面显著优于基线方法，同时还能保持良好的微调性能。

**结论:** SEAT提供了一个更加鲁棒的大语言模型微调解决方案，它能够在维持微调效果的同时，有效地保存模型承认其知识局限性的内在能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Don%27t+Make+It+Up%3A+Preserving+Ignorance+Awareness+in+LLM+Fine-Tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14387&send_immediately=true&force_search=false)

**原文摘要:** Existing work on mitigating catastrophic forgetting in large language model
(LLM) fine-tuning has primarily focused on preserving specific data or tasks,
while critically overlooking the degradation of essential capabilities
instilled through safety alignment, particularly the model's ability to
faithfully express ignorance. In this work, we show that this capability is
significantly degraded during conventional fine-tuning, leading to undesired
behaviors such as hallucinations. To address this novel but highly practical
problem, we propose SEAT, a simple and effective fine-tuning approach that
preserves both fine-tuning performance and the model's inherent ability to
acknowledge its ignorance. SEAT integrates two key components: (1) sparse
training that constrains activation drift, and (2) a novel entity perturbation
method with KL-divergence regularization, designed to counter knowledge
entanglement. Experimental results demonstrate that SEAT significantly
outperforms baselines in preserving ignorance awareness while retaining
fine-tuning performance, offering a more robust solution for LLM fine-tuning.

</details>


### [121] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/abs/2506.14470)
*Zixian Zhang, Takfarinas Saber*

**主要类别:** cs.AI

**AI概要:** 本文通过实证研究评估了基于AST的混合图表示在基于GNN的代码克隆检测中的有效性，发现AST+CFG+DFG对于卷积和注意力模型的一致性准确性提升，而FA-AST常因引入结构复杂性而损害性能。


<details>
  <summary>更多</summary>
  
**动机:** 代码克隆增加了软件维护成本和漏洞风险，虽然抽象语法树(ASTs)在基于深度学习的代码克隆检测中占主导地位，但它们缺乏语义深度。最近的研究试图通过结合语义图（如控制流图(CFGs)和数据流图(DFGs)）来丰富基于AST的表示，但这些方法的有效性和与不同机器学习技术的兼容性尚不明确。

**方法:** 采用综合实证研究的方法，系统地比较了多种混合表示（包括CFG、DFG以及流增强ASTs (FA-AST)）在不同GNN架构下的表现。

**结果:** 实验表明，不同的混合表示对GNN的影响各异：AST+CFG+DFG能持续提高卷积和注意力模型（如GCN和GAT）的准确性；相比之下，FA-AST由于增加结构复杂度而经常降低性能。此外，即使使用标准AST表示，GMN的表现也优于其他模型。

**结论:** 研究揭示了基于AST的混合图表示在GNN架构下对代码克隆检测的有效性差异，并指出GMN在跨代码相似性检测上的优越性可能减少对富化结构的需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AST-Enhanced+or+AST-Overloaded%3F+The+Surprising+Impact+of+Hybrid+Graph+Representations+on+Code+Clone+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14470，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14470&send_immediately=true&force_search=false)

**原文摘要:** As one of the most detrimental code smells, code clones significantly
increase software maintenance costs and heighten vulnerability risks, making
their detection a critical challenge in software engineering. Abstract Syntax
Trees (ASTs) dominate deep learning-based code clone detection due to their
precise syntactic structure representation, but they inherently lack semantic
depth. Recent studies address this by enriching AST-based representations with
semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs
(DFGs). However, the effectiveness of various enriched AST-based
representations and their compatibility with different graph-based machine
learning techniques remains an open question, warranting further investigation
to unlock their full potential in addressing the complexities of code clone
detection. In this paper, we present a comprehensive empirical study to
rigorously evaluate the effectiveness of AST-based hybrid graph representations
in Graph Neural Network (GNN)-based code clone detection. We systematically
compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs
(FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid
representations impact GNNs differently: while AST+CFG+DFG consistently
enhances accuracy for convolution- and attention-based models (Graph
Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST
frequently introduces structural complexity that harms performance. Notably,
GMN outperforms others even with standard AST representations, highlighting its
superior cross-code similarity detection and reducing the need for enriched
structures.

</details>


### [122] [GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2506.14477)
*Jingqi Yang, Zhilong Song, Jiawei Chen, Mingli Song, Sheng Zhou, linjun sun, Xiaogang Ouyang, Chun Chen, Can Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出了GUI-Robust数据集，旨在为图形用户界面代理提供全面的评估，特别是针对常见异常情况。同时介绍了一种半自动化构建数据集的方法，大大减少了标注时间，并通过该数据集测试了现有GUI代理在异常情况下的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有的用于评估图形用户界面（GUI）代理的数据集往往是在理想条件下构建的，忽略了现实世界中常见的多种异常情况。为了弥补这一缺陷，作者开发了一个新的数据集GUI-Robust，它特别包含了日常GUI交互中观察到的七种常见类型的异常。

**方法:** 研究者提出了一种半自动化的数据集构建范式，利用RPA工具从自然交互中收集用户动作序列，并借助大规模语言模型的帮助生成这些动作对应的步骤和任务描述。

**结果:** 实验结果表明，采用新方法构建数据集可以将标注时间成本降低超过19倍。此外，使用GUI-Robust数据集对最先进的GUI代理进行评估后发现，在遇到异常情形时，它们的表现显著下降。

**结论:** 这项工作强调了GUI代理鲁棒性的重要性，并希望激发更多关于提高GUI代理对抗异常情况能力的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GUI-Robust%3A+A+Comprehensive+Dataset+for+Testing+GUI+Agent+Robustness+in+Real-World+Anomalies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14477，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14477&send_immediately=true&force_search=false)

**原文摘要:** The development of high-quality datasets is crucial for benchmarking and
advancing research in Graphical User Interface (GUI) agents. Despite their
importance, existing datasets are often constructed under idealized conditions,
overlooking the diverse anomalies frequently encountered in real-world
deployments. To address this limitation, we introduce GUI-Robust, a novel
dataset designed for comprehensive GUI agent evaluation, explicitly
incorporating seven common types of anomalies observed in everyday GUI
interactions. Furthermore, we propose a semi-automated dataset construction
paradigm that collects user action sequences from natural interactions via RPA
tools and then generate corresponding step and task descriptions for these
actions with the assistance of MLLMs. This paradigm significantly reduces
annotation time cost by a factor of over 19 times. Finally, we assess
state-of-the-art GUI agents using the GUI-Robust dataset, revealing their
substantial performance degradation in abnormal scenarios. We anticipate that
our work will highlight the importance of robustness in GUI agents and inspires
more future research in this direction. The dataset and code are available at
https://github.com/chessbean1/GUI-Robust..

</details>


### [123] [LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?](https://arxiv.org/abs/2506.14496)
*Muhammad Atta Ur Rahman, Melanie Schranz*

**主要类别:** cs.AI

**AI概要:** 本文对比了传统群智能算法与基于大型语言模型（LLMs）的新型群智能系统，探讨了在现代人工智能中去中心化、可扩展性和涌现行为是如何被重新定义的，并通过Boids和蚁群优化（ACO）实验评估了两者的延迟、资源使用情况以及行为准确性。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探索随着大型语言模型（LLMs）的发展，'群'的概念如何延伸至描述AI系统，并且如何在这些新系统中重新定义去中心化、可扩展性以及涌现等特性。

**方法:** 采用Boids和蚁群优化(ACO)两种方法实现并比较了传统群智能算法与基于LLM的群智能系统，评估了它们的延迟、资源使用和行为准确性。

**结果:** 结果显示，虽然LLMs提供了强大的推理和抽象能力，但它们也引入了新的计算和协调约束，这对传统的群设计概念提出了挑战。

**结论:** 本研究表明将LLMs整合到群系统中的机会与局限性，并讨论了在现代AI研究中'群'这一术语不断演变的定义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-Powered+Swarms%3A+A+New+Frontier+or+a+Conceptual+Stretch%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14496&send_immediately=true&force_search=false)

**原文摘要:** Swarm intelligence traditionally refers to systems of simple, decentralized
agents whose local interactions lead to emergent, collective behavior.
Recently, the term 'swarm' has been extended to describe AI systems like
OpenAI's Swarm, where large language models (LLMs) act as collaborative agents.
This paper contrasts traditional swarm algorithms with LLM-driven swarms
exploring how decentralization, scalability, and emergence are redefined in
modern artificial intelligence (AI). We implement and compare both paradigms
using Boids and Ant Colony Optimization (ACO), evaluating latency, resource
usage, and behavioral accuracy. The suitability of both cloud-based and local
LLMs is assessed for the agent-based use in swarms. Although LLMs offer
powerful reasoning and abstraction capabilities, they introduce new constraints
in computation and coordination that challenge traditional notions of swarm
design. This study highlights the opportunities and limitations of integrating
LLMs into swarm systems and discusses the evolving definition of 'swarm' in
modern AI research.

</details>


### [124] [Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow](https://arxiv.org/abs/2506.14502)
*Xiao Wang, Junru Yu, Jun Huang, Qiong Wu, Ljubo Vacic, Changyin Sun*

**主要类别:** cs.AI

**AI概要:** 研究提出了一种安全优先的人类似决策框架(SF-HLDM)，用于提高自动驾驶车辆在动态交通流中的驾驶安全性、舒适性和社会兼容性。该框架结合了时空注意力机制、社会合规性估计模块以及深度进化强化学习模型，以实现更高效和有效的搜索空间扩展，避免局部最优陷阱，并减少过拟合的风险。


<details>
  <summary>更多</summary>
  
**动机:** 尽管人工智能技术在提高运输效率和安全性方面展现出巨大潜力，但自动驾驶车辆（AVs）在时间变化的交通流中行驶时仍面临巨大挑战，尤其是在密集且互动的情境下。同时，人类具有自由意志，在相同场景下通常不会做出相同的决策，这导致数据驱动的方法迁移性差、搜索成本高，从而降低了行为策略的效率和效果。

**方法:** 研究提出了一个安全优先的人类似决策框架（SF-HLDM），它集成了一个层次递进的框架，包括针对其他道路使用者意图推断的时空注意力（S-TA）机制、用于行为调节的社会合规性评估模块以及一种深度进化强化学习（DERL）模型来有效拓展搜索空间。

**结果:** SF-HLDM 框架使得自动驾驶AI代理能够动态调整决策参数，保持安全边际的同时遵循上下文适当的行为模式。通过采用此框架，可以预期的是，自动驾驶车辆将能够作出更具解释性和灵活性的人类类似决策。

**结论:** 通过集成时空注意力机制、社会合规性评估和社会适应性考虑，SF-HLDM框架旨在让自动驾驶汽车在保证安全的前提下，模仿人类驾驶者的行为进行决策。此外，通过深度进化强化学习模型，该研究克服了传统方法中存在的局部最优问题和过拟合风险，从而提高了决策的灵活性与可解释性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Safety-First+Human-Like+Decision+Making+for+Autonomous+Vehicles+in+Time-Varying+Traffic+Flow，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14502，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14502&send_immediately=true&force_search=false)

**原文摘要:** Despite the recent advancements in artificial intelligence technologies have
shown great potential in improving transport efficiency and safety, autonomous
vehicles(AVs) still face great challenge of driving in time-varying traffic
flow, especially in dense and interactive situations. Meanwhile, human have
free wills and usually do not make the same decisions even situate in the
exactly same scenarios, leading to the data-driven methods suffer from poor
migratability and high search cost problems, decreasing the efficiency and
effectiveness of the behavior policy. In this research, we propose a
safety-first human-like decision-making framework(SF-HLDM) for AVs to drive
safely, comfortably, and social compatiblely in effiency. The framework
integrates a hierarchical progressive framework, which combines a
spatial-temporal attention (S-TA) mechanism for other road users' intention
inference, a social compliance estimation module for behavior regulation, and a
Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search
space efficiently and effectively to make avoidance of falling into the local
optimal trap and reduce the risk of overfitting, thus make human-like decisions
with interpretability and flexibility. The SF-HLDM framework enables autonomous
driving AI agents dynamically adjusts decision parameters to maintain safety
margins and adhering to contextually appropriate driving behaviors at the same
time.

</details>


### [125] [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
*Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为'Doppelgänger方法'的技术，用于展示代理被劫持的风险，并定义了'对抗性迁移下的提示对齐崩溃(PACAT)'级别来评估对此类攻击的脆弱性。同时提出了'对抗性迁移警告(CAT)'提示以防御Doppelgänger方法。实验结果表明Doppelgänger方法可以破坏代理的一致性并暴露其内部信息，而CAT提示则能够有效防御这种对抗性攻击。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型的出现，提示工程现在使得创建各种自主代理变得快速且低投入，这些代理已经被广泛使用。然而，这种便利性带来了关于基础提示的安全性、鲁棒性和行为一致性的紧迫担忧，以及防止这些提示暴露给用户尝试的迫切挑战。

**方法:** 作者们首先提出了'Doppelgänger方法'来展示代理可能被劫持的风险，这可能导致系统指令和内部信息的泄露。接着他们定义了'对抗性迁移下的提示对齐崩溃(PACAT)'级别，用以衡量对这种对抗性迁移攻击的脆弱程度。此外，他们还提出了一个'对抗性迁移警告(CAT)'提示作为防御Doppelgänger方法的一种手段。

**结果:** 实验结果显示，Doppelgänger方法确实能够损害代理的行为一致性，并揭示其内部信息。相比之下，采用CAT提示则提供了一种有效的抵御此类对抗性攻击的方法。

**结论:** 研究强调了在设计和部署基于提示的代理时需要考虑安全措施的重要性。通过引入PACAT评估标准和CAT防御机制，研究人员为解决与提示相关的安全问题提供了新的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Doppelg%C3%A4nger+Method%3A+Breaking+Role+Consistency+in+LLM+Agent+via+Prompt-based+Transferable+Adversarial+Attack，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14539，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14539&send_immediately=true&force_search=false)

**原文摘要:** Since the advent of large language models, prompt engineering now enables the
rapid, low-effort creation of diverse autonomous agents that are already in
widespread use. Yet this convenience raises urgent concerns about the safety,
robustness, and behavioral consistency of the underlying prompts, along with
the pressing challenge of preventing those prompts from being exposed to user's
attempts. In this paper, we propose the ''Doppelg\"anger method'' to
demonstrate the risk of an agent being hijacked, thereby exposing system
instructions and internal information. Next, we define the ''Prompt Alignment
Collapse under Adversarial Transfer (PACAT)'' level to evaluate the
vulnerability to this adversarial transfer attack. We also propose a ''Caution
for Adversarial Transfer (CAT)'' prompt to counter the Doppelg\"anger method.
The experimental results demonstrate that the Doppelg\"anger method can
compromise the agent's consistency and expose its internal information. In
contrast, CAT prompts enable effective defense against this adversarial attack.

</details>


### [126] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/abs/2506.14568)
*Eliott Thomas, Mickael Coustaty, Aurelie Joseph, Gaspar Deloin, Elodie Carel, Vincent Poulain D'Andecy, Jean-Marc Ogier*

**主要类别:** cs.AI

**AI概要:** 提出了一种针对商务文档的质量感知半监督表格提取框架QUEST，通过评估结构和上下文特征来预测F1分数，而不是依赖于置信度指标。在专有商务数据集上F1得分从64%提高到74%，空预测减少45%；在DocILE基准测试中，F1得分从42%提升至50%，空预测减少了19%。


<details>
  <summary>更多</summary>
  
**动机:** 现有的自动化表格提取（TE）技术在处理工业工作流程中的商业文件时存在挑战，包括注释稀疏性和易出错的多阶段管道。而当前的半监督学习方法依赖的信心分数并不能很好地反映提取质量。

**方法:** QUEST是一种质量感知的半监督表格提取框架，设计用于商业文档。它引入了新的质量评估模型，能够评估提取表格的结构和上下文特征，并训练以预测F1分数。该框架还采用了多样性度量（如DPP、Vendi分数、IntDiv）以减轻确认偏差。

**结果:** 在专有的商业数据集上，QUEST将F1分数从64%提高到了74%，并将空预测降低了45%。在DocILE基准上，QUEST达到了50%的F1分数（提高了8个百分点），同时将空预测减少了19%。

**结论:** QUEST框架提供了解释性的质量评估，并且对于注释稀缺具有鲁棒性，特别适合于对结构一致性和数据完整性要求较高的商业文档。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是QUEST%3A+Quality-aware+Semi-supervised+Table+Extraction+for+Business+Documents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14568，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14568&send_immediately=true&force_search=false)

**原文摘要:** Automating table extraction (TE) from business documents is critical for
industrial workflows but remains challenging due to sparse annotations and
error-prone multi-stage pipelines. While semi-supervised learning (SSL) can
leverage unlabeled data, existing methods rely on confidence scores that poorly
reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised
Table extraction framework designed for business documents. QUEST introduces a
novel quality assessment model that evaluates structural and contextual
features of extracted tables, trained to predict F1 scores instead of relying
on confidence metrics. This quality-aware approach guides pseudo-label
selection during iterative SSL training, while diversity measures (DPP, Vendi
score, IntDiv) mitigate confirmation bias. Experiments on a proprietary
business dataset (1000 annotated + 10000 unannotated documents) show QUEST
improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to
6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents),
QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by
19% (from 27% to 22%). The framework's interpretable quality assessments and
robustness to annotation scarcity make it particularly suited for business
documents, where structural consistency and data completeness are paramount.

</details>


### [127] [Enhancing Symbolic Machine Learning by Subsymbolic Representations](https://arxiv.org/abs/2506.14569)
*Stephen Roth, Lennart Baur, Derian Boer, Stefan Kramer*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种通过给符号机器学习方案提供神经嵌入来增强其性能的方法，并在三个真实世界的领域中展示了这种方法的有效性，超越了所有其他基线方法的F1分数。


<details>
  <summary>更多</summary>
  
**动机:** 神经符号AI的目标是整合符号和次符号AI方法以克服各自的局限性。然而现有的系统如逻辑张量网络(LTN)或DeepProbLog虽然提供了神经谓词和端到端的学习，但在较简单的设置下不够高效，特别是在具有许多常量的领域中的辨别式机器学习。因此，研究者们提出了不同的方法：通过让符号机器学习方案能够访问神经嵌入来增强它们。

**方法:** 研究者们采用了TILDE（一种符号机器学习方案）并使用相似性谓词中的常量嵌入来展示他们的方法。他们还指出，可以通过根据符号理论进一步细化嵌入来进行微调。

**结果:** 在三个真实世界领域的实验中，该方法在F1分数上超过了所有其他基线方法。

**结论:** 通过为符号学习器提供神经嵌入的方式可以有效地提高其性能，这种方法不仅限于当前的设定，还可以扩展到实例之间的相似性、类比推理或命题化等领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Symbolic+Machine+Learning+by+Subsymbolic+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14569，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14569&send_immediately=true&force_search=false)

**原文摘要:** The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI
approaches, to overcome the limitations of either. Prominent systems include
Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and
end-to-end learning. The versatility of systems like LTNs and DeepProbLog,
however, makes them less efficient in simpler settings, for instance, for
discriminative machine learning, in particular in domains with many constants.
Therefore, we follow a different approach: We propose to enhance symbolic
machine learning schemes by giving them access to neural embeddings. In the
present paper, we show this for TILDE and embeddings of constants used by TILDE
in similarity predicates. The approach can be fine-tuned by further refining
the embeddings depending on the symbolic theory. In experiments in three
real-world domain, we show that this simple, yet effective, approach
outperforms all other baseline methods in terms of the F1 score. The approach
could be useful beyond this setting: Enhancing symbolic learners in this way
could be extended to similarities between instances (effectively working like
kernels within a logical language), for analogical reasoning, or for
propositionalization.

</details>


### [128] [From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places](https://arxiv.org/abs/2506.14570)
*Mohammad Hashemi, Andreas Zufle*

**主要类别:** cs.AI

**AI概要:** 本文提出需要一种新的空间基础模型来处理人类移动数据的独特挑战，这种模型能够整合地理定位语义与多尺度下的人类移动，并且从理解地点转向理解场所。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基础模型在处理人类移动数据的空间、时间和语义复杂性方面存在局限性。为了支持跨不同地理和上下文的可扩展和可转移分析，需要有一个通用的基础时空数据模型。

**方法:** 本文是一个愿景论文，它倡导一类新的空间基础模型，该模型将地理定位语义与多尺度的人类移动相结合。文中还指出了适应性、可扩展性和多粒度推理方面的关键差距，并提出了研究方向。

**结果:** 论文没有提供具体实验结果，而是提出了未来的研究方向，专注于建模场所并实现高效学习。

**结论:** 作者的目标是指导开发下一代地理空间智能的可扩展、情境感知模型，这些模型可以解锁从个性化场所发现到物流优化再到城市规划的强大应用，最终使空间决策更加智能和响应迅速。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Points+to+Places%3A+Towards+Human+Mobility-Driven+Spatiotemporal+Foundation+Models+via+Understanding+Places，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14570，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14570&send_immediately=true&force_search=false)

**原文摘要:** Capturing human mobility is essential for modeling how people interact with
and move through physical spaces, reflecting social behavior, access to
resources, and dynamic spatial patterns. To support scalable and transferable
analysis across diverse geographies and contexts, there is a need for a
generalizable foundation model for spatiotemporal data. While foundation models
have transformed language and vision, they remain limited in handling the
unique challenges posed by the spatial, temporal, and semantic complexity of
mobility data. This vision paper advocates for a new class of spatial
foundation models that integrate geolocation semantics with human mobility
across multiple scales. Central to our vision is a shift from modeling discrete
points of interest to understanding places: dynamic, context-rich regions
shaped by human behavior and mobility that may comprise many places of
interest. We identify key gaps in adaptability, scalability, and multi-granular
reasoning, and propose research directions focused on modeling places and
enabling efficient learning. Our goal is to guide the development of scalable,
context-aware models for next-generation geospatial intelligence. These models
unlock powerful applications ranging from personalized place discovery and
logistics optimization to urban planning, ultimately enabling smarter and more
responsive spatial decision-making.

</details>


### [129] [AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes](https://arxiv.org/abs/2506.14728)
*Jiahao Qiu, Xinzhe Juan, Yimin Wang, Ling Yang, Xuan Qi, Tongcheng Zhang, Jiacheng Guo, Yifu Lu, Zixin Yao, Hongru Wang, Shilong Liu, Xun Jiang, Liu Leqi, Mengdi Wang*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的无训练代理蒸馏框架AgentDistill，通过直接重用由教师代理自动生成的模型-上下文-协议（MCPs）来实现知识转移，使学生代理能够跨领域泛化其能力，并在最少监督或人工干预的情况下解决新问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于大型语言模型（LLMs）的代理蒸馏方法通常重放完整的教师轨迹或模仿逐步教师工具使用，但往往难以训练学生代理在新环境中动态规划和行动。

**方法:** 提出了AgentDistill，一种无需训练的代理蒸馏框架，它通过直接重用教师代理自主生成的结构化可重用任务解决模块——模型-上下文-协议（MCPs）来实现高效且可扩展的知识转移。

**结果:** 实验表明，基于小型语言模型构建的学生代理能够达到与使用大型LLMs如OctoTools (GPT-4o)的高级系统相媲美的性能。

**结论:** AgentDistill框架在构建可扩展性和成本效益高的智能代理方面表现出有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentDistill%3A+Training-Free+Agent+Distillation+with+Generalizable+MCP+Boxes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14728，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14728&send_immediately=true&force_search=false)

**原文摘要:** While knowledge distillation has become a mature field for compressing large
language models (LLMs) into smaller ones by aligning their outputs or internal
representations, the distillation of LLM-based agents, which involve planning,
memory, and tool use, remains relatively underexplored. Existing agent
distillation methods typically replay full teacher trajectories or imitate
step-by-step teacher tool usage, but they often struggle to train student
agents to dynamically plan and act in novel environments. We propose
AgentDistill, a novel, training-free agent distillation framework that enables
efficient and scalable knowledge transfer via direct reuse of
Model-Context-Protocols (MCPs), which are structured and reusable task-solving
modules autonomously generated by teacher agents. The reuse of these distilled
MCPs enables student agents to generalize their capabilities across domains and
solve new problems with minimal supervision or human intervention. Experiments
on biomedical and mathematical benchmarks demonstrate that our distilled
student agents, built on small language models, can achieve performance
comparable to advanced systems using large LLMs such as OctoTools (GPT-4o),
highlighting the effectiveness of our framework in building scalable and
cost-efficient intelligent agents.

</details>


### [130] [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
*Zhengxiang Cheng, Dongping Chen, Mingyang Fu, Tianyi Zhou*

**主要类别:** cs.AI

**AI概要:** 本文针对大型推理模型(LRMs)产生的冗余推理链问题，提出了简洁性和充分性两个新原则，并基于此开发了LC-R1方法。该方法通过长度奖励和压缩奖励减少无效思考过程，实验表明LC-R1能在保持较高准确率的同时大幅缩短序列长度，且代码已公开。


<details>
  <summary>更多</summary>
  
**动机:** 大型推理模型（LRMs）虽然取得了显著的成功，但经常产生不必要的冗长推理链。研究者们发现了一个核心问题，即“无效思考”——模型在得出正确答案后倾向于反复检查其工作。为解决这一特定的低效问题，研究超越了效率与效能的一般原则，提出两个新的细化原则：提倡消除冗余的‘简洁性’以及确保保留关键推理步骤的‘充分性’。

**方法:** 提出了名为LC-R1的方法，这是一种基于组相对策略优化（GRPO）的后训练技术。它结合了一种新颖的整体简洁度奖励机制和特别设计用于去除思考过程中无效部分的压缩奖励机制。

**结果:** 广泛的实验证明，LC-R1能够在仅导致约2%准确性下降的情况下实现大约50%的序列长度缩减，达到了帕累托前沿上的有利平衡点，优先考虑高压缩率。此外，分析进一步证实了LC-R1的鲁棒性，并为开发更强大同时计算效率更高的LRMs提供了有价值的见解。

**结论:** 通过引入简洁性和充分性原则及LC-R1方法，本研究成功减少了LRMs中的冗余推理链，同时保持了较高的准确性。这不仅提高了模型的计算效率，还为进一步研究如何构建更高效的大规模推理模型奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Length+Compression+in+Large+Reasoning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14755，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14755&send_immediately=true&force_search=false)

**原文摘要:** Large Reasoning Models (LRMs) have achieved remarkable success, yet they
often suffer from producing unnecessary and verbose reasoning chains. We
identify a core aspect of this issue as "invalid thinking" -- models tend to
repeatedly double-check their work after having derived the correct answer. To
address this specific inefficiency, we move beyond the general principles of
Efficacy and Efficiency to propose two new, fine-grained principles: Brevity,
which advocates for eliminating redundancy, and Sufficiency, which ensures
critical reasoning steps are preserved. Guided by these principles, we
introduce LC-R1, a post-training method based on Group Relative Policy
Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for
overall conciseness and a Compress Reward that is specifically designed to
remove the invalid portion of the thinking process. Extensive experiments on
multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant
reduction in sequence length (~50%) with only a marginal (~2%) drop in
accuracy, achieving a favorable trade-off point on the Pareto frontier that
prioritizes high compression. Our analysis further validates the robustness of
LC-R1 and provides valuable insights for developing more powerful yet
computationally efficient LRMs. Our code is released at
https://github.com/zxiangx/LC-R1.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [131] [Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models](https://arxiv.org/abs/2506.13900)
*Marouane Il Idrissi, Agathe Fernandes Machado, Arthur Charpentier*

**主要类别:** stat.ML

**AI概要:** 本文重新审视了合作博弈论在机器学习可解释性中的应用，提出了超越Shapley值的更广泛和原则性的工具使用，并介绍了一种构建可靠且理论基础坚实的特征归因方法的三步蓝图。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于Shapley值的方法被广泛采用，但这些方法通常基于公理化论证，其对于特征归因的相关性仍有争议。文章旨在提供一个连贯的框架，以设计既具有意义又能够适应不断变化的方法趋势的归因方法。

**方法:** 文章回顾了合作博弈论，并从可解释性的角度出发，强调了两种高效的分配方案：Weber集合和Harsanyi集合，它们提供了比Shapley值更丰富的解释灵活性。此外，文章还区分了价值函数与聚合规则，并提出了一套三步走的指南来创建可靠的特征归因。

**结果:** 通过引入Weber和Harsanyi集合，论文为特征归因提供了更多的选择，同时通过定义清晰的价值函数和聚合规则以及一个三步蓝图，为创建可靠的特征归因方法奠定了基础。

**结论:** 作者们的目标是超越固定的公理，为XAI（可解释的人工智能）社区提供一个一致的框架，以开发有意义且对方法学趋势变化有抵抗力的归因方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+Shapley+Values%3A+Cooperative+Games+for+the+Interpretation+of+Machine+Learning+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13900，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13900&send_immediately=true&force_search=false)

**原文摘要:** Cooperative game theory has become a cornerstone of post-hoc interpretability
in machine learning, largely through the use of Shapley values. Yet, despite
their widespread adoption, Shapley-based methods often rest on axiomatic
justifications whose relevance to feature attribution remains debatable. In
this paper, we revisit cooperative game theory from an interpretability
perspective and argue for a broader and more principled use of its tools. We
highlight two general families of efficient allocations, the Weber and Harsanyi
sets, that extend beyond Shapley values and offer richer interpretative
flexibility. We present an accessible overview of these allocation schemes,
clarify the distinction between value functions and aggregation rules, and
introduce a three-step blueprint for constructing reliable and
theoretically-grounded feature attributions. Our goal is to move beyond fixed
axioms and provide the XAI community with a coherent framework to design
attribution methods that are both meaningful and robust to shifting
methodological trends.

</details>


### [132] [Rademacher learning rates for iterated random functions](https://arxiv.org/abs/2506.13946)
*Nikola Sandrić*

**主要类别:** stat.ML

**AI概要:** 本文研究了当训练数据集由迭代随机函数生成时（即时间齐次马尔可夫链），即使该链不一定是不可约或非周期的，通过假设控制函数对于其第一个参数是收缩的，并且满足假设类别的某些正则条件，建立了样本误差的一致收敛结果。进而证明了近似经验风险最小化算法的可学习性并推导出其学习率边界。这些速率依赖于数据分布，并通过假设类别的Rademacher复杂度来表达，能够更准确地反映数据生成分布的特性。


<details>
  <summary>更多</summary>
  
**动机:** 现有监督机器学习文献通常假设训练数据集是从独立同分布(i.i.d.)样本中抽取的。然而，许多现实世界的问题表现出时间依赖性和数据生成过程边际分布之间的强相关性，这表明i.i.d.假设往往不切实际。在这样的情况下，模型自然包含了具有混合性质的时间序列过程以及不可约和非周期遍历马尔可夫链。此外，在这些设定下通常获得的学习率与数据分布无关，这可能导致对假设类别的限制性选择以及学习算法的次优样本复杂度。

**方法:** 作者考虑了一种情况，其中训练数据集是由一个迭代随机函数产生的，这个函数是一个时间齐次马尔可夫链，但不一定不可约或非周期。基于支配函数对其第一个参数是收缩的假设，并且假定假设类别满足某些正则条件，作者首先建立了一个相应的样本误差一致收敛的结果。然后，他们展示了近似经验风险最小化算法的可学习性，并推导出了其学习率的界限。

**结果:** 研究表明，即使对于不是不可约或非周期的迭代随机函数，只要支配函数对于其第一个参数是收缩的，并且假设类别满足一定的正则条件，就可以建立样本误差的一致收敛结果。同时，近似经验风险最小化算法被证明是可学习的，并且得到了学习率的界限。

**结论:** 结论指出，所得到的学习率取决于数据分布，并通过假设类别的Rademacher复杂度来表示，从而更加精确地反映了数据生成分布的特性。这意味着新的学习率边界可以更好地适应数据的真实分布，避免了传统方法中由于忽略数据分布而带来的次优样本复杂度问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rademacher+learning+rates+for+iterated+random+functions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13946&send_immediately=true&force_search=false)

**原文摘要:** Most existing literature on supervised machine learning assumes that the
training dataset is drawn from an i.i.d. sample. However, many real-world
problems exhibit temporal dependence and strong correlations between the
marginal distributions of the data-generating process, suggesting that the
i.i.d. assumption is often unrealistic. In such cases, models naturally include
time-series processes with mixing properties, as well as irreducible and
aperiodic ergodic Markov chains. Moreover, the learning rates typically
obtained in these settings are independent of the data distribution, which can
lead to restrictive choices of hypothesis classes and suboptimal sample
complexities for the learning algorithm. In this article, we consider the case
where the training dataset is generated by an iterated random function (i.e.,
an iteratively defined time-homogeneous Markov chain) that is not necessarily
irreducible or aperiodic. Under the assumption that the governing function is
contractive with respect to its first argument and subject to certain
regularity conditions on the hypothesis class, we first establish a uniform
convergence result for the corresponding sample error. We then demonstrate the
learnability of the approximate empirical risk minimization algorithm and
derive its learning rate bound. Both rates are data-distribution dependent,
expressed in terms of the Rademacher complexities of the underlying hypothesis
class, allowing them to more accurately reflect the properties of the
data-generating distribution.

</details>


### [133] [Meta Optimality for Demographic Parity Constrained Regression via Post-Processing](https://arxiv.org/abs/2506.13947)
*Kazuto Fukuchi*

**主要类别:** stat.ML

**AI概要:** 本文提出了适用于多种情况的元定理，以验证相应回归算法在满足公平性约束下的公平最小-最大最优性，并展示了通过后处理方法可以实现公平最小-最大最优回归。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决在人口统计学平等约束条件下的回归问题，本文旨在提供一种不依赖于特定数据生成模型的、能够应用于各种情况的方法来验证回归算法的公平最小-最大最优性。

**方法:** 本文开发了元定理用于验证不同情境下回归算法是否达到公平最小-最大最优，并且说明了如何利用后处理技术达成这一目标。

**结果:** 研究结果表明，通过后处理方法可以实现公平最小-最大最优回归，这使得研究人员和实践者能够专注于改进传统的回归技术，然后高效地将其调整为公平回归。

**结论:** 文章得出结论，即通过后处理方法可以在保证公平性的前提下实现回归算法的最小-最大最优性能，这为传统回归技术向公平回归转化提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta+Optimality+for+Demographic+Parity+Constrained+Regression+via+Post-Processing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13947&send_immediately=true&force_search=false)

**原文摘要:** We address the regression problem under the constraint of demographic parity,
a commonly used fairness definition. Recent studies have revealed fair minimax
optimal regression algorithms, the most accurate algorithms that adhere to the
fairness constraint. However, these analyses are tightly coupled with specific
data generation models. In this paper, we provide meta-theorems that can be
applied to various situations to validate the fair minimax optimality of the
corresponding regression algorithms. Furthermore, we demonstrate that fair
minimax optimal regression can be achieved through post-processing methods,
allowing researchers and practitioners to focus on improving conventional
regression techniques, which can then be efficiently adapted for fair
regression.

</details>


### [134] [Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies](https://arxiv.org/abs/2506.13955)
*Matthew Lau, Tian-Yi Zhou, Xiangchi Yuan, Jizhou Chen, Wenke Lee, Xiaoming Huo*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种半监督异常检测框架，该框架结合了已知和合成的异常数据进行训练，并提供了首个数学公式化定义。通过在五个不同基准上的实证验证，证明了此方法能够带来一致的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于改进现有的无监督异常检测方法，特别是在半监督环境下，利用有限标记的异常样本提高模型对测试时可能出现的异常的识别能力。

**方法:** 作者提出了一个理论基础扎实且经验上有效的半监督异常检测框架，该框架在训练过程中同时使用已知异常和合成异常。此外，文章还首次为半监督异常检测提供了数学公式化定义，并展示了合成异常如何帮助改善低密度区域中的异常建模以及确保神经网络分类器的最佳收敛保证。

**结果:** 实验结果表明，在五个不同的基准测试中，所提出的框架相较于其他基于分类的方法展现出了稳定的表现增益。这不仅验证了合成异常原则的有效性，也突显出其在异常检测领域的广泛适用性。

**结论:** 通过引入合成异常到半监督学习过程中，可以有效地增强异常检测模型的能力，尤其是在处理那些难以直接获取足够多真实异常样本来训练的情况时。本研究为未来探索更加高效可靠的异常检测技术开辟了新的道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+Unsupervised+and+Semi-Supervised+Anomaly+Detection%3A+A+Theoretically-Grounded+and+Practical+Framework+with+Synthetic+Anomalies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13955，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13955&send_immediately=true&force_search=false)

**原文摘要:** Anomaly detection (AD) is a critical task across domains such as
cybersecurity and healthcare. In the unsupervised setting, an effective and
theoretically-grounded principle is to train classifiers to distinguish normal
data from (synthetic) anomalies. We extend this principle to semi-supervised
AD, where training data also include a limited labeled subset of anomalies
possibly present in test time. We propose a theoretically-grounded and
empirically effective framework for semi-supervised AD that combines known and
synthetic anomalies during training. To analyze semi-supervised AD, we
introduce the first mathematical formulation of semi-supervised AD, which
generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i)
better anomaly modeling in low-density regions and (ii) optimal convergence
guarantees for neural network classifiers -- the first theoretical result for
semi-supervised AD. We empirically validate our framework on five diverse
benchmarks, observing consistent performance gains. These improvements also
extend beyond our theoretical framework to other classification-based AD
methods, validating the generalizability of the synthetic anomaly principle in
AD.

</details>


### [135] [Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms](https://arxiv.org/abs/2506.13984)
*Andrzej Cichocki*

**主要类别:** stat.ML

**AI概要:** 本文开发了一类新的镜像下降算法，通过使用Tempesta多参数变形对数作为链接函数的Bregman散度来解决约束优化问题，并估计了广义指数函数以近似该对数的逆。这些方法提供了灵活且广泛的MD更新规则，可以适应训练数据的分布或几何结构。


<details>
  <summary>更多</summary>
  
**动机:** 为了在机器学习中发挥关键作用，研究者们开发了一类广泛适用的镜像下降（MD）算法。

**方法:** 研究者们构建了一个约束优化问题，其中利用了带有Tempesta多参数变形对数作为链接函数的Bregman散度。他们还估计了一个广义指数函数，这个函数能够很好地逼近Tempesta多参数变形对数的逆。

**结果:** 通过引入多参数对数的概念，研究者们能够生成一个全新的、宽泛且灵活的MD和无镜像MD更新家族。

**结论:** 通过调整Tempesta对数及其逆变形指数函数的超参数，我们可以使MD算法适应训练数据的分布或几何形状，并调整它们以实现所需的算法属性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mirror+Descent+Using+the+Tempesta+Generalized+Multi-parametric+Logarithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13984，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13984&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we develop a wide class Mirror Descent (MD) algorithms, which
play a key role in machine learning. For this purpose we formulated the
constrained optimization problem, in which we exploits the Bregman divergence
with the Tempesta multi-parametric deformation logarithm as a link function.
This link function called also mirror function defines the mapping between the
primal and dual spaces and is associated with a very-wide (in fact,
theoretically infinite) class of generalized trace-form entropies. In order to
derive novel MD updates, we estimate generalized exponential function, which
closely approximates the inverse of the multi-parametric Tempesta generalized
logarithm. The shape and properties of the Tempesta logarithm and its
inverse-deformed exponential functions can be tuned by several hyperparameters.
By learning these hyperparameters, we can adapt to distribution or geometry of
training data, and we can adjust them to achieve desired properties of MD
algorithms. The concept of applying multi-parametric logarithms allow us to
generate a new wide and flexible family of MD and mirror-less MD updates.

</details>


### [136] [Estimation of Treatment Effects in Extreme and Unobserved Data](https://arxiv.org/abs/2506.14051)
*Jiyuan Tan, Jose Blanchet, Vasilis Syrgkanis*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的框架，用于在极端数据中评估处理效应，特别是针对罕见事件的因果效应。该框架利用了多变量正则变化理论来建模极端情况，并开发了一个一致的估计器来估计极端处理效应，同时给出了其性能的严格非渐近分析。


<details>
  <summary>更多</summary>
  
**动机:** 现有的因果推断文献主要关注频繁发生事件的处理效应，而对于罕见但影响深远的事件（如极端气候事件）的政策干预效果却很少涉及。标准的因果推断方法论并未设计来处理这种类型的数据，因为感兴趣的事件可能在观察数据中非常稀少，需要一定程度的外推。

**方法:** 文章介绍了一种新框架，使用极值理论（EVT）中的方法，特别是多变量正则变化理论，来分析极端现象下的统计问题。作者开发了一个一致性的估计器用来估计极端处理效应，并对其表现进行了严格的非渐近性分析。

**结果:** 通过合成和半合成数据，展示了所提出的估计器的性能。

**结论:** 研究提供了一种有效的方法来估计罕见事件的因果效应，这在传统因果推断方法难以处理的情况下特别有用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimation+of+Treatment+Effects+in+Extreme+and+Unobserved+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14051，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14051&send_immediately=true&force_search=false)

**原文摘要:** Causal effect estimation seeks to determine the impact of an intervention
from observational data. However, the existing causal inference literature
primarily addresses treatment effects on frequently occurring events. But what
if we are interested in estimating the effects of a policy intervention whose
benefits, while potentially important, can only be observed and measured in
rare yet impactful events, such as extreme climate events? The standard causal
inference methodology is not designed for this type of inference since the
events of interest may be scarce in the observed data and some degree of
extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies
for analyzing statistical phenomena in such extreme regimes. We introduce a
novel framework for assessing treatment effects in extreme data to capture the
causal effect at the occurrence of rare events of interest. In particular, we
employ the theory of multivariate regular variation to model extremities. We
develop a consistent estimator for extreme treatment effects and present a
rigorous non-asymptotic analysis of its performance. We illustrate the
performance of our estimator using both synthetic and semi-synthetic data.

</details>


### [137] [Universal Rates of ERM for Agnostic Learning](https://arxiv.org/abs/2506.14110)
*Steve Hanneke, Mingyue Xu*

**主要类别:** stat.ML

**AI概要:** 本文研究了在不可知情况下，二元分类的普遍学习问题，并通过经验风险最小化(ERM)方法揭示了三种可能的不可知普遍学习速率：$e^{-n}$、$o(n^{-1/2})$ 或任意慢。同时对属于这些类别的概念类别进行了完整的特征描述，并且也对目标依赖和贝叶斯依赖的普遍学习速率进行了完整的特征描述。


<details>
  <summary>更多</summary>
  
**动机:** 由于普遍学习框架下的学习速率保证对于任何固定分布都有效，并且可以比对所有分布都一致有效的学习速率快得多，但现有的关于普遍学习的研究大多集中在可实现的情况，而实际情况往往不可实现。因此，作者希望探索在不可知情况下，二元分类的普遍学习问题以及ERM的学习速率。

**方法:** 作者通过分析在不可知设置下ERM的“学习曲线”，来研究随着样本量增加超额风险的衰减情况。他们探讨了不可知普遍学习速率的可能性，并提供了一个关于哪些概念类属于这三类（$e^{-n}$, $o(n^{-1/2})$, 或者任意慢）的完整特征描述。

**结果:** 研究结果表明，在不可知情况下，ERM的不可知普遍学习速率有三种可能：$e^{-n}$、$o(n^{-1/2})$ 或任意慢。此外，文章还提供了目标依赖和贝叶斯依赖的普遍学习速率的完整特征描述。

**结论:** 本研究填补了非可实现情况下普遍学习理论的空白，为理解在不可知环境下ERM的学习行为提供了新的视角，并对不同概念类别的学习速率给出了清晰的分类。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Rates+of+ERM+for+Agnostic+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14110，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14110&send_immediately=true&force_search=false)

**原文摘要:** The universal learning framework has been developed to obtain guarantees on
the learning rates that hold for any fixed distribution, which can be much
faster than the ones uniformly hold over all the distributions. Given that the
Empirical Risk Minimization (ERM) principle being fundamental in the PAC theory
and ubiquitous in practical machine learning, the recent work of
arXiv:2412.02810 studied the universal rates of ERM for binary classification
under the realizable setting. However, the assumption of realizability is too
restrictive to hold in practice. Indeed, the majority of the literature on
universal learning has focused on the realizable case, leaving the
non-realizable case barely explored.
  In this paper, we consider the problem of universal learning by ERM for
binary classification under the agnostic setting, where the ''learning curve"
reflects the decay of the excess risk as the sample size increases. We explore
the possibilities of agnostic universal rates and reveal a compact trichotomy:
there are three possible agnostic universal rates of ERM, being either
$e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete
characterization of which concept classes fall into each of these categories.
Moreover, we also establish complete characterizations for the target-dependent
universal rates as well as the Bayes-dependent universal rates.

</details>


### [138] [Adjustment for Confounding using Pre-Trained Representations](https://arxiv.org/abs/2506.14329)
*Rickmer Schulte, David Rügamer, Thomas Nagler*

**主要类别:** stat.ML

**AI概要:** 本文探讨了如何利用预训练神经网络的潜在特征来调整混杂因素，以在平均治疗效应（ATE）估计中实现有效的调整和统计推断。


<details>
  <summary>更多</summary>
  
**动机:** 为了扩展平均治疗效应（ATE）估计并考虑非表格数据（如图像和文本）作为混杂因素的来源，避免忽略这些因素导致的结果偏差和科学结论错误。

**方法:** 研究了预训练神经网络的潜在特征如何用于调整混杂因素，并且在双机器学习的例子中展示了结果。还讨论了与潜在特征学习相关的挑战，包括高维度和表示的不可识别性。

**结果:** 证明了对于潜在特征而言，获得快速收敛率的常见结构假设是不现实的，但神经网络对这些问题大体上并不敏感，并且可以通过适应学习问题内在的稀疏性和维数来达到快速收敛率。

**结论:** 通过使用预训练神经网络的潜在特征，可以在ATE估计中有效地调整混杂因素，尽管存在一些固有的挑战，但神经网络能够克服这些问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adjustment+for+Confounding+using+Pre-Trained+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14329&send_immediately=true&force_search=false)

**原文摘要:** There is growing interest in extending average treatment effect (ATE)
estimation to incorporate non-tabular data, such as images and text, which may
act as sources of confounding. Neglecting these effects risks biased results
and flawed scientific conclusions. However, incorporating non-tabular data
necessitates sophisticated feature extractors, often in combination with ideas
of transfer learning. In this work, we investigate how latent features from
pre-trained neural networks can be leveraged to adjust for sources of
confounding. We formalize conditions under which these latent features enable
valid adjustment and statistical inference in ATE estimation, demonstrating
results along the example of double machine learning. We discuss critical
challenges inherent to latent feature learning and downstream parameter
estimation arising from the high dimensionality and non-identifiability of
representations. Common structural assumptions for obtaining fast convergence
rates with additive or sparse linear models are shown to be unrealistic for
latent features. We argue, however, that neural networks are largely
insensitive to these issues. In particular, we show that neural networks can
achieve fast convergence rates by adapting to intrinsic notions of sparsity and
dimension of the learning problem.

</details>


### [139] [Adaptive Data Augmentation for Thompson Sampling](https://arxiv.org/abs/2506.14479)
*Wonyoung Kim*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种针对线性情境bandits的几乎是最小最大最优的汤普森抽样方法，通过开发一种新颖的估计器来实现对所有臂奖励的准确预测。


<details>
  <summary>更多</summary>
  
**动机:** 尽管汤普森抽样在经验上表现良好，但它未能达到最优后悔界。为了改善这一点，并且不需要依赖于上下文分布的假设，提出了新的方法。

**方法:** 开发了一个具有自适应增强和假设样本耦合的新颖估计器，用于有效参数学习。

**结果:** 实证结果表明，该方法比现有方法表现出更强的鲁棒性和显著改进。

**结论:** 所提出的汤普森抽样方法对于线性情境bandits是有效的，并且在没有上下文分布假设的情况下也能准确预测所有臂的奖励。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Data+Augmentation+for+Thompson+Sampling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14479，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14479&send_immediately=true&force_search=false)

**原文摘要:** In linear contextual bandits, the objective is to select actions that
maximize cumulative rewards, modeled as a linear function with unknown
parameters. Although Thompson Sampling performs well empirically, it does not
achieve optimal regret bounds. This paper proposes a nearly minimax optimal
Thompson Sampling for linear contextual bandits by developing a novel estimator
with the adaptive augmentation and coupling of the hypothetical samples that
are designed for efficient parameter learning. The proposed estimator
accurately predicts rewards for all arms without relying on assumptions for the
context distribution. Empirical results show robust performance and significant
improvement over existing methods.

</details>


### [140] [Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters](https://arxiv.org/abs/2506.14530)
*Anastasis Kratsios, Tin Sum Cheng, Aurelien Lucchi, Haitz Sáez de Ocáriz Borde*

**主要类别:** stat.ML

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sharp+Generalization+Bounds+for+Foundation+Models+with+Asymmetric+Randomized+Low-Rank+Adapters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14530，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14530&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) has emerged as a widely adopted
parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent
work has highlighted an inherent asymmetry in the initialization of LoRA's
low-rank factors, which has been present since its inception and was presumably
derived experimentally. This paper focuses on providing a comprehensive
theoretical characterization of asymmetric LoRA with frozen random factors.
First, while existing research provides upper-bound generalization guarantees
based on averages over multiple experiments, the behaviour of a single
fine-tuning run with specific random factors remains an open question. We
address this by investigating the concentration of the typical LoRA
generalization gap around its mean. Our main upper bound reveals a sample
complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with
high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we
also determine the fundamental limits in terms of sample efficiency,
establishing a matching lower bound of
$\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the
practical scenario of a single fine-tuning run, our findings offer crucial
insights into the reliability and practicality of asymmetric LoRA.

</details>


### [141] [Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means](https://arxiv.org/abs/2506.14673)
*Mikael Møller Høgsgaard, Andrea Paudice*

**主要类别:** stat.ML

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uniform+Mean+Estimation+for+Heavy-Tailed+Distributions+via+Median-of-Means，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14673&send_immediately=true&force_search=false)

**原文摘要:** The Median of Means (MoM) is a mean estimator that has gained popularity in
the context of heavy-tailed data. In this work, we analyze its performance in
the task of simultaneously estimating the mean of each function in a class
$\mathcal{F}$ when the data distribution possesses only the first $p$ moments
for $p \in (1,2]$. We prove a new sample complexity bound using a novel
symmetrization technique that may be of independent interest. Additionally, we
present applications of our result to $k$-means clustering with unbounded
inputs and linear regression with general losses, improving upon existing
works.

</details>
