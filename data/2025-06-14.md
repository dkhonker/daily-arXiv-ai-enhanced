<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 89]
- [cs.AI](#cs.AI) [总数: 27]
- [stat.ML](#stat.ML) [总数: 10]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion](https://arxiv.org/abs/2506.09999)
*Yukun Chen, Zihuan Qiu, Fanman Meng, Hongliang Li, Linfeng Xu, Qingbo Wu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于多模态预训练模型的多模态类增量学习（MCIL）方法，通过引入多模态增量特征提取器、自适应视听融合模块以及一种新的多模态类增量对比训练损失来解决跨视觉、音频和文本模态的挑战。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态类增量学习方法主要集中在视觉和文本上，而忽略了音频信息的整合。本文旨在探索如何在视觉、音频和文本模态之间进行有效的信息互补，并缓解灾难性遗忘问题。

**方法:** 1. 提出基于混合专家结构（MoE）的多模态增量特征提取器（MIFE），以实现AudioCLIP的有效增量微调。
2. 设计了自适应视听融合模块（AAVFM），包含掩码阈值机制和动态特征融合机制，同时增强文本多样性。
3. 引入了一种新颖的多模态类增量对比训练损失，以优化MCIL中的跨模态对齐。
4. 定义了两个针对MCIL的具体评估指标，以便进行全面评估。

**结果:** 通过对三个多模态数据集上的广泛实验验证了所提方法的有效性。

**结论:** 本文提出的MCIL方法能够有效地将视觉、音频和文本模态的信息结合起来，并且在防止灾难性遗忘方面表现出色。此外，该方法还提供了一套新的评估标准用于衡量MCIL系统的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Pre-Trained+Models+for+Multimodal+Class-Incremental+Learning+under+Adaptive+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.09999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.09999&send_immediately=true&force_search=false)

**原文摘要:** Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that
focus only on vision and text, this paper explores MCIL across vision, audio
and text modalities, addressing challenges in integrating complementary
information and mitigating catastrophic forgetting. To tackle these issues, we
propose an MCIL method based on multimodal pre-trained models. Firstly, a
Multimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts
(MoE) structure is introduced to achieve effective incremental fine-tuning for
AudioCLIP. Secondly, to enhance feature discriminability and generalization, we
propose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking
threshold mechanism and a dynamic feature fusion mechanism, along with a
strategy to enhance text diversity. Thirdly, a novel multimodal
class-incremental contrastive training loss is proposed to optimize cross-modal
alignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced
for comprehensive assessment. Extensive experiments on three multimodal
datasets validate the effectiveness of our method.

</details>


### [2] [NOCL: Node-Oriented Conceptualization LLM for Graph Tasks without Message Passing](https://arxiv.org/abs/2506.10014)
*Wei Li, Mengcheng Lan, Jiaxing Xu, Yiping Ke*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的框架Node-Oriented Conceptualization LLM (NOCL)，它通过节点描述和节点概念两种核心技术，解决了大型语言模型在处理图数据时面临的挑战，并且在零样本设置中展示了更好的泛化性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统的图神经网络（如消息传递神经网络）依赖于监督学习，这限制了它们在标签稀缺场景下的泛化能力和适用性；而最近的自监督方法仍然需要有标签的微调，这限制了它们在零样本场景中的有效性。同时，大型语言模型虽然在自然语言任务上表现出色，但在应用于图数据时面临许多挑战，包括保持推理能力、管理来自丰富节点属性的大量标记长度以及局限于文本属性图(TAGs)和单一层次任务。

**方法:** 提出了一种名为Node-Oriented Conceptualization LLM (NOCL)的新框架，该框架利用两项核心技术：1) 节点描述，将异构节点属性转换为结构化的自然语言，使LLM能够从TAGs扩展到非TAGs；2) 节点概念，使用预训练的语言模型将节点描述编码成紧凑的语义嵌入，与直接使用节点描述相比显著减少了标记长度高达93.9%。此外，NOCL采用图形表示描述符来统一不同级别的图形任务，将其转化为共享的语言查询格式。

**结果:** 实验结果验证了相对于传统MPNNs和混合LLM-MPNN方法，NOCL具有竞争性的监督性能，并且在零样本设置中展现了优越的泛化能力。

**结论:** NOCL提供了一种新的方法来克服现有图神经网络和大型语言模型在处理图数据方面的问题，特别是在零样本情况下提高了泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NOCL%3A+Node-Oriented+Conceptualization+LLM+for+Graph+Tasks+without+Message+Passing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10014&send_immediately=true&force_search=false)

**原文摘要:** Graphs are essential for modeling complex interactions across domains such as
social networks, biology, and recommendation systems. Traditional Graph Neural
Networks, particularly Message Passing Neural Networks (MPNNs), rely heavily on
supervised learning, limiting their generalization and applicability in
label-scarce scenarios. Recent self-supervised approaches still require labeled
fine-tuning, limiting their effectiveness in zero-shot scenarios. Meanwhile,
Large Language Models (LLMs) excel in natural language tasks but face
significant challenges when applied to graphs, including preserving reasoning
abilities, managing extensive token lengths from rich node attributes, and
being limited to textual-attributed graphs (TAGs) and a single level task. To
overcome these limitations, we propose the Node-Oriented Conceptualization LLM
(NOCL), a novel framework that leverages two core techniques: 1) node
description, which converts heterogeneous node attributes into structured
natural language, extending LLM from TAGs to non-TAGs; 2) node concept, which
encodes node descriptions into compact semantic embeddings using pretrained
language models, significantly reducing token lengths by up to 93.9% compared
to directly using node descriptions. Additionally, our NOCL employs graph
representation descriptors to unify graph tasks at various levels into a
shared, language-based query format, paving a new direction for Graph
Foundation Models. Experimental results validate NOCL's competitive supervised
performance relative to traditional MPNNs and hybrid LLM-MPNN methods and
demonstrate superior generalization in zero-shot settings.

</details>


### [3] [Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks](https://arxiv.org/abs/2506.10044)
*Uijun Jung, Deokho Jang, Sungchul Kim, Jungho Kim*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于深度学习的串联神经网络(TNN)来设计SiO2/TiO2多层薄膜的透射光谱，通过比较不同配置的TNN发现，基于CNN-LSTM的TNN在准确性和速度上达到了最优平衡。


<details>
  <summary>更多</summary>
  
**动机:** 传统的逆向设计方法需要大量的数值模拟和优化过程，耗时较长。为了提高设计效率并解决基于深度学习的逆向设计中的一对多映射问题，提出了使用串联神经网络（TNN）的方法。

**方法:** 研究人员实现了一个由逆向神经网络和预训练的前向神经网络背靠背连接构成的串联神经网络（TNN），并且在TNN的配置中不仅采用了多层感知器(MLP)，还尝试了卷积神经网络(CNN)或长短期记忆(LSTM)算法。

**结果:** 研究结果表明，基于LSTM-LSTM的TNN具有最高的准确性但同时训练时间最长；而基于CNN-LSTM的TNN则在准确度和速度之间提供了最佳解决方案。

**结论:** 该研究表明，利用深度学习特别是结合CNN与LSTM算法的TNN结构，可以有效提升SiO2/TiO2多层薄膜透射光谱逆向设计的效率和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+the+performance+of+optical+inverse+design+of+multilayer+thin+films+using+CNN-LSTM+tandem+neural+networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10044，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10044&send_immediately=true&force_search=false)

**原文摘要:** Optical properties of thin film are greatly influenced by the thickness of
each layer. Accurately predicting these thicknesses and their corresponding
optical properties is important in the optical inverse design of thin films.
However, traditional inverse design methods usually demand extensive numerical
simulations and optimization procedures, which are time-consuming. In this
paper, we utilize deep learning for the inverse design of the transmission
spectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural
network (TNN), which can solve the one-to-many mapping problem that greatly
degrades the performance of deep-learning-based inverse designs. In general,
the TNN has been implemented by a back-to-back connection of an inverse neural
network and a pre-trained forward neural network, both of which have been
implemented based on multilayer perceptron (MLP) algorithms. In this paper, we
propose to use not only MLP, but also convolutional neural network (CNN) or
long short-term memory (LSTM) algorithms in the configuration of the TNN. We
show that an LSTM-LSTM-based TNN yields the highest accuracy but takes the
longest training time among nine configurations of TNNs. We also find that a
CNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed
because it could integrate the strengths of the CNN and LSTM algorithms.

</details>


### [4] [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
*Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的优化框架Omni-DPO，它通过同时考虑每个偏好对的质量和模型在这些偏好对上的表现来改进直接偏好优化（DPO），从而更有效地利用训练数据并提高性能。实验结果表明，Omni-DPO在文本理解和数学推理任务上均优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于DPO的方法通常将所有偏好对视为同等重要，忽略了它们内在质量和学习效用的差异，导致数据利用率不高和表现不佳。

**方法:** 提出了Omni-DPO，一个双重视角的优化框架，它联合考虑了每个偏好对的固有质量以及模型在这些偏好对上的演变表现，并且根据数据质量和模型的学习动态自适应地加权样本。

**结果:** 实验结果显示，在各种模型和基准测试中Omni-DPO表现出优越性和泛化能力。特别是在文本理解任务中，使用Omni-DPO微调的Gemma-2-9b-it比领先的大型语言模型Claude 3 Opus高出6.7分；在数学推理任务中，Omni-DPO在所有基准测试中持续优于基线方法。

**结论:** Omni-DPO通过更有效的训练数据利用达到了更好的性能，为直接偏好优化提供了一个强有力的改进方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Omni-DPO%3A+A+Dual-Perspective+Paradigm+for+Dynamic+Preference+Learning+of+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10054&send_immediately=true&force_search=false)

**原文摘要:** Direct Preference Optimization (DPO) has become a cornerstone of
reinforcement learning from human feedback (RLHF) due to its simplicity and
efficiency. However, existing DPO-based approaches typically treat all
preference pairs uniformly, ignoring critical variations in their inherent
quality and learning utility, leading to suboptimal data utilization and
performance. To address this challenge, we propose Omni-DPO, a dual-perspective
optimization framework that jointly accounts for (1) the inherent quality of
each preference pair and (2) the model's evolving performance on those pairs.
By adaptively weighting samples according to both data quality and the model's
learning dynamics during training, Omni-DPO enables more effective training
data utilization and achieves better performance. Experimental results on
various models and benchmarks demonstrate the superiority and generalization
capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it
finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant
margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning
tasks, Omni-DPO consistently outperforms the baseline methods across all
benchmarks, providing strong empirical evidence for the effectiveness and
robustness of our approach. Code and models will be available at
https://github.com/pspdada/Omni-DPO.

</details>


### [5] [Textual Bayes: Quantifying Uncertainty in LLM-Based Systems](https://arxiv.org/abs/2506.10060)
*Brendan Leigh Ross, Noël Vouitsis, Atiyeh Ashari Ghomi, Rasa Hosseinzadeh, Ji Xin, Zhaoyan Liu, Yi Sui, Shiyi Hou, Kin Kwan Leung, Gabriel Loaiza-Ganem, Jesse C. Cresswell*

**主要类别:** cs.LG

**AI概要:** 本文通过贝叶斯视角看待基于大型语言模型（LLM）的系统，将提示视为统计模型中的文本参数，并引入了一种新的马尔可夫链蒙特卡洛算法MHLP以执行贝叶斯推理。实验表明，该方法在预测准确性和不确定性量化方面都有所改进。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在解决具有挑战性的现实世界任务方面变得越来越强大，但准确量化它们的不确定性仍然是一个关键的开放问题，这限制了它们在高风险领域的应用性。此外，许多最先进的LLMs是闭源的黑盒性质，且基于LLM的系统对提示非常敏感，往往需要大量的手动调优。

**方法:** 本文将基于LLM的系统置于贝叶斯视角下，把提示解释为统计模型中的文本参数，并利用小规模训练数据集对这些提示进行贝叶斯推理。为此，提出了一种新颖的马尔可夫链蒙特卡洛（MCMC）算法——Metropolis-Hastings through LLM Proposals (MHLP)，它结合了提示优化技术和标准MCMC方法。

**结果:** 实证研究表明，本方法在一系列LLM基准和不确定性量化任务上提高了预测准确性和不确定性量化的效果。

**结论:** 这项工作展示了一条可行的道路，将丰富的贝叶斯文献方法整合到LLMs时代，为构建更可靠和校准良好的基于LLM的系统铺平了道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Textual+Bayes%3A+Quantifying+Uncertainty+in+LLM-Based+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10060，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10060&send_immediately=true&force_search=false)

**原文摘要:** Although large language models (LLMs) are becoming increasingly capable of
solving challenging real-world tasks, accurately quantifying their uncertainty
remains a critical open problem, which limits their applicability in
high-stakes domains. This challenge is further compounded by the closed-source,
black-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can
be highly sensitive to the prompts that bind them together, which often require
significant manual tuning (i.e., prompt engineering). In this work, we address
these challenges by viewing LLM-based systems through a Bayesian lens. We
interpret prompts as textual parameters in a statistical model, allowing us to
use a small training dataset to perform Bayesian inference over these prompts.
This novel perspective enables principled uncertainty quantification over both
the model's textual parameters and its downstream predictions, while also
incorporating prior beliefs about these parameters expressed in free-form text.
To perform Bayesian inference, a difficult problem even for well-studied data
modalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a
novel Markov chain Monte Carlo (MCMC) algorithm that combines prompt
optimization techniques with standard MCMC methods. MHLP is a turnkey
modification to existing LLM pipelines, including those that rely exclusively
on closed-source models. Empirically, we demonstrate that our method yields
improvements in both predictive accuracy and uncertainty quantification (UQ) on
a range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a
viable path for incorporating methods from the rich Bayesian literature into
the era of LLMs, paving the way for more reliable and calibrated LLM-based
systems.

</details>


### [6] [Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection](https://arxiv.org/abs/2506.10089)
*Dane Williamson, Yangfeng Ji, Matthew Dwyer*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Latent+Dimension+Allocation+in+Hierarchical+VAEs%3A+Balancing+Attenuation+and+Information+Retention+for+OOD+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10089&send_immediately=true&force_search=false)

**原文摘要:** Out-of-distribution (OOD) detection is a critical task in machine learning,
particularly for safety-critical applications where unexpected inputs must be
reliably flagged. While hierarchical variational autoencoders (HVAEs) offer
improved representational capacity over traditional VAEs, their performance is
highly sensitive to how latent dimensions are distributed across layers.
Existing approaches often allocate latent capacity arbitrarily, leading to
ineffective representations or posterior collapse. In this work, we introduce a
theoretically grounded framework for optimizing latent dimension allocation in
HVAEs, drawing on principles from information theory to formalize the trade-off
between information loss and representational attenuation. We prove the
existence of an optimal allocation ratio $r^{\ast}$ under a fixed latent
budget, and empirically show that tuning this ratio consistently improves OOD
detection performance across datasets and architectures. Our approach
outperforms baseline HVAE configurations and provides practical guidance for
principled latent structure design, leading to more robust OOD detection with
deep generative models.

</details>


### [7] [Efficient kernelized bandit algorithms via exploration distributions](https://arxiv.org/abs/2506.10091)
*Bingshan Hu, Zheng He, Danica J. Sutherland*

**主要类别:** cs.LG

**AI概要:** 本文提出了一类基于探索分布概念的高效核化bandit算法GP-Generic，该算法不仅包括基于置信上界的方法，也允许各种随机化方法，并且在适当选择探索分布的情况下，能够实现与现有算法相匹配的后悔界。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们考虑了一个具有紧凑臂集和未知但固定的奖励函数的问题，该奖励函数在一个再生核希尔伯特空间(RKHS)中具有有限范数。他们的动机是设计一类计算效率高的核化bandit算法。

**方法:** 提出了一个名为GP-Generic的算法类，它基于新颖的概念：探索分布。这类算法涵盖了基于置信上界的策略作为特殊情况，同时支持多种随机化算法。

**结果:** 通过精心选择探索分布，所提出的通用算法能够实现广泛的特定算法，这些算法达到$\tilde{O}(\gamma_T\sqrt{T})$遗憾界限，这与已知的基于UCB- 和 Thompson Sampling的算法结果一致。此外，实证显示，在实践中随机化可以产生更好的实际效果。

**结论:** GP-Generic算法提供了一个灵活的框架，允许结合不同的探索策略，从而在理论和实践上都表现出色，特别是在处理复杂的RKHS时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+kernelized+bandit+algorithms+via+exploration+distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10091，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10091&send_immediately=true&force_search=false)

**原文摘要:** We consider a kernelized bandit problem with a compact arm set ${X} \subset
\mathbb{R}^d $ and a fixed but unknown reward function $f^*$ with a finite norm
in some Reproducing Kernel Hilbert Space (RKHS). We propose a class of
computationally efficient kernelized bandit algorithms, which we call
GP-Generic, based on a novel concept: exploration distributions. This class of
algorithms includes Upper Confidence Bound-based approaches as a special case,
but also allows for a variety of randomized algorithms. With careful choice of
exploration distribution, our proposed generic algorithm realizes a wide range
of concrete algorithms that achieve $\tilde{O}(\gamma_T\sqrt{T})$ regret
bounds, where $\gamma_T$ characterizes the RKHS complexity. This matches known
results for UCB- and Thompson Sampling-based algorithms; we also show that in
practice, randomization can yield better practical results.

</details>


### [8] [Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders](https://arxiv.org/abs/2506.10094)
*Md. Faizul Islam Ansari*

**主要类别:** cs.LG

**AI概要:** 该研究通过两阶段深度自动编码器架构实现了先进的无监督聚类系统，用于MNIST手写数字。模型结合了批量归一化、dropout和权重衰减，并在重构误差上统一了KMeans聚类损失，以实现图像特征的卓越聚类性能。


<details>
  <summary>更多</summary>
  
**动机:** 开发一种能够有效对MNIST手写数字进行无监督聚类的方法，同时保持数据重构准确性和簇分离纯度。

**方法:** 采用两阶段方法：第一阶段训练深度神经自动编码器以最小化重构误差来获得紧凑且可解释的图像表示；第二阶段将重构误差与KMeans聚类损失相结合，通过联合距离目标函数优化学习到的潜在嵌入。模型还包括批量归一化、dropout和权重衰减技术。

**结果:** 框架在广泛的测试中表现出色，使用了轮廓系数、Davies-Bouldin指数等内在度量以及NMI和ARI等外在度量评估图像特征处理效果。t-SNE可视化展示了学习到的嵌入具有明显的数字簇。

**结论:** 所提出的方法在数据重构精度和簇分离纯度之间达到了最佳平衡，结果易于理解且实现可扩展，为大规模图像聚类应用中的无监督表征学习提供了可靠基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+Deep+Clustering+of+MNIST+with+Triplet-Enhanced+Convolutional+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10094，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10094&send_immediately=true&force_search=false)

**原文摘要:** This research implements an advanced unsupervised clustering system for MNIST
handwritten digits through two-phase deep autoencoder architecture. A deep
neural autoencoder requires a training process during phase one to develop
minimal yet interpretive representations of images by minimizing reconstruction
errors. During the second phase we unify the reconstruction error with a KMeans
clustering loss for learned latent embeddings through a joint distance-based
objective. Our model contains three elements which include batch normalization
combined with dropout and weight decay for achieving generalized and stable
results. The framework achieves superior clustering performance during
extensive tests which used intrinsic measurements including Silhouette Score
and Davies-Bouldin Index coupled with extrinsic metrics NMI and ARI when
processing image features. The research uses t-SNE visualization to present
learned embeddings that show distinct clusters for digits. Our approach reaches
an optimal combination between data reconstruction accuracy and cluster
separation purity when adding the benefit of understandable results and
scalable implementations. The approach creates a dependable base that helps
deploy unsupervised representation learning in different large-scale image
clustering applications.

</details>


### [9] [Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach](https://arxiv.org/abs/2506.10102)
*Ahmed Elbakary, Chaouki Ben Issaid, Mehdi Bennis*

**主要类别:** cs.LG

**AI概要:** 提出了一种新颖的联邦多任务学习方法，利用客户端之间的相似性来实现个性化学习。通过引入特征锚点和轻量级线性层，以及基于图的正则化和社区检测的方法，实现了高效的通信、协作与个性化。实验表明该方法在异构数据集上优于现有技术，并且具有更好的计算和通信效率，同时促进了客户端间的公平性。


<details>
  <summary>更多</summary>
  
**动机:** 为了实现每个客户端的个性化学习并避免将整个模型传输到参数服务器，研究者们旨在开发一种通信高效且能够促进客户端间积极协作的联邦多任务学习方法。

**方法:** 该论文提出的方法包括：1) 引入特征锚点——一个紧凑向量表示，用于总结从客户端本地类学到的特征；2) 客户端共享分类头，即一个轻量级线性层；3) 执行基于图的正则化以促进客户端间协作；4) 通过建模客户端之间协作的动态图，并持续更新此图来适应变化；5) 使用基于社区检测的方法划分动态图，以确保仅在高度相似的客户端之间进行协作。

**结果:** 广泛的实验结果显示，所提方法在两个异构数据集上显著优于最先进基线。此外，还展示了该方法在计算和通信效率方面的优越性，并且促进了客户端间的公平性。

**结论:** 提出的联邦多任务学习方法不仅能够有效地支持个性化学习，还能提高通信效率，增强跨客户端协作的效果，同时保持了良好的性能表现和公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Collaborate+Over+Graphs%3A+A+Selective+Federated+Multi-Task+Learning+Approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10102&send_immediately=true&force_search=false)

**原文摘要:** We present a novel federated multi-task learning method that leverages
cross-client similarity to enable personalized learning for each client. To
avoid transmitting the entire model to the parameter server, we propose a
communication-efficient scheme that introduces a feature anchor, a compact
vector representation that summarizes the features learned from the client's
local classes. This feature anchor is shared with the server to account for
local clients' distribution. In addition, the clients share the classification
heads, a lightweight linear layer, and perform a graph-based regularization to
enable collaboration among clients. By modeling collaboration between clients
as a dynamic graph and continuously updating and refining this graph, we can
account for any drift from the clients. To ensure beneficial knowledge transfer
and prevent negative collaboration, we leverage a community detection-based
approach that partitions this dynamic graph into homogeneous communities,
maximizing the sum of task similarities, represented as the graph edges'
weights, within each community. This mechanism restricts collaboration to
highly similar clients within their formed communities, ensuring positive
interaction and preserving personalization. Extensive experiments on two
heterogeneous datasets demonstrate that our method significantly outperforms
state-of-the-art baselines. Furthermore, we show that our method exhibits
superior computation and communication efficiency and promotes fairness across
clients.

</details>


### [10] [NnD: Diffusion-based Generation of Physically-Nonnegative Objects](https://arxiv.org/abs/2506.10112)
*Nadav Torem, Tamar Sde-Chen, Yoav Y. Schechner*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种非负扩散模型(NnD)，它是一种基于分数的生成模型，通过学习来显著降低物理上有意义且非负的对象的计算成本。NnD可以用于生成与云物理趋势一致的三维体积云。


<details>
  <summary>更多</summary>
  
**动机:** 许多现实世界的现象（如云形成）需要计算昂贵的模拟，这限制了可扩展性。为了大幅减少计算成本，作者提出了非负扩散(NnD)模型。

**方法:** 使用基于分数的扩散方法，并适应退火朗之万动力学以保证迭代场景生成和分析过程中保持非负性。NnD在高质量的物理模拟对象上进行训练。

**结果:** 成功生成了三维体积云，这些云由内在的非负微物理场组成，并且符合云物理学的趋势。专家感知无法将其区分成非物理现象。

**结论:** NnD提供了一种有效的方法来生成物理上有意义的非负对象，同时大大降低了所需的计算成本。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NnD%3A+Diffusion-based+Generation+of+Physically-Nonnegative+Objects，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10112，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10112&send_immediately=true&force_search=false)

**原文摘要:** Most natural objects have inherent complexity and variability. While some
simple objects can be modeled from first principles, many real-world phenomena,
such as cloud formation, require computationally expensive simulations that
limit scalability. This work focuses on a class of physically meaningful,
nonnegative objects that are computationally tractable but costly to simulate.
To dramatically reduce computational costs, we propose nonnegative diffusion
(NnD). This is a learned generative model using score based diffusion. It
adapts annealed Langevin dynamics to enforce, by design, non-negativity
throughout iterative scene generation and analysis (inference). NnD trains on
high-quality physically simulated objects. Once trained, it can be used for
generation and inference. We demonstrate generation of 3D volumetric clouds,
comprising inherently nonnegative microphysical fields. Our generated clouds
are consistent with cloud physics trends. They are effectively not
distinguished as non-physical by expert perception.

</details>


### [11] [GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments](https://arxiv.org/abs/2506.10120)
*Maryam Khalid, Akane Sano*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种新的基准测试框架GRAIL，用于评估动态真实环境中的图主动学习策略，引入了新度量来评估持续有效性、多样性和用户负担。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于图的主动学习方法通常在静态图数据集上进行评估，并且主要关注预测准确性，忽略了用户为中心的考量，例如采样多样性、查询公平性以及对动态环境的适应性。

**方法:** 提出了一个名为GRAIL的新基准测试框架，旨在动态和真实环境中评估图主动学习策略。该框架引入了新颖的指标来衡量持续有效性、多样性和用户负担。

**结果:** 通过在具有动态真实人类传感器数据的数据集上的广泛实验，揭示了预测性能与用户负担之间的权衡，并强调了现有主动学习策略的局限性。

**结论:** GRAIL证明了平衡节点重要性、查询多样性和网络拓扑的重要性，并为动态环境中的图主动学习解决方案提供了一个评估机制。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GRAIL%3A+A+Benchmark+for+GRaph+ActIve+Learning+in+Dynamic+Sensing+Environments，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10120，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10120&send_immediately=true&force_search=false)

**原文摘要:** Graph-based Active Learning (AL) leverages the structure of graphs to
efficiently prioritize label queries, reducing labeling costs and user burden
in applications like health monitoring, human behavior analysis, and sensor
networks. By identifying strategically positioned nodes, graph AL minimizes
data collection demands while maintaining model performance, making it a
valuable tool for dynamic environments. Despite its potential, existing graph
AL methods are often evaluated on static graph datasets and primarily focus on
prediction accuracy, neglecting user-centric considerations such as sampling
diversity, query fairness, and adaptability to dynamic settings. To bridge this
gap, we introduce GRAIL, a novel benchmarking framework designed to evaluate
graph AL strategies in dynamic, real-world environments. GRAIL introduces novel
metrics to assess sustained effectiveness, diversity, and user burden, enabling
a comprehensive evaluation of AL methods under varying conditions. Extensive
experiments on datasets featuring dynamic, real-life human sensor data reveal
trade-offs between prediction performance and user burden, highlighting
limitations in existing AL strategies. GRAIL demonstrates the importance of
balancing node importance, query diversity, and network topology, providing an
evaluation mechanism for graph AL solutions in dynamic environments.

</details>


### [12] [Meet Me at the Arm: The Cooperative Multi-Armed Bandits Problem with Shareable Arms](https://arxiv.org/abs/2506.10127)
*Xinyi Hu, Aldo Pacchiano*

**主要类别:** cs.LG

**AI概要:** 本文研究了无感知设置下的去中心化多人多臂老虎机问题，提出了一种名为A-CAPELLA的算法，该算法通过精心设计的碰撞模式实现了同步连续淘汰和容量估计，从而在未知臂容量的情况下达成了可证明的有效学习结果。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决一个更普遍化的多人多臂老虎机问题，在这个问题中每个臂都有未知的容量限制，并且玩家只能获得自己的奖励信息而无法得知是否与其他玩家发生碰撞。在这种反馈极其有限的条件下，协调玩家动作和发现臂的容量成为新的挑战。

**方法:** 提出了A-CAPELLA（一种用于学习与分配的容量感知并行消除算法），这是一种去中心化的算法，它使用协作假设检验协议，允许通过特定结构的碰撞模式进行同步连续消除和容量估计。

**结果:** A-CAPELLA 算法在未知臂容量的去中心化无感知MMAB环境中实现了对数级别的遗憾度表现。

**结论:** 通过A-CAPELLA算法，作者们为去中心化无感知MMAB问题提供了一个有效的解决方案，特别是在处理未知容量限制的场景时，能够实现高效的协调和学习。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meet+Me+at+the+Arm%3A+The+Cooperative+Multi-Armed+Bandits+Problem+with+Shareable+Arms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10127，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10127&send_immediately=true&force_search=false)

**原文摘要:** We study the decentralized multi-player multi-armed bandits (MMAB) problem
under a no-sensing setting, where each player receives only their own reward
and obtains no information about collisions. Each arm has an unknown capacity,
and if the number of players pulling an arm exceeds its capacity, all players
involved receive zero reward. This setting generalizes the classical
unit-capacity model and introduces new challenges in coordination and capacity
discovery under severe feedback limitations. We propose A-CAPELLA (Algorithm
for Capacity-Aware Parallel Elimination for Learning and Allocation), a
decentralized algorithm that achieves logarithmic regret in this generalized
regime. Our main contribution is a collaborative hypothesis testing protocol
that enables synchronized successive elimination and capacity estimation
through carefully structured collision patterns. This represents a provably
efficient learning result in decentralized no-sensing MMAB with unknown arm
capacities.

</details>


### [13] [Provable Sim-to-Real Transfer via Offline Domain Randomization](https://arxiv.org/abs/2506.10133)
*Arnaud Fickinger, Abderrahim Bendahi, Stuart Russell*

**主要类别:** cs.LG

**AI概要:** 本文研究了离线域随机化(ODR)方法，通过将模拟器参数分布拟合到离线数据集来减少仿真到现实的差距。作者提出了ODR的形式化定义，并证明了在温和的正则性和可识别性条件下估计量的一致性。此外，他们还推导出了仿真到现实误差的界，并引入了一个新的算法E-DROPO以提高零样本迁移的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习代理从仿真环境部署到现实世界时经常遇到困难。虽然领域随机化(DR)是一种减少仿真与现实差距的主要策略，但它忽略了已经可以从真实系统中获得的离身数据。因此，本研究旨在探索一种利用这些离身数据的方法，即离线领域随机化(ODR)，并为这种方法提供理论基础。

**方法:** 文章首先将离线领域随机化（ODR）形式化为一个参数化模拟器族的最大似然估计问题。然后，对这个估计器的一致性进行了证明，在温和的正则性和可识别性条件下表明随着数据集的增长它会收敛于真实的动态。接着，得出了仿真到现实误差的界限，显示ODR相比均匀DR具有更紧致的误差范围。最后，提出了一种新版本DROPO算法——E-DROPO，该算法增加了一个熵奖励以避免方差崩溃，从而在实践中实现更广泛的随机化和更强健的零样本迁移。

**结果:** 研究表明，当使用有限数量的模拟器时，ODR的仿真到现实误差可以比传统的均匀DR最多减少O(M)倍（对于连续设置也有类似的优势）。同时，所提出的E-DROPO算法不仅防止了方差崩溃，还在实践中展示了更广泛的随机化以及更加稳健的零样本迁移性能。

**结论:** 本文为离线域随机化(ODR)提供了坚实的理论依据，并通过新的E-DROPO算法展示了其在减少仿真至实际转换过程中错误方面的潜力。这些发现有助于开发出更能适应现实世界变化的强化学习系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provable+Sim-to-Real+Transfer+via+Offline+Domain+Randomization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10133，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10133&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement-learning agents often struggle when deployed from simulation to
the real-world. A dominant strategy for reducing the sim-to-real gap is domain
randomization (DR) which trains the policy across many simulators produced by
sampling dynamics parameters, but standard DR ignores offline data already
available from the real system. We study offline domain randomization (ODR),
which first fits a distribution over simulator parameters to an offline
dataset. While a growing body of empirical work reports substantial gains with
algorithms such as DROPO, the theoretical foundations of ODR remain largely
unexplored. In this work, we (i) formalize ODR as a maximum-likelihood
estimation over a parametric simulator family, (ii) prove consistency of this
estimator under mild regularity and identifiability conditions, showing it
converges to the true dynamics as the dataset grows, (iii) derive gap bounds
demonstrating ODRs sim-to-real error is up to an O(M) factor tighter than
uniform DR in the finite-simulator case (and analogous gains in the continuous
setting), and (iv) introduce E-DROPO, a new version of DROPO which adds an
entropy bonus to prevent variance collapse, yielding broader randomization and
more robust zero-shot transfer in practice.

</details>


### [14] [Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning](https://arxiv.org/abs/2506.10137)
*Daniel Lawson, Adriana Hugessen, Charlotte Cloutier, Glen Berseth, Khimya Khetarpal*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的表示学习目标，称为BYOL-γ增强的GCBC，它能够在无需对比样本或时序差分学习的情况下理论上近似后继表示，并在需要组合泛化的挑战性任务中表现出竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于监督学习的行为克隆方法，特别是目标条件下的行为克隆（GCBC），虽然能够很好地完成训练集内的任务，但在面对新状态-目标对的任务时缺乏零样本泛化能力。这种局限性部分归因于行为克隆所学的状态表征缺乏时间一致性。为了促进组合泛化，作者提出利用后继表征来编码从当前状态出发未来可能访问到的状态分布。

**方法:** 提出的方法是BYOL-γ增强的目标条件行为克隆（GCBC），这是一种新的表示学习目标，它不需要对比样本或者时序差分学习就能在有限马尔可夫决策过程情况下近似后继表示。

**结果:** 实验结果表明，BYOL-γ增强的GCBC不仅能在理论上近似后继表示，而且在一系列要求组合泛化的具有挑战性的任务上取得了有竞争力的实际表现。

**结论:** 通过引入BYOL-γ增强的目标条件行为克隆，该研究为解决组合泛化问题提供了一个简单而有效的方案，从而改善了模型对于未见状态-目标对的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Predictive+Representations+for+Combinatorial+Generalization+in+Behavioral+Cloning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10137&send_immediately=true&force_search=false)

**原文摘要:** Behavioral cloning (BC) methods trained with supervised learning (SL) are an
effective way to learn policies from human demonstrations in domains like
robotics. Goal-conditioning these policies enables a single generalist policy
to capture diverse behaviors contained within an offline dataset. While
goal-conditioned behavior cloning (GCBC) methods can perform well on
in-distribution training tasks, they do not necessarily generalize zero-shot to
tasks that require conditioning on novel state-goal pairs, i.e. combinatorial
generalization. In part, this limitation can be attributed to a lack of
temporal consistency in the state representation learned by BC; if temporally
related states are encoded to similar latent representations, then the
out-of-distribution gap for novel state-goal pairs would be reduced. Hence,
encouraging this temporal consistency in the representation space should
facilitate combinatorial generalization. Successor representations, which
encode the distribution of future states visited from the current state, nicely
encapsulate this property. However, previous methods for learning successor
representations have relied on contrastive samples, temporal-difference (TD)
learning, or both. In this work, we propose a simple yet effective
representation learning objective, $\text{BYOL-}\gamma$ augmented GCBC, which
is not only able to theoretically approximate the successor representation in
the finite MDP case without contrastive samples or TD learning, but also,
results in competitive empirical performance across a suite of challenging
tasks requiring combinatorial generalization.

</details>


### [15] [Survival Analysis as Imprecise Classification with Trainable Kernels](https://arxiv.org/abs/2506.10140)
*Andrei V. Konstantinov, Vlada A. Efremenko, Lev V. Utkin*

**主要类别:** cs.LG

**AI概要:** 本文提出了三种新的生存模型iSurvM、iSurvQ和iSurvJ，它们结合了不精确概率论与注意力机制来处理删失数据。实验表明这些模型在准确性和计算复杂度方面优于传统的Beran估计量。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法如Beran估计量在处理复杂的资料结构和严重删失时存在困难，而本文旨在提出不需要参数假设的方法来处理删失数据。

**方法:** 提出基于平均似然函数的不精确生存模型iSurvM、基于似然函数分位数的不精确生存模型iSurvQ以及基于联合学习的不精确生存模型iSurvJ；使用核基Nadaraya-Watson回归并带有可训练注意力权重来计算整个数据集的时间间隔上的不精确概率分布；考虑了对应于三个所提议模型的三种决策策略。

**结果:** 实验显示，特别是在合成数据集和真实数据集上，所提出的模型尤其是iSurvJ，在准确性和计算复杂度方面始终优于Beran估计量。

**结论:** 新提出的生存分析模型有效地解决了删失观测问题，并且在性能上超越了现有的非参数方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Survival+Analysis+as+Imprecise+Classification+with+Trainable+Kernels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10140，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10140&send_immediately=true&force_search=false)

**原文摘要:** Survival analysis is a fundamental tool for modeling time-to-event data in
healthcare, engineering, and finance, where censored observations pose
significant challenges. While traditional methods like the Beran estimator
offer nonparametric solutions, they often struggle with the complex data
structures and heavy censoring. This paper introduces three novel survival
models, iSurvM (the imprecise Survival model based on Mean likelihood
functions), iSurvQ (the imprecise Survival model based on the Quantiles of
likelihood functions), and iSurvJ (the imprecise Survival model based on the
Joint learning), that combine imprecise probability theory with attention
mechanisms to handle censored data without parametric assumptions. The first
idea behind the models is to represent censored observations by interval-valued
probability distributions for each instance over time intervals between events
moments. The second idea is to employ the kernel-based Nadaraya-Watson
regression with trainable attention weights for computing the imprecise
probability distribution over time intervals for the entire dataset. The third
idea is to consider three decision strategies for training, which correspond to
the proposed three models. Experiments on synthetic and real datasets
demonstrate that the proposed models, especially iSurvJ, consistently
outperform the Beran estimator from the accuracy and computational complexity
points of view. Codes implementing the proposed models are publicly available.

</details>


### [16] [Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138)
*Mohammad Taufeeque, Aaron David Tucker, Adam Gleave, Adrià Garriga-Alonso*

**主要类别:** cs.LG

**AI概要:** 本文通过逆向工程分析了一个用于玩推箱子游戏的卷积递归神经网络，揭示了其内部机制与双向搜索组件类似，并且在测试时计算利用方面有一些不同于经典搜索算法的特点。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于理解一个通过无模型强化学习训练来玩推箱游戏的卷积循环神经网络，在测试阶段增加计算量能够解决更多关卡的原因。

**方法:** 采用部分逆向工程的方法，分析了该神经网络中类似于经典双向搜索组件的工作机制。

**结果:** 发现对于每个方格，RNN 在与特定方向相关的通道激活中表示计划；这些状态-动作激活类似于价值函数，它们的大小决定了何时回溯以及哪个计划分支存活下来；专门的内核向前和向后扩展这些激活（包含计划和价值）以创建路径，形成转换模型。

**结论:** 该网络虽然在某些方面与经典搜索不同，但其通过无模型训练学到的机制可以被理解为熟悉的术语，这表明深度学习系统中的决策过程是可以解释的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpreting+learned+search%3A+finding+a+transition+model+and+value+function+in+an+RNN+that+plays+Sokoban，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10138，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10138&send_immediately=true&force_search=false)

**原文摘要:** We partially reverse-engineer a convolutional recurrent neural network (RNN)
trained to play the puzzle game Sokoban with model-free reinforcement learning.
Prior work found that this network solves more levels with more test-time
compute. Our analysis reveals several mechanisms analogous to components of
classic bidirectional search. For each square, the RNN represents its plan in
the activations of channels associated with specific directions. These
state-action activations are analogous to a value function - their magnitudes
determine when to backtrack and which plan branch survives pruning. Specialized
kernels extend these activations (containing plan and value) forward and
backward to create paths, forming a transition model. The algorithm is also
unlike classical search in some ways. State representation is not unified;
instead, the network considers each box separately. Each layer has its own plan
representation and value function, increasing search depth. Far from being
inscrutable, the mechanisms leveraging test-time compute learned in this
network by model-free training can be understood in familiar terms.

</details>


### [17] [Probabilistic Variational Contrastive Learning](https://arxiv.org/abs/2506.10159)
*Minoh Jeong, Seonho Kim, Alfred Hero*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的对比学习框架——变分对比学习（VCL），该框架通过将InfoNCE损失解释为替代重构项，并在单位超球面上添加KL散度正则化项到均匀先验，来最大化证据下界。VCL能够提供有意义的不确定性估计，同时在分类准确性上与确定性基线相匹配或超越。


<details>
  <summary>更多</summary>
  
**动机:** 现有的对比学习方法如SimCLR和SupCon虽然取得了最先进的性能，但缺乏一种原则性的机制来量化不确定性。

**方法:** 提出了一个无解码器的框架——变分对比学习（VCL），它通过将InfoNCE损失视为代理重构项，并向单位超球体上的统一先验添加KL散度正则化项，来最大化证据下界(ELBO)。近似后验$q_θ(z|x)$被建模为投影正态分布，从而可以抽样概率嵌入。

**结果:** 实验表明，VCL减轻了维度坍缩现象，增强了与类别标签的互信息，并且在分类准确率方面与确定性基线相匹配或优于它们。此外，VCL还提供了通过后验模型得出的有意义的不确定性估计。

**结论:** VCL为对比学习提供了一个概率基础，可以作为对比学习方法的新基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probabilistic+Variational+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10159，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10159&send_immediately=true&force_search=false)

**原文摘要:** Deterministic embeddings learned by contrastive learning (CL) methods such as
SimCLR and SupCon achieve state-of-the-art performance but lack a principled
mechanism for uncertainty quantification. We propose Variational Contrastive
Learning (VCL), a decoder-free framework that maximizes the evidence lower
bound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction
term and adding a KL divergence regularizer to a uniform prior on the unit
hypersphere. We model the approximate posterior $q_\theta(z|x)$ as a projected
normal distribution, enabling the sampling of probabilistic embeddings. Our two
instantiations--VSimCLR and VSupCon--replace deterministic embeddings with
samples from $q_\theta(z|x)$ and incorporate a normalized KL term into the
loss. Experiments on multiple benchmarks demonstrate that VCL mitigates
dimensional collapse, enhances mutual information with class labels, and
matches or outperforms deterministic baselines in classification accuracy, all
the while providing meaningful uncertainty estimates through the posterior
model. VCL thus equips contrastive learning with a probabilistic foundation,
serving as a new basis for contrastive approaches.

</details>


### [18] [Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models](https://arxiv.org/abs/2506.10177)
*Defang Chen, Zhenyu Zhou, Can Wang, Siwei Lyu*

**主要类别:** cs.LG

**AI概要:** 本文揭示了基于扩散的生成模型中确定性采样动态的几何规律性，并提出了一种基于动态规划的方法来更好地对齐采样时间计划与轨迹结构，从而以最小的修改和计算开销提高了图像生成性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们观察到在基于扩散的生成模型中，尽管模型架构、应用条件或生成内容不同，模拟的每个采样轨迹都位于一个极低维的子空间内，并且所有轨迹都表现出几乎相同的“回力镖”形状。

**方法:** 通过对基于SDEs和ODEs的生成模型进行分析，研究者们发现了采样轨迹的一些有趣特性，并提出了基于动态规划的方案来改进采样时间表。

**结果:** 所提出的简单策略仅需对现有的基于ODE的数值求解器做微小改动，不会带来明显的计算负担，并且尤其在仅有5~10次函数评估的区域中能够实现更好的图像生成表现。

**结论:** 发现的轨迹规律性不仅加深了对基于扩散模型的理解，还为提高这类模型效率提供了一个实用方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Geometric+Regularity+in+Deterministic+Sampling+of+Diffusion-based+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10177，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10177&send_immediately=true&force_search=false)

**原文摘要:** Diffusion-based generative models employ stochastic differential equations
(SDEs) and their equivalent probability flow ordinary differential equations
(ODEs) to establish a smooth transformation between complex high-dimensional
data distributions and tractable prior distributions. In this paper, we reveal
a striking geometric regularity in the deterministic sampling dynamics: each
simulated sampling trajectory lies within an extremely low-dimensional
subspace, and all trajectories exhibit an almost identical ''boomerang'' shape,
regardless of the model architecture, applied conditions, or generated content.
We characterize several intriguing properties of these trajectories,
particularly under closed-form solutions based on kernel-estimated data
modeling. We also demonstrate a practical application of the discovered
trajectory regularity by proposing a dynamic programming-based scheme to better
align the sampling time schedule with the underlying trajectory structure. This
simple strategy requires minimal modification to existing ODE-based numerical
solvers, incurs negligible computational overhead, and achieves superior image
generation performance, especially in regions with only $5 \sim 10$ function
evaluations.

</details>


### [19] [Physiological-Model-Based Neural Network for Heart Rate Estimation during Daily Physical Activities](https://arxiv.org/abs/2506.10144)
*Yaowen Zhang, Libera Fresiello, Peter H. Veltink, Dirk W. Donker, Ying Wang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种基于生理模型的神经网络(PMB-NN)框架，用于根据日常体力活动中的摄氧量(VO2)数据估计心率(HR)。该框架在12名参与者的个体数据集上进行了训练和测试，通过将生理约束嵌入到神经网络训练过程中，PMB-NN模型遵循了人体生理原则，并且实现了高精度的心率估计。与基准神经网络模型相比，PMB-NN的表现相当，但显著优于传统的生理模型。此外，PMB-NN能够识别PM的个性化参数，使得PM可以生成合理的心率估计。


<details>
  <summary>更多</summary>
  
**动机:** 心脏衰竭（HF）是一个重要的全球健康挑战，早期检测能改善预后。心率异常可能是HF风险的早期指标，但现有的HF检测工具依赖于基于人群的平均值，限制了其可靠性。为了更精确地追踪心脏健康生物标志物，研究人员希望开发一种个体化的心率估计方法，它既高效又具有可解释性。

**方法:** 研究者引入了一种基于生理模型的神经网络（PMB-NN）框架，用于根据日常体力活动期间的摄氧量（VO2）数据来估计心率。该框架在进行休息、骑行和跑步等活动的12名参与者的数据集上进行了训练和测试。通过将从简化的人体运动生理模型中得出的生理约束嵌入到神经网络训练过程中，PMB-NN模型能够在遵循人体生理原则的同时实现高估测准确性。

**结果:** PMB-NN模型达到了高的估算准确度，中位R^2分数为0.8，RMSE为8.3 bpm。统计比较分析表明，PMB-NN模型性能与基准神经网络模型相当，但明显优于传统生理模型(p=0.002)。另外，PMB-NN还擅长识别PM的个性化参数，这使PM能够产生合理的心率估计。

**结论:** 所提出的PMB-NN框架结合了生理模型和神经网络的优势，不仅提高了心率估计的准确性，而且保持了良好的可解释性。这种框架为未来日常生活活动中个性化和实时心脏监测提供了可能性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Physiological-Model-Based+Neural+Network+for+Heart+Rate+Estimation+during+Daily+Physical+Activities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10144，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10144&send_immediately=true&force_search=false)

**原文摘要:** Heart failure (HF) poses a significant global health challenge, with early
detection offering opportunities for improved outcomes. Abnormalities in heart
rate (HR), particularly during daily activities, may serve as early indicators
of HF risk. However, existing HR monitoring tools for HF detection are limited
by their reliability on population-based averages. The estimation of
individualized HR serves as a dynamic digital twin, enabling precise tracking
of cardiac health biomarkers. Current HR estimation methods, categorized into
physiologically-driven and purely data-driven models, struggle with efficiency
and interpretability. This study introduces a novel physiological-model-based
neural network (PMB-NN) framework for HR estimation based on oxygen uptake
(VO2) data during daily physical activities. The framework was trained and
tested on individual datasets from 12 participants engaged in activities
including resting, cycling, and running. By embedding physiological
constraints, which were derived from our proposed simplified human movement
physiological model (PM), into the neural network training process, the PMB-NN
model adheres to human physiological principles while achieving high estimation
accuracy, with a median R$^2$ score of 0.8 and an RMSE of 8.3 bpm. Comparative
statistical analysis demonstrates that the PMB-NN achieves performance on par
with the benchmark neural network model while significantly outperforming
traditional physiological model (p=0.002). In addition, our PMB-NN is adept at
identifying personalized parameters of the PM, enabling the PM to generate
reasonable HR estimation. The proposed framework with a precise VO2 estimation
system derived from body movements enables the future possibilities of
personalized and real-time cardiac monitoring during daily life physical
activities.

</details>


### [20] [Meta-learning Representations for Learning from Multiple Annotators](https://arxiv.org/abs/2506.10259)
*Atsutoshi Kumagai, Tomoharu Iwata, Taishi Nishiyama, Yasutoshi Ida, Yasuhiro Fujiwara*

**主要类别:** cs.LG

**AI概要:** 提出了一种元学习方法，用于从多个噪声标注者中学习。该方法通过神经网络将每个任务示例嵌入到潜在空间，并构建概率模型以在估计标注者能力的同时学习特定于任务的分类器。神经网络被元学习以提高当分类器适应少量标注数据时的预期测试分类性能。


<details>
  <summary>更多</summary>
  
**动机:** 在众包服务等许多应用中，监督学习的标签由多个标注者提供。由于标注者具有不同的技能或偏见，给出的标签可能是有噪声的。为了学习准确的分类器，现有方法需要大量的噪声标注数据，但在实践中可能无法获得足够的数据。

**方法:** 所提出的方法利用在不同但相关任务中获得的标注数据。使用神经网络将每个任务中的示例嵌入到潜在空间，并构建一个概率模型来学习特定任务的分类器，同时估计标注者在这个潜在空间上的能力。神经网络被元学习以改进当分类器适应给定的小量标注数据时的期望测试分类性能。分类器的适应是通过最大化后验概率经由期望最大化（EM）算法完成的。

**结果:** 展示了该方法在合成噪声的真实世界数据集和真实世界众包数据集上的有效性。

**结论:** 所提出的元学习方法能够有效地从有限且含噪声的标注数据中学习，并且能够在相关的不同任务之间迁移知识，从而改善分类器的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meta-learning+Representations+for+Learning+from+Multiple+Annotators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10259，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10259&send_immediately=true&force_search=false)

**原文摘要:** We propose a meta-learning method for learning from multiple noisy
annotators. In many applications such as crowdsourcing services, labels for
supervised learning are given by multiple annotators. Since the annotators have
different skills or biases, given labels can be noisy. To learn accurate
classifiers, existing methods require many noisy annotated data. However,
sufficient data might be unavailable in practice. To overcome the lack of data,
the proposed method uses labeled data obtained in different but related tasks.
The proposed method embeds each example in tasks to a latent space by using a
neural network and constructs a probabilistic model for learning a
task-specific classifier while estimating annotators' abilities on the latent
space. This neural network is meta-learned to improve the expected test
classification performance when the classifier is adapted to a given small
amount of annotated data. This classifier adaptation is performed by maximizing
the posterior probability via the expectation-maximization (EM) algorithm.
Since each step in the EM algorithm is easily computed as a closed-form and is
differentiable, the proposed method can efficiently backpropagate the loss
through the EM algorithm to meta-learn the neural network. We show the
effectiveness of our method with real-world datasets with synthetic noise and
real-world crowdsourcing datasets.

</details>


### [21] [Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors](https://arxiv.org/abs/2506.10146)
*Tejaswi Kasarla, Max van Spengler, Pascal Mettes*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种平衡双曲学习方法，通过优化层次失真和浅层与宽子层次之间的平衡来嵌入类别，并使用这些嵌入作为双曲原型来进行分类。实验表明，该方法在13个数据集和13个评分函数上优于现有的离群分布方法。


<details>
  <summary>更多</summary>
  
**动机:** 研究的动机是为了解决深度学习中一个重要的问题，即识别出不属于网络训练时所用分布的样本。

**方法:** 提出了平衡双曲学习，定义了一个双曲类嵌入算法，该算法共同优化了层次失真以及浅层和宽子层次间的平衡。然后将类嵌入用作双曲原型以进行分布内数据的分类。此外，还介绍了如何将现有的离群分布评分函数推广到可以使用双曲原型工作。

**结果:** 实证评估表明，在相同的训练数据和相同的骨干网络条件下，所提出的双曲嵌入在13个数据集和13个评分函数上超越了现有的离群分布方法。同时，它们也优于其他双曲方法、最新的对比方法，并且能够自然地实现层次化的离群分布泛化。

**结论:** 良好的分层双曲嵌入对于区分分布内和分布外样本是优选的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Balanced+Hyperbolic+Embeddings+Are+Natural+Out-of-Distribution+Detectors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10146&send_immediately=true&force_search=false)

**原文摘要:** Out-of-distribution recognition forms an important and well-studied problem
in deep learning, with the goal to filter out samples that do not belong to the
distribution on which a network has been trained. The conclusion of this paper
is simple: a good hierarchical hyperbolic embedding is preferred for
discriminating in- and out-of-distribution samples. We introduce Balanced
Hyperbolic Learning. We outline a hyperbolic class embedding algorithm that
jointly optimizes for hierarchical distortion and balancing between shallow and
wide subhierarchies. We then use the class embeddings as hyperbolic prototypes
for classification on in-distribution data. We outline how to generalize
existing out-of-distribution scoring functions to operate with hyperbolic
prototypes. Empirical evaluations across 13 datasets and 13 scoring functions
show that our hyperbolic embeddings outperform existing out-of-distribution
approaches when trained on the same data with the same backbones. We also show
that our hyperbolic embeddings outperform other hyperbolic approaches, beat
state-of-the-art contrastive methods, and natively enable hierarchical
out-of-distribution generalization.

</details>


### [22] [Collaborative Min-Max Regret in Grouped Multi-Armed Bandits](https://arxiv.org/abs/2506.10313)
*Moïse Blanchard, Vineet Goyal*

**主要类别:** cs.LG

**AI概要:** 本文研究了在分组场景下多臂老虎机中共享探索的影响，引入了Col-UCB算法以动态协调各组之间的探索，并证明该算法达到了最优的最小最大和实例依赖的合作遗憾界。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在解决在分组多臂老虎机环境中，如何平衡不同组间或人群间的探索负担问题，因为标准算法可能导致组间的探索成本显著不平衡。

**方法:** 提出了一种名为Col-UCB的新算法，用于动态地跨组协调探索。

**结果:** Col-UCB算法实现了最优的最小最大和实例依赖的合作遗憾界，这些界限适应于组间共享动作集的结构。

**结论:** Col-UCB算法能够有效地减少合作遗憾，并且当组之间存在共享的动作集合时，协作可以比每个组独立学习最佳行动带来显著的好处。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Collaborative+Min-Max+Regret+in+Grouped+Multi-Armed+Bandits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10313，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10313&send_immediately=true&force_search=false)

**原文摘要:** We study the impact of sharing exploration in multi-armed bandits in a
grouped setting where a set of groups have overlapping feasible action sets
[Baek and Farias '24]. In this grouped bandit setting, groups share reward
observations, and the objective is to minimize the collaborative regret,
defined as the maximum regret across groups. This naturally captures
applications in which one aims to balance the exploration burden between groups
or populations -- it is known that standard algorithms can lead to
significantly imbalanced exploration cost between groups. We address this
problem by introducing an algorithm Col-UCB that dynamically coordinates
exploration across groups. We show that Col-UCB achieves both optimal minimax
and instance-dependent collaborative regret up to logarithmic factors. These
bounds are adaptive to the structure of shared action sets between groups,
providing insights into when collaboration yields significant benefits over
each group learning their best action independently.

</details>


### [23] [Air in Your Neighborhood: Fine-Grained AQI Forecasting Using Mobile Sensor Data](https://arxiv.org/abs/2506.10332)
*Aaryam Sharma*

**主要类别:** cs.LG

**AI概要:** 本文使用时空图神经网络（Spatio-temporal GNNs）来预测1平方公里范围内的空气质量指数(AQI)，以AirDelhi数据集为例，相比现有方法显著降低了均方误差，并揭示了AQI的新见解。


<details>
  <summary>更多</summary>
  
**动机:** 在发展中国家，空气污染已经成为一个重要的健康风险。尽管政府经常发布空气质量指数（AQI）数据来跟踪污染情况，但这些数值未能反映局部实际情况，因为传感器通常非常稀疏。

**方法:** 采用时空图神经网络（Spatio-temporal GNNs）对1平方公里范围的AQI进行预测，以AirDelhi数据集为案例研究。

**结果:** 使用该方法后，在未见过的坐标上相较于已有工作降低了71.654 MSE，即减少了79%的均方误差；同时发现了关于AQI的新洞察，比如存在强烈的短期重复模式和变化的空间关系。

**结论:** 本研究表明，通过运用时空图神经网络可以更准确地预测小区域范围内的AQI，并且提供了有关AQI特性的新理解。此外，研究代码已公开于GitHub。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Air+in+Your+Neighborhood%3A+Fine-Grained+AQI+Forecasting+Using+Mobile+Sensor+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10332&send_immediately=true&force_search=false)

**原文摘要:** Air pollution has become a significant health risk in developing countries.
While governments routinely publish air-quality index (AQI) data to track
pollution, these values fail to capture the local reality, as sensors are often
very sparse. In this paper, we address this gap by predicting AQI in 1 km^2
neighborhoods, using the example of AirDelhi dataset. Using Spatio-temporal
GNNs we surpass existing works by 71.654 MSE a 79% reduction, even on unseen
coordinates. New insights about AQI such as the existence of strong repetitive
short-term patterns and changing spatial relations are also discovered. The
code is available on GitHub.

</details>


### [24] [The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset](https://arxiv.org/abs/2506.10165)
*Gilad Landau, Miran Özdogan, Gereon Elvers, Francesco Mantegna, Pratik Somaiya, Dulhan Jayalath, Luisa Kurth, Teyun Kwon, Brendan Shillingford, Greg Farquhar, Minqi Jiang, Karim Jerbi, Hamza Abdelhedi, Yorguin Mantilla Ramos, Caglar Gulcehre, Mark Woolrich, Natalie Voets, Oiwi Parker Jones*

**主要类别:** cs.LG

**AI概要:** 摘要介绍了2025 PNPL竞赛的目标，即通过利用机器学习社区的力量来推动非侵入性神经解码技术的发展，特别是为了帮助有言语障碍的瘫痪者恢复沟通能力。为此，提供了迄今为止最大的单个受试者MEG数据集LibriBrain和一个便于访问该数据集的Python库pnpl。定义了两个基础任务（语音检测和从脑数据中进行音素分类），并设置了标准的数据分割、评估指标、基准模型、在线教程代码、讨论板和公开排行榜。比赛分为强调算法创新的标准赛道和可能奖励更大规模计算的扩展赛道。


<details>
  <summary>更多</summary>
  
**动机:** 促进非侵入性脑-机接口技术的进步，以帮助因言语缺陷而无法正常交流的瘫痪个体恢复沟通能力，避免高风险外科手术的需求。

**方法:** 创建了最大的单个受试者MEG数据集LibriBrain，并开发了一个易于使用的Python库pnpl用于数据访问；定义了两个基础任务：从脑数据中检测语音和音素分类；提供标准化数据划分、评价指标、示例基准模型、在线教程代码、社区讨论板块以及公开提交排行榜。

**结果:** 为2025 PNPL竞赛设立了框架，包括两大任务、数据集、工具库、基准模型及在线资源等，旨在鼓励机器学习社区参与并加速非侵入性神经解码领域的发展。

**结论:** 通过设立2025 PNPL竞赛及相关资源，期望能够汇聚机器学习领域的力量，在非侵入性脑-机接口尤其是言语恢复方面实现突破。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+2025+PNPL+Competition%3A+Speech+Detection+and+Phoneme+Classification+in+the+LibriBrain+Dataset，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10165&send_immediately=true&force_search=false)

**原文摘要:** The advance of speech decoding from non-invasive brain data holds the
potential for profound societal impact. Among its most promising applications
is the restoration of communication to paralysed individuals affected by speech
deficits such as dysarthria, without the need for high-risk surgical
interventions. The ultimate aim of the 2025 PNPL competition is to produce the
conditions for an "ImageNet moment" or breakthrough in non-invasive neural
decoding, by harnessing the collective power of the machine learning community.
  To facilitate this vision we present the largest within-subject MEG dataset
recorded to date (LibriBrain) together with a user-friendly Python library
(pnpl) for easy data access and integration with deep learning frameworks. For
the competition we define two foundational tasks (i.e. Speech Detection and
Phoneme Classification from brain data), complete with standardised data splits
and evaluation metrics, illustrative benchmark models, online tutorial code, a
community discussion board, and public leaderboard for submissions. To promote
accessibility and participation the competition features a Standard track that
emphasises algorithmic innovation, as well as an Extended track that is
expected to reward larger-scale computing, accelerating progress toward a
non-invasive brain-computer interface for speech.

</details>


### [25] [Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning](https://arxiv.org/abs/2506.10378)
*Jikai Jin, Vasilis Syrgkanis, Sham Kakade, Hanlin Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种因果表示学习框架来评估语言模型的能力，通过控制基础模型的混淆因素识别出三个潜在能力因子之间的线性因果结构，并揭示了从一般问题解决能力到指令遵循熟练度再到数学推理能力的清晰因果方向。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服在对语言模型进行严格因果评估时遇到的方法论挑战，包括复杂的混淆效应和与广泛再训练相关的计算成本问题。

**方法:** 采用一种因果表示学习框架，将观察到的基准性能建模为少数几个潜在能力因子的线性变换，并且这些潜在因子被确定为在适当控制基础模型作为共同混杂因素之后存在因果关系。

**结果:** 应用这种方法于包含超过1500个模型的数据集上，这些模型在六个基准测试中进行了评估，研究者发现了一个简洁的三节点线性因果结构，它能够可靠地解释所观察到的性能变化。

**结论:** 结果强调了在评估过程中仔细控制基础模型变体的重要性，这是准确揭示潜在模型能力之间因果关系的关键步骤。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discovering+Hierarchical+Latent+Capabilities+of+Language+Models+via+Causal+Representation+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10378，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10378&send_immediately=true&force_search=false)

**原文摘要:** Faithful evaluation of language model capabilities is crucial for deriving
actionable insights that can inform model development. However, rigorous causal
evaluations in this domain face significant methodological challenges,
including complex confounding effects and prohibitive computational costs
associated with extensive retraining. To tackle these challenges, we propose a
causal representation learning framework wherein observed benchmark performance
is modeled as a linear transformation of a few latent capability factors.
Crucially, these latent factors are identified as causally interrelated after
appropriately controlling for the base model as a common confounder. Applying
this approach to a comprehensive dataset encompassing over 1500 models
evaluated across six benchmarks from the Open LLM Leaderboard, we identify a
concise three-node linear causal structure that reliably explains the observed
performance variations. Further interpretation of this causal structure
provides substantial scientific insights beyond simple numerical rankings:
specifically, we reveal a clear causal direction starting from general
problem-solving capabilities, advancing through instruction-following
proficiency, and culminating in mathematical reasoning ability. Our results
underscore the essential role of carefully controlling base model variations
during evaluation, a step critical to accurately uncovering the underlying
causal relationships among latent model capabilities.

</details>


### [26] [Wasserstein Barycenter Soft Actor-Critic](https://arxiv.org/abs/2506.10167)
*Zahra Shahrooei, Ali Baheri*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的算法WBSAC，通过结合悲观和乐观策略的Wasserstein重心来指导探索，从而提高稀疏奖励环境中的样本效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的深度离线策略actor-critic算法在连续控制领域中表现出色，但它们在样本效率方面表现不佳，特别是在稀疏奖励环境中。

**方法:** 提出了Wasserstein Barycenter Soft Actor-Critic (WBSAC) 算法，该算法使用一个悲观的actor进行时序差分学习，并且使用一个乐观的actor促进探索。通过在整个学习过程中调整探索程度，利用悲观与乐观策略的Wasserstein重心作为探索策略。

**结果:** WBSAC 与最先进的离线策略 actor-critic 算法相比，在MuJoCo连续控制任务上显示出了更高的样本效率。

**结论:** WBSAC 提供了一个有原则的定向探索策略，能够有效提升在稀疏奖励环境下强化学习的样本效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wasserstein+Barycenter+Soft+Actor-Critic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10167，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10167&send_immediately=true&force_search=false)

**原文摘要:** Deep off-policy actor-critic algorithms have emerged as the leading framework
for reinforcement learning in continuous control domains. However, most of
these algorithms suffer from poor sample efficiency, especially in environments
with sparse rewards. In this paper, we take a step towards addressing this
issue by providing a principled directed exploration strategy. We propose
Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from
a pessimistic actor for temporal difference learning and an optimistic actor to
promote exploration. This is achieved by using the Wasserstein barycenter of
the pessimistic and optimistic policies as the exploration policy and adjusting
the degree of exploration throughout the learning process. We compare WBSAC
with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is
more sample-efficient on MuJoCo continuous control tasks.

</details>


### [27] [Size-adaptive Hypothesis Testing for Fairness](https://arxiv.org/abs/2506.10586)
*Antonio Ferrara, Francesco Cozzi, Alan Perotti, André Panisson, Francesco Bonchi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种统一的、适应规模的假设检验框架，用于将公平性评估转化为基于证据的统计决策。对于足够大的子群体，证明了统计奇偶差异的中心极限结果，并为小交叉群体导出了一个完全贝叶斯Dirichlet-多项式估计器。实证研究验证了该方法在不同数据可用性和交叉性程度下的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的算法决策系统中，对特定人群是否存在歧视的判断通常依赖于单点估计的公平度量与预定义阈值的比较，这种做法在统计上不够稳健，忽略了抽样误差，也没有区分大小不同的群体。当考虑多个敏感属性时，问题变得更加复杂，因为这会导致更多的小群体出现，而这些小群体的数据过于稀疏，无法进行可靠的估计。

**方法:** 作者引入了一个统一的、适应规模的假设检验框架，对于足够大的子群体，通过证明统计奇偶差异的中心极限定理来获得解析置信区间和Wald检验；对于较小的交叉群体，则使用完全贝叶斯Dirichlet-多项式估计器，并通过蒙特卡洛可信区间来校准任何样本大小的结果。

**结果:** 研究显示，所提出的测试能够在不同程度的数据可用性和交叉性情况下提供可解释且统计上严格的决策。

**结论:** 论文提出的框架能够改善传统公平性评估方法的统计脆弱性，提供了更加强大和灵活的方法来处理大数据集中的公平性问题，特别是对于那些数据较为稀疏的小型交叉群体。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Size-adaptive+Hypothesis+Testing+for+Fairness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10586，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10586&send_immediately=true&force_search=false)

**原文摘要:** Determining whether an algorithmic decision-making system discriminates
against a specific demographic typically involves comparing a single point
estimate of a fairness metric against a predefined threshold. This practice is
statistically brittle: it ignores sampling error and treats small demographic
subgroups the same as large ones. The problem intensifies in intersectional
analyses, where multiple sensitive attributes are considered jointly, giving
rise to a larger number of smaller groups. As these groups become more
granular, the data representing them becomes too sparse for reliable
estimation, and fairness metrics yield excessively wide confidence intervals,
precluding meaningful conclusions about potential unfair treatments.
  In this paper, we introduce a unified, size-adaptive, hypothesis-testing
framework that turns fairness assessment into an evidence-based statistical
decision. Our contribution is twofold. (i) For sufficiently large subgroups, we
prove a Central-Limit result for the statistical parity difference, leading to
analytic confidence intervals and a Wald test whose type-I (false positive)
error is guaranteed at level $\alpha$. (ii) For the long tail of small
intersectional groups, we derive a fully Bayesian Dirichlet-multinomial
estimator; Monte-Carlo credible intervals are calibrated for any sample size
and naturally converge to Wald intervals as more data becomes available. We
validate our approach empirically on benchmark datasets, demonstrating how our
tests provide interpretable, statistically rigorous decisions under varying
degrees of data availability and intersectionality.

</details>


### [28] [Rethinking Losses for Diffusion Bridge Samplers](https://arxiv.org/abs/2506.10982)
*Sebastian Sanokowski, Lukas Gruber, Christoph Bartmann, Sepp Hochreiter, Sebastian Lehner*

**主要类别:** cs.LG

**AI概要:** 研究发现，对于扩散桥来说，使用带有对数导数技巧的逆向KL损失（rKL-LD）不仅避免了概念上的问题，而且在实验中表现优于对数方差损失（LV）。此外，rKL-LD在实际应用中需要更少的超参数优化，并且提供了更稳定的训练行为。


<details>
  <summary>更多</summary>
  
**动机:** 该论文旨在探讨和比较两种用于从非标准化分布采样的方法：对数方差损失（LV）与结合了对数导数技巧的逆向Kullback-Leibler损失（rKL-LD），尤其是在扩散桥模型以及学习扩散系数的情况下的表现。

**方法:** 通过理论分析证明了LV损失与rKL损失在某些情况下的等价性并不适用于扩散桥或当扩散系数是可学习的时候。进一步地，基于这一观察，作者论证了为什么rKL-LD是更优的选择，并通过实验验证了这一点。

**结果:** 实验结果表明，在不同的扩散桥类型上，使用rKL-LD损失训练的采样器比使用LV损失的具有更好的性能。另外，rKL-LD还减少了所需的超参数调整工作量，并表现出更加稳定的训练过程。

**结论:** 对于扩散桥而言，采用带有对数导数技巧的逆向KL损失（rKL-LD）作为优化目标更为合适，因为它解决了LV损失存在的概念性问题，并且在实践中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking+Losses+for+Diffusion+Bridge+Samplers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10982，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10982&send_immediately=true&force_search=false)

**原文摘要:** Diffusion bridges are a promising class of deep-learning methods for sampling
from unnormalized distributions. Recent works show that the Log Variance (LV)
loss consistently outperforms the reverse Kullback-Leibler (rKL) loss when
using the reparametrization trick to compute rKL-gradients. While the on-policy
LV loss yields identical gradients to the rKL loss when combined with the
log-derivative trick for diffusion samplers with non-learnable forward
processes, this equivalence does not hold for diffusion bridges or when
diffusion coefficients are learned. Based on this insight we argue that for
diffusion bridges the LV loss does not represent an optimization objective that
can be motivated like the rKL loss via the data processing inequality. Our
analysis shows that employing the rKL loss with the log-derivative trick
(rKL-LD) does not only avoid these conceptual problems but also consistently
outperforms the LV loss. Experimental results with different types of diffusion
bridges on challenging benchmarks show that samplers trained with the rKL-LD
loss achieve better performance. From a practical perspective we find that
rKL-LD requires significantly less hyperparameter optimization and yields more
stable training behavior.

</details>


### [29] [A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes](https://arxiv.org/abs/2506.10180)
*Mowafaq Salem Alzboon, Mohammad Al-Batah, Muhyeeddin Alqaraleh, Ahmad Abuashour, Ahmad Fuad Bader*

**主要类别:** cs.LG

**AI概要:** 本研究使用Pima Indians Diabetes数据集评估了多种机器学习方法在糖尿病预测中的有效性，结果表明神经网络算法表现最佳，准确率达到78.57%，其次是随机森林方法，准确率为76.30%。


<details>
  <summary>更多</summary>
  
**动机:** 由于糖尿病正成为许多国家的重大健康问题，早期识别和控制非常重要。因此，利用机器学习算法来预测糖尿病具有重要的意义。

**方法:** 研究采用了Pima Indians Diabetes数据集，并评估了逻辑回归、决策树、随机森林、k-最近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络等几种机器学习方法的有效性。

**结果:** 研究发现神经网络算法的表现最好，其准确率为78.57%，其次是随机森林方法，准确率为76.30%。

**结论:** 研究表明，机器学习算法可以辅助糖尿病的预测，并且作为有效的早期检测工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comparative+Study+of+Machine+Learning+Techniques+for+Early+Prediction+of+Diabetes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10180，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10180&send_immediately=true&force_search=false)

**原文摘要:** In many nations, diabetes is becoming a significant health problem, and early
identification and control are crucial. Using machine learning algorithms to
predict diabetes has yielded encouraging results. Using the Pima Indians
Diabetes dataset, this study attempts to evaluate the efficacy of several
machine-learning methods for diabetes prediction. The collection includes
information on 768 patients, such as their ages, BMIs, and glucose levels. The
techniques assessed are Logistic Regression, Decision Tree, Random Forest,
k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting,
and Neural Network. The findings indicate that the Neural Network algorithm
performed the best, with an accuracy of 78.57 percent, followed by the Random
Forest method, with an accuracy of 76.30 percent. The study implies that
machine learning algorithms can aid diabetes prediction and be an efficient
early detection tool.

</details>


### [30] [Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition](https://arxiv.org/abs/2506.10184)
*Mohammad Subhi Al-Batah, Mowafaq Salem Alzboon, Muhyeeddin Alqaraleh*

**主要类别:** cs.LG

**AI概要:** 本研究通过三个不同数据集上的系统实验，探讨了多层感知器（MLP）网络的性能。研究使用了三种方法：默认设置训练MLP、基于遗传算法（GA）的特征选择和基于主成分分析（PCA）的降维。结果表明，对于低维度无噪声的数据集，PCA显示了优势；而对于复杂数据集，GA通过精确识别关键特征提高了准确性。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探索不同的特征选择和降维技术如何影响多层感知器（MLP）网络在不同类型数据集上的表现，并为机器学习任务提供实用指南。

**方法:** 研究采用了三种主要方法：a) 使用默认设置对多层感知器(MLP)进行基准训练；b) 采用基于遗传算法(GA)的方法进行特征选择；c) 利用主成分分析(PCA)执行基于维度缩减的操作。

**结果:** 结果显示，在低维度且没有噪声的数据集中，PCA表现出色；而GA则在复杂数据集中通过准确地识别重要特征持续提升了精度。比较揭示出特征选择与维度减少在提高MLP表现方面起着相互依赖的作用。

**结论:** 研究表明，特征选择和降维在提升MLP性能方面起着重要作用。这项工作为特征工程和神经网络参数优化提供了文献贡献，并为各种机器学习任务提出了实践指导方针。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Genetic+Algorithms+with+Multilayer+Perceptron+Networks+for+Enhancing+TinyFace+Recognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10184，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10184&send_immediately=true&force_search=false)

**原文摘要:** This study conducts an empirical examination of MLP networks investigated
through a rigorous methodical experimentation process involving three diverse
datasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes
three key methods: a) a baseline training using the default settings for the
Multi-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA)
based refinement c) Principal Component Analysis (PCA) based dimension
reduction. The results show important information on how such techniques affect
performance. While PCA had showed benefits in low-dimensional and noise-free
datasets GA consistently increased accuracy in complex datasets by accurately
identifying critical features. Comparison reveals that feature selection and
dimensionality reduction play interdependent roles in enhancing MLP
performance. The study contributes to the literature on feature engineering and
neural network parameter optimization, offering practical guidelines for a wide
range of machine learning tasks

</details>


### [31] [Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment](https://arxiv.org/abs/2506.10186)
*Yuhui Ding, Thomas Hofmann*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，通过学习每个分子的样本依赖SO(3)变换来构建对齐的潜在空间，并在该对齐表示上训练非等变扩散模型，从而在保持生成样本质量的同时提高训练和采样效率。


<details>
  <summary>更多</summary>
  
**动机:** 虽然等变扩散模型在3D分子生成中表现出色，但专门的等变架构限制了扩散模型的可扩展性和效率。

**方法:** 提出的方法放松了等变性约束，为每个分子学习一个样本依赖的SO(3)变换以构建对齐的潜在空间，并且在这个对齐的空间上训练了一个非等变扩散模型。

**结果:** 实验结果表明，该方法的表现显著优于先前报道的非等变模型，而且产生的样本质量与最先进的等变扩散模型相当，同时提供了更好的训练和采样效率。

**结论:** 通过构建对齐的潜在空间并采用非等变扩散模型，可以在不牺牲样本质量的情况下提高3D分子生成的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scalable+Non-Equivariant+3D+Molecule+Generation+via+Rotational+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10186，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10186&send_immediately=true&force_search=false)

**原文摘要:** Equivariant diffusion models have achieved impressive performance in 3D
molecule generation. These models incorporate Euclidean symmetries of 3D
molecules by utilizing an SE(3)-equivariant denoising network. However,
specialized equivariant architectures limit the scalability and efficiency of
diffusion models. In this paper, we propose an approach that relaxes such
equivariance constraints. Specifically, our approach learns a sample-dependent
SO(3) transformation for each molecule to construct an aligned latent space. A
non-equivariant diffusion model is then trained over the aligned
representations. Experimental results demonstrate that our approach performs
significantly better than previously reported non-equivariant models. It yields
sample quality comparable to state-of-the-art equivariant diffusion models and
offers improved training and sampling efficiency. Our code is available at
https://github.com/skeletondyh/RADM

</details>


### [32] [Improving Oral Cancer Outcomes Through Machine Learning and Dimensionality Reduction](https://arxiv.org/abs/2506.10189)
*Mohammad Subhi Al-Batah, Muhyeeddin Alqaraleh, Mowafaq Salem Alzboon*

**主要类别:** cs.LG

**AI概要:** 研究综述了机器学习和数据挖掘方法在口腔癌诊断和预后中的应用，发现神经网络模型在预测口腔癌方面具有最高的分类准确率93.6%，并强调了特征选择和降维技术对提高模型性能的潜在益处。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高口腔癌患者的生存率，需要早期诊断和准确预后。最近，机器学习和数据挖掘的进步为区分良性和恶性口腔病变提供了更加精细和自动化的工具。

**方法:** 本研究回顾了包括神经网络、K-最近邻（KNN）、支持向量机（SVM）和集成学习技术在内的最新数据挖掘方法，并专门应用于口腔癌的诊断和预后。通过严格的比较分析来评估不同模型的表现。

**结果:** 研究表明，神经网络模型在预测口腔癌方面表现最佳，达到了93.6%的分类准确率。此外，研究还指出结合特征选择和降维技术可以进一步提升模型性能。

**结论:** 先进的数据挖掘技术在加强早期检测、优化治疗策略以及最终改善口腔肿瘤学领域的患者结果方面展现出巨大的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Oral+Cancer+Outcomes+Through+Machine+Learning+and+Dimensionality+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10189&send_immediately=true&force_search=false)

**原文摘要:** Oral cancer presents a formidable challenge in oncology, necessitating early
diagnosis and accurate prognosis to enhance patient survival rates. Recent
advancements in machine learning and data mining have revolutionized
traditional diagnostic methodologies, providing sophisticated and automated
tools for differentiating between benign and malignant oral lesions. This study
presents a comprehensive review of cutting-edge data mining methodologies,
including Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), and ensemble learning techniques, specifically applied to the diagnosis
and prognosis of oral cancer. Through a rigorous comparative analysis, our
findings reveal that Neural Networks surpass other models, achieving an
impressive classification accuracy of 93,6 % in predicting oral cancer.
Furthermore, we underscore the potential benefits of integrating feature
selection and dimensionality reduction techniques to enhance model performance.
These insights underscore the significant promise of advanced data mining
techniques in bolstering early detection, optimizing treatment strategies, and
ultimately improving patient outcomes in the realm of oral oncology.

</details>


### [33] [DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection](https://arxiv.org/abs/2506.10200)
*Tina Behrouzi, Sana Tonekaboni, Rahul G. Krishnan, Anna Goldenberg*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种名为DynaSubVAE的动态子群变分自编码器框架，它能够同时进行表示学习和自适应的OOD检测。该模型能随着数据的变化动态更新其潜在结构以捕捉新趋势，并使用一种新的非参数聚类机制来发现和建模基于嵌入相似性的潜在子群。实验表明，DynaSubVAE在近OOD和远OOD检测中都表现出色，特别是在训练时整个类别缺失的情况下。此外，其动态子群机制在OOD准确性和遗憾精度方面优于GMM和KMeans++等独立聚类方法。


<details>
  <summary>更多</summary>
  
**动机:** 现实世界中的观察数据往往包含已存在或正在出现的异质子群体，这些子群体偏离了全局模式。大多数模型倾向于忽略这些代表性不足的群体，导致预测不准确甚至有害。现有的解决方案通常是将这些样本识别为域外（OOD）而不是让模型适应新的出现模式。

**方法:** 提出了DynaSubVAE，这是一种动态子群变分自编码器框架，它结合了表征学习和自适应OOD检测。与传统方法不同的是，DynaSubVAE通过动态更新其潜在结构来随数据演变并捕获新趋势。它采用了一种受高斯混合模型启发的新颖非参数聚类机制，根据嵌入相似性来发现和建模潜在子群。

**结果:** 广泛的实验表明，DynaSubVAE在近OOD和远OOD检测方面都能达到竞争性的表现，并且在训练过程中某个类别完全缺失的类OOD场景中尤为出色。进一步说明了我们的动态子群机制在OOD准确度和遗憾精度上超过了像GMM和KMeans++这样的独立聚类方法。

**结论:** DynaSubVAE作为一种新颖的方法，能够在不断变化的数据环境中识别出未被充分代表的子群体，并通过自适应地调整其内部结构来提高OOD检测性能。这种动态子群机制证明了比传统的固定结构模型更有效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DynaSubVAE%3A+Adaptive+Subgrouping+for+Scalable+and+Robust+OOD+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10200，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10200&send_immediately=true&force_search=false)

**原文摘要:** Real-world observational data often contain existing or emerging
heterogeneous subpopulations that deviate from global patterns. The majority of
models tend to overlook these underrepresented groups, leading to inaccurate or
even harmful predictions. Existing solutions often rely on detecting these
samples as Out-of-domain (OOD) rather than adapting the model to new emerging
patterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational
Autoencoder framework that jointly performs representation learning and
adaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with
the data by dynamically updating its latent structure to capture new trends. It
leverages a novel non-parametric clustering mechanism, inspired by Gaussian
Mixture Models, to discover and model latent subgroups based on embedding
similarity. Extensive experiments show that DynaSubVAE achieves competitive
performance in both near-OOD and far-OOD detection, and excels in class-OOD
scenarios where an entire class is missing during training. We further
illustrate that our dynamic subgrouping mechanism outperforms standalone
clustering methods such as GMM and KMeans++ in terms of both OOD accuracy and
regret precision.

</details>


### [34] [AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent](https://arxiv.org/abs/2506.10205)
*Jing Liu, Toshiaki Koike-Akino, Ye Wang, Hassan Mansour, Matthew Brand*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为AWP的统一方法，该方法结合了激活感知权重剪枝和量化，通过投影梯度下降实现。实验表明，AWP在大语言模型（LLM）的剪枝和量化上优于现有技术，并提供了理论收敛性保证。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决大型语言模型（LLMs）规模庞大带来的问题，特别是在边缘设备上的部署，需要使用诸如量化和剪枝等模型压缩方法。本文特别关注训练后的逐层量化和剪枝。

**方法:** 本文提出了一个称为AWP（Activation-aware Weight pruning）的方法，它是一个基于投影梯度下降的统一框架，用于激活感知的权重剪枝和量化。这一方法受到迭代硬阈值（IHT）成功的启发，并且与稀疏逼近问题相关联。

**结果:** 实验结果表明，AWP在大语言模型的剪枝和量化方面超过了当前最先进的方法。此外，还提供了所提剪枝方法的理论收敛性保障。

**结论:** 本文提出的AWP方法提供了一个有效的解决方案，以减少大型语言模型的大小，同时保持其性能。通过结合激活感知权重剪枝和量化，AWP展示了卓越的效果，并且具备理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AWP%3A+Activation-Aware+Weight+Pruning+and+Quantization+with+Projected+Gradient+Descent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10205，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10205&send_immediately=true&force_search=false)

**原文摘要:** To address the enormous size of Large Language Models (LLMs), model
compression methods, such as quantization and pruning, are often deployed,
especially on edge devices. In this work, we focus on layer-wise post-training
quantization and pruning. Drawing connections between activation-aware weight
pruning and sparse approximation problems, and motivated by the success of
Iterative Hard Thresholding (IHT), we propose a unified method for
Activation-aware Weight pruning and quantization via Projected gradient descent
(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM
pruning and quantization methods. Theoretical convergence guarantees of the
proposed method for pruning are also provided.

</details>


### [35] [Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms](https://arxiv.org/abs/2506.10212)
*Sajjad Karimi, Amit J. Shah, Gari D. Clifford, Reza Sameni*

**主要类别:** cs.LG

**AI概要:** 本研究使用EPHNOGRAM数据集系统地分析了心电图(ECG)和心音图(PCG)的共同与独特特征，通过多种线性和非线性机器学习模型来实现两者的互构，并且展示了如何从PCG估计ECG生物标志物。研究表明非因果LSTM网络在重建性能上表现更优，尤其是在跨个体情况下利用瞬时幅度特征的包络建模极大地提高了泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管同步记录的心电图和心音图提供了心脏电活动和机械活动的全面多模态视角，但这些信号中独特的、重叠的信息内容以及它们相互重建和提取生物标志物的可能性尚未完全理解，特别是在不同生理条件和个体之间。

**方法:** 研究者们使用了EPHNOGRAM数据集中的同时期ECG-PCG记录，在休息和运动状态下进行。他们运用了一系列线性和非线性的机器学习模型，包括非因果LSTM网络，来尝试从一种模式重建另一种模式，并分析因果关系、生理状态和跨个体变异性的影响。

**结果:** 结果表明，非线性模型尤其是非因果LSTM网络表现出更好的重建性能；从PCG重建ECG比反向操作更容易实现。运动和跨个体场景提出了显著挑战，但是基于包络建模并利用瞬时幅度特征的方法大大提升了跨模态学习的跨个体泛化能力。此外，还证明了诸如标记点和QT间隔等临床上相关的心电图生物标志物可以从PCG在跨个体环境中被估计出来。

**结论:** 这些发现增进了我们对心脏电机械模式之间关系的理解，不仅在于波形特性方面，也包括心脏事件的时间安排，并可能应用于新型多模态心脏监测技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Learning+Between+ECG+and+PCG%3A+Exploring+Common+and+Exclusive+Characteristics+of+Bimodal+Electromechanical+Cardiac+Waveforms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10212，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10212&send_immediately=true&force_search=false)

**原文摘要:** Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a
comprehensive, multimodal perspective on cardiac function by capturing the
heart's electrical and mechanical activities, respectively. However, the
distinct and overlapping information content of these signals, as well as their
potential for mutual reconstruction and biomarker extraction, remains
incompletely understood, especially under varying physiological conditions and
across individuals.
  In this study, we systematically investigate the common and exclusive
characteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous
ECG-PCG recordings during rest and exercise. We employ a suite of linear and
nonlinear machine learning models, including non-causal LSTM networks, to
reconstruct each modality from the other and analyze the influence of
causality, physiological state, and cross-subject variability. Our results
demonstrate that nonlinear models, particularly non-causal LSTM, provide
superior reconstruction performance, with reconstructing ECG from PCG proving
more tractable than the reverse. Exercise and cross-subject scenarios present
significant challenges, but envelope-based modeling that utilizes instantaneous
amplitude features substantially improves cross-subject generalizability for
cross-modal learning. Furthermore, we demonstrate that clinically relevant ECG
biomarkers, such as fiducial points and QT intervals, can be estimated from PCG
in cross-subject settings.
  These findings advance our understanding of the relationship between
electromechanical cardiac modalities, in terms of both waveform characteristics
and the timing of cardiac events, with potential applications in novel
multimodal cardiac monitoring technologies.

</details>


### [36] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/abs/2506.10235)
*Chen-Chia Chang, Wan-Hsuan Lin, Yikang Shen, Yiran Chen, Xin Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于语言模型的模拟拓扑生成方法LaMAGIC2，通过改进组件类型识别、减少token长度复杂度和提高数值精度敏感性来提升性能。实验表明，LaMAGIC2在严格的容差下成功率提高了34%，并且与先前的方法相比MSE降低了10倍。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于序列到序列方法和语言模型监督微调的方法在电路公式化方面效率低下，且对数值输入的精度敏感性低。

**方法:** 引入了LaMAGIC2（SFCI），一种简洁的浮点输入标准公式化方法，带有标识符表示法以改善组件类型识别，将token长度复杂度降低至O(|V|)，并增强了数值精度敏感性。

**结果:** LaMAGIC2在严格容差条件下成功率达到34%的提升，并且MSE比之前的方法减少了10倍。对于具有更多顶点的电路，LaMAGIC2表现出更好的迁移性，提升了高达58.5%。

**结论:** LaMAGIC2被证明是一种鲁棒的框架，适用于模拟拓扑生成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LaMAGIC2%3A+Advanced+Circuit+Formulations+for+Language+Model-Based+Analog+Topology+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10235，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10235&send_immediately=true&force_search=false)

**原文摘要:** Automation of analog topology design is crucial due to customized
requirements of modern applications with heavily manual engineering efforts.
The state-of-the-art work applies a sequence-to-sequence approach and
supervised finetuning on language models to generate topologies given user
specifications. However, its circuit formulation is inefficient due to O(|V |2)
token length and suffers from low precision sensitivity to numeric inputs. In
this work, we introduce LaMAGIC2, a succinct float-input canonical formulation
with identifier (SFCI) for language model-based analog topology generation.
SFCI addresses these challenges by improving component-type recognition through
identifier-based representations, reducing token length complexity to O(|V |),
and enhancing numeric precision sensitivity for better performance under tight
tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher
success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a
prior method. LaMAGIC2 also exhibits better transferability for circuits with
more vertices with up to 58.5% improvement. These advancements establish
LaMAGIC2 as a robust framework for analog topology generation.

</details>


### [37] [A new type of federated clustering: A non-model-sharing approach](https://arxiv.org/abs/2506.10244)
*Yuji Kawamata, Kaoru Kamijo, Maki Kihira, Akihiro Toyoda, Tomoru Nakayama, Akira Imakura, Tetsuya Sakurai, Yukihiko Okada*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的联邦聚类方法——数据协作聚类（DC-Clustering），它支持在复杂的数据划分场景下进行聚类，同时保持隐私并允许协作。


<details>
  <summary>更多</summary>
  
**动机:** 现有的联邦学习（FL）聚类方法通常只能处理简单的数据划分情况，如横向或纵向分割，而不能应对更复杂的分布式结构。研究旨在解决这一问题，并实现对分布式异构数据的有效知识发现。

**方法:** 提出了数据协作聚类（DC-Clustering），该方法允许机构间仅共享中间表示而非原始数据，从而确保隐私保护的同时促进协作聚类。此方法可以在k-means和谱聚类之间灵活选择，并通过与中心服务器的一轮通信完成最终结果。

**结果:** 通过使用合成数据集和开放基准数据集的广泛实验表明，该方法能够达到与集中式聚类相媲美的聚类性能，其中所有数据都被汇集起来。

**结论:** DC-聚类填补了当前联邦学习研究中的一个重要空白，即从分布式异构数据中有效发现知识。其实际特性——隐私保护、通信效率和灵活性——使其成为医疗保健和金融等敏感领域的有前途工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+new+type+of+federated+clustering%3A+A+non-model-sharing+approach，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10244&send_immediately=true&force_search=false)

**原文摘要:** In recent years, the growing need to leverage sensitive data across
institutions has led to increased attention on federated learning (FL), a
decentralized machine learning paradigm that enables model training without
sharing raw data. However, existing FL-based clustering methods, known as
federated clustering, typically assume simple data partitioning scenarios such
as horizontal or vertical splits, and cannot handle more complex distributed
structures. This study proposes data collaboration clustering (DC-Clustering),
a novel federated clustering method that supports clustering over complex data
partitioning scenarios where horizontal and vertical splits coexist. In
DC-Clustering, each institution shares only intermediate representations
instead of raw data, ensuring privacy preservation while enabling collaborative
clustering. The method allows flexible selection between k-means and spectral
clustering, and achieves final results with a single round of communication
with the central server. We conducted extensive experiments using synthetic and
open benchmark datasets. The results show that our method achieves clustering
performance comparable to centralized clustering where all data are pooled.
DC-Clustering addresses an important gap in current FL research by enabling
effective knowledge discovery from distributed heterogeneous data. Its
practical properties -- privacy preservation, communication efficiency, and
flexibility -- make it a promising tool for privacy-sensitive domains such as
healthcare and finance.

</details>


### [38] [Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification](https://arxiv.org/abs/2506.10269)
*Ryota Ueda, Takami Sato, Ken Kobayashi, Kazuhide Nakata*

**主要类别:** cs.LG

**AI概要:** 本文指出了使用半定规划（SDP）松弛方法验证深度神经网络时存在的内部点消失问题，这会导致数值稳定性和最优性的重要条件——严格可行性丢失。作者提出了五种解决方案以提高验证问题的可行性，并成功解决了现有方法无法解决的大部分问题。此外，文章还揭示了传统上为ReLU单元设定的上下界约束对问题的可行性不仅无益反而有害。


<details>
  <summary>更多</summary>
  
**动机:** 尽管半定规划（SDP）松弛在提供比其他凸松弛方法更紧致的边界方面显示出潜力，但当应用于深层神经网络时存在一个关键限制：内部点消失现象，这导致了严格可行性的丧失。这是对于扩展基于SDP验证的一个根本障碍。

**方法:** 通过严格的理论和实证分析，研究者们展示了随着DNN深度的增加，严格可行性越来越可能丧失。为了解决这个问题，他们设计并调查了五种方案来增强验证问题的可行性条件。

**结果:** 所提出的方法能够成功解决现有方法不能解决的问题中的88%，占总数的41%。另外，研究还表明，先前工作中沿用下来的针对每个ReLU单元的有效约束实际上对问题的可行性不仅没有好处，甚至是有害的。

**结论:** 这项工作提供了关于基于SDP的DNN验证的基本挑战的重要见解，并提供了实用的解决方案以改善其对更深神经网络的适用性，从而有助于开发更加可靠和安全的DNN系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interior-Point+Vanishing+Problem+in+Semidefinite+Relaxations+for+Neural+Network+Verification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10269，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10269&send_immediately=true&force_search=false)

**原文摘要:** Semidefinite programming (SDP) relaxation has emerged as a promising approach
for neural network verification, offering tighter bounds than other convex
relaxation methods for deep neural networks (DNNs) with ReLU activations.
However, we identify a critical limitation in the SDP relaxation when applied
to deep networks: interior-point vanishing, which leads to the loss of strict
feasibility -- a crucial condition for the numerical stability and optimality
of SDP. Through rigorous theoretical and empirical analysis, we demonstrate
that as the depth of DNNs increases, the strict feasibility is likely to be
lost, creating a fundamental barrier to scaling SDP-based verification. To
address the interior-point vanishing, we design and investigate five solutions
to enhance the feasibility conditions of the verification problem. Our methods
can successfully solve 88% of the problems that could not be solved by existing
methods, accounting for 41% of the total. Our analysis also reveals that the
valid constraints for the lower and upper bounds for each ReLU unit are
traditionally inherited from prior work without solid reasons, but are actually
not only unbeneficial but also even harmful to the problem's feasibility. This
work provides valuable insights into the fundamental challenges of SDP-based
DNN verification and offers practical solutions to improve its applicability to
deeper neural networks, contributing to the development of more reliable and
secure systems with DNNs.

</details>


### [39] [Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning](https://arxiv.org/abs/2506.10282)
*Jiajin Liu, Dongzhe Fan, Jiacheng Shen, Chuanhao Ji, Daochen Zha, Qiaoyu Tan*

**主要类别:** cs.LG

**AI概要:** 本文提出了Graph-MLLM，一个用于多模态图学习的全面基准，评估了三种基于多模态大语言模型（MLLMs）的学习范式：编码器、对齐器和预测器。通过六个不同领域的数据集实验，研究发现联合考虑节点的视觉和文本属性有助于图学习，并且将视觉属性转换为文本描述可以进一步提高性能。此外，针对特定多模态图微调MLLMs在大多数情况下能够达到最先进的结果，即使没有明确的图结构信息。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态图（MMG）学习方法依赖于多模态大语言模型（MLLMs），但缺乏一个统一的基准来公平地评估这些方法之间的进展。为了填补这一空白，作者们希望创建一个综合基准，以促进该领域快速而公正的评估和发展。

**方法:** 论文中定义并系统地评估了三种利用MLLMs进行MMG学习的方法：1) MLLM-as-Encoder，通过多模态特征融合增强图神经网络；2) MLLM-as-Aligner，在语言或隐藏空间中对齐多模态属性，以支持基于LLM的图推理；3) MLLM-as-Predictor，将MLLMs视为独立的推理机，采用上下文学习或微调。

**结果:** 研究结果显示，结合节点的视觉和文本属性有利于图学习过程，尤其是当使用预训练的文本到图像对齐模型作为编码器时。另外，把视觉属性转化为文本描述比直接使用视觉输入有更好的表现。而且，对于特定的MMGs进行MLLMs的微调，通常能够在没有显式图结构信息的情况下实现最佳结果。

**结论:** 作者们提出了一种新的多模态图学习基准Graph-MLLM，它提供了对当前多模态图学习方法的系统性评估。通过实验证明了联合处理视觉与文本属性的有效性以及微调MLLMs的强大能力。此工作期望能推动该领域的进一步创新研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph-MLLM%3A+Harnessing+Multimodal+Large+Language+Models+for+Multimodal+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10282&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in representing and understanding diverse modalities. However,
they typically focus on modality alignment in a pairwise manner while
overlooking structural relationships across data points. Integrating
multimodality with structured graph information (i.e., multimodal graphs, MMGs)
is essential for real-world applications such as social networks, healthcare,
and recommendation systems. Existing MMG learning methods fall into three
paradigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.
MLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via
multimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in
language or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor
treats MLLMs as standalone reasoners with in-context learning or fine-tuning.
Despite their advances, the MMG field lacks a unified benchmark to fairly
evaluate across these approaches, making it unclear what progress has been
made. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for
multimodal graph learning by systematically evaluating these three paradigms
across six datasets with different domains. Through extensive experiments, we
observe that jointly considering the visual and textual attributes of the nodes
benefits graph learning, even when using pre-trained text-to-image alignment
models (e.g., CLIP) as encoders. We also find that converting visual attributes
into textual descriptions further improves performance compared to directly
using visual inputs. Moreover, we observe that fine-tuning MLLMs on specific
MMGs can achieve state-of-the-art results in most scenarios, even without
explicit graph structure information. We hope that our open-sourced library
will facilitate rapid, equitable evaluation and inspire further innovative
research in this field.

</details>


### [40] [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/abs/2506.10314)
*Luc Raszewski, Christine De Kock*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种使用元学习的方法来检测维基百科上的恶意傀儡账户，这种方法在数据稀缺的情况下能够快速适应新的傀儡群组的写作风格，并且与预训练模型相比显著提高了预测精度。此外还发布了一个新的傀儡调查数据集以促进相关领域的未来研究。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习方法依赖于风格和元数据特征，但不能很好地适应特定作者的行为，特别是在文本数据有限时难以有效建模特定傀儡群组的行为。

**方法:** 采用元学习技术，这种技术通过跨多个任务训练模型来改善数据稀缺环境下的性能，优化了对新傀儡群组写作风格的快速适应能力。

**结果:** 元学习与预训练模型相比，在预测准确性上有显著提高。

**结论:** 元学习方法为开放编辑平台打击傀儡行为带来了进步，同时提供了一个新的傀儡调查数据集以支持未来的相关研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+Sockpuppetry+on+Wikipedia+Using+Meta-Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10314，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10314&send_immediately=true&force_search=false)

**原文摘要:** Malicious sockpuppet detection on Wikipedia is critical to preserving access
to reliable information on the internet and preventing the spread of
disinformation. Prior machine learning approaches rely on stylistic and
meta-data features, but do not prioritise adaptability to author-specific
behaviours. As a result, they struggle to effectively model the behaviour of
specific sockpuppet-groups, especially when text data is limited. To address
this, we propose the application of meta-learning, a machine learning technique
designed to improve performance in data-scarce settings by training models
across multiple tasks. Meta-learning optimises a model for rapid adaptation to
the writing style of a new sockpuppet-group. Our results show that
meta-learning significantly enhances the precision of predictions compared to
pre-trained models, marking an advancement in combating sockpuppetry on open
editing platforms. We release a new dataset of sockpuppet investigations to
foster future research in both sockpuppetry and meta-learning fields.

</details>


### [41] [PyLO: Towards Accessible Learned Optimizers in PyTorch](https://arxiv.org/abs/2506.10315)
*Paul Janson, Benjamin Therien, Quentin Anthony, Xiaolong Huang, Abhinav Moudgil, Eugene Belilovsky*

**主要类别:** cs.LG

**AI概要:** 本文介绍了PyLO，一个基于PyTorch的库，它使得学习型优化器能够被更广泛的机器学习社区所使用。PyLO提供了CUDA加速版本的小型全连接学习优化器架构，并且可以轻松地与现有的优化工具如学习率调度和权重衰减结合使用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管学习型优化器在过去十年中是一个活跃的研究领域，但最近的一些进展，比如VeLO，由于其依赖JAX以及缺乏用户友好的包来应用经过元训练的优化器，仍然难以被更广泛的社区所采用。

**方法:** 为了解决这个问题，研究者们推出了PyLO，这是一个基于PyTorch的库，它通过熟悉且广泛接受的工作流程将学习型优化器带给更广大的机器学习群体。此外，PyLO还提供了一个CUDA加速版本的学习优化器架构，并允许用户将其与现有优化工具相结合。

**结果:** PyLO的发布使得学习型优化器在现实世界的大规模预训练任务中的应用成为可能，并且当与现有的优化工具结合时，学习型优化器表现出了显著的好处。CUDA加速版本提高了从39.36到205.59样本/秒的吞吐量，用于批量大小为32的ViT B/16训练。

**结论:** PyLO旨在填补学习型优化器实际应用上的空白，通过提供一个易于使用的库来促进这些优化器在更广泛应用场景下的使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PyLO%3A+Towards+Accessible+Learned+Optimizers+in+PyTorch，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10315，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10315&send_immediately=true&force_search=false)

**原文摘要:** Learned optimizers have been an active research topic over the past decade,
with increasing progress toward practical, general-purpose optimizers that can
serve as drop-in replacements for widely used methods like Adam. However,
recent advances -- such as VeLO, which was meta-trained for 4000 TPU-months --
remain largely inaccessible to the broader community, in part due to their
reliance on JAX and the absence of user-friendly packages for applying the
optimizers after meta-training. To address this gap, we introduce PyLO, a
PyTorch-based library that brings learned optimizers to the broader machine
learning community through familiar, widely adopted workflows. Unlike prior
work focused on synthetic or convex tasks, our emphasis is on applying learned
optimization to real-world large-scale pre-training tasks. Our release includes
a CUDA-accelerated version of the small_fc_lopt learned optimizer architecture
from (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to
205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO
also allows us to easily combine learned optimizers with existing optimization
tools such as learning rate schedules and weight decay. When doing so, we find
that learned optimizers can substantially benefit. Our code is available at
https://github.com/Belilovsky-Lab/pylo

</details>


### [42] [Provably Learning from Language Feedback](https://arxiv.org/abs/2506.10341)
*Wanqiao Xu, Allen Nie, Ruijie Zheng, Aditya Modi, Adith Swaminathan, Ching-An Cheng*

**主要类别:** cs.LG

**AI概要:** 本文提出了从语言反馈中学习（LLF）的问题框架，引入了转移逃避维度作为衡量LLF问题难度的复杂性指标，并开发了一种名为HELiX的无悔算法来解决这些问题。实证研究表明，与仅依赖奖励相比，从丰富的语言反馈中学习可以显著加快学习速度，且HELiX即使在大型语言模型反馈不可靠的情况下也表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型（LLM）代理人的出现，从观察和语言反馈中进行交互式学习成为了一个日益研究的领域。尽管已经展示了令人印象深刻的实证演示，但目前这些决策问题缺乏一个有原则的框架。

**方法:** 作者们形式化定义了从语言反馈中学习的问题，并提出了足够的假设以允许在存在潜在回报的情况下进行学习。他们引入了“转移逃避维度”这一复杂度量来描述LLF问题的难度，并基于此设计了一个称为HELiX的无悔算法，该算法通过顺序互动来解决LLF问题。

**结果:** 研究表明，当从丰富的语言反馈中学习时，其效率可能比从奖励中学习快得多。此外，HELiX算法在多种实证领域中表现良好，即使重复提示LLM不能可靠工作时也是如此。

**结论:** 本研究为从通用语言反馈设计有原则的交互式学习算法迈出了第一步，提供了理论上的见解以及实际应用中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provably+Learning+from+Language+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10341，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10341&send_immediately=true&force_search=false)

**原文摘要:** Interactively learning from observation and language feedback is an
increasingly studied area driven by the emergence of large language model (LLM)
agents. While impressive empirical demonstrations have been shown, so far a
principled framing of these decision problems remains lacking. In this paper,
we formalize the Learning from Language Feedback (LLF) problem, assert
sufficient assumptions to enable learning despite latent rewards, and introduce
$\textit{transfer eluder dimension}$ as a complexity measure to characterize
the hardness of LLF problems. We show that transfer eluder dimension captures
the intuition that information in the feedback changes the learning complexity
of the LLF problem. We demonstrate cases where learning from rich language
feedback can be exponentially faster than learning from reward. We develop a
no-regret algorithm, called $\texttt{HELiX}$, that provably solves LLF problems
through sequential interactions, with performance guarantees that scale with
the transfer eluder dimension of the problem. Across several empirical domains,
we show that $\texttt{HELiX}$ performs well even when repeatedly prompting LLMs
does not work reliably. Our contributions mark a first step towards designing
principled interactive learning algorithms from generic language feedback.

</details>


### [43] [PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation](https://arxiv.org/abs/2506.10351)
*Yanlong Chen, Mattia Orlandi, Pierangelo Maria Rapa, Simone Benatti, Luca Benini, Yawei Li*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于小波的生理信号分析新方法，建立了针对EMG和ECG的大规模预训练模型，并通过集成预训练EEG模型构建了多模态框架，为生理信号处理提供了坚实的基础。


<details>
  <summary>更多</summary>
  
**动机:** 解决生理信号中运动伪影、基线漂移等低信噪比干扰问题，以及非平稳性带来的挑战。

**方法:** 开发了一种新的基于小波的方法来捕捉不同生理信号中的多尺度时频特征，并首次介绍了两个专用于EMG和ECG的大规模预训练模型。此外，通过整合预训练的EEG模型创建了一个统一的多模态框架，其中每种模式都通过其专用分支引导并通过可学习的加权融合进行融合。

**结果:** 该方法在下游任务中达到了优异的表现，并为多模态任务设定了新的基准。设计有效地解决了诸如低信噪比、高个体间变异性以及设备不匹配等问题，在多模态任务上优于现有方法。

**结论:** 所提出的基于小波的架构为多种生理信号分析奠定了坚实基础，而多模态设计则指向了下一代生理信号处理，对可穿戴健康监测、临床诊断及更广泛的生物医学应用具有潜在影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PhysioWave%3A+A+Multi-Scale+Wavelet-Transformer+for+Physiological+Signal+Representation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10351，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10351&send_immediately=true&force_search=false)

**原文摘要:** Physiological signals are often corrupted by motion artifacts, baseline
drift, and other low-SNR disturbances, which pose significant challenges for
analysis. Additionally, these signals exhibit strong non-stationarity, with
sharp peaks and abrupt changes that evolve continuously, making them difficult
to represent using traditional time-domain or filtering methods. To address
these issues, a novel wavelet-based approach for physiological signal analysis
is presented, aiming to capture multi-scale time-frequency features in various
physiological signals. Leveraging this technique, two large-scale pretrained
models specific to EMG and ECG are introduced for the first time, achieving
superior performance and setting new baselines in downstream tasks.
Additionally, a unified multi-modal framework is constructed by integrating
pretrained EEG model, where each modality is guided through its dedicated
branch and fused via learnable weighted fusion. This design effectively
addresses challenges such as low signal-to-noise ratio, high inter-subject
variability, and device mismatch, outperforming existing methods on multi-modal
tasks. The proposed wavelet-based architecture lays a solid foundation for
analysis of diverse physiological signals, while the multi-modal design points
to next-generation physiological signal processing with potential impact on
wearable health monitoring, clinical diagnostics, and broader biomedical
applications.

</details>


### [44] [History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials](https://arxiv.org/abs/2506.10352)
*Binyao Guo, Zihan Lin, QiZhi He*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种端到端的学习框架，名为历史感知神经算子（HANO），用于基于数据驱动的路径依赖非弹性材料建模。HANO 通过从近期应变-应力历史的小段中预测材料响应，而无需依赖隐藏状态变量，从而克服了循环神经网络模型常见的自一致性问题，并且在复杂条件下表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 该研究旨在解决使用传统循环神经网络（RNN）进行路径依赖材料建模时遇到的自一致性问题以及对初始隐藏状态敏感的问题。此外，目标是开发一种能够处理不同路径离散化、不规则采样和噪声数据等复杂情况的数据驱动模型。

**方法:** 研究人员开发了历史感知神经算子（HANO），这是一种自回归模型，它利用傅里叶基础神经算子来实现离散化不变学习，并嵌入了分层自我注意机制以促进多尺度特征提取。这种方法将应力-应变演化建模为连续算子而非固定输入-输出映射，从而自然地适应各种路径离散化并提高鲁棒性。

**结果:** HANO 在两个基准问题上进行了评估：硬化弹塑性和脆性固体中的渐进各向异性损伤。结果表明，HANO 在预测准确性、泛化能力和鲁棒性方面始终优于基线模型。

**结论:** HANO 提供了一个有效的数据驱动代理，适用于模拟非弹性材料，并且非常适合与经典数值求解器集成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是History-Aware+Neural+Operator%3A+Robust+Data-Driven+Constitutive+Modeling+of+Path-Dependent+Materials，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10352，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10352&send_immediately=true&force_search=false)

**原文摘要:** This study presents an end-to-end learning framework for data-driven modeling
of path-dependent inelastic materials using neural operators. The framework is
built on the premise that irreversible evolution of material responses,
governed by hidden dynamics, can be inferred from observable data.
  We develop the History-Aware Neural Operator (HANO), an autoregressive model
that predicts path-dependent material responses from short segments of recent
strain-stress history without relying on hidden state variables, thereby
overcoming self-consistency issues commonly encountered in recurrent neural
network (RNN)-based models. Built on a Fourier-based neural operator backbone,
HANO enables discretization-invariant learning. To enhance its ability to
capture both global loading patterns and critical local path dependencies, we
embed a hierarchical self-attention mechanism that facilitates multiscale
feature extraction.
  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial
hidden states, a commonly overlooked issue that can lead to instability in
recurrent models when applied to generalized loading paths. By modeling
stress-strain evolution as a continuous operator rather than relying on fixed
input-output mappings, HANO naturally accommodates varying path discretizations
and exhibits robust performance under complex conditions, including irregular
sampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate
HANO on two benchmark problems: elastoplasticity with hardening and progressive
anisotropic damage in brittle solids. Results show that HANO consistently
outperforms baseline models in predictive accuracy, generalization, and
robustness. With its demonstrated capabilities, HANO provides an effective
data-driven surrogate for simulating inelastic materials and is well-suited for
integration with classical numerical solvers.

</details>


### [45] [TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree](https://arxiv.org/abs/2506.10355)
*Yu-Yang Qian, Yuan-Ze Xu, Zhen-Yu Zhang, Peng Zhao, Zhi-Hua Zhou*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的持续学习方法TreeLoRA，通过利用分层梯度相似性来构建层级适配器，并结合稀疏梯度更新和基于下置信界的算法来有效探索任务结构，从而提高了大型预训练模型的效率。


<details>
  <summary>更多</summary>
  
**动机:** 在数据流环境中，连续遇到学习任务需要持续学习（CL）在线更新模型，以适应新任务同时保留过去的知识防止灾难性遗忘。随着大型预训练模型（LPMs）的兴起，由于其巨大的计算需求和参数规模的增长，对于CL来说效率变得尤为重要。

**方法:** TreeLoRA方法通过使用低秩适配器和分层梯度相似性来构造层级适配器，以实现高效的持续学习。为了减少任务相似性估计的计算负担，采用了基于下置信界的算法有效地探索任务结构。此外，还使用了稀疏梯度更新来促进参数优化。

**结果:** 理论分析支持了这种方法的合理性，实验结果表明该方法在视觉转换器（ViTs）和大型语言模型（LLMs）上都是有效的，并且在不同领域中都表现出色，包括视觉和自然语言处理任务。

**结论:** TreeLoRA提供了一种有效的方法，使得大型预训练模型能够高效地进行持续学习，而不会导致灾难性的遗忘，而且在多个领域得到了验证。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TreeLoRA%3A+Efficient+Continual+Learning+via+Layer-Wise+LoRAs+Guided+by+a+Hierarchical+Gradient-Similarity+Tree，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10355&send_immediately=true&force_search=false)

**原文摘要:** Many real-world applications collect data in a streaming environment, where
learning tasks are encountered sequentially. This necessitates continual
learning (CL) to update models online, enabling adaptation to new tasks while
preserving past knowledge to prevent catastrophic forgetting. Nowadays, with
the flourish of large pre-trained models (LPMs), efficiency has become
increasingly critical for CL, due to their substantial computational demands
and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of
Low-Rank Adapters), a novel approach that constructs layer-wise adapters by
leveraging hierarchical gradient similarity to enable efficient CL,
particularly for LPMs. To reduce the computational burden of task similarity
estimation, we employ bandit techniques to develop an algorithm based on lower
confidence bounds to efficiently explore the task structure. Furthermore, we
use sparse gradient updates to facilitate parameter optimization, making the
approach better suited for LPMs. Theoretical analysis is provided to justify
the rationale behind our approach, and experiments on both vision transformers
(ViTs) and large language models (LLMs) demonstrate the effectiveness and
efficiency of our approach across various domains, including vision and natural
language processing tasks.

</details>


### [46] [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364)
*Penguin Huang, Chhavi Yadav, Ruihan Wu, Kamalika Chaudhuri*

**主要类别:** cs.LG

**AI概要:** 本文介绍了PropInfer，一个用于评估在两种微调范式下的大型语言模型属性推断的基准任务，并提出了两种针对性攻击方法。实验结果表明这些攻击能够揭示大型语言模型中未被认识到的脆弱性。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型越来越多地在特定领域数据集上进行微调以支持医疗、金融和法律等领域的应用，如何防止敏感和机密数据属性泄露成为一个亟待解决的问题。之前的研究已经探讨了对判别模型（如图像分类模型）和生成模型（如用于图像数据的GANs）的属性推断攻击，但对于大型语言模型是否也存在类似的风险尚不清楚。

**方法:** 研究者们引入了名为PropInfer的基准任务来评价大型语言模型在问答和聊天完成这两种微调模式下的属性推断能力。他们基于ChatDoctor数据集构建了该基准，涵盖了多种类型的属性和任务配置。此外，还提出了两种定制化攻击：一种是基于提示的生成攻击，另一种则是利用词频信号的影子模型攻击。

**结果:** 通过多个预训练的大型语言模型上的实证评估显示，所提出的攻击手段成功揭示了大型语言模型中存在的一个此前未被认知到的安全漏洞。

**结论:** 研究表明，即使是专门为处理文本而设计的大型语言模型也可能遭受属性推断攻击，这表明有必要开发新的防御措施来保护敏感信息不被泄露。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+We+Infer+Confidential+Properties+of+Training+Data+from+LLMs%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10364，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10364&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly fine-tuned on domain-specific
datasets to support applications in fields such as healthcare, finance, and
law. These fine-tuning datasets often have sensitive and confidential
dataset-level properties -- such as patient demographics or disease prevalence
-- that are not intended to be revealed. While prior work has studied property
inference attacks on discriminative models (e.g., image classification models)
and generative models (e.g., GANs for image data), it remains unclear if such
attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark
task for evaluating property inference in LLMs under two fine-tuning paradigms:
question-answering and chat-completion. Built on the ChatDoctor dataset, our
benchmark includes a range of property types and task configurations. We
further propose two tailored attacks: a prompt-based generation attack and a
shadow-model attack leveraging word frequency signals. Empirical evaluations
across multiple pretrained LLMs show the success of our attacks, revealing a
previously unrecognized vulnerability in LLMs.

</details>


### [47] [Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation](https://arxiv.org/abs/2506.10403)
*Tzu-Heng Huang, Harit Vishwakarma, Frederic Sala*

**主要类别:** cs.LG

**AI概要:** 介绍了一种新的评估模型生成质量的方法PAJAMA，它使用大型语言模型来合成可执行的评判程序，而不是直接评分。这种方法成本更低，且具有更高的判断一致性和更少的偏见响应。


<details>
  <summary>更多</summary>
  
**动机:** 现有的使用大型语言模型（LLMs）直接评估生成内容质量的方法存在高API成本、可靠性不确定、流程不灵活和内在偏见等问题。

**方法:** 提出了PAJAMA（Program-As-a-Judge for Automated Model Assessment），通过让LLM生成可执行的评判程序来替代直接打分的方式。这些程序可以本地存储并运行，极大降低了成本，并提供了可解释和可审计的评判逻辑。

**结果:** 与基于Qwen2.5-14B的LLM-as-a-judge相比，PAJAMA在一致性上提高了15.83%，减少了平均23.7%的偏见回应。在CHAT-HARD子集上的表现优于其他指标，Prometheus上提高2.19%，JudgeLM数据集上提升8.67%，同时成本低三个数量级。

**结论:** PAJAMA作为一种新的自动化模型评估方法，不仅显著降低了评估成本，还提高了评估的一致性和公正性，为模型生成质量评价提供了一个有效的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+To+Impeach+LLM-as-a-Judge%3A+Programs+are+the+Future+of+Evaluation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10403，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10403&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are widely used to evaluate the quality of LLM
generations and responses, but this leads to significant challenges: high API
costs, uncertain reliability, inflexible pipelines, and inherent biases. To
address these, we introduce PAJAMA (Program-As-a-Judge for Automated Model
Assessment), a new alternative that uses LLMs to synthesize executable judging
programs instead of directly scoring responses. These synthesized programs can
be stored and run locally, costing orders of magnitude less while providing
interpretable, and auditable judging logic that can be easily adapted.
Program-based judges mitigate biases, improving judgment consistency by 15.83%
and reducing biased responses by 23.7% on average compared to a
Qwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a
model, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of
RewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the
JudgeLM dataset, all at three orders of magnitude lower cost.

</details>


### [48] [EQA-RM: A Generative Embodied Reward Model with Test-time Scaling](https://arxiv.org/abs/2506.10389)
*Yuhang Chen, Zhen Tan, Tianlong Chen*

**主要类别:** cs.LG

**AI概要:** 我们提出了一种新的生成式多模态奖励模型EQA-RM，专为具身问答任务设计，并通过创新的对比组相对策略优化（C-GRPO）策略训练。EQA-RM能够提供可解释的结构化奖励反馈，并且在不重新训练的情况下调整评估粒度。此外，我们还提出了EQARewardBench基准来标准化EQA奖励模型的评估。EQA-RM表现出高样本效率，在仅使用700个样本的情况下达到了61.9%的准确率，优于多个强大的基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 现有的奖励模型（RMs）对于复杂的具身任务如具身问答（EQA），在评估代理的空间、时间和逻辑理解方面存在不足。通用的方法未能考虑这些细微差别。

**方法:** 引入了EQA-RM，这是一种专门为EQA设计的新生成式多模态奖励模型，并通过创新的对比组相对策略优化（C-GRPO）策略进行训练。该模型能够学习细粒度的行为差异，并提供超越简单标量的可解释的结构化奖励反馈。

**结果:** EQA-RM展现出了高的样本效率，在仅使用700个样本的情况下达到了61.9%的准确性，这一表现超过了包括Gemini-2.5-Flash、GPT-4o、Claude-3.5-Haiku以及开源的最先进模型如RoVRM和VisualPRM在内的强大基线。

**结论:** EQA-RM是一种有效的解决方案，它能够针对复杂具身任务中的细微行为差异提供细致的评价，同时保持了较高的样本效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EQA-RM%3A+A+Generative+Embodied+Reward+Model+with+Test-time+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10389&send_immediately=true&force_search=false)

**原文摘要:** Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.

</details>


### [49] [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/abs/2506.10412)
*Ching Chang, Jeehyun Hwang, Yidan Shi, Haixin Wang, Wen-Chih Peng, Tien-Fu Chen, Wei Wang*

**主要类别:** cs.LG

**AI概要:** 本文介绍了Time-IMM数据集和IMM-TSF基准库，用于处理真实世界中不规则、多模态的时间序列数据。这些工具旨在弥补现有研究与实际部署之间的差距，并提供了专门的融合模块来提高预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列分析基准通常假设数据是干净的、定期采样的单模态数据，这与现实中经常遇到的不规则、多模态且有缺失的数据存在显著差异。为了解决这一问题并促进现实条件下的时间序列分析进展，作者提出了Time-IMM数据集和IMM-TSF基准库。

**方法:** 创建了Time-IMM数据集以捕捉由原因驱动的不规则性在多变量多模态时间序列中的表现，并引入了IMM-TSF，一个针对不规则多模态时间序列预测的基准库。IMM-TSF包含专门的融合模块，如时间戳到文本融合模块和多模态融合模块，支持基于最近性的平均策略和基于注意力的集成策略。

**结果:** 实证结果表明，在不规则时间序列数据上显式建模多模态可以大幅度提高预测性能。

**结论:** Time-IMM数据集和IMM-TSF基准库为在现实条件下推进时间序列分析提供了一个坚实的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time-IMM%3A+A+Dataset+and+Benchmark+for+Irregular+Multimodal+Multivariate+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10412，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10412&send_immediately=true&force_search=false)

**原文摘要:** Time series data in real-world applications such as healthcare, climate
modeling, and finance are often irregular, multimodal, and messy, with varying
sampling rates, asynchronous modalities, and pervasive missingness. However,
existing benchmarks typically assume clean, regularly sampled, unimodal data,
creating a significant gap between research and real-world deployment. We
introduce Time-IMM, a dataset specifically designed to capture cause-driven
irregularity in multimodal multivariate time series. Time-IMM represents nine
distinct types of time series irregularity, categorized into trigger-based,
constraint-based, and artifact-based mechanisms. Complementing the dataset, we
introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal
time series, enabling asynchronous integration and realistic evaluation.
IMM-TSF includes specialized fusion modules, including a timestamp-to-text
fusion module and a multimodality fusion module, which support both
recency-aware averaging and attention-based integration strategies. Empirical
results demonstrate that explicitly modeling multimodality on irregular time
series data leads to substantial gains in forecasting performance. Time-IMM and
IMM-TSF provide a foundation for advancing time series analysis under
real-world conditions. The dataset is publicly available at
https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the
benchmark library can be accessed at
https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.

</details>


### [50] [Generative Algorithms for Wildfire Progression Reconstruction from Multi-Modal Satellite Active Fire Measurements and Terrain Height](https://arxiv.org/abs/2506.10404)
*Bryan Shaddy, Brianna Binder, Agnimitra Dasgupta, Haitong Qin, James Haley, Angel Farguell, Kyle Hilburn, Derek V. Mallia, Adam Kochanski, Jan Mandel, Assad Oberai*

**主要类别:** cs.LG

**AI概要:** 本研究开发了一种方法，通过VIIRS活动火点测量、GOES衍生的点火时间和地形高度数据来估计火灾进程，并使用条件生成对抗网络结合WRF-SFIRE模型的物理特性进行训练。该方法在五个太平洋美国野火案例中得到验证，平均Sørensen-Dice系数为0.81。


<details>
  <summary>更多</summary>
  
**动机:** 由于多日模拟过程中最复杂的野火模型与观察到的进展出现分歧，因此需要数据同化来改进预测。

**方法:** 采用条件生成对抗网络（cGAN）并用历史野火模拟进行训练，从而将WRF-SFIRE模型的物理特性纳入估计之中。火灾进程由火到达时间简洁表示，且训练数据通过对WRF-SFIRE解决方案应用近似观测算子获得。

**结果:** 所提出的方法在五个太平洋美国野火案例上进行了验证，结果表明与飞机测量的高分辨率周长相比，平均Sørensen-Dice系数为0.81。此外，还评估了地形高度对到达时间推断的影响，发现当推断基于卫星测量时，地形影响最小。

**结论:** 研究表明，利用cGAN和WRF-SFIRE模型物理学特性的方法可以有效地从卫星测量中估计出火灾进程，而地形高度对基于卫星测量的到达时间推断影响较小。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+Algorithms+for+Wildfire+Progression+Reconstruction+from+Multi-Modal+Satellite+Active+Fire+Measurements+and+Terrain+Height，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10404，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10404&send_immediately=true&force_search=false)

**原文摘要:** Increasing wildfire occurrence has spurred growing interest in wildfire
spread prediction. However, even the most complex wildfire models diverge from
observed progression during multi-day simulations, motivating need for data
assimilation. A useful approach to assimilating measurement data into complex
coupled atmosphere-wildfire models is to estimate wildfire progression from
measurements and use this progression to develop a matching atmospheric state.
In this study, an approach is developed for estimating fire progression from
VIIRS active fire measurements, GOES-derived ignition times, and terrain height
data. A conditional Generative Adversarial Network is trained with simulations
of historic wildfires from the atmosphere-wildfire model WRF-SFIRE, thus
allowing incorporation of WRF-SFIRE physics into estimates. Fire progression is
succinctly represented by fire arrival time, and measurements for training are
obtained by applying an approximate observation operator to WRF-SFIRE
solutions, eliminating need for satellite data during training. The model is
trained on tuples of fire arrival times, measurements, and terrain, and once
trained leverages measurements of real fires and corresponding terrain data to
generate samples of fire arrival times. The approach is validated on five
Pacific US wildfires, with results compared against high-resolution perimeters
measured via aircraft, finding an average Sorensen-Dice coefficient of 0.81.
The influence of terrain height on the arrival time inference is also evaluated
and it is observed that terrain has minimal influence when the inference is
conditioned on satellite measurements.

</details>


### [51] [Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code](https://arxiv.org/abs/2506.10617)
*Reza Karbasi, Masoud Rahimi, Abdol-Hossein Vahabie, Hadi Moradi*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种两阶段的管道来解决心电图信号重叠问题，提高了纸基心电图记录数字化的准确性。实验结果显示，该方法在处理非重叠和重叠的心电图样本时均优于现有技术，并且在公开的GitHub存储库中提供了实现代码。


<details>
  <summary>更多</summary>
  
**动机:** 针对纸基心电图（ECG）记录准确数字化这一持续存在的挑战，特别是对于因信号重叠而受损的单导联，提出了新的解决方案。

**方法:** 提出了一种两阶段的处理流程：第一阶段使用基于U-Net的分割网络，训练数据集包含了增强的重叠信号；第二阶段通过既定的数字化技术将优化后的二进制掩模转换为时间序列信号，并引入自适应网格检测模块以提高不同ECG格式和尺度下的通用性。

**结果:** 实验结果表明所提出的方法有效。U-Net架构对精细分割任务实现了0.87的IoU值，在非重叠和重叠信号上都表现出比基准更好的性能。具体来说，在非重叠信号上MSE为0.0010、相关系数rho为0.9644；而在重叠信号上MSE为0.0029、rho为0.9641，明显优于基准方法。

**结论:** 本研究提出的方法显著提升了存在信号重叠情况下的心电图数字化精度，为可靠地将模拟心电图记录转化为可分析的数字数据奠定了坚实基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Learning-Based+Digitization+of+Overlapping+ECG+Images+with+Open-Source+Python+Code，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10617，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10617&send_immediately=true&force_search=false)

**原文摘要:** This paper addresses the persistent challenge of accurately digitizing
paper-based electrocardiogram (ECG) recordings, with a particular focus on
robustly handling single leads compromised by signal overlaps-a common yet
under-addressed issue in existing methodologies. We propose a two-stage
pipeline designed to overcome this limitation. The first stage employs a U-Net
based segmentation network, trained on a dataset enriched with overlapping
signals and fortified with custom data augmentations, to accurately isolate the
primary ECG trace. The subsequent stage converts this refined binary mask into
a time-series signal using established digitization techniques, enhanced by an
adaptive grid detection module for improved versatility across different ECG
formats and scales. Our experimental results demonstrate the efficacy of our
approach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained
segmentation task. Crucially, our proposed digitization method yields superior
performance compared to a well-established baseline technique across both
non-overlapping and challenging overlapping ECG samples. For non-overlapping
signals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson
Correlation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366,
respectively, for the baseline. On samples with signal overlap, our method
achieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the
baseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to
significantly enhance digitization accuracy, especially in the presence of
signal overlaps, thereby laying a strong foundation for the reliable conversion
of analog ECG records into analyzable digital data for contemporary research
and clinical applications. The implementation is publicly available at this
GitHub repository: https://github.com/masoudrahimi39/ECG-code.

</details>


### [52] [Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning](https://arxiv.org/abs/2506.10629)
*Yucheng Yang, Tianyi Zhou, Qiang He, Lei Han, Mykola Pechenizkiy, Meng Fang*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种新的技能学习目标WSEP以及基于Wasserstein距离的算法PWSEP，旨在改进MISL方法，并且能够为下游任务发现更多的初始策略。


<details>
  <summary>更多</summary>
  
**动机:** 无监督强化学习（URL）的目标是学习适用于未见过的下游任务的一般技能。尽管互信息技能学习（MISL）通过最大化状态与技能之间的互信息来解决URL问题，但缺乏足够的理论分析来说明其学到的技能如何初始化下游任务的策略。

**方法:** 提出了一个新的解缠度量LSEPIN，并建立了LSEPIN和下游任务适应成本之间的信息几何联系。为了获得更好的几何特性，研究了用Wasserstein距离替换信息几何中的KL散度的新策略，并进一步扩展了几何分析，从而形成了新的技能学习目标WSEP。此外，还提出了另一个基于Wasserstein距离的算法PWSEP。

**结果:** 理论上证明了WSEP对于下游任务适应是有帮助的，并且可以比MISL发现更多用于下游任务的初始策略。PWSEP则被设计成能够理论上发现所有最优初始策略。

**结论:** 论文的工作表明，所学技能的多样性和可分离性对下游任务适应至关重要，而MISL并不保证这些性质。提出的WSEP和PWSEP算法有望改善这种情况，并有助于下游任务的策略初始化。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Task+Adaptation+from+Skills%3A+Information+Geometry%2C+Disentanglement%2C+and+New+Objectives+for+Unsupervised+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10629，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10629&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised reinforcement learning (URL) aims to learn general skills for
unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL
by maximizing the mutual information between states and skills but lacks
sufficient theoretical analysis, e.g., how well its learned skills can
initialize a downstream task's policy. Our new theoretical analysis in this
paper shows that the diversity and separability of learned skills are
fundamentally critical to downstream task adaptation but MISL does not
necessarily guarantee these properties. To complement MISL, we propose a novel
disentanglement metric LSEPIN. Moreover, we build an information-geometric
connection between LSEPIN and downstream task adaptation cost. For better
geometric properties, we investigate a new strategy that replaces the KL
divergence in information geometry with Wasserstein distance. We extend the
geometric analysis to it, which leads to a novel skill-learning objective WSEP.
It is theoretically justified to be helpful to downstream task adaptation and
it is capable of discovering more initial policies for downstream tasks than
MISL. We finally propose another Wasserstein distance-based algorithm PWSEP
that can theoretically discover all optimal initial policies.

</details>


### [53] [Data-Driven Soil Organic Carbon Sampling: Integrating Spectral Clustering with Conditioned Latin Hypercube Optimization](https://arxiv.org/abs/2506.10419)
*Weiying Zhao, Aleksei Unagaev, Natalia Efremova*

**主要类别:** cs.LG

**AI概要:** 提出了一种结合谱聚类和条件拉丁超立方体采样(cLHS)的新混合方法，以提高土壤有机碳(SOC)采样的代表性。该方法通过多变量协变量数据将研究区域划分为K个同质区域，并在每个区域内应用cLHS来选择能够全面捕捉环境条件多样性的采样点。实验证明此方法比标准cLHS提供了更均匀的协变量特征空间和空间异质性覆盖。


<details>
  <summary>更多</summary>
  
**动机:** 传统的SOC监测依赖于基于环境协变量选取有代表性的野外采样位置，但单纯使用cLHS可能会忽略一些重要的小环境簇群。为了改善这一局限性，研究者提出了一个新方法，旨在提供更好的样本多样性，从而可能提高机器学习模型预测SOC的准确性。

**方法:** 研究采用了谱聚类算法，这是一种无监督的机器学习技术，用于根据多变量协变量数据将研究区域划分成K个同质区。然后，在每个区内使用条件拉丁超立方体采样(cLHS)来挑选出能够反映整个环境条件多样性的采样点。

**结果:** 实验结果表明，与传统的cLHS相比，所提出的光谱-cLHS方法为协变量特征空间和空间异质性提供了更加均匀的覆盖。

**结论:** 通过引入谱聚类到cLHS中形成的混合方法提高了SOC采样的代表性，有助于更好地平衡训练数据，进而有可能提高机器学习模型对SOC预测的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-Driven+Soil+Organic+Carbon+Sampling%3A+Integrating+Spectral+Clustering+with+Conditioned+Latin+Hypercube+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10419，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10419&send_immediately=true&force_search=false)

**原文摘要:** Soil organic carbon (SOC) monitoring often relies on selecting representative
field sampling locations based on environmental covariates. We propose a novel
hybrid methodology that integrates spectral clustering - an unsupervised
machine learning technique with conditioned Latin hypercube sampling (cLHS) to
enhance the representativeness of SOC sampling. In our approach, spectral
clustering partitions the study area into $K$ homogeneous zones using
multivariate covariate data, and cLHS is then applied within each zone to
select sampling locations that collectively capture the full diversity of
environmental conditions. This hybrid spectral-cLHS method ensures that even
minor but important environmental clusters are sampled, addressing a key
limitation of vanilla cLHS which can overlook such areas. We demonstrate on a
real SOC mapping dataset that spectral-cLHS provides more uniform coverage of
covariate feature space and spatial heterogeneity than standard cLHS. This
improved sampling design has the potential to yield more accurate SOC
predictions by providing better-balanced training data for machine learning
models.

</details>


### [54] [Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs](https://arxiv.org/abs/2506.10630)
*Yucong Luo, Yitong Zhou, Mingyue Cheng, Jiahao Wang, Daoyu Wang, Tingyue Pan, Jintao Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Time-R1的两阶段强化微调框架，旨在增强大型语言模型在时间序列预测中的多步推理能力。通过监督微调和强化学习相结合的方式，并设计了针对时间序列预测的细粒度多目标奖励机制以及基于组的相对重要性策略优化方法（GRIP），实验表明该方法能显著提高不同数据集上的预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的时间序列预测方法大多依赖于历史模式的提取并映射到未来值，缺乏一个明确的中间时间序列推理过程。虽然新出现的慢思考型大语言模型展示了出色的多步推理能力，但仅依靠提示工程存在高计算成本、隐私风险及领域特定时间序列深入推理能力有限等问题。为解决这些问题，训练大语言模型发展慢思考能力和强时间序列推理技能成为一种更有前景的方法。

**方法:** 提出了Time-R1，这是一个两阶段的强化微调框架，第一阶段进行有监督的微调以适应初步调整，第二阶段使用强化学习来提高模型的泛化能力。特别地，设计了一个专门针对时间序列预测的细粒度多目标奖励，并引入了GRIP（基于群体的相对重要性政策优化），利用非均匀抽样进一步鼓励和优化模型探索有效推理路径。

**结果:** 实验结果表明，Time-R1能够在不同的数据集上显著改善预测表现。

**结论:** 通过结合监督学习与强化学习，Time-R1能够有效地提升大型语言模型的时间序列预测能力，特别是在需要复杂推理的情况下。所提出的GRIP方法也证明了其在促进模型发现更优解方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+Series+Forecasting+as+Reasoning%3A+A+Slow-Thinking+Approach+with+Reinforced+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10630，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10630&send_immediately=true&force_search=false)

**原文摘要:** To advance time series forecasting (TSF), various methods have been proposed
to improve prediction accuracy, evolving from statistical techniques to
data-driven deep learning architectures. Despite their effectiveness, most
existing methods still adhere to a fast thinking paradigm-relying on extracting
historical patterns and mapping them to future values as their core modeling
philosophy, lacking an explicit thinking process that incorporates intermediate
time series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)
have shown remarkable multi-step reasoning capabilities, offering an
alternative way to overcome these issues. However, prompt engineering alone
presents several limitations - including high computational cost, privacy
risks, and limited capacity for in-depth domain-specific time series reasoning.
To address these limitations, a more promising approach is to train LLMs to
develop slow thinking capabilities and acquire strong time series reasoning
skills. For this purpose, we propose Time-R1, a two-stage reinforcement
fine-tuning framework designed to enhance multi-step reasoning ability of LLMs
for time series forecasting. Specifically, the first stage conducts supervised
fine-tuning for warmup adaptation, while the second stage employs reinforcement
learning to improve the model's generalization ability. Particularly, we design
a fine-grained multi-objective reward specifically for time series forecasting,
and then introduce GRIP (group-based relative importance for policy
optimization), which leverages non-uniform sampling to further encourage and
optimize the model's exploration of effective reasoning paths. Experiments
demonstrate that Time-R1 significantly improves forecast performance across
diverse datasets.

</details>


### [55] [System Identification Using Kolmogorov-Arnold Networks: A Case Study on Buck Converters](https://arxiv.org/abs/2506.10434)
*Nart Gashi, Panagiotis Kakosimos, George Papafotiou*

**主要类别:** cs.LG

**AI概要:** 本文研究了Kolmogorov-Arnold网络（KANs）在降压转换器系统动态建模和分析中的应用，展示了KANs能够准确识别系统动力学、验证模型一致性和检测参数变化的能力。


<details>
  <summary>更多</summary>
  
**动机:** 探索Kolmogorov-Arnold网络作为一种强大且可解释的框架来识别动态系统的状态空间参数，并发现系统方程。

**方法:** 使用KANs近似状态导数，构建可解释的状态空间表示，并通过数值实验验证这些模型。

**结果:** 结果表明，KANs能够准确地识别系统动态，验证模型一致性，并检测参数变化。

**结论:** KANs为现代工业系统中的系统识别提供了宝贵见解，并证明了其作为传统神经网络更优选择的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是System+Identification+Using+Kolmogorov-Arnold+Networks%3A+A+Case+Study+on+Buck+Converters，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10434，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10434&send_immediately=true&force_search=false)

**原文摘要:** Kolmogorov-Arnold Networks (KANs) are emerging as a powerful framework for
interpretable and efficient system identification in dynamic systems. By
leveraging the Kolmogorov-Arnold representation theorem, KANs enable function
approximation through learnable activation functions, offering improved
scalability, accuracy, and interpretability compared to traditional neural
networks. This paper investigates the application of KANs to model and analyze
the dynamics of a buck converter system, focusing on state-space parameter
estimation along with discovering the system equations. Using simulation data,
the methodology involves approximating state derivatives with KANs,
constructing interpretable state-space representations, and validating these
models through numerical experiments. The results demonstrate the ability of
KANs to accurately identify system dynamics, verify model consistency, and
detect parameter changes, providing valuable insights into their applicability
for system identification in modern industrial systems.

</details>


### [56] [Data Shifts Hurt CoT: A Theoretical Study](https://arxiv.org/abs/2506.10647)
*Lang Yin, Debangshu Banerjee, Gagandeep Singh*

**主要类别:** cs.LG

**AI概要:** 该研究首次严格探讨了数据偏移对Chain of Thought (CoT)方法的影响，特别是在解决$k$-parity问题时联合考虑分布偏移和数据中毒两种数据偏移对模型质量的影响。此外，还揭示了使用CoT方法在学习奇偶性问题上可能比直接生成预测表现更差的现象，并对此现象的机制原因进行了严谨且全面的解释。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先前的研究表明通过Chain of Thought (CoT)方法可以增强transformers处理复杂计算问题的能力，但这些研究基于两个关键假设：训练与测试数据分布相同以及训练数据不含错误。本工作旨在探究当现实世界中这些假设不成立时，不同类型的数据偏移（如分布偏移、数据中毒）如何影响采用CoT分解训练所得模型的质量。

**方法:** 研究集中在$k$-parity问题上，通过实验分析了当出现分布偏移和数据中毒这两种情况时，利用成熟CoT技术训练出的模型性能变化。

**结果:** 研究表明，在遇到特定类型的数据偏移情况下，采用CoT方法训练的模型在学习$k$-parity问题时的表现反而不如直接进行预测的方法；并从机制层面详细解释了背后的原因。

**结论:** 虽然CoT对于提升大型语言模型输出质量有效，但在面对现实世界中存在的数据偏移挑战时，其效果可能会受到负面影响。此发现提示未来研究需更加重视提高模型对抗此类问题的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Shifts+Hurt+CoT%3A+A+Theoretical+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10647，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10647&send_immediately=true&force_search=false)

**原文摘要:** Chain of Thought (CoT) has been applied to various large language models
(LLMs) and proven to be effective in improving the quality of outputs. In
recent studies, transformers are proven to have absolute upper bounds in terms
of expressive power, and consequently, they cannot solve many computationally
difficult problems. However, empowered by CoT, transformers are proven to be
able to solve some difficult problems effectively, such as the $k$-parity
problem. Nevertheless, those works rely on two imperative assumptions: (1)
identical training and testing distribution, and (2) corruption-free training
data with correct reasoning steps. However, in the real world, these
assumptions do not always hold. Although the risks of data shifts have caught
attention, our work is the first to rigorously study the exact harm caused by
such shifts to the best of our knowledge. Focusing on the $k$-parity problem,
in this work we investigate the joint impact of two types of data shifts: the
distribution shifts and data poisoning, on the quality of trained models
obtained by a well-established CoT decomposition. In addition to revealing a
surprising phenomenon that CoT leads to worse performance on learning parity
than directly generating the prediction, our technical results also give a
rigorous and comprehensive explanation of the mechanistic reasons of such
impact.

</details>


### [57] [MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices](https://arxiv.org/abs/2506.10443)
*Zhaode Wang, Jingbang Yang, Xinyu Qian, Shiwen Xing, Xiaotang Jiang, Chengfei Lv, Shengyu Zhang*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为MNN-LLM的框架，该框架通过模型量化和DRAM-Flash混合存储技术来减少大型语言模型在移动设备上部署时的内存使用，并且通过多种策略提升了性能，相比于当前主流的LLM特定框架，实现了高达8.6倍的速度提升。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）虽然在各种任务中表现出色，但其巨大的规模导致推理过程中计算资源消耗巨大，成本高昂。边缘设备推理提供了一个有希望的解决方案，但面临内存使用和推理速度的主要挑战。

**方法:** MNN-LLM是一个专门设计用于加速大型语言模型在移动设备上部署的框架。它通过模型量化和DRAM-Flash混合存储来解决LLMs运行时的特点，从而有效减少了内存使用。此外，根据移动CPU指令集和GPU特性重新排列权重和输入，并采用多核负载均衡、混合精度浮点运算以及几何计算等策略来提高性能。

**结果:** MNN-LLM与当前主流的LLM专用框架相比，实现了最高达8.6倍的速度提升。

**结论:** MNN-LLM框架为大型语言模型在移动设备上的高效部署提供了有效的方案，通过一系列优化措施显著提高了性能并降低了内存占用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MNN-LLM%3A+A+Generic+Inference+Engine+for+Fast+Large+Language+Model+Deployment+on+Mobile+Devices，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10443&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have demonstrated exceptional performance across
a variety of tasks. However, their substantial scale leads to significant
computational resource consumption during inference, resulting in high costs.
Consequently, edge device inference presents a promising solution. The primary
challenges of edge inference include memory usage and inference speed. This
paper introduces MNN-LLM, a framework specifically designed to accelerate the
deployment of large language models on mobile devices. MNN-LLM addresses the
runtime characteristics of LLMs through model quantization and DRAM-Flash
hybrid storage, effectively reducing memory usage. It rearranges weights and
inputs based on mobile CPU instruction sets and GPU characteristics while
employing strategies such as multicore load balancing, mixed-precision
floating-point operations, and geometric computations to enhance performance.
Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current
mainstream LLM-specific frameworks.

</details>


### [58] [Saturation Self-Organizing Map](https://arxiv.org/abs/2506.10680)
*Igor Urbanik, Paweł Gajewski*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Saturation Self-Organizing Maps (SatSOM)的方法，它通过引入新的饱和机制来减少神经元的学习率和邻域半径，从而提高连续学习场景中的知识保留。


<details>
  <summary>更多</summary>
  
**动机:** 持续学习对神经系统构成了一个基本的挑战，当面对顺序任务时，它们经常遭受灾难性的遗忘。尽管自组织映射（SOMs）具有可解释性和效率，但也不能幸免于此问题。

**方法:** 提出了Saturation Self-Organizing Maps (SatSOM)，它是SOMs的一种扩展，设计用于改善持续学习情景下的知识保留。SatSOM引入了一种新颖的饱和机制，随着神经元累积信息，该机制逐渐减少其学习率和邻域半径。

**结果:** 这种方法有效地冻结了训练良好的神经元，并将学习重定向到映射中未充分利用的区域。

**结论:** SatSOM方法提供了一种有效的方式来对抗在持续学习过程中出现的灾难性遗忘问题，通过调整神经元的学习速率和邻居范围来保持已有知识的同时吸收新信息。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Saturation+Self-Organizing+Map，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10680，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10680&send_immediately=true&force_search=false)

**原文摘要:** Continual learning poses a fundamental challenge for neural systems, which
often suffer from catastrophic forgetting when exposed to sequential tasks.
Self-Organizing Maps (SOMs), despite their interpretability and efficiency, are
not immune to this issue. In this paper, we introduce Saturation
Self-Organizing Maps (SatSOM)-an extension of SOMs designed to improve
knowledge retention in continual learning scenarios. SatSOM incorporates a
novel saturation mechanism that gradually reduces the learning rate and
neighborhood radius of neurons as they accumulate information. This effectively
freezes well-trained neurons and redirects learning to underutilized areas of
the map.

</details>


### [59] [Equivariant Neural Diffusion for Molecule Generation](https://arxiv.org/abs/2506.10532)
*François Cornet, Grigory Bartosh, Mikkel N. Schmidt, Christian A. Naesseth*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的3D分子生成模型Equivariant Neural Diffusion (END)，该模型对欧几里得变换具有等变性，并通过可学习的前向过程改进了生成建模。实验表明，END在标准分子生成基准测试中与多个强大的基线相比表现出了竞争力。


<details>
  <summary>更多</summary>
  
**动机:** 为了改进当前最先进的等变扩散模型，研究者引入了Equivariant Neural Diffuction (END) 模型，它对刚性变换具有等变性并且通过一个可学习的前向过程来提高生成建模的能力。

**方法:** END的关键创新在于它的可学习前向过程，该过程通过时间及数据相关的转换参数化，这个转换对于刚性变换是等变的。

**结果:** 一系列实验结果表明，END在标准分子生成基准上与若干强基线相比，在无条件和有条件生成方面都表现出竞争性的性能。

**结论:** END模型为3D分子生成提供了一个新的方法，通过对欧几里得变换保持等变性以及采用可学习的前向过程，它在生成任务中展示了良好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Equivariant+Neural+Diffusion+for+Molecule+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10532&send_immediately=true&force_search=false)

**原文摘要:** We introduce Equivariant Neural Diffusion (END), a novel diffusion model for
molecule generation in 3D that is equivariant to Euclidean transformations.
Compared to current state-of-the-art equivariant diffusion models, the key
innovation in END lies in its learnable forward process for enhanced generative
modelling. Rather than pre-specified, the forward process is parameterized
through a time- and data-dependent transformation that is equivariant to rigid
transformations. Through a series of experiments on standard molecule
generation benchmarks, we demonstrate the competitive performance of END
compared to several strong baselines for both unconditional and conditional
generation.

</details>


### [60] [ConTextTab: A Semantics-Aware Tabular In-Context Learner](https://arxiv.org/abs/2506.10707)
*Marco Spinaci, Marek Polewczyk, Maximilian Schambach, Sam Thelin*

**主要类别:** cs.LG

**AI概要:** 本文介绍了ConTextTab，一种结合了语义理解和对齐的表格原生ICL框架。通过使用针对不同数据模态的专业嵌入并在大规模真实世界表格数据上进行训练，该模型在多个基准测试中与最先进水平相当，并在语义丰富的CARTE基准上树立了新的标准。


<details>
  <summary>更多</summary>
  
**动机:** 目前的表格原生ICL架构虽然在处理表格数据方面效率高且适应性好，但仅使用合成数据进行训练，无法充分利用现实世界表格数据中的丰富语义和世界知识。另一方面，基于预训练大型语言模型的表格ICL模型能够整合深度语义理解与世界知识，但由于架构限制只能利用少量上下文。为了解决这些问题，研究者们希望将两者的优势结合起来。

**方法:** 研究人员提出了ConTextTab，这是一种新型的表格原生ICL框架，它结合了语义理解和对齐。该方法采用了针对不同类型数据模式（如文本、数值等）定制化的嵌入技术，并在大量的实际表格数据集上进行了训练。

**结果:** ConTextTab在一系列广泛的基准测试中表现出了与当前最先进技术相媲美的竞争力，并且在需要深层次语义理解的任务上，比如CARTE基准测试中，达到了一个新的高度。

**结论:** ConTextTab提供了一种有效的方法来结合语义理解和表格原生ICL的优点，从而在多种任务上实现了高水平的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ConTextTab%3A+A+Semantics-Aware+Tabular+In-Context+Learner，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10707，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10707&send_immediately=true&force_search=false)

**原文摘要:** Tabular in-context learning (ICL) has recently achieved state-of-the-art
(SOTA) performance on several tabular prediction tasks. Previously restricted
to classification problems on small tables, recent advances such as TabPFN and
TabICL have extended its use to larger datasets. While being architecturally
efficient and well-adapted to tabular data structures, current table-native ICL
architectures, being trained exclusively on synthetic data, do not fully
leverage the rich semantics and world knowledge contained in real-world tabular
data. On another end of this spectrum, tabular ICL models based on pretrained
large language models such as TabuLa-8B integrate deep semantic understanding
and world knowledge but are only able to make use of a small amount of context
due to inherent architectural limitations. With the aim to combine the best of
both these worlds, we introduce ConTextTab, integrating semantic understanding
and alignment into a table-native ICL framework. By employing specialized
embeddings for different data modalities and by training on large-scale
real-world tabular data, our model is competitive with SOTA across a broad set
of benchmarks while setting a new standard on the semantically rich CARTE
benchmark.

</details>


### [61] [Data-driven Day Ahead Market Prices Forecasting: A Focus on Short Training Set Windows](https://arxiv.org/abs/2506.10536)
*Vasilis Michalakopoulos, Christoforos Menos-Aikateriniadis, Elissaios Sarmas, Antonis Zakynthinos, Pavlos S. Georgilakis, Dimitris Askounis*

**主要类别:** cs.LG

**AI概要:** 研究了机器学习模型在使用短期历史训练窗口预测电力日前市场价格的表现，特别关注季节性趋势和价格峰值的检测。LightGBM 在不同训练窗口长度下表现最佳，特别是在45天和60天时，在波动大、数据稀缺的环境下结合短窗口训练方法和提升方法能够有效支持日前市场的预测。


<details>
  <summary>更多</summary>
  
**动机:** 该研究旨在评估机器学习模型在利用有限的历史数据进行训练时对于电力日前市场价格的预测能力，并着重于识别季节性模式和价格高峰。

**方法:** 研究采用了四种模型：带前馈误差校正（FFEC）的LSTM、XGBoost、LightGBM以及CatBoost，在三个欧洲能源市场（希腊、比利时、爱尔兰）上进行了测试。使用的特征集来源于ENTSO-E的预测数据，训练窗口期从7天到90天不等。

**结果:** 结果显示，LightGBM持续达到最高的预测准确性和鲁棒性，尤其是在45天和60天训练窗口下表现最好。此外，相对于LSTM和其他提升模型，LightGBM显示出更好的季节效应和价格高峰事件检测能力。

**结论:** 研究表明，通过采用较短的训练窗口与提升方法相结合的方式，即使在数据高度波动且相对稀缺的情况下，也能有效地辅助日前市场价格预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data-driven+Day+Ahead+Market+Prices+Forecasting%3A+A+Focus+on+Short+Training+Set+Windows，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10536，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10536&send_immediately=true&force_search=false)

**原文摘要:** This study investigates the performance of machine learning models in
forecasting electricity Day-Ahead Market (DAM) prices using short historical
training windows, with a focus on detecting seasonal trends and price spikes.
We evaluate four models, namely LSTM with Feed Forward Error Correction (FFEC),
XGBoost, LightGBM, and CatBoost, across three European energy markets (Greece,
Belgium, Ireland) using feature sets derived from ENTSO-E forecast data.
Training window lengths range from 7 to 90 days, allowing assessment of model
adaptability under constrained data availability. Results indicate that
LightGBM consistently achieves the highest forecasting accuracy and robustness,
particularly with 45 and 60 day training windows, which balance temporal
relevance and learning depth. Furthermore, LightGBM demonstrates superior
detection of seasonal effects and peak price events compared to LSTM and other
boosting models. These findings suggest that short-window training approaches,
combined with boosting methods, can effectively support DAM forecasting in
volatile, data-scarce environments.

</details>


### [62] [Efficiency Robustness of Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.10831)
*Ravishka Rathnasuriya, Tingxi Li, Zexin Xu, Zihe Song, Mirazul Haque, Simin Chen, Wei Yang*

**主要类别:** cs.LG

**AI概要:** 本文系统地探讨了动态深度学习系统的效率鲁棒性，首次提出了效率攻击的全面分类，并分析了针对这些系统效率的对抗策略以及现有防御机制的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 随着动态深度学习系统（DDLSs）在实时应用中的部署增加，尤其是在资源受限环境如移动和物联网设备中，为了提高效率而引入的新攻击面成为安全问题。效率对抗攻击利用这些动态机制来降低系统性能。

**方法:** 文章通过深入评估分析了针对DDLSs效率的对抗策略，并基于三种动态行为对攻击进行了分类：每推理的动态计算攻击、动态推理迭代攻击及面向下游任务的动态输出产生攻击。

**结果:** 研究揭示了保障这些系统安全的关键挑战，并调查了现有的防御机制，展示了它们在面对日益流行的效率攻击时的局限性。

**结论:** 对于未来适应性的DDLSs来说，需要新的缓解策略来确保其安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficiency+Robustness+of+Dynamic+Deep+Learning+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10831，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10831&send_immediately=true&force_search=false)

**原文摘要:** Deep Learning Systems (DLSs) are increasingly deployed in real-time
applications, including those in resourceconstrained environments such as
mobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning
Systems (DDLSs) adapt inference computation based on input complexity, reducing
overhead. While this dynamic behavior improves efficiency, such behavior
introduces new attack surfaces. In particular, efficiency adversarial attacks
exploit these dynamic mechanisms to degrade system performance. This paper
systematically explores efficiency robustness of DDLSs, presenting the first
comprehensive taxonomy of efficiency attacks. We categorize these attacks based
on three dynamic behaviors: (i) attacks on dynamic computations per inference,
(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic
output production for downstream tasks. Through an in-depth evaluation, we
analyze adversarial strategies that target DDLSs efficiency and identify key
challenges in securing these systems. In addition, we investigate existing
defense mechanisms, demonstrating their limitations against increasingly
popular efficiency attacks and the necessity for novel mitigation strategies to
secure future adaptive DDLSs.

</details>


### [63] [Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics](https://arxiv.org/abs/2506.10577)
*Pascal Plettenberg, André Alcalde, Bernhard Sick, Josephine M. Thomas*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于图神经网络的节点对预测模型的方法，用于自动化添加新的组件到PCB原理图中，以优化设计。该方法在三个关键的PCB设计优化任务上进行了测试，并展示了GNN可以高精度地解决这些问题，为时间和成本效益的PCB设计自动化提供了潜力。


<details>
  <summary>更多</summary>
  
**动机:** 由于熟练工程师短缺和手动优化非常耗时，最佳实践经常被忽视，这导致了后期开发阶段更高的故障排除成本、产品生命周期缩短以及难以回收的电子废物增加。因此需要一种能够自动优化PCB原理图的方法来提高电路的鲁棒性和可靠性。

**方法:** 通过将PCB原理图表示为二分图，并利用基于图神经网络（GNNs）的节点对预测模型来实现自动化添加新组件。研究团队在真实世界数据集上应用了这种方法，并与人工专家标注的数据进行了比较。

**结果:** 实验结果表明，GNNs可以在高精度下解决PCB设计优化问题，并且所提出的方法有潜力以时间和成本效益的方式自动化PCB设计优化。

**结论:** 基于GNN的方法对于自动化PCB设计中的组件添加具有显著的优势，它不仅提高了设计的质量，还可能减少因设计不佳而导致的额外成本和环境影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Neural+Networks+for+Automatic+Addition+of+Optimizing+Components+in+Printed+Circuit+Board+Schematics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10577，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10577&send_immediately=true&force_search=false)

**原文摘要:** The design and optimization of Printed Circuit Board (PCB) schematics is
crucial for the development of high-quality electronic devices. Thereby, an
important task is to optimize drafts by adding components that improve the
robustness and reliability of the circuit, e.g., pull-up resistors or
decoupling capacitors. Since there is a shortage of skilled engineers and
manual optimizations are very time-consuming, these best practices are often
neglected. However, this typically leads to higher costs for troubleshooting in
later development stages as well as shortened product life cycles, resulting in
an increased amount of electronic waste that is difficult to recycle. Here, we
present an approach for automating the addition of new components into PCB
schematics by representing them as bipartite graphs and utilizing a node pair
prediction model based on Graph Neural Networks (GNNs). We apply our approach
to three highly relevant PCB design optimization tasks and compare the
performance of several popular GNN architectures on real-world datasets labeled
by human experts. We show that GNNs can solve these problems with high accuracy
and demonstrate that our approach offers the potential to automate PCB design
optimizations in a time- and cost-efficient manner.

</details>


### [64] [The Diffusion Duality](https://arxiv.org/abs/2506.10892)
*Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, Volodymyr Kuleshov*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为Duo的新方法，该方法通过将高斯扩散中的强大技术转移至均匀状态离散扩散模型来提高文本生成的速度和性能。


<details>
  <summary>更多</summary>
  
**动机:** 虽然均匀状态离散扩散模型具有自我纠正的能力，可以快速生成文本，但它们的表现通常不如自回归模型和遮罩扩散模型。研究者们希望通过新的方法来缩小这一表现差距。

**方法:** Duo方法利用了从底层高斯扩散自然产生的均匀状态扩散过程，它包括两部分：一是基于高斯过程引导的课程学习策略，这可以减少方差并加速训练；二是离散一致性蒸馏算法，它将连续环境下的连贯性蒸馏适应到离散环境中，从而大大加快采样速度。

**结果:** 使用课程学习策略训练的模型在7个基准测试中的3个超过了自回归模型的零样本困惑度。此外，离散一致性蒸馏算法使扩散语言模型能够进行几步生成，并且采样速度提高了两个数量级。

**结论:** Duo方法成功地将高斯扩散的技术应用于均匀状态离散扩散模型，从而提高了训练效率和采样速度，在多个基准上超越了自回归模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Diffusion+Duality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10892&send_immediately=true&force_search=false)

**原文摘要:** Uniform-state discrete diffusion models hold the promise of fast text
generation due to their inherent ability to self-correct. However, they are
typically outperformed by autoregressive models and masked diffusion models. In
this work, we narrow this performance gap by leveraging a key insight:
Uniform-state diffusion processes naturally emerge from an underlying Gaussian
diffusion. Our method, Duo, transfers powerful techniques from Gaussian
diffusion to improve both training and sampling. First, we introduce a
curriculum learning strategy guided by the Gaussian process, doubling training
speed by reducing variance. Models trained with curriculum learning surpass
autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we
present Discrete Consistency Distillation, which adapts consistency
distillation from the continuous to the discrete setting. This algorithm
unlocks few-step generation in diffusion language models by accelerating
sampling by two orders of magnitude. We provide the code and model checkpoints
on the project page: http://s-sahoo.github.io/duo

</details>


### [65] [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922)
*Adam Karvonen, Samuel Marks*

**主要类别:** cs.LG

**AI概要:** 研究发现，当在实际应用场景中引入真实上下文细节时，简单的反偏见提示无法消除大型语言模型（LLMs）中的人口统计学偏见。为了解决这个问题，研究人员提出了一种内部偏见缓解方法，通过识别并中和模型激活内的敏感属性方向来实现所有测试场景下的稳健偏见减少。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型越来越多地应用于高风险招聘场景，直接影响个人职业和生计的决策变得至关重要。然而，先前的研究表明，尽管简单的反偏见提示可以在控制评估中消除人口统计学偏见，但在加入现实世界的上下文细节后这些措施失效了。

**方法:** 本研究通过一种称为内部偏见缓解的方法来解决上述问题：即识别并中和模型激活过程中与敏感属性相关的方向，并在推理时应用仿射概念编辑技术。该方法即使使用来自简单合成数据集的方向信息也能够广泛适用，从而将偏见显著降低至非常低水平。

**结果:** 结果表明，在包括顶尖商业级及开源模型在内的多个领先模型上，增加诸如公司名称、公共职业页面的文化描述以及选择性招聘约束等现实情境会导致显著的种族和性别偏见（面试率差异可达12%）。采用内部偏见缓解策略后，能够将偏见稳定地降至极低水平（通常低于1%，始终不超过2.5%），同时保持模型性能基本不变。

**结论:** 结论是，部署LLM进行招聘工作的实践者应该采取更贴近实际情况的评估方法，并考虑采用内部缓解策略以确保公平的结果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robustly+Improving+LLM+Fairness+in+Realistic+Settings+via+Interpretability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10922，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10922&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly deployed in high-stakes hiring
applications, making decisions that directly impact people's careers and
livelihoods. While prior studies suggest simple anti-bias prompts can eliminate
demographic biases in controlled evaluations, we find these mitigations fail
when realistic contextual details are introduced. We address these failures
through internal bias mitigation: by identifying and neutralizing sensitive
attribute directions within model activations, we achieve robust bias reduction
across all tested scenarios. Across leading commercial (GPT-4o, Claude 4
Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,
Mistral-24B), we find that adding realistic context such as company names,
culture descriptions from public careers pages, and selective hiring
constraints (e.g.,``only accept candidates in the top 10\%") induces
significant racial and gender biases (up to 12\% differences in interview
rates). When these biases emerge, they consistently favor Black over White
candidates and female over male candidates across all tested models and
scenarios. Moreover, models can infer demographics and become biased from
subtle cues like college affiliations, with these biases remaining invisible
even when inspecting the model's chain-of-thought reasoning. To address these
limitations, our internal bias mitigation identifies race and gender-correlated
directions and applies affine concept editing at inference time. Despite using
directions from a simple synthetic dataset, the intervention generalizes
robustly, consistently reducing bias to very low levels (typically under 1\%,
always below 2.5\%) while largely maintaining model performance. Our findings
suggest that practitioners deploying LLMs for hiring should adopt more
realistic evaluation methodologies and consider internal mitigation strategies
for equitable outcomes.

</details>


### [66] [Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability](https://arxiv.org/abs/2506.10616)
*Yu-Jie Zhang, Peng Zhao, Masashi Sugiyama*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Non-stationary+Online+Learning+for+Curved+Losses%3A+Improved+Dynamic+Regret+via+Mixability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10616&send_immediately=true&force_search=false)

**原文摘要:** Non-stationary online learning has drawn much attention in recent years.
Despite considerable progress, dynamic regret minimization has primarily
focused on convex functions, leaving the functions with stronger curvature
(e.g., squared or logistic loss) underexplored. In this work, we address this
gap by showing that the regret can be substantially improved by leveraging the
concept of mixability, a property that generalizes exp-concavity to effectively
capture loss curvature. Let $d$ denote the dimensionality and $P_T$ the path
length of comparators that reflects the environmental non-stationarity. We
demonstrate that an exponential-weight method with fixed-share updates achieves
an $\mathcal{O}(d T^{1/3} P_T^{2/3} \log T)$ dynamic regret for mixable losses,
improving upon the best-known $\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \log T)$
result (Baby and Wang, 2021) in $d$. More importantly, this improvement arises
from a simple yet powerful analytical framework that exploits the mixability,
which avoids the Karush-Kuhn-Tucker-based analysis required by existing work.

</details>


### [67] [GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models](https://arxiv.org/abs/2506.10946)
*Evelyn Ma, Duo Zhou, Peizhi Niu, Huiting Zhou, Huan Zhang, Olgica Milenkovic, S. Rasoul Etesami*

**主要类别:** cs.LG

**AI概要:** 提出了GUARD框架，通过数据归因指导大型语言模型的去学习过程，以减少对高影响数据遗忘时的非预期损失，并在保持与先前方法相似的遗忘度量的同时，显著提高保留性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于法规遵从、版权保护和隐私问题，大语言模型（LLMs）中的去学习变得越来越重要。然而，在LLM去学习中一个关键挑战是意外遗忘，即删除特定数据会无意中损害模型的实用性及其对有价值信息的保留。尽管之前的工作主要集中在架构创新上，但数据层面因素对去学习表现的影响仍然未被充分探索。现有方法往往在忘记高影响力数据时保留性能下降。

**方法:** 提出了一种名为GUARD的新框架，该框架通过数据归因来指导去学习和保留。GUARD引入了一个轻量级的数据归因指标，专为LLM去学习定制，可以量化“忘记”集和“保留”集之间的“一致性”，同时保持计算效率。基于此，设计了一个新的去学习目标，它根据样本的代理归因分数反比地分配自适应、非均匀的去学习权重。

**结果:** 广泛的实验表明，GUARD在确保有效去学习的同时大幅提高了实用性的保留。特别是在忘记10%的训练数据时，GUARD将保留集上的实用性牺牲降低了高达194.92%（以真实率衡量）。

**结论:** GUARD框架通过优化去学习过程中的数据处理方式，能够在保证足够遗忘效果的同时，极大地改善了模型对于需要保留的信息的维持能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GUARD%3A+Guided+Unlearning+and+Retention+via+Data+Attribution+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10946&send_immediately=true&force_search=false)

**原文摘要:** Unlearning in large language models (LLMs) is becoming increasingly important
due to regulatory compliance, copyright protection, and privacy concerns.
However, a key challenge in LLM unlearning is unintended forgetting, where the
removal of specific data inadvertently impairs the utility of the model and its
retention of valuable, desired information. While prior work has primarily
focused on architectural innovations, the influence of data-level factors on
unlearning performance remains underexplored. As a result, existing methods
often suffer from degraded retention when forgetting high-impact data. To
address this, we propose GUARD-a novel framework for Guided Unlearning And
Retention via Data attribution. At its core, GUARD introduces a lightweight
proxy data attribution metric tailored for LLM unlearning, which quantifies the
"alignment" between the forget and retain sets while remaining computationally
efficient. Building on this, we design a novel unlearning objective that
assigns adaptive, nonuniform unlearning weights to samples, inversely
proportional to their proxy attribution scores. Through such a reallocation of
unlearning power, GUARD mitigates unintended losses in retention. We provide
rigorous theoretical guarantees that GUARD significantly enhances retention
while maintaining forgetting metrics comparable to prior methods. Extensive
experiments on the TOFU benchmark across multiple LLM architectures demonstrate
that GUARD substantially improves utility preservation while ensuring effective
unlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to
194.92% in terms of Truth Ratio when forgetting 10% of the training data.

</details>


### [68] [ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems](https://arxiv.org/abs/2506.10955)
*Aayush Karan, Kulin Shah, Sitan Chen*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ReGuidance%3A+A+Simple+Diffusion+Wrapper+for+Boosting+Sample+Quality+on+Hard+Inverse+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10955，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10955&send_immediately=true&force_search=false)

**原文摘要:** There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.

</details>


### [69] [Leveraging Low-rank Factorizations of Conditional Correlation Matrices in Graph Learning](https://arxiv.org/abs/2506.10628)
*Thu Ha Phi, Alexandre Hippert-Ferrer, Florent Bouchard, Arnaud Breloy*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于条件相关矩阵低秩分解的图学习框架，利用黎曼优化技术解决优化问题，并将其应用于GLasso算法的一个低秩约束版本，从而在高维情况下实现高效的维度-性能权衡。


<details>
  <summary>更多</summary>
  
**动机:** 针对从节点收集的数据中学习无向图的问题，在图形信号处理框架下，图的拓扑结构与数据的条件相关矩阵的支持有关。然而，当变量数量很大时，相应的图学习问题规模会变得非常大，成为一个难题。

**方法:** 提出一种新的图学习框架，该框架利用条件相关矩阵的低秩分解来减少问题规模。为了解决由此产生的优化问题，作者开发了适用于这种特定结构的黎曼优化技术工具。此外，还特别化了GLasso算法（即高斯图模型的惩罚最大似然估计）的低秩约束版本。

**结果:** 通过合成和真实数据实验表明，所提方法能够在保持良好性能的同时显著降低计算复杂度，实现了高效的维度-性能权衡。

**结论:** 所提出的基于低秩分解的图学习方法能够有效地解决大规模图学习中的计算复杂性问题，同时保持良好的学习性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Low-rank+Factorizations+of+Conditional+Correlation+Matrices+in+Graph+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10628，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10628&send_immediately=true&force_search=false)

**原文摘要:** This paper addresses the problem of learning an undirected graph from data
gathered at each nodes. Within the graph signal processing framework, the
topology of such graph can be linked to the support of the conditional
correlation matrix of the data. The corresponding graph learning problem then
scales to the squares of the number of variables (nodes), which is usually
problematic at large dimension. To tackle this issue, we propose a graph
learning framework that leverages a low-rank factorization of the conditional
correlation matrix. In order to solve for the resulting optimization problems,
we derive tools required to apply Riemannian optimization techniques for this
particular structure. The proposal is then particularized to a low-rank
constrained counterpart of the GLasso algorithm, i.e., the penalized maximum
likelihood estimation of a Gaussian graphical model. Experiments on synthetic
and real data evidence that a very efficient dimension-versus-performance
trade-off can be achieved with this approach.

</details>


### [70] [Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods](https://arxiv.org/abs/2506.10959)
*Zhaiming Shen, Alexander Hsu, Rongjie Lai, Wenjing Liao*

**主要类别:** cs.LG

**AI概要:** 本文从理论上研究了在流形上的Hölder函数回归中情境学习（ICL）的表现，通过将注意力机制与经典核方法联系起来，得出了泛化误差界限，并揭示了几何结构在ICL中的作用。


<details>
  <summary>更多</summary>
  
**动机:** 尽管情境学习（ICL）在自然语言和视觉领域取得了显著的成功，但在结构化几何数据背景下的理论理解仍待探索。

**方法:** 建立注意力机制与经典核方法之间的新联系，推导出基于提示长度和训练任务数量的泛化误差界。

**结果:** 当观察到足够多的训练任务时，转换器能够达到流形上Hölder函数的最小最大回归率，该回归率随流形内在维度而非环境空间维度呈指数增长。

**结论:** 研究结果为理解几何在ICL中的角色提供了基础性的见解，并且为研究非线性模型的情境学习提供了新的工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+In-Context+Learning+on+Structured+Manifolds%3A+Bridging+Attention+to+Kernel+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10959，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10959&send_immediately=true&force_search=false)

**原文摘要:** While in-context learning (ICL) has achieved remarkable success in natural
language and vision domains, its theoretical understanding--particularly in the
context of structured geometric data--remains unexplored. In this work, we
initiate a theoretical study of ICL for regression of H\"older functions on
manifolds. By establishing a novel connection between the attention mechanism
and classical kernel methods, we derive generalization error bounds in terms of
the prompt length and the number of training tasks. When a sufficient number of
training tasks are observed, transformers give rise to the minimax regression
rate of H\"older functions on manifolds, which scales exponentially with the
intrinsic dimension of the manifold, rather than the ambient space dimension.
Our result also characterizes how the generalization error scales with the
number of training tasks, shedding light on the complexity of transformers as
in-context algorithm learners. Our findings provide foundational insights into
the role of geometry in ICL and novels tools to study ICL of nonlinear models.

</details>


### [71] [Farseer: A Refined Scaling Law in Large Language Models](https://arxiv.org/abs/2506.10972)
*Houyi Li, Wenzhen Zheng, Qiufeng Wang, Zhenyu Ding, Haoying Wang, Zili Wang, Shijie Xuyang, Ning Ding, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的扩展法则Farseer，用于提高大型语言模型训练中的预测准确性，并通过大约1000个不同规模和配置的LLM实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 当前大型语言模型（LLMs）的训练成本非常高昂，导致从小规模实验中获得的洞见往往无法直接应用于资源密集型生产系统，阻碍了高效创新。

**方法:** 研究者们构建了一个名为Farseer的新扩展法则，它通过系统地建立模型损失面$L(N,D)$来提供更好的经验数据拟合度。

**结果:** Farseer相比之前的法则（如Chinchilla定律）提高了433%的外推误差减少率，允许在所有$(N,D)$设置下可靠地评估竞争性的训练策略，并为最优计算分配提供了新的见解。

**结论:** Farseer方法能够支持小规模消融研究结果到大规模性能的可信外推，并且对于现代LLM训练的复杂需求有了更好的反映。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Farseer%3A+A+Refined+Scaling+Law+in+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10972&send_immediately=true&force_search=false)

**原文摘要:** Training Large Language Models (LLMs) is prohibitively expensive, creating a
critical scaling gap where insights from small-scale experiments often fail to
transfer to resource-intensive production systems, thereby hindering efficient
innovation. To bridge this, we introduce Farseer, a novel and refined scaling
law offering enhanced predictive accuracy across scales. By systematically
constructing a model loss surface $L(N,D)$, Farseer achieves a significantly
better fit to empirical data than prior laws (e.g., Chinchilla's law). Our
methodology yields accurate, robust, and highly generalizable predictions,
demonstrating excellent extrapolation capabilities, improving upon Chinchilla's
law by reducing extrapolation error by 433\%. This allows for the reliable
evaluation of competing training strategies across all $(N,D)$ settings,
enabling conclusions from small-scale ablation studies to be confidently
extrapolated to predict large-scale performance. Furthermore, Farseer provides
new insights into optimal compute allocation, better reflecting the nuanced
demands of modern LLM training. To validate our approach, we trained an
extensive suite of approximately 1,000 LLMs across diverse scales and
configurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are
comprehensively open-sourcing all models, data, results, and logs at
https://github.com/Farseer-Scaling-Law/Farseer to foster further research.

</details>


### [72] [Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning](https://arxiv.org/abs/2506.10973)
*Julius Berner, Miguel Liu-Schiaffini, Jean Kossaifi, Valentin Duruisseaux, Boris Bonev, Kamyar Azizzadenesheli, Anima Anandkumar*

**主要类别:** cs.LG

**AI概要:** 本文探讨了如何将现有的神经网络架构转化为神经算子，以处理无限维函数空间之间的映射问题，并为实践者提供了一种实现方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统的深度学习在计算机视觉和自然语言处理领域取得了成功，但这些应用主要集中在有限维空间的映射上。然而科学问题往往涉及无限维函数空间，如连续时间动力系统和偏微分方程。因此需要一种新的方法来推广神经网络，使其能够处理函数空间间的映射。

**方法:** 作者提出了一个将流行的神经网络架构转换成神经算子的方案，该方案基于对构建无限维函数空间之间实际映射的关键原则的理解。

**结果:** 通过遵循所提出的原则，可以将几种流行的神经网络架构转变为神经算子，只需进行最小的修改。

**结论:** 论文旨在为从业人员提供指导，帮助他们将神经网络架构转变为神经算子，以便解决科学问题中的无限维函数空间映射问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Principled+Approaches+for+Extending+Neural+Architectures+to+Function+Spaces+for+Operator+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10973，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10973&send_immediately=true&force_search=false)

**原文摘要:** A wide range of scientific problems, such as those described by
continuous-time dynamical systems and partial differential equations (PDEs),
are naturally formulated on function spaces. While function spaces are
typically infinite-dimensional, deep learning has predominantly advanced
through applications in computer vision and natural language processing that
focus on mappings between finite-dimensional spaces. Such fundamental
disparities in the nature of the data have limited neural networks from
achieving a comparable level of success in scientific applications as seen in
other fields. Neural operators are a principled way to generalize neural
networks to mappings between function spaces, offering a pathway to replicate
deep learning's transformative impact on scientific problems. For instance,
neural operators can learn solution operators for entire classes of PDEs, e.g.,
physical systems with different boundary conditions, coefficient functions, and
geometries. A key factor in deep learning's success has been the careful
engineering of neural architectures through extensive empirical testing.
Translating these neural architectures into neural operators allows operator
learning to enjoy these same empirical optimizations. However, prior neural
operator architectures have often been introduced as standalone models, not
directly derived as extensions of existing neural network architectures. In
this paper, we identify and distill the key principles for constructing
practical implementations of mappings between infinite-dimensional function
spaces. Using these principles, we propose a recipe for converting several
popular neural architectures into neural operators with minimal modifications.
This paper aims to guide practitioners through this process and details the
steps to make neural operators work in practice. Our code can be found at
https://github.com/neuraloperator/NNs-to-NOs

</details>


### [73] [Hessian Geometry of Latent Space in Generative Models](https://arxiv.org/abs/2506.10632)
*Alexander Lobashev, Dmitry Guskov, Maria Larchenko, Mikhail Tamm*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种通过重建Fisher信息度量来分析生成模型潜在空间几何形状的新方法，该方法在统计物理模型和扩散模型上得到验证，并揭示了扩散模型潜在空间中的相变分形结构。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地理解生成模型的潜在空间几何结构，特别是与统计物理模型和扩散模型相关的结构，以及它们如何影响热力学量的重构。

**方法:** 本研究的方法是基于重建Fisher信息度量，通过对给定生成样本下的潜在变量后验分布进行近似，并据此学习对数配分函数，后者定义了指数族的Fisher度量。

**结果:** 该方法在Ising模型和TASEP模型上得到了验证，并且在重构热力学量方面优于现有的基线。对于扩散模型，该方法揭示了潜在空间中相变的分形结构。

**结论:** 研究结果为理解扩散模型潜在空间的复杂结构及其与相变等现象之间的联系提供了新的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hessian+Geometry+of+Latent+Space+in+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10632，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10632&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a novel method for analyzing the latent space geometry of
generative models, including statistical physics models and diffusion models,
by reconstructing the Fisher information metric. The method approximates the
posterior distribution of latent variables given generated samples and uses
this to learn the log-partition function, which defines the Fisher metric for
exponential families. Theoretical convergence guarantees are provided, and the
method is validated on the Ising and TASEP models, outperforming existing
baselines in reconstructing thermodynamic quantities. Applied to diffusion
models, the method reveals a fractal structure of phase transitions in the
latent space, characterized by abrupt changes in the Fisher metric. We
demonstrate that while geodesic interpolations are approximately linear within
individual phases, this linearity breaks down at phase boundaries, where the
diffusion model exhibits a divergent Lipschitz constant with respect to the
latent space. These findings provide new insights into the complex structure of
diffusion model latent spaces and their connection to phenomena like phase
transitions. Our source code is available at
https://github.com/alobashev/hessian-geometry-of-diffusion-models.

</details>


### [74] [Preserving Task-Relevant Information Under Linear Concept Removal](https://arxiv.org/abs/2506.10703)
*Floris Holstege, Shauli Ravfogel, Bram Wouters*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为SPLICE的方法，它能够在移除表示中的敏感概念的同时保持与目标标签的协方差。理论和实证结果都表明SPLICE在去除不需要的概念时能够最小化对主要任务信息的影响。


<details>
  <summary>更多</summary>
  
**动机:** 现代神经网络经常将不希望出现的概念与任务相关信息一起编码，导致公平性和可解释性问题。现有事后方法虽然可以移除不需要的概念，但往往也会损害有用信号。

**方法:** 提出了SPLICE（同时投影以实现线性概念移除和协方差保留），通过斜投影的方式“剪除”不需要的方向，同时保护重要的标签相关性。

**结果:** SPLICE在理论上是唯一一个可以在移除线性概念预测能力的同时保持目标协方差并最小化嵌入失真的解决方案。实验上，在Bias in Bios和Winobias等基准测试中，SPLICE的表现优于基线方法，在移除受保护属性的同时对主任务信息的影响最小。

**结论:** SPLICE为解决神经网络中公平性和可解释性问题提供了一个有效的方法，能在移除敏感信息的同时保持模型性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Preserving+Task-Relevant+Information+Under+Linear+Concept+Removal，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10703&send_immediately=true&force_search=false)

**原文摘要:** Modern neural networks often encode unwanted concepts alongside task-relevant
information, leading to fairness and interpretability concerns. Existing
post-hoc approaches can remove undesired concepts but often degrade useful
signals. We introduce SPLICE-Simultaneous Projection for LInear concept removal
and Covariance prEservation-which eliminates sensitive concepts from
representations while exactly preserving their covariance with a target label.
SPLICE achieves this via an oblique projection that "splices out" the unwanted
direction yet protects important label correlations. Theoretically, it is the
unique solution that removes linear concept predictability and maintains target
covariance with minimal embedding distortion. Empirically, SPLICE outperforms
baselines on benchmarks such as Bias in Bios and Winobias, removing protected
attributes while minimally damaging main-task information.

</details>


### [75] [Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering](https://arxiv.org/abs/2506.10751)
*Sai Prasanna Teja Reddy Bogireddy, Abrar Majeedi, Viswanatha Reddy Gajjala, Zhuoyan Xu, Siddhant Rai, Vaishnav Potlapalli*

**主要类别:** cs.LG

**AI概要:** 本文介绍了在BioNLP 2025 ArchEHR-QA共享任务中获得亚军的Neural系统，该系统通过数据驱动的提示优化来提高临床问答的准确性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决临床环境下的自动问答问题，需要精准的证据检索和忠实的答案生成，并且在有限的监督下进行。

**方法:** 研究者提出的方法将任务分解为句子级证据识别和带有明确引用的答案合成两个阶段，并使用DSPy的MIPROv2优化器自动探索提示空间，在开发集上联合调整指令和少量样本示例。此外，采用自一致性投票方案进一步提高了证据召回率而不牺牲精确度。

**结果:** 在隐藏测试集上，该方法获得了51.5的总分，位居第二，相比标准零样本和少样本提示分别高出超过20和10个点。

**结论:** 结果表明，对于高风险的临床问答来说，数据驱动的提示优化是一种成本效益高的替代模型微调的方式，它能够提升医疗领域AI助手的可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+at+ArchEHR-QA+2025%3A+Agentic+Prompt+Optimization+for+Evidence-Grounded+Clinical+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10751，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10751&send_immediately=true&force_search=false)

**原文摘要:** Automated question answering (QA) over electronic health records (EHRs) can
bridge critical information gaps for clinicians and patients, yet it demands
both precise evidence retrieval and faithful answer generation under limited
supervision. In this work, we present Neural, the runner-up in the BioNLP 2025
ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method
decouples the task into (1) sentence-level evidence identification and (2)
answer synthesis with explicit citations. For each stage, we automatically
explore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning
instructions and few-shot demonstrations on the development set. A
self-consistency voting scheme further improves evidence recall without
sacrificing precision. On the hidden test set, our method attains an overall
score of 51.5, placing second stage while outperforming standard zero-shot and
few-shot prompting by over 20 and 10 points, respectively. These results
indicate that data-driven prompt optimization is a cost-effective alternative
to model fine-tuning for high-stakes clinical QA, advancing the reliability of
AI assistants in healthcare.

</details>


### [76] [Skillful joint probabilistic weather forecasting from marginals](https://arxiv.org/abs/2506.10772)
*Ferran Alet, Ilan Price, Andrew El-Kadi, Dominic Masters, Stratis Markou, Tom R. Andersson, Jacklynn Stott, Remi Lam, Matthew Willson, Alvaro Sanchez-Gonzalez, Peter Battaglia*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为FGN的新方法，该方法通过学习模型扰动生成集成预测，并直接训练以最小化连续排名概率得分(CRPS)，从而在各种确定性和概率性指标上优于当前最先进的天气预报模型。


<details>
  <summary>更多</summary>
  
**动机:** 由于基于机器学习的天气模型比传统的数值天气预报(NWP)具有更高的准确性和速度，最近甚至在全球概率天气预报中超越了传统集合预报的表现，因此提出了新的FGN方法来进一步提高天气预报的质量。

**方法:** FGN方法通过学习得到的模型扰动生成集成预测，并且使用一组适当约束的模型。它直接针对每个位置的预报来最小化连续排名概率得分(CRPS)进行训练。

**结果:** FGN在一系列确定性和概率性度量标准上产生了最先进的集成预报结果，能够做出有技巧的热带气旋路径预测，并捕捉到联合空间结构，尽管它仅在边缘数据上进行了训练。

**结论:** FGN是一种简单、可扩展且灵活的建模方法，显著优于当前最先进模型，在天气预报领域展现了其优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Skillful+joint+probabilistic+weather+forecasting+from+marginals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10772，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10772&send_immediately=true&force_search=false)

**原文摘要:** Machine learning (ML)-based weather models have rapidly risen to prominence
due to their greater accuracy and speed than traditional forecasts based on
numerical weather prediction (NWP), recently outperforming traditional
ensembles in global probabilistic weather forecasting. This paper presents FGN,
a simple, scalable and flexible modeling approach which significantly
outperforms the current state-of-the-art models. FGN generates ensembles via
learned model-perturbations with an ensemble of appropriately constrained
models. It is trained directly to minimize the continuous rank probability
score (CRPS) of per-location forecasts. It produces state-of-the-art ensemble
forecasts as measured by a range of deterministic and probabilistic metrics,
makes skillful ensemble tropical cyclone track predictions, and captures joint
spatial structure despite being trained only on marginals.

</details>


### [77] [Monotone Classification with Relative Approximations](https://arxiv.org/abs/2506.10775)
*Yufei Tao*

**主要类别:** cs.LG

**AI概要:** 本文首次研究了在单调分类中以最低成本找到一个误差最多为最优单调分类器误差的$(1 + \epsilon)$倍的单调分类器的问题，并提供了几乎匹配的上下界。


<details>
  <summary>更多</summary>
  
**动机:** 研究目的是探索在给定相对误差容忍度的情况下，找到一个接近最优解的单调分类器所需的最小标签查询成本。

**方法:** 文章提出了针对不同$\epsilon$值范围的新的算法和理论分析来估计所需成本的上下界。

**结果:** 对于$\epsilon$的全范围给出了几乎匹配的成本上下界。

**结论:** 该研究解决了允许误差超过最优解的相对因子时，确定单调分类器所需最低成本的问题，并且相比之前的工作只能实现绝对因子的误差改进而言，这是一个重要的进展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Monotone+Classification+with+Relative+Approximations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10775，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10775&send_immediately=true&force_search=false)

**原文摘要:** In monotone classification, the input is a multi-set $P$ of points in
$\mathbb{R}^d$, each associated with a hidden label from $\{-1, 1\}$. The goal
is to identify a monotone function $h$, which acts as a classifier, mapping
from $\mathbb{R}^d$ to $\{-1, 1\}$ with a small {\em error}, measured as the
number of points $p \in P$ whose labels differ from the function values $h(p)$.
The cost of an algorithm is defined as the number of points having their labels
revealed. This article presents the first study on the lowest cost required to
find a monotone classifier whose error is at most $(1 + \epsilon) \cdot k^*$
where $\epsilon \ge 0$ and $k^*$ is the minimum error achieved by an optimal
monotone classifier -- in other words, the error is allowed to exceed the
optimal by at most a relative factor. Nearly matching upper and lower bounds
are presented for the full range of $\epsilon$. All previous work on the
problem can only achieve an error higher than the optimal by an absolute
factor.

</details>


### [78] [Dense Associative Memory with Epanechnikov Energy](https://arxiv.org/abs/2506.10801)
*Benjamin Hoover, Zhaoyang Shi, Krishnakumar Balasubramanian, Dmitry Krotov, Parikshit Ram*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的密集关联记忆网络的能量函数——log-sum-ReLU（LSR），它基于Epanechnikov核，能够在不需要指数分离函数的情况下实现精确的记忆检索，并且具有指数容量。此外，该方法还引入了大量额外的局部最小值，同时保持了完美的模式恢复。实验结果表明，与基于LSE的模型相比，LSR能量函数具有更多的局部极小值（记忆），并且在图像数据集上的分析显示了这种记忆的创造性和新颖性。


<details>
  <summary>更多</summary>
  
**动机:** 作者受到最优核密度估计的启发，旨在为DenseAM网络设计一种新型的能量函数，以克服现有LSE函数需要指数分离才能达到精确记忆检索的问题，并希望提升记忆容量的同时保持准确的模式恢复。

**方法:** 论文中介绍了一种新的能量函数——log-sum-ReLU (LSR)，它是基于Epanechnikov核的，并且能够不依赖于指数分离函数就实现精确的记忆检索。

**结果:** 实证研究表明，LSR能量函数比基于LSE的模型有显著更多的局部极小值（记忆），这些记忆的对数似然度相当。在图像数据集上分析LSR产生的新兴记忆时，发现它们具有一定程度的创新和新颖性。

**结论:** LSR能量函数不仅提供了指数级的记忆容量，而且还在保持完美模式恢复的同时增加了大量的局部最小值。这表明LSR在大规模记忆存储和生成任务方面具有潜在的应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dense+Associative+Memory+with+Epanechnikov+Energy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10801，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10801&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel energy function for Dense Associative Memory (DenseAM)
networks, the log-sum-ReLU (LSR), inspired by optimal kernel density
estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based
on the Epanechnikov kernel and enables exact memory retrieval with exponential
capacity without requiring exponential separation functions. Moreover, it
introduces abundant additional \emph{emergent} local minima while preserving
perfect pattern recovery -- a characteristic previously unseen in DenseAM
literature. Empirical results show that LSR energy has significantly more local
minima (memories) that have comparable log-likelihood to LSE-based models.
Analysis of LSR's emergent memories on image datasets reveals a degree of
creativity and novelty, hinting at this method's potential for both large-scale
memory storage and generative tasks.

</details>


### [79] [Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/abs/2506.10805)
*Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, Dmitrii Krasheninnikov*

**主要类别:** cs.LG

**AI概要:** 本文研究了用于检测可能造成重大伤害的高风险交互的激活探针，并发现它们在多样化的、分布外的真实世界数据上表现出稳健的泛化能力。这些探针的性能与提示或微调后的中型语言模型监控器相当，但计算成本节省了六个数量级。


<details>
  <summary>更多</summary>
  
**动机:** 为了安全地部署大型语言模型（LLMs），监测是至关重要的方面。特别是对于那些可能会导致严重后果的高风险交互，需要一种有效的监测方法。然而，针对这种监测目标的研究还比较少。

**方法:** 研究者们评估了几种基于合成数据训练的探针架构，并测试了它们在多样化、分布外的真实世界数据上的表现。

**结果:** 实验结果表明，这些探针不仅在未见过的数据上展现出强大的泛化能力，而且其性能可以与经过提示或微调的中等规模的语言模型监控器相媲美，同时提供了六倍数量级的计算资源节省。

**结论:** 研究表明，激活探针作为高效初始过滤器，在构建资源意识的层次化监控系统中具有潜力，可以为更昂贵的下游分析标记出需要关注的情况。为此，研究团队发布了他们的新颖合成数据集和代码库以促进进一步的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Detecting+High-Stakes+Interactions+with+Activation+Probes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10805，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10805&send_immediately=true&force_search=false)

**原文摘要:** Monitoring is an important aspect of safely deploying Large Language Models
(LLMs). This paper examines activation probes for detecting "high-stakes"
interactions -- where the text indicates that the interaction might lead to
significant harm -- as a critical, yet underexplored, target for such
monitoring. We evaluate several probe architectures trained on synthetic data,
and find them to exhibit robust generalization to diverse, out-of-distribution,
real-world data. Probes' performance is comparable to that of prompted or
finetuned medium-sized LLM monitors, while offering computational savings of
six orders-of-magnitude. Our experiments also highlight the potential of
building resource-aware hierarchical monitoring systems, where probes serve as
an efficient initial filter and flag cases for more expensive downstream
analysis. We release our novel synthetic dataset and codebase to encourage
further study.

</details>


### [80] [Advanced fraud detection using machine learning models: enhancing financial transaction security](https://arxiv.org/abs/2506.10842)
*Nudrat Fariha, Md Nazmuddin Moin Khan, Md Iqbal Hossain, Syed Ali Reza, Joy Chakra Bortty, Kazi Sharmin Sultana, Md Shadidur Islam Jawad, Saniah Safat, Md Abdul Ahad, Maksuda Begum*

**主要类别:** cs.LG

**AI概要:** 该研究开发了一个端到端的、特征丰富的机器学习框架，用于检测信用卡交易异常和欺诈行为。通过对真实数据集进行特征工程处理，并使用多种无监督模型（如孤立森林、单类SVM和深度自编码器）来识别潜在的欺诈模式。PCA可视化有助于区分正常与异常交易，并通过K-Means聚类和DBSCAN进一步划分交易活动区域。


<details>
  <summary>更多</summary>
  
**动机:** 随着数字支付的兴起，对于能够智能且可扩展的系统的需求日益增加，以应对欺诈检测的挑战。

**方法:** 研究首先将来自关系数据库的交易、持卡人、商家以及商家类别数据集合并成一个统一的分析视图。接着，通过特征工程提取了诸如平均消费、偏离历史模式的程度、交易时间不规则性以及类别频率指标等行为信号，并结合了小时、星期几及周末指示符等时间标记来揭示所有可能指示欺诈行为的潜在模式。利用这些数据训练并评估了一系列无监督模型，包括孤立森林、单类SVM以及经过训练用以重构正常行为的深度自编码器。此外，还采用了PCA可视化技术展示了每个模型在二维潜空间中分离异常值的能力，并运用K-Means聚类和DBSCAN算法对交易场景进行了进一步细分，以便识别出密集的正常活动群组和稀疏可疑区域。

**结果:** 探索性数据分析揭示了整个数据集特征中的上下文交易趋势。各无监督模型被用来标记重建误差排名前1%的数据点作为离群值。PCA可视化表明了各模型区分异常进入二维潜空间的能力。K-Means聚类和DBSCAN帮助确定了正常的密集交易活动簇以及稀疏可疑区域。

**结论:** 本研究提出了一种先进的机器学习框架，能够有效地从现实世界的数据集中检测信用卡交易中的异常和潜在欺诈行为。该框架不仅提升了欺诈检测系统的智能化水平，同时也证明了通过综合多维度特征工程与无监督学习方法可以显著提高欺诈识别精度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advanced+fraud+detection+using+machine+learning+models%3A+enhancing+financial+transaction+security，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10842，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10842&send_immediately=true&force_search=false)

**原文摘要:** The rise of digital payments has accelerated the need for intelligent and
scalable systems to detect fraud. This research presents an end-to-end,
feature-rich machine learning framework for detecting credit card transaction
anomalies and fraud using real-world data. The study begins by merging
transactional, cardholder, merchant, and merchant category datasets from a
relational database to create a unified analytical view. Through the feature
engineering process, we extract behavioural signals such as average spending,
deviation from historical patterns, transaction timing irregularities, and
category frequency metrics. These features are enriched with temporal markers
such as hour, day of week, and weekend indicators to expose all latent patterns
that indicate fraudulent behaviours. Exploratory data analysis reveals
contextual transaction trends across all the dataset features. Using the
transactional data, we train and evaluate a range of unsupervised models:
Isolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct
normal behavior. These models flag the top 1% of reconstruction errors as
outliers. PCA visualizations illustrate each models ability to separate
anomalies into a two-dimensional latent space. We further segment the
transaction landscape using K-Means clustering and DBSCAN to identify dense
clusters of normal activity and isolate sparse, suspicious regions.

</details>


### [81] [Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization](https://arxiv.org/abs/2506.10871)
*Pierre-François Massiani, Alexander von Rohr, Lukas Haverbeck, Sebastian Trimpe*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在未知干扰下，通过熵正则化和约束惩罚两种技术的相互作用来学习能够稳健满足状态约束的策略，从而提高强化学习的安全性和鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管强化学习领域取得了许多最新进展，但在未知干扰的情况下，学习能够稳健地满足状态约束的策略仍然是一个未解决的问题。

**方法:** 分析了无模型强化学习中两种成熟的技术——熵正则化和约束惩罚之间的相互作用，并通过实证揭示了熵正则化倾向于最大化未来可行动作的数量，促进对动作噪声的约束满足；同时表明通过惩罚放松严格的安全约束，可以将受约束的RL问题近似为一个不受约束的问题，从而使用标准的无模型RL方法解决。

**结果:** 研究结果表明，熵正则化与鲁棒性之间的联系是进一步实证和理论研究的一个有前景的方向，因为它可以通过简单的奖励塑形实现强化学习中的鲁棒安全性。

**结论:** 通过结合熵正则化和约束惩罚的方法，可以在保持安全性和最优性的同时提高对干扰的韧性，这为实现强化学习中的鲁棒安全提供了新的视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Viability+of+Future+Actions%3A+Robust+Safety+in+Reinforcement+Learning+via+Entropy+Regularization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10871&send_immediately=true&force_search=false)

**原文摘要:** Despite the many recent advances in reinforcement learning (RL), the question
of learning policies that robustly satisfy state constraints under unknown
disturbances remains open. In this paper, we offer a new perspective on
achieving robust safety by analyzing the interplay between two well-established
techniques in model-free RL: entropy regularization, and constraints
penalization. We reveal empirically that entropy regularization in constrained
RL inherently biases learning toward maximizing the number of future viable
actions, thereby promoting constraints satisfaction robust to action noise.
Furthermore, we show that by relaxing strict safety constraints through
penalties, the constrained RL problem can be approximated arbitrarily closely
by an unconstrained one and thus solved using standard model-free RL. This
reformulation preserves both safety and optimality while empirically improving
resilience to disturbances. Our results indicate that the connection between
entropy regularization and robustness is a promising avenue for further
empirical and theoretical investigation, as it enables robust safety in RL
through simple reward shaping.

</details>


### [82] [Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers](https://arxiv.org/abs/2506.10888)
*Lucas Gnecco-Heredia, Benjamin Negrevergne, Yann Chevaleyre*

**主要类别:** cs.LG

**AI概要:** 本文讨论了攻击分类器混合体的问题，并引入了两种基于几何分析的攻击属性（有效性和最大性）。现有攻击未能满足这两种属性。为此，作者提出了一种新的攻击方法——格子攀爬攻击，并在二元线性设置下提供了理论保证，通过合成数据集和真实数据集上的实验展示了其性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的针对分类器混合体的攻击手段并不适用于此类分类器，因此需要一种更有效的攻击方式。

**方法:** 提出了两种理想的攻击特性：有效性与最大性，并且发现现有攻击无法同时满足这两个特性。随后，开发了一种称为“格子攀爬攻击”的新攻击方法，在二进制线性环境下有理论保障。

**结果:** 格子攀爬攻击在合成数据集和实际数据集中表现出色。

**结论:** 该研究提供了一种新的攻击方法来对抗有限混合分类器，这种方法比现有攻击更加有效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lattice+Climber+Attack%3A+Adversarial+attacks+for+randomized+mixtures+of+classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10888，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10888&send_immediately=true&force_search=false)

**原文摘要:** Finite mixtures of classifiers (a.k.a. randomized ensembles) have been
proposed as a way to improve robustness against adversarial attacks. However,
existing attacks have been shown to not suit this kind of classifier. In this
paper, we discuss the problem of attacking a mixture in a principled way and
introduce two desirable properties of attacks based on a geometrical analysis
of the problem (effectiveness and maximality). We then show that existing
attacks do not meet both of these properties. Finally, we introduce a new
attack called {\em lattice climber attack} with theoretical guarantees in the
binary linear setting, and demonstrate its performance by conducting
experiments on synthetic and real datasets.

</details>


### [83] [NoLoCo: No-all-reduce Low Communication Training Method for Large Models](https://arxiv.org/abs/2506.10911)
*Jari Kolehmainen, Nikolay Blagoev, John Donaghy, Oğuzhan Ersoy, Christopher Nies*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为NoLoCo的新型优化方法，该方法在训练过程中不显式地同步所有模型参数，因此不需要任何集体通信。通过与随机选择的其他模型部分平均权重，NoLoCo隐含地同步了模型权重。实验结果表明，此方法相较于完全分片数据并行训练和低通信训练方法DiLoCo，在减少通信开销、加速同步步骤以及提高收敛速度方面具有显著优势。


<details>
  <summary>更多</summary>
  
**动机:** 当前大规模语言模型的训练通常需要在包含数万个加速器的集群上进行，并依赖于高带宽互连进行通信。然而，扩大这种集群的成本很高且可能变得不切实际，限制了可以训练的模型大小。虽然已有研究提出了几种降低通信需求的训练方法，但这些方法仍然会执行一个针对所有模型副本的同步步骤，这在低带宽网络下可能会变得非常昂贵。

**方法:** 提出了NoLoCo这一新的优化方法，它避免了训练期间显式的模型参数同步过程，从而不需要任何形式的集体通信。NoLoCo利用一种新颖的Nesterov动量优化器变体，通过与另一个随机选取的模型部分平均权重来实现模型权重的隐式同步。

**结果:** NoLoCo在不同数量的加速器和模型规模（从1.25亿到68亿参数）上进行了基准测试。结果显示，相比全分片数据并行训练或广泛使用的低通信训练方法DiLoCo，NoLoCo所需的通信开销明显更低。对于数百个加速器通过互联网进行训练的情况，其同步步骤比DiLoCo中使用的all-reduce快一个数量级。此外，由于没有全局阻塞通信，减少了加速器的空闲时间；与DiLoCo相比，在广泛的模型规模和加速器数量范围内观察到了高达4%的更快收敛率。

**结论:** 本研究介绍了一个创新性的优化方法NoLoCo，它可以大幅减少大规模语言模型训练时的通信成本，同时保持良好的收敛性能。通过理论分析及实验证明了该方法的有效性，为未来更大规模的语言模型训练提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NoLoCo%3A+No-all-reduce+Low+Communication+Training+Method+for+Large+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10911，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10911&send_immediately=true&force_search=false)

**原文摘要:** Training large language models is generally done via optimization methods on
clusters containing tens of thousands of accelerators, communicating over a
high-bandwidth interconnect. Scaling up these clusters is expensive and can
become impractical, imposing limits on the size of models that can be trained.
Several recent studies have proposed training methods that are less
communication intensive, avoiding the need for a highly connected compute
cluster. These state-of-the-art low communication training methods still employ
a synchronization step for model parameters, which, when performed over all
model replicas, can become costly on a low-bandwidth network.
  In this work, we propose a novel optimization method, NoLoCo, that does not
explicitly synchronize all model parameters during training and, as a result,
does not require any collective communication. NoLoCo implicitly synchronizes
model weights via a novel variant of the Nesterov momentum optimizer by
partially averaging model weights with a randomly selected other one. We
provide both a theoretical convergence analysis for our proposed optimizer as
well as empirical results from language model training.
  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,
between 125M to 6.8B parameters. Our method requires significantly less
communication overhead than fully sharded data parallel training or even widely
used low communication training method, DiLoCo. The synchronization step itself
is estimated to be one magnitude faster than the all-reduce used in DiLoCo for
few hundred accelerators training over the internet. We also do not have any
global blocking communication that reduces accelerator idling time. Compared to
DiLoCo, we also observe up to $4\%$ faster convergence rate with wide range of
model sizes and accelerator counts.

</details>


### [84] [Foundation Models for Causal Inference via Prior-Data Fitted Networks](https://arxiv.org/abs/2506.10914)
*Yuchen Ma, Dennis Frauen, Emil Javurek, Stefan Feuerriegel*

**主要类别:** cs.LG

**AI概要:** 提出了CausalFM，这是一个基于PFNs的框架，用于在多种因果推理设置中训练基础模型。通过形式化构建贝叶斯先验和提出新的先验分布族，CausalFM能够在不同的情况下执行贝叶斯因果推理，并且在CATEs估计上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于提供一种通用的方法来训练适用于各种因果推理场景的基础模型，同时改变从业者在医学、经济学等领域进行因果推理的方式。

**方法:** 首先正式定义了基于结构因果模型（SCMs）的贝叶斯先验构建方法，并推导出保证这些先验有效的必要条件。然后引入了一类受因果启发的贝叶斯神经网络新先验分布族。最后，实例化CausalFM并专门训练了一个用于估计条件平均处理效应（CATEs）的基础模型。

**结果:** CausalFM在使用合成数据和半合成基准测试的CATEs估计方面表现出了竞争力。

**结论:** 该框架可以作为训练不同因果推理设置下基础模型的一般方法，与当前最先进的因果推理技术相比，CausalFM提供了新的范式，有潜力从根本上改变实践者在多个学科领域进行因果推理的方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+Models+for+Causal+Inference+via+Prior-Data+Fitted+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10914，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10914&send_immediately=true&force_search=false)

**原文摘要:** Prior-data fitted networks (PFNs) have recently been proposed as a promising
way to train tabular foundation models. PFNs are transformers that are
pre-trained on synthetic data generated from a prespecified prior distribution
and that enable Bayesian inference through in-context learning. In this paper,
we introduce CausalFM, a comprehensive framework for training PFN-based
foundation models in various causal inference settings. First, we formalize the
construction of Bayesian priors for causal inference based on structural causal
models (SCMs) in a principled way and derive necessary criteria for the
validity of such priors. Building on this, we propose a novel family of prior
distributions using causality-inspired Bayesian neural networks that enable
CausalFM to perform Bayesian causal inference in various settings, including
back-door, front-door, and instrumental variable adjustment. Finally, we
instantiate CausalFM and explicitly train a foundation model for estimating
conditional average treatment effects (CATEs) using back-door adjustment. We
show that CausalFM performs competitively for CATE estimation using various
synthetic and semi-synthetic benchmarks. In sum, our framework can be used as a
general recipe to train foundation models for various causal inference
settings. In contrast to the current state-of-the-art in causal inference,
CausalFM offers a novel paradigm with the potential to fundamentally change how
practitioners perform causal inference in medicine, economics, and other
disciplines.

</details>


### [85] [Sequential-Parallel Duality in Prefix Scannable Models](https://arxiv.org/abs/2506.10918)
*Morris Yau, Sharut Gupta, Valerie Engelmayer, Kazuki Irie, Stefanie Jegelka, Jacob Andreas*

**主要类别:** cs.LG

**AI概要:** 本文探讨了能够支持近常数时间并行评估和线性时间、常数空间顺序推理的神经序列模型的完整类别。通过定义前缀可扫描模型（PSM），它统一了许多现有架构，同时引入了新的模型。这些模型在小型语言建模和合成任务上进行了实证评估，并显示出与基于变压器的架构相当的表现力，同时保持了状态空间模型的推理效率。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探索一类新型的神经序列模型，这类模型可以同时满足并行化训练和快速顺序推理的需求。特别是，作者想要刻画出所有能够支持近常数时间并行评估以及线性时间、常数空间顺序推理的模型。

**方法:** 作者首先描述了一类广泛的状态空间模型，其状态更新可以使用经典并行前缀扫描算法来计算。接着定义了一个更一般的类别——前缀可扫描模型（PSMs），它放宽了状态聚合操作符的要求，允许任意可能非结合性的函数，如softmax注意力。

**结果:** 实验结果表明，PSMs保留了基于transformer架构的表现力，同时也具有状态空间模型的推理效率，在某些情况下长度泛化甚至优于两者。

**结论:** 结论是，前缀可扫描模型（PSMs）提供了一种新框架，该框架不仅统一了现有多种架构，还引入了新的模型，这些模型在表现力和效率之间取得了良好的平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sequential-Parallel+Duality+in+Prefix+Scannable+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10918，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10918&send_immediately=true&force_search=false)

**原文摘要:** Modern neural sequence models are designed to meet the dual mandate of
parallelizable training and fast sequential inference. Recent developments have
given rise to various models, such as Gated Linear Attention (GLA) and Mamba,
that achieve such ``sequential-parallel duality.'' This raises a natural
question: can we characterize the full class of neural sequence models that
support near-constant-time parallel evaluation and linear-time, constant-space
sequential inference? We begin by describing a broad class of such models --
state space models -- as those whose state updates can be computed using the
classic parallel prefix scan algorithm with a custom associative aggregation
operator. We then define a more general class, Prefix-Scannable Models (PSMs),
by relaxing the state aggregation operator to allow arbitrary (potentially
non-associative) functions such as softmax attention. This generalization
unifies many existing architectures, including element-wise RNNs (e.g., Mamba)
and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new
models with softmax-like operators that achieve O(1) amortized compute per
token and log(N) memory for sequence length N. We empirically evaluate such
models on illustrative small-scale language modeling and canonical synthetic
tasks, including state tracking and associative recall. Empirically, we find
that PSMs retain the expressivity of transformer-based architectures while
matching the inference efficiency of state space models -- in some cases
exhibiting better length generalization than either.

</details>


### [86] [Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction](https://arxiv.org/abs/2506.10930)
*Thanathai Lertpetchpun, Tiantian Feng, Dani Byrd, Shrikanth Narayanan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种可复现的框架，通过多模态学习、多任务学习和处理不平衡数据来解决自然条件下语音情感识别的挑战，在IS25-SER挑战赛中取得了最佳表现。


<details>
  <summary>更多</summary>
  
**动机:** 在自然条件下进行语音情感识别存在诸多挑战，包括标注者之间的标注分歧以及数据分布不均衡的问题。

**方法:** 作者设计了一个系统，它采用了多模态学习、多任务学习，并且能够处理不平衡的数据集。具体来说，最好的系统是通过添加文本嵌入、预测性别以及将'其他'（O）和'无共识'（X）样本纳入训练集来训练的。

**结果:** 该系统在IS25-SER挑战赛的任务2中获得了第一的成绩，并且通过简单的双系统集成达到了最佳性能。

**结论:** 本研究提出的框架在MSP-Podcast数据集上评估，成功地解决了自然条件下的语音情感识别问题，并在竞赛中赢得了首奖。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Developing+a+High-performance+Framework+for+Speech+Emotion+Recognition+in+Naturalistic+Conditions+Challenge+for+Emotional+Attribute+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10930，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10930&send_immediately=true&force_search=false)

**原文摘要:** Speech emotion recognition (SER) in naturalistic conditions presents a
significant challenge for the speech processing community. Challenges include
disagreement in labeling among annotators and imbalanced data distributions.
This paper presents a reproducible framework that achieves superior (top 1)
performance in the Emotion Recognition in Naturalistic Conditions Challenge
(IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system
is designed to tackle the aforementioned challenges through multimodal
learning, multi-task learning, and imbalanced data handling. Specifically, our
best system is trained by adding text embeddings, predicting gender, and
including ``Other'' (O) and ``No Agreement'' (X) samples in the training set.
Our system's results secured both first and second places in the IS25-SER
Challenge, and the top performance was achieved by a simple two-system
ensemble.

</details>


### [87] [Self-Adapting Language Models](https://arxiv.org/abs/2506.10943)
*Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, Pulkit Agrawal*

**主要类别:** cs.LG

**AI概要:** 本文提出了Self-Adapting LLMs (SEAL)框架，使大型语言模型能够通过生成自己的微调数据和更新指令来自我适应。使用强化学习循环训练模型以产生有效的自我编辑，并以更新后的模型性能作为奖励信号。实验表明SEAL是实现语言模型自我导向适应的有希望的一步。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型虽然强大但静态，缺乏针对新任务、知识或示例调整权重的机制。

**方法:** SEAL框架允许LLMs通过生成它们自己的微调数据和更新指令来适应。给定新的输入时，模型会产生一个自我编辑，这可能包括信息重构、指定优化超参数或调用工具进行数据增强和基于梯度的更新。通过监督微调（SFT），这些自我编辑会导致持久的权重更新，从而实现持续适应。为了训练模型生成有效的自我编辑，采用了强化学习循环，以下游更新后模型的表现为奖励信号。

**结果:** 实验显示，SEAL在知识整合和少量样本泛化方面表现出色，是朝着能够自我导向适应的语言模型迈出的有希望的一步。

**结论:** SEAL提供了一种新颖的方法，让大型语言模型能够自我适应，不需要额外的适应模块或辅助网络，而是直接利用模型自身生成来控制其适应过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Adapting+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10943，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10943&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are powerful but static; they lack mechanisms to
adapt their weights in response to new tasks, knowledge, or examples. We
introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to
self-adapt by generating their own finetuning data and update directives. Given
a new input, the model produces a self-edit-a generation that may restructure
the information in different ways, specify optimization hyperparameters, or
invoke tools for data augmentation and gradient-based updates. Through
supervised finetuning (SFT), these self-edits result in persistent weight
updates, enabling lasting adaptation. To train the model to produce effective
self-edits, we use a reinforcement learning loop with the downstream
performance of the updated model as the reward signal. Unlike prior approaches
that rely on separate adaptation modules or auxiliary networks, SEAL directly
uses the model's own generation to control its adaptation process. Experiments
on knowledge incorporation and few-shot generalization show that SEAL is a
promising step toward language models capable of self-directed adaptation. Our
website and code is available at https://jyopari.github.io/posts/seal.

</details>


### [88] [Execution Guided Line-by-Line Code Generation](https://arxiv.org/abs/2506.10948)
*Boaz Lavon, Shahar Katz, Lior Wolf*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的神经代码生成方法，称为执行引导的无分类器指导(EG-CFG)，该方法在生成代码时动态地结合了执行信号，并通过实验表明这种方法在各种复杂度的编码任务中都显著提高了代码生成性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）已经展示了令人印象深刻的代码生成功能，但它们通常不会在推断过程中利用执行反馈，而这是人类程序员经常使用的关键信号。

**方法:** Execution-Guided Classifier-Free Guidance (EG-CFG) 方法，包括：1. 使用束搜索为每行代码采样候选完成；2. 通过对测试用例执行这些候选来提取执行信号；3. 在生成过程中将这些信号合并到提示中。

**结果:** EG-CFG 相比标准方法显著提升了代码生成的表现，在基础问题和具有挑战性的竞争编程任务等各种复杂度上均取得了最先进成果。

**结论:** 通过引入实时执行信号，EG-CFG 提供了一种新颖且有效的代码生成方式，能够提高生成代码的质量并保持语法结构的一致性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Execution+Guided+Line-by-Line+Code+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10948，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10948&send_immediately=true&force_search=false)

**原文摘要:** We present a novel approach to neural code generation that incorporates
real-time execution signals into the language model generation process. While
large language models (LLMs) have demonstrated impressive code generation
capabilities, they typically do not utilize execution feedback during
inference, a critical signal that human programmers regularly leverage. Our
method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically
incorporates execution signals as the model generates code, providing
line-by-line feedback that guides the generation process toward executable
solutions. EG-CFG employs a multi-stage process: first, we conduct beam search
to sample candidate program completions for each line; second, we extract
execution signals by executing these candidates against test cases; and
finally, we incorporate these signals into the prompt during generation. By
maintaining consistent signals across tokens within the same line and
refreshing signals at line boundaries, our approach provides coherent guidance
while preserving syntactic structure. Moreover, the method naturally supports
native parallelism at the task level in which multiple agents operate in
parallel, exploring diverse reasoning paths and collectively generating a broad
set of candidate solutions. Our experiments across diverse coding tasks
demonstrate that EG-CFG significantly improves code generation performance
compared to standard approaches, achieving state-of-the-art results across
various levels of complexity, from foundational problems to challenging
competitive programming tasks. Our code is available at:
https://github.com/boazlavon/eg_cfg

</details>


### [89] [Build the web for agents, not agents for the web](https://arxiv.org/abs/2506.10953)
*Xing Han Lù, Gaurav Kamath, Marius Mosbach, Siva Reddy*

**主要类别:** cs.LG

**AI概要:** 该论文提出了一种新的交互范式，即Agentic Web Interface (AWI)，旨在为代理设计专门的网站导航界面，以克服现有方法在处理复杂网页输入时遇到的问题。文章提出了六个指导原则来设计AWI，强调安全性、效率和标准化，以便更高效、可靠且透明地设计web agent。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于大语言模型（LLM）和多模态技术开发的web agents在自主浏览和完成网络任务方面展现了巨大潜力，但现有的方法因为人机界面与LLM能力之间存在根本性不匹配而面临重大挑战。

**方法:** 本文倡导在网络代理研究中进行范式转变，不再让网络代理去适应为人类设计的界面，而是开发一种新的交互范式——Agentic Web Interface（AWI），专门为代理优化。文中提出了六个关于AWI设计的指导原则，这些原则关注于安全、效率和标准化。

**结果:** 通过引入Agentic Web Interface的概念并制定六项设计原则，研究人员期望能够解决现有界面的根本限制，从而实现更高效、可靠和透明的web agent设计。

**结论:** 为了使web agents能够更好地执行复杂的网络互动，需要转向一种专为这些智能体设计的新交互模式。Agentic Web Interface及其六项设计原则为未来的web agent设计提供了一个新的方向，这将是一个需要机器学习社区共同努力的工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Build+the+web+for+agents%2C+not+agents+for+the+web，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10953，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10953&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in Large Language Models (LLMs) and multimodal
counterparts have spurred significant interest in developing web agents -- AI
systems capable of autonomously navigating and completing tasks within web
environments. While holding tremendous promise for automating complex web
interactions, current approaches face substantial challenges due to the
fundamental mismatch between human-designed interfaces and LLM capabilities.
Current methods struggle with the inherent complexity of web inputs, whether
processing massive DOM trees, relying on screenshots augmented with additional
information, or bypassing the user interface entirely through API interactions.
This position paper advocates for a paradigm shift in web agent research:
rather than forcing web agents to adapt to interfaces designed for humans, we
should develop a new interaction paradigm specifically optimized for agentic
capabilities. To this end, we introduce the concept of an Agentic Web Interface
(AWI), an interface specifically designed for agents to navigate a website. We
establish six guiding principles for AWI design, emphasizing safety,
efficiency, and standardization, to account for the interests of all primary
stakeholders. This reframing aims to overcome fundamental limitations of
existing interfaces, paving the way for more efficient, reliable, and
transparent web agent design, which will be a collaborative effort involving
the broader ML community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/abs/2506.10130)
*Luciano Floridi*

**主要类别:** cs.AI

**AI概要:** 本文提出一个猜想，该猜想明确指出了AI系统中可证明正确性与广泛数据映射能力之间的基本权衡。当AI系统被设计为具有演绎上无懈可击的保证时，其操作领域必须被严格限制；反之，能够处理高维数据并产生丰富信息输出的系统则不可避免地会带来错误风险。文章通过信息论形式阐述了这一猜想，并探讨了它对工程目标、哲学期望以及AI评估标准、治理框架和混合系统设计的影响。


<details>
  <summary>更多</summary>
  
**动机:** 本文的动机是解决在人工智能系统中长期存在的隐含矛盾：即绝对正确性和处理复杂现实世界数据的能力之间的权衡。这种张力在历史上一直存在，但没有得到充分的认识和正式化。

**方法:** 论文首先回顾了造成这种紧张关系的历史动因，然后将猜想表述成信息论的形式，并将其置于更广泛的认知论、形式验证和技术哲学辩论的背景之中。接着，文章分析了这一猜想可能带来的影响和后果。

**结果:** 结果表明，如果这个猜想成立，它将有助于重塑AI系统的评价标准、治理架构以及混合系统的设计方法。

**结论:** 结论强调了最终证明或反驳这个不等式对于未来可信AI发展的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Conjecture+on+a+Fundamental+Trade-Off+between+Certainty+and+Scope+in+Symbolic+and+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10130，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10130&send_immediately=true&force_search=false)

**原文摘要:** This article introduces a conjecture that formalises a fundamental trade-off
between provable correctness and broad data-mapping capacity in Artificial
Intelligence (AI) systems. When an AI system is engineered for deductively
watertight guarantees (demonstrable certainty about the error-free nature of
its outputs) -- as in classical symbolic AI -- its operational domain must be
narrowly circumscribed and pre-structured. Conversely, a system that can input
high-dimensional data to produce rich information outputs -- as in contemporary
generative models -- necessarily relinquishes the possibility of zero-error
performance, incurring an irreducible risk of errors or misclassification. By
making this previously implicit trade-off explicit and open to rigorous
verification, the conjecture significantly reframes both engineering ambitions
and philosophical expectations for AI. After reviewing the historical
motivations for this tension, the article states the conjecture in
information-theoretic form and contextualises it within broader debates in
epistemology, formal verification, and the philosophy of technology. It then
offers an analysis of its implications and consequences, drawing on notions of
underdetermination, prudent epistemic risk, and moral responsibility. The
discussion clarifies how, if correct, the conjecture would help reshape
evaluation standards, governance frameworks, and hybrid system design. The
conclusion underscores the importance of eventually proving or refuting the
inequality for the future of trustworthy AI.

</details>


### [91] [One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence](https://arxiv.org/abs/2506.10157)
*Michelle M. Li, Ben Y. Reis, Adam Rodman, Tianxi Cai, Noa Dagan, Ran D. Balicer, Joseph Loscalzo, Isaac S. Kohane, Marinka Zitnik*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个愿景，即开发能够动态适应不同医疗背景的人工智能模型，以克服现有医学基础模型在处理未见过的输入和适应新临床情境时的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 当前医学基础模型在遇到训练过程中未曾出现的临床情况时，无法很好地解释陌生输入或进行相应调整，容易产生上下文错误，这限制了它们的应用范围。

**方法:** 作者提出了一种具有上下文切换能力的AI模型概念，这种模型能够在不重新训练的情况下，根据不同的专业、人群、工作流程以及临床角色动态调整其推理方式。

**结果:** 预期这样的上下文切换AI能够诊断、管理和治疗跨专科和地区广泛疾病，并扩大医疗服务的可及性。

**结论:** 文章认为，具备上下文切换能力的医学AI是未来的发展方向，它能够更好地服务于不断变化的医疗环境。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One+Patient%2C+Many+Contexts%3A+Scaling+Medical+AI+Through+Contextual+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10157&send_immediately=true&force_search=false)

**原文摘要:** Medical foundation models, including language models trained on clinical
notes, vision-language models on medical images, and multimodal models on
electronic health records, can summarize clinical notes, answer medical
questions, and assist in decision-making. Adapting these models to new
populations, specialties, or settings typically requires fine-tuning, careful
prompting, or retrieval from knowledge bases. This can be impractical, and
limits their ability to interpret unfamiliar inputs and adjust to clinical
situations not represented during training. As a result, models are prone to
contextual errors, where predictions appear reasonable but fail to account for
critical patient-specific or contextual information. These errors stem from a
fundamental limitation that current models struggle with: dynamically adjusting
their behavior across evolving contexts of medical care. In this Perspective,
we outline a vision for context-switching in medical AI: models that
dynamically adapt their reasoning without retraining to new specialties,
populations, workflows, and clinical roles. We envision context-switching AI to
diagnose, manage, and treat a wide range of diseases across specialties and
regions, and expand access to medical care.

</details>


### [92] [Correlation vs causation in Alzheimer's disease: an interpretability-driven study](https://arxiv.org/abs/2506.10179)
*Hamzah Dabool, Raghad Mustafa*

**主要类别:** cs.AI

**AI概要:** 本研究通过结合相关性分析、机器学习分类和模型可解释性技术，探讨了临床、认知、遗传及生物标志物特征之间的关系，使用XGBoost算法识别出影响AD分类的关键特征，并利用SHAP值深入了解特征在不同疾病阶段的贡献。研究强调强相关并不意味着因果关系，为未来揭示真正病理机制的因果推断研究奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 理解因果关系与相关性的区别对阿尔茨海默病（AD）的研究至关重要，因为这会影响诊断、治疗以及真正疾病驱动因素的识别。

**方法:** 实验采用了一种综合方法来探究临床、认知、遗传和生物标志物特征之间的关系，包括使用相关性分析、基于XGBoost算法的机器学习分类技术和模型可解释性技术如SHAP值。

**结果:** 研究确定了影响AD分类的关键特征，例如认知分数和遗传风险因子，并通过SHAP值提供了关于特征在整个疾病过程中贡献度的深入见解。此外，还发现强烈的相关性并不总是暗示着因果联系。

**结论:** 通过整合特征重要性和可解释性与经典统计分析，这项工作为旨在揭示真实病理机制的未来因果推理研究打下了基础。最终，区分因果因素与关联标记可以导致阿尔茨海默病的早期诊断改进和靶向干预措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Correlation+vs+causation+in+Alzheimer%27s+disease%3A+an+interpretability-driven+study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10179，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10179&send_immediately=true&force_search=false)

**原文摘要:** Understanding the distinction between causation and correlation is critical
in Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and
the identification of true disease drivers. This experiment investigates the
relationships among clinical, cognitive, genetic, and biomarker features using
a combination of correlation analysis, machine learning classification, and
model interpretability techniques. Employing the XGBoost algorithm, we
identified key features influencing AD classification, including cognitive
scores and genetic risk factors. Correlation matrices revealed clusters of
interrelated variables, while SHAP (SHapley Additive exPlanations) values
provided detailed insights into feature contributions across disease stages.
Our results highlight that strong correlations do not necessarily imply
causation, emphasizing the need for careful interpretation of associative data.
By integrating feature importance and interpretability with classical
statistical analysis, this work lays groundwork for future causal inference
studies aimed at uncovering true pathological mechanisms. Ultimately,
distinguishing causal factors from correlated markers can lead to improved
early diagnosis and targeted interventions for Alzheimer's disease.

</details>


### [93] [Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems](https://arxiv.org/abs/2506.10192)
*Filip Cano*

**主要类别:** cs.AI

**AI概要:** 该论文推进了人工智能系统在安全性、公平性、透明度和问责制方面的知识，通过提出新的防护措施和技术框架，为实现更安全、更公平且更有责任的人工智能系统奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 随着自主系统对社会关键领域的影响越来越大，确保人工智能的负责任使用变得至关重要。但可信AI的概念仍然宽泛且多方面，需要进一步研究来具体化。

**方法:** 论文扩展了经典确定性屏蔽技术以应对延迟观察，并将确定性和概率性安全屏蔽实施到模拟自动驾驶车辆中；引入了公平屏蔽的新方法，在有限和周期时间范围内强制执行序列决策设置中的群体公平；提出了一个正式框架来评估概率决策代理中的故意行为，引入了能动性和意图商数的定量指标。

**结果:** 通过优化干预成本同时严格确保公平约束，新方法有效地平衡了公平与最小干预；提出的回顾性意图分析有助于确定当自主系统造成意外伤害时的责任归属；通过“反应式决策”框架统一了这些贡献，提供了一个综合以前方法的一般形式化。

**结论:** 本文所作的贡献实际上促进了更安全、更公平、更有责任的人工智能系统的实现，并为未来可信AI的研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Responsible+AI%3A+Advances+in+Safety%2C+Fairness%2C+and+Accountability+of+Autonomous+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10192&send_immediately=true&force_search=false)

**原文摘要:** Ensuring responsible use of artificial intelligence (AI) has become
imperative as autonomous systems increasingly influence critical societal
domains. However, the concept of trustworthy AI remains broad and
multi-faceted. This thesis advances knowledge in the safety, fairness,
transparency, and accountability of AI systems. In safety, we extend classical
deterministic shielding techniques to become resilient against delayed
observations, enabling practical deployment in real-world conditions. We also
implement both deterministic and probabilistic safety shields into simulated
autonomous vehicles to prevent collisions with road users, validating the use
of these techniques in realistic driving simulators. We introduce fairness
shields, a novel post-processing approach to enforce group fairness in
sequential decision-making settings over finite and periodic time horizons. By
optimizing intervention costs while strictly ensuring fairness constraints,
this method efficiently balances fairness with minimal interference. For
transparency and accountability, we propose a formal framework for assessing
intentional behaviour in probabilistic decision-making agents, introducing
quantitative metrics of agency and intention quotient. We use these metrics to
propose a retrospective analysis of intention, useful for determining
responsibility when autonomous systems cause unintended harm. Finally, we unify
these contributions through the ``reactive decision-making'' framework,
providing a general formalization that consolidates previous approaches.
Collectively, the advancements presented contribute practically to the
realization of safer, fairer, and more accountable AI systems, laying the
foundations for future research in trustworthy AI.

</details>


### [94] [WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2506.10264)
*Qiyue Yin, Pei Xu, Qiaozhe Li, Shengda Liu, Shengqi Shen, Tong Wang, Yihong Han, Xiaonan Zhao, Likun Yang, Shiyue Cao, Shiyu Qiu, Yuxuan Liu, Shizhao Yu, Lei Cui, Chengxin Yan, Jie Sun, Xiangquan Tang, Kaiqi Huang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了WGSR-Bench，这是一个使用战争游戏作为评估环境的首个策略推理基准，用于系统地评估大型语言模型在多智能体决策、意图推断和反事实推理方面的能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型（LLMs）在数学、符号和常识推理等任务上展示了显著的能力，但它们在战略推理方面的能力尚未得到系统的评估或建模。战略推理是高级人类认知的一个关键组成部分，包括评估动态环境中多智能体行为、制定行动计划和适应策略的能力。

**方法:** 提出了WGSR-Bench，这是第一个针对LLMs的战略推理基准，它利用了战争游戏这一高复杂度的战略场景来评估模型。该基准围绕三个核心任务设计测试样本：环境态势感知、对手风险建模以及策略生成，这些构成了S-POE架构的核心。此外，还设计了一个基于LLM的战争游戏代理，以进行全面的战略推理评估。

**结果:** 通过WGSR-Bench可以对当前最先进的LLMs在博弈论战略推理中的优势与局限性进行全面评价，并推动大型模型驱动的战略智能研究的发展。

**结论:** WGSR-Bench为评估和促进大型语言模型在复杂战略情境下的推理能力提供了新的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WGSR-Bench%3A+Wargame-based+Game-theoretic+Strategic+Reasoning+Benchmark+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10264&send_immediately=true&force_search=false)

**原文摘要:** Recent breakthroughs in Large Language Models (LLMs) have led to a
qualitative leap in artificial intelligence' s performance on reasoning tasks,
particularly demonstrating remarkable capabilities in mathematical, symbolic,
and commonsense reasoning. However, as a critical component of advanced human
cognition, strategic reasoning, i.e., the ability to assess multi-agent
behaviors in dynamic environments, formulate action plans, and adapt
strategies, has yet to be systematically evaluated or modeled. To address this
gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark
for LLMs using wargame as its evaluation environment. Wargame, a quintessential
high-complexity strategic scenario, integrates environmental uncertainty,
adversarial dynamics, and non-unique strategic choices, making it an effective
testbed for assessing LLMs' capabilities in multi-agent decision-making, intent
inference, and counterfactual reasoning. WGSR-Bench designs test samples around
three core tasks, i.e., Environmental situation awareness, Opponent risk
modeling and Policy generation, which serve as the core S-POE architecture, to
systematically assess main abilities of strategic reasoning. Finally, an
LLM-based wargame agent is designed to integrate these parts for a
comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess
the strengths and limitations of state-of-the-art LLMs in game-theoretic
strategic reasoning and to advance research in large model-driven strategic
intelligence.

</details>


### [95] [Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution](https://arxiv.org/abs/2506.10281)
*Xinmin Fang, Lingfeng Tao, Zhengxiong Li*

**主要类别:** cs.AI

**AI概要:** 本文将人工智能重新定义为一种认知引擎，推动着一场与工业革命不同的生产力革命。通过跨学科视角结合计算机科学、经济学和社会学的观点，探讨了AI如何重塑工作和社会，并提出了AI作为认知工具对生产力的深远影响。


<details>
  <summary>更多</summary>
  
**动机:** 作者意图重新构建对人工智能的理解，将其视为一种类似于书写语言的认知革命，旨在放大人类智力而不是作为一个简单的机械化工具。

**方法:** 采用多学科视角，结合了计算机科学的进步、经济洞察力以及社会学视角来分析AI如何改变工作和社会。通过概念框架描绘从手工到认知生产力的转变。

**结果:** 研究表明，AI如同人类语言一样，作为认知引擎发挥作用，预示着新的生产力范式。它要求我们重新思考技能、组织结构和政策。

**结论:** 结论是人工智能的潜力在于补充人类的认知能力，标志着生产力演变的新篇章。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Closer+to+Language+than+Steam%3A+AI+as+the+Cognitive+Engine+of+a+New+Productivity+Revolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10281，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10281&send_immediately=true&force_search=false)

**原文摘要:** Artificial Intelligence (AI) is reframed as a cognitive engine driving a
novel productivity revolution distinct from the Industrial Revolution's
physical thrust. This paper develops a theoretical framing of AI as a cognitive
revolution akin to written language - a transformative augmentation of human
intellect rather than another mechanized tool. We compare AI's emergence to
historical leaps in information technology to show how it amplifies knowledge
work. Examples from various domains demonstrate AI's impact as a driver of
productivity in cognitive tasks. We adopt a multidisciplinary perspective
combining computer science advances with economic insights and sociological
perspectives on how AI reshapes work and society. Through conceptual
frameworks, we visualize the shift from manual to cognitive productivity. Our
central argument is that AI functions as an engine of cognition - comparable to
how human language revolutionized knowledge - heralding a new productivity
paradigm. We discuss how this revolution demands rethinking of skills,
organizations, and policies. This paper, balancing academic rigor with clarity,
concludes that AI's promise lies in complementing human cognitive abilities,
marking a new chapter in productivity evolution.

</details>


### [96] [The Alignment Trap: Complexity Barriers](https://arxiv.org/abs/2506.10304)
*Jasper Yao*

**主要类别:** cs.AI

**AI概要:** 本文建立了AI系统能力扩展时验证AI安全性的基本计算复杂性障碍，证明了当AI系统的表达能力超过某一临界阈值时，安全性验证需要指数时间且是coNP完全问题。通过四个核心定理，文章揭示了随着AI能力的增加，社会对安全的要求趋向于完美，与验证复杂性之间形成了不可避免的紧张关系，并提出了一个战略上的三难困境：限制系统复杂性、接受不可验证的风险或开发新的安全范式。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探讨随着AI系统能力的提升，验证这些系统安全性的计算复杂性所带来的挑战。目的是要理解在AI系统能力增长过程中，确保其安全运行所面临的根本障碍。

**方法:** 采用计算复杂性理论来分析AI安全验证问题，定义了表达能力EXP(m)和临界阈值τ的概念，并通过Capability-Risk Scaling (CRS)动态形式化描述了AI能力与社会安全需求之间的关系。此外，还通过四个核心定理从理论上论证了安全验证问题的复杂性。

**结果:** 主要发现包括：(1) 验证复杂度随系统表达能力呈指数增长；(2) 安全策略仅占策略空间的一小部分（最多为$2^{-2^m}$）；(3) 任何有限数量的对齐技术都无法提供全面覆盖；(4) 对于神经网络而言，强健的安全属性构成了零测集。

**结论:** 研究结论指出了一个“难以处理的差距”，即实际安全要求处于计算上难以处理的区域内。最后提出了一种战略上的三难困境，强调了AI发展必须面对的选择：要么限制系统复杂度以保持可验证的安全性，要么在扩展能力时接受无法验证的风险，或者开发超出传统验证方法的新安全范例。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Alignment+Trap%3A+Complexity+Barriers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10304，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10304&send_immediately=true&force_search=false)

**原文摘要:** We establish fundamental computational complexity barriers to verifying AI
safety as system capabilities scale. Our main results show that for AI systems
with expressiveness EXP$(m)$ above a critical threshold $\tau$, safety
verification requires exponential time and is coNP-complete. We formalize the
Capability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI
capability drives societal safety requirements toward perfection, creating an
inescapable tension with verification complexity. Through four core theorems,
we prove that (1) verification complexity grows exponentially with system
expressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the
policy space, (3) no finite set of alignment techniques can provide universal
coverage, and (4) robust safety properties form measure-zero sets for neural
networks. These results characterize an "intractability gap" where practical
safety requirements fall within the region of computational intractability. We
conclude by presenting a strategic trilemma: AI development must either
constrain system complexity to maintain verifiable safety, accept unverifiable
risks while scaling capabilities, or develop fundamentally new safety paradigms
beyond verification. Our work provides the first systematic
complexity-theoretic analysis of AI alignment and establishes rigorous bounds
that any safety approach must confront. A formal verification of the core
theorems in Lean4 is currently in progress.

</details>


### [97] [A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pokémon](https://arxiv.org/abs/2506.10326)
*Cameron Angliss, Jiaxun Cui, Jiaheng Hu, Arrasy Rahman, Peter Stone*

**主要类别:** cs.AI

**AI概要:** 研究者们开发了一个名为VGC-Bench的基准测试，它为Pokémon视频游戏锦标赛中多智能体学习的研究提供了关键基础设施、标准化评估协议以及从大型语言模型代理到强化学习等多种基线方法。在限定于单个队伍配置的情况下，所提出的方法能够战胜专业VGC选手，但当团队规模扩大时，最佳算法在扩展性上遇到困难。


<details>
  <summary>更多</summary>
  
**动机:** 多智能体学习面临的一个中心挑战是开发能够在不重新训练的情况下适应完全不同战略环境的人工智能代理。Pokémon视频游戏锦标赛（VGC）是一个具有非常大的可能队伍配置空间的领域，其离散性和组合性使得最优策略会根据不同的队伍和对手而剧烈变化，因此通用化尤为困难。

**方法:** 研究引入了VGC-Bench作为基准测试，该基准提供关键基础设施，标准化评估协议，并且提供了人类玩家数据集以及一系列基线方法，包括大型语言模型代理、行为克隆、强化学习及经验博弈论方法如自对弈、虚构对弈和双重oracle等。

**结果:** 在仅限于单一队伍配置的情况下，所提出的方法能够击败专业VGC竞争者；然而，在逐步扩大的队伍集合上的广泛评估表明，即使是在单队情况下表现最好的算法也难以随着队伍规模的增长而良好扩展。

**结论:** 政策泛化至多样化的队伍策略仍然是社区面临的开放挑战。代码已经开源，可供进一步研究使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Benchmark+for+Generalizing+Across+Diverse+Team+Strategies+in+Competitive+Pok%C3%A9mon，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10326，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10326&send_immediately=true&force_search=false)

**原文摘要:** Developing AI agents that can robustly adapt to dramatically different
strategic landscapes without retraining is a central challenge for multi-agent
learning. Pok\'emon Video Game Championships (VGC) is a domain with an
extraordinarily large space of possible team configurations of approximately
$10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete,
combinatorial nature of team building in Pok\'emon VGC causes optimal
strategies to shift dramatically depending on both the team being piloted and
the opponent's team, making generalization uniquely challenging. To advance
research on this problem, we introduce VGC-Bench: a benchmark that provides
critical infrastructure, standardizes evaluation protocols, and supplies
human-play datasets and a range of baselines - from large-language-model agents
and behavior cloning to reinforcement learning and empirical game-theoretic
methods such as self-play, fictitious play, and double oracle. In the
restricted setting where an agent is trained and evaluated on a single-team
configuration, our methods are able to win against a professional VGC
competitor. We extensively evaluated all baseline methods over progressively
larger team sets and find that even the best-performing algorithm in the
single-team setting struggles at scaling up as team size grows. Thus, policy
generalization across diverse team strategies remains an open challenge for the
community. Our code is open sourced at
https://github.com/cameronangliss/VGC-Bench.

</details>


### [98] [Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts](https://arxiv.org/abs/2506.10357)
*Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Weili Guan, Dongmei Jiang, Liqiang Nie*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个名为Optimus-3的多功能代理，它通过知识增强的数据生成管道、专家混合架构和多模态推理增强强化学习方法来解决在开放世界环境如Minecraft中构建具有感知、计划、行动、基础和反思能力的通用代理所面临的挑战。实验结果表明，Optimus-3在Minecraft环境中多种任务上的表现优于现有的多模态大型语言模型和其他最先进的代理。


<details>
  <summary>更多</summary>
  
**动机:** 尽管基于多模态大型语言模型（MLLMs）的代理在各个领域取得了显著进展，但在开放世界环境（如Minecraft）中构建具备感知、规划、动作执行、情境理解和自我反省等能力的通用代理仍面临以下挑战：领域特定数据不足、异构任务间的干扰以及开放世界设置中的视觉多样性。

**方法:** 1) 提出了一种知识增强的数据生成流程，为代理的发展提供可扩展且高质量的训练数据；2) 为了减少异构任务之间的干扰，引入了带有任务级路由的专家混合（MoE）架构；3) 开发了一种多模态推理增强的强化学习方法，以提高代理对Minecraft中视觉多样性的推理能力。

**结果:** 广泛的实验证明，Optimus-3在Minecraft环境中跨越广泛的任务上超越了通用多模态大型语言模型及现有最先进代理的表现。

**结论:** 通过一系列创新，研究者们展示了Optimus-3，一种专为Minecraft设计的通用代理，它有效地解决了当前构建跨功能代理所遇到的主要难题，并在不同任务上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimus-3%3A+Towards+Generalist+Multimodal+Minecraft+Agents+with+Scalable+Task+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10357，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10357&send_immediately=true&force_search=false)

**原文摘要:** Recently, agents based on multimodal large language models (MLLMs) have
achieved remarkable progress across various domains. However, building a
generalist agent with capabilities such as perception, planning, action,
grounding, and reflection in open-world environments like Minecraft remains
challenges: insufficient domain-specific data, interference among heterogeneous
tasks, and visual diversity in open-world settings. In this paper, we address
these challenges through three key contributions. 1) We propose a
knowledge-enhanced data generation pipeline to provide scalable and
high-quality training data for agent development. 2) To mitigate interference
among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture
with task-level routing. 3) We develop a Multimodal Reasoning-Augmented
Reinforcement Learning approach to enhance the agent's reasoning ability for
visual diversity in Minecraft. Built upon these innovations, we present
Optimus-3, a general-purpose agent for Minecraft. Extensive experimental
results demonstrate that Optimus-3 surpasses both generalist multimodal large
language models and existing state-of-the-art agents across a wide range of
tasks in the Minecraft environment. Project page:
https://cybertronagent.github.io/Optimus-3.github.io/

</details>


### [99] [NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War](https://arxiv.org/abs/2506.10384)
*Jim O'Connor, Yeonghun Lee, Gary B Parker*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为NeuroPAL的神经进化框架，它结合了拓扑结构增强的神经进化(NEAT)与间断性即时学习(PAL)，以提高StarCraft: Brood War游戏中宏观管理AI训练的效率。结果显示，NeuroPAL相比标准NEAT大大加速了学习过程，并且能够发展出人类玩家常用的策略。


<details>
  <summary>更多</summary>
  
**动机:** 传统的《星际争霸》人工智能依赖于基于规则的系统或有监督的深度学习，这些方法在适应性和计算效率方面存在局限性。为了克服这些限制，研究者们提出了一个新的框架。

**方法:** 研究者们引入了NeuroPAL框架，该框架将拓扑结构增强的神经进化（NEAT）与间断性任意时间学习（PAL）相结合，通过频繁的低精度训练和周期性的高精度评估交替进行来提升样本效率。

**结果:** 实验结果表明，PAL显著加快了学习进程，使得代理能够在大约一半的时间内达到具有竞争力的游戏水平。此外，进化的代理展现出了如代理兵营放置和防御建筑优化等高级行为。

**结论:** 研究表明，像PAL这样的结构化评估机制可以提高神经进化在复杂实时策略环境中的可扩展性和有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是NeuroPAL%3A+Punctuated+Anytime+Learning+with+Neuroevolution+for+Macromanagement+in+Starcraft%3A+Brood+War，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10384，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10384&send_immediately=true&force_search=false)

**原文摘要:** StarCraft: Brood War remains a challenging benchmark for artificial
intelligence research, particularly in the domain of macromanagement, where
long-term strategic planning is required. Traditional approaches to StarCraft
AI rely on rule-based systems or supervised deep learning, both of which face
limitations in adaptability and computational efficiency. In this work, we
introduce NeuroPAL, a neuroevolutionary framework that integrates
Neuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning
(PAL) to improve the efficiency of evolutionary training. By alternating
between frequent, low-fidelity training and periodic, high-fidelity
evaluations, PAL enhances the sample efficiency of NEAT, enabling agents to
discover effective strategies in fewer training iterations. We evaluate
NeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and
compare its performance to standard NEAT-based training. Our results show that
PAL significantly accelerates the learning process, allowing the agent to reach
competitive levels of play in approximately half the training time required by
NEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as
proxy barracks placement and defensive building optimization, strategies
commonly used by expert human players. These findings suggest that structured
evaluation mechanisms like PAL can enhance the scalability and effectiveness of
neuroevolution in complex real-time strategy environments.

</details>


### [100] [Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills](https://arxiv.org/abs/2506.10387)
*Yuquan Xie, Zaijing Li, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Dongmei Jiang, Liqiang Nie*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种层次化多模态技能模块（HMS）和技能增强的蒙特卡洛树搜索算法（SA-MCTS），并基于此开发了跨平台即插即用GUI代理Mirage-1，以解决多模态大语言模型在执行长周期任务时的知识不足问题，并缩小离线与在线环境之间的领域差距。实验结果显示，Mirage-1在多个基准测试中优于先前的代理。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态大语言模型作为图形用户界面代理在处理在线环境中的长期任务时表现不佳，主要原因是知识不够充分以及离线与在线领域之间存在固有的差异。

**方法:** 提出了一个层次化的多模态技能（HMS）模块来逐步抽象轨迹为执行技能、核心技能和元技能，从而提供了一个针对长期任务规划的分层知识结构；同时提出了技能增强型蒙特卡洛树搜索（SA-MCTS）算法，该算法能够有效利用离线环境中获得的技能来减少在线探索过程中的动作搜索空间。

**结果:** 通过构建一个新的基准AndroidLH，实验证明Mirage-1在AndroidWorld, MobileMiniWob++, Mind2Web-Live, 以及AndroidLH上的性能分别比之前的方法提高了32%, 19%, 15%, 和79%。

**结论:** 提出的HMS模块和SA-MCTS算法有助于提高多模态大语言模型作为GUI代理执行长期任务的能力，而Mirage-1作为一个实际应用案例，在现实世界场景下展示了显著的性能提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mirage-1%3A+Augmenting+and+Updating+GUI+Agent+with+Hierarchical+Multimodal+Skills，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10387，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10387&send_immediately=true&force_search=false)

**原文摘要:** Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI
agents have yielded promising outcomes. However, these agents still struggle
with long-horizon tasks in online environments, primarily due to insufficient
knowledge and the inherent gap between offline and online domains. In this
paper, inspired by how humans generalize knowledge in open-ended environments,
we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of
insufficient knowledge. It progressively abstracts trajectories into execution
skills, core skills, and ultimately meta-skills, providing a hierarchical
knowledge structure for long-horizon task planning. To bridge the domain gap,
we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,
which efficiently leverages skills acquired in offline environments to reduce
the action search space during online tree exploration. Building on HMS, we
propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To
validate the performance of Mirage-1 in real-world long-horizon scenarios, we
constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1
outperforms previous agents by 32\%, 19\%, 15\%, and 79\% on AndroidWorld,
MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:
https://cybertronagent.github.io/Mirage-1.github.io/

</details>


### [101] [Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges](https://arxiv.org/abs/2506.10408)
*Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, Ziyue Li*

**主要类别:** cs.AI

**AI概要:** 本文综述了推理代理RAG方法，将其分为预定义推理和代理推理两大类，并探讨了架构设计、推理策略及工具协调等方面的技术。文章还讨论了关键研究挑战并提出了未来的研究方向以提高推理代理RAG系统的灵活性、鲁棒性和适用性。


<details>
  <summary>更多</summary>
  
**动机:** 为了克服基于静态流水线的早期RAG系统在需要复杂推理、动态检索和多模态整合的真实世界场景中的局限性，领域转向了将决策制定和自适应工具使用直接嵌入检索过程的推理代理RAG范式。

**方法:** 对Reasoning Agentic RAG方法进行了全面回顾，将其分类为预定义推理和代理推理两种主要系统，并分析了两类范式下的代表性技术，包括架构设计、推理策略和工具协调。

**结果:** 提供了对于Reasoning Agentic RAG领域的深入理解，指出了当前存在的研究挑战，并对未来的发展方向提出了建议。

**结论:** 本文通过综述和分析推理代理RAG的方法，为推进该领域内系统的灵活性、鲁棒性和适用性提供了见解和未来的探索路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+RAG+via+System+1+or+System+2%3A+A+Survey+on+Reasoning+Agentic+Retrieval-Augmented+Generation+for+Industry+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10408&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
overcome the knowledge limitations of Large Language Models (LLMs) by
integrating external retrieval with language generation. While early RAG
systems based on static pipelines have shown effectiveness in well-structured
tasks, they struggle in real-world scenarios requiring complex reasoning,
dynamic retrieval, and multi-modal integration. To address these challenges,
the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds
decision-making and adaptive tool use directly into the retrieval process. In
this paper, we present a comprehensive review of Reasoning Agentic RAG methods,
categorizing them into two primary systems: predefined reasoning, which follows
fixed modular pipelines to boost reasoning, and agentic reasoning, where the
model autonomously orchestrates tool interaction during inference. We analyze
representative techniques under both paradigms, covering architectural design,
reasoning strategies, and tool coordination. Finally, we discuss key research
challenges and propose future directions to advance the flexibility,
robustness, and applicability of reasoning agentic RAG systems. Our collection
of the relevant research has been organized into a
https://github.com/ByebyeMonica/Reasoning-Agentic-RAG.

</details>


### [102] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/abs/2506.10420)
*Boris Sedlak, Alireza Furutanpey, Zihang Wang, Víctor Casamayor Pujol, Schahram Dustdar*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种基于代理的自动扩展框架，该框架在受限环境中动态调整硬件资源和内部服务配置，以最大化需求满足度。通过比较四种类型的扩展代理，并使用YOLOv8和OpenCV两个现实世界处理服务进行实验，结果表明所有代理都达到了可接受的服务水平目标性能，但收敛模式不同。


<details>
  <summary>更多</summary>
  
**动机:** 边缘计算由于严格的资源限制而与传统的自动扩展方式不相容，因此需要更加灵活的扩展行为，采用多个弹性维度来应对。

**方法:** 研究引入了一个基于代理的自动扩展框架，该框架能够动态地同时调整硬件资源和内部服务配置。研究中比较了四种类型的扩展代理：主动推理、深度Q网络、结构知识分析和深度主动推理，并且使用了两个并行运行的实际处理服务——用于视觉识别的YOLOv8和用于二维码检测的OpenCV。

**结果:** 结果显示所有代理都能达到可接受的服务等级目标（SLO）性能，但表现出不同的收敛模式。深度Q网络得益于预训练，结构分析则快速收敛，而深度主动推理代理结合了理论基础和实际扩展优势。

**结论:** 这些发现为多维基于代理的边缘环境自动扩展的可行性提供了证据，并鼓励在这个研究方向上进一步开展工作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-dimensional+Autoscaling+of+Processing+Services%3A+A+Comparison+of+Agent-based+Methods，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10420，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10420&send_immediately=true&force_search=false)

**原文摘要:** Edge computing breaks with traditional autoscaling due to strict resource
constraints, thus, motivating more flexible scaling behaviors using multiple
elasticity dimensions. This work introduces an agent-based autoscaling
framework that dynamically adjusts both hardware resources and internal service
configurations to maximize requirements fulfillment in constrained
environments. We compare four types of scaling agents: Active Inference, Deep Q
Network, Analysis of Structural Knowledge, and Deep Active Inference, using two
real-world processing services running in parallel: YOLOv8 for visual
recognition and OpenCV for QR code detection. Results show all agents achieve
acceptable SLO performance with varying convergence patterns. While the Deep Q
Network benefits from pre-training, the structural analysis converges quickly,
and the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environments
and encourage future work in this research direction.

</details>


### [103] [OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics](https://arxiv.org/abs/2506.10481)
*Yaoming Zhu, Junxin Wang, Yiyang Li, Lin Qiu, ZongYu Wang, Jun Xu, Xuezhi Cao, Yuhuai Wei, Mingshi Wang, Xunliang Cai, Rong Ma*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个名为OIBench的新基准，它是一个高质量、私密且具有挑战性的信息学奥林匹克竞赛级别的数据集，用于评估和提升算法推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 随着模型变得越来越复杂，传统的算法基准测试逐渐饱和，这突显了需要更加具有挑战性的基准来指导未来算法推理的改进。

**方法:** 构建了一个包含250个精心策划的原创问题的数据集，并详细介绍了该基准的构建方法，确保能够全面评估各种编程范式和复杂性。通过实验展示了其抗污染特性，并提出了时间/空间完成曲线来进行更细致的效率分析。

**结果:** 实验表明开源模型落后于闭源模型，但当前最先进的模型在正确性和效率方面已经超过了大多数人，尽管与标准解决方案相比仍存在不足。

**结论:** 通过发布完全开源的OIBench（https://huggingface.co/datasets/AGI-Eval/OIBench），作者希望这个基准有助于推进未来大型语言模型的代码推理能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OIBench%3A+Benchmarking+Strong+Reasoning+Models+with+Olympiad+in+Informatics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10481，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10481&send_immediately=true&force_search=false)

**原文摘要:** As models become increasingly sophisticated, conventional algorithm
benchmarks are increasingly saturated, underscoring the need for more
challenging benchmarks to guide future improvements in algorithmic reasoning.
This paper introduces OIBench, a high-quality, private, and challenging
olympiad-level informatics dataset comprising 250 carefully curated original
problems. We detail the construction methodology of the benchmark, ensuring a
comprehensive assessment across various programming paradigms and complexities,
and we demonstrate its contamination-resistant properties via experiments. We
propose Time/Space Completion Curves for finer-grained efficiency analysis and
enable direct human-model comparisons through high-level participant
evaluations. Our experiments reveal that while open-source models lag behind
closed-source counterparts, current SOTA models already outperform most human
participants in both correctness and efficiency, while still being suboptimal
compared to the canonical solutions. By releasing OIBench as a fully
open-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we
hope this benchmark will contribute to advancing code reasoning capabilities
for future LLMs.

</details>


### [104] [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
*Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai*

**主要类别:** cs.AI

**AI概要:** 本文介绍了Scientists' First Exam (SFE)基准，用于评估多模态大型语言模型（MLLMs）在科学认知能力上的表现。实验表明当前先进的GPT-o3和InternVL-3模型在SFE上得分较低，表明MLLMs在科学领域还有很大的改进空间。


<details>
  <summary>更多</summary>
  
**动机:** 当前科学发现越来越依赖于基于信息密集型科学数据和特定领域专业知识的复杂多模态推理。尽管现有的科学基准可以评估MLLM的知识理解能力，但它们对于感知和推理能力的评价不足。为了填补这一空白，作者提出了一个新的基准SFE来全面评估MLLMs的科学认知能力。

**方法:** 设计了Scientists' First Exam (SFE) 基准，它包括830对专家验证的VQA问题，覆盖三个问题类型，并且跨五个高价值学科中的66个多模态任务。

**结果:** 广泛实验显示，当前最先进的GPT-o3和InternVL-3模型在SFE上的得分分别只有34.08%和26.52%，这表明MLLMs在科学领域中还有很大提升空间。

**结论:** SFE基准为评估MLLMs在科学信号感知、科学属性理解和科学比较推理方面提供了新的视角，揭示了现有模型在这些方面的不足，希望SFE能够促进AI增强科学发现的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scientists%27+First+Exam%3A+Probing+Cognitive+Abilities+of+MLLM+via+Perception%2C+Understanding%2C+and+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10521，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10521&send_immediately=true&force_search=false)

**原文摘要:** Scientific discoveries increasingly rely on complex multimodal reasoning
based on information-intensive scientific data and domain-specific expertise.
Empowered by expert-level scientific benchmarks, scientific Multimodal Large
Language Models (MLLMs) hold the potential to significantly enhance this
discovery process in realistic workflows. However, current scientific
benchmarks mostly focus on evaluating the knowledge understanding capabilities
of MLLMs, leading to an inadequate assessment of their perception and reasoning
abilities. To address this gap, we present the Scientists' First Exam (SFE)
benchmark, designed to evaluate the scientific cognitive capacities of MLLMs
through three interconnected levels: scientific signal perception, scientific
attribute understanding, scientific comparative reasoning. Specifically, SFE
comprises 830 expert-verified VQA pairs across three question types, spanning
66 multimodal tasks across five high-value disciplines. Extensive experiments
reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%
and 26.52% on SFE, highlighting significant room for MLLMs to improve in
scientific realms. We hope the insights obtained in SFE will facilitate further
developments in AI-enhanced scientific discoveries.

</details>


### [105] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/abs/2506.10527)
*Yanan Cai, Ahmed Salem, Besmira Nushi, Mark Russinovich*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个名为LogiPlan的新基准，用于评估大型语言模型在复杂关系结构上的逻辑规划和推理能力。该框架通过控制对象数量、关系数量以及关系链的最小深度来动态调整任务复杂度，并包含了三个互补任务：计划生成、一致性检测和比较问题。此外，还测试了模型自我纠正的能力。研究发现，尽管最近增强推理能力的模型在较简单的实例上表现良好，但在需要更深层次逻辑规划的复杂配置中仍面临挑战。


<details>
  <summary>更多</summary>
  
**动机:** 为了评估大型语言模型（LLMs）在处理像网络基础设施、知识库或业务流程模式等复杂关系图时的逻辑关系推理能力。

**方法:** 开发了LogiPlan基准，它允许通过改变对象数量、关系数量及关系链深度来调控任务难度，并且包含三项互补任务以全面考察模型性能。

**结果:** 实验证明，虽然当前先进的增强型推理模型能够在简单情形下取得较好成绩，但当面对需要深层逻辑规划的复杂情况时，则显得力不从心。

**结论:** 研究表明，在处理涉及复杂逻辑规划的问题时，即使是最新的先进模型也存在局限性，这表明有必要进一步改进模型架构和规模以提高其处理此类问题的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LogiPlan%3A+A+Structured+Benchmark+for+Logical+Planning+and+Relational+Reasoning+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10527，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10527&send_immediately=true&force_search=false)

**原文摘要:** We introduce LogiPlan, a novel benchmark designed to evaluate the
capabilities of large language models (LLMs) in logical planning and reasoning
over complex relational structures. Logical relational reasoning is important
for applications that may rely on LLMs to generate and query structured graphs
of relations such as network infrastructure, knowledge bases, or business
process schema. Our framework allows for dynamic variation of task complexity
by controlling the number of objects, relations, and the minimum depth of
relational chains, providing a fine-grained assessment of model performance
across difficulty levels. LogiPlan encompasses three complementary tasks: (1)
Plan Generation, where models must construct valid directed relational graphs
meeting specified structural constraints; (2) Consistency Detection, testing
models' ability to identify inconsistencies in relational structures; and (3)
Comparison Question, evaluating models' capacity to determine the validity of
queried relationships within a given graph. Additionally, we assess models'
self-correction capabilities by prompting them to verify and refine their
initial solutions. We evaluate state-of-the-art models including DeepSeek R1,
Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,
O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant
performance gaps that correlate with model scale and architecture. Our analysis
demonstrates that while recent reasoning-enhanced models show promising results
on simpler instances, they struggle with more complex configurations requiring
deeper logical planning.

</details>


### [106] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/abs/2506.10585)
*Mohd Anwar Jamal Faiz*

**主要类别:** cs.AI

**AI概要:** 本文提出了Primender序列，这是一种结合了经典素数规则与基于模数位条件的新型整数序列。该序列被提议作为评估大型语言模型（LLMs）符号推理能力的基准。研究动机是需要可解释、基于规则的测试平台来评估LLM推断隐藏规则、验证数学假设和大规模推广符号逻辑的能力。通过设计结构化提示和评估框架，对多个最先进的LLM进行测试，并使用一系列比较指标来评估模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于为大型语言模型（LLMs）提供一个可解释且基于规则的测试平台，以评估这些模型在推断隐藏规则、验证数学假设以及大规模推广符号逻辑方面的能力。

**方法:** 作者提出了一种新的整数序列Primender序列，该序列结合了经典的素性检测与基于模数位的条件。为了探究这一序列的性质，作者们设计了一个结构化的提示和评估框架，用于测试多个最先进LLM的表现。这些模型被要求识别底层规则、验证假设并生成序列的下10万个项。

**结果:** 研究结果表明，不同LLM在规则推断准确性、假设评估、序列有效性和符号解释质量等评价指标上的表现各异。

**结论:** 这项工作贡献了一种新的数学构造以及一种可重复的方法论，用于在符号推理、假设检验和模式推广方面对LLM进行基准测试，从而架起了数论、人工智能和软件工程之间的桥梁。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Primender+Sequence%3A+A+Novel+Mathematical+Construct+for+Testing+Symbolic+Inference+and+AI+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10585，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10585&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces the Primender sequence, a novel integer sequence
defined by a hybrid rule that combines classical primality with modular
digit-based conditions. Specifically, a number n is included in the sequence if
it is prime or ends with a prime number of unit digit or any length. In other
words, numbers which are primes or have at least one prime suffix. The
resulting sequence exhibits a deterministic yet non-trivial structure, blending
number-theoretic properties with symbolic patterning. We propose the Primender
sequence as a benchmark for evaluating the symbolic reasoning capabilities of
Large Language Models (LLMs). The study is motivated by the need for
interpretable, rule-based testbeds that can assess an LLM's ability to infer
hidden rules, validate mathematical hypotheses, and generalize symbolic logic
at scale. A key hypothesis explored is: Whenever a number in the Primender
sequence is exactly one more than the largest prime less than or equal to it,
the difference between it and the previous number in the sequence is also 1. We
design a structured prompt and evaluation framework to test this hypothesis
across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,
Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying
rule, validating the hypothesis, and generating the next 100,000 terms of the
sequence. Comparative metrics such as rule inference accuracy, hypothesis
evaluation, sequence validity, and symbolic explanation quality are used to
assess model performance. This work contributes a novel mathematical construct
and a reproducible methodology for benchmarking LLMs in symbolic reasoning,
hypothesis testing, and scalable pattern generalization - bridging the domains
of number theory, artificial intelligence, and software engineering.

</details>


### [107] [Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information](https://arxiv.org/abs/2506.10613)
*Henrik Sebastian Steude, Alexander Diedrich, Ingo Pill, Lukas Moddemann, Daniel Vranješ, Oliver Niggemann*

**主要类别:** cs.AI

**AI概要:** 提出了一种新的诊断方法，该方法仅需最少的先验知识，通过结合基于神经网络的症状生成器和新的图诊断算法来工作。实验表明，该方法在大多数情况下能够有效地识别出真实的因果组件，并缩小搜索空间。


<details>
  <summary>更多</summary>
  
**动机:** 复杂的网络物理系统诊断过程通常需要详细的系统模型或全面的训练数据，但获取这些信息是一个重大挑战。为了解决这个问题，研究者们提出了一种只需要极少先验知识的新诊断方法。

**方法:** 所提出的方法结合了基于神经网络的症状生成器（用于子系统级别的异常检测）与一种新型的图诊断算法（利用子系统间最小的因果关系信息）。

**结果:** 使用完全可控的模拟数据集进行的实验显示，在所有案例中，该方法有82%的概率将真实因果组件包含在其诊断集合内，并且在73%的情况下有效减少了搜索空间。此外，在实际的安全水处理数据集上的测试也展示了该方法在实际场景中的潜力。

**结论:** 结果表明，这种方法对于具有有限先验知识的大规模复杂网络物理系统的实用应用具有潜在价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Data+Driven+Diagnosis+for+Large+Cyber-Physical-Systems+with+Minimal+Prior+Information，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10613&send_immediately=true&force_search=false)

**原文摘要:** Diagnostic processes for complex cyber-physical systems often require
extensive prior knowledge in the form of detailed system models or
comprehensive training data. However, obtaining such information poses a
significant challenge. To address this issue, we present a new diagnostic
approach that operates with minimal prior knowledge, requiring only a basic
understanding of subsystem relationships and data from nominal operations. Our
method combines a neural network-based symptom generator, which employs
subsystem-level anomaly detection, with a new graph diagnosis algorithm that
leverages minimal causal relationship information between
subsystems-information that is typically available in practice. Our experiments
with fully controllable simulated datasets show that our method includes the
true causal component in its diagnosis set for 82 p.c. of all cases while
effectively reducing the search space in 73 p.c. of the scenarios. Additional
tests on the real-world Secure Water Treatment dataset showcase the approach's
potential for practical scenarios. Our results thus highlight our approach's
potential for practical applications with large and complex cyber-physical
systems where limited prior knowledge is available.

</details>


### [108] [TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving](https://arxiv.org/abs/2506.10674)
*Vincenzo Colle, Mohamed Sana, Nicola Piovesan, Antonio De Domenico, Fadhel Ayed, Merouane Debbah*

**主要类别:** cs.AI

**AI概要:** 本文介绍了TeleMath，这是一个专为评估大型语言模型在解决电信领域数学问题方面表现而设计的基准数据集。通过500个问答对覆盖了电信领域的广泛主题，并且研究发现专门针对数学或逻辑推理设计的模型在TeleMath上的表现优于通用模型。


<details>
  <summary>更多</summary>
  
**动机:** 尽管最近的进步提高了大型语言模型（LLMs）在一般数学推理中的性能，但在信号处理、网络优化和性能分析等专业领域内的有效性仍然很大程度上未被探索。为了填补这一空白，作者们提出了TeleMath，这是第一个专门设计来评估LLMs在电信领域解决具有数值解的数学问题性能的基准数据集。

**方法:** TeleMath数据集包含500个问答对，涵盖电信领域的广泛话题。论文概述了从由主题专家精心设计的问题种子开始的QnA生成流程。此外，还对一系列开源LLM进行了评估，以揭示哪些模型能够最好地解决这些特定于电信领域的数学问题。

**结果:** 评估结果显示，那些专门为数学或逻辑推理设计的最新模型在TeleMath上的表现最佳，而即便是参数数量庞大的通用模型也常常难以应对这些挑战。

**结论:** 本文的工作强调了对于专业领域内复杂任务，特别是数学密集型任务，需要专门训练的语言模型。TeleMath数据集以及评估代码已经被公开，以促进结果的可重复性和支持未来的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TeleMath%3A+A+Benchmark+for+Large+Language+Models+in+Telecom+Mathematical+Problem+Solving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10674，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10674&send_immediately=true&force_search=false)

**原文摘要:** The increasing adoption of artificial intelligence in telecommunications has
raised interest in the capability of Large Language Models (LLMs) to address
domain-specific, mathematically intensive tasks. Although recent advancements
have improved the performance of LLMs in general mathematical reasoning, their
effectiveness within specialized domains, such as signal processing, network
optimization, and performance analysis, remains largely unexplored. To address
this gap, we introduce TeleMath, the first benchmark dataset specifically
designed to evaluate LLM performance in solving mathematical problems with
numerical solutions in the telecommunications domain. Comprising 500
question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the
telecommunications field. This paper outlines the proposed QnAs generation
pipeline, starting from a selected seed of problems crafted by Subject Matter
Experts. The evaluation of a wide range of open-source LLMs reveals that best
performance on TeleMath is achieved by recent models explicitly designed for
mathematical or logical reasoning. In contrast, general-purpose models, even
those with a large number of parameters, often struggle with these challenges.
We have released the dataset and the evaluation code to ease result
reproducibility and support future research.

</details>


### [109] [Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL](https://arxiv.org/abs/2506.10678)
*Tom Westermann, Aljosha Köcher, Felix Gehlhoff*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种将AutomationML模型映射到OWL本体并通过大语言模型将文本规则转化为SHACL约束的方法，实现了对AML建模规则的半自动化检查。


<details>
  <summary>更多</summary>
  
**动机:** 现有的AutomationML建模建议通常是以非正式的文本形式表达，这些约束无法在AML本身内自动验证。

**方法:** 首先通过RML和SPARQL将AML模型映射到OWL本体，然后使用大语言模型将文本规则转换为SHACL约束，并针对先前生成的AML本体进行验证。最后，自动以自然语言解释SHACL验证结果。

**结果:** 该方法在一个样本AML建议上进行了演示，结果显示即使是复杂的建模规则也可以被半自动化地检查，而不需要用户理解形式化方法或本体技术。

**结论:** 这项工作展示了一种新的方式来实现AML建模规则的形式化和验证，简化了用户的负担，提高了验证过程的效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Validation+of+Textual+Constraints+Against+AutomationML+via+LLMs+and+SHACL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10678，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10678&send_immediately=true&force_search=false)

**原文摘要:** AutomationML (AML) enables standardized data exchange in engineering, yet
existing recommendations for proper AML modeling are typically formulated as
informal and textual constraints. These constraints cannot be validated
automatically within AML itself. This work-in-progress paper introduces a
pipeline to formalize and verify such constraints. First, AML models are mapped
to OWL ontologies via RML and SPARQL. In addition, a Large Language Model
translates textual rules into SHACL constraints, which are then validated
against the previously generated AML ontology. Finally, SHACL validation
results are automatically interpreted in natural language. The approach is
demonstrated on a sample AML recommendation. Results show that even complex
modeling rules can be semi-automatically checked -- without requiring users to
understand formal methods or ontology technologies.

</details>


### [110] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/abs/2506.10708)
*Michael Bartholomew, Joohyung Lee*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一个名为aspsmt2smt的编译器，它能够将ASPMT程序转化为SMT实例，利用gringo和z3工具来处理部分变量，并能有效处理实数计算以进行连续变化的推理。


<details>
  <summary>更多</summary>
  
**动机:** 为了结合回答集编程（ASP）与理论上的可满足性（SMT），并基于函数稳定模型语义，开发出一种方法使得紧致片段的ASPMT程序可以转换为SMT实例，从而允许使用SMT求解器来计算ASPMT程序的稳定模型。

**方法:** 开发了aspsmt2smt编译器，该编译器实现了将ASPMT程序翻译成SMT实例的功能。系统中使用了ASP grounder gringo来部分地对输入程序进行基础化处理，同时保留一些变量供z3 SMT求解器处理。

**结果:** 通过aspsmt2smt编译器，能够有效地处理涉及实数运算的连续变化推理问题。

**结论:** aspsmt2smt编译器成功地将ASPMT程序转换为SMT实例，并展示了其在处理实数计算和连续变化推理方面的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是System+ASPMT2SMT%3AComputing+ASPMT+Theories+by+SMT+Solvers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10708，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10708&send_immediately=true&force_search=false)

**原文摘要:** Answer Set Programming Modulo Theories (ASPMT) is an approach to combining
answer set programming and satisfiability modulo theories based on the
functional stable model semantics. It is shown that the tight fragment of ASPMT
programs can be turned into SMT instances, thereby allowing SMT solvers to
compute stable models of ASPMT programs. In this paper we present a compiler
called {\sc aspsmt2smt}, which implements this translation. The system uses ASP
grounder {\sc gringo} and SMT solver {\sc z3}. {\sc gringo} partially grounds
input programs while leaving some variables to be processed by {\sc z3}. We
demonstrate that the system can effectively handle real number computations for
reasoning about continuous changes.

</details>


### [111] [Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering](https://arxiv.org/abs/2506.10753)
*Adam Ishay, Zhun Yang, Joohyung Lee, Ilgu Kang, Dongjae Lim*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种增强神经-符号模型以进行反事实推理的方法，通过定义因果图来表示事件之间的因果关系，并使用答案集编程(ASP)协调感知和模拟模块。在CLEVRER和CRAFT基准测试中验证了该方法的有效性，特别是在CLEVRER挑战中取得了最先进的表现。


<details>
  <summary>更多</summary>
  
**动机:** 神经-符号模型结合了符号推理与基于神经网络的感知和预测，在处理视频动态中的因果和时间推理问题上展现出潜力，但它们在回答反事实问题方面存在局限性。

**方法:** 引入了一种增强神经-符号模型用于反事实推理的方法，利用事件之间因果关系的符号推理。定义了因果图的概念来表示这些关系，并采用声明性的逻辑编程方法——答案集编程(ASP)，来寻找如何协调感知和模拟模块的方式。对于CRAFT基准测试，还利用了大型预训练语言模型（如GPT-3.5和GPT-4）作为动力学模拟器的代理。

**结果:** 所提出的方法在两个基准测试CLEVRER和CRAFT上验证了有效性，尤其在CLEVRER挑战中达到了最先进的性能。此外，通过由符号因果推理指导的替代提示，该方法还能进一步提高其在反事实问题上的表现。

**结论:** 这项研究展示了一种新的方法，能够有效提升神经-符号模型在视频动态因果及反事实推理任务中的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Think+before+You+Simulate%3A+Symbolic+Reasoning+to+Orchestrate+Neural+Computation+for+Counterfactual+Question+Answering，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10753，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10753&send_immediately=true&force_search=false)

**原文摘要:** Causal and temporal reasoning about video dynamics is a challenging problem.
While neuro-symbolic models that combine symbolic reasoning with neural-based
perception and prediction have shown promise, they exhibit limitations,
especially in answering counterfactual questions. This paper introduces a
method to enhance a neuro-symbolic model for counterfactual reasoning,
leveraging symbolic reasoning about causal relations among events. We define
the notion of a causal graph to represent such relations and use Answer Set
Programming (ASP), a declarative logic programming method, to find how to
coordinate perception and simulation modules. We validate the effectiveness of
our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves
state-of-the-art performance on the CLEVRER challenge, significantly
outperforming existing models. In the case of the CRAFT benchmark, we leverage
a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a
dynamics simulator. Our findings show that this method can further improve its
performance on counterfactual questions by providing alternative prompts
instructed by symbolic causal reasoning.

</details>


### [112] [OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems](https://arxiv.org/abs/2506.10764)
*Xiaozhe Li, Jixuan Chen, Xinyu Fang, Shengyuan Ding, Haodong Duan, Qingwen Liu, Kai Chen*

**主要类别:** cs.AI

**AI概要:** 本文提出了OPT-BENCH，一个用于评估大型语言模型在大规模搜索空间优化问题上的基准测试。通过实验分析了不同迭代次数、温度设置和模型架构对解决方案质量和收敛性的影响，并发现历史反馈可以显著提高优化性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在解决各种任务方面表现出色，但它们通过从前馈中学习来迭代优化复杂解决方案的能力尚未得到充分探索。

**方法:** 创建了一个名为OPT-BENCH的综合基准测试，包含20个来自Kaggle的真实机器学习任务和10个经典的NP问题。同时开发了OPT-Agent框架，该框架模仿人类处理复杂问题时的推理过程，生成、验证并利用历史反馈逐步改进解决方案。

**结果:** 实验结果表明，在机器学习任务和NP问题上，结合历史上下文显著提高了优化性能。此外，还探讨了优化迭代次数、温度设置以及模型架构等因素如何影响解的质量和收敛速度。

**结论:** 本研究为推动LLM驱动的优化和迭代推理领域的进一步研究提供了宝贵的资源，所有数据集、代码和评估工具均已开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OPT-BENCH%3A+Evaluating+LLM+Agent+on+Large-Scale+Search+Spaces+Optimization+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10764，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10764&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have shown remarkable capabilities in solving
diverse tasks. However, their proficiency in iteratively optimizing complex
solutions through learning from previous feedback remains insufficiently
explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark
designed to evaluate LLM agents on large-scale search space optimization
problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from
Kaggle and 10 classical NP problems, offering a diverse and challenging
environment for assessing LLM agents on iterative reasoning and solution
refinement. To enable rigorous evaluation, we introduce OPT-Agent, an
end-to-end optimization framework that emulates human reasoning when tackling
complex problems by generating, validating, and iteratively improving solutions
through leveraging historical feedback. Through extensive experiments on 9
state-of-the-art LLMs from 6 model families, we analyze the effects of
optimization iterations, temperature settings, and model architectures on
solution quality and convergence. Our results demonstrate that incorporating
historical context significantly enhances optimization performance across both
ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to
promote further research in advancing LLM-driven optimization and iterative
reasoning. Project page:
\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.

</details>


### [113] [A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models](https://arxiv.org/abs/2506.10853)
*Yu Zhang, Yang Hu, De Wang*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种结合思维链推理和模型上下文协议的框架，以增强大型语言模型在时空行为模拟中的能力，并通过上海陆家嘴地区的实验验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 传统基于规则和统计的方法在计算成本、泛化能力和可扩展性方面存在局限性；而大型语言模型虽然有潜力作为“世界模拟器”，但在时空推理方面面临挑战，如空间认知有限、缺乏物理约束理解以及群体同质化倾向。

**方法:** 该研究引入了一个集成思维链（CoT）推理与模型上下文协议（MCP）的框架来提高LLMs在模仿符合验证数据模式的时空行为方面的能力。方法论结合了五阶段认知框架的人类式渐进推理与通过六个专业MCP工具类别进行的全面数据处理：时间管理、空间导航、环境感知、个人记忆、社会协作和经验评估。

**结果:** 在上海陆家嘴地区进行的实验中，通过对1,000个生成样本的测试，结果表明所提出的框架能够产生与真实移动信号数据高度相似的结果，不同基础模型下的生成质量得分在7.86至8.36之间。并行处理实验显示效率有所提高，当从2个进程扩展到12个进程时，每个样本的生成时间从1.30分钟减少到了0.17分钟。

**结论:** 这项工作为城市行为建模整合了CoT推理与MCP，推动了大型语言模型在城市计算中的应用，并为合成移动数据生成提供了一种实用的方法。该框架为智慧城市建设、交通预测及参与式城市设计应用提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Study+on+Individual+Spatiotemporal+Activity+Generation+Method+Using+MCP-Enhanced+Chain-of-Thought+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10853，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10853&send_immediately=true&force_search=false)

**原文摘要:** Human spatiotemporal behavior simulation is critical for urban planning
research, yet traditional rule-based and statistical approaches suffer from
high computational costs, limited generalizability, and poor scalability. While
large language models (LLMs) show promise as "world simulators," they face
challenges in spatiotemporal reasoning including limited spatial cognition,
lack of physical constraint understanding, and group homogenization tendencies.
This paper introduces a framework integrating chain-of-thought (CoT) reasoning
with Model Context Protocol (MCP) to enhance LLMs' capability in simulating
spatiotemporal behaviors that correspond with validation data patterns. The
methodology combines human-like progressive reasoning through a five-stage
cognitive framework with comprehensive data processing via six specialized MCP
tool categories: temporal management, spatial navigation, environmental
perception, personal memory, social collaboration, and experience evaluation.
Experiments in Shanghai's Lujiazui district validate the framework's
effectiveness across 1,000 generated samples. Results demonstrate high
similarity with real mobile signaling data, achieving generation quality scores
of 7.86 to 8.36 across different base models. Parallel processing experiments
show efficiency improvements, with generation times decreasing from 1.30 to
0.17 minutes per sample when scaling from 2 to 12 processes. This work
contributes to integrating CoT reasoning with MCP for urban behavior modeling,
advancing LLMs applications in urban computing and providing a practical
approach for synthetic mobility data generation. The framework offers a
foundation for smart city planning, transportation forecasting, and
participatory urban design applications.

</details>


### [114] [GenPlanX. Generation of Plans and Execution](https://arxiv.org/abs/2506.10897)
*Daniel Borrajo, Giuseppe Canonaco, Tomás de la Rosa, Alfredo Garrachón, Sriram Gopalakrishnan, Simerjot Kaur, Marianela Morales, Sunandita Patra, Alberto Pozanco, Keshav Ramani, Charese Smiley, Pietro Totis, Manuela Veloso*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为GenPlanX的系统，该系统结合了大型语言模型来理解用自然语言描述的规划任务，并与经典的人工智能规划引擎及执行和监控框架集成。通过辅助办公相关任务，展示了其简化工作流程和提高生产力的潜力。


<details>
  <summary>更多</summary>
  
**动机:** 传统的人工智能规划技术在处理复杂任务时无法理解使用自然语言给出的任务说明。随着大型语言模型（LLMs）的发展，它们在解读人类意图方面表现出了特别的能力。因此，存在一种需求，即开发一个能够将自然语言处理能力与经典AI规划相结合的系统，以促进人机之间的无缝协作。

**方法:** 研究者们开发了一个名为GenPlanX的系统，它整合了大型语言模型来解析自然语言形式的规划任务，并且与传统的AI规划引擎相连接。此外，GenPlanX还包含了一个执行和监控架构。

**结果:** GenPlanX被证明能够在帮助用户完成办公相关任务方面有效，这表明该系统有潜力通过简化的工作流程和增强的生产效率来改善人与AI的合作。

**结论:** GenPlanX通过将大型语言模型与经典AI规划引擎以及执行监控框架相结合，为用户提供了一种新的方式来进行基于自然语言的规划任务。这种集成方法有望在多种场景下提高工作效率和人机协作水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GenPlanX.+Generation+of+Plans+and+Execution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10897，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10897&send_immediately=true&force_search=false)

**原文摘要:** Classical AI Planning techniques generate sequences of actions for complex
tasks. However, they lack the ability to understand planning tasks when
provided using natural language. The advent of Large Language Models (LLMs) has
introduced novel capabilities in human-computer interaction. In the context of
planning tasks, LLMs have shown to be particularly good in interpreting human
intents among other uses. This paper introduces GenPlanX that integrates LLMs
for natural language-based description of planning tasks, with a classical AI
planning engine, alongside an execution and monitoring framework. We
demonstrate the efficacy of GenPlanX in assisting users with office-related
tasks, highlighting its potential to streamline workflows and enhance
productivity through seamless human-AI collaboration.

</details>


### [115] [Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?](https://arxiv.org/abs/2506.10912)
*Fei Lin, Ziyang Gong, Cong Wang, Yonglin Tian, Tengchao Zhang, Xue Yang, Gen Luo, Fei-Yue Wang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了ToxiMol，这是首个专注于分子毒性修复的基准任务，为多模态大语言模型而设计。它包括一个标准化数据集、机制感知的任务适应性提示注解流程以及自动评估框架ToxiEval。尽管当前的大规模语言模型在该任务上仍面临挑战，但已显示出在理解毒性、遵守语义约束和结构感知分子编辑方面有希望的能力。


<details>
  <summary>更多</summary>
  
**动机:** 由于分子毒性修复任务尚未被系统定义或基准化，导致早期药物开发失败的主要原因之一是毒性问题。为了填补这一空白，研究者们提出了新的基准任务ToxiMol。

**方法:** 研究者构建了一个包含11个主要任务和560个代表性有毒分子的数据集，并且设计了结合专家毒理学知识的提示注解流程。此外，他们还提出了一种名为ToxiEval的自动化评估框架，可以综合评价毒性终点预测、合成可达性、类药性和结构相似性。

**结果:** 实验结果表明，尽管现有大规模语言模型在此任务上仍然存在显著挑战，但它们已经开始展现出对毒性理解、语义约束遵循及结构意识分子编辑方面的潜力。

**结论:** 虽然目前的多模态大型语言模型在分子毒性修复任务中遇到了不少困难，但这些模型已经开始展示出对于毒性理解、满足语义约束条件以及进行结构敏感的分子编辑上的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Breaking+Bad+Molecules%3A+Are+MLLMs+Ready+for+Structure-Level+Molecular+Detoxification%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10912，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10912&send_immediately=true&force_search=false)

**原文摘要:** Toxicity remains a leading cause of early-stage drug development failure.
Despite advances in molecular design and property prediction, the task of
molecular toxicity repair - generating structurally valid molecular
alternatives with reduced toxicity - has not yet been systematically defined or
benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task
for general-purpose Multimodal Large Language Models (MLLMs) focused on
molecular toxicity repair. We construct a standardized dataset covering 11
primary tasks and 560 representative toxic molecules spanning diverse
mechanisms and granularities. We design a prompt annotation pipeline with
mechanism-aware and task-adaptive capabilities, informed by expert
toxicological knowledge. In parallel, we propose an automated evaluation
framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic
accessibility, drug-likeness, and structural similarity into a high-throughput
evaluation chain for repair success. We systematically assess nearly 30
mainstream general-purpose MLLMs and design multiple ablation studies to
analyze key factors such as evaluation criteria, candidate diversity, and
failure attribution. Experimental results show that although current MLLMs
still face significant challenges on this task, they begin to demonstrate
promising capabilities in toxicity understanding, semantic constraint
adherence, and structure-aware molecule editing.

</details>


### [116] [Spurious Rewards: Rethinking Training Signals in RLVR](https://arxiv.org/abs/2506.10947)
*Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, Luke Zettlemoyer*

**主要类别:** cs.AI

**AI概要:** 研究发现，即使在奖励信号与正确答案几乎没有或负相关的情况下，带有可验证奖励的强化学习（RLVR）仍能激发某些模型的强大数学推理能力。这种现象在Qwen2.5-Math-7B模型中尤为显著，但在其他模型家族如Llama3或OLMo2中并不适用。


<details>
  <summary>更多</summary>
  
**动机:** 研究人员想要探索在奖励信号不可靠甚至误导的情况下，强化学习能否仍然促进模型的数学推理能力。

**方法:** 通过实验对比了使用不同类型虚假奖励时，Qwen2.5-Math-7B与其他模型家族在MATH-500测试集上的表现差异。

**结果:** 结果显示，尽管奖励信号不可靠，但RLVR能够显著提高Qwen2.5-Math-7B模型在MATH-500上的得分；然而，相同的方法对其他模型家族效果不佳。此外，观察到Qwen2.5-Math-7B表现出更频繁地以代码形式进行推理的现象。

**结论:** 研究推测，在缺乏有用奖励信号的情况下，RLVR可能激活了预训练期间学到的有效推理表示。未来的研究需要考虑在更多样化的模型上验证RLVR的效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Spurious+Rewards%3A+Rethinking+Training+Signals+in+RLVR，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10947&send_immediately=true&force_search=false)

**原文摘要:** We show that reinforcement learning with verifiable rewards (RLVR) can elicit
strong mathematical reasoning in certain models even with spurious rewards that
have little, no, or even negative correlation with the correct answer. For
example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute
points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect
label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the
29.1% gained with ground truth rewards. However, the spurious rewards that work
for Qwen often fail to yield gains with other model families like Llama3 or
OLMo2. In particular, we find code reasoning -- thinking in code without actual
code execution -- to be a distinctive Qwen2.5-Math behavior that becomes
significantly more frequent after RLVR, from 65% to over 90%, even with
spurious rewards. Overall, we hypothesize that, given the lack of useful reward
signal, RLVR must somehow be surfacing useful reasoning representations learned
during pretraining, although the exact mechanism remains a topic for future
work. We suggest that future RLVR research should possibly be validated on
diverse models rather than a single de facto choice, as we show that it is easy
to get significant performance gains on Qwen models even with completely
spurious reward signals.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [117] [Fundamental Limits of Learning High-dimensional Simplices in Noisy Regimes](https://arxiv.org/abs/2506.10101)
*Seyed Amir Hossein Saberi, Amir Najafi, Abolfazl Motahari, Babak H. khalaj*

**主要类别:** stat.ML

**AI概要:** 本文探讨了在存在噪声的情况下，从高维单纯形中学习所需的样本复杂度，并提供了上界和下界的理论分析。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于理解在有噪声数据条件下，学习高维单纯形的样本复杂度界限，以及如何在给定信噪比时达到一定的准确度。

**方法:** 使用了样本压缩技术，并引入了一种基于傅里叶的方法来从含噪声的观测中恢复分布。

**结果:** 证明了当信噪比足够大时，噪声情况下的样本复杂度与无噪声情况相匹配。此外，还给出了在总变差距离误差为ε时估计单纯形所需样本数量的信息论下界。

**结论:** 结论是对于高信噪比的情况，噪声对单纯形学习样本复杂度的影响可以忽略不计；并且提出的新方法可能适用于单纯形学习之外的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fundamental+Limits+of+Learning+High-dimensional+Simplices+in+Noisy+Regimes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10101，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10101&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we establish sample complexity bounds for learning
high-dimensional simplices in $\mathbb{R}^K$ from noisy data. Specifically, we
consider $n$ i.i.d. samples uniformly drawn from an unknown simplex in
$\mathbb{R}^K$, each corrupted by additive Gaussian noise of unknown variance.
We prove an algorithm exists that, with high probability, outputs a simplex
within $\ell_2$ or total variation (TV) distance at most $\varepsilon$ from the
true simplex, provided $n \ge (K^2/\varepsilon^2)
e^{\mathcal{O}(K/\mathrm{SNR}^2)}$, where $\mathrm{SNR}$ is the signal-to-noise
ratio. Extending our prior work~\citep{saberi2023sample}, we derive new
information-theoretic lower bounds, showing that simplex estimation within TV
distance $\varepsilon$ requires at least $n \ge \Omega(K^3
\sigma^2/\varepsilon^2 + K/\varepsilon)$ samples, where $\sigma^2$ denotes the
noise variance. In the noiseless scenario, our lower bound $n \ge
\Omega(K/\varepsilon)$ matches known upper bounds up to constant factors. We
resolve an open question by demonstrating that when $\mathrm{SNR} \ge
\Omega(K^{1/2})$, noisy-case complexity aligns with the noiseless case. Our
analysis leverages sample compression techniques (Ashtiani et al., 2018) and
introduces a novel Fourier-based method for recovering distributions from noisy
observations, potentially applicable beyond simplex learning.

</details>


### [118] [Momentum Multi-Marginal Schrödinger Bridge Matching](https://arxiv.org/abs/2506.10168)
*Panagiotis Theodoropoulos, Augustinos D. Saravanos, Evangelos A. Theodorou, Guan-Horng Liu*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的匹配框架3MSBM，它通过将动力学提升到相空间并推广条件随机最优控制问题来学习满足多个位置约束的随机系统的平滑测度值样条。这使得3MSBM能够捕捉长距离时间依赖性，并且在现实世界应用中比现有方法表现出更好的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Bridge和Flow匹配框架依赖于相邻快照之间的成对插值，这限制了它们捕获长距离时间依赖性的能力，并可能影响推断轨迹的一致性。

**方法:** Momentum Multi-Marginal Schrödinger Bridge Matching (3MSBM)，一种新的匹配框架，它通过将动力学提升到相空间并形成多边际条件下的随机最优控制问题来学习满足多个位置约束的随机系统平滑测度值样条。

**结果:** 实验表明，与现有方法相比，3MSBM在捕捉具有时间依赖性的复杂动态方面表现更优，为多边际设置中的匹配框架训练开辟了新途径。

**结论:** 3MSBM作为一种新的匹配方法，在保持中间边缘分布的同时提高了收敛性和可扩展性，为处理复杂的、具有时间依赖性的系统提供了改进的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Momentum+Multi-Marginal+Schr%C3%B6dinger+Bridge+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10168，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10168&send_immediately=true&force_search=false)

**原文摘要:** Understanding complex systems by inferring trajectories from sparse sample
snapshots is a fundamental challenge in a wide range of domains, e.g.,
single-cell biology, meteorology, and economics. Despite advancements in Bridge
and Flow matching frameworks, current methodologies rely on pairwise
interpolation between adjacent snapshots. This hinders their ability to capture
long-range temporal dependencies and potentially affects the coherence of the
inferred trajectories. To address these issues, we introduce \textbf{Momentum
Multi-Marginal Schr\"odinger Bridge Matching (3MSBM)}, a novel matching
framework that learns smooth measure-valued splines for stochastic systems that
satisfy multiple positional constraints. This is achieved by lifting the
dynamics to phase space and generalizing stochastic bridges to be conditioned
on several points, forming a multi-marginal conditional stochastic optimal
control problem. The underlying dynamics are then learned by minimizing a
variational objective, having fixed the path induced by the multi-marginal
conditional bridge. As a matching approach, 3MSBM learns transport maps that
preserve intermediate marginals throughout training, significantly improving
convergence and scalability. Extensive experimentation in a series of
real-world applications validates the superior performance of 3MSBM compared to
existing methods in capturing complex dynamics with temporal dependencies,
opening new avenues for training matching frameworks in multi-marginal
settings.

</details>


### [119] [Distributionally-Constrained Adversaries in Online Learning](https://arxiv.org/abs/2506.10293)
*Moïse Blanchard, Samory Kpotufe*

**主要类别:** stat.ML

**AI概要:** 本文研究了在线学习中从对抗性到随机性设置的连续体，提出了受分布约束的对手这一更通用灵活的框架。文章给出了在该框架下可学习的分布类别的特征，并且表明对于一些自然函数类别，学习可以无需事先知道分布类别。


<details>
  <summary>更多</summary>
  
**动机:** 理解在线学习中从对抗性到随机性设置的连续体，以及在这两个极端之间如何实现非平凡遗憾的学习设定。

**方法:** 采用受分布约束的对手框架来分析哪些分布类别是可学习的，并且考虑了无意识和适应性对手两种情况。

**结果:** 得出了在给定框架下可学习的分布类别的特征，并展示了对于某些自然函数类别（如线性分类器），可以在没有先验知识的情况下进行学习。

**结论:** 通过对分布约束对手的研究，为理解函数类别与对手分布约束之间的相互作用提供了新的见解，并推广了已知平滑设置下的学习能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Distributionally-Constrained+Adversaries+in+Online+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10293，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10293&send_immediately=true&force_search=false)

**原文摘要:** There has been much recent interest in understanding the continuum from
adversarial to stochastic settings in online learning, with various frameworks
including smoothed settings proposed to bridge this gap. We consider the more
general and flexible framework of distributionally constrained adversaries in
which instances are drawn from distributions chosen by an adversary within some
constrained distribution class [RST11]. Compared to smoothed analysis, we
consider general distributional classes which allows for a fine-grained
understanding of learning settings between fully stochastic and fully
adversarial for which a learner can achieve non-trivial regret. We give a
characterization for which distribution classes are learnable in this context
against both oblivious and adaptive adversaries, providing insights into the
types of interplay between the function class and distributional constraints on
adversaries that enable learnability. In particular, our results recover and
generalize learnability for known smoothed settings. Further, we show that for
several natural function classes including linear classifiers, learning can be
achieved without any prior knowledge of the distribution class -- in other
words, a learner can simultaneously compete against any constrained adversary
within learnable distribution classes.

</details>


### [120] [Measuring Semantic Information Production in Generative Diffusion Models](https://arxiv.org/abs/2506.10433)
*Florian Handke, Félix Koulischer, Gabriel Raya, Luca Ambrogioni*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种基于信息论的方法，用于测量在生成过程中类语义'决策'的时间。通过估计给定噪声状态下的类别标签的条件熵，并利用条件熵的时间导数来确定噪声状态和类别标签之间信息传递最大的时间间隔。该方法在高斯混合模型和CIFAR10数据集上训练的DDPM模型中进行了演示，结果表明语义信息传递在扩散过程的中间阶段最高，而在最后阶段消失，且不同类别的熵率曲线存在显著差异。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望了解在图像生成过程中，语义和结构特征何时出现，以及这些特征如何与物理相变相关联。为了更精确地定位生成过程中类语义'决策'的具体时间点，研究引入了信息论方法。

**方法:** 采用在线公式计算最优贝叶斯分类器，并据此估计给定噪声状态下的类别标签条件熵。使用条件熵的时间导数来识别噪声状态和类别标签间信息传递最大值的时间间隔。

**结果:** 实验证明，在扩散过程的中间阶段，语义信息传递达到峰值，而在最终阶段则趋于消失。此外，不同类别的熵率曲线显示出不同的模式，表明不同'语义决策'发生在不同的中间时间点。

**结论:** 通过新提出的信息论方法，研究人员能够精确定位生成过程中语义信息的形成时间，这有助于更好地理解扩散模型中的语义演变过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+Semantic+Information+Production+in+Generative+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10433，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10433&send_immediately=true&force_search=false)

**原文摘要:** It is well known that semantic and structural features of the generated
images emerge at different times during the reverse dynamics of diffusion, a
phenomenon that has been connected to physical phase transitions in magnets and
other materials. In this paper, we introduce a general information-theoretic
approach to measure when these class-semantic "decisions" are made during the
generative process. By using an online formula for the optimal Bayesian
classifier, we estimate the conditional entropy of the class label given the
noisy state. We then determine the time intervals corresponding to the highest
information transfer between noisy states and class labels using the time
derivative of the conditional entropy. We demonstrate our method on
one-dimensional Gaussian mixture models and on DDPM models trained on the
CIFAR10 dataset. As expected, we find that the semantic information transfer is
highest in the intermediate stages of diffusion while vanishing during the
final stages. However, we found sizable differences between the entropy rate
profiles of different classes, suggesting that different "semantic decisions"
are located at different intermediate times.

</details>


### [121] [Box-Constrained Softmax Function and Its Application for Post-Hoc Calibration](https://arxiv.org/abs/2506.10572)
*Kyohei Atarashi, Satoshi Oyama, Hiromi Arai, Hisashi Kashima*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种新的Softmax函数的泛化版本——BCSoftmax，它能够在输出概率上施加硬性约束，如上下界限制，并且开发了该函数的有效计算算法。基于BCSoftmax，作者引入了两种后校准方法来缓解预测模型中的不自信和过度自信问题，从而提高决策任务的可靠性。实验表明，在几个常用数据集上，所提方法在校准度量方面有所改进。


<details>
  <summary>更多</summary>
  
**动机:** 现有的Softmax函数虽然可以通过温度参数进行软控制，但是无法对输出概率施加强制性的硬约束（例如边界约束），这在某些需要可靠和可信模型的应用中是非常关键的。

**方法:** 提出了一个名为BCSoftmax的新函数，它是传统Softmax函数的一个泛化，能够明确地对输出概率实施下限和上限的约束。同时，为了解决这个带有箱式约束的优化问题，还开发了一个精确而高效的计算算法。此外，基于BCSoftmax函数，研究者引入了两种后校准方法以调整模型训练后的输出概率或logits的界限。

**结果:** 通过TinyImageNet、CIFAR-100 和 20NewsGroups等数据集上的实验证明了所提出方法的有效性，特别是在改善模型校准度量方面表现突出。

**结论:** BCSoftmax函数及其相关的后校准方法能够有效地解决现有Softmax函数无法提供硬性约束的问题，有助于提升预测模型在校准方面的性能，对于增强下游决策任务的可靠性具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Box-Constrained+Softmax+Function+and+Its+Application+for+Post-Hoc+Calibration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10572，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10572&send_immediately=true&force_search=false)

**原文摘要:** Controlling the output probabilities of softmax-based models is a common
problem in modern machine learning. Although the $\mathrm{Softmax}$ function
provides soft control via its temperature parameter, it lacks the ability to
enforce hard constraints, such as box constraints, on output probabilities,
which can be critical in certain applications requiring reliable and
trustworthy models. In this work, we propose the box-constrained softmax
($\mathrm{BCSoftmax}$) function, a novel generalization of the
$\mathrm{Softmax}$ function that explicitly enforces lower and upper bounds on
output probabilities. While $\mathrm{BCSoftmax}$ is formulated as the solution
to a box-constrained optimization problem, we develop an exact and efficient
computation algorithm for $\mathrm{BCSoftmax}$. As a key application, we
introduce two post-hoc calibration methods based on $\mathrm{BCSoftmax}$. The
proposed methods mitigate underconfidence and overconfidence in predictive
models by learning the lower and upper bounds of the output probabilities or
logits after model training, thereby enhancing reliability in downstream
decision-making tasks. We demonstrate the effectiveness of our methods
experimentally using the TinyImageNet, CIFAR-100, and 20NewsGroups datasets,
achieving improvements in calibration metrics.

</details>


### [122] [Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning](https://arxiv.org/abs/2506.10664)
*Maxime Haddouche, Otmane Sakhi*

**主要类别:** stat.ML

**AI概要:** 本文研究了自适应离策略学习，扩展了PAC-Bayesian带对数平滑的学习框架到自适应场景，并展示了在允许中间策略部署的情况下，该方法的表现优于领先的离线方法。


<details>
  <summary>更多</summary>
  
**动机:** 本文的动机是探讨一种更实用和灵活的方法，即自适应离策略学习，在这种情况下，可以迭代地改进并重新部署策略以收集更高质量的数据。

**方法:** 利用在线PAC-Bayesian理论中的工具，将PAC-Bayesian带对数平滑（LS）的学习框架从静态设置扩展到了自适应场景。

**结果:** 通过经验评估证明，所提出的方法在静态设置下与领先的离线方法表现相当，而在允许中间策略部署的情况下则显著优于这些方法。

**结论:** 调整后的LS估计器自然适应多轮部署，并且在温和条件下提供更快的收敛速度，这表明自适应数据收集的优势以及PAC-Bayesian公式的强大之处。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Logarithmic+Smoothing+for+Adaptive+PAC-Bayesian+Off-Policy+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10664，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10664&send_immediately=true&force_search=false)

**原文摘要:** Off-policy learning serves as the primary framework for learning optimal
policies from logged interactions collected under a static behavior policy. In
this work, we investigate the more practical and flexible setting of adaptive
off-policy learning, where policies are iteratively refined and re-deployed to
collect higher-quality data. Building on the success of PAC-Bayesian learning
with Logarithmic Smoothing (LS) in static settings, we extend this framework to
the adaptive scenario using tools from online PAC-Bayesian theory. Furthermore,
we demonstrate that a principled adjustment to the LS estimator naturally
accommodates multiple rounds of deployment and yields faster convergence rates
under mild conditions. Our method matches the performance of leading offline
approaches in static settings, and significantly outperforms them when
intermediate policy deployments are allowed. Empirical evaluations across
diverse scenarios highlight both the advantages of adaptive data collection and
the strength of the PAC-Bayesian formulation.

</details>


### [123] [Practical Improvements of A/B Testing with Off-Policy Estimation](https://arxiv.org/abs/2506.10677)
*Sakhi Otmane, Gilotte Alexandre, Rohde David*

**主要类别:** stat.ML

**AI概要:** 本文提出了一个方差更小的无偏离策略估计器族，用于A/B测试中比传统均值差异估计器更有效地评估系统改进。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在改善A/B测试中常用的均值差异估计器，该估计器尽管无偏但可以被优化以减少方差。

**方法:** 引入了一族新的无偏估计器，并从中确定了具有最低方差的估计器。新方法简单且当测试的两个系统相似时能显著降低方差。

**结果:** 理论分析和实验结果都验证了所提方法的有效性和实用性。

**结论:** 新提出的估计器在A/B测试中能够提供更加准确的效果测量，尤其是在待比较系统之间存在相似性的情况下。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Practical+Improvements+of+A%2FB+Testing+with+Off-Policy+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10677&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of A/B testing, a widely used protocol for evaluating
the potential improvement achieved by a new decision system compared to a
baseline. This protocol segments the population into two subgroups, each
exposed to a version of the system and estimates the improvement as the
difference between the measured effects. In this work, we demonstrate that the
commonly used difference-in-means estimator, while unbiased, can be improved.
We introduce a family of unbiased off-policy estimators that achieves lower
variance than the standard approach. Among this family, we identify the
estimator with the lowest variance. The resulting estimator is simple, and
offers substantial variance reduction when the two tested systems exhibit
similarities. Our theoretical analysis and experimental results validate the
effectiveness and practicality of the proposed method.

</details>


### [124] [Demystifying Spectral Feature Learning for Instrumental Variable Regression](https://arxiv.org/abs/2506.10899)
*Dimitri Meunier, Antoine Moulin, Jakub Wornbard, Vladimir R. Kostic, Arthur Gretton*

**主要类别:** stat.ML

**AI概要:** 本文研究了在存在隐藏混淆变量的情况下，使用非参数工具变量回归进行因果效应估计的问题。通过谱特征来解决这个问题，并给出了基于谱特征的两阶段最小二quares估计器的一般化误差界。研究结果表明方法的表现取决于两个关键因素：谱对齐和算子的特征值衰减速率，并据此提出了表现良好的、一般的和较差的情景分类。


<details>
  <summary>更多</summary>
  
**动机:** 本文旨在解决当存在隐藏混淆变量时如何准确估计因果效应的问题。

**方法:** 采用非参数工具变量回归，并利用谱特征，即学习到的特征跨越了将处理与工具变量联系起来的算子的顶级特征子空间。

**结果:** 得到了一个基于谱特征的两阶段最小二乘估计器的一般化误差界，并且发现了方法性能的关键决定因素是谱对齐和条件算子的特征值衰减速度。

**结论:** 该方法在强谱对齐和缓慢特征值衰减（强工具变量）的情况下表现最佳；在谱对齐强但特征值快速衰减（弱工具变量）的情况下需要更多样本才能有效学习特征；而在谱对齐弱的情况下，无论特征值特性如何，方法都会失败。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying+Spectral+Feature+Learning+for+Instrumental+Variable+Regression，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10899&send_immediately=true&force_search=false)

**原文摘要:** We address the problem of causal effect estimation in the presence of hidden
confounders, using nonparametric instrumental variable (IV) regression. A
leading strategy employs spectral features - that is, learned features spanning
the top eigensubspaces of the operator linking treatments to instruments. We
derive a generalization error bound for a two-stage least squares estimator
based on spectral features, and gain insights into the method's performance and
failure modes. We show that performance depends on two key factors, leading to
a clear taxonomy of outcomes. In a good scenario, the approach is optimal. This
occurs with strong spectral alignment, meaning the structural function is
well-represented by the top eigenfunctions of the conditional operator, coupled
with this operator's slow eigenvalue decay, indicating a strong instrument.
Performance degrades in a bad scenario: spectral alignment remains strong, but
rapid eigenvalue decay (indicating a weaker instrument) demands significantly
more samples for effective feature learning. Finally, in the ugly scenario,
weak spectral alignment causes the method to fail, regardless of the
eigenvalues' characteristics. Our synthetic experiments empirically validate
this taxonomy.

</details>


### [125] [Probably Approximately Correct Labels](https://arxiv.org/abs/2506.10908)
*Emmanuel J. Candès, Andrew Ilyas, Tijana Zrnic*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种方法，利用预训练模型的AI预测来补充专家标签，从而更经济地构建带有高质量标签的数据集。


<details>
  <summary>更多</summary>
  
**动机:** 获取高质量标记数据集通常成本高昂，需要大量的人工注释或昂贵的实验。

**方法:** 通过结合预训练模型的AI预测和专家标签来创建可能是近似正确的标签，并保证总体标签错误率较低。

**结果:** 该方法在文本、图像以及蛋白质折叠分析中展示了其有效性。

**结论:** 这种方法允许使用现代AI模型进行严格而有效的数据集整理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probably+Approximately+Correct+Labels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10908，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10908&send_immediately=true&force_search=false)

**原文摘要:** Obtaining high-quality labeled datasets is often costly, requiring either
extensive human annotation or expensive experiments. We propose a method that
supplements such "expert" labels with AI predictions from pre-trained models to
construct labeled datasets more cost-effectively. Our approach results in
probably approximately correct labels: with high probability, the overall
labeling error is small. This solution enables rigorous yet efficient dataset
curation using modern AI models. We demonstrate the benefits of the methodology
through text annotation with large language models, image labeling with
pre-trained vision models, and protein folding analysis with AlphaFold.

</details>


### [126] [What Exactly Does Guidance Do in Masked Discrete Diffusion Models](https://arxiv.org/abs/2506.10971)
*He Ye, Rojas Kevin, Tao Molei*

**主要类别:** stat.ML

**AI概要:** 本文研究了带有无分类器指导的掩码离散扩散模型，推导出引导反向动力学的显式解，并揭示了在1D和2D中不同的行为。研究表明强指导会加速总变差的双指数衰减，并且影响采样分布的协方差结构。


<details>
  <summary>更多</summary>
  
**动机:** 为了精确地描述指导如何影响采样行为，以及理解在特定类别的采样过程中，指导对样本分布的影响。

**方法:** 通过假设没有分数误差也没有离散化误差，研究者们为带指导的反向动力学导出了一个显式的解，并分析了不同维度下的采样行为及总变差随指导强度的变化。

**结果:** 发现当目标是采样自特定类别时，指导增强了类特定区域同时抑制了与其他类别共享的区域；对于大的指导强度w，在1D和2D情况下，总变差沿反向动力学的衰减速率与w呈双指数关系。

**结论:** 指导不仅塑造了输出分布，还控制了采样轨迹的动力学，这些理论结果得到了实验的支持，展示了指导对几何效应及其收敛性的影响。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是What+Exactly+Does+Guidance+Do+in+Masked+Discrete+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.10971，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.10971&send_immediately=true&force_search=false)

**原文摘要:** We study masked discrete diffusion models with classifier-free guidance
(CFG). Assuming no score error nor discretization error, we derive an explicit
solution to the guided reverse dynamics, so that how guidance influences the
sampling behavior can be precisely characterized. When the full data
distribution is a mixture over classes and the goal is to sample from a
specific class, guidance amplifies class-specific regions while suppresses
regions shared with other classes. This effect depends on the guidance strength
$w$ and induces distinct covariance structures in the sampled distribution.
Notably, we observe quantitatively different behaviors in $1$D and $2$D. We
also show that for large $w$, the decay rate of the total variation
($\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for
both $1$D and $2$D. These findings highlight the role of guidance, not just in
shaping the output distribution, but also in controlling the dynamics of the
sampling trajectory. Our theoretical analysis is supported by experiments that
illustrate the geometric effects of guidance and its impact on convergence.

</details>
