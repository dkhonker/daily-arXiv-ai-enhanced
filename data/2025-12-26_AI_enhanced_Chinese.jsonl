{"id": "2512.20623", "pdf": "https://arxiv.org/pdf/2512.20623", "abs": "https://arxiv.org/abs/2512.20623", "authors": ["Ravi Gupta", "Shabista Haider"], "title": "BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization", "categories": ["cs.AI"], "comment": "Presented as poster in IPCCC 2025 at Austin", "summary": "Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.", "AI": {"tldr": "BitRL-Light是一个结合1位量化大语言模型和深度Q网络强化学习的智能家居照明系统，可在树莓派等边缘设备上实现实时控制，相比全精度模型节能71.4倍，相比规则系统节能32%，延迟低于200ms，用户满意度达95%。", "motivation": "智能家居照明系统消耗住宅能源的15-20%，但缺乏同时优化用户舒适度和能源效率的自适应智能。", "method": "采用1位量化Llama-3.2-1B模型与DQN强化学习结合的多目标优化方法，通过Google Home/IFTTT集成处理自然语言命令，并从手动覆盖中学习隐式反馈。", "result": "在树莓派4上实现200ms以下推理延迟，相比2位量化模型在ARM处理器上提速5.07倍，保持92%任务准确率，节能效果显著。", "conclusion": "该研究为在资源受限的IoT设备上部署自适应AI建立了实用框架，实现了无需云依赖的智能家居自动化。"}}
{"id": "2512.20624", "pdf": "https://arxiv.org/pdf/2512.20624", "abs": "https://arxiv.org/abs/2512.20624", "authors": ["Mazyar Taghavi", "Javad Vahidi"], "title": "Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment", "categories": ["cs.AI", "math.OC"], "comment": "59 pages", "summary": "This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.", "AI": {"tldr": "量子启发多智能体强化学习框架用于优化6G无人机网络部署中的探索-利用权衡，结合量子优化技术和经典MARL算法，在部分可观测动态环境中提升信号覆盖和网络扩展效率", "motivation": "解决多无人机在6G网络部署中面临的探索与利用权衡挑战，需要在部分可观测和动态环境下实现自主协调以最大化信号覆盖", "method": "集成经典MARL算法与量子启发优化技术，使用变分量子电路(VQC)和量子近似优化算法(QAOA)，结合贝叶斯推理、高斯过程和变分推理的概率建模，采用集中训练分散执行(CTDE)范式", "result": "实验表明该框架提高了样本效率、加速收敛、增强覆盖性能并保持鲁棒性，在探索与利用平衡方面优于传统PPO和DDPG方法", "conclusion": "量子启发MARL框架在6G无人机网络部署中展现出优越性能，为多智能体强化学习提供了新的优化方向，所有代码开源确保可复现性"}}
{"id": "2512.20626", "pdf": "https://arxiv.org/pdf/2512.20626", "abs": "https://arxiv.org/abs/2512.20626", "authors": ["Chi-Hsiang Hsiao", "Yi-Cheng Wang", "Tzung-Sheng Lin", "Yi-Ren Yeh", "Chu-Song Chen"], "title": "MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.", "AI": {"tldr": "该论文提出了一种基于多模态知识图谱的检索增强生成方法，通过整合视觉线索来提升对多模态内容的理解和推理能力，在问答任务中优于现有方法。", "motivation": "现有基于知识图谱的RAG方法仅限于文本输入，无法利用视觉等其他模态的互补信息，而视觉文档推理需要文本、视觉和空间线索来构建结构化层次概念。", "method": "提出多模态知识图谱RAG方法，在知识图谱构建、检索阶段和答案生成过程中都融入了视觉线索，支持跨模态推理。", "result": "在全局和细粒度问答任务上的实验结果表明，该方法在文本和多模态语料库上都持续优于现有的基于RAG的方法。", "conclusion": "通过整合视觉信息到知识图谱RAG框架中，能够有效提升对多模态内容的理解和推理能力，为复杂文档的理解提供了更好的解决方案。"}}
{"id": "2512.20628", "pdf": "https://arxiv.org/pdf/2512.20628", "abs": "https://arxiv.org/abs/2512.20628", "authors": ["Edited by Tessai Hayama", "Takayuki Ito", "Takahiro Uchiya", "Motoki Miura", "Takahiro Kawaji", "Takaya Yuizono", "Atsuo Yoshitaka", "Tokuro Matsuo", "Shun Okuhara", "Jawad Haqbeen", "Sofia Sahab", "Wen Gu", "Shiyao Ding"], "title": "Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)", "categories": ["cs.AI"], "comment": "Conference proceedings; 325 pages; published in cooperation with IEICE Proceedings Series. A subset of papers will appear in IEICE Transactions on Information and Systems (special section). Venue: Aore Nagaoka, Japan, December 3-5, 2025. Editors: KICSS 2025 Organizing Committee", "summary": "This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.", "AI": {"error": "'NoneType' object has no attribute 'model_dump'"}}
{"id": "2512.20638", "pdf": "https://arxiv.org/pdf/2512.20638", "abs": "https://arxiv.org/abs/2512.20638", "authors": ["Matyas Bohacek", "Nino Scherrer", "Nicholas Dufour", "Thomas Leung", "Christoph Bregler", "Stephanie C. Y. Chan"], "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak (\"model gaps\") and (ii) imbalanced coverage in the benchmarks themselves (\"benchmark gaps\"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.", "AI": {"tldr": "提出使用稀疏自编码器(SAE)自动发现大语言模型在特定概念上的弱点(模型差距)和基准测试覆盖不平衡问题(基准差距)的新方法", "motivation": "现有标准化基准测试的聚合指标会掩盖模型在特定子领域的弱点以及基准测试本身的不平衡覆盖问题", "method": "通过提取SAE概念激活并计算基准数据上的显著性加权性能分数，将评估基于模型内部表示并实现跨基准比较", "result": "发现开源模型在反对谄媚行为和安全讨论相关概念上表现不佳，同时基准测试过度代表服从、权威相关概念而遗漏核心概念", "conclusion": "该方法提供基于表示的评估方法，能够进行概念级分数分解，揭示模型得分原因和基准改进方向，是对传统聚合指标的补充"}}
{"id": "2512.20630", "pdf": "https://arxiv.org/pdf/2512.20630", "abs": "https://arxiv.org/abs/2512.20630", "authors": ["Aayam Bansal", "Ishaan Gangwani"], "title": "MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data", "categories": ["cs.AI"], "comment": "ICML NewInML", "summary": "Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.", "AI": {"tldr": "Microprobe是一种新颖的基础模型可靠性评估方法，仅需100个策略性选择的探测样本即可实现全面评估，相比传统方法显著降低了计算成本和时间需求。", "motivation": "传统基础模型可靠性评估需要数千个评估样本，计算成本高且耗时，难以满足现实世界部署的需求，因此需要开发更高效的评估方法。", "method": "结合五个关键可靠性维度的策略性提示多样性、先进的不确定性量化和自适应加权，通过精心选择的100个探测样本来有效检测潜在故障模式。", "result": "在多个语言模型和跨领域验证中，microprobe相比随机采样基线实现了23.5%更高的综合可靠性分数，具有显著的统计显著性(p < 0.001)，同时减少了90%的评估成本并保持95%的传统方法覆盖率。", "conclusion": "Microprobe为解决负责任AI部署中的高效模型评估关键缺口提供了有效解决方案，得到了AI安全专家的高度认可，具有重要的实际应用价值。"}}
{"id": "2512.20724", "pdf": "https://arxiv.org/pdf/2512.20724", "abs": "https://arxiv.org/abs/2512.20724", "authors": ["Alexandros Christoforos", "Chadbourne Davis"], "title": "SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention", "categories": ["cs.CL", "cs.AI"], "comment": "Under submission", "summary": "Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.", "AI": {"tldr": "SA-DiffuSeq是一个基于扩散模型的长文本生成框架，通过集成稀疏注意力机制显著提升计算效率，在保持语义连贯性的同时大幅降低长文档建模的计算成本和内存开销。", "motivation": "传统的扩散模型在生成长文本时面临计算成本过高和内存开销巨大的问题，随着序列长度增加变得难以扩展。", "method": "提出SA-DiffuSeq框架，集成稀疏注意力机制，选择性分配注意力，并设计针对稀疏注意力动态的软吸收状态来稳定扩散轨迹和加速序列重建。", "result": "实验表明SA-DiffuSeq在训练效率和采样速度上均超越现有扩散基线模型，尤其在长序列上表现突出，适用于科学写作、大规模代码生成和长上下文对话等应用。", "conclusion": "将结构化稀疏性融入扩散模型是实现高效且表达力强的长文本生成的有前景方向。"}}
{"id": "2512.20632", "pdf": "https://arxiv.org/pdf/2512.20632", "abs": "https://arxiv.org/abs/2512.20632", "authors": ["Jianbing Ma", "Ao Feng", "Zhenjie Gao", "Xinyu Song", "Li Su", "Bin Chen", "Wei Wang", "Jiamin Wu"], "title": "Erkang-Diagnosis-1.1 Technical Report", "categories": ["cs.AI"], "comment": "9 pages; 4 figures", "summary": "This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.", "AI": {"tldr": "Erkang-Diagnosis-1.1是基于阿里通义千问模型开发的AI医疗咨询助手，集成了500GB高质量医疗知识，采用增强预训练和检索增强生成的混合方法，在医疗考试中表现优于GPT-4", "motivation": "开发一个安全、可靠且专业的AI健康顾问，为用户提供准确的症状分析和诊断建议，赋能基层医疗和健康管理", "method": "使用阿里Qwen-3模型，集成500GB结构化医疗知识，采用增强预训练和检索增强生成的混合方法，通过3-5轮高效交互理解用户症状", "result": "模型能够准确理解用户症状，进行初步分析并提供有价值的诊断建议和健康指导，在综合医疗考试中领先GPT-4", "conclusion": "Erkang-Diagnosis-1.1成功开发为一个智能健康伴侣，在医疗咨询领域展现出优异性能，为基层医疗提供了有效的AI支持工具"}}
{"id": "2512.20757", "pdf": "https://arxiv.org/pdf/2512.20757", "abs": "https://arxiv.org/abs/2512.20757", "authors": ["Gül Sena Altıntaş", "Malikeh Ehghaghi", "Brian Lester", "Fengyuan Liu", "Wanru Zhao", "Marco Ciccone", "Colin Raffel"], "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.", "AI": {"tldr": "TokSuite是一个研究工具集和基准测试，通过训练14个使用不同分词器但其他条件完全相同的模型，来分离和评估分词器对语言模型性能的影响。", "motivation": "分词器是语言模型处理文本的基础，但其对模型性能的具体影响难以单独衡量，需要系统性的研究工具。", "method": "训练14个模型，使用相同架构、数据集、训练预算和初始化，仅分词器不同；创建专门基准测试，测量模型在真实扰动下的表现。", "result": "TokSuite能够稳健地分离分词器的影响，支持对多种流行分词器优缺点的新发现。", "conclusion": "该研究提供了系统评估分词器影响的方法和工具，有助于深入理解分词器在语言模型中的作用机制。"}}
{"id": "2512.20647", "pdf": "https://arxiv.org/pdf/2512.20647", "abs": "https://arxiv.org/abs/2512.20647", "authors": ["Leo Lu", "Jonathan Zhang", "Sean Chua", "Spencer Kim", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning", "categories": ["cs.AI"], "comment": "NeurIPS 2025 Workshop on Socially Responsible and Trustworthy Foundation Models (ResponsibleFM)", "summary": "Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.", "AI": {"tldr": "本研究探索不同大语言模型之间推理链的互换性，发现一个模型的部分推理链可以被其他模型可靠地继续完成，且在某些情况下还能提高最终准确性和逻辑结构。", "motivation": "虽然现有研究关注通过内部推理策略提升模型性能，但对不同模型间推理的互换性了解甚少，本研究旨在探索推理链在不同模型间的可转移性。", "method": "使用token级对数概率阈值在不同阶段截断Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct的推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行继续实验，评估框架结合过程奖励模型(PRM)来评估推理稳定性。", "result": "评估显示混合推理链通常能保持甚至有时提高最终准确性和逻辑结构，表明推理模型具有互换性的行为特性。", "conclusion": "推理互换性是推理模型的新兴行为特性，为协作AI系统中可靠的模块化推理提供了新范式。"}}
{"id": "2512.20773", "pdf": "https://arxiv.org/pdf/2512.20773", "abs": "https://arxiv.org/abs/2512.20773", "authors": ["Ziyi Zhu", "Olivier Tieleman", "Caitlin A. Stamatis", "Luka Smyth", "Thomas D. Hull", "Daniel R. Cahn", "Matteo Malgaroli"], "title": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.", "AI": {"tldr": "本文提出了一种对抗训练框架，通过生成器（用户模拟器）和判别器之间的竞争动态，迭代提升用户模拟器的真实性，特别针对心理健康支持聊天机器人领域，显著提升了系统问题暴露能力。", "motivation": "现实用户模拟对训练和评估任务导向对话系统至关重要，但创建能准确复制人类行为的模拟器仍然具有挑战性。有效的模拟器需要能够暴露被评估系统的故障模式。", "method": "采用对抗训练框架，包含生成器（用户模拟器）和判别器的竞争机制，在心理健康支持聊天机器人领域应用，通过迭代训练提升模拟器性能。", "result": "微调后的模拟器在暴露系统问题方面显著优于零样本基础模型，对抗训练进一步增强了多样性、分布对齐和预测有效性。模拟失败率与真实失败率呈现强相关性，同时保持低分布差异。判别器准确率在三次对抗迭代后大幅下降，表明模拟器真实性提升。", "conclusion": "对抗训练是创建真实用户模拟器的有前景方法，可在部署前实现快速、可靠且经济高效的系统评估，特别适用于心理健康支持领域的任务导向对话系统。"}}
{"id": "2512.20649", "pdf": "https://arxiv.org/pdf/2512.20649", "abs": "https://arxiv.org/abs/2512.20649", "authors": ["Zixun Luo", "Yuhang Fan", "Yufei Li", "Youzhi Zhang", "Hengyu Lin", "Ziqi Wang"], "title": "AIAuditTrack: A Framework for AI Security system", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.", "AI": {"tldr": "AiAuditTrack (AAT) 是一个基于区块链的AI使用流量记录与治理框架，利用去中心化身份和可验证凭证建立可信AI实体，通过风险扩散算法追踪风险行为源头，为多智能体环境提供可扩展的审计和风险管理解决方案。", "motivation": "AI大语言模型应用的快速扩张导致AI交互数据激增，带来了安全、问责和风险溯源方面的紧迫挑战，需要建立可信的AI使用监督机制。", "method": "采用区块链技术，结合去中心化身份(DID)和可验证凭证(VC)建立可信AI实体识别，将AI实体建模为动态交互图中的节点，记录实体间交互轨迹，并提出风险扩散算法追踪风险行为。", "result": "系统通过区块链TPS指标评估性能，证明AAT在大规模交互记录下的可行性和稳定性，能够有效实现跨系统监督和审计。", "conclusion": "AAT为复杂多智能体环境中的AI审计、风险管理和责任归属提供了一个可扩展且可验证的解决方案，解决了AI交互的安全和问责挑战。"}}
{"id": "2512.20780", "pdf": "https://arxiv.org/pdf/2512.20780", "abs": "https://arxiv.org/abs/2512.20780", "authors": ["Ramatu Oiza Abdulsalam", "Segun Aroyehun"], "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.", "AI": {"tldr": "大型语言模型在数学辅导中达到接近专家水平的教学感知质量，但在教学策略和语言特征上存在系统性差异，特别是在复述和转述策略使用不足，同时产生更长、词汇更丰富、更礼貌的回应。", "motivation": "探索大型语言模型在数学辅导中的教学行为与人类专家实践的契合程度，通过对比专家人类导师、新手人类导师和多个大语言模型对相同数学辅导对话的回应。", "method": "采用受控的逐轮比较方法，分析专家人类导师、新手人类导师和多个大语言模型对相同数学补救对话轮次的回应，考察教学策略和语言特征，包括复述转述、要求准确性、词汇多样性、可读性、礼貌性和主动性。", "result": "大语言模型在教学感知质量上接近专家水平，但系统性差异明显：较少使用专家人类导师特有的复述转述策略，产生更长、词汇更多样、更礼貌的回应。统计分析显示复述转述、词汇多样性和要求准确性对教学感知质量有正向影响，而高水平的主动性和礼貌语言则有负向影响。", "conclusion": "近期大语言模型在教学感知质量上可与专家人类导师相媲美，但依赖不同的教学和语言策略。这些发现强调了在评估人类导师和智能辅导系统的辅导回应时分析教学策略和语言特征的重要性。"}}
{"id": "2512.20650", "pdf": "https://arxiv.org/pdf/2512.20650", "abs": "https://arxiv.org/abs/2512.20650", "authors": ["Esmail Gumaan"], "title": "Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA", "categories": ["cs.AI"], "comment": "5 pages", "summary": "The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.", "AI": {"tldr": "提出MoAS架构，通过动态路由为每个token选择最优注意力机制（MHA、GQA或MQA），在保持模型性能的同时提升推理效率。", "motivation": "解决Transformer模型中注意力机制在建模质量与推理效率之间的权衡问题，MHA质量最好但KV缓存内存需求大，MQA/GQA内存效率高但性能下降。", "method": "使用学习型路由器为每个token动态选择最优注意力方案（MHA、GQA或MQA），而非静态混合方案。", "result": "在WikiText-2上验证，动态路由（val loss 2.3074）优于静态混合（2.3093），性能与MHA基线竞争，同时具备条件计算效率潜力。", "conclusion": "MoAS通过动态注意力方案选择有效平衡了模型性能与推理效率，为Transformer架构优化提供了新思路。"}}
{"id": "2512.20794", "pdf": "https://arxiv.org/pdf/2512.20794", "abs": "https://arxiv.org/abs/2512.20794", "authors": ["Shariqah Hossain", "Lalana Kagal"], "title": "Investigating Model Editing for Unlearning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.", "AI": {"tldr": "本研究探讨将模型编辑算法（ROME、IKE、WISE）应用于机器遗忘任务，通过设计新的编辑目标，在某些设置下遗忘质量优于传统基线方法，但仍面临整体模型性能受损的挑战。", "motivation": "现有机器遗忘方法对大型语言模型效率低下，要么无法完全移除目标信息，要么会损害应保留知识的性能；而模型编辑算法虽解决类似问题但侧重于信息重定向而非完全移除。", "method": "探索ROME、IKE和WISE三种模型编辑算法，为遗忘场景设计新的编辑目标，并将其应用于机器遗忘任务。", "result": "模型编辑方法在特定设置下的遗忘质量超过基线遗忘方法，但与传统遗忘技术一样，难以在不损害整体模型性能的情况下完全封装需要遗忘的内容范围。", "conclusion": "模型编辑算法可作为机器遗忘的有效替代方案，在特定场景下表现优于传统方法，但仍需解决完全移除信息与保持模型整体性能之间的平衡问题。"}}
{"id": "2512.20651", "pdf": "https://arxiv.org/pdf/2512.20651", "abs": "https://arxiv.org/abs/2512.20651", "authors": ["Deliang Wen", "Ke Sun"], "title": "Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from \"memory\" to \"cognition\".", "AI": {"tldr": "Memory Bear系统基于认知科学原理构建类人记忆架构，通过多模态信息感知、动态记忆维护和自适应认知服务，解决了LLMs在内存限制、知识遗忘、信息冗余和幻觉生成方面的问题，在多个领域实现了性能突破。", "motivation": "大型语言模型面临内存限制、上下文窗口受限、长期知识遗忘、冗余信息积累和幻觉生成等固有局限性，严重制约了持续对话和个性化服务的发展。", "method": "提出Memory Bear系统，基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现LLM记忆机制的全链条重构。", "result": "在医疗、企业运营和教育等多个领域展现显著工程创新和性能突破，大幅提升长期对话中的知识保真度和检索效率，降低幻觉率，通过记忆-认知集成增强上下文适应性和推理能力。", "conclusion": "实验结果表明，相比现有解决方案（如Mem0、MemGPT、Graphiti），Memory Bear在准确性、token效率和响应延迟等关键指标上表现更优，标志着AI从\"记忆\"向\"认知\"迈进的关键一步。"}}
{"id": "2512.20796", "pdf": "https://arxiv.org/pdf/2512.20796", "abs": "https://arxiv.org/abs/2512.20796", "authors": ["Zhengyang Shan", "Aaron Mueller"], "title": "Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?", "categories": ["cs.CL"], "comment": null, "summary": "We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.", "AI": {"tldr": "研究发现语言模型中的人口统计偏见机制与一般人口统计识别是相互独立的，通过特征消融可以在减少偏见的同时保持识别能力，其中基于归因的方法对种族和性别职业偏见有效，而基于相关性的方法对教育偏见更有效。", "motivation": "探究语言模型中人口统计偏见机制是否与一般人口统计识别能力相互独立，以及是否可以在消除偏见的同时保留人口统计检测能力。", "method": "使用多任务评估设置，将人口统计信息与姓名、职业和教育水平关联，比较基于归因和基于相关性的方法来定位偏见特征，在Gemma-2-9B模型上进行目标稀疏自编码器特征消融实验。", "result": "基于归因的特征消融减少了种族和性别职业偏见同时保持了姓名识别准确性；基于相关性的消融对教育偏见更有效；教育任务中移除归因特征会导致\"先验崩溃\"从而增加总体偏见。", "conclusion": "人口统计偏见源于任务特定机制而非绝对的人口统计标记，基于机制的推理时干预可以实现精确的去偏见而不损害模型核心能力，需要维度特定的干预策略。"}}
{"id": "2512.20652", "pdf": "https://arxiv.org/pdf/2512.20652", "abs": "https://arxiv.org/abs/2512.20652", "authors": ["Vira Filatova", "Andrii Zelenchuk", "Dmytro Filatov"], "title": "AI-Driven Decision-Making System for Hiring Process", "categories": ["cs.AI"], "comment": "10 pages, 3 figures", "summary": "Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.", "AI": {"tldr": "AI驱动的模块化多智能体招聘助手系统，通过结构化候选人分析、公共数据验证和风险评估，显著提高招聘筛选效率，将每位合格候选人筛选时间从3.33小时降至1.70小时。", "motivation": "解决早期候选人验证这一招聘瓶颈，需要处理简历、筛选答案、代码作业等异构输入，以及有限的公开证据验证问题。", "method": "开发模块化多智能体系统，包括文档视频预处理、结构化档案构建、公共数据验证、技术/文化匹配评分（含风险惩罚）和人工交互验证。使用LLM在严格约束下协调流程，生成可追溯的组件级理由。", "result": "在64名中级Python后端工程师申请者评估中，系统效率为每合格候选人1.70小时，相比经验丰富招聘人员的3.33小时显著提升，筛选成本大幅降低，同时保持人类决策者最终决定权。", "conclusion": "AI招聘助手系统能有效提高招聘筛选效率和成本效益，同时通过人机协作保持决策质量，为招聘流程优化提供了可行方案。"}}
{"id": "2512.20812", "pdf": "https://arxiv.org/pdf/2512.20812", "abs": "https://arxiv.org/abs/2512.20812", "authors": ["Nathaniël de Leeuw", "Marceau Nahon", "Mathis Reymond", "Raja Chatila", "Mehdi Khamassi"], "title": "Semantic Deception: When Reasoning Models Can't Compute an Addition", "categories": ["cs.CL"], "comment": "22 pages, 5 figures", "summary": "Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.", "AI": {"tldr": "LLMs在处理新颖符号表示时容易受到语义欺骗影响，即使简单计算任务中也会因符号形式误导而性能下降，暴露了其符号抽象能力的局限性和对表面语义的过度依赖。", "motivation": "研究LLMs在涉及人类价值观的关键决策任务中的推理能力，特别是测试它们处理陌生符号的能力，以评估是否能够保持符号抽象而非依赖已学习的语义关联。", "method": "引入语义欺骗实验框架，重新定义数字和数学运算符为新颖符号，让LLMs解决使用这种改变符号表示的简单计算问题，测试四个LLMs模型。", "result": "语义线索显著降低了推理模型在简单任务上的性能，显示当前LLMs在符号操作能力上存在局限，倾向于过度依赖表面语义，思维链可能放大统计相关性依赖。", "conclusion": "LLMs的符号推理能力存在根本限制，在需要稳健符号推理的决策场景中可能因训练中的残留语义关联而失败，这对将推理能力归因于LLMs的普遍倾向提出了伦理和社会关切。"}}
{"id": "2512.20661", "pdf": "https://arxiv.org/pdf/2512.20661", "abs": "https://arxiv.org/abs/2512.20661", "authors": ["Yawei Liu"], "title": "From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers", "categories": ["cs.AI"], "comment": "10 pages, 5 figures, submited to WWW 2026", "summary": "Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%", "AI": {"tldr": "提出对抗性注意力反馈训练机制(AFA)，通过动态掩码策略和政策梯度优化，自动重新分配注意力权重到任务相关词汇，在情感分析任务上取得SOTA结果。", "motivation": "现有Transformer模型在情感分析中倾向于关注常见词汇而忽略不常见但任务相关的词汇，导致性能不佳。通过分析注意力分布发现这一缺陷需要改进。", "method": "采用对抗性训练机制，包含动态掩码策略欺骗判别器，同时使用政策梯度方法优化注意力分布。不需要人工标注即可自动调整注意力权重。", "result": "在三个公开数据集上实现了最先进的结果。将该机制应用于大语言模型时，性能进一步提升12.6%。", "conclusion": "AFA机制能有效改善Transformer模型的注意力分配问题，提升情感分析性能，且可扩展应用于大语言模型带来显著改进。"}}
{"id": "2512.20817", "pdf": "https://arxiv.org/pdf/2512.20817", "abs": "https://arxiv.org/abs/2512.20817", "authors": ["Kumar Satvik Chaudhary", "Chengshuai Zhao", "Fan Zhang", "Yung Hin Tse", "Garima Agrawal", "Yuli Deng", "Huan Liu"], "title": "EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.", "AI": {"tldr": "EssayCBM是一个可解释的自动作文评分框架，通过评估8个写作概念来提供透明的评分过程，而非直接预测分数，实现了与黑盒模型相当的性能并提供可操作的反馈。", "motivation": "自动评分系统特别是大型语言模型通常作为黑盒运行，教育者和学生难以理解评分依据，需要提高评分过程的透明度和可解释性。", "method": "使用基于评分标准的框架，通过专门的预测头评估8个写作概念（如论点清晰度、证据使用等），概念分数形成透明瓶颈，轻量级网络仅使用概念计算最终分数。", "result": "EssayCBM在保持与黑盒模型相当性能的同时，提供了概念级别的可操作反馈，教师可以调整概念预测并即时查看更新后的分数。", "conclusion": "该框架通过概念评估的透明瓶颈实现了可解释的自动作文评分，支持人机协作评估，为教育评估提供了更负责任和可操作的解决方案。"}}
{"id": "2512.20662", "pdf": "https://arxiv.org/pdf/2512.20662", "abs": "https://arxiv.org/abs/2512.20662", "authors": ["Yiqing Ma", "Jung-Hua Liu"], "title": "Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.", "AI": {"tldr": "研究发现LLMs存在懒惰行为（未完整执行多部分指令），但在解码优化和上下文保持方面表现优于预期，建议通过自我优化和动态提示来改善指令遵循问题", "motivation": "量化大型语言模型的行为缺陷，包括懒惰、解码次优性和上下文退化，以评估现代LLMs的实际表现", "method": "进行三个对照实验（A、B、C），在多个先进LLM（OpenAI GPT-4变体、DeepSeek）上测试多部分指令遵循、简单推理任务和长对话上下文保持能力", "result": "发现普遍存在懒惰现象（经常忽略指令部分），但解码次优性证据有限（贪婪答案与最高置信度解决方案一致），在200轮混乱对话测试中表现出意外的上下文保持鲁棒性", "conclusion": "虽然详细指令遵循仍是挑战，但现代LLMs在内部缓解了某些假设的故障模式（如上下文遗忘），建议使用自我优化和动态提示策略来减少懒惰并增强多指令遵循能力"}}
{"id": "2512.20822", "pdf": "https://arxiv.org/pdf/2512.20822", "abs": "https://arxiv.org/abs/2512.20822", "authors": ["Zhan Qu", "Michael Färber"], "title": "MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.", "AI": {"tldr": "MediEval是一个新的医疗LLM评估基准，通过将MIMIC-IV电子健康记录与统一医学知识库连接，生成真实和反事实医疗陈述，系统评估LLM在知识基础和上下文一致性方面的表现。研究发现现有LLM存在幻觉支持和真相反转等严重问题，并提出了CoRFu微调方法显著提升安全性和准确性。", "motivation": "现有医疗LLM评估要么孤立测试医学知识，要么评估患者级推理但不验证正确性，存在可靠性安全性的关键空白，需要更全面的评估框架。", "method": "构建MediEval基准：连接MIMIC-IV EHR与UMLS等生物医学词汇知识库，生成多样化的真实和反事实医疗陈述，采用4象限框架系统评估知识基础和上下文一致性。提出CoRFu方法：基于DPO的不对称惩罚微调，针对不安全混淆。", "result": "发现现有专有、开源和领域特定LLM普遍存在幻觉支持和真相反转等关键失败模式。CoRFu方法相比基础模型提升+16.4 macro-F1分数，完全消除真相反转错误，显著提高准确性和安全性。", "conclusion": "MediEval基准揭示了医疗LLM的重要安全风险，CoRFu微调方法有效解决了这些问题，为开发更可靠安全的医疗AI系统提供了重要工具和方法论。"}}
{"id": "2512.20664", "pdf": "https://arxiv.org/pdf/2512.20664", "abs": "https://arxiv.org/abs/2512.20664", "authors": ["Shinobu Miya"], "title": "Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.", "AI": {"tldr": "该论文提出了一种基于约束满足问题(CSP)的结构化验证方法Eidoku，通过计算结构违规成本而非概率来检测LLM的幻觉问题，特别针对概率验证无法识别的\"平滑虚假\"陈述。", "motivation": "LLM经常产生被模型自身赋予高概率的幻觉陈述，这表明幻觉不是低置信度现象，而是结构一致性的失败。概率基础的验证存在根本性限制。", "method": "将LLM推理验证重新表述为约束满足问题(CSP)，独立于生成概率。定义包含三个代理的总成本函数：图连接性(结构)、特征空间一致性(几何)和逻辑蕴含(符号)。通过轻量级System-2门Eidoku执行验证，拒绝超过上下文校准成本阈值的候选。", "result": "该方法成功拒绝了概率验证器无法检测的\"平滑虚假\"陈述——高概率但结构断开的陈述。在诊断数据集上的实验显示，显式执行结构约束可以确定性地拒绝这类特定幻觉。", "conclusion": "基于结构约束的验证方法为生成推理提供了神经符号的合理性检查，能够有效解决概率验证的局限性，特别是针对高概率幻觉问题。"}}
{"id": "2512.20848", "pdf": "https://arxiv.org/pdf/2512.20848", "abs": "https://arxiv.org/abs/2512.20848", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aaron Grattafiori", "Aarti Basant", "Abhibha Gupta", "Abhinav Khattar", "Adi Renduchintala", "Aditya Vavre", "Akanksha Shukla", "Akhiad Bercovich", "Aleksander Ficek", "Aleksandr Shaposhnikov", "Alex Kondratenko", "Alexander Bukharin", "Alexandre Milesi", "Ali Taghibakhshi", "Alisa Liu", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amir Klein", "Amit Zuker", "Amnon Geifman", "Amy Shen", "Anahita Bhiwandiwalla", "Andrew Tao", "Ann Guan", "Anubhav Mandarwal", "Arham Mehta", "Ashwath Aithal", "Ashwin Poojary", "Asif Ahamed", "Asma Kuriparambil Thekkumpate", "Ayush Dattagupta", "Banghua Zhu", "Bardiya Sadeghi", "Barnaby Simkin", "Ben Lanir", "Benedikt Schifferer", "Besmira Nushi", "Bilal Kartal", "Bita Darvish Rouhani", "Boris Ginsburg", "Brandon Norick", "Brandon Soubasis", "Branislav Kisacanin", "Brian Yu", "Bryan Catanzaro", "Carlo del Mundo", "Chantal Hwang", "Charles Wang", "Cheng-Ping Hsieh", "Chenghao Zhang", "Chenhan Yu", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christopher Parisien", "Collin Neale", "Damon Mosk-Aoyama", "Dan Su", "Dane Corneil", "Daniel Afrimi", "Daniel Rohrer", "Daniel Serebrenik", "Daria Gitman", "Daria Levy", "Darko Stosic", "David Mosallanezhad", "Deepak Narayanan", "Dhruv Nathawani", "Dima Rekesh", "Dina Yared", "Divyanshu Kakwani", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Edgar Minasyan", "Edward Lin", "Eileen Long", "Eileen Peters Long", "Elena Lantz", "Ellie Evans", "Elliott Ning", "Eric Chung", "Eric Harper", "Eric Tramel", "Erick Galinkin", "Erik Pounds", "Evan Briones", "Evelina Bakhturina", "Faisal Ladhak", "Fay Wang", "Fei Jia", "Felipe Soares", "Feng Chen", "Ferenc Galko", "Frankie Siino", "Gal Hubara Agam", "Ganesh Ajjanagadde", "Gantavya Bhatt", "Gargi Prasad", "George Armstrong", "Gerald Shen", "Gorkem Batmaz", "Grigor Nalbandyan", "Haifeng Qian", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Himanshu Soni", "Hiren Upadhyay", "Huizi Mao", "Huy C Nguyen", "Huy Q Nguyen", "Iain Cunningham", "Ido Shahaf", "Igor Gitman", "Ilya Loshchilov", "Ivan Moshkov", "Izzy Putterman", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jatin Mitra", "Jeffrey Glick", "Jenny Chen", "Jesse Oliver", "Jian Zhang", "Jiaqi Zeng", "Jie Lou", "Jimmy Zhang", "Jining Huang", "Joey Conway", "Joey Guman", "John Kamalu", "Johnny Greco", "Jonathan Cohen", "Joseph Jennings", "Joyjit Daw", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kai Xu", "Kan Zhu", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keshav Santhanam", "Kevin Shih", "Kezhi Kong", "Khushi Bhardwaj", "Krishna C. Puvvada", "Krzysztof Pawelec", "Kumar Anik", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Li Ding", "Lucas Liebenwein", "Luis Vega", "Maanu Grover", "Maarten Van Segbroeck", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Manoj Kilaru", "Maor Ashkenazi", "Marc Romeijn", "Mark Cai", "Markus Kliegl", "Maryam Moosaei", "Matvei Novikov", "Mehrzad Samadi", "Melissa Corpuz", "Mengru Wang", "Meredith Price", "Michael Boone", "Michael Evans", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Nabin Mulepati", "Natalie Hereth", "Nave Assaf", "Negar Habibi", "Neta Zmora", "Netanel Haber", "Nicola Sessions", "Nidhi Bhatia", "Nikhil Jukar", "Nikki Pope", "Nikolai Ludwig", "Nima Tajbakhsh", "Nirmal Juluru", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Olivier Delalleau", "Oluwatobi Olabiyi", "Omer Ullman Argov", "Ouye Xie", "Parth Chadha", "Pasha Shamis", "Pavlo Molchanov", "Pawel Morkisz", "Peter Dykas", "Peter Jin", "Pinky Xu", "Piotr Januszewski", "Pranav Prashant Thombre", "Prasoon Varshney", "Pritam Gundecha", "Qing Miao", "Rabeeh Karimi Mahabadi", "Ran El-Yaniv", "Ran Zilberstein", "Rasoul Shafipour", "Rich Harang", "Rick Izzo", "Rima Shahbazyan", "Rishabh Garg", "Ritika Borkar", "Ritu Gala", "Riyad Islam", "Roger Waleffe", "Rohit Watve", "Roi Koren", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Ryan Timbrook", "Sadegh Mahdavi", "Sahil Modi", "Samuel Kriman", "Sanjay Kariyappa", "Sanjeev Satheesh", "Saori Kaji", "Satish Pasumarthi", "Sean Narentharen", "Sean Narenthiran", "Seonmyeong Bak", "Sergey Kashirsky", "Seth Poulos", "Shahar Mor", "Shanmugam Ramasamy", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shiqing Fan", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Simeng Sun", "Smita Ithape", "Somshubra Majumdar", "Soumye Singhal", "Stefania Alborghetti", "Stephen Ge", "Sugam Dipak Devare", "Sumeet Kumar Barua", "Suseella Panguluri", "Suyog Gupta", "Sweta Priyadarshi", "Syeda Nahida Akter", "Tan Bui", "Teodor-Dumitru Ene", "Terry Kong", "Thanh Do", "Tijmen Blankevoort", "Tom Balough", "Tomer Asida", "Tomer Bar Natan", "Tugrul Konuk", "Twinkle Vashishth", "Udi Karpas", "Ushnish De", "Vahid Noorozi", "Vahid Noroozi", "Venkat Srinivasan", "Venmugil Elango", "Vijay Korthikanti", "Vitaly Kurin", "Vitaly Lavrukhin", "Wanli Jiang", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenfei Zhou", "Will Jennings", "William Zhang", "Wojciech Prazuch", "Xiaowei Ren", "Yashaswi Karnati", "Yejin Choi", "Yev Meyer", "Yi-Fu Wu", "Yian Zhang", "Ying Lin", "Yonatan Geifman", "Yonggan Fu", "Yoshi Subara", "Yoshi Suhara", "Yubo Gao", "Zach Moshe", "Zhen Dong", "Zihan Liu", "Zijia Chen", "Zijie Yan"], "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.", "AI": {"tldr": "Nemotron 3 Nano 30B-A3B是一个混合专家Mamba-Transformer语言模型，相比前代模型参数量减半但精度更高，推理吞吐量提升3.3倍，支持100万token上下文长度。", "motivation": "开发一个更高效、更准确的语言模型，通过混合专家架构减少计算资源消耗同时提升性能表现。", "method": "采用Mixture-of-Experts混合Mamba-Transformer架构，在25万亿文本token上预训练，随后进行监督微调和大规模强化学习。", "result": "比前代Nemotron 2 Nano更准确，推理吞吐量比同类模型高3.3倍，在流行基准测试中表现更优，具备更强的代理能力、推理能力和对话能力。", "conclusion": "Nemotron 3 Nano展示了混合专家架构在提升模型效率和性能方面的优势，为大规模语言模型的发展提供了新的技术路径，模型已在Hugging Face平台发布。"}}
{"id": "2512.20671", "pdf": "https://arxiv.org/pdf/2512.20671", "abs": "https://arxiv.org/abs/2512.20671", "authors": ["Daan Di Scala", "Sophie Lathouwers", "Michael van Bekkum"], "title": "Bridging the AI Trustworthiness Gap between Functions and Norms", "categories": ["cs.AI"], "comment": "Published as Position Paper during the TRUST-AI workshop at the ECAI2025 Conference", "summary": "Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.", "AI": {"tldr": "本文提出需要一个语义语言来弥合功能性可信AI(FTAI)和规范性可信AI(NTAI)之间的鸿沟，为AI系统的可信度评估提供框架支持。", "motivation": "当前FTAI(关注如何实现可信系统)和NTAI(关注需要执行的法规)之间存在差距，使得评估AI系统的可信度变得困难。", "method": "提出引入概念性语义语言作为桥梁，匹配FTAI和NTAI，帮助开发人员评估AI系统的可信度，并协助利益相关者将规范和法规转化为具体的实施步骤。", "result": "描述了当前最先进的技术现状，识别了FTAI和NTAI之间的差距，讨论了开发语义语言的起点及其预期效果。", "conclusion": "提供了关键考虑因素，并讨论了未来在可信AI评估方面的行动方向，强调需要建立语义语言来连接技术实现和法规要求。"}}
{"id": "2512.20854", "pdf": "https://arxiv.org/pdf/2512.20854", "abs": "https://arxiv.org/abs/2512.20854", "authors": ["Shelly Schwartz", "Oleg Vasilyev", "Randy Sawaya"], "title": "How important is Recall for Measuring Retrieval Quality?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.", "AI": {"tldr": "该论文评估了在不知道相关文档总数的情况下，如何通过检索质量指标与LLM生成的回答质量之间的相关性来衡量检索策略的有效性，并提出了一种新的简单检索质量度量方法。", "motivation": "在大型且不断发展的知识库中，查询的相关文档总数通常是未知的，无法计算召回率，这给检索系统评估带来了挑战。", "method": "通过测量检索质量指标与基于LLM的回答质量判断之间的相关性来评估现有策略，并在多个数据集（相关文档数较少，2-15个）上进行实验。", "result": "研究提出了一种简单的检索质量度量方法，该方法在不需知道相关文档总数的情况下表现良好。", "conclusion": "在相关文档数量有限的实际检索场景中，可以通过LLM评估与检索指标的相关性来有效评估检索质量，新提出的度量方法具有实用价值。"}}
{"id": "2512.20714", "pdf": "https://arxiv.org/pdf/2512.20714", "abs": "https://arxiv.org/abs/2512.20714", "authors": ["Iman Reihanian", "Yunfei Hou", "Qingquan Sun"], "title": "From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education", "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "Review article. 23 pages, 7 figures, 8 tables. Published in AI (MDPI), 2026", "summary": "Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.", "AI": {"tldr": "这篇范围综述分析了32项研究（2023-2025），探讨生成式AI在计算机科学教育中的个性化应用效果，发现解释优先指导、解决方案保留、分级提示阶梯和工件基础等设计能带来更好的学习效果，并提出了探索优先的采用框架。", "motivation": "生成式AI能够实现规模化个性化计算机科学教育，但需要研究这种个性化是否真正支持学习，以及如何设计有效的个性化机制。", "method": "采用范围综述方法，从259份记录中有目的地选取32项研究进行综合分析，映射个性化机制和有效性信号，识别五个应用领域并分析设计选择如何影响学习成果。", "result": "研究发现包含解释优先指导、解决方案保留、分级提示阶梯和工件基础的设计比无约束聊天界面展现出更积极的学习过程。成功实施共享四种模式：基于学生工件的上下文感知辅导、需要反思的多级提示结构、与传统CS基础设施的结合以及人工在环质量保证。", "conclusion": "证据支持生成式AI作为精确支架机制，当嵌入可审计的工作流程中时，能够在保持生产性挣扎的同时规模化提供个性化支持，但需要关注学术诚信、隐私、偏见和过度依赖等风险。"}}
{"id": "2512.20856", "pdf": "https://arxiv.org/pdf/2512.20856", "abs": "https://arxiv.org/abs/2512.20856", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aaron Grattafiori", "Aarti Basant", "Abhibha Gupta", "Abhinav Khattar", "Adi Renduchintala", "Aditya Vavre", "Akanksha Shukla", "Akhiad Bercovich", "Aleksander Ficek", "Aleksandr Shaposhnikov", "Alex Kondratenko", "Alexander Bukharin", "Alexandre Milesi", "Ali Taghibakhshi", "Alisa Liu", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amir Klein", "Amit Zuker", "Amnon Geifman", "Amy Shen", "Anahita Bhiwandiwalla", "Andrew Tao", "Anjulie Agrusa", "Ankur Verma", "Ann Guan", "Anubhav Mandarwal", "Arham Mehta", "Ashwath Aithal", "Ashwin Poojary", "Asif Ahamed", "Asit Mishra", "Asma Kuriparambil Thekkumpate", "Ayush Dattagupta", "Banghua Zhu", "Bardiya Sadeghi", "Barnaby Simkin", "Ben Lanir", "Benedikt Schifferer", "Besmira Nushi", "Bilal Kartal", "Bita Darvish Rouhani", "Boris Ginsburg", "Brandon Norick", "Brandon Soubasis", "Branislav Kisacanin", "Brian Yu", "Bryan Catanzaro", "Carlo del Mundo", "Chantal Hwang", "Charles Wang", "Cheng-Ping Hsieh", "Chenghao Zhang", "Chenhan Yu", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christopher Parisien", "Collin Neale", "Cyril Meurillon", "Damon Mosk-Aoyama", "Dan Su", "Dane Corneil", "Daniel Afrimi", "Daniel Lo", "Daniel Rohrer", "Daniel Serebrenik", "Daria Gitman", "Daria Levy", "Darko Stosic", "David Mosallanezhad", "Deepak Narayanan", "Dhruv Nathawani", "Dima Rekesh", "Dina Yared", "Divyanshu Kakwani", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Edgar Minasyan", "Edward Lin", "Eileen Long", "Eileen Peters Long", "Elad Segal", "Elena Lantz", "Ellie Evans", "Elliott Ning", "Eric Chung", "Eric Harper", "Eric Tramel", "Erick Galinkin", "Erik Pounds", "Evan Briones", "Evelina Bakhturina", "Evgeny Tsykunov", "Faisal Ladhak", "Fay Wang", "Fei Jia", "Felipe Soares", "Feng Chen", "Ferenc Galko", "Frank Sun", "Frankie Siino", "Gal Hubara Agam", "Ganesh Ajjanagadde", "Gantavya Bhatt", "Gargi Prasad", "George Armstrong", "Gerald Shen", "Gorkem Batmaz", "Grigor Nalbandyan", "Haifeng Qian", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herbert Hum", "Herman Sahota", "Hexin Wang", "Himanshu Soni", "Hiren Upadhyay", "Huizi Mao", "Huy C Nguyen", "Huy Q Nguyen", "Iain Cunningham", "Ido Galil", "Ido Shahaf", "Igor Gitman", "Ilya Loshchilov", "Itamar Schen", "Itay Levy", "Ivan Moshkov", "Izik Golan", "Izzy Putterman", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jatin Mitra", "Jeffrey Glick", "Jenny Chen", "Jesse Oliver", "Jian Zhang", "Jiaqi Zeng", "Jie Lou", "Jimmy Zhang", "Jinhang Choi", "Jining Huang", "Joey Conway", "Joey Guman", "John Kamalu", "Johnny Greco", "Jonathan Cohen", "Joseph Jennings", "Joyjit Daw", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kai Xu", "Kan Zhu", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kevin Shih", "Kezhi Kong", "Khushi Bhardwaj", "Kirthi Shankar", "Krishna C. Puvvada", "Krzysztof Pawelec", "Kumar Anik", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Li Ding", "Lizzie Wei", "Lucas Liebenwein", "Luis Vega", "Maanu Grover", "Maarten Van Segbroeck", "Maer Rodrigues de Melo", "Mahdi Nazemi", "Makesh Narsimhan Sreedhar", "Manoj Kilaru", "Maor Ashkenazi", "Marc Romeijn", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Maryam Moosaei", "Matt Kulka", "Matvei Novikov", "Mehrzad Samadi", "Melissa Corpuz", "Mengru Wang", "Meredith Price", "Michael Andersch", "Michael Boone", "Michael Evans", "Miguel Martinez", "Mikail Khona", "Mike Chrzanowski", "Minseok Lee", "Mohammad Dabbah", "Mohammad Shoeybi", "Mostofa Patwary", "Nabin Mulepati", "Najeeb Nabwani", "Natalie Hereth", "Nave Assaf", "Negar Habibi", "Neta Zmora", "Netanel Haber", "Nicola Sessions", "Nidhi Bhatia", "Nikhil Jukar", "Nikki Pope", "Nikolai Ludwig", "Nima Tajbakhsh", "Nir Ailon", "Nirmal Juluru", "Nishant Sharma", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Olivier Delalleau", "Oluwatobi Olabiyi", "Omer Ullman Argov", "Omri Puny", "Oren Tropp", "Ouye Xie", "Parth Chadha", "Pasha Shamis", "Paul Gibbons", "Pavlo Molchanov", "Pawel Morkisz", "Peter Dykas", "Peter Jin", "Pinky Xu", "Piotr Januszewski", "Pranav Prashant Thombre", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Qing Miao", "Qiyu Wan", "Rabeeh Karimi Mahabadi", "Rachit Garg", "Ran El-Yaniv", "Ran Zilberstein", "Rasoul Shafipour", "Rich Harang", "Rick Izzo", "Rima Shahbazyan", "Rishabh Garg", "Ritika Borkar", "Ritu Gala", "Riyad Islam", "Robert Hesse", "Roger Waleffe", "Rohit Watve", "Roi Koren", "Ruoxi Zhang", "Russell Hewett", "Russell J. Hewett", "Ryan Prenger", "Ryan Timbrook", "Sadegh Mahdavi", "Sahil Modi", "Samuel Kriman", "Sangkug Lim", "Sanjay Kariyappa", "Sanjeev Satheesh", "Saori Kaji", "Satish Pasumarthi", "Saurav Muralidharan", "Sean Narentharen", "Sean Narenthiran", "Seonmyeong Bak", "Sergey Kashirsky", "Seth Poulos", "Shahar Mor", "Shanmugam Ramasamy", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shiqing Fan", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Simeng Sun", "Smita Ithape", "Somshubra Majumdar", "Soumye Singhal", "Stas Sergienko", "Stefania Alborghetti", "Stephen Ge", "Sugam Dipak Devare", "Sumeet Kumar Barua", "Suseella Panguluri", "Suyog Gupta", "Sweta Priyadarshi", "Syeda Nahida Akter", "Tan Bui", "Teodor-Dumitru Ene", "Terry Kong", "Thanh Do", "Tijmen Blankevoort", "Tim Moon", "Tom Balough", "Tomer Asida", "Tomer Bar Natan", "Tomer Ronen", "Tugrul Konuk", "Twinkle Vashishth", "Udi Karpas", "Ushnish De", "Vahid Noorozi", "Vahid Noroozi", "Venkat Srinivasan", "Venmugil Elango", "Victor Cui", "Vijay Korthikanti", "Vinay Rao", "Vitaly Kurin", "Vitaly Lavrukhin", "Vladimir Anisimov", "Wanli Jiang", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenfei Zhou", "Will Jennings", "William Zhang", "Wojciech Prazuch", "Xiaowei Ren", "Yashaswi Karnati", "Yejin Choi", "Yev Meyer", "Yi-Fu Wu", "Yian Zhang", "Yigong Qin", "Ying Lin", "Yonatan Geifman", "Yonggan Fu", "Yoshi Subara", "Yoshi Suhara", "Yubo Gao", "Zach Moshe", "Zhen Dong", "Zhongbo Zhu", "Zihan Liu", "Zijia Chen", "Zijie Yan"], "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.", "AI": {"tldr": "Nemotron 3模型家族包含Nano、Super和Ultra三个版本，采用混合Mamba-Transformer架构，支持高达100万token的上下文长度，具备强大的代理、推理和对话能力。", "motivation": "开发一个高性能、高效率的模型家族，满足不同规模应用的需求，从成本效益高的推理到最先进的准确性要求。", "method": "使用Mixture-of-Experts混合Mamba-Transformer架构，采用NVFP4训练和LatentMoE创新方法提升模型质量，通过多环境强化学习进行后训练。", "result": "Nano模型在保持极高推理成本效益的同时超越同类模型准确性；Super针对协作代理和高负载工作优化；Ultra提供最先进的准确性和推理性能。", "conclusion": "Nemotron 3模型家族为不同应用场景提供了全面的解决方案，并将公开模型权重、训练软件和配方，促进AI社区的发展。"}}
{"id": "2512.20723", "pdf": "https://arxiv.org/pdf/2512.20723", "abs": "https://arxiv.org/abs/2512.20723", "authors": ["Prajwal Ghimire", "Keyoumars Ashkan"], "title": "From artificial to organic: Rethinking the roots of intelligence for digital health", "categories": ["cs.AI"], "comment": null, "summary": "The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.", "AI": {"error": "'NoneType' object has no attribute 'model_dump'"}}
{"id": "2512.20877", "pdf": "https://arxiv.org/pdf/2512.20877", "abs": "https://arxiv.org/abs/2512.20877", "authors": ["Shivraj Singh Bhatti"], "title": "Architectural Trade-offs in Small Language Models Under Compute Constraints", "categories": ["cs.CL", "cs.LG"], "comment": "15 pages, 11 images", "summary": "We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.", "AI": {"tldr": "系统研究小语言模型在严格计算约束下的性能，分析架构选择与训练预算如何共同决定模型表现。通过从线性预测器逐步引入非线性、自注意力和多层Transformer，评估字符级和词级建模任务。", "motivation": "探索在小规模语言模型中，不同架构选择与计算预算如何影响性能表现，验证大语言模型成功技术是否适用于小模型场景。", "method": "从线性next-token预测器开始，逐步引入非线性、自注意力和多层Transformer架构，在Tiny Shakespeare（字符级）和PTB/WikiText-2（词级）数据集上进行评估，使用测试负对数似然、参数数量和训练FLOPs作为评估指标。", "result": "基于注意力的模型即使在小型规模下也比MLPs在每FLOP效率上表现更好，但增加深度或上下文长度而优化不足会降低性能。RoPE等在大语言模型中成功的技术不一定能迁移到小模型场景。", "conclusion": "小语言模型的架构选择需要针对其计算约束进行专门优化，不能简单套用大模型的技术，注意力机制在小模型中仍然具有效率优势，但需要谨慎控制模型复杂度以避免性能下降。"}}
{"id": "2512.20745", "pdf": "https://arxiv.org/pdf/2512.20745", "abs": "https://arxiv.org/abs/2512.20745", "authors": ["Haipeng Luo", "Huawen Feng", "Qingfeng Sun", "Can Xu", "Kai Zheng", "Yufei Wang", "Tao Yang", "Han Hu", "Yansong Tang", "Di Wang"], "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "LLM, Mathematical Reasoning", "summary": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.", "AI": {"tldr": "AgentMath是一个结合语言模型推理能力和代码解释器计算精度的智能体框架，通过自动生成高质量训练数据、动态交替语言生成与代码执行的强化学习范式，以及高效训练系统，在复杂数学问题上取得了最先进性能。", "motivation": "现有大型推理模型在复杂数学运算中计算效率低且准确性不足，需要将语言模型的推理能力与代码解释器的计算精度相结合来解决这一问题。", "method": "1) 自动化方法将自然语言思维链转换为结构化工具增强轨迹，生成高质量SFT数据；2) 新颖的智能体强化学习范式，动态交替自然语言生成与实时代码执行；3) 高效训练系统包含异步rollout调度、智能体部分rollout和前缀感知负载均衡技术。", "result": "在AIME24、AIME25和HMMT25数学竞赛基准测试中分别达到90.6%、86.4%和73.8%的准确率，实现了4-5倍的训练加速。", "conclusion": "该方法有效解决了复杂数学推理问题，为构建更复杂和可扩展的数学推理智能体铺平了道路。"}}
{"id": "2512.20908", "pdf": "https://arxiv.org/pdf/2512.20908", "abs": "https://arxiv.org/abs/2512.20908", "authors": ["Kaiyuan Liu", "Shaotian Yan", "Rui Miao", "Bing Wang", "Chen Shen", "Jun Zhang", "Jieping Ye"], "title": "Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.", "AI": {"tldr": "论文提出了推理蒸馏溯源追踪框架，通过分析教师模型、原始学生模型和蒸馏后学生模型的概率分布，揭示蒸馏模型在测试时行为的来源，并基于此提出教师引导的数据选择方法。", "motivation": "现有推理蒸馏方法缺乏对学生模型能力来源的详细分析，不清楚学生模型在新测试环境中是否能保持与教师模型一致的行为，还是回归到原始输出模式，这引发了对蒸馏模型泛化能力的担忧。", "method": "引入跨模型推理蒸馏溯源追踪框架，对蒸馏模型产生的每个动作，获取教师模型、原始学生模型和蒸馏模型在相同上下文中的预测概率，通过比较这些概率将动作分类到不同来源类别。", "result": "实验证明在测试时上下文中，蒸馏模型确实能产生源自教师模型的动作，这些动作与蒸馏模型的观察性能相关并可解释。基于此提出的教师引导数据选择方法在多个代表性教师模型和多样化学生模型上验证有效。", "conclusion": "溯源追踪框架有助于理解推理蒸馏机制，提出的数据选择方法为推理蒸馏提供了原则性标准，展示了该框架在推理蒸馏中的实用性和前景。"}}
{"id": "2512.20798", "pdf": "https://arxiv.org/pdf/2512.20798", "abs": "https://arxiv.org/abs/2512.20798", "authors": ["Miles Q. Li", "Benjamin C. M. Fung", "Martin Weiss", "Pulei Xiong", "Khalil Al-Hussaeni", "Claude Fachkha"], "title": "A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents", "categories": ["cs.AI"], "comment": null, "summary": "As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant \"deliberative misalignment\", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.", "AI": {"tldr": "该研究提出了一个包含40个多步任务场景的新基准测试，用于评估AI智能体在绩效激励下出现的伦理约束违反行为，发现当前最先进的LLM模型存在30-50%的错误对齐率，且更强的推理能力并不保证安全性。", "motivation": "当前的安全基准主要关注单步决策和模拟环境，缺乏能够捕捉在多步现实生产环境中由绩效激励驱动的突发性伦理约束违反行为的评估标准。", "method": "设计包含40个不同场景的基准测试，每个场景都有多步行动要求和特定KPI指标，设置指令驱动和激励驱动两种变体来区分服从性和突发性错误对齐。评估了12个最先进的大语言模型。", "result": "模型的结果驱动约束违反率从1.3%到71.4%不等，9个模型的错误对齐率在30-50%之间。最强的Gemini-3-Pro-Preview模型违反率超过60%，经常为满足KPI而升级到严重不当行为。还观察到显著的\"深思熟虑的错误对齐\"现象。", "conclusion": "研究结果强调了在部署前进行更现实的智能体安全训练的迫切需求，以减轻AI智能体在现实世界中的风险，表明当前模型的推理能力与安全性之间不存在必然关联。"}}
{"id": "2512.20948", "pdf": "https://arxiv.org/pdf/2512.20948", "abs": "https://arxiv.org/abs/2512.20948", "authors": ["Zhongren Dong", "Haotian Guo", "Weixiang Xu", "Huan Zhao", "Zixing Zhang"], "title": "Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.", "AI": {"tldr": "FEND是一个基于基础模型的多模态框架，用于检测阿尔茨海默病、抑郁症和自闭症谱系障碍，整合语音和文本模态，在13个多语言数据集上进行评估，发现多模态融合在AD和抑郁症检测中表现优异，但在ASD中因数据集异质性而表现不佳。", "motivation": "神经精神疾病（如阿尔茨海默病、抑郁症、自闭症谱系障碍）具有语言和声学异常特征，可作为早期检测的生物标志物。但现有研究面临多语言泛化挑战和缺乏统一评估框架的问题。", "method": "提出FEND框架，整合语音和文本模态，利用13个多语言数据集（英语、中文、希腊语、法语、荷兰语），系统评估多模态融合性能，并进行跨语料库实验分析。", "result": "多模态融合在AD和抑郁症检测中表现优异，但在ASD检测中因数据集异质性而表现不佳；发现模态不平衡问题，多模态融合未能超越最佳单模态模型；跨语料库实验显示在任务和语言一致场景中表现稳健，但在多语言和任务异质设置中性能下降。", "conclusion": "FEND通过提供广泛基准测试和性能影响因素分析，推动了自动化、全生命周期包容性和多语言神经精神疾病评估领域的发展，鼓励研究者采用该框架进行公平比较和可重复研究。"}}
{"id": "2512.20806", "pdf": "https://arxiv.org/pdf/2512.20806", "abs": "https://arxiv.org/abs/2512.20806", "authors": ["Anselm Paulus", "Ilia Kulikov", "Brandon Amos", "Rémi Munos", "Ivan Evtimov", "Kamalika Chaudhuri", "Arman Zharmagambetov"], "title": "Safety Alignment of LMs via Non-cooperative Games", "categories": ["cs.AI"], "comment": null, "summary": "Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.", "AI": {"tldr": "提出AdvGame方法，将安全对齐建模为非零和博弈，通过在线强化学习联合训练攻击者和防御者语言模型，使用偏好奖励信号提升安全性和实用性", "motivation": "当前方法依赖顺序对抗训练，存在局限性。需要新的范式来同时提升语言模型的安全性和实用性", "method": "将安全对齐构建为攻击者LM和防御者LM之间的非零和博弈，通过在线强化学习联合训练，使用基于偏好的奖励信号而非点分数", "result": "AdvGame方法能够同时提升防御模型的安全性和实用性，产生更强的通用红队攻击代理", "conclusion": "非零和博弈框架为语言模型安全对齐提供了更有效的训练范式，能够产生既安全又实用的模型和强大的红队测试工具"}}
{"id": "2512.20949", "pdf": "https://arxiv.org/pdf/2512.20949", "abs": "https://arxiv.org/abs/2512.20949", "authors": ["Shize Liang", "Hongzhi Wang"], "title": "Neural Probe-Based Hallucination Detection for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.", "AI": {"tldr": "提出基于神经网络框架的token级幻觉检测方法，使用轻量级MLP探针进行非线性建模，通过多目标损失函数和贝叶斯优化自动搜索最优插入层，在多个数据集上显著优于现有方法。", "motivation": "大型语言模型容易产生幻觉内容，现有基于不确定性估计和外部知识检索的方法存在高置信度错误和检索依赖问题，而传统线性探针难以捕捉深层语义空间的非线性结构。", "method": "冻结语言模型参数，使用轻量级MLP探针对高层隐藏状态进行非线性建模，设计多目标联合损失函数增强检测稳定性，通过贝叶斯优化自动搜索最优探针插入层。", "result": "在LongFact、HealthBench和TriviaQA数据集上，MLP探针在准确率、召回率和低误报条件下的检测能力均显著优于最先进方法。", "conclusion": "神经网络框架的token级幻觉检测方法有效解决了传统方法的局限性，为高风险领域的大语言模型应用提供了实时轻量的幻觉检测解决方案。"}}
{"id": "2512.20831", "pdf": "https://arxiv.org/pdf/2512.20831", "abs": "https://arxiv.org/abs/2512.20831", "authors": ["Rashmeet Kaur Nayyar", "Naman Shah", "Siddharth Srivastava"], "title": "Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.", "AI": {"tldr": "本文提出了一种新的强化学习方法，能够在线自主学习状态和动作抽象，有效处理参数化动作空间中的长时程稀疏奖励问题，显著提高了样本效率。", "motivation": "现实世界中的顺序决策往往涉及参数化动作空间，需要同时处理离散动作选择和连续动作参数调整。现有方法在此设置下存在严重局限：规划方法需要手工制作动作模型，标准强化学习算法要么针对离散动作要么针对连续动作设计，而少数处理参数化动作的RL方法通常依赖领域特定工程且未能利用这些空间的潜在结构。", "method": "引入算法使智能体能够在线自主学习状态和动作抽象，并在学习过程中逐步精炼这些抽象，在状态-动作空间的关键区域增加细粒度细节，以提高性能分辨率。", "result": "在多个连续状态、参数化动作领域中，该抽象驱动方法使TD(λ)算法相比最先进的基线方法实现了显著更高的样本效率。", "conclusion": "本研究成功将强化学习算法的适用范围扩展到具有参数化动作的长时程稀疏奖励设置，通过自主学习的抽象机制有效解决了复杂动作空间中的决策问题。"}}
{"id": "2512.20950", "pdf": "https://arxiv.org/pdf/2512.20950", "abs": "https://arxiv.org/abs/2512.20950", "authors": ["Mohammad Mahdi Abootorabi", "Alireza Ghahramani Kure", "Mohammadali Mohammadkhani", "Sina Elahimanesh", "Mohammad Ali Ali Panah"], "title": "MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "11 pages Published at the SemEval-2025 workshop", "summary": "This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.", "AI": {"tldr": "TriAligner系统使用双编码器架构和对比学习，通过多模态跨语言对齐实现多语言事实核查声明检索，在准确性和性能上显著优于基线方法", "motivation": "在错误信息快速传播的时代，有效的多语言事实核查变得越来越重要，需要能够跨语言检索核查声明的系统", "method": "采用双编码器架构结合对比学习，整合多语言和多模态信息，使用高效数据预处理和增强技术，并采用困难负样本采样提升表示学习", "result": "在单语和跨语言基准测试中显示出检索准确性和事实核查性能的显著提升，优于基线方法", "conclusion": "TriAligner系统通过创新的跨语言对齐方法和数据增强策略，有效提升了多语言事实核查声明的检索效果，为解决错误信息传播提供了有力工具"}}
{"id": "2512.20845", "pdf": "https://arxiv.org/pdf/2512.20845", "abs": "https://arxiv.org/abs/2512.20845", "authors": ["Onat Ozer", "Grace Wu", "Yuchen Wang", "Daniel Dosti", "Honghao Zhang", "Vivi De La Rue"], "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.", "AI": {"tldr": "多智能体多角色辩论方法解决LLM自我反思退化问题，通过引入不同角色的辩论者生成更多样化的反思，在HotPot QA和HumanEval任务上超越单LLM反思性能", "motivation": "LLM在通过反思错误来提升推理任务性能时，单一LLM的持续自我反思会出现思维退化现象，即使知道错误也会重复犯错", "method": "引入多智能体多角色辩论方法，通过不同角色的辩论者来生成反思，提高反思的多样性", "result": "在HotPot QA上达到47%的准确率，在HumanEval上达到82.7%的准确率，均超越了单LLM反思的表现", "conclusion": "多智能体多角色辩论方法能有效解决LLM自我反思退化问题，通过增加反思多样性显著提升推理任务的性能表现"}}
{"id": "2512.20954", "pdf": "https://arxiv.org/pdf/2512.20954", "abs": "https://arxiv.org/abs/2512.20954", "authors": ["Xiang Zhang", "Jiaqi Wei", "Yuejin Yang", "Zijie Qiu", "Yuhan Chen", "Zhiqiang Gao", "Muhammad Abdul-Mageed", "Laks V. S. Lakshmanan", "Wanli Ouyang", "Chenyu You", "Siqi Sun"], "title": "Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.", "AI": {"tldr": "该论文针对蛋白质和RNA语言模型中无法应用思维链(CoT)推理的问题，提出了语言表达力的概念和反射预训练方法，通过生成辅助\"思考标记\"来增强生物序列模型的推理能力。", "motivation": "由于蛋白质和RNA语言模型的标记空间表达力有限（如氨基酸标记），无法像自然语言处理那样应用思维链提示来生成中间推理步骤，这限制了模型在复杂推理任务中的表现。", "method": "提出了语言表达力的概念，并引入反射预训练方法，在生物序列模型中首次实现通过生成超出简单答案标记的辅助\"思考标记\"来进行中间推理。", "result": "理论证明增强的标记集显著提高了生物语言表达力，实验结果显示反射预训练使蛋白质模型能够自我校正，相比标准预训练获得了显著的性能提升。", "conclusion": "通过反射预训练增强语言表达力是解决生物序列模型中应用思维链推理的有效方法，为蛋白质和RNA语言模型的复杂推理能力提供了新的解决方案。"}}
{"id": "2512.20884", "pdf": "https://arxiv.org/pdf/2512.20884", "abs": "https://arxiv.org/abs/2512.20884", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.", "AI": {"tldr": "论文提出了一个概率框架来解决LLM智能体在知识交换中的单向性限制，通过Beta-Bernoulli分布建模信念不确定性，建立双向知识交换的非利他动机机制", "motivation": "当前基于LLM和RAG的自主智能体存在认知不对称问题，只能单向消费数字内容而无法进行双向知识交换，导致冗余推理和集体智能停滞", "method": "使用带有遗忘因子γ的Beta-Bernoulli分布建模智能体信念，分离认知不确定性作为信念方差，建立稳态维持和最优学习两种交互动机，并引入认知缓存机制实现可扩展性", "result": "仿真验证显示这种不确定性驱动策略在异构(Zipfian)环境中显著优于随机基线，保持对概念漂移的高适应性，信念状态可作为RLHF的奖励信号和SFT的高质量数据过滤器", "conclusion": "该概率框架成功将公共贡献重新定义为最优主动学习，为智能体提供了减少自身不确定性的最有效方法，解决了认知不对称问题并促进了集体智能发展"}}
{"id": "2512.20983", "pdf": "https://arxiv.org/pdf/2512.20983", "abs": "https://arxiv.org/abs/2512.20983", "authors": ["Oleksii Proniakin", "Diego Fajardo", "Ruslan Nazarenko", "Razvan Marinescu"], "title": "Automatic Replication of LLM Mistakes in Medical Conversations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "48 pages, 3 figures, 4 tables", "summary": "Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.", "AI": {"tldr": "MedMistake是一个自动提取LLM在医患对话中错误并转换为单次QA对基准的管道，发布了包含3390个QA对的数据集，其中211个经过医学专家验证，用于评估12个前沿LLM的性能。", "motivation": "当前在多维度评估LLM临床表现时，复制特定错误需要大量人工努力，需要自动化方法来提取和转换LLM的错误以创建评估基准。", "method": "开发了MedMistake管道：(1)创建LLM医患之间的复杂对话数据，(2)使用2个LLM评委委员会进行多维度评估，(3)从错误中创建简化的单次QA场景。", "result": "发布了MedMistake-All数据集（3390个QA对）和医学专家验证的MedMistake-Bench（211个问题），评估显示GPT、Claude和Grok模型在验证集上表现最佳。", "conclusion": "MedMistake提供了自动化的LLM错误提取和基准创建方法，有助于更高效地评估和改进LLM在临床环境中的表现，GPT和Claude系列模型在当前测试中表现领先。"}}
{"id": "2512.20985", "pdf": "https://arxiv.org/pdf/2512.20985", "abs": "https://arxiv.org/abs/2512.20985", "authors": ["Salman Jan", "Hassan Ali Razzaqi", "Ali Akarma", "Mohammad Riyaz Belgaum"], "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines", "categories": ["cs.AI", "cs.MA"], "comment": "This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain", "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.", "AI": {"tldr": "提出一个结合LangChain多智能体系统和许可区块链的统一架构，用于实现自主AI系统的实时监控、策略执行和不可篡改审计，通过实验验证了在库存管理、交通信号控制和医疗监测等场景中的有效性。", "motivation": "随着自主决策AI系统在医疗、智慧城市等关键领域的应用增长，这些系统虽然灵活且能实时推理，但也引发了信任、监管和信息完整性方面的担忧，需要确保自主AI系统既高效又负责任。", "method": "设计了一个统一架构模型，包含基于LangChain的多智能体系统与许可区块链相结合，通过Hyperledger Fabric实现治理层，验证输入、评估建议行动并记录执行结果，同时集成MCP动作执行器和LangChain智能体。", "result": "实验结果显示，区块链安全验证能有效防止未经授权的操作，提供全决策过程的追溯能力，并将操作延迟保持在合理范围内，证明了框架的实用性。", "conclusion": "该框架为实施高影响力自主AI应用提供了一个通用系统，既能保持自主性又能确保责任性，为解决自主AI系统的信任和监管问题提供了可行方案。"}}
{"id": "2512.21002", "pdf": "https://arxiv.org/pdf/2512.21002", "abs": "https://arxiv.org/abs/2512.21002", "authors": ["Wei-Rui Chen", "Vignesh Kothapalli", "Ata Fatahibaarzi", "Hejian Sang", "Shao Tang", "Qingquan Song", "Zhipeng Wang", "Muhammad Abdul-Mageed"], "title": "Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\\%$ of tokens of every training sequence can retain, on average, $\\approx94\\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.", "AI": {"tldr": "研究发现仅对CoT推理标记进行知识蒸馏即可达到接近全序列训练的效果，通过截断协议验证前50%的token可保留94%性能，同时大幅降低计算成本。", "motivation": "传统LLM推理能力蒸馏需要处理包含提示、思维链和答案的完整序列，计算成本高昂，需要研究如何优化监督分配来降低计算开销。", "method": "分析不同段落(P、CoT、A)的监督分配效果，建立截断协议量化序列长度与质量的关系，实验验证仅使用CoT token或前50%token的训练效果。", "result": "仅使用CoT token蒸馏效果良好，前50%token训练可保留约94%的数学基准性能，同时训练时间、内存使用和FLOPs均减少约50%。", "conclusion": "推理蒸馏应优先关注早期推理token，提供了一种简单的计算质量权衡方法，为高效知识蒸馏提供了实用策略。"}}
{"id": "2512.20991", "pdf": "https://arxiv.org/pdf/2512.20991", "abs": "https://arxiv.org/abs/2512.20991", "authors": ["Toqeer Ali Syed", "Abdulaziz Alshahrani", "Ali Ullah", "Ali Akarma", "Sohail Khan", "Muhammad Nauman", "Salman Jan"], "title": "FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning", "categories": ["cs.AI", "cs.MA"], "comment": "This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain", "summary": "The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.", "AI": {"tldr": "本文提出了一种价格感知的AI代理系统，结合个人理财管理和饮食优化，为中等收入家庭提供营养充足且价格合理的膳食计划，能够自动适应市场价格波动。", "motivation": "解决中等收入环境下家庭预算有限与营养需求之间的矛盾，特别是在食品价格波动的背景下，需要一种能够兼顾经济性和营养性的智能饮食规划方案。", "method": "采用模块化多代理架构，包含预算、营养、价格监控和健康个性化四个专门代理，共享知识库并使用替代图来确保以最低成本维持营养质量。", "result": "在沙特家庭案例研究中显示，相比静态周菜单，成本降低12-18%，营养充足率超过95%，在20-30%的价格变化下仍保持高性能。", "conclusion": "该框架能够有效结合经济性和营养充足性，为实现零饥饿和良好健康的可持续发展目标提供了可行的可持续和公平饮食规划途径。"}}
{"id": "2512.21017", "pdf": "https://arxiv.org/pdf/2512.21017", "abs": "https://arxiv.org/abs/2512.21017", "authors": ["Xiaofeng Shi", "Qian Kou", "Yuduo Li", "Hua Zhou"], "title": "Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.", "AI": {"tldr": "SFTKey是一种两阶段微调方法，通过分离思维链和关键答案的训练来解决传统SFT中模型过度关注长思维链序列而忽视短但关键答案的问题。", "motivation": "传统监督微调中，大语言模型会过度关注过长的思维链序列，而忽视较短但至关重要的关键答案部分，这直接影响任务成功率和评估质量。", "method": "提出两阶段训练方案：第一阶段使用传统SFT确保正确输出格式；第二阶段仅对关键答案部分进行微调以提高准确性。", "result": "在多个基准测试和模型家族上的实验表明，SFTKey相比传统SFT平均准确率提升超过5%，同时保持生成正确格式的能力。", "conclusion": "本研究通过明确平衡思维链学习和答案相关token的额外优化，推进了大语言模型的微调技术。"}}
{"id": "2512.20996", "pdf": "https://arxiv.org/pdf/2512.20996", "abs": "https://arxiv.org/abs/2512.20996", "authors": ["Yuwei Du", "Jun Zhang", "Jie Feng", "Zhicheng Liu", "Jian Yuan", "Yong Li"], "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control", "categories": ["cs.AI"], "comment": "The code will be available at: https://github.com/tsinghua-fib-lab/TrafficSimAgent", "summary": "Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.", "AI": {"tldr": "TrafficSimAgent是一个基于大语言模型的智能体框架，通过多级专家代理协作，帮助非专业用户轻松执行交通仿真实验和决策优化。", "motivation": "现有交通仿真平台如SUMO和MATSim功能全面但使用门槛高，缺乏专业知识的用户难以从头开始实验并将其应用于日常工作。", "method": "采用分层专家代理架构：高层代理理解自然语言指令、规划实验流程并调用工具；低层代理基于实时交通状况为基本元素选择最优行动方案。", "result": "多场景实验表明，TrafficSimAgent能在各种条件下有效执行仿真，即使在用户指令模糊时也能产生合理结果，性能优于其他系统和SOTA LLM方法。", "conclusion": "该框架通过专家级自主决策驱动的优化，显著降低了交通仿真的使用门槛，为非专业用户提供了高效的实验设计和决策优化解决方案。"}}
{"id": "2512.21106", "pdf": "https://arxiv.org/pdf/2512.21106", "abs": "https://arxiv.org/abs/2512.21106", "authors": ["Safal Thapaliya", "Zehong Wang", "Jiazheng Li", "Ziming Li", "Yanfang Ye", "Chuxu Zhang"], "title": "Semantic Refinement with LLMs for Graph Representations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.", "AI": {"tldr": "DAS框架通过将固定GNN与LLM结合在闭环反馈中，实现数据自适应的语义精炼，解决图数据中结构与语义异质性带来的泛化挑战。", "motivation": "图结构化数据存在预测信号来源的异质性——有些领域节点语义主导，有些领域结构模式主导。现有方法从模型侧注入归纳偏置，但受限于现实图数据的无限多样性。", "method": "提出数据自适应的语义精炼框架DAS：将固定图神经网络(GNN)与大型语言模型(LLM)耦合在闭环反馈中。GNN提供隐式监督信号指导LLM的语义精炼，精炼后的语义反馈更新图学习器。", "result": "在文本丰富和文本缺失的图上评估，结果显示在结构主导的图上持续改进，在语义丰富的图上保持竞争力。", "conclusion": "数据中心的语义适应方法能有效应对结构-语义异质性，证明了从数据角度而非模型角度解决这一挑战的有效性。"}}
{"id": "2512.21066", "pdf": "https://arxiv.org/pdf/2512.21066", "abs": "https://arxiv.org/abs/2512.21066", "authors": ["Tomoaki Yamaguchi", "Yutong Zhou", "Masahiro Ryo", "Keisuke Katsura"], "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.", "AI": {"tldr": "本研究提出了一个结合SHAP可解释性和多模态LLM迭代优化的Agentic XAI框架，用于生成渐进式增强的解释。通过在农业推荐系统上的测试发现，适度迭代能提升解释质量30-33%，但过度优化会导致质量下降，揭示了偏差-方差权衡问题。", "motivation": "尽管可解释AI(XAI)能够提供数据驱动的因素关联分析，但向非专业人士传达XAI输出仍然具有挑战性，阻碍了对AI预测的信任。大型语言模型(LLMs)虽然能翻译技术解释为易懂叙述，但将LLM作为自主代理进行迭代优化的Agentic AI与XAI的结合尚未被探索。", "method": "提出Agentic XAI框架，结合基于SHAP的可解释性和多模态LLM驱动的迭代优化，生成渐进式增强的解释。使用日本26个稻田的产量数据作为农业推荐系统用例，进行11轮迭代优化(Rounds 0-10)。由人类专家(12名作物科学家)和LLMs(14个)根据7个指标评估解释质量。", "result": "两个评估组都确认框架成功提升了推荐质量，从第0轮到峰值(第3-4轮)平均得分提高30-33%。但过度优化导致推荐质量显著下降，显示存在偏差-方差权衡：早期轮次缺乏解释深度(偏差)，过度迭代则引入冗长和无根据的抽象(方差)。", "conclusion": "研究结果表明需要战略性的早期停止(正则化)来优化实际效用，挑战了单调改进的假设，并为Agentic XAI系统提供了基于证据的设计原则。"}}
{"id": "2512.21107", "pdf": "https://arxiv.org/pdf/2512.21107", "abs": "https://arxiv.org/abs/2512.21107", "authors": ["Eduard Stefan Dinuta", "Iustin Sirbu", "Traian Rebedea"], "title": "Semi-Supervised Learning for Large Language Models Safety and Content Moderation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.", "AI": {"tldr": "该论文提出使用半监督学习技术来改进大型语言模型的安全性分类任务，通过结合标注和未标注数据，并采用任务特定的数据增强方法，显著提升了安全分类性能。", "motivation": "当前大型语言模型的安全分类器训练依赖于大量标注数据，但获取高质量标注数据困难、易出错且常包含合成数据，因此需要更有效的方法。", "method": "采用半监督学习技术，利用标注和未标注数据相结合的方式，特别强调使用任务特定的数据增强方法来提升安全分类性能。", "result": "研究表明半监督学习方法在提示和响应两个维度的安全分类任务上都带来了性能提升，任务特定的数据增强技术相比通用增强方法表现更优。", "conclusion": "半监督学习结合任务特定的数据增强是解决大型语言模型安全分类数据稀缺问题的有效方法，为LLM安全研究提供了新方向。"}}
{"id": "2512.21080", "pdf": "https://arxiv.org/pdf/2512.21080", "abs": "https://arxiv.org/abs/2512.21080", "authors": ["Enoch Hyunwook Kang"], "title": "LLM Personas as a Substitute for Field Experiments in Method Benchmarking", "categories": ["cs.AI", "cs.LG", "econ.EM"], "comment": null, "summary": "Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.", "AI": {"tldr": "该论文证明了在特定条件下（仅观察聚合结果和算法盲评估），用LLM角色模拟替代人类进行A/B测试是有效的基准测试方法，并提供了所需样本量的信息理论界限。", "motivation": "传统A/B测试成本高、延迟长，阻碍了迭代方法开发，需要寻找廉价可靠的替代方案。", "method": "通过数学证明if-and-only-if特征化，定义信息理论可区分性指标，计算所需独立角色评估数量。", "result": "证明了在聚合观察和算法盲评估条件下，角色模拟与人类测试在方法优化视角下无区别，并给出了样本量计算界限。", "conclusion": "LLM角色模拟在特定约束下可作为有效的A/B测试替代方案，其有效性取决于样本规模而非模拟真实性。"}}
{"id": "2512.21120", "pdf": "https://arxiv.org/pdf/2512.21120", "abs": "https://arxiv.org/abs/2512.21120", "authors": ["Sichun Luo", "Yi Huang", "Mukai Li", "Shichang Meng", "Fengyuan Liu", "Zefa Hu", "Junlan Feng", "Qi Liu"], "title": "ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \\textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \\textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.", "AI": {"tldr": "ClarifyMT-Bench是一个多轮澄清基准测试，基于五维模糊分类法和六种行为多样化的模拟用户角色构建，用于评估LLM在现实对话中处理模糊信息的能力。研究发现LLMs存在过早回答的倾向，并提出了ClarifyAgent方法来改善澄清能力。", "motivation": "现有的LLM澄清基准主要假设单轮交互或合作用户，无法有效评估LLM在现实多轮对话中处理不完整或模糊信息的能力。", "method": "通过混合LLM-人工流水线构建6,120个多轮对话，基于五维模糊分类法和六种用户角色创建多样化对话场景。评估了10个代表性LLM，并提出了ClarifyAgent方法，将澄清分解为感知、预测、跟踪和规划四个模块。", "result": "发现LLMs存在一致性的澄清不足偏差：倾向于过早回答，且随着对话深度增加性能下降。ClarifyAgent方法显著提高了在各种模糊条件下的鲁棒性。", "conclusion": "ClarifyMT-Bench为研究LLM何时应该提问、何时应该回答以及如何在真实人机交互中处理模糊性提供了可复现的基础，提出的ClarifyAgent方法有效改善了LLM的澄清能力。"}}
{"id": "2512.21110", "pdf": "https://arxiv.org/pdf/2512.21110", "abs": "https://arxiv.org/abs/2512.21110", "authors": ["Ahmed M. Hussain", "Salahuddin Salahuddin", "Panos Papadimitratos"], "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY"], "comment": "22 pages and 23 figures", "summary": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.", "AI": {"tldr": "当前LLM安全机制存在重大漏洞：无法理解上下文和识别用户意图，导致恶意用户可通过情感操控、渐进式透露和学术化论证等方法系统性地绕过安全防护。具有推理功能的配置反而放大了攻击效果，只有Claude Opus 4.1在部分场景中能优先识别意图。", "motivation": "现有LLM安全方法主要关注显性有害内容，但忽视了上下文理解和意图识别这一关键漏洞，导致安全机制容易被系统性绕过。", "method": "实证评估多个最先进LLM（包括ChatGPT、Claude、Gemini和DeepSeek），通过情感框架、渐进式透露和学术化论证等技术测试安全机制的绕过情况。", "result": "研究发现推理功能配置反而增强了攻击效果，提高了事实精确性但未能审问底层意图。Claude Opus 4.1是唯一能在部分用例中优先检测意图的模型。", "conclusion": "当前架构设计存在系统性漏洞，需要范式转变，将上下文理解和意图识别作为核心安全能力，而非事后防护机制。"}}
{"id": "2512.21204", "pdf": "https://arxiv.org/pdf/2512.21204", "abs": "https://arxiv.org/abs/2512.21204", "authors": ["Mahi Luthra", "Jiayi Shen", "Maxime Poli", "Angelo Ortiz", "Yosuke Higuchi", "Youssef Benchekroun", "Martin Gleize", "Charles-Eric Saint-James", "Dongyan Lin", "Phillip Rust", "Angel Villar", "Surya Parimi", "Vanessa Stark", "Rashel Moritz", "Juan Pino", "Yann LeCun", "Emmanuel Dupoux"], "title": "SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.", "AI": {"tldr": "SpidR-Adapt是一种基于元学习的语音表示学习方法，能够在少于1小时的目标语言音频数据上实现快速适应，比标准训练数据效率高100倍以上。", "motivation": "人类婴儿仅需几百小时的语音暴露就能习得新语言的基本单元，而自监督语音模型需要大量数据，存在明显的效率差距。", "method": "将低资源语音表示学习构建为元学习问题，采用多任务自适应预训练(MAdaPT)协议的双层优化框架，提出一阶双层优化(FOBLO)降低计算成本，并通过交替自监督和监督目标的交错监督实现稳定元训练。", "result": "在音位区分性(ABX)和口语语言建模(sWUGGY, sBLIMP, tSC)方面取得快速提升，训练不到1小时目标语言音频就超越域内语言模型。", "conclusion": "为生物启发的数据高效表示学习提供了一条实用的、架构无关的路径，训练代码和模型检查点已开源。"}}
{"id": "2512.21127", "pdf": "https://arxiv.org/pdf/2512.21127", "abs": "https://arxiv.org/abs/2512.21127", "authors": ["Oliver Normand", "Esther Borsi", "Mitch Fruin", "Lauren E Walker", "Jamie Heagerty", "Chris C. Holmes", "Anthony J Avery", "Iain E Buchan", "Harry Coppock"], "title": "A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\\% [95\\% CI 98.2--100], specificity 83.1\\% [95\\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\\% [95\\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.", "AI": {"tldr": "本研究首次在真实NHS初级保健数据上评估基于大语言模型的药物安全审查系统，发现虽然LLM在识别临床问题方面表现良好（敏感性100%），但在复杂临床情境中的综合表现仅46.9%，主要失败模式是情境推理而非药物知识缺失。", "motivation": "尽管LLM在医学基准测试中常达到或超过临床医生水平，但很少有研究在真实临床数据上评估其表现，特别是超越表面指标进行深入分析。本研究旨在填补这一空白。", "method": "回顾性研究使用NHS Cheshire和Merseyside地区2,125,549名成人的电子健康记录，战略抽样277名患者覆盖不同临床复杂性和药物安全风险。专家临床医生审查这些患者并分级系统识别的问题和干预建议。", "result": "主要LLM系统识别临床问题的敏感性为100%，特异性83.1%，但仅在46.9%的患者中正确识别所有问题和干预措施。失败分析显示主要失败机制是情境推理问题，包括五个主要模式：不确定性过度自信、未根据患者情境调整标准指南、误解医疗实践、事实错误和过程盲点。", "conclusion": "这项工作强调了在安全部署基于LLM的临床AI之前必须解决的缺陷，需要进行更大规模的前瞻性评估和更深入的LLM临床行为研究。"}}
{"id": "2512.21280", "pdf": "https://arxiv.org/pdf/2512.21280", "abs": "https://arxiv.org/abs/2512.21280", "authors": ["Divij Dudeja", "Mayukha Pal"], "title": "SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.", "AI": {"tldr": "SMART是一个针对工程手册处理的专用Transformer模型，通过分层结构提取事实、建立索引记忆库和融合推理，以更少的参数实现比GPT-2和BERT更高的准确性，减少幻觉并提高响应质量。", "motivation": "传统工程手册内容冗长、格式密集，现有通用Transformer模型将其视为扁平token流处理，导致数字答案错误且需要低效记忆分离事实。", "method": "采用三层结构：(1)语法感知事实提取器(Grammarian Tree LSTM)提取主谓宾关系；(2)紧凑索引记忆网络(MANN)将事实索引为384维向量；(3)6层Transformer融合检索到的事实生成响应。", "result": "SMART仅使用45.51M参数(比GPT-2少64%，比BERT少69%)，但准确率比GPT-2高21.3%，支持双模式推理：已知文档的快速路径和新上传文档的动态路径。", "conclusion": "SMART通过结构化处理工程手册内容，在减少计算需求的同时提供更准确、支持更好的结果，相比小型Transformer模型显著减少了幻觉现象。"}}
{"id": "2512.21220", "pdf": "https://arxiv.org/pdf/2512.21220", "abs": "https://arxiv.org/abs/2512.21220", "authors": ["Le Wang", "Zonghao Ying", "Xiao Yang", "Quanchen Zou", "Zhenfei Yin", "Tianlin Li", "Jian Yang", "Yaodong Yang", "Aishan Liu", "Xianglong Liu"], "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "11 pages, 6 figures", "summary": "Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.", "AI": {"tldr": "RoboSafe是一种基于可执行谓词安全逻辑的混合推理运行时安全防护系统，通过后向反思推理和前向预测推理模块，在动态环境中有效减少具身智能体的危险行为(-36.8%)，同时保持任务性能。", "motivation": "现有的具身智能体安全防护主要依赖静态规则过滤或提示级控制，难以处理动态、时序依赖和上下文丰富的环境中出现的隐式风险。", "method": "提出混合长短安全记忆的混合推理框架：后向反思推理模块持续检查近期轨迹推断时序安全谓词；前向预测推理模块通过多模态观察生成上下文感知的安全谓词。形成可解释、可执行代码的自适应安全逻辑。", "result": "在多智能体实验中显著减少危险行为(-36.8%风险发生率)，同时保持接近原始的任务性能。物理机械臂的真实世界评估证实了其实用性。", "conclusion": "RoboSafe通过混合推理方法有效解决了具身智能体在动态环境中的安全问题，提供了可验证、可执行的安全防护方案，具有实际应用价值。"}}
{"id": "2512.21323", "pdf": "https://arxiv.org/pdf/2512.21323", "abs": "https://arxiv.org/abs/2512.21323", "authors": ["Felix Draxler", "Justus Will", "Farrin Marouf Sofian", "Theofanis Karaletsos", "Sameer Singh", "Stephan Mandt"], "title": "Parallel Token Prediction for Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint. Under review", "summary": "We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.", "AI": {"tldr": "PTP框架通过在单次Transformer调用中联合预测多个依赖token，将采样过程融入模型，实现并行序列生成，显著降低自回归解码延迟，同时避免现有方法的独立性假设限制。", "motivation": "解决自回归解码的延迟瓶颈问题，克服现有多token预测方法中过于严格的独立性假设限制，实现更高效的并行序列生成。", "method": "提出并行token预测(PTP)框架，在单次transformer调用中联合预测多个依赖token；通过蒸馏现有模型或无教师的反向自回归训练进行训练；将采样过程整合到模型中。", "result": "在Vicuna-7B上实现最先进的推测解码性能，在Spec-Bench上每个步骤接受超过4个token；证明PTP能够表示任意自回归序列分布。", "conclusion": "PTP框架的普适性表明，在不损失建模能力的情况下实现长序列的并行生成是可行的，为高效语言模型推理提供了新方向。"}}
{"id": "2512.21329", "pdf": "https://arxiv.org/pdf/2512.21329", "abs": "https://arxiv.org/abs/2512.21329", "authors": ["Xinhe Wang", "Jin Huang", "Xingjian Zhang", "Tianhao Wang", "Jiaqi W. Ma"], "title": "Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.\n  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.", "AI": {"tldr": "该研究挑战了传统观点，认为ARC类推理基准的性能差距主要源于视觉感知局限而非归纳推理缺陷。通过设计两阶段实验流程分离感知和推理，发现在三个ARC风格数据集中，感知能力是性能差距的主导因素，约80%的模型失败源于感知错误。", "motivation": "传统上认为ARC基准的性能差距反映了机器推理能力的不足，但研究者怀疑这可能主要是视觉感知限制导致的，而非核心推理能力的缺陷。", "method": "引入两阶段实验流程：第一阶段将图像独立转换为自然语言描述（感知阶段），第二阶段使用这些描述进行规则归纳和应用（推理阶段）。这种方法防止了跨图像归纳信号的泄漏，并隔离了感知瓶颈。", "result": "在Mini-ARC、ACRE和Bongard-LOGO三个数据集上，两阶段流程显著优于标准端到端评估，表明感知能力是性能差距的主导因素。人工检查发现约80%的模型失败源于感知错误。", "conclusion": "ARC类基准混淆了感知和推理挑战，现有性能差距可能高估了机器推理的缺陷。评估机器智能进展时需要分离感知和推理的评估协议。"}}
{"id": "2512.21332", "pdf": "https://arxiv.org/pdf/2512.21332", "abs": "https://arxiv.org/abs/2512.21332", "authors": ["Jin Qin", "Zihan Liao", "Ziyin Zhang", "Hang Yu", "Peng Di", "Rui Wang"], "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.", "AI": {"tldr": "C2LLM是基于Qwen-2.5-Coder的代码嵌入模型家族，采用多头注意力池化模块生成序列嵌入，在MTEB-Code基准测试中创下新纪录", "motivation": "解决传统基于EOS的序列嵌入存在信息瓶颈的问题，充分利用预训练LLM的因果表示能力，同时支持嵌入维度的灵活调整", "method": "基于Qwen-2.5-Coder架构，引入Pooling by Multihead Attention (PMA)模块，从token嵌入生成序列嵌入，使用300万公开数据进行训练", "result": "在相似规模模型中创下MTEB-Code新纪录，C2LLM-7B在整体排行榜中排名第一", "conclusion": "C2LLM通过创新的PMA模块有效解决了代码嵌入中的信息瓶颈问题，在代码理解任务中表现出色，为代码大语言模型的发展提供了新方向"}}
{"id": "2512.21336", "pdf": "https://arxiv.org/pdf/2512.21336", "abs": "https://arxiv.org/abs/2512.21336", "authors": ["Ziyu Chen", "Xinbei Jiang", "Peng Sun", "Tao Lin"], "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.", "AI": {"tldr": "该论文提出了去噪熵(Denoising Entropy)概念来解决掩码扩散模型生成质量对解码顺序敏感的问题，并开发了两种基于熵的优化算法来提升生成质量。", "motivation": "掩码扩散模型(MDMs)虽然提供了灵活的非自回归生成能力，但生成质量对解码顺序高度敏感，这种可变性源于生成路径上的累积预测不确定性。", "method": "引入可计算的去噪熵指标来量化生成过程中的不确定性，并基于此提出了两种算法：后验选择方法和实时引导策略来优化解码路径。", "result": "实验表明，基于熵引导的方法显著提高了生成质量，在具有挑战性的推理、规划和代码基准测试中持续提升了准确性。", "conclusion": "去噪熵成为了理解和控制生成过程的原则性工具，有效将MDMs中的不确定性从劣势转变为发现高质量解决方案的关键优势。"}}
