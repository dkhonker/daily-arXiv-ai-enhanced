<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 60]
- [cs.AI](#cs.AI) [总数: 17]
- [stat.ML](#stat.ML) [总数: 11]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Stochastic Fractional Neural Operators: A Symmetrized Approach to Modeling Turbulence in Complex Fluid Dynamics](https://arxiv.org/abs/2505.14700)
*Rômulo Damasclin Chaves dos Santos, Jorge Henrique de Oliveira Sales*

**主要类别:** cs.LG

**概要:** This paper introduces a novel neural network operator combining symmetrized activation functions, Caputo-type fractional derivatives, and stochastic perturbations to approximate functions with long-term memory and uncertain dynamics.


<details>
  <summary>更多</summary>
  
**动机:** To address problems involving memory effects and randomness.

**方法:** Develops a new class of neural network operators integrating symmetrized activation functions, Caputo-type fractional derivatives, and stochastic perturbations.

**结果:** Proves three key Voronovskaya-type theorems describing the asymptotic behavior, mean-square convergence, and consistency under fractional regularity assumptions.

**结论:** The proposed operators provide theoretical guarantees for approximating functions with long-term memory and uncertain dynamics, suggesting potential applications in complex system analysis and simulation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stochastic+Fractional+Neural+Operators%3A+A+Symmetrized+Approach+to+Modeling+Turbulence+in+Complex+Fluid+Dynamics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14700，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14700&send_immediately=true&force_search=false)

**原文摘要:** In this work, we introduce a new class of neural network operators designed
to handle problems where memory effects and randomness play a central role. In
this work, we introduce a new class of neural network operators designed to
handle problems where memory effects and randomness play a central role. These
operators merge symmetrized activation functions, Caputo-type fractional
derivatives, and stochastic perturbations introduced via It\^o type noise. The
result is a powerful framework capable of approximating functions that evolve
over time with both long-term memory and uncertain dynamics. We develop the
mathematical foundations of these operators, proving three key theorems of
Voronovskaya type. These results describe the asymptotic behavior of the
operators, their convergence in the mean-square sense, and their consistency
under fractional regularity assumptions. All estimates explicitly account for
the influence of the memory parameter $\alpha$ and the noise level $\sigma$. As
a practical application, we apply the proposed theory to the fractional
Navier-Stokes equations with stochastic forcing, a model often used to describe
turbulence in fluid flows with memory. Our approach provides theoretical
guarantees for the approximation quality and suggests that these neural
operators can serve as effective tools in the analysis and simulation of
complex systems. By blending ideas from neural networks, fractional calculus,
and stochastic analysis, this research opens new perspectives for modeling
turbulent phenomena and other multiscale processes where memory and randomness
are fundamental. The results lay the groundwork for hybrid learning-based
methods with strong analytical backing.

</details>


### [2] [The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents](https://arxiv.org/abs/2505.14727)
*Mohammad Rubyet Islam*

**主要类别:** cs.LG

**概要:** This paper presents a five-stage taxonomy that tracks the evolution of alpha-generating systems from manual strategies to AI-powered architectures, emphasizing the shift towards context-aware financial agents. It also examines challenges related to interpretability, data, governance, and compliance.


<details>
  <summary>更多</summary>
  
**动机:** To provide a comprehensive understanding of the transformation in alpha generation from human intuition to autonomous AI systems.

**方法:** Develops a five-stage taxonomy including manual strategies, statistical models, classical machine learning, deep learning, and AI agents powered by large language models.

**结果:** Introduces a system-level approach integrating advancements in representation learning, multimodal data fusion, and tool-augmented LLM agents.

**结论:** Proposes a unified framework for assessing the maturity of alpha systems, aligning infrastructure, and promoting responsible development.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Evolution+of+Alpha+in+Finance+Harnessing+Human+Insight+and+LLM+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14727，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14727&send_immediately=true&force_search=false)

**原文摘要:** The pursuit of alpha returns that exceed market benchmarks has undergone a
profound transformation, evolving from intuition-driven investing to
autonomous, AI powered systems. This paper introduces a comprehensive five
stage taxonomy that traces this progression across manual strategies,
statistical models, classical machine learning, deep learning, and agentic
architectures powered by large language models (LLMs). Unlike prior surveys
focused narrowly on modeling techniques, this review adopts a system level
lens, integrating advances in representation learning, multimodal data fusion,
and tool augmented LLM agents. The strategic shift from static predictors to
contextaware financial agents capable of real time reasoning, scenario
simulation, and cross modal decision making is emphasized. Key challenges in
interpretability, data fragility, governance, and regulatory compliance areas
critical to production deployment are examined. The proposed taxonomy offers a
unified framework for evaluating maturity, aligning infrastructure, and guiding
the responsible development of next generation alpha systems.

</details>


### [3] [The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute](https://arxiv.org/abs/2505.14733)
*Yunho Jin, Gu-Yeon Wei, David Brooks*

**主要类别:** cs.LG

**概要:** This paper introduces test-time compute (TTC) for large language models, showing it improves accuracy-energy trade-offs over traditional model scaling, especially for complex reasoning tasks.


<details>
  <summary>更多</summary>
  
**动机:** Traditional scaling strategies face diminishing returns and high energy demands.

**方法:** Investigates the use of TTC to allocate additional computational resources during inference.

**结果:** TTC surpasses traditional scaling in accuracy/energy efficiency, particularly in complex reasoning tasks.

**结论:** TTC is a promising approach for deploying more sustainable, accurate, and adaptable language models without extra pretraining costs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Energy+Cost+of+Reasoning%3A+Analyzing+Energy+Usage+in+LLMs+with+Test-time+Compute，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14733，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14733&send_immediately=true&force_search=false)

**原文摘要:** Scaling large language models (LLMs) has driven significant advancements, yet
it faces diminishing returns and escalating energy demands. This work
introduces test-time compute (TTC)-allocating additional computational
resources during inference-as a compelling complement to conventional scaling
strategies. Specifically, we investigate whether employing TTC can achieve
superior accuracy-energy trade-offs compared to simply increasing model size.
Our empirical analysis reveals that TTC surpasses traditional model scaling in
accuracy/energy efficiency, with notable gains in tasks demanding complex
reasoning rather than mere factual recall. Further, we identify a critical
interaction between TTC performance and output sequence length, demonstrating
that strategically adjusting compute resources at inference time according to
query complexity can substantially enhance efficiency. Our findings advocate
for TTC as a promising direction, enabling more sustainable, accurate, and
adaptable deployment of future language models without incurring additional
pretraining costs.

</details>


### [4] [Leveraging Multivariate Long-Term History Representation for Time Series Forecasting](https://arxiv.org/abs/2505.14737)
*Huiliang Zhang, Di Wu, Arnaud Zinflou, Stephane Dellacherie, Mouhamadou Makhtar Dione, Benoit Boulet*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的STGNN框架，用于多变量时间序列预测，该框架通过整合长期历史信息来提高预测性能。


<details>
  <summary>更多</summary>
  
**动机:** 大多数STGNNs主要关注短期和局部时空依赖性，而忽略长期时空相似性和多变量时间序列（MTS）之间的相关性。

**方法:** 提出的框架包括一个长期历史编码器（LHEncoder），一个非参数层次表示检索器（HRetriever）和一个基于Transformer的聚合器（TAggregator）。

**结果:** 实验结果表明，所提出的方法在平均预测范围上比典型的STGNNs高出10.72％，在几个真实世界的数据集上比最先进的方法高出4.12％。

**结论:** 提出的方法在多个真实数据集上比典型STGNN和最先进的方法表现更好。此外，在数据集中快速变化模式的前10％的预测准确性提高了9.8％。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Multivariate+Long-Term+History+Representation+for+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14737&send_immediately=true&force_search=false)

**原文摘要:** Multivariate Time Series (MTS) forecasting has a wide range of applications
in both industry and academia. Recent advances in Spatial-Temporal Graph Neural
Network (STGNN) have achieved great progress in modelling spatial-temporal
correlations. Limited by computational complexity, most STGNNs for MTS
forecasting focus primarily on short-term and local spatial-temporal
dependencies. Although some recent methods attempt to incorporate univariate
history into modeling, they still overlook crucial long-term spatial-temporal
similarities and correlations across MTS, which are essential for accurate
forecasting. To fill this gap, we propose a framework called the Long-term
Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.
Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively
encode the long-term history into segment-level contextual representations and
reduce point-level noise. A non-parametric Hierarchical Representation
Retriever (HRetriever) is designed to include the spatial information in the
long-term spatial-temporal dependency modelling and pick out the most valuable
representations with no additional training. A Transformer-based Aggregator
(TAggregator) selectively fuses the sparsely retrieved contextual
representations based on the ranking positional embedding efficiently.
Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%
on the average prediction horizons and state-of-the-art methods by 4.12% on
several real-world datasets. Additionally, it consistently improves prediction
accuracy by 9.8% on the top 10% of rapidly changing patterns across the
datasets.

</details>


### [5] [Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs](https://arxiv.org/abs/2505.14739)
*Heiko Oppel, Andreas Spilz, Michael Munz*

**主要类别:** cs.LG

**概要:** 提出一种改进的相似性度量方法来评估去噪扩散概率模型生成的数据质量，并通过微调该度量以适应分类任务需求，从而显著减少训练周期数且不降低分类性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决去噪扩散概率模型生成数据质量难以估计的问题，提供一种监控和优化训练及数据合成过程的方法。

**方法:** 引入并调整现有的相似性度量方法，使其能够更好地适应分类任务需求，同时减少训练周期数。

**结果:** 提出的改进度量方法能够有效评估生成数据的质量，并在不降低分类任务性能的情况下显著减少训练周期数。

**结论:** 优化后的训练过程不仅节省资源，还减少了生成模型的训练时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time+Series+Similarity+Score+Functions+to+Monitor+and+Interact+with+the+Training+and+Denoising+Process+of+a+Time+Series+Diffusion+Model+applied+to+a+Human+Activity+Recognition+Dataset+based+on+IMUs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14739，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14739&send_immediately=true&force_search=false)

**原文摘要:** Denoising diffusion probabilistic models are able to generate synthetic
sensor signals. The training process of such a model is controlled by a loss
function which measures the difference between the noise that was added in the
forward process and the noise that was predicted by the diffusion model. This
enables the generation of realistic data. However, the randomness within the
process and the loss function itself makes it difficult to estimate the quality
of the data. Therefore, we examine multiple similarity metrics and adapt an
existing metric to overcome this issue by monitoring the training and
synthetisation process using those metrics. The adapted metric can even be
fine-tuned on the input data to comply with the requirements of an underlying
classification task. We were able to significantly reduce the amount of
training epochs without a performance reduction in the classification task. An
optimized training process not only saves resources, but also reduces the time
for training generative models.

</details>


### [6] [Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism](https://arxiv.org/abs/2505.14741)
*Kunyun Wang, Bohan Li, Kai Yu, Minyi Guo, Jieru Zhao*

**主要类别:** cs.LG

**概要:** 提出了一种名为 ParaStep 的新方法，通过优化通信减少扩散模型推理的延迟，显著提高了多种任务的速度，同时保持生成质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的并行化策略通常会带来高通信开销，限制了扩散模型在商业硬件上的部署。

**方法:** ParaStep 基于重用-然后预测机制，通过利用相邻去噪步骤之间的相似性来并行化扩散推理，并采用轻量级的步骤间通信，大幅减少了开销。

**结果:** ParaStep 在 SVD 上实现了高达 3.88 倍的速度提升，在 CogVideoX-2b 上实现了 2.43 倍，在 AudioLDM2-large 上实现了 6.56 倍，同时保持生成质量。

**结论:** ParaStep 提出了一种新的并行化方法，通过重用-然后预测机制来加速扩散推理，特别是在带宽受限的环境中表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Communication-Efficient+Diffusion+Denoising+Parallelization+via+Reuse-then-Predict+Mechanism，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14741，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14741&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models have emerged as a powerful class of generative models across
various modalities, including image, video, and audio synthesis. However, their
deployment is often limited by significant inference latency, primarily due to
the inherently sequential nature of the denoising process. While existing
parallelization strategies attempt to accelerate inference by distributing
computation across multiple devices, they typically incur high communication
overhead, hindering deployment on commercial hardware. To address this
challenge, we propose \textbf{ParaStep}, a novel parallelization method based
on a reuse-then-predict mechanism that parallelizes diffusion inference by
exploiting similarity between adjacent denoising steps. Unlike prior approaches
that rely on layer-wise or stage-wise communication, ParaStep employs
lightweight, step-wise communication, substantially reducing overhead. ParaStep
achieves end-to-end speedups of up to \textbf{3.88}$\times$ on SVD,
\textbf{2.43}$\times$ on CogVideoX-2b, and \textbf{6.56}$\times$ on
AudioLDM2-large, while maintaining generation quality. These results highlight
ParaStep as a scalable and communication-efficient solution for accelerating
diffusion inference, particularly in bandwidth-constrained environments.

</details>


### [7] [Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis](https://arxiv.org/abs/2505.14742)
*Hong Huang, Dapeng Wu*

**主要类别:** cs.LG

**概要:** 提出了一种名为Quaff的量化高效微调框架，通过目标动量缩放优化低精度激活表示，在保证性能的同时减少了计算和内存开销。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在平衡量化微调中的性能与开销方面存在困难，要么计算/内存成本高，要么无法解决激活异常值这一关键瓶颈。

**方法:** 基于提出的Outlier Spatial Stability Hypothesis (OSSH)，Quaff框架通过轻量级操作动态抑制不变通道中的异常值，并减少量化误差。

**结果:** 在十个基准数据集上的广泛实验验证了OSSH的有效性，并展示了Quaff框架的效能，例如在GPQA推理基准上实现了1.73倍的延迟减少和30%的内存节省。

**结论:** Quaff使得消费级GPU（如RTX 2080 Super）上的个人化大型语言模型部署成为可能，同时保持了模型的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Quaff%3A+Quantized+Parameter-Efficient+Fine-Tuning+under+Outlier+Spatial+Stability+Hypothesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14742&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have made exciting achievements across various
domains, yet their deployment on resource-constrained personal devices remains
hindered by the prohibitive computational and memory demands of task-specific
fine-tuning. While quantization offers a pathway to efficiency, existing
methods struggle to balance performance and overhead, either incurring high
computational/memory costs or failing to address activation outliers, a
critical bottleneck in quantized fine-tuning. To address these challenges, we
propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,
certain activation outlier channels retain stable spatial positions across
training iterations. Building on OSSH, we propose Quaff, a Quantized
parameter-efficient fine-tuning framework for LLMs, optimizing low-precision
activation representations through targeted momentum scaling. Quaff dynamically
suppresses outliers exclusively in invariant channels using lightweight
operations, eliminating full-precision weight storage and global rescaling
while reducing quantization errors. Extensive experiments across ten benchmarks
validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA
reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory
savings over full-precision fine-tuning while improving accuracy by 0.6% on the
Phi-3 model, reconciling the triple trade-off between efficiency, performance,
and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080
Super) without sacrificing model utility, Quaff democratizes personalized LLM
deployment. The code is available at https://github.com/Little0o0/Quaff.git.

</details>


### [8] [Explainable Prediction of the Mechanical Properties of Composites with CNNs](https://arxiv.org/abs/2505.14745)
*Varun Raaghav, Dimitrios Bikos, Antonio Rago, Francesca Toni, Maria Charalambides*

**主要类别:** cs.LG

**概要:** 本文提出了一种基于卷积神经网络和可解释AI方法的新框架，用于预测复合材料的机械性能，该方法比现有技术更准确且更具透明性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的有限元建模在评估复合材料机械性能时计算成本高；已有的AI模型架构简单，准确度有限，且缺乏透明性，难以获得用户信任。

**方法:** 使用自定义的CNNs，训练集由有限元建模中的横拉测试数据生成，用于预测复合材料的杨氏模量和屈服强度。使用SHAP和Integrated Gradients两种后验XAI方法解释预测。

**结果:** 提出的CNNs方法在估计机械性能方面表现出高准确性，并优于基准模型ResNet-34。通过XAI方法证明CNNs使用影响复合材料行为的关键几何特征。

**结论:** 证明了配备可解释AI(XAI)方法的卷积神经网络(CNNs)可以成功解决复合材料机械性能预测的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Explainable+Prediction+of+the+Mechanical+Properties+of+Composites+with+CNNs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14745&send_immediately=true&force_search=false)

**原文摘要:** Composites are amongst the most important materials manufactured today, as
evidenced by their use in countless applications. In order to establish the
suitability of composites in specific applications, finite element (FE)
modelling, a numerical method based on partial differential equations, is the
industry standard for assessing their mechanical properties. However, FE
modelling is exceptionally costly from a computational viewpoint, a limitation
which has led to efforts towards applying AI models to this task. However, in
these approaches: the chosen model architectures were rudimentary, feed-forward
neural networks giving limited accuracy; the studies focus on predicting
elastic mechanical properties, without considering material strength limits;
and the models lacked transparency, hindering trustworthiness by users. In this
paper, we show that convolutional neural networks (CNNs) equipped with methods
from explainable AI (XAI) can be successfully deployed to solve this problem.
Our approach uses customised CNNs trained on a dataset we generate using
transverse tension tests in FE modelling to predict composites' mechanical
properties, i.e., Young's modulus and yield strength. We show empirically that
our approach achieves high accuracy, outperforming a baseline, ResNet-34, in
estimating the mechanical properties. We then use SHAP and Integrated
Gradients, two post-hoc XAI methods, to explain the predictions, showing that
the CNNs use the critical geometrical features that influence the composites'
behaviour, thus allowing engineers to verify that the models are trustworthy by
representing the science of composites.

</details>


### [9] [Cooperative Causal GraphSAGE](https://arxiv.org/abs/2505.14748)
*Zaifa Xue, Tao Zhang, Tuo Xu, Huaixin Liang, Le Gao*

**主要类别:** cs.LG

**概要:** This paper introduces Cooperative Causal GraphSAGE (CoCa-GraphSAGE), which enhances the robustness of Causal GraphSAGE by integrating cooperative game theory and considering cooperative relationships among sampling nodes.


<details>
  <summary>更多</summary>
  
**动机:** To improve the robustness of GraphSAGE and address the limitation of Causal GraphSAGE in neglecting cooperative relationships among sampling nodes.

**方法:** Combining cooperative game theory with Causal GraphSAGE to construct a cooperative causal structure model and proposing Cooperative Causal sampling (CoCa-sampling) algorithm using Shapley values.

**结果:** Experiments show that the proposed method has comparable classification performance to compared methods and outperforms under perturbations, demonstrating improved robustness.

**结论:** CoCa-GraphSAGE improves the robustness of GraphSAGE by considering cooperative relationships among sampling nodes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cooperative+Causal+GraphSAGE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14748，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14748&send_immediately=true&force_search=false)

**原文摘要:** GraphSAGE is a widely used graph neural network. The introduction of causal
inference has improved its robust performance and named as Causal GraphSAGE.
However, Causal GraphSAGE focuses on measuring causal weighting among
individual nodes, but neglecting the cooperative relationships among sampling
nodes as a whole. To address this issue, this paper proposes Cooperative Causal
GraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal
GraphSAGE. Initially, a cooperative causal structure model is constructed in
the case of cooperation based on the graph structure. Subsequently, Cooperative
Causal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley
values to calculate the cooperative contribution based on causal weights of the
nodes sets. CoCa-sampling guides the selection of nodes with significant
cooperative causal effects during the neighborhood sampling process, thus
integrating the selected neighborhood features under cooperative relationships,
which takes the sampled nodes as a whole and generates more stable target node
embeddings. Experiments on publicly available datasets show that the proposed
method has comparable classification performance to the compared methods and
outperforms under perturbations, demonstrating the robustness improvement by
CoCa-sampling.

</details>


### [10] [Self Distillation via Iterative Constructive Perturbations](https://arxiv.org/abs/2505.14751)
*Maheak Dave, Aniket Kumar Singh, Aryan Pareek, Harshita Jha, Debasis Chaudhuri, Manish Pratap Singh*

**主要类别:** cs.LG

**概要:** This paper introduces a novel framework using a cyclic optimization strategy to improve deep neural network training by iteratively perturbing input data and applying self-distillation, showing enhanced performance and generalization.


<details>
  <summary>更多</summary>
  
**动机:** Balancing performance and generalization in training deep neural networks remains challenging.

**方法:** Proposes a cyclic optimization strategy including Iterative Constructive Perturbation (ICP) to perturb input data and uses self-distillation to refine features.

**结果:** The approach mitigates performance bottlenecks and improves performance across different training variations.

**结论:** The proposed method effectively addresses the gap between fitting and generalization in neural network training.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self+Distillation+via+Iterative+Constructive+Perturbations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14751，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14751&send_immediately=true&force_search=false)

**原文摘要:** Deep Neural Networks have achieved remarkable achievements across various
domains, however balancing performance and generalization still remains a
challenge while training these networks. In this paper, we propose a novel
framework that uses a cyclic optimization strategy to concurrently optimize the
model and its input data for better training, rethinking the traditional
training paradigm. Central to our approach is Iterative Constructive
Perturbation (ICP), which leverages the model's loss to iteratively perturb the
input, progressively constructing an enhanced representation over some
refinement steps. This ICP input is then fed back into the model to produce
improved intermediate features, which serve as a target in a self-distillation
framework against the original features. By alternately altering the model's
parameters to the data and the data to the model, our method effectively
addresses the gap between fitting and generalization, leading to enhanced
performance. Extensive experiments demonstrate that our approach not only
mitigates common performance bottlenecks in neural networks but also
demonstrates significant improvements across training variations.

</details>


### [11] [Large Language Models for Data Synthesis](https://arxiv.org/abs/2505.14752)
*Yihong Tang, Menglin Kong, Lijun Sun*

**主要类别:** cs.LG

**概要:** 提出LLMSynthor框架，利用大语言模型生成高保真合成数据，适用于隐私敏感领域。


<details>
  <summary>更多</summary>
  
**动机:** 传统方法在高维或异构域中表现不佳，标准LLM采样效率低且无法保证统计一致性。

**方法:** 将LLM作为非参数copula模拟器，并引入LLM提议采样以提高效率，通过分布反馈使合成数据与真实数据对齐。

**结果:** 在多个真实世界和控制环境中测试，合成数据具有高统计保真度和实用价值。

**结论:** LLMSynthor是一种有价值的工具，可用于经济学、社会学等多个领域。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Large+Language+Models+for+Data+Synthesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14752，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14752&send_immediately=true&force_search=false)

**原文摘要:** Generating synthetic data that faithfully captures the statistical structure
of real-world distributions is a fundamental challenge in data modeling.
Classical approaches often depend on strong parametric assumptions or manual
structural design and struggle in high-dimensional or heterogeneous domains.
Recent progress in Large Language Models (LLMs) reveals their potential as
flexible, high-dimensional priors over real-world distributions. However, when
applied to data synthesis, standard LLM-based sampling is inefficient,
constrained by fixed context limits, and fails to ensure statistical alignment.
Given this, we introduce LLMSynthor, a general framework for data synthesis
that transforms LLMs into structure-aware simulators guided by distributional
feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for
modeling high-order dependencies and introduces LLM Proposal Sampling to
generate grounded proposal distributions that improve sampling efficiency
without requiring rejection. By minimizing discrepancies in the summary
statistics space, the iterative synthesis loop aligns real and synthetic data
while gradually uncovering and refining the latent generative structure. We
evaluate LLMSynthor in both controlled and real-world settings using
heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,
population, and mobility) that encompass both structured and unstructured
formats. The synthetic data produced by LLMSynthor shows high statistical
fidelity, practical utility, and cross-data adaptability, positioning it as a
valuable tool across economics, social science, urban studies, and beyond.

</details>


### [12] [Assimilative Causal Inference](https://arxiv.org/abs/2505.14825)
*Marios Andreou, Nan Chen, Erik Bollt*

**主要类别:** cs.LG

**概要:** 本文提出了一种名为ACI的新因果推理框架，该框架解决了传统方法的局限性，能够处理短时间序列和不完整数据集，并在复杂动力系统中展示其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的时间序列方法通常只能在时间平均的意义上揭示因果链，而基于集合的信息传递方法虽然可以检测短期因果关系的时间演化，但通常局限于低维系统。因此，开发了一种新的因果推理框架ACI。

**方法:** ACI使用一个动力系统和状态变量子集的一个单一实现来识别瞬时因果关系及其相关的因果影响范围（CIR）的动态演化。它通过贝叶斯数据同化解决逆问题，从而从观察到的效果追溯原因。

**结果:** ACI能够捕获变量之间的动态相互作用，其数学上合理的客观标准确定了CIR，而不需要经验阈值。ACI可以扩展到高维问题，并适用于短时间序列和不完整数据集。ACI不依赖于候选原因的观测，这是一个关键优势，因为潜在驱动因素通常是未知或未测量的。

**结论:** ACI通过贝叶斯数据同化解决逆问题来确定因果关系，能够捕获变量之间的动态相互作用，并且可以通过利用计算高效的贝叶斯数据同化技术扩展到高维问题。ACI在复杂动力系统中展示了其有效性，这些系统展示了间歇性和极端事件。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assimilative+Causal+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14825，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14825&send_immediately=true&force_search=false)

**原文摘要:** Causal inference determines cause-and-effect relationships between variables
and has broad applications across disciplines. Traditional time-series methods
often reveal causal links only in a time-averaged sense, while ensemble-based
information transfer approaches detect the time evolution of short-term causal
relationships but are typically limited to low-dimensional systems. In this
paper, a new causal inference framework, called assimilative causal inference
(ACI), is developed. Fundamentally different from the state-of-the-art methods,
ACI uses a dynamical system and a single realization of a subset of the state
variables to identify instantaneous causal relationships and the dynamic
evolution of the associated causal influence range (CIR). Instead of
quantifying how causes influence effects as done traditionally, ACI solves an
inverse problem via Bayesian data assimilation, thus tracing causes backward
from observed effects with an implicit Bayesian hypothesis. Causality is
determined by assessing whether incorporating the information of the effect
variables reduces the uncertainty in recovering the potential cause variables.
ACI has several desirable features. First, it captures the dynamic interplay of
variables, where their roles as causes and effects can shift repeatedly over
time. Second, a mathematically justified objective criterion determines the CIR
without empirical thresholds. Third, ACI is scalable to high-dimensional
problems by leveraging computationally efficient Bayesian data assimilation
techniques. Finally, ACI applies to short time series and incomplete datasets.
Notably, ACI does not require observations of candidate causes, which is a key
advantage since potential drivers are often unknown or unmeasured. The
effectiveness of ACI is demonstrated by complex dynamical systems showcasing
intermittency and extreme events.

</details>


### [13] [$\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization](https://arxiv.org/abs/2505.14756)
*Chih-Yu Chang, Milad Azvar, Chinedum Okwudire, Raed Al Kontar*

**主要类别:** cs.LG

**概要:** This paper presents LLINBO, a hybrid Bayesian optimization framework combining Large Language Models with statistical surrogate models like Gaussian Processes for better exploration and exploitation.


<details>
  <summary>更多</summary>
  
**动机:** To improve Bayesian optimization by using LLMs for contextual reasoning in exploration while maintaining reliability with statistical models for exploitation.

**方法:** Introduces LLINBO which integrates LLMs and statistical surrogate experts, with three mechanisms ensuring theoretical guarantees.

**结果:** Demonstrates the effectiveness of LLINBO through a real-life application in 3D printing and provides reproducible code.

**结论:** LLINBO offers a reliable and efficient approach for black-box function optimization by blending the strengths of LLMs and statistical models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%24%5Ctexttt%7BLLINBO%7D%24%3A+Trustworthy+LLM-in-the-Loop+Bayesian+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14756，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14756&send_immediately=true&force_search=false)

**原文摘要:** Bayesian optimization (BO) is a sequential decision-making tool widely used
for optimizing expensive black-box functions. Recently, Large Language Models
(LLMs) have shown remarkable adaptability in low-data regimes, making them
promising tools for black-box optimization by leveraging contextual knowledge
to propose high-quality query points. However, relying solely on LLMs as
optimization agents introduces risks due to their lack of explicit surrogate
modeling and calibrated uncertainty, as well as their inherently opaque
internal mechanisms. This structural opacity makes it difficult to characterize
or control the exploration-exploitation trade-off, ultimately undermining
theoretical tractability and reliability. To address this, we propose LLINBO:
LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with
statistical surrogate experts (e.g., Gaussian Processes (GP)). The core
philosophy is to leverage contextual reasoning strengths of LLMs for early
exploration, while relying on principled statistical models to guide efficient
exploitation. Specifically, we introduce three mechanisms that enable this
collaboration and establish their theoretical guarantees. We end the paper with
a real-life proof-of-concept in the context of 3D printing. The code to
reproduce the results can be found at
https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.

</details>


### [14] [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/abs/2505.14826)
*Rohan Deb, Kiran Thekumparampil, Kousha Kalantari, Gaurush Hiranandani, Shoham Sabach, Branislav Kveton*

**主要类别:** cs.LG

**概要:** 提出一种通过选择信息量最大的训练样本子集来提高监督微调统计效率的方法。


<details>
  <summary>更多</summary>
  
**动机:** 提高监督微调方法在固定训练样本预算下的统计效率。

**方法:** 通过最大化对数似然函数的Hessian来选择信息量最大的训练样本，并用多元逻辑回归模型近似。

**结果:** 在多个问题上表现出色，提供了定量结果和专家评估支持。

**结论:** 所提方法计算高效、可分析且实证表现良好。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FisherSFT%3A+Data-Efficient+Supervised+Fine-Tuning+of+Language+Models+Using+Information+Gain，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14826，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14826&send_immediately=true&force_search=false)

**原文摘要:** Supervised fine-tuning (SFT) is a standard approach to adapting large
language models (LLMs) to new domains. In this work, we improve the statistical
efficiency of SFT by selecting an informative subset of training examples.
Specifically, for a fixed budget of training examples, which determines the
computational cost of fine-tuning, we determine the most informative ones. The
key idea in our method is to select examples that maximize information gain,
measured by the Hessian of the log-likelihood of the LLM. We approximate it
efficiently by linearizing the LLM at the last layer using multinomial logistic
regression models. Our approach is computationally efficient, analyzable, and
performs well empirically. We demonstrate this on several problems, and back
our claims with both quantitative results and an LLM evaluation.

</details>


### [15] [Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding](https://arxiv.org/abs/2505.14765)
*Orhun Vural, Bunyamin Ozaydin, Khalid Y. Aram, James Booth, Brittany F. Lindsey, Abdulaziz Ahmed*

**主要类别:** cs.LG

**概要:** 研究开发了深度学习模型，利用非临床数据提前六小时预测急诊科候诊人数，以支持主动决策，N-BEATSx模型表现最佳。


<details>
  <summary>更多</summary>
  
**动机:** 支持主动运营决策制定，仅使用非临床、运营和上下文特征。

**方法:** 使用来自五个来源的数据，包括急诊科跟踪系统、住院患者人口记录、天气报告、联邦节假日日历和本地活动时间表，并使用Optuna和网格搜索进行超参数调整训练了多个时间序列深度学习模型。

**结果:** N-BEATSx模型在平均急诊科候诊计数为28.7，标准差为11.2的情况下表现最佳，其平均绝对误差为2.10，均方误差为7.08，均方根误差为2.66，确定系数为0.95。即使在极端高候诊计数期间，模型也保持了稳定的准确性。

**结论:** 该框架为医院系统提供了一种实用且可推广的方法来预测候诊室水平并帮助缓解急诊室过度拥挤的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Learning-Based+Forecasting+of+Boarding+Patient+Counts+to+Address+ED+Overcrowding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14765，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14765&send_immediately=true&force_search=false)

**原文摘要:** This study develops deep learning models to forecast the number of patients
in the emergency department (ED) boarding phase six hours in advance, aiming to
support proactive operational decision-making using only non-clinical,
operational, and contextual features. Data were collected from five sources: ED
tracking systems, inpatient census records, weather reports, federal holiday
calendars, and local event schedules. After feature engineering, the data were
aggregated at an hourly level, cleaned, and merged into a unified dataset for
model training. Several time series deep learning models, including ResNetPlus,
TSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using
Optuna and grid search for hyperparameter tuning. The average ED boarding count
was 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best
performance, with a mean absolute error of 2.10, mean squared error of 7.08,
root mean squared error of 2.66, and a coefficient of determination of 0.95.
The model maintained stable accuracy even during periods of extremely high
boarding counts, defined as values exceeding one, two, or three standard
deviations above the mean. Results show that accurate six-hour-ahead forecasts
are achievable without using patient-level clinical data. While strong
performance was observed even with a basic feature set, the inclusion of
additional features improved prediction stability under extreme conditions.
This framework offers a practical and generalizable approach for hospital
systems to anticipate boarding levels and help mitigate ED overcrowding.

</details>


### [16] [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
*Eric Hanchen Jiang, Haozheng Luo, Shengyuan Pang, Xiaomin Li, Zhenting Qi, Hengli Li, Cheng-Fu Yang, Zongyu Lin, Xinfeng Li, Hao Xu, Kai-Wei Chang, Ying Nian Wu*

**主要类别:** cs.LG

**概要:** This paper presents EORM, a lightweight verifier that improves the reliability of mathematical reasoning in large language models by leveraging energy-based models and outcome labels.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of mathematical reasoning in LLMs, which requires robust multi-step logical consistency, and to improve the reliability of Chain of Thought prompting without extensive computational cost.

**方法:** Introducing EORM, which uses energy-based models to assign scalar energy scores to CoT solutions based on outcome labels, avoiding detailed annotations.

**结果:** EORM significantly improves final answer accuracy on mathematical benchmarks like GSM8k and MATH.

**结论:** EORM is an effective, lightweight post hoc verifier that enhances the reliability of LLM reasoning outcomes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+to+Rank+Chain-of-Thought%3A+An+Energy-Based+Approach+with+Outcome+Supervision，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14999，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14999&send_immediately=true&force_search=false)

**原文摘要:** Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs), often requiring robust multi step logical consistency. While
Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee
correctness, and improving reliability via extensive sampling is
computationally costly. This paper introduces the Energy Outcome Reward Model
(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy
Based Models (EBMs) to simplify the training of reward models by learning to
assign a scalar energy score to CoT solutions using only outcome labels,
thereby avoiding detailed annotations. It achieves this by interpreting
discriminator output logits as negative energies, effectively ranking
candidates where lower energy is assigned to solutions leading to correct final
outcomes implicitly favoring coherent reasoning. On mathematical benchmarks
(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with
Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively
leverages a given pool of candidate solutions to match or exceed the
performance of brute force sampling, thereby enhancing LLM reasoning outcome
reliability through its streamlined post hoc verification process.

</details>


### [17] [This Time is Different: An Observability Perspective on Time Series Foundation Models](https://arxiv.org/abs/2505.14766)
*Ben Cohen, Emaad Khwaja, Youssef Doubli, Salahidine Lemaachi, Chris Lettieri, Charles Masson, Hugo Miccinilli, Elise Ramé, Qiqi Ren, Afshin Rostamizadeh, Jean Ogier du Terrail, Anna-Monica Toon, Kan Wang, Stephan Xie, David Asker, Ameet Talwalkar, Othmane Abou-Amal*

**主要类别:** cs.LG

**概要:** 我们介绍了Toto，这是一个具有1.51亿参数的时间序列预测基础模型，它在多个基准测试中表现优异，并且所有相关资源都是开源的。


<details>
  <summary>更多</summary>
  
**动机:** 介绍了一个新的时间序列预测基础模型Toto，它具有1.51亿个参数，并且其预训练语料库比领先的时序基础模型大4-10倍。

**方法:** Toto采用了一种现代的解码器-只架构，并结合了针对多变量可观察时间序列数据中特定挑战设计的架构创新。

**结果:** Toto在BOOM和现有的通用时序预测基准上都取得了最先进的性能。

**结论:** Toto实现了最先进的性能，并且其模型权重、推理代码和评估脚本以及BOOM的数据和评估代码都作为开源发布。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是This+Time+is+Different%3A+An+Observability+Perspective+on+Time+Series+Foundation+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14766，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14766&send_immediately=true&force_search=false)

**原文摘要:** We introduce Toto, a time series forecasting foundation model with 151
million parameters. Toto uses a modern decoder-only architecture coupled with
architectural innovations designed to account for specific challenges found in
multivariate observability time series data. Toto's pre-training corpus is a
mixture of observability data, open datasets, and synthetic data, and is
4-10$\times$ larger than those of leading time series foundation models.
Additionally, we introduce BOOM, a large-scale benchmark consisting of 350
million observations across 2,807 real-world time series. For both Toto and
BOOM, we source observability data exclusively from Datadog's own telemetry and
internal observability metrics. Extensive evaluations demonstrate that Toto
achieves state-of-the-art performance on both BOOM and on established general
purpose time series forecasting benchmarks. Toto's model weights, inference
code, and evaluation scripts, as well as BOOM's data and evaluation code, are
all available as open source under the Apache 2.0 License available at
https://huggingface.co/Datadog/Toto-Open-Base-1.0 and
https://github.com/DataDog/toto.

</details>


### [18] [Know When to Abstain: Optimal Selective Classification with Likelihood Ratios](https://arxiv.org/abs/2505.15008)
*Alvin Heng, Harold Soh*

**主要类别:** cs.LG

**概要:** This work applies the Neyman-Pearson lemma to selective classification, unifying existing methods and introducing new approaches that perform better under covariate shift.


<details>
  <summary>更多</summary>
  
**动机:** To enhance the reliability of predictive models by developing improved selective classification methods, especially under covariate shift.

**方法:** Revisiting the design of optimal selection functions using the Neyman-Pearson lemma and proposing new approaches based on this perspective.

**结果:** The proposed methods outperform existing baselines across various vision and language tasks.

**结论:** Likelihood ratio-based selection provides a robust mechanism for improving selective classification under covariate shifts.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Know+When+to+Abstain%3A+Optimal+Selective+Classification+with+Likelihood+Ratios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15008&send_immediately=true&force_search=false)

**原文摘要:** Selective classification enhances the reliability of predictive models by
allowing them to abstain from making uncertain predictions. In this work, we
revisit the design of optimal selection functions through the lens of the
Neyman--Pearson lemma, a classical result in statistics that characterizes the
optimal rejection rule as a likelihood ratio test. We show that this
perspective not only unifies the behavior of several post-hoc selection
baselines, but also motivates new approaches to selective classification which
we propose here. A central focus of our work is the setting of covariate shift,
where the input distribution at test time differs from that at training. This
realistic and challenging scenario remains relatively underexplored in the
context of selective classification. We evaluate our proposed methods across a
range of vision and language tasks, including both supervised learning and
vision-language models. Our experiments demonstrate that our
Neyman--Pearson-informed methods consistently outperform existing baselines,
indicating that likelihood ratio-based selection offers a robust mechanism for
improving selective classification under covariate shifts. Our code is publicly
available at https://github.com/clear-nus/sc-likelihood-ratios.

</details>


### [19] [KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches](https://arxiv.org/abs/2505.14777)
*Mingquan Feng, Yixin Huang, Yifan Fu, Shaobo Wang, Junchi Yan*

**主要类别:** cs.LG

**概要:** 提出了一种基于动力学理论和偏微分方程的新型神经网络优化器KO，该方法通过模拟粒子系统的演化来重新定义网络参数的训练动态，并通过数值方案模拟参数更新。实验表明KO在图像分类和文本分类任务上优于Adam和SGD等基线优化器。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大多数神经网络优化算法依赖于基于梯度的启发式调整方法，存在参数凝聚问题，即网络参数坍塌到低维子空间的现象。

**方法:** 受动力学理论和偏微分方程的启发，设计了一种名为KO的新型优化器，通过模拟玻尔兹曼输运方程（BTE）中的随机粒子碰撞来模拟参数更新。

**结果:** 在CIFAR-10/100、ImageNet、IMDB和Snips等任务上的广泛实验证明，KO在保持计算成本相当的情况下，始终优于Adam和SGD等基线优化器，提高了准确性。

**结论:** 提出的KO优化器通过物理驱动的方法促进了参数多样性，缓解了参数凝聚现象，且在多个任务上表现优异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KO%3A+Kinetics-inspired+Neural+Optimizer+with+PDE+Simulation+Approaches，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14777，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14777&send_immediately=true&force_search=false)

**原文摘要:** The design of optimization algorithms for neural networks remains a critical
challenge, with most existing methods relying on heuristic adaptations of
gradient-based approaches. This paper introduces KO (Kinetics-inspired
Optimizer), a novel neural optimizer inspired by kinetic theory and partial
differential equation (PDE) simulations. We reimagine the training dynamics of
network parameters as the evolution of a particle system governed by kinetic
principles, where parameter updates are simulated via a numerical scheme for
the Boltzmann transport equation (BTE) that models stochastic particle
collisions. This physics-driven approach inherently promotes parameter
diversity during optimization, mitigating the phenomenon of parameter
condensation, i.e. collapse of network parameters into low-dimensional
subspaces, through mechanisms analogous to thermal diffusion in physical
systems. We analyze this property, establishing both a mathematical proof and a
physical interpretation. Extensive experiments on image classification
(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks
demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,
SGD), achieving accuracy improvements while computation cost remains
comparable.

</details>


### [20] [Generalization Through Growth: Hidden Dynamics Controls Depth Dependence](https://arxiv.org/abs/2505.15064)
*Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架来分析深度神经网络的泛化界限，该框架适用于伪度量空间，并揭示了几何二分法背后的深度贡献。


<details>
  <summary>更多</summary>
  
**动机:** 现有理论的泛化界限与特定架构和欧几里得输入相关联，需要一种更通用的方法。

**方法:** 提出了一个统一的框架，用于任意伪度量空间中的深度-k网络，并推导出一个新的泛化界限。

**结果:** 得到了一个新的泛化界限，揭示了深度贡献的几何二分法，并提供了覆盖数估计。

**结论:** 结果表明新框架可以提供与架构无关且与动态系统相关的保证，适用于现代深度学习范式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalization+Through+Growth%3A+Hidden+Dynamics+Controls+Depth+Dependence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15064，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15064&send_immediately=true&force_search=false)

**原文摘要:** Recent theory has reduced the depth dependence of generalization bounds from
exponential to polynomial and even depth-independent rates, yet these results
remain tied to specific architectures and Euclidean inputs. We present a
unified framework for arbitrary \blue{pseudo-metric} spaces in which a
depth-\(k\) network is the composition of continuous hidden maps
\(f:\mathcal{X}\to \mathcal{X}\) and an output map \(h:\mathcal{X}\to
\mathbb{R}\). The resulting bound $O(\sqrt{(\alpha + \log \beta(k))/n})$
isolates the sole depth contribution in \(\beta(k)\), the word-ball growth of
the semigroup generated by the hidden layers. By Gromov's theorem polynomial
(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)
dynamics, revealing a geometric dichotomy behind existing $O(\sqrt{k})$
(sublinear depth) and $\tilde{O}(1)$ (depth-independent) rates. We further
provide covering-number estimates showing that expanding dynamics yield an
exponential parameter saving via compositional expressivity. Our results
decouple specification from implementation, offering architecture-agnostic and
dynamical-systems-aware guarantees applicable to modern deep-learning paradigms
such as test-time inference and diffusion models.

</details>


### [21] [Text embedding models can be great data engineers](https://arxiv.org/abs/2505.14802)
*Iman Kazemian, Paritosh Ramanan, Murat Yildirim*

**主要类别:** cs.LG

**概要:** This paper introduces ADEPT, an automated data engineering pipeline using text embeddings to handle diverse data sources and construct variational information bottleneck criteria for time series data. It shows superior predictive performance across various datasets.


<details>
  <summary>更多</summary>
  
**动机:** To reduce the significant engineering time and domain expertise required for traditional data engineering pipelines.

**方法:** Using text embeddings to represent diverse data sources and constructing a variational information bottleneck criteria for time series data.

**结果:** ADEPT outperforms existing benchmarks in diverse datasets from healthcare, finance, science, and industrial IoT.

**结论:** ADEPT can potentially leapfrog many conventional data pipeline steps, enabling efficient and scalable automation for data science applications.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Text+embedding+models+can+be+great+data+engineers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14802&send_immediately=true&force_search=false)

**原文摘要:** Data engineering pipelines are essential - albeit costly - components of
predictive analytics frameworks requiring significant engineering time and
domain expertise for carrying out tasks such as data ingestion, preprocessing,
feature extraction, and feature engineering. In this paper, we propose ADEPT,
an automated data engineering pipeline via text embeddings. At the core of the
ADEPT framework is a simple yet powerful idea that the entropy of embeddings
corresponding to textually dense raw format representation of time series can
be intuitively viewed as equivalent (or in many cases superior) to that of
numerically dense vector representations obtained by data engineering
pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text
embeddings to represent the diverse data sources, and (ii) constructs a
variational information bottleneck criteria to mitigate entropy variance in
text embeddings of time series data. ADEPT provides an end-to-end automated
implementation of predictive models that offers superior predictive performance
despite issues such as missing data, ill-formed records, improper or corrupted
data formats and irregular timestamps. Through exhaustive experiments, we show
that the ADEPT outperforms the best existing benchmarks in a diverse set of
datasets from large-scale applications across healthcare, finance, science and
industrial internet of things. Our results show that ADEPT can potentially
leapfrog many conventional data pipeline steps thereby paving the way for
efficient and scalable automation pathways for diverse data science
applications.

</details>


### [22] [BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms](https://arxiv.org/abs/2505.15141)
*Yunlong Hou, Fengzhuo Zhang, Cunxiao Du, Xuan Zhang, Jiachun Pan, Tianyu Pang, Chao Du, Vincent Y. F. Tan, Zhuoran Yang*

**主要类别:** cs.LG

**概要:** 提出了一种无需训练的在线学习框架，用于自适应选择大型语言模型推测解码的超参数配置，设计了UCBSpec和EXP3Spec两种算法，并在多个实验中展示了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 现有推测解码方法要么采用固定配置，要么需要离线或在线训练草案模型来对齐上下文，缺乏自适应性。

**方法:** 将超参数选择问题形式化为多臂老虎机问题，提出了BanditSpec框架，并设计了UCBSpec和EXP3Spec两种算法。

**结果:** 推导了停止时间遗憾的上界，并证明UCBSpec在随机和对抗奖励设置下的遗憾性能是最优的。

**结论:** 提出的算法在多种实验中表现出色，与现有方法相比具有竞争力，且在模拟的真实LLM服务场景中的吞吐量接近最佳超参数。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BanditSpec%3A+Adaptive+Speculative+Decoding+via+Bandit+Algorithms，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15141&send_immediately=true&force_search=false)

**原文摘要:** Speculative decoding has emerged as a popular method to accelerate the
inference of Large Language Models (LLMs) while retaining their superior text
generation performance. Previous methods either adopt a fixed speculative
decoding configuration regardless of the prefix tokens, or train draft models
in an offline or online manner to align them with the context. This paper
proposes a training-free online learning framework to adaptively choose the
configuration of the hyperparameters for speculative decoding as text is being
generated. We first formulate this hyperparameter selection problem as a
Multi-Armed Bandit problem and provide a general speculative decoding framework
BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,
UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,
the stopping time regret. We upper bound this regret under both stochastic and
adversarial reward settings. By deriving an information-theoretic impossibility
result, it is shown that the regret performance of UCBSpec is optimal up to
universal constants. Finally, extensive empirical experiments with LLaMA3 and
Qwen2 demonstrate that our algorithms are effective compared to existing
methods, and the throughput is close to the oracle best hyperparameter in
simulated real-life LLM serving scenarios with diverse input prompts.

</details>


### [23] [SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis](https://arxiv.org/abs/2505.14803)
*Yu Liu, Weiyao Tao, Tong Xia, Simon Knight, Tingting Zhu*

**主要类别:** cs.LG

**概要:** 提出SurvUnc框架用于生存模型的后验不确定性量化，该框架基于锚点学习策略并兼容多种生存模型。实验表明SurvUnc在多个评估场景中优于其他方法，提高了模型的可解释性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 生存模型在许多实际应用中至关重要，但其预测不确定性量化仍具有挑战性，限制了模型的可信度和临床决策中的应用。

**方法:** 提出SurvUnc框架，引入基于锚点的学习策略，并通过整合一致性知识优化元模型来有效估计不确定性，且该框架与模型无关。

**结果:** 在四个公开基准数据集和五个代表性生存模型上进行广泛实验，SurvUnc在选择性预测、错误预测检测和跨域检测等多个评估场景中表现出色。

**结论:** SurvUnc提高了生存模型的解释性和可靠性，在实际应用中提供了更值得信赖的生存预测。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SurvUnc%3A+A+Meta-Model+Based+Uncertainty+Quantification+Framework+for+Survival+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14803，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14803&send_immediately=true&force_search=false)

**原文摘要:** Survival analysis, which estimates the probability of event occurrence over
time from censored data, is fundamental in numerous real-world applications,
particularly in high-stakes domains such as healthcare and risk assessment.
Despite advances in numerous survival models, quantifying the uncertainty of
predictions from these models remains underexplored and challenging. The lack
of reliable uncertainty quantification limits the interpretability and
trustworthiness of survival models, hindering their adoption in clinical
decision-making and other sensitive applications. To bridge this gap, in this
work, we introduce SurvUnc, a novel meta-model based framework for post-hoc
uncertainty quantification for survival models. SurvUnc introduces an
anchor-based learning strategy that integrates concordance knowledge into
meta-model optimization, leveraging pairwise ranking performance to estimate
uncertainty effectively. Notably, our framework is model-agnostic, ensuring
compatibility with any survival model without requiring modifications to its
architecture or access to its internal parameters. Especially, we design a
comprehensive evaluation pipeline tailored to this critical yet overlooked
problem. Through extensive experiments on four publicly available benchmarking
datasets and five representative survival models, we demonstrate the
superiority of SurvUnc across multiple evaluation scenarios, including
selective prediction, misprediction detection, and out-of-domain detection. Our
results highlight the effectiveness of SurvUnc in enhancing model
interpretability and reliability, paving the way for more trustworthy survival
predictions in real-world applications.

</details>


### [24] [Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing](https://arxiv.org/abs/2505.15195)
*Adel Javanmard, Rudrajit Das, Alessandro Epasto, Vahab Mirrokni*

**主要类别:** cs.LG

**概要:** This paper develops a principled framework using approximate message passing (AMP) to find the Bayes optimal way to combine model predictions and noisy labels for iterative retraining in binary classification tasks.


<details>
  <summary>更多</summary>
  
**动机:** To address the question of how to optimally combine model predictions and potentially noisy labels during retraining to improve model performance.

**方法:** Develops a principled framework based on approximate message passing (AMP) for analyzing iterative retraining procedures under Gaussian mixture model (GMM) and generalized linear model (GLM) settings.

**结果:** Derives the Bayes optimal aggregator function that minimizes prediction error when retraining the same model. Quantifies the performance of this optimal retraining strategy across multiple rounds.

**结论:** Proposes a practical version of the theoretically-optimal aggregator function for linear probing with cross-entropy loss and shows its superiority in high label noise regimes.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Boost+via+Optimal+Retraining%3A+An+Analysis+via+Approximate+Message+Passing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15195，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15195&send_immediately=true&force_search=false)

**原文摘要:** Retraining a model using its own predictions together with the original,
potentially noisy labels is a well-known strategy for improving the model
performance. While prior works have demonstrated the benefits of specific
heuristic retraining schemes, the question of how to optimally combine the
model's predictions and the provided labels remains largely open. This paper
addresses this fundamental question for binary classification tasks. We develop
a principled framework based on approximate message passing (AMP) to analyze
iterative retraining procedures for two ground truth settings: Gaussian mixture
model (GMM) and generalized linear model (GLM). Our main contribution is the
derivation of the Bayes optimal aggregator function to combine the current
model's predictions and the given labels, which when used to retrain the same
model, minimizes its prediction error. We also quantify the performance of this
optimal retraining strategy over multiple rounds. We complement our theoretical
results by proposing a practically usable version of the theoretically-optimal
aggregator function for linear probing with the cross-entropy loss, and
demonstrate its superiority over baseline methods in the high label noise
regime.

</details>


### [25] [Imitation Learning via Focused Satisficing](https://arxiv.org/abs/2505.14820)
*Rushit N. Shah, Nikolaos Agadakos, Synthia Sasulski, Ali Farajzadeh, Sanjiban Choudhury, Brian Ziebart*

**主要类别:** cs.LG

**概要:** This paper presents a focused satisficing approach to imitation learning using a margin-based objective, which aims to surpass demonstrator's aspiration levels without explicitly learning them.


<details>
  <summary>更多</summary>
  
**动机:** Imitation learning typically assumes that demonstrations are near-optimal, but this paper challenges that by considering human behavior as based on personal aspiration levels.

**方法:** The method uses a margin-based objective to guide deep reinforcement learning, focusing on surpassing demonstrator's aspiration levels on unseen demonstrations.

**结果:** Experiments show that this approach improves the quality of imitated demonstrations and provides higher rates of guaranteed acceptability, with competitive returns across environments.

**结论:** This focused satisficing approach to imitation learning offers a novel way to improve the quality of learned policies by surpassing demonstrator's aspiration levels.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Imitation+Learning+via+Focused+Satisficing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14820，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14820&send_immediately=true&force_search=false)

**原文摘要:** Imitation learning often assumes that demonstrations are close to optimal
according to some fixed, but unknown, cost function. However, according to
satisficing theory, humans often choose acceptable behavior based on their
personal (and potentially dynamic) levels of aspiration, rather than achieving
(near-) optimality. For example, a lunar lander demonstration that successfully
lands without crashing might be acceptable to a novice despite being slow or
jerky. Using a margin-based objective to guide deep reinforcement learning, our
focused satisficing approach to imitation learning seeks a policy that
surpasses the demonstrator's aspiration levels -- defined over trajectories or
portions of trajectories -- on unseen demonstrations without explicitly
learning those aspirations. We show experimentally that this focuses the policy
to imitate the highest quality (portions of) demonstrations better than
existing imitation learning methods, providing much higher rates of guaranteed
acceptability to the demonstrator, and competitive true returns on a range of
environments.

</details>


### [26] [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
*Christian Walder, Deep Karkhanis*

**主要类别:** cs.LG

**概要:** 本文提出了Pass-at-k策略优化(PKPO)，一种新的强化学习优化方法，通过优化多个样本集合的整体奖励，提高了对更难问题的探索和改进能力。


<details>
  <summary>更多</summary>
  
**动机:** 传统的强化学习算法只优化单个样本（pass@1），忽略了多个样本集合的多样性和整体效用，限制了探索能力和对更难问题的改进。

**方法:** 提出了一种新的策略优化方法Pass-at-k策略优化(PKPO)，它改变了最终奖励，直接优化pass@k性能，并推导出pass@k及其梯度的新低方差无偏估计量。

**结果:** 在玩具实验和真实世界例子中验证了我们的奖励变换方法。发现我们的变换有效地优化了目标k值。更高的k值能够解决更多和更难的问题，而k的退火同时提升了pass@1和pass@k的性能。对于常规pass@1优化停滞的挑战性任务集，我们的pass@k方法解除了学习障碍。

**结论:** 我们提出的Pass-at-k策略优化(PKPO)解决了传统强化学习中只优化单个样本的问题，通过优化多个样本集合的整体奖励，提高了对更难问题的探索和改进能力。实验表明，我们的方法不仅在理论上有创新，在实际应用中也有效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pass%40K+Policy+Optimization%3A+Solving+Harder+Reinforcement+Learning+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15201&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.

</details>


### [27] [Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation](https://arxiv.org/abs/2505.14821)
*Runze Zhao, Yue Yu, Adams Yiyue Zhu, Chen Yang, Dongruo Zhou*

**主要类别:** cs.LG

**概要:** This paper proposes a model-based continuous-time reinforcement learning algorithm that is both sample and computationally efficient, providing the first sample complexity guarantee for general function approximation.


<details>
  <summary>更多</summary>
  
**动机:** To address the limited theoretical understanding of continuous-time reinforcement learning, particularly in settings with general function approximation.

**方法:** Proposes an optimism-based confidence set approach and introduces structured policy updates with an alternative measurement strategy.

**结果:** Achieves sample complexity guarantee and demonstrates competitive performance with fewer policy updates and rollouts.

**结论:** The proposed algorithm offers a significant advancement in the theoretical understanding of continuous-time reinforcement learning with general function approximation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sample+and+Computationally+Efficient+Continuous-Time+Reinforcement+Learning+with+General+Function+Approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14821，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14821&send_immediately=true&force_search=false)

**原文摘要:** Continuous-time reinforcement learning (CTRL) provides a principled framework
for sequential decision-making in environments where interactions evolve
continuously over time. Despite its empirical success, the theoretical
understanding of CTRL remains limited, especially in settings with general
function approximation. In this work, we propose a model-based CTRL algorithm
that achieves both sample and computational efficiency. Our approach leverages
optimism-based confidence sets to establish the first sample complexity
guarantee for CTRL with general function approximation, showing that a
near-optimal policy can be learned with a suboptimality gap of
$\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$
measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the
distributional Eluder dimensions of the reward and dynamic functions,
respectively, capturing the complexity of general function approximation in
reinforcement learning. Moreover, we introduce structured policy updates and an
alternative measurement strategy that significantly reduce the number of policy
updates and rollouts while maintaining competitive sample efficiency. We
implemented experiments to backup our proposed algorithms on continuous control
tasks and diffusion model fine-tuning, demonstrating comparable performance
with significantly fewer policy updates and rollouts.

</details>


### [28] [Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers](https://arxiv.org/abs/2505.15239)
*Peter Súkeník, Christoph H. Lampert, Marco Mondelli*

**主要类别:** cs.LG

**概要:** 研究了深度正则化Transformer和ResNet网络在数据感知情况下的神经坍塌现象。证明了这些模型的全局最优解近似坍塌，并且随着深度增加，这种近似更加紧密。此外，通过实验证明了随着深度增长，神经坍塌现象变得更加显著。


<details>
  <summary>更多</summary>
  
**动机:** 理解深度神经网络在训练过程中出现的神经坍塌现象。

**方法:** 分析现代架构在数据感知情况下的神经坍塌现象，并证明了全局最优解的近似坍塌性质。

**结果:** 证明了深度正则化Transformer和ResNet网络的全局最优解近似坍塌，并且随着深度增加，这种近似更加紧密。

**结论:** 本研究填补了现有工作的空白，提供了对神经坍塌现象的新见解，并为理解深度学习模型提供了理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Collapse+is+Globally+Optimal+in+Deep+Regularized+ResNets+and+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15239，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15239&send_immediately=true&force_search=false)

**原文摘要:** The empirical emergence of neural collapse -- a surprising symmetry in the
feature representations of the training data in the penultimate layer of deep
neural networks -- has spurred a line of theoretical research aimed at its
understanding. However, existing work focuses on data-agnostic models or, when
data structure is taken into account, it remains limited to multi-layer
perceptrons. Our paper fills both these gaps by analyzing modern architectures
in a data-aware regime: we prove that global optima of deep regularized
transformers and residual networks (ResNets) with LayerNorm trained with cross
entropy or mean squared error loss are approximately collapsed, and the
approximation gets tighter as the depth grows. More generally, we formally
reduce any end-to-end large-depth ResNet or transformer training into an
equivalent unconstrained features model, thus justifying its wide use in the
literature even beyond data-agnostic settings. Our theoretical results are
supported by experiments on computer vision and language datasets showing that,
as the depth grows, neural collapse indeed becomes more prominent.

</details>


### [29] [Human in the Loop Adaptive Optimization for Improved Time Series Forecasting](https://arxiv.org/abs/2505.15354)
*Malik Tiomoko, Hamza Cherkaoui, Giuseppe Paolo, Zhang Yili, Yu Meng, Zhang Keli, Hafiz Tiomoko Ali*

**主要类别:** cs.LG

**概要:** 提出了一种新的后训练自适应优化框架，无需重新训练或架构更改即可提高预测准确性。


<details>
  <summary>更多</summary>
  
**动机:** 时间序列预测模型即使在关键领域（如能源、金融和医疗）中也会产生系统性的可预测错误。

**方法:** 自动应用通过强化学习、上下文乐队或遗传算法优化的表达性变换，以轻量级且与模型无关的方式校正模型输出。

**结果:** 在多个基准测试中观察到一致的准确性提升，计算开销最小。

**结论:** 结合了自动化事后细化与可解释和可扩展机制，为实际预测系统提供了新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Human+in+the+Loop+Adaptive+Optimization+for+Improved+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15354&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting models often produce systematic, predictable errors
even in critical domains such as energy, finance, and healthcare. We introduce
a novel post training adaptive optimization framework that improves forecast
accuracy without retraining or architectural changes. Our method automatically
applies expressive transformations optimized via reinforcement learning,
contextual bandits, or genetic algorithms to correct model outputs in a
lightweight and model agnostic way. Theoretically, we prove that affine
corrections always reduce the mean squared error; practically, we extend this
idea with dynamic action based optimization. The framework also supports an
optional human in the loop component: domain experts can guide corrections
using natural language, which is parsed into actions by a language model.
Across multiple benchmarks (e.g., electricity, weather, traffic), we observe
consistent accuracy gains with minimal computational overhead. Our interactive
demo shows the framework's real time usability. By combining automated post hoc
refinement with interpretable and extensible mechanisms, our approach offers a
powerful new direction for practical forecasting systems.

</details>


### [30] [Deep Koopman operator framework for causal discovery in nonlinear dynamical systems](https://arxiv.org/abs/2505.14828)
*Juan Nathaniel, Carla Roesch, Jatan Buch, Derek DeSantis, Adam Rupe, Kara Lamb, Pierre Gentine*

**主要类别:** cs.LG

**概要:** 提出了一种基于深度Koopman算子理论的新因果发现算法Kausal，该算法在非线性动力学中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 标准统计框架无法量化非线性动力学中的因果关系，因此需要新的方法来研究复杂的现实世界系统。

**方法:** 利用深度学习推断最优观测值，并通过再生核Hilbert空间评估因果估计值。

**结果:** 数值实验表明Kausal在发现和表征因果信号方面优于现有方法。

**结论:** Kausal算法展示了其在处理真实世界现象（如厄尔尼诺-南方涛动）中的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Koopman+operator+framework+for+causal+discovery+in+nonlinear+dynamical+systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14828&send_immediately=true&force_search=false)

**原文摘要:** We use a deep Koopman operator-theoretic formalism to develop a novel causal
discovery algorithm, Kausal. Causal discovery aims to identify cause-effect
mechanisms for better scientific understanding, explainable decision-making,
and more accurate modeling. Standard statistical frameworks, such as Granger
causality, lack the ability to quantify causal relationships in nonlinear
dynamics due to the presence of complex feedback mechanisms, timescale mixing,
and nonstationarity. This presents a challenge in studying many real-world
systems, such as the Earth's climate. Meanwhile, Koopman operator methods have
emerged as a promising tool for approximating nonlinear dynamics in a linear
space of observables. In Kausal, we propose to leverage this powerful idea for
causal analysis where optimal observables are inferred using deep learning.
Causal estimates are then evaluated in a reproducing kernel Hilbert space, and
defined as the distance between the marginal dynamics of the effect and the
joint dynamics of the cause-effect observables. Our numerical experiments
demonstrate Kausal's superior ability in discovering and characterizing causal
signals compared to existing approaches of prescribed observables. Lastly, we
extend our analysis to observations of El Ni\~no-Southern Oscillation
highlighting our algorithm's applicability to real-world phenomena. Our code is
available at https://github.com/juannat7/kausal.

</details>


### [31] [SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding](https://arxiv.org/abs/2505.15423)
*Marcell T. Kurbucz, Nikolaos Tzivanakis, Nilufer Sari Aslam, Adam M. Sykulski*

**主要类别:** cs.LG

**概要:** 提出SplitWise框架增强逐步回归，通过决策树处理非线性关系，保持模型透明性同时提高性能。


<details>
  <summary>更多</summary>
  
**动机:** 捕捉非线性关系而不牺牲可解释性仍然是回归建模中的一个持续挑战。

**方法:** 引入浅层决策树自适应地将数值预测变量转换为基于阈值的二元特征，仅当这种转换能改善模型拟合时才进行转换，使用赤池信息量准则(AIC)或贝叶斯信息量准则(BIC)评估模型拟合。

**结果:** SplitWise方法在合成数据集和现实世界的数据集上均表现出色，产生的模型比传统方法更简洁且泛化能力更强。

**结论:** SplitWise方法在保持线性模型透明性的同时，灵活捕捉非线性效应，比传统的逐步回归和惩罚回归技术产生更简洁且泛化的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SplitWise+Regression%3A+Stepwise+Modeling+with+Adaptive+Dummy+Encoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15423，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15423&send_immediately=true&force_search=false)

**原文摘要:** Capturing nonlinear relationships without sacrificing interpretability
remains a persistent challenge in regression modeling. We introduce SplitWise,
a novel framework that enhances stepwise regression. It adaptively transforms
numeric predictors into threshold-based binary features using shallow decision
trees, but only when such transformations improve model fit, as assessed by the
Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).
This approach preserves the transparency of linear models while flexibly
capturing nonlinear effects. Implemented as a user-friendly R package,
SplitWise is evaluated on both synthetic and real-world datasets. The results
show that it consistently produces more parsimonious and generalizable models
than traditional stepwise and penalized regression techniques.

</details>


### [32] [Subquadratic Algorithms and Hardness for Attention with Any Temperature](https://arxiv.org/abs/2505.14840)
*Shreya Gupta, Boyang Huang, Barna Saha, Yinzhan Xu, Christopher Ye*

**主要类别:** cs.LG

**概要:** This paper provides a new algorithm for efficient attention computation with large input sizes, applicable for constant head dimensions, and shows that any substantial improvement is unlikely.


<details>
  <summary>更多</summary>
  
**动机:** To address the inefficiency of the standard attention computation in transformers due to its quadratic time complexity.

**方法:** Developing a new algorithm for attention computation that scales polylogarithmically with entry size B for constant head dimensions.

**结果:** The proposed algorithm achieves subquadratic time complexity for attention computation with large B values and has a similar running time for attention gradient computation and LLM training processes.

**结论:** The paper concludes that any significant improvement on their algorithm is improbable, especially when head dimension d equals 2 raised to a function of log-star n.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Subquadratic+Algorithms+and+Hardness+for+Attention+with+Any+Temperature，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14840，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14840&send_immediately=true&force_search=false)

**原文摘要:** Despite the popularity of the Transformer architecture, the standard
algorithm for computing Attention suffers from quadratic time complexity in
context length $n$. Alman and Song [NeurIPS 2023] showed that when the head
dimension $d = \Theta(\log n)$, subquadratic Attention is possible if and only
if the inputs have small entries bounded by $B = o(\sqrt{\log n})$ in absolute
values, under the Strong Exponential Time Hypothesis ($\mathsf{SETH}$).
Equivalently, subquadratic Attention is possible if and only if the softmax is
applied with high temperature for $d=\Theta(\log n)$. Running times of these
algorithms depend exponentially on $B$ and thus they do not lead to even a
polynomial-time algorithm outside the specific range of $B$.
  This naturally leads to the question: when can Attention be computed
efficiently without strong assumptions on temperature? Are there fast attention
algorithms that scale polylogarithmically with entry size $B$? In this work, we
resolve this question and characterize when fast Attention for arbitrary
temperatures is possible. First, for all constant $d = O(1)$, we give the first
subquadratic $\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$ time algorithm
for Attention with large $B$. Our result holds even for matrices with large
head dimension if they have low rank. In this regime, we also give a similar
running time for Attention gradient computation, and therefore for the full LLM
training process. Furthermore, we show that any substantial improvement on our
algorithm is unlikely. In particular, we show that even when $d =
2^{\Theta(\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under
$\mathsf{SETH}$.
  Finally, in the regime where $d = \mathrm{poly}(n)$, we show that the
standard algorithm is optimal under popular fine-grained complexity
assumptions.

</details>


### [33] [Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes](https://arxiv.org/abs/2505.15496)
*Hossein Zakerinia, Christoph H. Lampert*

**主要类别:** cs.LG

**概要:** This paper presents new fast-rate generalization bounds for multi-task and meta-learning in unbalanced settings, showing stronger guarantees than previous bounds.


<details>
  <summary>更多</summary>
  
**动机:** To address the gap in fast-rate bounds for unbalanced multi-task learning scenarios which are common in real-world applications.

**方法:** Developing new numerical and interpretable fast-rate generalization bounds for unbalanced task training sets.

**结果:** The new bounds provide stronger guarantees than previous standards, especially in certain cases.

**结论:** Unbalanced multi-task learning has distinct statistical properties compared to balanced settings, requiring new proof techniques and offering two ways to define multi-task risk.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fast+Rate+Bounds+for+Multi-Task+and+Meta-Learning+with+Different+Sample+Sizes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15496&send_immediately=true&force_search=false)

**原文摘要:** We present new fast-rate generalization bounds for multi-task and
meta-learning in the unbalanced setting, i.e. when the tasks have training sets
of different sizes, as is typically the case in real-world scenarios.
Previously, only standard-rate bounds were known for this situation, while
fast-rate bounds were limited to the setting where all training sets are of
equal size. Our new bounds are numerically computable as well as interpretable,
and we demonstrate their flexibility in handling a number of cases where they
give stronger guarantees than previous bounds. Besides the bounds themselves,
we also make conceptual contributions: we demonstrate that the unbalanced
multi-task setting has different statistical properties than the balanced
situation, specifically that proofs from the balanced situation do not carry
over to the unbalanced setting. Additionally, we shed light on the fact that
the unbalanced situation allows two meaningful definitions of multi-task risk,
depending on whether if all tasks should be considered equally important or if
sample-rich tasks should receive more weight than sample-poor ones.

</details>


### [34] [A self-regulated convolutional neural network for classifying variable stars](https://arxiv.org/abs/2505.14877)
*Francisco Pérez-Galarce, Jorge Martínez-Palomera, Karim Pichara, Pablo Huijse, Márcio Catelan*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的自调节训练方法，通过生成合成样本来改善变星分类中的偏差问题，并在有偏数据集上取得了显著的性能提升。


<details>
  <summary>更多</summary>
  
**动机:** 现有的机器学习模型在处理变星分类时容易受到训练数据中固有偏差的影响，并且这种偏差难以检测。

**方法:** 一种自调节训练过程，结合物理增强的潜在空间变分自动编码器生成合成样本。

**结果:** 实验表明，该方法在多个场景下优于传统训练方法。

**结论:** 提出的方法在处理有偏数据集的变星分类上表现出显著改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+self-regulated+convolutional+neural+network+for+classifying+variable+stars，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14877，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14877&send_immediately=true&force_search=false)

**原文摘要:** Over the last two decades, machine learning models have been widely applied
and have proven effective in classifying variable stars, particularly with the
adoption of deep learning architectures such as convolutional neural networks,
recurrent neural networks, and transformer models. While these models have
achieved high accuracy, they require high-quality, representative data and a
large number of labelled samples for each star type to generalise well, which
can be challenging in time-domain surveys. This challenge often leads to models
learning and reinforcing biases inherent in the training data, an issue that is
not easily detectable when validation is performed on subsamples from the same
catalogue. The problem of biases in variable star data has been largely
overlooked, and a definitive solution has yet to be established. In this paper,
we propose a new approach to improve the reliability of classifiers in variable
star classification by introducing a self-regulated training process. This
process utilises synthetic samples generated by a physics-enhanced latent space
variational autoencoder, incorporating six physical parameters from Gaia Data
Release 3. Our method features a dynamic interaction between a classifier and a
generative model, where the generative model produces ad-hoc synthetic light
curves to reduce confusion during classifier training and populate
underrepresented regions in the physical parameter space. Experiments conducted
under various scenarios demonstrate that our self-regulated training approach
outperforms traditional training methods for classifying variable stars on
biased datasets, showing statistically significant improvements.

</details>


### [35] [Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions](https://arxiv.org/abs/2505.15579)
*Hossein Zakerinia, Jonathan Scott, Christoph H. Lampert*

**主要类别:** cs.LG

**概要:** 本文提出了一种名为FLowDUP的新方法，它可以通过无标签数据生成个性化模型，解决了个性化联邦学习中的一个关键问题。


<details>
  <summary>更多</summary>
  
**动机:** 大多数现有方法需要有标签数据进行训练，而FLowDUP解决了这一问题。

**方法:** 通过前向传递生成个性化模型，并在低维子空间中表示模型参数。

**结果:** FLowDUP在不同类型的统计异构客户端的数据集上展示了强大的实证性能。

**结论:** FLowDUP能通过无标签数据生成个性化模型，且在多种数据集上表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Learning+with+Unlabeled+Clients%3A+Personalization+Can+Happen+in+Low+Dimensions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15579&send_immediately=true&force_search=false)

**原文摘要:** Personalized federated learning has emerged as a popular approach to training
on devices holding statistically heterogeneous data, known as clients. However,
most existing approaches require a client to have labeled data for training or
finetuning in order to obtain their own personalized model. In this paper we
address this by proposing FLowDUP, a novel method that is able to generate a
personalized model using only a forward pass with unlabeled data. The generated
model parameters reside in a low-dimensional subspace, enabling efficient
communication and computation. FLowDUP's learning objective is theoretically
motivated by our new transductive multi-task PAC-Bayesian generalization bound,
that provides performance guarantees for unlabeled clients. The objective is
structured in such a way that it allows both clients with labeled data and
clients with only unlabeled data to contribute to the training process. To
supplement our theoretical results we carry out a thorough experimental
evaluation of FLowDUP, demonstrating strong empirical performance on a range of
datasets with differing sorts of statistically heterogeneous clients. Through
numerous ablation studies, we test the efficacy of the individual components of
the method.

</details>


### [36] [An active learning framework for multi-group mean estimation](https://arxiv.org/abs/2505.14882)
*Abdellah Aznag, Rachel Cummings, Adam N. Elmachtoub*

**主要类别:** cs.LG

**概要:** 提出了一种名为Variance-UCB的算法，用于在动态收集样本时最小化估计均值向量方差的范数。该算法在理论上优于现有方法，并适用于多种分布和目标。


<details>
  <summary>更多</summary>
  
**动机:** 研究多个群体的平均值学习问题，确保数据收集过程公平，噪声合理。

**方法:** 采用主动学习框架，利用bandit反馈动态选择群体，观察样本并更新估计值。提出Variance-UCB算法，基于方差估计的置信上限选择群体。

**结果:** 提出的Variance-UCB算法在理论分析上显著改进了现有方法的后悔界，并为不同目标和分布提供了新的结果。

**结论:** 所提出的方法在适应性实验和临床试验中具有重要应用价值，能有效处理未知数据分布下的学习问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+active+learning+framework+for+multi-group+mean+estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14882&send_immediately=true&force_search=false)

**原文摘要:** We study a fundamental learning problem over multiple groups with unknown
data distributions, where an analyst would like to learn the mean of each
group. Moreover, we want to ensure that this data is collected in a relatively
fair manner such that the noise of the estimate of each group is reasonable. In
particular, we focus on settings where data are collected dynamically, which is
important in adaptive experimentation for online platforms or adaptive clinical
trials for healthcare. In our model, we employ an active learning framework to
sequentially collect samples with bandit feedback, observing a sample in each
period from the chosen group. After observing a sample, the analyst updates
their estimate of the mean and variance of that group and chooses the next
group accordingly. The analyst's objective is to dynamically collect samples to
minimize the collective noise of the estimators, measured by the norm of the
vector of variances of the mean estimators.
  We propose an algorithm, Variance-UCB, that sequentially selects groups
according to an upper confidence bound on the variance estimate. We provide a
general theoretical framework for providing efficient bounds on learning from
any underlying distribution where the variances can be estimated reasonably.
This framework yields upper bounds on regret that improve significantly upon
all existing bounds, as well as a collection of new results for different
objectives and distributions than those previously studied.

</details>


### [37] [Aligning Explanations with Human Communication](https://arxiv.org/abs/2505.15626)
*Jacopo Teneggi, Zhenzhen Wang, Paul H. Yi, Tianmin Shu, Jeremias Sulam*

**主要类别:** cs.LG

**概要:** 本文提出了一种新的解释方法，该方法考虑了听众的理解能力，并在图像分类任务中展示了更好的效果。


<details>
  <summary>更多</summary>
  
**动机:** 现有机器学习可解释性方法通常忽略了解释的交流背景，即用户从解释中理解模型预测的能力。

**方法:** 基于语用推理和理性话语行为原则，提出了一个迭代过程来生成最大化沟通效用的解释。

**结果:** 在三个数据集上的图像分类任务中，展示了改进的解释与听众偏好的对齐，并通过用户研究证明了其提高了沟通效用。

**结论:** 提出了一种名为听众自适应解释的新方法，该方法仅需要访问候选解释之间的成对偏好，在图像分类任务中展示了改进的解释与听众偏好的对齐，并通过用户研究证明了其提高了沟通效用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Aligning+Explanations+with+Human+Communication，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15626&send_immediately=true&force_search=false)

**原文摘要:** Machine learning explainability aims to make the decision-making process of
black-box models more transparent by finding the most important input features
for a given prediction task. Recent works have proposed composing explanations
from semantic concepts (e.g., colors, patterns, shapes) that are inherently
interpretable to the user of a model. However, these methods generally ignore
the communicative context of explanation-the ability of the user to understand
the prediction of the model from the explanation. For example, while a medical
doctor might understand an explanation in terms of clinical markers, a patient
may need a more accessible explanation to make sense of the same diagnosis. In
this paper, we address this gap with listener-adaptive explanations. We propose
an iterative procedure grounded in principles of pragmatic reasoning and the
rational speech act to generate explanations that maximize communicative
utility. Our procedure only needs access to pairwise preferences between
candidate explanations, relevant in real-world scenarios where a listener model
may not be available. We evaluate our method in image classification tasks,
demonstrating improved alignment between explanations and listener preferences
across three datasets. Furthermore, we perform a user study that demonstrates
our explanations increase communicative utility.

</details>


### [38] [Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity](https://arxiv.org/abs/2505.14884)
*Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy*

**主要类别:** cs.LG

**概要:** 提出Polar稀疏性，使上下文稀疏性能够扩展至大批次大小，显著加速LLMs推理，适用于实际的大规模部署。


<details>
  <summary>更多</summary>
  
**动机:** 加速大规模语言模型（LLM）推理对于需要高吞吐量和低延迟的实际部署至关重要。

**方法:** 引入Polar稀疏性，开发硬件高效、感知稀疏性的GPU内核用于选择性MLP和注意力计算。

**结果:** 在OPT、LLaMA-2和3等模型上实现了最高2.2倍的端到端加速，适用于各种批次大小和序列长度，且不损失准确性。

**结论:** 首次证明上下文稀疏性可以有效扩展到大批次大小，并在各种批次大小和序列长度下提供高达2.2倍的端到端加速，而不会影响准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Polar+Sparsity%3A+High+Throughput+Batched+LLM+Inferencing+with+Scalable+Contextual+Sparsity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14884，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14884&send_immediately=true&force_search=false)

**原文摘要:** Accelerating large language model (LLM) inference is critical for real-world
deployments requiring high throughput and low latency. Contextual sparsity,
where each token dynamically activates only a small subset of the model
parameters, shows promise but does not scale to large batch sizes due to union
of active neurons quickly approaching dense computation. We introduce Polar
Sparsity, highlighting a key shift in sparsity importance from MLP to Attention
layers as we scale batch size and sequence length. While MLP layers become more
compute-efficient under batching, their sparsity vanishes. In contrast,
attention becomes increasingly more expensive at scale, while their head
sparsity remains stable and batch-invariant. We develop hardware-efficient,
sparsity-aware GPU kernels for selective MLP and Attention computations,
delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2
\& 3, across various batch sizes and sequence lengths without compromising
accuracy. To our knowledge, this is the first work to demonstrate that
contextual sparsity can scale effectively to large batch sizes, delivering
substantial inference acceleration with minimal changes, making Polar Sparsity
practical for large-scale, high-throughput LLM deployment systems. Our code is
available at: https://github.com/susavlsh10/Polar-Sparsity.

</details>


### [39] [Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes](https://arxiv.org/abs/2505.15638)
*Daniel Waxman, Fernando Llorente, Petar M. Djurić*

**主要类别:** cs.LG

**概要:** This paper revisits Bayesian ensembles and introduces Online Bayesian Stacking (OBS), which adapts Bayesian models using log-score optimization. It connects OBS to portfolio selection and compares it with online Bayesian model averaging.


<details>
  <summary>更多</summary>
  
**动机:** To learn optimal combinations of Bayesian models in an online, continual learning setting while addressing limitations of existing methods like Bayesian model averaging.

**方法:** Reinterpreting existing approaches via empirical Bayes and proposing OBS for log-score optimization, connecting it to portfolio selection.

**结果:** Established a novel connection between OBS and portfolio selection, clarified its relationship with online BMA, and identified scenarios where OBS performs better.

**结论:** Online Bayesian Stacking is a promising approach for adaptive Bayesian model combination, offering advantages over traditional methods in certain contexts.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bayesian+Ensembling%3A+Insights+from+Online+Optimization+and+Empirical+Bayes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15638，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15638&send_immediately=true&force_search=false)

**原文摘要:** We revisit the classical problem of Bayesian ensembles and address the
challenge of learning optimal combinations of Bayesian models in an online,
continual learning setting. To this end, we reinterpret existing approaches
such as Bayesian model averaging (BMA) and Bayesian stacking through a novel
empirical Bayes lens, shedding new light on the limitations and pathologies of
BMA. Further motivated by insights from online optimization, we propose Online
Bayesian Stacking (OBS), a method that optimizes the log-score over predictive
distributions to adaptively combine Bayesian models. A key contribution of our
work is establishing a novel connection between OBS and portfolio selection,
bridging Bayesian ensemble learning with a rich, well-studied theoretical
framework that offers efficient algorithms and extensive regret analysis. We
further clarify the relationship between OBS and online BMA, showing that they
optimize related but distinct cost functions. Through theoretical analysis and
empirical evaluation, we identify scenarios where OBS outperforms online BMA
and provide principled guidance on when practitioners should prefer one
approach over the other.

</details>


### [40] [Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis](https://arxiv.org/abs/2505.14896)
*Hootan Mahmoodiyan, Maryam Ahang, Mostafa Abbasi, Homayoun Najjaran*

**主要类别:** cs.LG

**概要:** This study introduces a feature-weighted domain adaptation technique combining MMD, CORAL, and feature-specific weighting to improve fault diagnosis accuracy of power transformers.


<details>
  <summary>更多</summary>
  
**动机:** To address inconsistent results from traditional DGA methods due to varying conditions and distribution shifts in diagnostic data.

**方法:** Proposes a MCW method using K-S statistics to assign adaptable weights to features with larger distributional discrepancies.

**结果:** Improves diagnostic accuracy, achieving a 7.9% improvement over Fine-Tuning and a 2.2% improvement over MMD-CORAL.

**结论:** The proposed method demonstrates effectiveness and robustness in domain adaptation for power transformer fault diagnosis.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Feature-Weighted+MMD-CORAL+for+Domain+Adaptation+in+Power+Transformer+Fault+Diagnosis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14896，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14896&send_immediately=true&force_search=false)

**原文摘要:** Ensuring the reliable operation of power transformers is critical to grid
stability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but
traditional methods rely on heuristic rules, which may lead to inconsistent
results. Machine learning (ML)-based approaches have improved diagnostic
accuracy; however, power transformers operate under varying conditions, and
differences in transformer type, environmental factors, and operational
settings create distribution shifts in diagnostic data. Consequently, direct
model transfer between transformers often fails, making techniques for domain
adaptation a necessity. To tackle this issue, this work proposes a
feature-weighted domain adaptation technique that combines Maximum Mean
Discrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific
weighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign
adaptable weights, prioritizing features with larger distributional
discrepancies and thereby improving source and target domain alignment.
Experimental evaluations on datasets for power transformers demonstrate the
effectiveness of the proposed method, which achieves a 7.9% improvement over
Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it
outperforms both techniques across various training sample sizes, confirming
its robustness for domain adaptation.

</details>


### [41] [Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima](https://arxiv.org/abs/2505.15643)
*Lan V. Truong*

**主要类别:** cs.LG

**概要:** 研究了在固定置信度设置下的随机多臂老虎机中最优臂识别问题，特别关注存在多个最优臂的情况。提出了一个新的停止规则以确保实例最优性，并引入了一个新的信息论下界来明确考虑多个最优臂。


<details>
  <summary>更多</summary>
  
**动机:** 广泛认为Garivier和Kaufmann（2016）提出的Track-and-Stop算法是实例最优的，但在存在多个最优解的情况下其表现仍未被充分理解。

**方法:** 重新审视Track-and-Stop策略并提出一个修改后的停止规则。

**结果:** 提出的新停止规则确保了即使在最优臂不是单一的情况下也能实现实例最优性。

**结论:** 我们的分析引入了一个新的信息论下界，并证明所提出的停止规则紧密匹配这个界限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Best-Arm+Identification+under+Fixed+Confidence+with+Multiple+Optima，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15643，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15643&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of best-arm identification in stochastic multi-armed
bandits under the fixed-confidence setting, with a particular focus on
instances that admit multiple optimal arms. While the Track-and-Stop algorithm
of Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,
its performance in the presence of multiple optima has remained insufficiently
understood. In this work, we revisit the Track-and-Stop strategy and propose a
modified stopping rule that ensures instance-optimality even when the set of
optimal arms is not a singleton. Our analysis introduces a new
information-theoretic lower bound that explicitly accounts for multiple optimal
arms, and we demonstrate that our stopping rule tightly matches this bound.

</details>


### [42] [Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction](https://arxiv.org/abs/2505.14897)
*Ali Mohajerzarrinkelk, Maryam Ahang, Mehran Zoravar, Mostafa Abbasi, Homayoun Najjaran*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架来精确估计滚动轴承的剩余使用寿命，该框架结合了多种技术，并在多个实验中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 精确估计滚动轴承的剩余使用寿命（RUL）对于避免意外故障、减少停机时间、提高工业系统的安全性和效率至关重要。

**方法:** 结合小波去噪方法、小波包分解和定制多通道Swin变换模型(MCSFormer)，并采用注意力机制进行特征融合。

**结果:** 实验表明，MCSFormer在不同操作条件下比最先进的模型表现更好，并且自定义损失函数有效地减少了晚期预测。

**结论:** 提出的方法在工业应用中表现出色，具有鲁棒性、泛化能力和安全性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Channel+Swin+Transformer+Framework+for+Bearing+Remaining+Useful+Life+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14897，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14897&send_immediately=true&force_search=false)

**原文摘要:** Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is
an important consideration to avoid unexpected failures, reduce downtime, and
promote safety and efficiency in industrial systems. Complications in
degradation trends, noise presence, and the necessity to detect faults in
advance make estimation of RUL a challenging task. This paper introduces a
novel framework that combines wavelet-based denoising method, Wavelet Packet
Decomposition (WPD), and a customized multi-channel Swin Transformer model
(MCSFormer) to address these problems. With attention mechanisms incorporated
for feature fusion, the model is designed to learn global and local degradation
patterns utilizing hierarchical representations for enhancing predictive
performance. Additionally, a customized loss function is developed as a key
distinction of this work to differentiate between early and late predictions,
prioritizing accurate early detection and minimizing the high operation risks
of late predictions. The proposed model was evaluated with the PRONOSTIA
dataset using three experiments. Intra-condition experiments demonstrated that
MCSFormer outperformed state-of-the-art models, including the Adaptive
Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on
average across different operating conditions, respectively. In terms of
cross-condition testing, it achieved superior generalization under varying
operating conditions compared to the adapted ViT and Swin Transformer. Lastly,
the custom loss function effectively reduced late predictions, as evidenced in
a 6.3% improvement in the scoring metric while maintaining competitive overall
performance. The model's robust noise resistance, generalization capability,
and focus on safety make MCSFormer a trustworthy and effective predictive
maintenance tool in industrial applications.

</details>


### [43] [Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities](https://arxiv.org/abs/2505.14943)
*Ross Nordby*

**主要类别:** cs.LG

**概要:** This paper introduces an approach using optimized input embeddings ('soft prompts') to assess the latent capabilities of language models.


<details>
  <summary>更多</summary>
  
**动机:** To help evaluate and understand the latent capabilities of language models.

**方法:** Using optimized input embeddings ('soft prompts') as a metric of conditional distance between a model and a target behavior.

**结果:** The technique was demonstrated in natural language, chess, and pathfinding tasks.

**结论:** The technique can facilitate latent capability discovery and provide quantitative feedback about the accessibility of potentially concerning behaviors.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Soft+Prompts+for+Evaluation%3A+Measuring+Conditional+Distance+of+Capabilities，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14943，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14943&send_immediately=true&force_search=false)

**原文摘要:** To help evaluate and understand the latent capabilities of language models,
this paper introduces an approach using optimized input embeddings, or 'soft
prompts,' as a metric of conditional distance between a model and a target
behavior. The technique aims to facilitate latent capability discovery as a
part of automated red teaming/evaluation suites and to provide quantitative
feedback about the accessibility of potentially concerning behaviors in a way
that may scale to powerful future models, including those which may otherwise
be capable of deceptive alignment. An evaluation framework using soft prompts
is demonstrated in natural language, chess, and pathfinding, and the technique
is extended with generalized conditional soft prompts to aid in constructing
task evaluations.

</details>


### [44] [Privacy-Preserving Conformal Prediction Under Local Differential Privacy](https://arxiv.org/abs/2505.15721)
*Coby Penso, Bar Mahpud, Jacob Goldberger, Or Sheffet*

**主要类别:** cs.LG

**概要:** 提出两种在局部差分隐私下的方法来解决在不可信聚合器情况下带有干净标签校准集的确认预测问题。


<details>
  <summary>更多</summary>
  
**动机:** 在不可信的聚合器只能访问真实标签的扰动版本的隐私敏感场景下改进确认预测。

**方法:** 1. 用户通过k元随机响应提供输入特征和扰动标签；2. 用户通过二分搜索响应向其符合性分数添加噪声。

**结果:** 两种方法都能直接从噪声数据计算确认阈值且不访问真实标签，证明了有限样本覆盖率保证，并展示了即使在严重随机化的情况下也能保持稳健的覆盖率。

**结论:** 该方法统一了强本地隐私与预测不确定性控制，适用于敏感应用如医学成像或大型语言模型查询。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy-Preserving+Conformal+Prediction+Under+Local+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15721，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15721&send_immediately=true&force_search=false)

**原文摘要:** Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.

</details>


### [45] [When to retrain a machine learning model](https://arxiv.org/abs/2505.14903)
*Regol Florence, Schwinn Leo, Sprague Kyle, Coates Mark, Markovich Thomas*

**主要类别:** cs.LG

**概要:** 提出了一种基于不确定性的方法来决定何时重新训练机器学习模型，该方法在7个数据集的分类任务上表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 应对现实世界中机器学习模型面临的持续和不可预测的数据演变挑战，解决何时重新训练或更新模型的问题。

**方法:** 提出了一种基于不确定性并不断预测模型性能评估指标变化的方法。

**结果:** 该方法在处理分类任务时，在7个数据集上的表现优于现有基准方法。

**结论:** 本文提供了一个重新训练问题的原则性公式，并提出了一种新的方法，解决了现有方法未能全面解决的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+to+retrain+a+machine+learning+model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14903，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14903&send_immediately=true&force_search=false)

**原文摘要:** A significant challenge in maintaining real-world machine learning models is
responding to the continuous and unpredictable evolution of data. Most
practitioners are faced with the difficult question: when should I retrain or
update my machine learning model? This seemingly straightforward problem is
particularly challenging for three reasons: 1) decisions must be made based on
very limited information - we usually have access to only a few examples, 2)
the nature, extent, and impact of the distribution shift are unknown, and 3) it
involves specifying a cost ratio between retraining and poor performance, which
can be hard to characterize. Existing works address certain aspects of this
problem, but none offer a comprehensive solution. Distribution shift detection
falls short as it cannot account for the cost trade-off; the scarcity of the
data, paired with its unusual structure, makes it a poor fit for existing
offline reinforcement learning methods, and the online learning formulation
overlooks key practical considerations. To address this, we present a
principled formulation of the retraining problem and propose an
uncertainty-based method that makes decisions by continually forecasting the
evolution of model performance evaluated with a bounded metric. Our experiments
addressing classification tasks show that the method consistently outperforms
existing baselines on 7 datasets.

</details>


### [46] [The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models](https://arxiv.org/abs/2505.14964)
*Dave Cook, Tim Klawa*

**主要类别:** cs.LG

**概要:** This paper presents smart-sizing, a training data strategy focusing on label diversity and model-guided selection, which allows models trained on less data to perform as well as those trained on full datasets, especially in rare-event detection and edge-case generalization.


<details>
  <summary>更多</summary>
  
**动机:** To improve model generalization in AI systems used in high-consequence domains by optimizing the use of limited resources and reducing redundancy and noise in traditional annotation strategies.

**方法:** Introducing smart-sizing, which includes Adaptive Label Optimization (ALO) for prioritizing meaningful labels, using pre-labeling triage, annotator disagreement analysis, and iterative feedback.

**结果:** Models trained on 20-40% of curated data can match or exceed full-data baselines in rare-class recall and edge-case generalization, and latent labeling errors can distort evaluation.

**结论:** Smart-sizing optimizes annotation processes to align with mission outcomes, enabling robust models with fewer labels, and supports efficient AI development pipelines.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Achilles+Heel+of+AI%3A+Fundamentals+of+Risk-Aware+Training+Data+for+High-Consequence+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14964，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14964&send_immediately=true&force_search=false)

**原文摘要:** AI systems in high-consequence domains such as defense, intelligence, and
disaster response must detect rare, high-impact events while operating under
tight resource constraints. Traditional annotation strategies that prioritize
label volume over informational value introduce redundancy and noise, limiting
model generalization. This paper introduces smart-sizing, a training data
strategy that emphasizes label diversity, model-guided selection, and marginal
utility-based stopping. We implement this through Adaptive Label Optimization
(ALO), combining pre-labeling triage, annotator disagreement analysis, and
iterative feedback to prioritize labels that meaningfully improve model
performance. Experiments show that models trained on 20 to 40 percent of
curated data can match or exceed full-data baselines, particularly in
rare-class recall and edge-case generalization. We also demonstrate how latent
labeling errors embedded in training and validation sets can distort
evaluation, underscoring the need for embedded audit tools and
performance-aware governance. Smart-sizing reframes annotation as a
feedback-driven process aligned with mission outcomes, enabling more robust
models with fewer labels and supporting efficient AI development pipelines for
frontier models and operational systems.

</details>


### [47] [Neural Conditional Transport Maps](https://arxiv.org/abs/2505.15808)
*Carlos Rodriguez-Pardo, Leonardo Chiani, Emanuele Borgonovo, Massimo Tavoni*

**主要类别:** cs.LG

**概要:** 提出了一种神经框架来学习概率分布之间的条件最优传输映射，并引入了处理分类和连续条件变量的机制。该方法在多个基准测试中表现出色，并在全局灵敏度分析中展示了高性能。


<details>
  <summary>更多</summary>
  
**动机:** 扩展最优传输原则在复杂高维领域的应用，如生成建模和黑盒模型可解释性。

**方法:** 核心是一个超网络，根据输入生成传输层参数，从而创建自适应映射。

**结果:** 在多个基准测试中表现优于基线配置，并在计算基于最优传输的灵敏度指数时表现出高效率。

**结论:** 此工作推进了条件最优传输的最先进水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Conditional+Transport+Maps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15808，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15808&send_immediately=true&force_search=false)

**原文摘要:** We present a neural framework for learning conditional optimal transport (OT)
maps between probability distributions. Our approach introduces a conditioning
mechanism capable of processing both categorical and continuous conditioning
variables simultaneously. At the core of our method lies a hypernetwork that
generates transport layer parameters based on these inputs, creating adaptive
mappings that outperform simpler conditioning methods. Comprehensive ablation
studies demonstrate the superior performance of our method over baseline
configurations. Furthermore, we showcase an application to global sensitivity
analysis, offering high performance in computing OT-based sensitivity indices.
This work advances the state-of-the-art in conditional optimal transport,
enabling broader application of optimal transport principles to complex,
high-dimensional domains such as generative modeling and black-box model
explainability.

</details>


### [48] [TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction](https://arxiv.org/abs/2505.14919)
*Frederik Wenkel, Wilson Tu, Cassandra Masschelein, Hamed Shirzad, Cian Eastwood, Shawn T. Whitfield, Ihab Bendidi, Craig Russell, Liam Hodgson, Yassir El Mesbahi, Jiarui Ding, Marta M. Fay, Berton Earnshaw, Emmanuel Noutahi, Alisandra K. Denton*

**主要类别:** cs.LG

**概要:** 提出TxPert方法，利用多个生物知识网络，在未见条件下预测转录反应，对图谱、模型架构和数据影响进行了深入分析，并扩展了扰动建模的评估框架。


<details>
  <summary>更多</summary>
  
**动机:** 准确预测细胞对基因扰动的响应对于理解疾病机制和设计有效疗法至关重要，但穷尽所有可能的扰动（如多基因扰动或跨组织和细胞类型）的探索成本过高，因此需要能推广到未知条件的方法。

**方法:** 提出了TxPert方法，该方法利用多个生物知识网络来预测未见条件下的转录反应。

**结果:** 在三种具有挑战性的设置下验证了所提方法的有效性：未见过的单扰动；未见过的双扰动；和未见过的细胞系。

**结论:** 本研究展示了基因关系知识图谱如何改善分布外预测，并通过深入分析和扩展基准测试框架提高了对扰动建模的评价标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TxPert%3A+Leveraging+Biochemical+Relationships+for+Out-of-Distribution+Transcriptomic+Perturbation+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14919，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14919&send_immediately=true&force_search=false)

**原文摘要:** Accurately predicting cellular responses to genetic perturbations is
essential for understanding disease mechanisms and designing effective
therapies. Yet exhaustively exploring the space of possible perturbations
(e.g., multi-gene perturbations or across tissues and cell types) is
prohibitively expensive, motivating methods that can generalize to unseen
conditions. In this work, we explore how knowledge graphs of gene-gene
relationships can improve out-of-distribution (OOD) prediction across three
challenging settings: unseen single perturbations; unseen double perturbations;
and unseen cell lines. In particular, we present: (i) TxPert, a new
state-of-the-art method that leverages multiple biological knowledge networks
to predict transcriptional responses under OOD scenarios; (ii) an in-depth
analysis demonstrating the impact of graphs, model architecture, and data on
performance; and (iii) an expanded benchmarking framework that strengthens
evaluation standards for perturbation modeling.

</details>


### [49] [Anomaly Detection Based on Critical Paths for Deep Neural Networks](https://arxiv.org/abs/2505.14967)
*Fangzhen Zhao, Chenyi Zhang, Naipeng Dong, Ming Li, Jinxiao Shan*

**主要类别:** cs.LG

**概要:** 提出一种基于遗传演化和突变的方法来从深度神经网络中提取关键路径，并利用这些路径进行异常检测。该方法在多个路径上集成随机子空间采样和投票机制来综合检测结果。实验表明，该方法不仅优于现有技术，而且能以高精度检测多种类型的异常。


<details>
  <summary>更多</summary>
  
**动机:** 理解深度神经网络（DNN）的决策过程以及防御DNN的困难促使研究者寻找新的解释性方法。

**方法:** 通过遗传演化和突变识别关键检测路径，并通过随机子空间采样和投票机制整合多个路径的检测结果。

**结果:** 该方法能够有效检测多种类型的异常，并且在准确性上优于当前最先进的方法。

**结论:** 所提出的基于路径的异常检测方法展示了良好的性能和广泛适用性，为进一步研究DNN的可解释性和安全性提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Anomaly+Detection+Based+on+Critical+Paths+for+Deep+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14967，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14967&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks (DNNs) are notoriously hard to understand and difficult
to defend. Extracting representative paths (including the neuron activation
values and the connections between neurons) from DNNs using software
engineering approaches has recently shown to be a promising approach in
interpreting the decision making process of blackbox DNNs, as the extracted
paths are often effective in capturing essential features. With this in mind,
this work investigates a novel approach that extracts critical paths from DNNs
and subsequently applies the extracted paths for the anomaly detection task,
based on the observation that outliers and adversarial inputs do not usually
induce the same activation pattern on those paths as normal (in-distribution)
inputs.
  In our approach, we first identify critical detection paths via genetic
evolution and mutation. Since different paths in a DNN often capture different
features for the same target class, we ensemble detection results from multiple
paths by integrating random subspace sampling and a voting mechanism. Compared
with state-of-the-art methods, our experimental results suggest that our method
not only outperforms them, but it is also suitable for the detection of a broad
range of anomaly types with high accuracy.

</details>


### [50] [Foundations of Unknown-aware Machine Learning](https://arxiv.org/abs/2505.14933)
*Xuefeng Du*

**主要类别:** cs.LG

**概要:** 本文提出了新的算法和理论框架，以解决从标准神经网络到现代基础模型的分布不确定性及未知类别问题，促进未知感知学习作为新的学习范式的发展。


<details>
  <summary>更多</summary>
  
**动机:** 确保机器学习模型在开放世界部署中的可靠性和安全性是人工智能安全的核心挑战。传统学习范式假设训练和推理之间没有分布偏移，这通常会导致对分布外输入的过于自信的预测。

**方法:** 本文提出了新的异常合成方法（VOS、NPOS和DREAM-OOD）来生成训练过程中的有用未知信息，并引入了SAL理论和算法框架，利用野外未标记数据在现实部署条件下增强OOD检测。此外，还扩展了可靠学习到基础模型，包括开发了HaloScope用于检测LLMs中的幻觉，MLLMGuard用于防御多模态模型中的恶意提示，以及数据清理方法以去噪人类反馈。

**结果:** 所提出的方法在提高模型对未知输入的识别和适应能力方面表现出色，并提供了正式的可靠性保证。此外，这些工具解决了威胁大规模模型部署安全性的失效模式。

**结论:** 本文提出了一种新的未知感知学习框架，使模型能够识别和处理新颖的输入，而无需标记的OOD数据。此外，还开发了针对基础模型的可靠性学习扩展，包括LLMs中的幻觉检测、多模态模型中的恶意提示防御以及用于更好对齐的人类反馈去噪的数据清理方法。这些贡献促进了未知感知学习作为一种新范式的推广，并希望它能以最小的人力投入推进AI系统的可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundations+of+Unknown-aware+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14933，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14933&send_immediately=true&force_search=false)

**原文摘要:** Ensuring the reliability and safety of machine learning models in open-world
deployment is a central challenge in AI safety. This thesis develops both
algorithmic and theoretical foundations to address key reliability issues
arising from distributional uncertainty and unknown classes, from standard
neural networks to modern foundation models like large language models (LLMs).
  Traditional learning paradigms, such as empirical risk minimization (ERM),
assume no distribution shift between training and inference, often leading to
overconfident predictions on out-of-distribution (OOD) inputs. This thesis
introduces novel frameworks that jointly optimize for in-distribution accuracy
and reliability to unseen data. A core contribution is the development of an
unknown-aware learning framework that enables models to recognize and handle
novel inputs without labeled OOD data.
  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to
generate informative unknowns during training. Building on this, we present
SAL, a theoretical and algorithmic framework that leverages unlabeled
in-the-wild data to enhance OOD detection under realistic deployment
conditions. These methods demonstrate that abundant unlabeled data can be
harnessed to recognize and adapt to unforeseen inputs, providing formal
reliability guarantees.
  The thesis also extends reliable learning to foundation models. We develop
HaloScope for hallucination detection in LLMs, MLLMGuard for defending against
malicious prompts in multimodal models, and data cleaning methods to denoise
human feedback used for better alignment. These tools target failure modes that
threaten the safety of large-scale models in deployment.
  Overall, these contributions promote unknown-aware learning as a new
paradigm, and we hope it can advance the reliability of AI systems with minimal
human efforts.

</details>


### [51] [STree: Speculative Tree Decoding for Hybrid State-Space Models](https://arxiv.org/abs/2505.14969)
*Yangchao Wu, Zongyue Qin, Alex Wong, Stefano Soatto*

**主要类别:** cs.LG

**概要:** 提出了一种新的基于树的推测解码算法用于状态空间模型(SSMs)和混合架构，提高了效率并优于现有方法。


<details>
  <summary>更多</summary>
  
**动机:** 提高大规模自回归Transformer模型和状态空间模型的硬件并发性及效率。

**方法:** 开发了一种新的基于树的推测解码算法，利用累积状态转移矩阵结构来最小化对当前SSM状态更新实现的开销。

**结果:** 在三个不同基准上超越了简单的推测解码方法，为SSM和混合模型推理提供了进一步加速的可能性。

**结论:** 提出的算法可以显著提高状态空间模型及其混合架构的推测解码效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是STree%3A+Speculative+Tree+Decoding+for+Hybrid+State-Space+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14969&send_immediately=true&force_search=false)

**原文摘要:** Speculative decoding is a technique to leverage hardware concurrency to
improve the efficiency of large-scale autoregressive (AR) Transformer models by
enabling multiple steps of token generation in a single forward pass.
State-space models (SSMs) are already more efficient than AR Transformers,
since their state summarizes all past data with no need to cache or re-process
tokens in the sliding window context. However, their state can also comprise
thousands of tokens; so, speculative decoding has recently been extended to
SSMs. Existing approaches, however, do not leverage the tree-based verification
methods, since current SSMs lack the means to compute a token tree efficiently.
We propose the first scalable algorithm to perform tree-based speculative
decoding in state-space models (SSMs) and hybrid architectures of SSMs and
Transformer layers. We exploit the structure of accumulated state transition
matrices to facilitate tree-based speculative decoding with minimal overhead to
current SSM state update implementations. With the algorithm, we describe a
hardware-aware implementation that improves naive application of AR Transformer
tree-based speculative decoding methods to SSMs. Furthermore, we outperform
vanilla speculative decoding with SSMs even with a baseline drafting model and
tree structure on three different benchmarks, opening up opportunities for
further speed up with SSM and hybrid model inference. Code will be released
upon paper acceptance.

</details>


### [52] [Flattening Hierarchies with Policy Bootstrapping](https://arxiv.org/abs/2505.14975)
*John L. Zhou, Jonathan C. Kao*

**主要类别:** cs.LG

**概要:** 提出一种通过优势加权重要性采样利用子目标条件策略来训练平滑目标条件策略的算法。该方法在广泛的连续控制任务上与最先进的离线GCRL算法相当或更好，并扩展到复杂的长时序任务。


<details>
  <summary>更多</summary>
  
**动机:** 解决传统GCRL在长时序任务中的挑战以及现有分层方法的复杂性和局限性。

**方法:** 引入一种通过优势加权重要性采样从子目标条件策略中学习平滑目标条件策略的方法，避免了生成子目标空间的生成模型需求。

**结果:** 该方法在多种状态和像素级运动和操作基准测试中表现优异，能够处理复杂的长时序任务。

**结论:** 所提出的算法可以有效扩展离线GCRL到高维状态空间和长时序任务，同时简化了分层方法的复杂性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flattening+Hierarchies+with+Policy+Bootstrapping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14975，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14975&send_immediately=true&force_search=false)

**原文摘要:** Offline goal-conditioned reinforcement learning (GCRL) is a promising
approach for pretraining generalist policies on large datasets of reward-free
trajectories, akin to the self-supervised objectives used to train foundation
models for computer vision and natural language processing. However, scaling
GCRL to longer horizons remains challenging due to the combination of sparse
rewards and discounting, which obscures the comparative advantages of primitive
actions with respect to distant goals. Hierarchical RL methods achieve strong
empirical results on long-horizon goal-reaching tasks, but their reliance on
modular, timescale-specific policies and subgoal generation introduces
significant additional complexity and hinders scaling to high-dimensional goal
spaces. In this work, we introduce an algorithm to train a flat
(non-hierarchical) goal-conditioned policy by bootstrapping on
subgoal-conditioned policies with advantage-weighted importance sampling. Our
approach eliminates the need for a generative model over the (sub)goal space,
which we find is key for scaling to high-dimensional control in large state
spaces. We further show that existing hierarchical and bootstrapping-based
approaches correspond to specific design choices within our derivation. Across
a comprehensive suite of state- and pixel-based locomotion and manipulation
benchmarks, our method matches or surpasses state-of-the-art offline GCRL
algorithms and scales to complex, long-horizon tasks where prior approaches
fail.

</details>


### [53] [Unlearning Algorithmic Biases over Graphs](https://arxiv.org/abs/2505.14945)
*O. Deniz Kose, Gonzalo Mateos, Yanning Shen*

**主要类别:** cs.LG

**概要:** 提出了一种新的图无学习策略，通过单步牛顿更新提供可验证的偏差缓解，作为现有公平性增强方法的轻量级替代方案。


<details>
  <summary>更多</summary>
  
**动机:** 解决图数据固有的偏差放大问题，并将其作为偏差缓解工具。

**方法:** 开发了一种无需训练的无学习过程，包括公平感知节点特征无学习策略和基于严格偏差分析的结构无学习方法。

**结果:** 实验证明了无学习策略在偏差缓解方面的有效性，并展示了与从头重新训练相比有利的效用-复杂度权衡。

**结论:** 提出的方法为处理遗忘权法规下的机器学习模型提供了新的思路，并且在计算效率和性能保证方面具有优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlearning+Algorithmic+Biases+over+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14945，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14945&send_immediately=true&force_search=false)

**原文摘要:** The growing enforcement of the right to be forgotten regulations has
propelled recent advances in certified (graph) unlearning strategies to comply
with data removal requests from deployed machine learning (ML) models.
Motivated by the well-documented bias amplification predicament inherent to
graph data, here we take a fresh look at graph unlearning and leverage it as a
bias mitigation tool. Given a pre-trained graph ML model, we develop a
training-free unlearning procedure that offers certifiable bias mitigation via
a single-step Newton update on the model weights. This way, we contribute a
computationally lightweight alternative to the prevalent training- and
optimization-based fairness enhancement approaches, with quantifiable
performance guarantees. We first develop a novel fairness-aware nodal feature
unlearning strategy along with refined certified unlearning bounds for this
setting, whose impact extends beyond the realm of graph unlearning. We then
design structural unlearning methods endowed with principled selection
mechanisms over nodes and edges informed by rigorous bias analyses. Unlearning
these judiciously selected elements can mitigate algorithmic biases with
minimal impact on downstream utility (e.g., node classification accuracy).
Experimental results over real networks corroborate the bias mitigation
efficacy of our unlearning strategies, and delineate markedly favorable
utility-complexity trade-offs relative to retraining from scratch using
augmented graph data obtained via removals.

</details>


### [54] [Privacy Preserving Conversion Modeling in Data Clean Room](https://arxiv.org/abs/2505.14959)
*Kungang Li, Xiangyi Chen, Ling Leng, Jiajing Xu, Jiankai Sun, Behnam Rezaei*

**主要类别:** cs.LG

**概要:** 提出了一种新的模型训练框架，在保护隐私的同时实现了高效的转化率预测。


<details>
  <summary>更多</summary>
  
**动机:** 在线广告中准确预测转化率(CVR)对于提高广告效率和用户满意度至关重要，但传统方法面临广告商不愿分享敏感转化数据和在安全环境如数据清洁室中模型训练的限制。

**方法:** 引入了新的模型训练框架，包括使用批量级聚合梯度、基于适配器的参数高效微调和梯度压缩以及去偏技术。

**结果:** 实验结果显示，该方法在工业数据集上达到了与其他方法相当的ROCAUC性能，同时大幅减少了通信开销并符合广告商的隐私要求和用户的隐私选择。

**结论:** 提出的方法在保护隐私的同时实现了与其他方法相当的ROCAUC性能，并显著降低了通信开销。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Privacy+Preserving+Conversion+Modeling+in+Data+Clean+Room，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14959，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14959&send_immediately=true&force_search=false)

**原文摘要:** In the realm of online advertising, accurately predicting the conversion rate
(CVR) is crucial for enhancing advertising efficiency and user satisfaction.
This paper addresses the challenge of CVR prediction while adhering to user
privacy preferences and advertiser requirements. Traditional methods face
obstacles such as the reluctance of advertisers to share sensitive conversion
data and the limitations of model training in secure environments like data
clean rooms. We propose a novel model training framework that enables
collaborative model training without sharing sample-level gradients with the
advertising platform. Our approach introduces several innovative components:
(1) utilizing batch-level aggregated gradients instead of sample-level
gradients to minimize privacy risks; (2) applying adapter-based
parameter-efficient fine-tuning and gradient compression to reduce
communication costs; and (3) employing de-biasing techniques to train the model
under label differential privacy, thereby maintaining accuracy despite
privacy-enhanced label perturbations. Our experimental results, conducted on
industrial datasets, demonstrate that our method achieves competitive ROCAUC
performance while significantly decreasing communication overhead and complying
with both advertiser privacy requirements and user privacy choices. This
framework establishes a new standard for privacy-preserving, high-performance
CVR prediction in the digital advertising landscape.

</details>


### [55] [One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks](https://arxiv.org/abs/2505.15009)
*Quan Nguyen, Thanh Nguyen-Tang*

**主要类别:** cs.LG

**概要:** 研究了一层Transformer在无噪声和有噪声的情况下对下一个Token预测的上下文推理能力，证明了其贝叶斯最优性，并且展示了预期损失以线性速率收敛到贝叶斯风险。


<details>
  <summary>更多</summary>
  
**动机:** 现有理论主要集中在理解单一梯度步长或者样本数无限情况下的上下文推理行为，缺乏收敛率和泛化能力的研究。

**方法:** 通过有限样本分析，证明一层Transformer在具有线性和ReLU注意力时的贝叶斯最优性，并展示其收敛性。

**结果:** 这些Transformer的预期损失以线性速率收敛到贝叶斯风险，并且模型能够泛化到未见样本上。

**结论:** 研究表明一层Transformer在特定条件下具有强大的逼近能力和良好的收敛及泛化性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One-Layer+Transformers+are+Provably+Optimal+for+In-context+Reasoning+and+Distributional+Association+Learning+in+Next-Token+Prediction+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15009，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15009&send_immediately=true&force_search=false)

**原文摘要:** We study the approximation capabilities and on-convergence behaviors of
one-layer transformers on the noiseless and noisy in-context reasoning of
next-token prediction. Existing theoretical results focus on understanding the
in-context reasoning behaviors for either the first gradient step or when the
number of samples is infinite. Furthermore, no convergence rates nor
generalization abilities were known. Our work addresses these gaps by showing
that there exists a class of one-layer transformers that are provably
Bayes-optimal with both linear and ReLU attention. When being trained with
gradient descent, we show via a finite-sample analysis that the expected loss
of these transformers converges at linear rate to the Bayes risk. Moreover, we
prove that the trained models generalize to unseen samples as well as exhibit
learning behaviors that were empirically observed in previous works. Our
theoretical findings are further supported by extensive empirical validations.

</details>


### [56] [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
*Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane S. Boning, Dina Katabi*

**主要类别:** cs.LG

**概要:** 提出了一种新的框架Tango，通过同时训练生成器和验证器来提高大型语言模型的推理能力，验证器通过生成式强化学习进行训练并随生成器共同进化。实验表明Tango在多个数学基准测试和领域外推理任务中达到了最先进的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大型语言模型的强化学习后训练方法容易受到奖励黑客攻击并且泛化性差。

**方法:** 提出了Tango框架，该框架使用强化学习同时训练生成器和生成式的、过程级别的验证器，并且验证器仅基于结果级的验证正确性奖励进行训练。

**结果:** Tango的生成器在五个竞争级别的数学基准测试和四个具有挑战性的跨领域推理任务中表现出最佳性能；验证器在ProcessBench数据集上领先。

**结论:** Tango展示了在提高大型语言模型推理能力方面的显著效果，特别是在困难的数学推理问题上。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RL+Tango%3A+Reinforcing+Generator+and+Verifier+Together+for+Language+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15034，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15034&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.

</details>


### [57] [PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration](https://arxiv.org/abs/2505.15047)
*Yingming Pu, Tao Lin, Hongyu Chen*

**主要类别:** cs.LG

**概要:** 提出了一种基于信息论框架的插件式方法PiFlow，显著提高了自动化科学发现的效率和解决方案质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法在自动化科学发现中缺乏理性约束，导致假设无目标且无法系统地减少不确定性。

**方法:** 引入了PiFlow框架，将自动化科学发现视为由原则指导的结构化不确定性减少问题。

**结果:** 在三个不同科学领域中，PiFlow提升了73.55%的AUC，并提高了94.06%的解决方案质量。

**结论:** PiFlow是一种插件式方法，推动了高效自动化科学发现的新范式转变。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PiFlow%3A+Principle-aware+Scientific+Discovery+with+Multi-Agent+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15047，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15047&send_immediately=true&force_search=false)

**原文摘要:** Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate
remarkable potential for scientific discovery. Existing approaches, however,
often automate scientific discovery using predefined workflows that lack
rationality constraints. This often leads to aimless hypothesizing and a
failure to consistently link hypotheses with evidence, thereby hindering
systematic uncertainty reduction. Overcoming these limitations fundamentally
requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an
information-theoretical framework, treating automated scientific discovery as a
structured uncertainty reduction problem guided by principles (e.g., scientific
laws). In evaluations across three distinct scientific domains -- discovering
nanomaterial structures, bio-molecules, and superconductor candidates with
targeted properties -- our method significantly improves discovery efficiency,
reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property
values versus exploration steps, and enhances solution quality by 94.06\%
compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a
Plug-and-Play method, establishing a novel paradigm shift in highly efficient
automated scientific discovery, paving the way for more robust and accelerated
AI-driven research. Code is publicly available at our
\href{https://github.com/amair-lab/PiFlow}{GitHub}.

</details>


### [58] [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
*Sergey Pankov, Georges Harik*

**主要类别:** cs.LG

**概要:** 提出一种简单概率规则来削减注意力机制中的大部分反向传播计算，将计算复杂度从O(n²)降低到O(nc)，实验表明对于典型Transformer模型，在长序列训练时该方法相对梯度方差增加仅约1%，并且适合高效稀疏矩阵实现。


<details>
  <summary>更多</summary>
  
**动机:** 减少Transformer架构中注意力机制的计算成本，特别是在长序列情况下，其计算需求随序列长度n呈二次增长，而大部分注意力权重非常小。

**方法:** 提出一个由单一参数c控制的简单概率规则，通过削减大部分注意力权重的反向传播，每个注意力头每token最多保留c个交互。

**结果:** 实验验证了对于典型的Transformer模型，削减99%的注意力梯度流（即选择c约为20-30）时，对于n约为2000的情况，相对梯度方差增加仅约1%，且随着n增大该值会减小。

**结论:** 此方法适用于高效的稀疏矩阵实现，有望使长序列Transformer模型训练时的反向传播计算成本相对于前向传播可以忽略不计。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SUS+backprop%3A+linear+backpropagation+algorithm+for+long+inputs+in+transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15080，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15080&send_immediately=true&force_search=false)

**原文摘要:** It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient sparse matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.

</details>


### [59] [Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features](https://arxiv.org/abs/2505.15083)
*Jeremy Qin*

**主要类别:** cs.LG

**概要:** 提出了一种结合静态和时间相关特征的透明时间序列预测方法，该方法在保持预测准确性的同时增强了模型的解释性。


<details>
  <summary>更多</summary>
  
**动机:** 确保医疗领域时间序列预测模型的透明性和可解释性对于其广泛应用至关重要。

**方法:** 扩展了现有的双层透明框架，通过结构化的方式整合外生时间序列特征，并引入编码机制来分解这些特征以提取可解释的模式。

**结果:** 实验表明，新方法在多个合成数据集上保持了预测性能，并提升了模型的解释性和鲁棒性。

**结论:** 这项研究朝着构建更健壮且通用的时间序列预测模型迈出了重要一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Multi-Modal+Forecasting%3A+Integrating+Static+and+Dynamic+Features，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15083，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15083&send_immediately=true&force_search=false)

**原文摘要:** Time series forecasting plays a crucial role in various applications,
particularly in healthcare, where accurate predictions of future health
trajectories can significantly impact clinical decision-making. Ensuring
transparency and explainability of the models responsible for these tasks is
essential for their adoption in critical settings. Recent work has explored a
top-down approach to bi-level transparency, focusing on understanding trends
and properties of predicted time series using static features. In this work, we
extend this framework by incorporating exogenous time series features alongside
static features in a structured manner, while maintaining cohesive
interpretation. Our approach leverages the insights of trajectory comprehension
to introduce an encoding mechanism for exogenous time series, where they are
decomposed into meaningful trends and properties, enabling the extraction of
interpretable patterns. Through experiments on several synthetic datasets, we
demonstrate that our approach remains predictive while preserving
interpretability and robustness. This work represents a step towards developing
robust, and generalized time series forecasting models. The code is available
at https://github.com/jeremy-qin/TIMEVIEW

</details>


### [60] [Graph Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15116)
*Zehong Wang, Zheyuan Liu, Tianyi Ma, Jiazheng Li, Zheyuan Zhang, Xingbo Fu, Yiyang Li, Zhengqing Yuan, Wei Song, Yijun Ma, Qingkai Zeng, Xiusi Chen, Jianan Zhao, Jundong Li, Meng Jiang, Pietro Lio, Nitesh Chawla, Chuxu Zhang, Yanfang Ye*

**主要类别:** cs.LG

**概要:** 本文综述了图基础模型（GFMs），提供了包括backbone架构、预训练策略和适应机制在内的模块框架统一不同工作，并探讨了其理论基础和关键挑战。


<details>
  <summary>更多</summary>
  
**动机:** 虽然foundation models在自然语言处理、视觉和多模态学习方面取得了巨大成功，但将其能力扩展到具有非欧几里得结构和复杂关系语义的图上带来了独特的挑战和新的机会。

**方法:** 通过backbone architectures、pretraining strategies和adaptation mechanisms这三部分组成模块框架统一不同的努力。

**结果:** 对GFMs进行了全面概述，分类并回顾了代表性方法、关键创新和理论见解。

**结论:** GFMs有潜力成为结构化数据开放推理的基础架构。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Foundation+Models%3A+A+Comprehensive+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15116，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15116&send_immediately=true&force_search=false)

**原文摘要:** Graph-structured data pervades domains such as social networks, biological
systems, knowledge graphs, and recommender systems. While foundation models
have transformed natural language processing, vision, and multimodal learning
through large-scale pretraining and generalization, extending these
capabilities to graphs -- characterized by non-Euclidean structures and complex
relational semantics -- poses unique challenges and opens new opportunities. To
this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose
intelligence to structured data, enabling broad transfer across graph-centric
tasks and domains. This survey provides a comprehensive overview of GFMs,
unifying diverse efforts under a modular framework comprising three key
components: backbone architectures, pretraining strategies, and adaptation
mechanisms. We categorize GFMs by their generalization scope -- universal,
task-specific, and domain-specific -- and review representative methods, key
innovations, and theoretical insights within each category. Beyond methodology,
we examine theoretical foundations including transferability and emergent
capabilities, and highlight key challenges such as structural alignment,
heterogeneity, scalability, and evaluation. Positioned at the intersection of
graph learning and general-purpose AI, GFMs are poised to become foundational
infrastructure for open-ended reasoning over structured data. This survey
consolidates current progress and outlines future directions to guide research
in this rapidly evolving field. Resources are available at
https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies](https://arxiv.org/abs/2505.14689)
*Ashwani Anand, Satya Prakash Nayak, Ritam Raha, Anne-Kathrin Schmuck*

**主要类别:** cs.AI

**概要:** 提出了一种新的动态后屏蔽框架，用于确保预计算的概率策略满足完整的ω-正则正确性属性，实现了从安全性屏蔽到同时保证活跃性的转变。主要特征包括基于策略模板的自适应运行时屏蔽（STARs），它能最小化干扰并支持运行时适应变化的规范或执行器故障。


<details>
  <summary>更多</summary>
  
**动机:** 从只关注安全性的安全性屏蔽转向既能确保安全性又能确保活跃性的屏蔽过程。

**方法:** 使用基于策略模板的自适应运行时屏蔽（STARs），通过可调的执行参数在运行时平衡正式义务和特定任务的行为。

**结果:** 在移动机器人基准上评估了STARs，展示了其可控的干扰能力，用于强制执行更新的ω-正则正确性属性。

**结论:** 提出的框架和方法为确保复杂的ω-正则正确性属性提供了新的解决方案，并且特别适用于网络物理系统应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Follow+the+STARs%3A+Dynamic+%24%CF%89%24-Regular+Shielding+of+Learned+Policies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14689&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a novel dynamic post-shielding framework that enforces
the full class of $\omega$-regular correctness properties over pre-computed
probabilistic policies. This constitutes a paradigm shift from the predominant
setting of safety-shielding -- i.e., ensuring that nothing bad ever happens --
to a shielding process that additionally enforces liveness -- i.e., ensures
that something good eventually happens. At the core, our method uses
Strategy-Template-based Adaptive Runtime Shields (STARs), which leverage
permissive strategy templates to enable post-shielding with minimal
interference. As its main feature, STARs introduce a mechanism to dynamically
control interference, allowing a tunable enforcement parameter to balance
formal obligations and task-specific behavior at runtime. This allows to
trigger more aggressive enforcement when needed, while allowing for optimized
policy choices otherwise. In addition, STARs support runtime adaptation to
changing specifications or actuator failures, making them especially suited for
cyber-physical applications. We evaluate STARs on a mobile robot benchmark to
demonstrate their controllable interference when enforcing (incrementally
updated) $\omega$-regular correctness properties over learned probabilistic
policies.

</details>


### [62] [R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution](https://arxiv.org/abs/2505.14738)
*Xu Yang, Xiao Yang, Shikai Fang, Bowen Xian, Yuante Li, Jian Wang, Minrui Xu, Haoran Pan, Xinpeng Hong, Weiqing Liu, Yelong Shen, Weizhu Chen, Jiang Bian*

**主要类别:** cs.AI

**概要:** R&D-Agent是一种双代理框架，用于数据科学任务的迭代探索，通过自动化减少专家级性能差距。


<details>
  <summary>更多</summary>
  
**动机:** 尽管AI和ML的进步推动了数据科学的发展，但复杂性和专业需求阻碍了进一步进步。

**方法:** R&D-Agent包含两个代理：Researcher生成想法，Developer优化代码。

**结果:** 在MLE-Bench上表现最佳，展示了加速创新和提高多种数据科学应用精度的潜力。

**结论:** 开源R&D-Agent，促进更广泛的应用和发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是R%26D-Agent%3A+Automating+Data-Driven+AI+Solution+Building+Through+LLM-Powered+Automated+Research%2C+Development%2C+and+Evolution，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14738，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14738&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in AI and ML have transformed data science, yet increasing
complexity and expertise requirements continue to hinder progress. While
crowdsourcing platforms alleviate some challenges, high-level data science
tasks remain labor-intensive and iterative. To overcome these limitations, we
introduce R&D-Agent, a dual-agent framework for iterative exploration. The
Researcher agent uses performance feedback to generate ideas, while the
Developer agent refines code based on error feedback. By enabling multiple
parallel exploration traces that merge and enhance one another, R&D-Agent
narrows the gap between automated solutions and expert-level performance.
Evaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine
learning engineering agent, demonstrating its potential to accelerate
innovation and improve precision across diverse data science applications. We
have open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.

</details>


### [63] [FOL-Pretrain: A complexity annotated corpus of first-order logic](https://arxiv.org/abs/2505.14932)
*Isabelle Lee, Sarah Liaw, Dani Yogatama*

**主要类别:** cs.AI

**概要:** 提出一个大规模开放的数据集，用于研究大型语言模型中的逻辑推理。


<details>
  <summary>更多</summary>
  
**动机:** 理解大型语言模型如何内化和执行复杂算法的能力有限，且预训练数据难以隔离推理机制。

**方法:** 引入一个大规模、完全开放、复杂性注释的数据集，包含人类标注和合成生成的例子，并提供每个合成例子的元数据来追踪其算法起源。

**结果:** 数据集由35亿个标记组成，包括880万个LLM增强的人类注释示例和750万个合成生成的例子。

**结论:** 该数据集旨在为研究大型语言模型学习和泛化符号推理过程提供可扩展且可解释的工具，促进对现代模型算法能力的更透明和有针对性的研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FOL-Pretrain%3A+A+complexity+annotated+corpus+of+first-order+logic，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14932，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14932&send_immediately=true&force_search=false)

**原文摘要:** Transformer-based large language models (LLMs) have demonstrated remarkable
reasoning capabilities such as coding and solving mathematical problems to
commonsense inference. While these tasks vary in complexity, they all require
models to integrate and compute over structured information. Despite recent
efforts to reverse-engineer LLM behavior through controlled experiments, our
understanding of how these models internalize and execute complex algorithms
remains limited. Progress has largely been confined to small-scale studies or
shallow tasks such as basic arithmetic and grammatical pattern matching. One
barrier to deeper understanding is the nature of pretraining data -- vast,
heterogeneous, and often poorly annotated, making it difficult to isolate
mechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully
open, complexity-annotated dataset of first-order logic reasoning traces,
designed to probe and analyze algorithmic reasoning in LLMs. The dataset
consists of 3.5 billion tokens, including 8.8 million LLM-augmented,
human-annotated examples and 7.5 million synthetically generated examples. Each
synthetic example is verifiably correct, produced by a custom automated theorem
solver, and accompanied by metadata tracing its algorithmic provenance. We aim
to provide a scalable, interpretable artifact for studying how LLMs learn and
generalize symbolic reasoning processes, paving the way for more transparent
and targeted investigations into the algorithmic capabilities of modern models.

</details>


### [64] [To Be or Not To Be: Vector ontologies as a truly formal ontological framework](https://arxiv.org/abs/2505.14940)
*Kaspar Rothenfusser*

**主要类别:** cs.AI

**概要:** 本文探讨了形式本体论的概念，并提出重新定义它为真正的形式本体论，强调其在人工智能中的应用潜力。


<details>
  <summary>更多</summary>
  
**动机:** 作者旨在澄清和纠正对胡塞尔形式本体论的理解偏差，并探索形式本体论在人工智能等信息系统的潜在应用。

**方法:** 作者通过理论分析和论证指出所谓的形式本体论并非真正符合胡塞尔的形式本体论概念，并提出一种新的视角来理解本体论。

**结果:** 作者证明了所研究的形式本体论违反了胡塞尔逻辑调查中的两个重要概念，并提出了一种新的方法来设计可扩展且互操作的信息制品。此外，展示了基于向量空间公理的形式本体论可以表达大多数基础本体论的概念化。

**结论:** 作者提出重新定义先前被认为是形式本体论的工作，将其视为基础本体论，并认为遵循胡塞尔条件的形式本体论能够捕捉更客观的事物，同时提出需要深入研究向量本体论作为人机互操作的本体论框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是To+Be+or+Not+To+Be%3A+Vector+ontologies+as+a+truly+formal+ontological+framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14940，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14940&send_immediately=true&force_search=false)

**原文摘要:** Since Edmund Husserl coined the term "Formal Ontologies" in the early 20th
century, a field that identifies itself with this particular branch of sciences
has gained increasing attention. Many authors, and even Husserl himself have
developed what they claim to be formal ontologies. I argue that under close
inspection, none of these so claimed formal ontologies are truly formal in the
Husserlian sense. More concretely, I demonstrate that they violate the two most
important notions of formal ontology as developed in Husserl's Logical
Investigations, namely a priori validity independent of perception and
formalism as the total absence of content. I hence propose repositioning the
work previously understood as formal ontology as the foundational ontology it
really is. This is to recognize the potential of a truly formal ontology in the
Husserlian sense. Specifically, I argue that formal ontology following his
conditions, allows us to formulate ontological structures, which could capture
what is more objectively without presupposing a particular framework arising
from perception. I further argue that the ability to design the formal
structure deliberately allows us to create highly scalable and interoperable
information artifacts. As concrete evidence, I showcase that a class of formal
ontology, which uses the axioms of vector spaces, is able to express most of
the conceptualizations found in foundational ontologies. Most importantly, I
argue that many information systems, specifically artificial intelligence, are
likely already using some type of vector ontologies to represent reality in
their internal worldviews and elaborate on the evidence that humans do as well.
I hence propose a thorough investigation of the ability of vector ontologies to
act as a human-machine interoperable ontological framework that allows us to
understand highly sophisticated machines and machines to understand us.

</details>


### [65] [Reinforcement Learning from User Feedback](https://arxiv.org/abs/2505.14946)
*Eric Han, Jun Chen, Karthik Abinav Sankararaman, Xiaoliang Peng, Tengyu Xu, Eryk Helenowski, Kaiyan Peng, Mrinal Kumar, Sinong Wang, Han Fang, Arya Talebzadeh*

**主要类别:** cs.AI

**概要:** This paper introduces RLUF, a method for aligning large language models with real user preferences by using implicit feedback such as emoji reactions.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods rely on expert annotators whose judgments may not reflect everyday user priorities.

**方法:** Introduces a reward model, P[Love], to predict the likelihood of receiving a positive user reaction and integrates it into policy optimization with helpfulness and safety objectives.

**结果:** P[Love] is predictive of increased positive feedback and improves observed positive-feedback rates by 28% in live tests, but introduces reward hacking challenges.

**结论:** RLUF provides a way to align LLMs with real-world user preferences at scale by directly leveraging implicit user signals.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+from+User+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14946&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) are increasingly deployed in diverse user
facing applications, aligning them with real user preferences becomes
essential. Existing methods like Reinforcement Learning from Human Feedback
(RLHF) rely on expert annotators trained on manually defined guidelines, whose
judgments may not reflect the priorities of everyday users. We introduce
Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs
directly to implicit signals from users in production. RLUF addresses key
challenges of user feedback: user feedback is often binary (e.g., emoji
reactions), sparse, and occasionally adversarial. We train a reward model,
P[Love], to predict the likelihood that an LLM response will receive a Love
Reaction, a lightweight form of positive user feedback, and integrate P[Love]
into a multi-objective policy optimization framework alongside helpfulness and
safety objectives. In large-scale experiments, we show that P[Love] is
predictive of increased positive feedback and serves as a reliable offline
evaluator of future user behavior. Policy optimization using P[Love]
significantly raises observed positive-feedback rates, including a 28% increase
in Love Reactions during live A/B tests. However, optimizing for positive
reactions introduces reward hacking challenges, requiring careful balancing of
objectives. By directly leveraging implicit signals from users, RLUF offers a
path to aligning LLMs with real-world user preferences at scale.

</details>


### [66] [Self-Evolving Curriculum for LLM Reasoning](https://arxiv.org/abs/2505.14970)
*Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, Ehsan Kamalloo*

**主要类别:** cs.AI

**概要:** 提出了一种名为Self-Evolving Curriculum (SEC) 的自动课程学习方法，用于强化学习微调大型语言模型，显著提升了模型在规划、归纳推理和数学等不同推理领域中的推理能力，并且在多推理领域的同时微调中实现了更好的技能平衡。


<details>
  <summary>更多</summary>
  
**动机:** 强化学习对微调大型语言模型有效，但训练课程（问题呈现顺序）对微调成功至关重要。现有随机课程和手动设计课程存在局限性。

**方法:** 将课程选择建模为非平稳多臂老虎机问题，利用策略梯度方法作为即时学习增益的代理度量，通过TD(0)方法更新课程策略。

**结果:** 实验表明，SEC在提升模型推理能力以及处理更难、分布外测试问题的泛化能力方面表现优异；同时，在多推理领域的微调中实现更好的技能平衡。

**结论:** SEC为大型语言模型的强化学习微调提供了一个有前景的策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Evolving+Curriculum+for+LLM+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14970&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.

</details>


### [67] [Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility](https://arxiv.org/abs/2505.14983)
*Zahra Zahedi, Shashank Mehrotra, Teruhisa Misu, Kumar Akash*

**主要类别:** cs.AI

**概要:** Develops a computational model using Dynamic Bayesian Network (DBN) to infer cognitive states of AV users and other road users, integrating this information into AV decision-making.


<details>
  <summary>更多</summary>
  
**动机:** To ensure effective and smooth human-AV interactions by analyzing and aligning human needs with automation decisions.

**方法:** Presents a novel computational model in the form of DBN to infer cognitive states like well-being, trust, and intention of AV users and other road users.

**结果:** The model is refined using collected data and its performance is empirically assessed.

**结论:** Extends the model into a causal inference model (CIM) framework for AV decision-making to enhance user well-being and trust while balancing operational costs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Toward+Informed+AV+Decision-Making%3A+Computational+Model+of+Well-being+and+Trust+in+Mobility，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14983，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14983&send_immediately=true&force_search=false)

**原文摘要:** For future human-autonomous vehicle (AV) interactions to be effective and
smooth, human-aware systems that analyze and align human needs with automation
decisions are essential. Achieving this requires systems that account for human
cognitive states. We present a novel computational model in the form of a
Dynamic Bayesian Network (DBN) that infers the cognitive states of both AV
users and other road users, integrating this information into the AV's
decision-making process. Specifically, our model captures the well-being of
both an AV user and an interacting road user as cognitive states alongside
trust. Our DBN models infer beliefs over the AV user's evolving well-being,
trust, and intention states, as well as the possible well-being of other road
users, based on observed interaction experiences. Using data collected from an
interaction study, we refine the model parameters and empirically assess its
performance. Finally, we extend our model into a causal inference model (CIM)
framework for AV decision-making, enabling the AV to enhance user well-being
and trust while balancing these factors with its own operational costs and the
well-being of interacting road users. Our evaluation demonstrates the model's
effectiveness in accurately predicting user's states and guiding informed,
human-centered AV decisions.

</details>


### [68] [HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning](https://arxiv.org/abs/2505.15011)
*Kryspin Varys, Federico Cerutti, Adam Sobey, Timothy J. Norman*

**主要类别:** cs.AI

**概要:** This paper introduces a novel method to integrate both explicit and implicit norms into reinforcement learning, promoting value-aligned behavior.


<details>
  <summary>更多</summary>
  
**动机:** To address the lack of approaches combining different norm representations in literature for creating value-aligned agents.

**方法:** Integrating norms into reinforcement learning by monitoring compliance and summarizing it as 'reputation', which weighs rewards.

**结果:** The proposed method demonstrates its effectiveness in continuous state space traffic problems and shows benefits over using either group of norms separately through ablation studies.

**结论:** Combining explicit and implicit norms improves value-alignment in agents through reinforcement learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HAVA%3A+Hybrid+Approach+to+Value-Alignment+through+Reward+Weighing+for+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15011，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15011&send_immediately=true&force_search=false)

**原文摘要:** Our society is governed by a set of norms which together bring about the
values we cherish such as safety, fairness or trustworthiness. The goal of
value-alignment is to create agents that not only do their tasks but through
their behaviours also promote these values. Many of the norms are written as
laws or rules (legal / safety norms) but even more remain unwritten (social
norms). Furthermore, the techniques used to represent these norms also differ.
Safety / legal norms are often represented explicitly, for example, in some
logical language while social norms are typically learned and remain hidden in
the parameter space of a neural network. There is a lack of approaches in the
literature that could combine these various norm representations into a single
algorithm. We propose a novel method that integrates these norms into the
reinforcement learning process. Our method monitors the agent's compliance with
the given norms and summarizes it in a quantity we call the agent's reputation.
This quantity is used to weigh the received rewards to motivate the agent to
become value-aligned. We carry out a series of experiments including a
continuous state space traffic problem to demonstrate the importance of the
written and unwritten norms and show how our method can find the value-aligned
policies. Furthermore, we carry out ablations to demonstrate why it is better
to combine these two groups of norms rather than using either separately.

</details>


### [69] [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/abs/2505.15068)
*Cheng Qian, Hongyi Du, Hongru Wang, Xiusi Chen, Yuji Zhang, Avirup Sil, Chengxiang Zhai, Kathleen McKeown, Heng Ji*

**主要类别:** cs.AI

**概要:** 提出了一种新的基准测试集ModelingBench和一个多智能体框架ModelingAgent，用于解决开放性、跨学科的数学建模问题，并引入了ModelingJudge系统来评估这些解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基准测试无法反映现实世界中的复杂性，需要开放性和跨学科推理以及计算工具的集成。

**方法:** 引入了ModelingBench，包含来自不同领域的开放性问题；开发了ModelingAgent，一个支持多智能体协作、结构化工作流并能自我完善以生成高质量解决方案的框架；提出了ModelingJudge，一个人类专家参与的循环系统，利用大型语言模型作为领域专家评估解决方案。

**结果:** ModelingAgent在性能上显著优于强基线模型，并且其产生的解决方案经常与人类专家的解决方案难以区分。

**结论:** 这项研究提供了一个全面的框架来评估和推进开放性、跨学科数学建模挑战中的实际问题解决能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ModelingAgent%3A+Bridging+LLMs+and+Mathematical+Modeling+for+Real-World+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15068，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15068&send_immediately=true&force_search=false)

**原文摘要:** Recent progress in large language models (LLMs) has enabled substantial
advances in solving mathematical problems. However, existing benchmarks often
fail to reflect the complexity of real-world problems, which demand open-ended,
interdisciplinary reasoning and integration of computational tools. To address
this gap, we introduce ModelingBench, a novel benchmark featuring
real-world-inspired, open-ended problems from math modeling competitions across
diverse domains, ranging from urban traffic optimization to ecosystem resource
planning. These tasks require translating natural language into formal
mathematical formulations, applying appropriate tools, and producing
structured, defensible reports. ModelingBench also supports multiple valid
solutions, capturing the ambiguity and creativity of practical modeling. We
also present ModelingAgent, a multi-agent framework that coordinates tool use,
supports structured workflows, and enables iterative self-refinement to
generate well-grounded, creative solutions. To evaluate outputs, we further
propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as
domain-specialized judges assessing solutions from multiple expert
perspectives. Empirical results show that ModelingAgent substantially
outperforms strong baselines and often produces solutions indistinguishable
from those of human experts. Together, our work provides a comprehensive
framework for evaluating and advancing real-world problem-solving in
open-ended, interdisciplinary modeling challenges.

</details>


### [70] [lmgame-Bench: How Good are LLMs at Playing Games?](https://arxiv.org/abs/2505.15146)
*Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P. Xing, Ion Stoica, Tajana Rosing, Haojian Jin, Hao Zhang*

**主要类别:** cs.AI

**概要:** This paper introduces lmgame-Bench, a benchmark for evaluating modern large language models using video games. It addresses issues like brittle vision perception and prompt sensitivity by providing a unified API and lightweight perception and memory scaffolds.


<details>
  <summary>更多</summary>
  
**动机:** To create a reliable evaluation method for modern LLMs using video games by addressing challenges such as vision perception, prompt sensitivity, and potential data contamination.

**方法:** Introduce lmgame-Bench which includes platformer, puzzle, and narrative games with a unified Gym-style API and lightweight perception and memory scaffolds.

**结果:** The benchmark is challenging yet capable of differentiating between 13 leading models. Correlation analysis reveals each game tests a unique combination of capabilities. Reinforcement learning on one game transfers to unseen games and external planning tasks.

**结论:** lmgame-Bench provides a robust evaluation framework for LLMs in video games, addressing key challenges and showing transferable skills.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是lmgame-Bench%3A+How+Good+are+LLMs+at+Playing+Games%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15146&send_immediately=true&force_search=false)

**原文摘要:** Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.

</details>


### [71] [Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge](https://arxiv.org/abs/2505.15240)
*Yassir Fathullah, Mark J. F. Gales*

**主要类别:** cs.AI

**概要:** This paper studies uncertainty estimation in LLM comparison frameworks and proposes improved methods for individual and overall ranking uncertainty, showing they can significantly increase system efficiency.


<details>
  <summary>更多</summary>
  
**动机:** To explore uncertainty estimation in comparative LLM frameworks and improve the efficiency of selection processes.

**方法:** Extending Product-of-Experts methods and introducing new uncertainty estimation techniques for individual comparisons and overall ranking.

**结果:** Improved uncertainty estimates can reduce the number of required comparisons by about 50% and ranking-level uncertainty metrics can identify low-performing predictions.

**结论:** Combining absolute and comparative scoring improves performance, and proposed uncertainty estimates enhance system efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalised+Probabilistic+Modelling+and+Improved+Uncertainty+Estimation+in+Comparative+LLM-as-a-judge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15240，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15240&send_immediately=true&force_search=false)

**原文摘要:** This paper explores generalised probabilistic modelling and uncertainty
estimation in comparative LLM-as-a-judge frameworks. We show that existing
Product-of-Experts methods are specific cases of a broader framework, enabling
diverse modelling options. Furthermore, we propose improved uncertainty
estimates for individual comparisons, enabling more efficient selection and
achieving strong performance with fewer evaluations. We also introduce a method
for estimating overall ranking uncertainty. Finally, we demonstrate that
combining absolute and comparative scoring improves performance. Experiments
show that the specific expert model has a limited impact on final rankings but
our proposed uncertainty estimates, especially the probability of reordering,
significantly improve the efficiency of systems reducing the number of needed
comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used
to identify low-performing predictions, where the nature of the probabilistic
model has a notable impact on the quality of the overall uncertainty.

</details>


### [72] [Identification of Probabilities of Causation: A Complete Characterization](https://arxiv.org/abs/2505.15274)
*Xin Shu, Shuai Wang, Ang Li*

**主要类别:** cs.AI

**概要:** This paper proposes a complete set of representative probabilities of causation for multi-valued treatments and outcomes, proves their sufficiency within SCMs, derives tight bounds for them mathematically, and demonstrates their practical relevance.


<details>
  <summary>更多</summary>
  
**动机:** To address the unresolved theoretical characterization of probabilities of causation with multi-valued treatments and outcomes, which limits the scope of causality-based decision-making.

**方法:** Proposing representative probabilities of causation, proving their sufficiency within SCMs, deriving tight bounds using mathematical proofs.

**结果:** A complete set of representative probabilities of causation that can characterize all possible probabilities of causation, tight bounds for these quantities, and demonstration of practical relevance via toy examples.

**结论:** The work resolves a foundational gap in the theoretical characterization of probabilities of causation for multi-valued treatments and outcomes, expanding the scope of causality-based decision-making.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Identification+of+Probabilities+of+Causation%3A+A+Complete+Characterization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15274&send_immediately=true&force_search=false)

**原文摘要:** Probabilities of causation are fundamental to modern decision-making. Pearl
first introduced three binary probabilities of causation, and Tian and Pearl
later derived tight bounds for them using Balke's linear programming. The
theoretical characterization of probabilities of causation with multi-valued
treatments and outcomes has remained unresolved for decades, limiting the scope
of causality-based decision-making. In this paper, we resolve this foundational
gap by proposing a complete set of representative probabilities of causation
and proving that they are sufficient to characterize all possible probabilities
of causation within the framework of Structural Causal Models (SCMs). We then
formally derive tight bounds for these representative quantities using formal
mathematical proofs. Finally, we demonstrate the practical relevance of our
results through illustrative toy examples.

</details>


### [73] [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/abs/2505.15276)
*Rongzhi Zhu, Yi Liu, Zequn Sun, Yiwei Wang, Wei Hu*

**主要类别:** cs.AI

**概要:** This study examines the internal mechanisms of reinforcement learning-trained large reasoning models when prompted to save thinking, identifying three distinct thinking modes and finding that not thinking reduces output length but sacrifices accuracy.


<details>
  <summary>更多</summary>
  
**动机:** Investigating inefficiencies introduced by overthinking in large reasoning models trained with reinforcement learning.

**方法:** Comprehensive analysis of confidence in thinking termination, attention from thinking to generation, and attentional focus on input sections.

**结果:** Three distinct thinking modes identified: no thinking, explicit thinking, and implicit thinking. No thinking reduces output length but sacrifices accuracy, whereas explicit and implicit thinking maintain accuracy with shorter responses.

**结论:** The study reveals fundamental inconsistencies in reinforcement learning-optimized large reasoning models and suggests the need for adaptive improvements for reliable efficiency.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Can+Large+Reasoning+Models+Save+Thinking%3F+Mechanistic+Analysis+of+Behavioral+Divergence+in+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15276&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs) have significantly advanced performance on
complex tasks, yet their tendency to overthink introduces inefficiencies. This
study investigates the internal mechanisms of reinforcement learning
(RL)-trained LRMs when prompted to save thinking, revealing three distinct
thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking
(IT). Through comprehensive analysis of confidence in thinking termination,
attention from thinking to generation, and attentional focus on input sections,
we uncover key factors influencing the reasoning behaviors. We further find
that NT reduces output length at the cost of accuracy, while ET and IT maintain
accuracy with reduced response length. Our findings expose fundamental
inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for
reliable efficiency.

</details>


### [74] [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/abs/2505.15400)
*Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Haodong Zhao, Hao Li, Jiansong Chen, Ke Zeng, Xunliang Cai*

**主要类别:** cs.AI

**概要:** 提出自适应自我恢复推理（ASRR）框架，通过抑制不必要的推理和启用隐式恢复，在保持性能的同时显著减少大推理模型的计算开销。


<details>
  <summary>更多</summary>
  
**动机:** 解决大推理模型在简单任务上因冗余推理导致的过度计算开销问题。

**方法:** 引入准确性感知长度奖励调节，根据问题难度自适应分配推理努力，提出ASRR框架。

**结果:** 在多个基准测试和模型上实验显示，与GRPO相比，ASRR能减少推理预算且几乎不损失准确性，同时显著提高安全基准上的无害率。

**结论:** ASRR展示了在大推理模型中实现高效、自适应和更安全推理的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+to+Continue+Thinking%3A+Adaptive+Thinking+Mode+Switching+for+Efficient+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15400，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15400&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs) achieve remarkable performance via long
reasoning chains, but often incur excessive computational overhead due to
redundant reasoning, especially on simple tasks. In this work, we
systematically quantify the upper bounds of LRMs under both Long-Thinking and
No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery
Mechanism" where models implicitly supplement reasoning during answer
generation. Building on this insight, we propose Adaptive Self-Recovery
Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables
implicit recovery. By introducing accuracy-aware length reward regulation, ASRR
adaptively allocates reasoning effort according to problem difficulty,
achieving high efficiency with negligible performance sacrifice. Experiments
across multiple benchmarks and models show that, compared with GRPO, ASRR
reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal
accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates
on safety benchmarks (up to +21.7%). Our results highlight the potential of
ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.

</details>


### [75] [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/abs/2505.15410)
*Bahar Radmehr, Ekaterina Shved, Fatma Betül Güreş, Adish Singla, Tanja Käser*

**主要类别:** cs.AI

**概要:** ClickSight，一个基于大型语言模型的管道，可以从点击流数据中解释学生的学习策略。尽管提示策略不同影响了解释质量，但结果显示LLMs可以合理地从点击流数据中解释学习策略。


<details>
  <summary>更多</summary>
  
**动机:** Clickstream数据提供了有关学生学习行为的有价值的见解，但由于其高维度和粒度而难以解释。现有的方法通常缺乏通用性和可扩展性。

**方法:** 引入ClickSight，一个基于大型语言模型的管道，它将原始点击流和学习策略列表作为输入，并生成学生交互期间的行为文本解释。评估了四种不同的提示策略并研究了自我精炼对解释质量的影响。

**结果:** 在两个开放式学习环境中进行评估，并使用基于评分卡的领域专家评估。结果表明，虽然LLMs可以从点击流数据中合理解释学习策略，但解释质量因提示策略而异，自我精炼提供的改进有限。

**结论:** ClickSight展示了LLMs从教育互动数据中生成理论驱动的见解的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ClickSight%3A+Interpreting+Student+Clickstreams+to+Reveal+Insights+on+Learning+Strategies+via+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15410，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15410&send_immediately=true&force_search=false)

**原文摘要:** Clickstream data from digital learning environments offer valuable insights
into students' learning behaviors, but are challenging to interpret due to
their high dimensionality and granularity. Prior approaches have relied mainly
on handcrafted features, expert labeling, clustering, or supervised models,
therefore often lacking generalizability and scalability. In this work, we
introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline
that interprets student clickstreams to reveal their learning strategies.
ClickSight takes raw clickstreams and a list of learning strategies as input
and generates textual interpretations of students' behaviors during
interaction. We evaluate four different prompting strategies and investigate
the impact of self-refinement on interpretation quality. Our evaluation spans
two open-ended learning environments and uses a rubric-based domain-expert
evaluation. Results show that while LLMs can reasonably interpret learning
strategies from clickstreams, interpretation quality varies by prompting
strategy, and self-refinement offers limited improvement. ClickSight
demonstrates the potential of LLMs to generate theory-driven insights from
educational interaction data.

</details>


### [76] [Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives](https://arxiv.org/abs/2505.15693)
*Milad Kazemi, Mateo Perez, Fabio Somenzi, Sadegh Soudjani, Ashutosh Trivedi, Alvaro Velasquez*

**主要类别:** cs.AI

**概要:** This paper proposes a model-free reinforcement learning framework that uses absolute liveness specifications to achieve better performance than discount-based methods in infinite-horizon, continuing tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation stems from the limitations of manually designing reward functions in reinforcement learning and the need for a principled alternative based on formal languages.

**方法:** The proposed method uses absolute liveness specifications within omega-regular languages, translating them into average-reward objectives for continuous environments, and includes a reward structure for lexicographic multi-objective optimization.

**结果:** The result shows that the average-reward approach outperforms discount-based methods across various benchmarks in continuing settings.

**结论:** The paper introduces a novel model-free reinforcement learning framework that translates absolute liveness specifications into average-reward objectives, ensuring convergence in unknown communicating MDPs without requiring full knowledge of the environment.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Average+Reward+Reinforcement+Learning+for+Omega-Regular+and+Mean-Payoff+Objectives，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15693，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15693&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in reinforcement learning (RL) have renewed focus on the
design of reward functions that shape agent behavior. Manually designing reward
functions is tedious and error-prone. A principled alternative is to specify
behaviors in a formal language that can be automatically translated into
rewards. Omega-regular languages are a natural choice for this purpose, given
their established role in formal verification and synthesis. However, existing
methods using omega-regular specifications typically rely on discounted reward
RL in episodic settings, with periodic resets. This setup misaligns with the
semantics of omega-regular specifications, which describe properties over
infinite behavior traces. In such cases, the average reward criterion and the
continuing setting -- where the agent interacts with the environment over a
single, uninterrupted lifetime -- are more appropriate.
  To address the challenges of infinite-horizon, continuing tasks, we focus on
absolute liveness specifications -- a subclass of omega-regular languages that
cannot be violated by any finite behavior prefix, making them well-suited to
the continuing setting. We present the first model-free RL framework that
translates absolute liveness specifications to average-reward objectives. Our
approach enables learning in communicating MDPs without episodic resetting. We
also introduce a reward structure for lexicographic multi-objective
optimization, aiming to maximize an external average-reward objective among the
policies that also maximize the satisfaction probability of a given
omega-regular specification. Our method guarantees convergence in unknown
communicating MDPs and supports on-the-fly reductions that do not require full
knowledge of the environment, thus enabling model-free RL. Empirical results
show our average-reward approach in continuing setting outperforms
discount-based methods across benchmarks.

</details>


### [77] [Neuro-Argumentative Learning with Case-Based Reasoning](https://arxiv.org/abs/2505.15742)
*Adam Gould, Francesca Toni*

**主要类别:** cs.AI

**概要:** 提出了一种名为Gradual AA-CBR的数据驱动神经符号分类模型，它结合了案例推理和论证辩论结构，并展示了与神经网络相当的表现，同时显著优于现有的AA-CBR公式。


<details>
  <summary>更多</summary>
  
**动机:** 提高模型的解释性并实现多类别分类能力，同时自动学习特征和数据点的重要性。

**方法:** 通过梯度方法学习论证辩论结构以及基于神经网络的特征提取器，其中每个论点代表训练数据中的一个案例。

**结果:** Gradual AA-CBR在表现上与神经网络相当，但显著优于现有的AA-CBR公式，且具备多类别分类等特性。

**结论:** Gradual AA-CBR提供了人类对齐的推理，提高了模型的可解释性，并且不需要二进制特征。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neuro-Argumentative+Learning+with+Case-Based+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15742，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15742&send_immediately=true&force_search=false)

**原文摘要:** We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual
AA-CBR), a data-driven, neurosymbolic classification model in which the outcome
is determined by an argumentation debate structure that is learned
simultaneously with neural-based feature extractors. Each argument in the
debate is an observed case from the training data, favouring their labelling.
Cases attack or support those with opposing or agreeing labellings, with the
strength of each argument and relationship learned through gradient-based
methods. This argumentation debate structure provides human-aligned reasoning,
improving model interpretability compared to traditional neural networks (NNs).
Unlike the existing purely symbolic variant, Abstract Argumentation for
Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class
classification, automatic learning of feature and data point importance,
assigning uncertainty values to outcomes, using all available data points, and
does not require binary features. We show that Gradual AA-CBR performs
comparably to NNs whilst significantly outperforming existing AA-CBR
formulations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [78] [Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective](https://arxiv.org/abs/2505.14808)
*Soo Min Kwon, Alec S. Xu, Can Yaras, Laura Balzano, Qing Qu*

**主要类别:** stat.ML

**概要:** 研究了线性回归任务中上下文学习（ICL）的分布外（OOD）能力，揭示了其对分布偏移的敏感性和在特定条件下的泛化特性。


<details>
  <summary>更多</summary>
  
**动机:** 理解ICL在处理分布偏移时的能力和局限性。

**方法:** 通过低秩协方差矩阵参数化的线性回归任务来研究ICL的OOD能力，并分析训练和测试协方差子空间之间的角度变化。

**结果:** 单层线性注意模型在测试风险上表现出对角度的依赖性，但ICL能在任务向量的子空间跨度内推广。

**结论:** ICL的OOD泛化能力可能源于新任务位于训练期间遇到的任务子空间的跨度内，LoRA能捕捉分布偏移。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Out-of-Distribution+Generalization+of+In-Context+Learning%3A+A+Low-Dimensional+Subspace+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14808，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14808&send_immediately=true&force_search=false)

**原文摘要:** This work aims to demystify the out-of-distribution (OOD) capabilities of
in-context learning (ICL) by studying linear regression tasks parameterized
with low-rank covariance matrices. With such a parameterization, we can model
distribution shifts as a varying angle between the subspace of the training and
testing covariance matrices. We prove that a single-layer linear attention
model incurs a test risk with a non-negligible dependence on the angle,
illustrating that ICL is not robust to such distribution shifts. However, using
this framework, we also prove an interesting property of ICL: when trained on
task vectors drawn from a union of low-dimensional subspaces, ICL can
generalize to any subspace within their span, given sufficiently long prompt
lengths. This suggests that the OOD generalization ability of Transformers may
actually stem from the new task lying within the span of those encountered
during training. We empirically show that our results also hold for models such
as GPT-2, and conclude with (i) experiments on how our observations extend to
nonlinear function classes and (ii) results on how LoRA has the ability to
capture distribution shifts.

</details>


### [79] [LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks](https://arxiv.org/abs/2505.14867)
*So Won Jeong, Claire Donnat*

**主要类别:** stat.ML

**概要:** 提出了一种新的框架LOBSTUR-GNN，用于无监督图神经网络的超参数调整和表示学习。实验表明该方法在分类准确率上有显著提升，并在实际应用中验证了其有效性。


<details>
  <summary>更多</summary>
  
**动机:** 解决图神经网络在无监督学习中的高敏感性以及缺乏优化模型选择方法的问题。

**方法:** 设计并实现了一个名为LOBSTUR-GNN的新框架，利用本地引导重采样和典型相关分析(CCA)来评估嵌入一致性。

**结果:** 在多个学术数据集上的实验显示，与未经调整的超参数相比，分类准确性提高了65.9%。

**结论:** LOBSTUR-GNN框架不仅在理论上有贡献，在实际应用中也展示了其有效性和实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LOBSTUR%3A+A+Local+Bootstrap+Framework+for+Tuning+Unsupervised+Representations+in+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.14867，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.14867&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) are increasingly used in conjunction with
unsupervised learning techniques to learn powerful node representations, but
their deployment is hindered by their high sensitivity to hyperparameter tuning
and the absence of established methodologies for selecting the optimal models.
To address these challenges, we propose LOBSTUR-GNN ({\bf Lo}cal {\bf B}oot{\bf
s}trap for {\bf T}uning {\bf U}nsupervised {\bf R}epresentations in GNNs) i), a
novel framework designed to adapt bootstrapping techniques for unsupervised
graph representation learning. LOBSTUR-GNN tackles two main challenges: (a)
adapting the bootstrap edge and feature resampling process to account for local
graph dependencies in creating alternative versions of the same graph, and (b)
establishing robust metrics for evaluating learned representations without
ground-truth labels. Using locally bootstrapped resampling and leveraging
Canonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR
provides a principled approach for hyperparameter tuning in unsupervised GNNs.
We validate the effectiveness and efficiency of our proposed method through
extensive experiments on established academic datasets, showing an 65.9\%
improvement in the classification accuracy compared to an uninformed selection
of hyperparameters. Finally, we deploy our framework on a real-world
application, thereby demonstrating its validity and practical utility in
various settings. \footnote{The code is available at
\href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}

</details>


### [80] [Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds](https://arxiv.org/abs/2505.15013)
*Anupama Sridhar, Alexander Johansen*

**主要类别:** stat.ML

**概要:** 提出了一种新的理论分析框架，用于理解和优化Adam在深度ReLU网络中的表现。


<details>
  <summary>更多</summary>
  
**动机:** 虽然一阶自适应优化方法如Adam在现代深度神经网络训练中被广泛采用并取得实证成功，但其在非光滑环境下的理论理解仍然有限，尤其是深度ReLU网络。ReLU激活函数在指数数量的区域边界上打破了标准平滑性假设。

**方法:** 基于分层Morse理论和Kakeya集的新成果，开发了多层细化框架来逐步收紧区域交叉的界限。

**结果:** 证明了区域交叉的数量从指数级下降到接近线性的有效维度。利用基于Kakeya的方法给出了比PAC-Bayes方法更紧致的广义界，并展示了在轻微一致低障碍假设下的收敛性。

**结论:** 提出了对Adam优化器在深度ReLU网络中的首个广义界，并且在非光滑、非凸ReLU环境中实现了Adam的首个全局最优收敛性，无需全局PL或凸性假设。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Convergence+of+Adam+in+Deep+ReLU+Networks+via+Directional+Complexity+and+Kakeya+Bounds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15013，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15013&send_immediately=true&force_search=false)

**原文摘要:** First-order adaptive optimization methods like Adam are the default choices
for training modern deep neural networks. Despite their empirical success, the
theoretical understanding of these methods in non-smooth settings, particularly
in Deep ReLU networks, remains limited. ReLU activations create exponentially
many region boundaries where standard smoothness assumptions break down.
\textbf{We derive the first
\(\tilde{O}\!\bigl(\sqrt{d_{\mathrm{eff}}/n}\bigr)\) generalization bound for
Adam in Deep ReLU networks and the first global-optimal convergence for Adam in
the non smooth, non convex relu landscape without a global PL or convexity
assumption.} Our analysis is based on stratified Morse theory and novel results
in Kakeya sets. We develop a multi-layer refinement framework that
progressively tightens bounds on region crossings. We prove that the number of
region crossings collapses from exponential to near-linear in the effective
dimension. Using a Kakeya based method, we give a tighter generalization bound
than PAC-Bayes approaches and showcase convergence using a mild uniform low
barrier assumption.

</details>


### [81] [Infinite hierarchical contrastive clustering for personal digital envirotyping](https://arxiv.org/abs/2505.15022)
*Ya-Yun Huang, Joseph McClernon, Jason A. Oliver, Matthew M. Engelhard*

**主要类别:** stat.ML

**概要:** This paper introduces a method called infinite hierarchical contrastive clustering to identify and group personal environments from daily life images for linking with health outcomes.


<details>
  <summary>更多</summary>
  
**动机:** To systematically study the effects of daily environments on health at an individual level by grouping images into distinct environments and analyzing their relationships with health outcomes.

**方法:** Infinite hierarchical contrastive clustering that allows an arbitrary number of clusters and encourages distinct environments to form well-defined sub-clusters within each cluster of related environments.

**结果:** The model effectively identifies distinct personal environments and groups them into meaningful environment types, showing potential to advance the envirotyping paradigm.

**结论:** The introduced method provides a way to systematically study the relationship between daily environments and health outcomes at an individual level.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Infinite+hierarchical+contrastive+clustering+for+personal+digital+envirotyping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15022，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15022&send_immediately=true&force_search=false)

**原文摘要:** Daily environments have profound influence on our health and behavior. Recent
work has shown that digital envirotyping, where computer vision is applied to
images of daily environments taken during ecological momentary assessment
(EMA), can be used to identify meaningful relationships between environmental
features and health outcomes of interest. To systematically study such effects
on an individual level, it is helpful to group images into distinct
environments encountered in an individual's daily life; these may then be
analyzed, further grouped into related environments with similar features, and
linked to health outcomes. Here we introduce infinite hierarchical contrastive
clustering to address this challenge. Building on the established contrastive
clustering framework, our method a) allows an arbitrary number of clusters
without requiring the full Dirichlet Process machinery by placing a
stick-breaking prior on predicted cluster probabilities; and b) encourages
distinct environments to form well-defined sub-clusters within each cluster of
related environments by incorporating a participant-specific prediction loss.
Our experiments show that our model effectively identifies distinct personal
environments and groups these environments into meaningful environment types.
We then illustrate how the resulting clusters can be linked to various health
outcomes, highlighting the potential of our approach to advance the
envirotyping paradigm.

</details>


### [82] [A Linear Approach to Data Poisoning](https://arxiv.org/abs/2505.15175)
*Diego Granziol, Donald Flynn*

**主要类别:** stat.ML

**概要:** This paper investigates data poisoning attacks on machine learning models, developing theories and methods to detect and mitigate such attacks using Hessian spectral signatures and random matrix theory.


<details>
  <summary>更多</summary>
  
**动机:** To explore the theoretical foundations of data poisoning attacks and develop ways to detect and address them.

**方法:** Analyzing the Hessian with respect to the input, using random matrix theory for linear regression, performing QR stepwise regression, and conducting experiments on deep networks.

**结果:** Found spectral signatures in the Hessian that can characterize compromised datasets and developed algorithms to detect poisoning without additional training.

**结论:** Theoretical and experimental results suggest that the developed methods can be used to detect and mitigate data poisoning attacks in various types of machine learning models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Linear+Approach+to+Data+Poisoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15175，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15175&send_immediately=true&force_search=false)

**原文摘要:** We investigate the theoretical foundations of data poisoning attacks in
machine learning models. Our analysis reveals that the Hessian with respect to
the input serves as a diagnostic tool for detecting poisoning, exhibiting
spectral signatures that characterize compromised datasets. We use random
matrix theory (RMT) to develop a theory for the impact of poisoning proportion
and regularisation on attack efficacy in linear regression. Through QR stepwise
regression, we study the spectral signatures of the Hessian in multi-output
regression. We perform experiments on deep networks to show experimentally that
this theory extends to modern convolutional and transformer networks under the
cross-entropy loss. Based on these insights we develop preliminary algorithms
to determine if a network has been poisoned and remedies which do not require
further training.

</details>


### [83] [Clustering and Pruning in Causal Data Fusion](https://arxiv.org/abs/2505.15215)
*Otto Tabell, Santtu Tikka, Juha Karvanen*

**主要类别:** stat.ML

**概要:** This paper addresses computational challenges in causal data fusion by proposing pruning and clustering as preprocessing operations.


<details>
  <summary>更多</summary>
  
**动机:** To enable identification of causal effects that would otherwise remain non-identifiable using data fusion.

**方法:** Proposing pruning and clustering techniques for reducing model size while preserving essential features.

**结果:** Generalized earlier results on a single data source and derived conditions for applying these techniques in multiple data sources.

**结论:** The proposed methods can infer the identifiability of causal effects in larger graphs based on smaller ones and have applications in epidemiology and social sciences.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Clustering+and+Pruning+in+Causal+Data+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15215，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15215&send_immediately=true&force_search=false)

**原文摘要:** Data fusion, the process of combining observational and experimental data,
can enable the identification of causal effects that would otherwise remain
non-identifiable. Although identification algorithms have been developed for
specific scenarios, do-calculus remains the only general-purpose tool for
causal data fusion, particularly when variables are present in some data
sources but not others. However, approaches based on do-calculus may encounter
computational challenges as the number of variables increases and the causal
graph grows in complexity. Consequently, there exists a need to reduce the size
of such models while preserving the essential features. For this purpose, we
propose pruning (removing unnecessary variables) and clustering (combining
variables) as preprocessing operations for causal data fusion. We generalize
earlier results on a single data source and derive conditions for applying
pruning and clustering in the case of multiple data sources. We give sufficient
conditions for inferring the identifiability or non-identifiability of a causal
effect in a larger graph based on a smaller graph and show how to obtain the
corresponding identifying functional for identifiable causal effects. Examples
from epidemiology and social science demonstrate the use of the results.

</details>


### [84] [Policy Testing in Markov Decision Processes](https://arxiv.org/abs/2505.15342)
*Kaito Ariu, Po-An Wang, Alexandre Proutiere, Kenshi Abe*

**主要类别:** stat.ML

**概要:** 在折扣马尔可夫决策过程中提出了一种新的策略测试算法，该算法在统计和计算上都是有效的。


<details>
  <summary>更多</summary>
  
**动机:** 研究折扣马尔可夫决策过程（MDPs）中的策略测试问题，在固定置信度设置下，确定给定策略的价值是否超过指定阈值，同时最小化观察次数。

**方法:** 通过重新构建下界问题，将其转化为一个具有非凸目标但凸约束的问题，并将其解释为一个新的反转MDP中的策略优化任务。利用最新的策略梯度方法来高效解决这个问题，并设计了一个统计最优的策略测试算法。

**结果:** 提出的算法在样本复杂度上达到了实例特定的下界匹配，并且在计算上是可行的。

**结论:** 提出了一个在折扣马尔可夫决策过程中固定置信度设置下的策略测试算法。该算法在样本复杂度上实现了实例特定的下界匹配，并保持了计算上的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Policy+Testing+in+Markov+Decision+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15342，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15342&send_immediately=true&force_search=false)

**原文摘要:** We study the policy testing problem in discounted Markov decision processes
(MDPs) under the fixed-confidence setting. The goal is to determine whether the
value of a given policy exceeds a specified threshold while minimizing the
number of observations. We begin by deriving an instance-specific lower bound
that any algorithm must satisfy. This lower bound is characterized as the
solution to an optimization problem with non-convex constraints. We propose a
policy testing algorithm inspired by this optimization problem--a common
approach in pure exploration problems such as best-arm identification, where
asymptotically optimal algorithms often stem from such optimization-based
characterizations. As for other pure exploration tasks in MDPs, however, the
non-convex constraints in the lower-bound problem present significant
challenges, raising doubts about whether statistically optimal and
computationally tractable algorithms can be designed. To address this, we
reformulate the lower-bound problem by interchanging the roles of the objective
and the constraints, yielding an alternative problem with a non-convex
objective but convex constraints. Strikingly, this reformulated problem admits
an interpretation as a policy optimization task in a newly constructed reversed
MDP. Leveraging recent advances in policy gradient methods, we efficiently
solve this problem and use it to design a policy testing algorithm that is
statistically optimal--matching the instance-specific lower bound on sample
complexity--while remaining computationally tractable. We validate our approach
with numerical experiments.

</details>


### [85] [Robust Multimodal Learning via Entropy-Gated Contrastive Fusion](https://arxiv.org/abs/2505.15417)
*Leon Chlon, Maggie Chlon, MarcAntonio M. Awada*

**主要类别:** stat.ML

**概要:** 提出一种新的融合层AECF，可提高多模态系统在缺失输入情况下的鲁棒性和校准性，同时保持较低的运行时间。


<details>
  <summary>更多</summary>
  
**动机:** 标准融合层无法同时保证鲁棒性和校准性，而实际应用中多模态系统常面临缺失输入的情况。

**方法:** 引入自适应熵门限对比融合层（AECF），该层可根据实例调整熵系数，确保所有模态子集的单调校准，并从训练时的熵直接驱动课程掩码。

**结果:** 在AV-MNIST和MS-COCO数据集上，AECF在50%丢失率下提升了18个百分点的掩码输入mAP，同时减少了高达200%的ECE，且增加了1%的运行时间。

**结论:** AECF是一种轻量级的融合层，可以轻松集成到现有的多模态推理系统中，提高其鲁棒性和校准性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Multimodal+Learning+via+Entropy-Gated+Contrastive+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15417，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15417&send_immediately=true&force_search=false)

**原文摘要:** Real-world multimodal systems routinely face missing-input scenarios, and in
reality, robots lose audio in a factory or a clinical record omits lab tests at
inference time. Standard fusion layers either preserve robustness or
calibration but never both. We introduce Adaptive Entropy-Gated Contrastive
Fusion (AECF), a single light-weight layer that (i) adapts its entropy
coefficient per instance, (ii) enforces monotone calibration across all
modality subsets, and (iii) drives a curriculum mask directly from
training-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP
by +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%
run-time. All back-bones remain frozen, making AECF an easy drop-in layer for
robust, calibrated multimodal inference.

</details>


### [86] [Uncertainty Quantification in SVM prediction](https://arxiv.org/abs/2505.15429)
*Pritam Anand*

**主要类别:** stat.ML

**概要:** This paper explores Uncertainty Quantification (UQ) in SVM predictions for regression and forecasting tasks. It evaluates existing Prediction Interval (PI) estimation and probabilistic forecasting methods, proposes a Sparse Support Vector Quantile Regression (SSVQR) model, develops a feature selection algorithm, and compares SVM models with deep learning models.


<details>
  <summary>更多</summary>
  
**动机:** To address the lack of literature on UQ in SVM predictions and improve the stability, sparsity, and interpretability of SVM models.

**方法:** Comprehensive evaluation of existing PI estimation methods, proposal of SSVQR model, development of feature selection algorithm, extension of SVM models in Conformal Regression setting, and comparison with deep learning models.

**结果:** None of the existing SVM PI models achieves sparsity. The proposed SSVQR model introduces sparsity and improves PI quality in high-dimensional datasets. SVM models show comparable or superior performance to deep learning models for probabilistic forecasting tasks.

**结论:** The proposed methods improve the stability, sparsity, and interpretability of SVM models for UQ in predictions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+Quantification+in+SVM+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15429，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15429&send_immediately=true&force_search=false)

**原文摘要:** This paper explores Uncertainty Quantification (UQ) in SVM predictions,
particularly for regression and forecasting tasks. Unlike the Neural Network,
the SVM solutions are typically more stable, sparse, optimal and interpretable.
However, there are only few literature which addresses the UQ in SVM
prediction. At first, we provide a comprehensive summary of existing Prediction
Interval (PI) estimation and probabilistic forecasting methods developed in the
SVM framework and evaluate them against the key properties expected from an
ideal PI model. We find that none of the existing SVM PI models achieves a
sparse solution. To introduce sparsity in SVM model, we propose the Sparse
Support Vector Quantile Regression (SSVQR) model, which constructs PIs and
probabilistic forecasts by solving a pair of linear programs. Further, we
develop a feature selection algorithm for PI estimation using SSVQR that
effectively eliminates a significant number of features while improving PI
quality in case of high-dimensional dataset. Finally we extend the SVM models
in Conformal Regression setting for obtaining more stable prediction set with
finite test set guarantees. Extensive experiments on artificial, real-world
benchmark datasets compare the different characteristics of both existing and
proposed SVM-based PI estimation methods and also highlight the advantages of
the feature selection in PI estimation. Furthermore, we compare both, the
existing and proposed SVM-based PI estimation models, with modern deep learning
models for probabilistic forecasting tasks on benchmark datasets. Furthermore,
SVM models show comparable or superior performance to modern complex deep
learning models for probabilistic forecasting task in our experiments.

</details>


### [87] [Adaptive Temperature Scaling with Conformal Prediction](https://arxiv.org/abs/2505.15437)
*Nikita Kotelevskii, Mohsen Guizani, Eric Moulines, Maxim Panov*

**主要类别:** stat.ML

**概要:** 提出一种方法为非概率预测集分配校准概率，实验表明在保持覆盖保证的同时显著减少了预期校准误差。


<details>
  <summary>更多</summary>
  
**动机:** 现有非概率预测集不能提供单个标签的概率估计，限制了实际应用。

**方法:** 将问题转化为自适应校准问题，选择特定的温度参数以匹配所需覆盖水平。

**结果:** 在几个具有挑战性的图像分类数据集上的实验证明该方法的有效性。

**结论:** 所提出的方法在保持覆盖保证的同时显著减少了预期校准误差。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Adaptive+Temperature+Scaling+with+Conformal+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15437，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15437&send_immediately=true&force_search=false)

**原文摘要:** Conformal prediction enables the construction of high-coverage prediction
sets for any pre-trained model, guaranteeing that the true label lies within
the set with a specified probability. However, these sets do not provide
probability estimates for individual labels, limiting their practical use. In
this paper, we propose, to the best of our knowledge, the first method for
assigning calibrated probabilities to elements of a conformal prediction set.
Our approach frames this as an adaptive calibration problem, selecting an
input-specific temperature parameter to match the desired coverage level.
Experiments on several challenging image classification datasets demonstrate
that our method maintains coverage guarantees while significantly reducing
expected calibration error.

</details>


### [88] [Are machine learning interpretations reliable? A stability study on global interpretations](https://arxiv.org/abs/2505.15728)
*Luqin Gan, Tarek M. Zikry, Genevera I. Allen*

**主要类别:** stat.ML

**概要:** This paper examines the stability of popular machine learning global interpretations for supervised and unsupervised tasks on tabular data.


<details>
  <summary>更多</summary>
  
**动机:** To determine if these interpretations are reliable.

**方法:** Systematic, large-scale empirical stability study.

**结果:** Popular interpretation methods are frequently unstable, less stable than the predictions themselves, and there is no association between the accuracy of predictions and the stability of interpretations.

**结论:** Interpretability alone does not warrant trust, and future work needs to rigorously evaluate interpretation stability.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Are+machine+learning+interpretations+reliable%3F+A+stability+study+on+global+interpretations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2505.15728，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2505.15728&send_immediately=true&force_search=false)

**原文摘要:** As machine learning systems are increasingly used in high-stakes domains,
there is a growing emphasis placed on making them interpretable to improve
trust in these systems. In response, a range of interpretable machine learning
(IML) methods have been developed to generate human-understandable insights
into otherwise black box models. With these methods, a fundamental question
arises: Are these interpretations reliable? Unlike with prediction accuracy or
other evaluation metrics for supervised models, the proximity to the true
interpretation is difficult to define. Instead, we ask a closely related
question that we argue is a prerequisite for reliability: Are these
interpretations stable? We define stability as findings that are consistent or
reliable under small random perturbations to the data or algorithms. In this
study, we conduct the first systematic, large-scale empirical stability study
on popular machine learning global interpretations for both supervised and
unsupervised tasks on tabular data. Our findings reveal that popular
interpretation methods are frequently unstable, notably less stable than the
predictions themselves, and that there is no association between the accuracy
of machine learning predictions and the stability of their associated
interpretations. Moreover, we show that no single method consistently provides
the most stable interpretations across a range of benchmark datasets. Overall,
these results suggest that interpretability alone does not warrant trust, and
underscores the need for rigorous evaluation of interpretation stability in
future work. To support these principles, we have developed and released an
open source IML dashboard and Python package to enable researchers to assess
the stability and reliability of their own data-driven interpretations and
discoveries.

</details>
