{"id": "2601.02514", "pdf": "https://arxiv.org/pdf/2601.02514", "abs": "https://arxiv.org/abs/2601.02514", "authors": ["Ahmad Terra", "Mohit Ahmed", "Rafia Inam", "Elena Fersman", "Martin Törngren"], "title": "Textual Explanations and Their Evaluations for Reinforcement Learning Policy", "categories": ["cs.AI"], "comment": null, "summary": "Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.", "AI": {"tldr": "提出了一个新的可解释强化学习框架，使用大语言模型生成文本解释，将其转换为透明规则，并通过两种精炼技术提高解释质量，在多个环境中验证了其有效性和工业适用性。", "motivation": "虽然文本解释对人类易于理解，但确保其正确性仍具挑战性，且现有评估方法有限。需要一种能够生成高质量文本解释并系统评估其性能的框架。", "method": "使用大语言模型生成文本解释，通过聚类技术识别频繁条件，将其转换为透明规则。提出自动谓词生成器确定状态语义信息，并采用两种精炼技术改进解释质量和减少冲突信息。", "result": "在三个开源环境和电信用例中验证了框架的有效性。生成的透明规则在特定任务中能达到满意性能，解决了现有方法的局限性。", "conclusion": "该框架为XRL领域提供了系统化和定量化的文本解释评估方法，能够生成高质量的解释规则，具有工业应用价值，并为该领域提供了有价值的见解。"}}
{"id": "2601.02553", "pdf": "https://arxiv.org/pdf/2601.02553", "abs": "https://arxiv.org/abs/2601.02553", "authors": ["Jiaqi Liu", "Yaofeng Su", "Peng Xia", "Siwei Han", "Zeyu Zheng", "Cihang Xie", "Mingyu Ding", "Huaxiu Yao"], "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.", "AI": {"tldr": "SimpleMem是一个基于语义无损压缩的高效内存框架，通过三阶段流水线实现信息密度最大化，在保持高精度的同时大幅降低推理成本。", "motivation": "现有LLM智能体内存系统要么保留完整交互历史导致冗余，要么依赖迭代推理产生高token成本，需要更高效的内存管理方案来支持复杂环境中的长期交互。", "method": "三阶段流水线：1)语义结构化压缩-通过熵感知过滤将非结构化交互压缩为紧凑的多视图索引内存单元；2)递归内存整合-异步整合相关单元为高级抽象表示以减少冗余；3)自适应查询感知检索-根据查询复杂度动态调整检索范围以高效构建精确上下文。", "result": "在基准数据集上表现优于基线方法，平均F1提升26.4%，推理时token消耗减少高达30倍，在性能和效率之间实现了优越平衡。", "conclusion": "SimpleMem框架通过语义无损压缩有效解决了LLM智能体内存管理的效率和冗余问题，为复杂环境中的长期可靠交互提供了实用解决方案。"}}
{"id": "2601.02577", "pdf": "https://arxiv.org/pdf/2601.02577", "abs": "https://arxiv.org/abs/2601.02577", "authors": ["Alexander Roman", "Jacob Roman"], "title": "Orchestral AI: A Framework for Agent Orchestration", "categories": ["cs.AI", "astro-ph.IM", "hep-ph"], "comment": "17 pages, 3 figures. For more information visit https://orchestral-ai.com", "summary": "The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.", "AI": {"tldr": "Orchestral是一个轻量级Python框架，提供统一的类型安全接口来构建跨LLM提供商的AI代理，解决了多提供商集成中的API碎片化、消息格式不兼容和工具调用行为不一致等问题。", "motivation": "LLM代理框架的快速发展导致开发者面临供应商锁定或复杂多包生态系统的选择困境，跨提供商工具集成存在API碎片化、消息格式不兼容和流式处理行为不一致等工程挑战，难以构建可移植、可靠的代理系统。", "method": "设计统一的通用消息、工具和LLM使用表示；基于Python类型提示自动生成工具模式；采用同步执行模型支持流式处理；模块化架构分离提供商集成、工具执行、对话编排和用户界面。", "result": "实现了跨提供商的无缝操作，消除了手动格式转换，减少了框架引入的复杂性，支持确定性行为、直接调试和实时交互，无需服务器依赖。", "conclusion": "Orchestral提供了一个轻量级但功能完整的解决方案，支持高级代理能力（包括丰富的工具调用、上下文压缩、工作空间沙盒、用户审批流程等），同时保持科学计算和生产部署所需的简洁性。"}}
{"id": "2601.02641", "pdf": "https://arxiv.org/pdf/2601.02641", "abs": "https://arxiv.org/abs/2601.02641", "authors": ["Jeiyoon Park", "Daehwan Lee", "Changmin Yeo", "Yongshin Han", "Minseop Kim"], "title": "An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices", "categories": ["cs.AI"], "comment": "preprint", "summary": "Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.", "AI": {"tldr": "本文研究了设备端AI模型在实际部署中的关键问题，包括模型选择、资源消耗和领域适应能力，通过构建LiveChatBench基准测试并在5台移动设备上进行实验，证明了设备端模型可以达到与商业模型相当的性能。", "motivation": "尽管设备端AI模型效率高，但缺乏对其实际部署中关键问题（如CPU利用率和热条件）的研究，需要解决模型选择、资源消耗和领域适应能力等问题以实现真实世界服务部署。", "method": "构建LiveChatBench基准测试（包含1000个韩英平行句对），在5台移动设备上进行广泛实验，评估不同设备端模型的资源消耗和领域适应能力。", "result": "实验表明，虽然服务大规模异质用户需要考虑高度受限的部署环境和模型选择，但所提出的方法在针对性任务上达到了与GPT-5.1等商业模型相当的性能。", "conclusion": "研究结果为设备端AI社区提供了有意义的见解，证明了设备端模型在实际部署中的可行性和潜力，尽管需要仔细考虑部署约束和模型选择。"}}
{"id": "2601.02391", "pdf": "https://arxiv.org/pdf/2601.02391", "abs": "https://arxiv.org/abs/2601.02391", "authors": ["Zhaojiang Lin", "Yong Xu", "Kai Sun", "Jing Zheng", "Yin Huang", "Surya Teja Appini", "Krish Narang", "Renjie Tao", "Ishan Kapil Jain", "Siddhant Arora", "Ruizhi Li", "Yiteng Huang", "Kaushik Patnaik", "Wenfang Xu", "Suwon Shon", "Yue Liu", "Ahmed A Aly", "Anuj Kumar", "Florian Metze", "Xin Luna Dong"], "title": "WearVox: An Egocentric Multichannel Voice Assistant Benchmark for Wearables", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Wearable devices such as AI glasses are transforming voice assistants into always-available, hands-free collaborators that integrate seamlessly with daily life, but they also introduce challenges like egocentric audio affected by motion and noise, rapid micro-interactions, and the need to distinguish device-directed speech from background conversations. Existing benchmarks largely overlook these complexities, focusing instead on clean or generic conversational audio. To bridge this gap, we present WearVox, the first benchmark designed to rigorously evaluate voice assistants in realistic wearable scenarios. WearVox comprises 3,842 multi-channel, egocentric audio recordings collected via AI glasses across five diverse tasks including Search-Grounded QA, Closed-Book QA, Side-Talk Rejection, Tool Calling, and Speech Translation, spanning a wide range of indoor and outdoor environments and acoustic conditions. Each recording is accompanied by rich metadata, enabling nuanced analysis of model performance under real-world constraints. We benchmark leading proprietary and open-source speech Large Language Models (SLLMs) and find that most real-time SLLMs achieve accuracies on WearVox ranging from 29% to 59%, with substantial performance degradation on noisy outdoor audio, underscoring the difficulty and realism of the benchmark. Additionally, we conduct a case study with two new SLLMs that perform inference with single-channel and multi-channel audio, demonstrating that multi-channel audio inputs significantly enhance model robustness to environmental noise and improve discrimination between device-directed and background speech. Our results highlight the critical importance of spatial audio cues for context-aware voice assistants and establish WearVox as a comprehensive testbed for advancing wearable voice AI research.", "AI": {"tldr": "WearVox是首个专为评估可穿戴设备语音助手设计的基准测试，包含3842个多通道自中心音频记录，覆盖5个任务场景，测试显示当前SLLM模型准确率仅29%-59%，多通道音频能显著提升噪声环境下的性能。", "motivation": "现有语音助手基准测试主要关注干净或通用对话音频，忽略了可穿戴设备面临的特殊挑战：运动噪声影响的自中心音频、快速微交互需求、设备定向语音与背景对话的区分等问题。", "method": "通过AI眼镜收集3842个多通道自中心音频记录，涵盖5个任务类型（搜索问答、闭卷问答、侧聊拒绝、工具调用、语音翻译），包含丰富室内外环境和声学条件，并对比评估主流专有和开源语音大语言模型。", "result": "实时语音大语言模型在WearVox上的准确率为29%-59%，在嘈杂室外音频上性能显著下降；多通道音频输入能显著增强模型对环境噪声的鲁棒性，改善设备定向语音与背景语音的区分能力。", "conclusion": "空间音频线索对情境感知语音助手至关重要，WearVox为推进可穿戴语音AI研究提供了全面的测试平台，证明了多通道音频在处理现实世界噪声和区分语音类型方面的优势。"}}
{"id": "2601.02643", "pdf": "https://arxiv.org/pdf/2601.02643", "abs": "https://arxiv.org/abs/2601.02643", "authors": ["Mehmet Kurmaz"], "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents", "categories": ["cs.AI"], "comment": "19 pages, 2 figures, 6 tables", "summary": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.", "AI": {"tldr": "该论文提出了一种基于LLM的对话式工具调用代理，通过推断用户约束的相对重要性来解决查询不可行性问题，避免随意放松关键约束。", "motivation": "现有工具调用对话代理在面对查询不可行性时，要么返回无结果，要么使用临时规则放松约束，这可能会违反用户意图，丢弃用户最关心的需求。", "method": "提出了三种基于LLM的方法来从对话中推断约束的相对重要性：局部加权、全局一次性加权和成对排序，并将不可行性处理构建为偏好感知的查询修复问题。", "result": "实验显示局部加权在偏好对齐方面表现最佳，而全局加权在正确约束放松方面表现最好。", "conclusion": "研究提出了AWARE-US基准测试，要求代理通过对话消歧请求，并以符合角色隐含偏好的方式解决不可行性问题，为解决查询不可行性提供了更智能的方法。"}}
{"id": "2601.02404", "pdf": "https://arxiv.org/pdf/2601.02404", "abs": "https://arxiv.org/abs/2601.02404", "authors": ["Inpyo Song", "Eunji Jeon", "Jangwon Lee"], "title": "PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Code and Dataset available at https://github.com/Null99-Dog/PCEval-Dataset", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \\textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \\textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \\textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.", "AI": {"tldr": "PCEval是首个针对大语言模型在物理计算领域的自动化评估基准，测试了13个主流模型在电路生成和代码兼容性方面的能力，发现LLMs在代码生成和逻辑电路设计表现良好，但在物理面包板布局方面存在显著困难。", "motivation": "虽然大语言模型在软件开发等领域表现出色，但在需要考虑硬件约束的物理计算领域（软件需要与物理硬件交互和控制）的效果尚未得到充分探索，存在研究空白。", "method": "开发了PCEval评估框架，通过模拟环境对13个主流LLM进行自动化评估，测试它们在生成电路和产生兼容代码方面的能力，涵盖不同复杂度的项目，无需人工评估。", "result": "LLMs在代码生成和逻辑电路设计方面表现良好，但在物理面包板布局创建方面表现显著较差，特别是在管理正确的引脚连接和避免电路错误方面存在困难。", "conclusion": "PCEval增进了对AI在硬件依赖计算环境中辅助能力的理解，为开发更有效的物理计算教育工具奠定了基础，揭示了LLMs在物理实现约束推理方面的局限性。"}}
{"id": "2601.02666", "pdf": "https://arxiv.org/pdf/2601.02666", "abs": "https://arxiv.org/abs/2601.02666", "authors": ["Hadi Partovi Aria", "Zhe Xu"], "title": "Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks", "categories": ["cs.AI", "cs.LO"], "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.", "AI": {"tldr": "GTL-CIRL是一个闭环框架，通过同时学习策略和挖掘因果图时序逻辑规范，解决了黑盒强化学习在时空图决策中忽视局部变化传播的问题，提高了样本效率和可解释性。", "motivation": "传统黑盒强化学习方法在处理具有时空动态特性的图结构决策任务时，往往忽视局部变化如何通过网络结构传播，这限制了样本效率和模型的可解释性。", "method": "提出GTL-CIRL框架：1）使用鲁棒性塑造奖励函数；2）在效应失败时收集反例；3）利用高斯过程驱动的贝叶斯优化来精化参数化原因模板；4）高斯过程捕获系统动态中的时空相关性，实现复杂参数空间的高效探索。", "result": "在基因网络和电力网络的案例研究中，相比标准RL基线方法，GTL-CIRL展现出更快的学习速度和更清晰、可验证的行为表现。", "conclusion": "GTL-CIRL框架通过结合因果推理和时序逻辑规范，有效解决了时空图决策问题，提供了比传统强化学习方法更高的效率和更好的可解释性。"}}
{"id": "2601.02531", "pdf": "https://arxiv.org/pdf/2601.02531", "abs": "https://arxiv.org/abs/2601.02531", "authors": ["Mattia Ottoborgo", "Daniele Rege Cambrin", "Paolo Garza"], "title": "Losses that Cook: Topological Optimal Transport for Structured Recipe Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cooking recipes are complex procedures that require not only a fluent and factual text, but also accurate timing, temperature, and procedural coherence, as well as the correct composition of ingredients. Standard training procedures are primarily based on cross-entropy and focus solely on fluency. Building on RECIPE-NLG, we investigate the use of several composite objectives and present a new topological loss that represents ingredient lists as point clouds in embedding space, minimizing the divergence between predicted and gold ingredients. Using both standard NLG metrics and recipe-specific metrics, we find that our loss significantly improves ingredient- and action-level metrics. Meanwhile, the Dice loss excels in time/temperature precision, and the mixed loss yields competitive trade-offs with synergistic gains in quantity and time. A human preference analysis supports our finding, showing our model is preferred in 62% of the cases.", "AI": {"tldr": "该论文提出了一种新的拓扑损失函数来改进烹饪食谱生成，通过将食材列表表示为嵌入空间中的点云来最小化预测食材与真实食材之间的差异，显著提升了食材和动作层面的指标。", "motivation": "烹饪食谱需要流畅且事实准确的文本，同时需要精确的时间、温度和程序连贯性，以及正确的食材组成。标准的训练程序主要基于交叉熵，仅关注流畅性，无法充分处理这些复杂要求。", "method": "基于RECIPE-NLG，研究使用多种复合目标函数，提出新的拓扑损失函数，将食材列表表示为嵌入空间中的点云，最小化预测食材与真实食材之间的差异。使用标准NLG指标和食谱特定指标进行评估。", "result": "拓扑损失显著改善了食材和动作层面的指标；Dice损失在时间/温度精度方面表现优异；混合损失在数量和时间方面实现了协同增益的竞争性权衡。人工偏好分析显示该模型在62%的情况下更受青睐。", "conclusion": "复合目标函数和拓扑损失能够有效提升食谱生成的质量，特别是在食材准确性、时间温度精度和程序连贯性方面，为复杂的程序性文本生成提供了有效的解决方案。"}}
{"id": "2601.02683", "pdf": "https://arxiv.org/pdf/2601.02683", "abs": "https://arxiv.org/abs/2601.02683", "authors": ["Dongyu Chen", "Jian Ma", "Xianpeng Zhang", "Lei Zhang", "Haonan Lu", "Chen Chen", "Chuangchuang Wang", "Kai Tang"], "title": "Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.", "AI": {"tldr": "HAPO框架通过动态归因机制、语义单元优化和多模态支持，解决了提示优化中的提示漂移和可解释性问题，在多项任务中展现出优越性能。", "motivation": "当前提示优化方法存在提示漂移问题（新提示修复旧问题但影响原有性能）和可解释性差的问题，需要更有效的自动化优化方法。", "method": "提出分层归因提示优化(HAPO)框架，包含：1)针对训练数据和提示历史的动态归因机制；2)用于编辑功能提示段的语义单元优化；3)支持LLM和LLM-MLLM工作流的多模态友好流程。", "result": "在单/多图像QA（如OCRV2）和复杂任务分析（如BBH）中，HAPO显示出更高的优化效率，优于同类自动化提示优化方法。", "conclusion": "HAPO建立了一个可扩展的提示工程范式，有效解决了提示漂移问题，同时保持了优化的可解释性和多模态兼容性。"}}
{"id": "2601.02535", "pdf": "https://arxiv.org/pdf/2601.02535", "abs": "https://arxiv.org/abs/2601.02535", "authors": ["Hyeong Kyu Choi", "Sharon Li"], "title": "ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX-Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks -- including text summarization, code generation, and mathematical reasoning -- our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.", "AI": {"tldr": "ModeX是一个无需外部评估器的多文本选择框架，通过识别生成文本中的主导语义共识来选择最佳输出，在开放式任务中表现优于现有方法。", "motivation": "解决大语言模型在开放式任务中选择最佳生成输出的难题，现有方法依赖外部评估器或精确字符串匹配，限制了适用性和效率。", "method": "提出ModeX框架，构建候选文本的相似性图，应用谱聚类递归选择代表性中心点；进一步开发ModeX-Lite版本，通过早期剪枝提高效率。", "result": "在文本摘要、代码生成和数学推理等开放式任务中，该方法持续优于标准的单路径和多路径基线方法。", "conclusion": "ModeX提供了一个计算效率高的解决方案，为稳健的开放式文本生成提供了有效的选择框架，代码已开源。"}}
{"id": "2601.02702", "pdf": "https://arxiv.org/pdf/2601.02702", "abs": "https://arxiv.org/abs/2601.02702", "authors": ["Shuhaib Mehri", "Priyanka Kargupta", "Tal August", "Dilek Hakkani-Tür"], "title": "Learning User Preferences Through Interaction for Long-Term Collaboration", "categories": ["cs.AI"], "comment": null, "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.", "AI": {"tldr": "MultiSessionCollab是一个评估智能代理在多会话中学习用户偏好并提升协作质量的基准测试。研究通过为代理配备持久化记忆机制来不断精炼用户偏好，并利用用户模拟器行为作为学习信号来训练代理生成更全面的反思和更有效的记忆更新。实验表明记忆机制能显著提高任务成功率、交互效率和用户体验。", "motivation": "随着对话代理与用户协作经验的积累，适应用户偏好对于建立长期关系和提升协作质量至关重要。需要开发能够学习用户偏好并在多会话中持续改进协作效果的智能代理。", "method": "1. 提出MultiSessionCollab基准测试框架；2. 为代理配备持久化记忆机制，随着交互经验积累不断精炼用户偏好；3. 从用户模拟器行为中提取学习信号，训练代理生成更全面的反思和更有效的记忆更新；4. 进行大量实验和人类用户研究验证效果。", "result": "实验结果显示：1. 配备记忆的代理显著提高了长期协作效果；2. 获得了更高的任务成功率；3. 实现了更高效的交互；4. 减少了用户努力；5. 人类用户研究证实记忆机制在真实场景中改善了用户体验。", "conclusion": "通过MultiSessionCollab基准和记忆机制，智能代理能够有效学习用户偏好并在多会话协作中持续提升性能，为构建长期有效的用户-代理协作关系提供了重要技术路径。"}}
{"id": "2601.02569", "pdf": "https://arxiv.org/pdf/2601.02569", "abs": "https://arxiv.org/abs/2601.02569", "authors": ["Hossein Rajabzadeh", "Maryam Dialameh", "Chul B. Park", "Il-Min Kim", "Hyock Ju Kwon"], "title": "LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \\textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \\emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \\emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \\textbf{LLaMA2-7B}, \\textbf{LLaMA3-8B}, \\textbf{Qwen2.5-7B}, and \\textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \\textbf{2.6$\\times$ faster decoding} and \\textbf{45--55\\% KV-cache reduction} while staying within \\textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \\emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.", "AI": {"tldr": "LoRA-Drop是一个即插即用的推理框架，通过时间计算调度在中间层重用前一token的隐藏状态并应用低秩LoRA校正，定期刷新完整执行以防止漂移，实现2.6倍解码加速和45-55% KV缓存减少，同时保持基准精度在0.5个百分点内。", "motivation": "自回归大语言模型受限于顺序解码，现有动态深度和层跳过方法要么依赖辅助路由机制，要么在绕过层未补偿时导致精度下降。", "method": "LoRA-Drop在大多数解码步骤中，选定层重用前一token隐藏状态并应用低秩LoRA校正，定期执行完整模型刷新防止漂移，无需路由网络，兼容标准KV缓存。", "result": "在LLaMA2-7B、LLaMA3-8B、Qwen2.5-7B和Qwen2.5-14B上实现最高2.6倍解码加速和45-55% KV缓存减少，精度损失小于0.5个百分点。", "conclusion": "LoRA-Drop提供了一个简单路径实现自适应容量推理，在推理、代码生成和长上下文/多语言基准测试中识别出保持质量同时提供显著效率增益的安全调度配置区间。"}}
{"id": "2601.02714", "pdf": "https://arxiv.org/pdf/2601.02714", "abs": "https://arxiv.org/abs/2601.02714", "authors": ["Zhi Liu", "Guangzhi Wang"], "title": "Time-Scaling Is What Agents Need Now", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities.\n  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.\n  This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.", "AI": {"tldr": "论文提出时间扩展（Time-Scaling）概念，通过架构设计扩展时间推理路径，增强AI代理的深度推理能力，解决现有方法在搜索完整性和效率上的局限。", "motivation": "早期AI范式功能分离，现有大语言模型虽然能生成流畅文本但缺乏强语义推理能力，CoT和ToT等提示技术虽能扩展推理路径但仍存在搜索完整性和效率问题。", "method": "提出时间扩展的架构设计，利用扩展的时间路径实现更深层次的问题空间探索、动态策略调整和增强的元认知控制，模拟人类在认知约束下的序列推理。", "result": "时间扩展能够在不成比例增加静态模型参数的情况下，显著增强深度推理和问题解决能力，是提升智能代理能力的关键前沿技术。", "conclusion": "时间扩展原则应置于前沿位置，将显式时间推理管理作为基础，是推进智能代理能力发展的关键方向。"}}
{"id": "2601.02574", "pdf": "https://arxiv.org/pdf/2601.02574", "abs": "https://arxiv.org/abs/2601.02574", "authors": ["Haoran Wang", "Maryam Khalid", "Qiong Wu", "Jian Gao", "Cheng Cao"], "title": "Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.", "AI": {"tldr": "PCC框架通过联合建模LLM的概率确定性和推理一致性来估计事实置信度，实现自适应验证策略：高置信度时直接回答，不确定时触发定向检索，高模糊度时深度搜索，显著提升事实检查的效率和准确性。", "motivation": "LLM在需要事实准确性的应用中经常产生幻觉响应，现有方法通常不加区分地检索外部证据，忽略了模型内部知识并可能引入无关噪声，且缺乏解决模型推理中特定不确定性的机制。", "method": "引入概率确定性和一致性(PCC)框架，通过联合建模LLM的概率确定性和推理一致性来估计事实置信度，实现基于置信度的自适应验证策略。", "result": "在三个挑战性基准测试中，PCC在不确定性量化方面优于口头置信度方法，并持续超越基于LLM的事实检查基线方法，且在不同LLM间具有良好的泛化能力。", "conclusion": "PCC框架通过自适应地利用内部知识和外部检索，有效减少了幻觉响应，提高了事实检查的效率和可靠性，为LLM的事实准确性提供了有前景的解决方案。"}}
{"id": "2601.02749", "pdf": "https://arxiv.org/pdf/2601.02749", "abs": "https://arxiv.org/abs/2601.02749", "authors": ["Nadia Sibai", "Yara Ahmed", "Serry Sibaee", "Sawsan AlHalawani", "Adel Ammar", "Wadii Boulila"], "title": "The Path Ahead for Agentic AI: Challenges and Opportunities", "categories": ["cs.AI"], "comment": null, "summary": "The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.", "AI": {"tldr": "本论文分析了大型语言模型从被动文本生成器向自主目标驱动系统的演变，提出了实现自主行为的核心架构框架，并指出了安全性、对齐性和可靠性等关键挑战。", "motivation": "研究LLM从语言理解向自主行动的架构转型，探索如何通过规划、记忆、工具使用和迭代推理来实现真正的自主智能系统。", "method": "通过分析LLM架构的演进历程，识别实现自主行为的关键能力（长程推理、情境感知、自适应决策），并构建包含感知、记忆、规划和工具执行的核心组件框架。", "result": "提出了三个主要贡献：1）LLM通过推理-行动-反思循环实现自主性的综合分析；2）连接LLM与自主行为的集成框架；3）对应用和挑战的批判性评估。", "conclusion": "负责任地推进自主AI系统需要在技术鲁棒性、可解释性和伦理保障方面同步进展，同时解决可验证规划、多智能体协调、持久记忆架构和治理框架等关键研究重点。"}}
{"id": "2601.02578", "pdf": "https://arxiv.org/pdf/2601.02578", "abs": "https://arxiv.org/abs/2601.02578", "authors": ["Mengyi Sun"], "title": "DataParasite Enables Scalable and Repurposable Online Data Curation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.", "AI": {"tldr": "DataParasite是一个开源的模块化网络数据采集管道，通过轻量级配置文件和自然语言指令实现可扩展的在线数据收集，显著降低了计算社会科学中的数据采集成本。", "motivation": "计算社会科学研究需要从异构在线来源收集数据，但现有方法通常劳动密集、成本高昂且难以复现，需要更高效透明的数据收集工具。", "method": "将表格化数据收集任务分解为独立的实体级别搜索，通过轻量级配置文件定义，使用任务无关的Python脚本执行，支持自然语言指令适应新任务。", "result": "在多个计算社会科学任务中实现高准确率，数据收集成本相比人工收集降低了一个数量级。", "conclusion": "DataParasite通过降低技术和劳动力门槛，为计算社会科学及其他领域提供了可扩展、透明和可复用的数据收集基础。"}}
{"id": "2601.02757", "pdf": "https://arxiv.org/pdf/2601.02757", "abs": "https://arxiv.org/abs/2601.02757", "authors": ["Zixuan Xiao", "Jun Ma"], "title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "categories": ["cs.AI"], "comment": null, "summary": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.", "AI": {"tldr": "ChangeGPT是一个集成大语言模型与视觉基础模型的通用智能体框架，用于遥感变化检测，在复杂多步推理和工具选择方面表现优异，达到90.71%的匹配准确率。", "motivation": "现有变化检测方法缺乏处理多样化真实世界查询的通用性和进行综合分析的能力，需要更智能的解决方案。", "method": "采用分层结构集成LLM与视觉基础模型构建ChangeGPT，在包含140个问题的精选数据集上进行评估，涵盖多种问题类型和复杂度。", "result": "ChangeGPT特别是基于GPT-4-turbo的版本表现卓越，匹配率达到90.71%，在需要多步推理的变化相关查询中表现尤为突出。", "conclusion": "ChangeGPT通过提供智能化、适应性和多类型变化分析能力，为遥感应用决策提供了强大的解决方案，并在深圳前海湾的实际城市变化监测案例中得到验证。"}}
{"id": "2601.02580", "pdf": "https://arxiv.org/pdf/2601.02580", "abs": "https://arxiv.org/abs/2601.02580", "authors": ["Christopher Ormerod"], "title": "Reconstructing Item Characteristic Curves using Fine-Tuned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 5 tables, 3 figures", "summary": "Traditional methods for determining assessment item parameters, such as difficulty and discrimination, rely heavily on expensive field testing to collect student performance data for Item Response Theory (IRT) calibration. This study introduces a novel approach that implicitly models these psychometric properties by fine-tuning Large Language Models (LLMs) to simulate student responses across a spectrum of latent abilities. Leveraging the Qwen-3 dense model series and Low-Rank Adaptation (LoRA), we train models to generate responses to multiple choice questions conditioned on discrete ability descriptors. We reconstruct the probability of a correct response as a function of student ability, effectively generating synthetic Item Characteristic Curves (ICCs) to estimate IRT parameters. Evaluation on a dataset of Grade 6 English Language Arts (ELA) items and the BEA 2024 Shared Task dataset demonstrates that this method competes with or outperforms baseline approaches. This simulation-based technique seems particularly effective at modeling item discrimination.", "AI": {"tldr": "本研究提出了一种使用大语言模型模拟学生回答来估计题目参数的新方法，通过微调LLMs生成不同能力水平学生的选择题回答，从而构建项目特征曲线并估计IRT参数，在ELA和BEA数据集上表现优于传统基线方法。", "motivation": "传统方法依赖昂贵的现场测试收集学生表现数据进行IRT校准，需要大量时间和资源，因此需要开发更高效的题目参数估计方法。", "method": "使用Qwen-3密集模型系列和低秩适应(LoRA)技术，微调大语言模型来模拟不同潜在能力水平学生对选择题的回答，通过生成的项目特征曲线重建正确回答概率与能力的关系函数。", "result": "在六年级英语语言艺术项目和BEA 2024共享任务数据集上的评估表明，该方法与基线方法竞争或优于基线方法，特别在模拟题目区分度方面表现有效。", "conclusion": "基于仿真的技术为题目参数估计提供了一种有前景的替代方案，能够减少对传统现场测试的依赖，特别是在建模题目区分度方面显示出显著效果。"}}
{"id": "2601.02813", "pdf": "https://arxiv.org/pdf/2601.02813", "abs": "https://arxiv.org/abs/2601.02813", "authors": ["Masum Hasan", "Junjie Zhao", "Ehsan Hoque"], "title": "HAL: Inducing Human-likeness in LLMs with Alignment", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.", "AI": {"tldr": "HAL框架通过可解释的数据驱动奖励机制，将语言模型与对话人类相似性对齐，使用对比对话数据提取显性对话特征并组合成标量评分，通过偏好优化方法实现对齐而不影响模型整体性能。", "motivation": "对话人类相似性在人机交互中至关重要，但难以定义、测量和优化，现有改进主要依赖规模扩展或广泛监督训练，缺乏针对性对齐方法。", "method": "从对比对话数据中提取显性对话特征，组合成紧凑的标量评分，作为透明奖励信号，使用标准偏好优化方法进行模型对齐。", "result": "人类大规模评估显示，HAL对齐的模型在对话中更频繁被感知为人类化，且不影响模型整体性能，同时支持对齐行为的检查和意外效应的诊断。", "conclusion": "HAL展示了如何将语言中先前无法对齐的软性、定性属性变得可测量，并以可解释和可说明的方式进行对齐，扩展了对齐技术的范围。"}}
{"id": "2601.02589", "pdf": "https://arxiv.org/pdf/2601.02589", "abs": "https://arxiv.org/abs/2601.02589", "authors": ["Kris W Pan", "Yongmin Yoo"], "title": "FlowPlan-G2P: A Structured Generation Framework for Transforming Scientific Papers into Patent Descriptions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Over 3.5 million patents are filed annually, with drafting patent descriptions requiring deep technical and legal expertise. Transforming scientific papers into patent descriptions is particularly challenging due to their differing rhetorical styles and stringent legal requirements. Unlike black-box text-to-text approaches that struggle to model structural reasoning and legal constraints, we propose FlowPlan-G2P, a novel framework that mirrors the cognitive workflow of expert drafters by reformulating this task into three stages: (1) Concept Graph Induction, extracting technical entities and relationships into a directed graph via expert-like reasoning; (2) Paragraph and Section Planning, reorganizing the graph into coherent clusters aligned with canonical patent sections; and (3) Graph-Conditioned Generation, producing legally compliant paragraphs using section-specific subgraphs and tailored prompts. Experiments demonstrate that FlowPlan-G2P significantly improves logical coherence and legal compliance over end-to-end LLM baselines. Our framework establishes a new paradigm for paper-to-patent generation and advances structured text generation for specialized domains.", "AI": {"tldr": "FlowPlan-G2P是一个将科学论文转化为专利描述的三阶段框架，通过概念图提取、段落章节规划和图条件生成，显著提升了逻辑连贯性和法律合规性。", "motivation": "每年有超过350万份专利申请，将科学论文转化为专利描述具有挑战性，因为两者的修辞风格和法律要求不同，现有端到端方法难以处理结构推理和法律约束。", "method": "提出FlowPlan-G2P框架，模拟专家起草者的认知流程：1)概念图归纳提取技术实体和关系；2)段落章节规划将图重组为符合专利章节的连贯簇；3)图条件生成使用特定子图和提示生成合规段落。", "result": "实验表明FlowPlan-G2P在逻辑连贯性和法律合规性方面显著优于端到端LLM基线方法。", "conclusion": "该框架为论文到专利生成建立了新范式，并推动了专业领域的结构化文本生成进展。"}}
{"id": "2601.02814", "pdf": "https://arxiv.org/pdf/2601.02814", "abs": "https://arxiv.org/abs/2601.02814", "authors": ["Duc Ngo", "Arya Rahgoza"], "title": "Causal-Enhanced AI Agents for Medical Research Screening", "categories": ["cs.AI"], "comment": "for submission to The 39th Canadian Conference on Artificial Intelligence", "summary": "Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.\n  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.\n  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.", "AI": {"tldr": "提出一个结合因果图增强的检索增强生成系统，通过双重知识图谱整合显式因果推理，在系统综述任务中实现95%准确率和零幻觉，显著优于基线AI模型。", "motivation": "手动审查每年150万+医学出版物不可行，现有AI方法在系统综述任务中存在幻觉问题（2-40%的错误率），这在影响患者护理时不可接受。", "method": "开发因果图增强的检索增强生成系统，整合显式因果推理与双重知识图谱，执行证据优先协议，每个因果声明都追溯到检索文献，并自动生成可视化干预-结果路径的有向无环图。", "result": "在234篇痴呆症运动研究摘要评估中，CausalAgent达到95%准确率、100%检索成功率和零幻觉，而基线AI只有34%准确率和10%幻觉。自动因果图支持显式机制建模、可视化合成和增强可解释性。", "conclusion": "虽然概念验证评估仅针对痴呆症运动研究的十个问题，但该架构方法展示了可信赖医学AI的可转移原则，以及因果推理在高风险医疗保健中的潜力。"}}
{"id": "2601.02604", "pdf": "https://arxiv.org/pdf/2601.02604", "abs": "https://arxiv.org/abs/2601.02604", "authors": ["Cesar Felipe Martínez Cisneros", "Jesús Ulises Quiroz Bautista", "Claudia Anahí Guzmán Solano", "Bogdan Kaleb García Rivera", "Iván García Pacheco", "Yalbi Itzel Balderas Martínez", "Kolawole John Adebayoc", "Ignacio Arroyo Fernández"], "title": "Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs", "categories": ["cs.CL"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.", "AI": {"tldr": "本研究开发了一个基于OpenIE的肺癌知识库构建流程，通过提取医学文献中的三元组来增强大语言模型在生物医学领域的微调效果，显著提升了模型性能和语义一致性。", "motivation": "在肿瘤学研究中，大语言模型的性能高度依赖于训练数据的语义质量，需要可扩展的方法来构建结构化知识库以实现有效的领域特定微调。", "method": "提出四步流程：(1)使用MeSH词典识别医学概念；(2)筛选CC0许可的PubMed文献；(3)用OpenIE方法提取(主语,关系,宾语)三元组；(4)通过命名实体识别(NER)增强三元组的生物医学相关性。", "result": "使用该数据集通过监督语义微调训练T5模型，ROUGE和BERTScore评估显示模型性能和语义连贯性显著提升。", "conclusion": "OpenIE衍生的资源可作为可扩展、低成本的解决方案，有效增强生物医学自然语言处理能力，为领域特定的大语言模型微调提供了有价值的知识基础。"}}
{"id": "2601.02818", "pdf": "https://arxiv.org/pdf/2601.02818", "abs": "https://arxiv.org/abs/2601.02818", "authors": ["Muzhen Zhang", "Yujie Cheng", "Zhanxiang Lei"], "title": "Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs", "categories": ["cs.AI", "quant-ph"], "comment": "22 pages, 7 figures", "summary": "Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.", "AI": {"tldr": "本研究提出了一种量子增强的长短时记忆注意力模型(QLSTMA)，首次将变分量子电路集成到循环单元中，用于储层渗透率预测，相比传统方法显著降低了预测误差。", "motivation": "储层参数特别是渗透率的空间预测对油气勘探开发至关重要，但现有方法难以可靠预测渗透率的广泛范围和高变异性。", "method": "设计了两种量子化结构(QLSTMA-SG和QLSTMA-IG)，将变分量子电路(VQCs)融入循环单元，利用量子纠缠和叠加原理，研究量子结构配置和量子比特数对模型性能的影响。", "result": "8量子比特的QLSTMA-IG模型显著优于传统LSTMA，MAE降低19%，RMSE降低20%，在复杂测井数据区域表现尤为突出。", "conclusion": "研究验证了量子-经典混合神经网络在储层预测中的潜力，为未来在真实量子硬件上部署此类模型及其在石油工程和地球科学中的更广泛应用奠定了基础框架。"}}
{"id": "2601.02627", "pdf": "https://arxiv.org/pdf/2601.02627", "abs": "https://arxiv.org/abs/2601.02627", "authors": ["Nelvin Tan", "Yaowen Zhang", "James Asikin Cheung", "Fusheng Liu", "Yu-Ching Shih", "Dong Yang"], "title": "Improved Evidence Extraction for Document Inconsistency Detection with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. However, research on LLM-based approaches to document inconsistency detection is relatively limited. There are two key aspects of document inconsistency detection: (i) classification of whether there exists any inconsistency, and (ii) providing evidence of the inconsistent sentences. We focus on the latter, and introduce new comprehensive evidence-extraction metrics and a redact-and-retry framework with constrained filtering that substantially improves LLM-based document inconsistency detection over direct prompting. We back our claims with promising experimental results.", "AI": {"tldr": "论文提出了一个基于大语言模型的文档不一致性检测方法，通过redact-and-retry框架和约束过滤显著改进了证据提取性能", "motivation": "现有基于大语言模型的文档不一致性检测研究相对有限，特别是在提取不一致证据方面需要更好的方法", "method": "引入了新的全面证据提取指标，并提出了redact-and-retry框架与约束过滤技术", "result": "实验结果显示该方法在文档不一致性检测方面相比直接提示方法有显著提升", "conclusion": "所提出的框架和指标有效提升了LLM在文档不一致证据提取方面的能力，为文档质量评估提供了实用工具"}}
{"id": "2601.02850", "pdf": "https://arxiv.org/pdf/2601.02850", "abs": "https://arxiv.org/abs/2601.02850", "authors": ["Celeste Veronese", "Daniele Meli", "Alessandro Farinelli"], "title": "Sample-Efficient Neurosymbolic Deep Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.", "AI": {"tldr": "提出一种神经符号深度强化学习方法，通过整合符号知识来提升样本效率和泛化能力，在稀疏奖励和长规划视野任务中表现优异", "motivation": "当前深度强化学习算法需要大量训练数据且泛化能力有限，特别是在小规模训练场景之外表现不佳，需要改进样本效率和跨任务泛化能力", "method": "将部分策略表示为逻辑规则，通过在线推理指导训练过程：(i)在探索期间偏置动作分布，(ii)在利用期间重新缩放Q值，实现从简单任务到复杂任务的知识迁移", "result": "在网格世界环境的各种挑战性变体上实证验证，包括完全可观测和部分可观测设置，相比最先进的奖励机器基线显示出更好的性能", "conclusion": "神经符号集成方法不仅加速了收敛过程，特别是在稀疏奖励环境中，还增强了算法的可解释性和可信度，为复杂环境中的强化学习提供了有效解决方案"}}
{"id": "2601.02659", "pdf": "https://arxiv.org/pdf/2601.02659", "abs": "https://arxiv.org/abs/2601.02659", "authors": ["Kuo Wang", "Haowei Hua", "Pengfei Yan", "Hong Jiao", "Dan Song"], "title": "Empirical Comparison of Encoder-Based Language Models and Feature-Based Supervised Machine Learning Approaches to Automated Scoring of Long Essays", "categories": ["cs.CL", "cs.LG"], "comment": "22 pages, 5 figures, 3 tables, presented at National Council on Measurement in Education 2025", "summary": "Long context may impose challenges for encoder-only language models in text processing, specifically for automated scoring of essays. This study trained several commonly used encoder-based language models for automated scoring of long essays. The performance of these trained models was evaluated and compared with the ensemble models built upon the base language models with a token limit of 512?. The experimented models include BERT-based models (BERT, RoBERTa, DistilBERT, and DeBERTa), ensemble models integrating embeddings from multiple encoder models, and ensemble models of feature-based supervised machine learning models, including Gradient-Boosted Decision Trees, eXtreme Gradient Boosting, and Light Gradient Boosting Machine. We trained, validated, and tested each model on a dataset of 17,307 essays, with an 80%/10%/10% split, and evaluated model performance using Quadratic Weighted Kappa. This study revealed that an ensemble-of-embeddings model that combines multiple pre-trained language model representations with gradient-boosting classifier as the ensemble model significantly outperforms individual language models at scoring long essays.", "AI": {"tldr": "本研究通过集成多个预训练语言模型的嵌入表示，结合梯度提升分类器，显著提升了长文本自动评分性能，优于单一语言模型。", "motivation": "长文本处理对仅编码器语言模型构成挑战，特别是在长论文自动评分任务中，需要探索更有效的解决方案。", "method": "训练了多种基于BERT的模型（BERT、RoBERTa、DistilBERT、DeBERTa），构建了集成多个编码器模型嵌入的集成模型，以及基于特征的有监督机器学习集成模型（梯度提升决策树、XGBoost、LightGBM）。在17,307篇论文数据集上以80%/10%/10%的比例划分训练/验证/测试集。", "result": "集成嵌入模型（结合多个预训练语言模型表示和梯度提升分类器）在长论文评分方面显著优于单个语言模型。", "conclusion": "集成多个预训练语言模型的嵌入表示是解决长文本自动评分问题的有效方法，梯度提升分类器作为集成模型能够显著提升评分性能。"}}
{"id": "2601.02854", "pdf": "https://arxiv.org/pdf/2601.02854", "abs": "https://arxiv.org/abs/2601.02854", "authors": ["Ao Li", "Jinghui Zhang", "Luyu Li", "Yuxiang Duan", "Lang Gao", "Mingcai Chen", "Weijun Qin", "Shaopeng Li", "Fengxian Ji", "Ning Liu", "Lizhen Cui", "Xiuying Chen", "Yuntao Du"], "title": "M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?", "categories": ["cs.AI"], "comment": null, "summary": "As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.", "AI": {"tldr": "该论文提出了M3MAD-Bench基准测试，用于标准化评估多智能体辩论方法，涵盖多领域任务、多模态输入和多维度指标，解决了现有评估方法碎片化和单模态限制的问题。", "motivation": "现有多智能体辩论研究存在两个根本限制：评估在碎片化且不一致的设置下进行，阻碍了公平比较；主要局限于仅依赖文本输入的单模态场景。", "method": "建立统一的M3MAD-Bench基准，包含五个核心任务领域（知识、数学、医学、自然科学和复杂推理），系统覆盖纯文本和视觉语言数据集，评估9个不同架构、规模和模态能力的基础模型。", "result": "通过广泛实验获得了关于多智能体辩论在纯文本和多模态场景下有效性、鲁棒性和效率的系统性见解。", "conclusion": "M3MAD-Bench为未来标准化多智能体辩论评估研究提供了可靠基础，代码已开源。"}}
{"id": "2601.02663", "pdf": "https://arxiv.org/pdf/2601.02663", "abs": "https://arxiv.org/abs/2601.02663", "authors": ["Subha Ghoshal", "Ali Al-Bustami"], "title": "When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan-execute-replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\\% $\\rightarrow$ 67.5\\% for GPT-4o) while increasing latency by orders of magnitude ($\\sim$8s $\\rightarrow$ $\\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\\% at $\\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.", "AI": {"tldr": "论文评估了大型语言模型在推理时使用规划和外部工具的性能，发现在事件问答任务中工具增强能显著提升准确率但大幅增加延迟，而在说服性回复生成任务中单次提示效果最佳，工具使用反而增加延迟且无稳定收益。", "motivation": "现代大语言模型越来越依赖推理时规划和外部工具来改进推理能力，需要在实际场景中系统评估这种行为的性能表现和成本效益。", "method": "使用LangChain和LangGraph框架，在Event-QA（图结构知识的事件问答）和CMV（Reddit说服性回复生成）两个真实场景中，比较单次提示基线与配备任务特定工具（DBpedia查询、维基百科检索、网络搜索）的规划-执行-重规划代理的性能。", "result": "在Event-QA任务中，最佳工具增强配置将GPT-4o准确率从47.5%提升至67.5%，但延迟从约8秒大幅增加至约317秒；在CMV任务中，单次提示效果最好（GPT-4o-mini达到75%准确率，约6秒延迟），工具使用显著增加延迟但无稳定收益提升。", "conclusion": "研究强调需要根据具体任务、成本意识来选择模型大小和代理/工具复杂度，工具增强并非在所有场景都有效，需要权衡准确率提升与延迟成本。"}}
{"id": "2601.02871", "pdf": "https://arxiv.org/pdf/2601.02871", "abs": "https://arxiv.org/abs/2601.02871", "authors": ["Zhiyong Cao", "Dunqiang Liu", "Qi Dai", "Haojun Xu", "Huaiyan Xu", "Huan He", "Yafei Liu", "Siyuan Liu", "XiaoLin Lin", "Ke Ma", "Ruqian Shi", "Sijia Yao", "Hao Wang", "Sicheng Zhou"], "title": "SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection", "categories": ["cs.AI"], "comment": null, "summary": "Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.", "AI": {"tldr": "SimRPD是一个三阶段框架，通过高保真用户模拟器生成大规模对话数据，使用多维度评估选择高质量数据，训练招聘主动对话代理，在真实招聘场景中表现优于现有方法。", "motivation": "招聘等任务导向的主动对话代理面临高质量领域特定训练数据稀缺的问题，限制了监督微调和强化学习的效果。", "method": "1. 开发高保真用户模拟器生成多轮对话数据；2. 引入基于意图链的多维度评估框架（全局和实例级指标）选择高质量数据；3. 在筛选后的数据集上训练对话代理。", "result": "在真实招聘场景实验中，SimRPD优于现有的基于模拟器的数据选择策略。", "conclusion": "SimRPD具有工业部署的实用价值，并可能适用于其他业务导向的对话场景。"}}
{"id": "2601.02669", "pdf": "https://arxiv.org/pdf/2601.02669", "abs": "https://arxiv.org/abs/2601.02669", "authors": ["Hongzhan Lin", "Zixin Chen", "Zhiqi Shen", "Ziyang Luo", "Zhen Ye", "Jing Ma", "Tat-Seng Chua", "Guandong Xu"], "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking", "categories": ["cs.CL"], "comment": "17 pages, 21 figures, 7 tables", "summary": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.", "AI": {"tldr": "FactArena是一个自动化评估框架，对大型语言模型在完整事实核查流程中的表现进行综合评估，发现静态声明验证与端到端事实核查能力存在显著差异。", "motivation": "现有评估主要关注声明验证，忽略了完整的事实核查工作流程（包括声明提取和证据检索），无法揭示LLMs的系统性推理失败、事实盲点和鲁棒性限制。", "method": "FactArena包含三个核心组件：LLM驱动的事实核查流程（标准化声明分解、工具增强的证据检索、基于理由的裁决预测）；竞技场式判断机制（确保无偏见的成对比较）；竞技场驱动的声明演化模块（生成更具挑战性的声明）。", "result": "在16个最先进的LLM上进行测试，产生了稳定且可解释的排名，揭示了静态声明验证准确性与端到端事实核查能力之间的显著差异。", "conclusion": "FactArena提供了一个可扩展且可信赖的范式，用于诊断LLMs的事实推理能力，指导未来模型开发，并推进LLMs在安全关键的事实核查应用中的可靠部署。"}}
{"id": "2601.02880", "pdf": "https://arxiv.org/pdf/2601.02880", "abs": "https://arxiv.org/abs/2601.02880", "authors": ["Abhishek HS", "Pavan C Shekar", "Arpit Jain", "Ashwanth Krishnan"], "title": "ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "14 pages, 1 figure, 5 tables", "summary": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.", "AI": {"tldr": "ReTreVal是一个结合树状思维探索、自我精炼、LLM批判评分和反思记忆的混合框架，通过结构化探索替代解决方案路径和跨问题学习来提升多步推理能力，在数学和创意写作任务上显著优于现有方法。", "motivation": "现有方法如ReAct、Reflexion和Self-Refine在多步推理中存在不足，缺乏对替代解决方案路径的结构化探索和跨问题的持续学习能力。", "method": "构建基于问题复杂度的自适应深度推理树，每个节点进行迭代自我批判和精炼，采用双重验证机制评估推理质量，使用基于批判的剪枝保留高分节点，并通过反思记忆缓冲区存储成功路径和失败模式。", "result": "在500个数学问题和创意写作任务上使用Qwen 2.5 7B进行评估，ReTreVal在结构化探索、批判驱动精炼和跨问题记忆的综合作用下，一致优于现有方法。", "conclusion": "ReTreVal特别适用于需要探索性推理、严格验证和知识迁移的任务，通过有界且经过验证的多步推理有效解决了LLM在复杂领域的推理挑战。"}}
{"id": "2601.02670", "pdf": "https://arxiv.org/pdf/2601.02670", "abs": "https://arxiv.org/abs/2601.02670", "authors": ["Devang Kulshreshtha", "Hang Su", "Chinmay Hegde", "Haohan Wang"], "title": "Multi-Turn Jailbreaking of Aligned LLMs via Lexical Anchor Tree Search", "categories": ["cs.CL"], "comment": null, "summary": "Most jailbreak methods achieve high attack success rates (ASR) but require attacker LLMs to craft adversarial queries and/or demand high query budgets. These resource limitations make jailbreaking expensive, and the queries generated by attacker LLMs often consist of non-interpretable random prefixes. This paper introduces Lexical Anchor Tree Search (), addressing these limitations through an attacker-LLM-free method that operates purely via lexical anchor injection. LATS reformulates jailbreaking as a breadth-first tree search over multi-turn dialogues, where each node incrementally injects missing content words from the attack goal into benign prompts. Evaluations on AdvBench and HarmBench demonstrate that LATS achieves 97-100% ASR on latest GPT, Claude, and Llama models with an average of only ~6.4 queries, compared to 20+ queries required by other methods. These results highlight conversational structure as a potent and under-protected attack surface, while demonstrating superior query efficiency in an era where high ASR is readily achievable. Our code will be released to support reproducibility.", "AI": {"tldr": "LATS是一种无需攻击者LLM的越狱方法，通过词汇锚注入和广度优先树搜索实现高效对话攻击，仅需约6.4次查询即可在最新模型上达到97-100%的攻击成功率", "motivation": "现有越狱方法需要攻击者LLM生成对抗性查询且查询预算高，导致成本昂贵且生成的查询包含不可解释的随机前缀", "method": "将越狱重新定义为多轮对话的广度优先树搜索，每个节点逐步将攻击目标中的缺失内容词注入到良性提示中", "result": "在AdvBench和HarmBench评估中，LATS在最新GPT、Claude和Llama模型上达到97-100%攻击成功率，平均仅需约6.4次查询，而其他方法需要20+次查询", "conclusion": "对话结构是一个强大且保护不足的攻击面，在攻击成功率容易达到的时代展示了卓越的查询效率"}}
{"id": "2601.02902", "pdf": "https://arxiv.org/pdf/2601.02902", "abs": "https://arxiv.org/abs/2601.02902", "authors": ["Xinglang Zhang", "Yunyao Zhang", "ZeLiang Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": null, "summary": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.", "AI": {"tldr": "研究发现大语言模型在逻辑推理中存在逻辑相变现象：在临界逻辑深度之前性能稳定，超过后突然崩溃。提出了神经符号课程调优框架，通过自适应对齐自然语言与逻辑符号来增强复杂逻辑推理能力。", "motivation": "符号逻辑推理是大语言模型的关键但未被充分探索的能力，在数学推理和法律判决等高风险领域需要可靠且可验证的决策。当前模型在复杂逻辑推理方面存在性能突然崩溃的问题。", "method": "提出神经符号课程调优框架，自适应对齐自然语言与逻辑符号建立共享表示，围绕相变边界重塑训练动态，逐步增强递增逻辑深度的推理能力。", "result": "在五个基准测试中，该方法有效缓解了高复杂度下的逻辑推理崩溃，在普通提示和思维链提示下分别获得+1.26和+3.95的平均准确率提升，并改善了未见逻辑组合的泛化能力。", "conclusion": "逻辑相变现象揭示了LLMs逻辑推理的临界行为特征，神经符号课程调优提供了一种原则性方法来系统提升模型的复杂逻辑推理性能，具有重要的理论和实践意义。"}}
{"id": "2601.02671", "pdf": "https://arxiv.org/pdf/2601.02671", "abs": "https://arxiv.org/abs/2601.02671", "authors": ["Ahmed Ahmed", "A. Feder Cooper", "Sanmi Koyejo", "Percy Liang"], "title": "Extracting books from production language models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "We ran experiments from mid-August to mid-September 2025, notified affected providers shortly after, and now make our findings public after a 90-day disclosure window", "summary": "Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.", "AI": {"tldr": "研究发现即使有安全措施，生产级LLM仍可能被提取受版权保护的训练数据，不同模型提取难度和效果差异显著", "motivation": "解决LLM是否会在权重中记忆训练数据以及能否从生产级LLM中提取这些数据的关键法律问题", "method": "采用两阶段程序：初始探测测试提取可行性（有时使用Best-of-N越狱），然后通过迭代续写提示尝试提取整本书内容", "result": "成功从四个生产级LLM中提取了不同数量的文本，Gemini 2.5 Pro和Grok 3无需越狱即可提取（nv-recall达76.8%和70.3%），Claude 3.7 Sonnet越狱后可输出近完整书籍（95.8%），GPT-4.1需要更多尝试但最终拒绝继续（4.0%）", "conclusion": "即使有模型和系统级安全保护，提取受版权训练数据仍然是生产级LLM的现实风险"}}
{"id": "2601.02950", "pdf": "https://arxiv.org/pdf/2601.02950", "abs": "https://arxiv.org/abs/2601.02950", "authors": ["Xuan Yang", "Furong Jia", "Roy Xie", "Xiong Xi", "Hengwei Bian", "Jian Li", "Monica Agrawal"], "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.", "AI": {"tldr": "Batch-of-Thought (BoT)是一种无需训练的方法，通过批量处理相关查询来实现跨实例学习，利用多智能体反射架构(BoT-R)提升LLM推理性能", "motivation": "当前大型语言模型独立处理查询，忽略了跨实例信号如共享推理模式和一致性约束，导致推理效率和质量受限", "method": "引入BoT方法，通过批量处理相关查询进行跨实例比较分析，识别高质量推理模板，通过一致性检查检测错误，并采用多智能体反射架构(BoT-R)进行联合评估", "result": "在三个模型系列和六个基准测试中，BoT-R持续提升准确率和置信度校准，同时将推理成本降低高达61%", "conclusion": "批量感知推理在特定条件下可为LLM系统带来显著收益，理论和实验分析揭示了其有效性的条件和原因"}}
{"id": "2601.02674", "pdf": "https://arxiv.org/pdf/2601.02674", "abs": "https://arxiv.org/abs/2601.02674", "authors": ["Guangxin Wu", "Hao Zhang", "Zhang Zhibin", "Jiafeng Guo", "Xueqi Cheng"], "title": "Iterative Structured Pruning for Large Language Models with Multi-Domain Calibration", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide spectrum of natural language processing tasks. However, their ever-growing scale introduces significant barriers to real-world deployment, including substantial computational overhead, memory footprint, and inference latency. While model pruning presents a viable solution to these challenges, existing unstructured pruning techniques often yield irregular sparsity patterns that necessitate specialized hardware or software support. In this work, we explore structured pruning, which eliminates entire architectural components and maintains compatibility with standard hardware accelerators. We introduce a novel structured pruning framework that leverages a hybrid multi-domain calibration set and an iterative calibration strategy to effectively identify and remove redundant channels. Extensive experiments on various models across diverse downstream tasks show that our approach achieves significant compression with minimal performance degradation.", "AI": {"tldr": "该论文提出了一个新颖的结构化剪枝框架，使用混合多域校准集和迭代校准策略来有效识别和移除冗余通道，实现显著模型压缩且性能损失最小。", "motivation": "大型语言模型规模不断增长带来了计算开销、内存占用和推理延迟等部署障碍，现有非结构化剪枝技术产生的不规则稀疏模式需要专用硬件或软件支持。", "method": "采用结构化剪枝方法，消除整个架构组件以保持与标准硬件加速器的兼容性。使用混合多域校准集和迭代校准策略来识别冗余通道。", "result": "在各种模型和下游任务的广泛实验中，该方法实现了显著的压缩效果且性能下降最小。", "conclusion": "结构化剪枝是解决LLM部署挑战的有效方案，提出的框架在保持硬件兼容性的同时实现了高效的模型压缩。"}}
{"id": "2601.02968", "pdf": "https://arxiv.org/pdf/2601.02968", "abs": "https://arxiv.org/abs/2601.02968", "authors": ["Qingxiang Liu", "Zhiqing Cui", "Xiaoliang Luo", "Yuqian Wu", "Zhuoyang Jiang", "Huaiyu Wan", "Sheng Sun", "Lvchun Wang", "Wei Yu", "Yuxuan Liang"], "title": "Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.", "AI": {"tldr": "RationaleTS方法通过引入基于推理路径的rationale先验知识，改进多模态大语言模型在时间序列推理中的表现，避免模型仅依赖表面模式匹配而进行原则性推理。", "motivation": "现有多模态大语言模型在时间序列推理中表现不佳，主要原因是缺乏将时间观察与下游结果连接起来的rationale先验知识，导致模型依赖表面模式匹配而非原则性推理。", "method": "提出rationale-grounded in-context learning方法：1) 诱导标签条件rationale，构建从观察证据到潜在结果的推理路径；2) 设计混合检索策略，平衡时间模式和语义上下文，为新的样本检索相关rationale先验进行上下文推理。", "result": "在三个领域的时间序列推理任务上进行了广泛实验，证明了RationaleTS方法的有效性和效率。", "conclusion": "RationaleTS通过引入rationale作为指导推理单元而非事后解释，显著提升了时间序列推理性能，代码将开源以供复现。"}}
{"id": "2601.02695", "pdf": "https://arxiv.org/pdf/2601.02695", "abs": "https://arxiv.org/abs/2601.02695", "authors": ["Guibin Zhang", "Haiyang Yu", "Kaiming Yang", "Bingli Wu", "Fei Huang", "Yongbin Li", "Shuicheng Yan"], "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "AI": {"tldr": "EvoRoute是一个自进化的模型路由框架，通过动态选择Pareto最优的LLM骨干网络来解决Agent系统三难问题（性能、成本、延迟的权衡），在保持性能的同时显著降低成本和延迟。", "motivation": "复杂的多智能体AI系统虽然表现出色，但存在高昂的经济成本和严重延迟问题，需要解决性能、成本和延迟之间的三难权衡。", "method": "提出EvoRoute自进化模型路由范式，利用不断扩展的经验知识库，在每一步动态选择Pareto最优的LLM骨干网络，并通过环境反馈持续优化选择策略。", "result": "在GAIA和BrowseComp+等基准测试中，EvoRoute在保持或提升系统性能的同时，将执行成本降低高达80%，延迟减少超过70%。", "conclusion": "EvoRoute成功解决了Agent系统的三难问题，为构建高效、经济、高性能的智能体系统提供了有效解决方案。"}}
{"id": "2601.03062", "pdf": "https://arxiv.org/pdf/2601.03062", "abs": "https://arxiv.org/abs/2601.03062", "authors": ["Qusai Khaled", "Pasquale De Marinis", "Moez Louati", "David Ferras", "Laura Genga", "Uzay Kaymak"], "title": "Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at IFSA-NAFIPS 2025", "summary": "Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.", "AI": {"tldr": "提出了一种可解释的模糊图神经网络框架(FGENConv)，用于水管网泄漏检测与定位，在保持较高精度的同时提供基于模糊逻辑的规则化解释。", "motivation": "现有图神经网络在水管网泄漏检测中虽然性能优异，但黑盒特性阻碍了实际应用，缺乏基于图的可解释模型。", "method": "整合互信息识别关键网络区域，使用模糊逻辑为节点分类任务提供规则化解释。在比较多种GNN架构后选择GENConv，并开发了模糊增强版本FGENConv。", "result": "FGENConv在检测和定位任务上分别获得0.889和0.814的Graph F1分数，略低于原始GENConv的0.938和0.858，但能提供空间局部化的模糊规则解释。", "conclusion": "该框架在精度和可解释性之间取得了良好平衡，有助于水力工程师验证预测结果、节省人力资源并优化维护策略。"}}
{"id": "2601.02697", "pdf": "https://arxiv.org/pdf/2601.02697", "abs": "https://arxiv.org/abs/2601.02697", "authors": ["Meysam Shirdel Bilehsavar", "Negin Mahmoudi", "Mohammad Jalili Torkamani", "Kiana Kiashemshaki"], "title": "Boosting Accuracy and Interpretability in Multilingual Hate Speech Detection Through Layer Freezing and Explainable AI", "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Sentiment analysis focuses on identifying the emotional polarity expressed in textual data, typically categorized as positive, negative, or neutral. Hate speech detection, on the other hand, aims to recognize content that incites violence, discrimination, or hostility toward individuals or groups based on attributes such as race, gender, sexual orientation, or religion. Both tasks play a critical role in online content moderation by enabling the detection and mitigation of harmful or offensive material, thereby contributing to safer digital environments. In this study, we examine the performance of three transformer-based models: BERT-base-multilingual-cased, RoBERTa-base, and XLM-RoBERTa-base with the first eight layers frozen, for multilingual sentiment analysis and hate speech detection. The evaluation is conducted across five languages: English, Korean, Japanese, Chinese, and French. The models are compared using standard performance metrics, including accuracy, precision, recall, and F1-score. To enhance model interpretability and provide deeper insight into prediction behavior, we integrate the Local Interpretable Model-agnostic Explanations (LIME) framework, which highlights the contribution of individual words to the models decisions. By combining state-of-the-art transformer architectures with explainability techniques, this work aims to improve both the effectiveness and transparency of multilingual sentiment analysis and hate speech detection systems.", "AI": {"tldr": "本研究评估了三种基于Transformer的模型（BERT、RoBERTa、XLM-RoBERTa）在五种语言上的情感分析和仇恨言论检测性能，并集成LIME框架提升模型可解释性。", "motivation": "情感分析和仇恨言论检测对于在线内容审核至关重要，需要有效的多语言解决方案来检测和缓解有害内容，创造更安全的数字环境。", "method": "使用BERT-base-multilingual-cased、RoBERTa-base和XLM-RoBERTa-base（前八层冻结）三种模型，在英语、韩语、日语、汉语和法语五种语言上进行评估，采用准确率、精确率、召回率和F1分数等标准指标，并集成LIME框架进行可解释性分析。", "result": "研究比较了三种模型在五种语言上的性能表现，并通过LIME框架揭示了各个单词对模型决策的贡献程度。", "conclusion": "通过结合最先进的Transformer架构和可解释性技术，这项工作旨在提高多语言情感分析和仇恨言论检测系统的有效性和透明度。"}}
{"id": "2601.03120", "pdf": "https://arxiv.org/pdf/2601.03120", "abs": "https://arxiv.org/abs/2601.03120", "authors": ["Adam Keane", "Nick Pepper", "Chris Burr", "Amy Hodgkin", "Dewi Gould", "John Korna", "Marc Thomas"], "title": "A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace", "categories": ["cs.AI"], "comment": null, "summary": "Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.\n  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.\n  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.", "AI": {"tldr": "该论文提出了一个用于数字孪生验证的保证框架，基于可信和伦理保证(TEA)方法，为航空交通管理中的AI/ATC代理提供结构化验证方法", "motivation": "数字孪生技术在航空业具有巨大潜力，但缺乏统一的监管框架和验证标准，需要为AI空中交通控制代理的训练和测试环境提供可信保证", "method": "采用可信和伦理保证(TEA)方法，开发嵌套式结构化论证框架，定义可操作目标和所需证据，验证数字孪生对其物理对应物的准确表示", "result": "提出了一个完整的保证框架，包含结构化论证和深度分析，为研究人员评估数字孪生优势与局限性提供系统方法", "conclusion": "该框架不仅支持数字孪生的技术验证，还为与利益相关者和监管机构的沟通提供了基础，有助于推动数字孪生技术的监管标准发展"}}
{"id": "2601.02700", "pdf": "https://arxiv.org/pdf/2601.02700", "abs": "https://arxiv.org/abs/2601.02700", "authors": ["Agniv Roy Choudhury", "Vignesh Ponselvan Rajasingh"], "title": "Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Question answering (QA) systems achieve impressive performance on standard benchmarks like SQuAD, but remain vulnerable to adversarial examples. This project investigates the adversarial robustness of transformer models on the AddSent adversarial dataset through systematic experimentation across model scales and targeted mitigation strategies. We perform comprehensive multi-level error analysis using five complementary categorization schemes, identifying negation confusion and entity substitution as the primary failure modes. Through systematic evaluation of adversarial fine-tuning ratios, we identify 80% clean + 20% adversarial data as optimal. Data augmentation experiments reveal a capacity bottleneck in small models. Scaling from ELECTRA-small (14M parameters) to ELECTRA-base (110M parameters) eliminates the robustness-accuracy trade-off, achieving substantial improvements on both clean and adversarial data. We implement three targeted mitigation strategies, with Entity-Aware contrastive learning achieving best performance: 89.89% AddSent Exact Match (EM) and 90.73% SQuAD EM, representing 94.9% closure of the adversarial gap. To our knowledge, this is the first work integrating comprehensive linguistic error analysis with Named Entity Recognition (NER)-guided contrastive learning for adversarial QA, demonstrating that targeted mitigation can achieve near-parity between clean and adversarial performance.", "AI": {"tldr": "该论文研究了Transformer模型在AddSent对抗数据集上的鲁棒性，通过多尺度实验和针对性缓解策略，发现实体感知对比学习能有效缩小干净数据与对抗数据之间的性能差距。", "motivation": "尽管问答系统在标准基准测试中表现优异，但对对抗样本仍然脆弱，需要研究其对抗鲁棒性和有效的缓解策略。", "method": "使用ELECTRA模型在不同规模上进行系统实验，采用五种分类方案进行多层级错误分析，评估对抗微调比例，并实施三种针对性缓解策略（包括实体感知对比学习）。", "result": "发现80%干净数据+20%对抗数据为最优比例，模型规模扩展消除了鲁棒性-准确性权衡，实体感知对比学习实现了最佳性能：89.89% AddSent EM和90.73% SQuAD EM，对抗差距缩小了94.9%。", "conclusion": "针对性缓解策略能够实现干净数据和对抗数据性能的近乎对等，首次将综合语言错误分析与NER引导的对比学习相结合，为对抗性问答研究提供了有效方法。"}}
{"id": "2601.03130", "pdf": "https://arxiv.org/pdf/2601.03130", "abs": "https://arxiv.org/abs/2601.03130", "authors": ["Faisal Chowdhury", "Nandana Mihindukulasooriya", "Niharika S D'Souza", "Horst Samulowitz", "Neeru Gupta", "Tomasz Hanusiak", "Michal Kapitonow"], "title": "Automatic Prompt Engineering with No Task Cues and No Tuning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.", "AI": {"tldr": "提出了一种简单但有效的自动提示工程系统，无需调优和任务显式线索，在数据库表的列名扩展任务上表现优异，支持英语和德语。", "motivation": "现有自动提示工程方法复杂且主要针对英语，需要开发更简单、无需调优、支持多语言的解决方案来处理数据库表列名扩展这一重要但研究较少的任务。", "method": "设计了一个无需调优和任务显式线索的自动提示工程系统，在英语和德语的数据库表列名扩展任务上进行评估。", "result": "系统在英语和德语数据集上都表现出色，是首个将自动提示工程应用于列名扩展任务的研究，也是首个支持非英语语言的自动提示工程工作。", "conclusion": "该方法证明了简单设计的自动提示工程系统同样有效，为多语言数据库表理解和搜索任务提供了实用解决方案，拓展了自动提示工程的应用范围。"}}
{"id": "2601.02739", "pdf": "https://arxiv.org/pdf/2601.02739", "abs": "https://arxiv.org/abs/2601.02739", "authors": ["Jinbo Hao", "Kai Yang", "Qingzhen Su", "Yang Chen", "Yifan Li", "Chao Jiang"], "title": "Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "To address hallucination issues in large language models (LLMs), this paper proposes a method for mitigating prompt-induced hallucinations. Building on a knowledge distillation chain-style model, we introduce a code module to guide knowledge-graph exploration and incorporate code as part of the chain-of-thought prompt, forming an external knowledge input that provides more accurate and structured information to the model. Based on this design, we develop an improved knowledge distillation chain-style model and leverage it to analyze and constrain the reasoning process of LLMs, thereby improving inference accuracy. We empirically evaluate the proposed approach using GPT-4 and LLaMA-3.3 on multiple public datasets. Experimental results demonstrate that incorporating code modules significantly enhances the model's ability to capture contextual information and effectively mitigates prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 improve by 15.64%, 13.38%, and 13.28%, respectively. Moreover, the proposed method achieves HIT@1, HIT@3, and HIT@5 scores exceeding 95% across several evaluation settings. These results indicate that the proposed approach substantially reduces hallucination behavior while improving the accuracy and verifiability of large language models.", "AI": {"tldr": "该论文提出一种通过代码模块增强知识图谱探索的方法，结合思维链提示来减轻大语言模型的提示诱导幻觉问题，显著提高了推理准确性和可验证性。", "motivation": "解决大语言模型中的幻觉问题，特别是由提示诱导产生的幻觉，需要更准确和结构化的外部知识输入来约束推理过程。", "method": "基于知识蒸馏链式模型，引入代码模块指导知识图谱探索，将代码作为思维链提示的一部分，形成外部知识输入，改进知识蒸馏链式模型以分析和约束LLMs的推理过程。", "result": "在GPT-4和LLaMA-3.3上的实验显示，HIT@1、HIT@3和HIT@5分别提升15.64%、13.38%和13.28%，多个评估设置中HIT分数超过95%。", "conclusion": "该方法能显著减少幻觉行为，同时提高大语言模型的准确性和可验证性，代码模块的引入有效增强了上下文信息捕捉能力。"}}
{"id": "2601.03204", "pdf": "https://arxiv.org/pdf/2601.03204", "abs": "https://arxiv.org/abs/2601.03204", "authors": ["Chenglin Yu", "Yuchen Wang", "Songmiao Wang", "Hongxia Yang", "Ming Li"], "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent", "AI": {"tldr": "InfiAgent框架通过外部化状态到文件中心抽象来解决LLM智能体在长时任务中的上下文膨胀问题，保持有界上下文的同时实现稳定推理", "motivation": "LLM智能体在长时任务中因上下文无限增长和错误累积而性能下降，现有方法在信息保真度和推理稳定性之间存在权衡", "method": "提出InfiAgent框架，将持久状态外部化为文件中心的状态抽象，每一步从工作空间状态快照加固定窗口的最近动作重建上下文", "result": "在DeepResearch和80篇文献综述任务中，20B开源模型的InfiAgent性能媲美更大专有系统，长时任务覆盖率显著高于基于上下文的基线方法", "conclusion": "显式状态外部化是构建稳定长时智能体的实用基础，为LLM智能体处理长时任务提供了有效解决方案"}}
{"id": "2601.02740", "pdf": "https://arxiv.org/pdf/2601.02740", "abs": "https://arxiv.org/abs/2601.02740", "authors": ["Luyao Chen", "Weibo Gao", "Junjie Wu", "Jinshan Wu", "Angela D. Friederici"], "title": "Language Hierarchization Provides the Optimal Solution to Human Working Memory Limits", "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Language is a uniquely human trait, conveying information efficiently by organizing word sequences in sentences into hierarchical structures. A central question persists: Why is human language hierarchical? In this study, we show that hierarchization optimally solves the challenge of our limited working memory capacity. We established a likelihood function that quantifies how well the average number of units according to the language processing mechanisms aligns with human working memory capacity (WMC) in a direct fashion. The maximum likelihood estimate (MLE) of this function, tehta_MLE, turns out to be the mean of units. Through computational simulations of symbol sequences and validation analyses of natural language sentences, we uncover that compared to linear processing, hierarchical processing far surpasses it in constraining the tehta_MLE values under the human WMC limit, along with the increase of sequence/sentence length successfully. It also shows a converging pattern related to children's WMC development. These results suggest that constructing hierarchical structures optimizes the processing efficiency of sequential language input while staying within memory constraints, genuinely explaining the universal hierarchical nature of human language.", "AI": {"tldr": "该研究表明人类语言的层级结构是为了优化有限工作记忆容量的处理效率，通过计算模拟和自然语言验证发现层级处理相比线性处理能更好地将记忆需求限制在人类工作记忆容量范围内。", "motivation": "探索为什么人类语言普遍具有层级结构，研究假设这与人类有限的工作记忆容量有关，层级结构可能是为了优化信息处理效率而演化出来的解决方案。", "method": "建立似然函数量化语言处理机制中的平均单元数与人类工作记忆容量的匹配程度，通过计算模拟符号序列和验证自然语言句子进行分析，比较层级处理和线性处理的性能差异。", "result": "层级处理在限制记忆需求方面远超线性处理，能成功将tehta_MLE值控制在人类工作记忆容量限制内，且随着序列/句子长度增加表现更好，与儿童工作记忆发展模式相吻合。", "conclusion": "构建层级结构优化了序列语言输入的处理效率，同时保持在记忆约束范围内，这真正解释了人类语言普遍层级性质的原因。"}}
{"id": "2601.03236", "pdf": "https://arxiv.org/pdf/2601.03236", "abs": "https://arxiv.org/abs/2601.03236", "authors": ["Dongming Jiang", "Yi Li", "Guanpeng Li", "Bingzhe Li"], "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.", "AI": {"tldr": "MAGMA提出了一种多图记忆架构，通过语义、时间、因果和实体四个正交图表示记忆项，使用策略引导的遍历进行检索，在长上下文推理任务中优于现有方法。", "motivation": "现有的基于语义相似度的单一记忆存储方法将时间、因果和实体信息混在一起，限制了可解释性和查询意图与检索证据的对齐，导致推理准确性不佳。", "method": "构建多图记忆架构（语义图、时间图、因果图、实体图），将每个记忆项在这些正交图中表示，通过策略引导的遍历实现查询自适应的检索和结构化上下文构建。", "result": "在LoCoMo和LongMemEval基准测试中，MAGMA在长时程推理任务上持续优于最先进的智能记忆系统。", "conclusion": "通过解耦记忆表示和检索逻辑，MAGMA提供了透明的推理路径和细粒度的检索控制，有效解决了现有记忆增强生成方法的局限性。"}}
{"id": "2601.02744", "pdf": "https://arxiv.org/pdf/2601.02744", "abs": "https://arxiv.org/abs/2601.02744", "authors": ["Hanqi Jiang", "Junhao Chen", "Yi Pan", "Ling Chen", "Weihang You", "Yifan Zhou", "Ruidong Zhang", "Yohannes Abate", "Tianming Liu"], "title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.", "AI": {"tldr": "Synapse是一种基于认知科学的内存架构，通过动态图模型和激活传播机制解决大语言模型在长期记忆中的局限性，显著提升了复杂时序和多跳推理任务的性能。", "motivation": "标准检索增强方法无法解决长期智能体记忆的离散性问题，需要超越静态向量相似度的统一内存架构来弥合这一差距。", "method": "借鉴认知科学，将内存建模为动态图，通过激活传播而非预计算链接来产生相关性；整合侧向抑制和时间衰减机制；实现几何嵌入与基于激活的图遍历相结合的三重混合检索策略。", "result": "在LoCoMo基准测试中，Synapse在复杂时序和多跳推理任务上显著优于最先进方法，有效解决了\"上下文隧道\"问题。", "conclusion": "Synapse提供了一个强大的解决方案，通过动态图模型和激活传播机制成功解决了大语言模型在长期记忆处理中的关键挑战。"}}
{"id": "2510.06747", "pdf": "https://arxiv.org/pdf/2510.06747", "abs": "https://arxiv.org/abs/2510.06747", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we propose a training-free and label-free method for short text clustering that can be used on top of any existing embedder. In the context of customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these commercial settings, no labeled data is typically available, and the number of clusters is not known. Our method is based on iterative vector updating: it constructs sparse vectors based on representative texts, and then iteratively refines them through LLM guidance. Our method achieves comparable or superior results to state-of-the-art methods that use contrastive learning, but without assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show that our method scales to large datasets, reducing the computational cost of the LLM. These low-resource, adaptable settings and the scalability of our method make it more aligned with real-world scenarios than existing clustering methods.", "AI": {"tldr": "提出一种无需训练和标注的无监督短文本聚类方法，通过迭代向量更新和LLM指导实现，在多种数据集上达到或超越现有方法性能。", "motivation": "商业聊天机器人场景中，企业需要处理大量无标注的用户话语聚类问题，但缺乏标注数据和先验的聚类数量知识。", "method": "基于迭代向量更新：构建基于代表性文本的稀疏向量，通过LLM指导进行迭代优化，不依赖聚类数量或标签的先验知识。", "result": "在多种数据集和小型LLM上验证，方法具有模型无关性，可应用于任何嵌入器，计算成本低且可扩展到大规模数据集。", "conclusion": "该方法在低资源、适应性强的设置下具有更好的现实场景适用性，比现有聚类方法更符合实际应用需求。"}}
{"id": "2601.02751", "pdf": "https://arxiv.org/pdf/2601.02751", "abs": "https://arxiv.org/abs/2601.02751", "authors": ["Yuetian Chen", "Yuntao Du", "Kaiyuan Zhang", "Ashish Kundu", "Charles Fleming", "Bruno Ribeiro", "Ninghui Li"], "title": "Window-based Membership Inference Attacks Against Fine-tuned Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "Code is available at [https://github.com/Stry233/WBC/](https://github.com/Stry233/WBC/). This arXiv version corresponds to the accepted paper and includes the full experimental results", "summary": "Most membership inference attacks (MIAs) against Large Language Models (LLMs) rely on global signals, like average loss, to identify training data. This approach, however, dilutes the subtle, localized signals of memorization, reducing attack effectiveness. We challenge this global-averaging paradigm, positing that membership signals are more pronounced within localized contexts. We introduce WBC (Window-Based Comparison), which exploits this insight through a sliding window approach with sign-based aggregation. Our method slides windows of varying sizes across text sequences, with each window casting a binary vote on membership based on loss comparisons between target and reference models. By ensembling votes across geometrically spaced window sizes, we capture memorization patterns from token-level artifacts to phrase-level structures. Extensive experiments across eleven datasets demonstrate that WBC substantially outperforms established baselines, achieving higher AUC scores and 2-3 times improvements in detection rates at low false positive thresholds. Our findings reveal that aggregating localized evidence is fundamentally more effective than global averaging, exposing critical privacy vulnerabilities in fine-tuned LLMs.", "AI": {"tldr": "WBC是一种基于滑动窗口的成员推理攻击方法，通过局部损失比较来检测语言模型的训练数据成员，相比传统的全局平均方法显著提高了攻击效果。", "motivation": "现有的大语言模型成员推理攻击主要依赖全局信号（如平均损失），这会稀释记忆化的局部细微信号，降低攻击效果。作者认为成员信号在局部上下文中更加明显。", "method": "提出WBC（基于窗口的比较）方法：使用滑动窗口在不同大小的文本序列上移动，每个窗口基于目标模型和参考模型的损失比较进行二值投票，通过几何间隔的窗口大小集成投票来捕获从词元级到短语级的记忆化模式。", "result": "在11个数据集上的实验表明，WBC显著优于现有基线方法，获得了更高的AUC分数，在低误报率阈值下检测率提高了2-3倍。", "conclusion": "聚合局部证据比全局平均方法更有效，揭示了微调大语言模型中的关键隐私漏洞。"}}
{"id": "2601.02752", "pdf": "https://arxiv.org/pdf/2601.02752", "abs": "https://arxiv.org/abs/2601.02752", "authors": ["Kaiyan Zhao", "Zijie Meng", "Zheyong Xie", "Jin Duan", "Yao Hu", "Zuozhu Liu", "Shaosheng Cao"], "title": "EComStage: Stage-wise and Orientation-specific Benchmarking for Large Language Models in E-commerce", "categories": ["cs.CL"], "comment": "preprint", "summary": "Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management. Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making. To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision). EComStage evaluates LLMs through seven separate representative tasks spanning diverse e-commerce scenarios, with all samples human-annotated and quality-checked. Unlike prior benchmarks that focus only on customer-oriented interactions, EComStage also evaluates merchant-oriented scenarios, including promotion management, content review, and operational support relevant to real-world applications. We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation-specific strengths and weaknesses. Our results provide fine-grained, actionable insights for designing and optimizing LLM-based agents in real-world e-commerce settings.", "AI": {"tldr": "提出了EComStage基准，用于评估LLM在电商场景中的分阶段推理能力（感知、规划、执行），涵盖7个任务和30+模型，提供细粒度分析。", "motivation": "现有基准只关注最终任务完成情况，忽略了中间推理阶段对有效决策的关键作用，需要更全面的评估框架。", "method": "构建统一基准EComStage，通过人工标注的样本评估LLM在感知、规划、行动三个阶段的性能，涵盖客户导向和商家导向的7个电商任务。", "result": "评估了30多个LLM（1B-200B参数），揭示了不同阶段和场景下的优劣势，开源和闭源模型表现各异。", "conclusion": "EComStage为实际电商应用中LLM代理的设计和优化提供了细粒度、可操作的见解，填补了现有评估空白。"}}
{"id": "2601.02780", "pdf": "https://arxiv.org/pdf/2601.02780", "abs": "https://arxiv.org/abs/2601.02780", "authors": ["Bangjun Xiao", "Bingquan Xia", "Bo Yang", "Bofei Gao", "Bowen Shen", "Chen Zhang", "Chenhong He", "Chiheng Lou", "Fuli Luo", "Gang Wang", "Gang Xie", "Hailin Zhang", "Hanglong Lv", "Hanyu Li", "Heyu Chen", "Hongshen Xu", "Houbin Zhang", "Huaqiu Liu", "Jiangshan Duo", "Jianyu Wei", "Jiebao Xiao", "Jinhao Dong", "Jun Shi", "Junhao Hu", "Kainan Bao", "Kang Zhou", "Lei Li", "Liang Zhao", "Linghao Zhang", "Peidian Li", "Qianli Chen", "Shaohui Liu", "Shihua Yu", "Shijie Cao", "Shimao Chen", "Shouqiu Yu", "Shuo Liu", "Tianling Zhou", "Weijiang Su", "Weikun Wang", "Wenhan Ma", "Xiangwei Deng", "Bohan Mao", "Bowen Ye", "Can Cai", "Chenghua Wang", "Chengxuan Zhu", "Chong Ma", "Chun Chen", "Chunan Li", "Dawei Zhu", "Deshan Xiao", "Dong Zhang", "Duo Zhang", "Fangyue Liu", "Feiyu Yang", "Fengyuan Shi", "Guoan Wang", "Hao Tian", "Hao Wu", "Heng Qu", "Hongfei Yi", "Hongxu An", "Hongyi Guan", "Xing Zhang", "Yifan Song", "Yihan Yan", "Yihao Zhao", "Yingchun Lai", "Yizhao Gao", "Yu Cheng", "Yuanyuan Tian", "Yudong Wang", "Zhen Tang", "Zhengju Tang", "Zhengtao Wen", "Zhichao Song", "Zhixian Zheng", "Zihan Jiang", "Jian Wen", "Jiarui Sun", "Jiawei Li", "Jinlong Xue", "Jun Xia", "Kai Fang", "Menghang Zhu", "Nuo Chen", "Qian Tu", "Qihao Zhang", "Qiying Wang", "Rang Li", "Rui Ma", "Shaolei Zhang", "Shengfan Wang", "Shicheng Li", "Shuhao Gu", "Shuhuai Ren", "Sirui Deng", "Tao Guo", "Tianyang Lu", "Weiji Zhuang", "Weikang Zhang", "Weimin Xiong", "Wenshan Huang", "Wenyu Yang", "Xin Zhang", "Xing Yong", "Xu Wang", "Xueyang Xie", "Yilin Jiang", "Yixin Yang", "Yongzhe He", "Yu Tu", "Yuanliang Dong", "Yuchen Liu", "Yue Ma", "Yue Yu", "Yuxing Xiang", "Zhaojun Huang", "Zhenru Lin", "Zhipeng Xu", "Zhiyang Chen", "Zhonghua Deng", "Zihan Zhang", "Zihao Yue"], "title": "MiMo-V2-Flash Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, technical report", "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.", "AI": {"tldr": "MiMo-V2-Flash是一个309B参数的MoE模型，采用混合注意力架构和MTP预训练，通过创新蒸馏方法实现高效推理能力，在参数效率和解码速度上表现优异。", "motivation": "开发一个参数效率高、推理能力强、解码速度快的开源大模型，以媲美当前顶级开源模型但使用更少的参数。", "method": "采用混合专家架构（15B激活参数），结合滑动窗口注意力和全局注意力（5:1比例），使用27T token进行多token预测预训练，并引入多教师策略蒸馏方法。", "result": "模型性能媲美DeepSeek-V3.2和Kimi-K2等顶级模型，但仅使用其1/2和1/3的参数；通过推测解码实现3.6接受长度和2.6倍解码加速。", "conclusion": "MiMo-V2-Flash展示了通过架构创新和蒸馏技术实现高效参数利用的可能性，为开源社区提供了强大的基础模型和加速推理方案。"}}
{"id": "2601.02819", "pdf": "https://arxiv.org/pdf/2601.02819", "abs": "https://arxiv.org/abs/2601.02819", "authors": ["Junxiang Qiu", "Shuo Wang", "Zhengsu Chen", "Hengheng Zhang", "Jinda Lu", "Changcheng Li", "Qi Tian"], "title": "Punctuation-aware Hybrid Trainable Sparse Attention for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Attention serves as the fundamental mechanism for long-context modeling in large language models (LLMs), yet dense attention becomes structurally prohibitive for long sequences due to its quadratic complexity. Consequently, sparse attention has received increasing attention as a scalable alternative. However, existing sparse attention methods rely on coarse-grained semantic representations during block selection, which blur intra-block semantic boundaries and lead to the loss of critical information. To address this issue, we propose \\textbf{P}unctuation-aware \\textbf{H}ybrid \\textbf{S}parse \\textbf{A}ttention \\textbf{(PHSA)}, a natively trainable sparse attention framework that leverages punctuation tokens as semantic boundary anchors. Specifically, (1) we design a dual-branch aggregation mechanism that fuses global semantic representations with punctuation-enhanced boundary features, preserving the core semantic structure while introducing almost no additional computational overhead; (2) we introduce an extreme-sparsity-adaptive training and inference strategy that stabilizes model behavior under very low token activation ratios; Extensive experiments on general benchmarks and long-context evaluations demonstrate that PHSA consistently outperforms dense attention and state-of-the-art sparse attention baselines, including InfLLM v2. Specifically, for the 0.6B-parameter model with 32k-token input sequences, PHSA can reduce the information loss by 10.8\\% at a sparsity ratio of 97.3\\%.", "AI": {"tldr": "PHSA是一种基于标点符号感知的混合稀疏注意力机制，通过利用标点作为语义边界锚点，在保持高稀疏度（97.3%）的同时减少信息损失10.8%，优于密集注意力和现有稀疏注意力方法。", "motivation": "现有稀疏注意力方法在块选择时使用粗粒度语义表示，模糊了块内语义边界并导致关键信息丢失，需要更精细的语义边界处理机制。", "method": "提出PHSA框架：1）设计双分支聚合机制融合全局语义表示和标点增强边界特征；2）引入极端稀疏自适应训练推理策略稳定模型在极低token激活率下的行为。", "result": "在通用基准和长文本评估中，PHSA始终优于密集注意力和最先进的稀疏注意力基线（包括InfLLM v2），32k token输入下在97.3%稀疏比时信息损失减少10.8%。", "conclusion": "PHSA通过标点符号作为语义边界锚点，有效解决了稀疏注意力中的信息丢失问题，为长序列建模提供了高效可扩展的解决方案，几乎不增加计算开销。"}}
{"id": "2601.02830", "pdf": "https://arxiv.org/pdf/2601.02830", "abs": "https://arxiv.org/abs/2601.02830", "authors": ["Feiyan Liu", "Chenxun Zhuo", "Siyan Zhao", "Bao Ge", "Tianming Liu"], "title": "The performances of the Chinese and U.S. Large Language Models on the Topic of Chinese Culture", "categories": ["cs.CL"], "comment": null, "summary": "Cultural backgrounds shape individuals' perspectives and approaches to problem-solving. Since the emergence of GPT-1 in 2018, large language models (LLMs) have undergone rapid development. To date, the world's ten leading LLM developers are primarily based in China and the United States. To examine whether LLMs released by Chinese and U.S. developers exhibit cultural differences in Chinese-language settings, we evaluate their performance on questions about Chinese culture. This study adopts a direct-questioning paradigm to evaluate models such as GPT-5.1, DeepSeek-V3.2, Qwen3-Max, and Gemini2.5Pro. We assess their understanding of traditional Chinese culture, including history, literature, poetry, and related domains. Comparative analyses between LLMs developed in China and the U.S. indicate that Chinese models generally outperform their U.S. counterparts on these tasks. Among U.S.-developed models, Gemini 2.5Pro and GPT-5.1 achieve relatively higher accuracy. The observed performance differences may potentially arise from variations in training data distribution, localization strategies, and the degree of emphasis on Chinese cultural content during model development.", "AI": {"tldr": "该研究比较了中美两国开发的大语言模型在中文文化相关任务上的表现差异，发现中国开发的模型在理解中国传统文化方面普遍优于美国模型。", "motivation": "文化背景影响问题解决方式，而中美是世界主要LLM开发者，研究旨在探索两国模型在中文文化理解上是否存在文化差异。", "method": "采用直接提问范式，评估GPT-5.1、DeepSeek-V3.2、Qwen3-Max和Gemini2.5Pro等模型对中国传统文化（历史、文学、诗歌等）的理解能力。", "result": "中国开发的模型在这些任务上表现优于美国模型。在美国模型中，Gemini 2.5Pro和GPT-5.1准确率相对较高。", "conclusion": "性能差异可能源于训练数据分布、本地化策略以及模型开发过程中对中国文化内容的重视程度不同。"}}
{"id": "2601.02845", "pdf": "https://arxiv.org/pdf/2601.02845", "abs": "https://arxiv.org/abs/2601.02845", "authors": ["Kai Li", "Xuanqing Yu", "Ziyi Ni", "Yi Zeng", "Yao Xu", "Zheqing Zhang", "Xin Li", "Jitao Sang", "Xiaogang Duan", "Xuelei Wang", "Chengbao Liu", "Jie Tan"], "title": "TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.", "AI": {"tldr": "TiMem是一个时序-层次化记忆框架，通过时间记忆树组织对话，实现从原始观察到抽象人格表示的系统化记忆整合，在长时对话任务中达到最先进性能。", "motivation": "现有记忆框架对跨层次时序结构化信息的支持有限，导致记忆碎片化和长时个性化不稳定，需要更好的长时对话记忆管理方案。", "method": "提出Temporal Memory Tree (TMT)结构，采用语义引导的记忆整合和无微调跨层次集成，结合复杂度感知的记忆召回机制。", "result": "在LoCoMo和LongMemEval-S基准测试中分别达到75.30%和76.88%的准确率，优于所有基线方法，记忆召回长度减少52.20%。", "conclusion": "TiMem将时序连续性作为对话代理长时记忆的首要组织原则，有效解决了长时对话中的记忆管理挑战。"}}
{"id": "2601.02858", "pdf": "https://arxiv.org/pdf/2601.02858", "abs": "https://arxiv.org/abs/2601.02858", "authors": ["Saurabh Kumar Pandey", "Sougata Saha", "Monojit Choudhury"], "title": "To Generate or Discriminate? Methodological Considerations for Measuring Cultural Alignment in LLMs", "categories": ["cs.CL"], "comment": "IJCNLP-AACL 2025", "summary": "Socio-demographic prompting (SDP) - prompting Large Language Models (LLMs) using demographic proxies to generate culturally aligned outputs - often shows LLM responses as stereotypical and biased. While effective in assessing LLMs' cultural competency, SDP is prone to confounding factors such as prompt sensitivity, decoding parameters, and the inherent difficulty of generation over discrimination tasks due to larger output spaces. These factors complicate interpretation, making it difficult to determine if the poor performance is due to bias or the task design. To address this, we use inverse socio-demographic prompting (ISDP), where we prompt LLMs to discriminate and predict the demographic proxy from actual and simulated user behavior from different users. We use the Goodreads-CSI dataset (Saha et al., 2025), which captures difficulty in understanding English book reviews for users from India, Mexico, and the USA, and test four LLMs: Aya-23, Gemma-2, GPT-4o, and LLaMA-3.1 with ISDP. Results show that models perform better with actual behaviors than simulated ones, contrary to what SDP suggests. However, performance with both behavior types diminishes and becomes nearly equal at the individual level, indicating limits to personalization.", "AI": {"tldr": "该论文提出逆社会人口统计提示(ISDP)方法，通过让大语言模型从用户行为中预测人口统计特征，来更准确地评估模型的文化能力，避免了传统SDP方法因生成任务复杂性带来的混淆因素。", "motivation": "传统的社会人口统计提示(SDP)方法在评估大语言模型文化能力时存在混淆因素，如提示敏感性、解码参数和生成任务的复杂性，使得难以区分性能不佳是由于模型偏见还是任务设计问题。", "method": "使用逆社会人口统计提示(ISDP)，让大语言模型从真实和模拟的用户行为中识别和预测人口统计特征。使用Goodreads-CSI数据集测试Aya-23、Gemma-2、GPT-4o和LLaMA-3.1四个模型。", "result": "模型在真实行为上的表现优于模拟行为，与传统SDP结果相反。但在个体层面上，两种行为类型的性能都下降且趋于相等，表明个性化存在局限性。", "conclusion": "ISDP方法提供了更可靠的模型文化能力评估方式，揭示了模型在个体层面个性化能力的限制，为更准确的文化对齐研究提供了新视角。"}}
{"id": "2601.02867", "pdf": "https://arxiv.org/pdf/2601.02867", "abs": "https://arxiv.org/abs/2601.02867", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "title": "Training Language Models with homotokens Leads to Delayed Overfitting", "categories": ["cs.CL"], "comment": "8 pages, 6 figures, 3 Appendices", "summary": "Subword tokenization introduces a computational layer in language models where many distinct token sequences decode to the same surface form and preserve meaning, yet induce different internal computations. Despite this non-uniqueness, language models are typically trained using a single canonical longest-prefix tokenization. We formalize homotokens-alternative valid subword segmentations of the same lexical item-as a strictly meaning-preserving form of data augmentation. We introduce a lightweight training architecture that conditions canonical next-token prediction on sampled homotoken variants via an auxiliary causal encoder and block-causal cross-attention, without modifying the training objective or token interface. In data-constrained pretraining, homotoken augmentation consistently delays overfitting under repeated data exposure and improves generalization across diverse evaluation datasets. In multilingual fine-tuning, we find that the effectiveness of homotokens depends on tokenizer quality: gains are strongest when canonical tokens are highly compressed and diminish when the tokenizer already over-fragments the input. Overall, homotokens provide a simple and modular mechanism for inducing tokenization invariance in language models.", "AI": {"tldr": "论文提出了一种称为\"同义词元\"的数据增强方法，通过使用不同的有效子词分割来训练语言模型，提高模型的泛化能力和抗过拟合能力。", "motivation": "当前语言模型通常使用单一的标准最长前缀分词方式，但同一表面形式可以有多种不同的子词分割方式，这些不同的分词序列会引发不同的内部计算，而模型训练时没有充分利用这种非唯一性。", "method": "提出轻量级训练架构，通过辅助因果编码器和块因果交叉注意力，在保持标准下一个词预测目标不变的情况下，对采样的同义词元变体进行条件化处理。", "result": "在数据受限的预训练中，同义词元增强能持续延迟重复数据暴露下的过拟合，并在多样化评估数据集上改善泛化能力。在多语言微调中，效果取决于分词器质量。", "conclusion": "同义词元为在语言模型中引入分词不变性提供了一种简单且模块化的机制，特别在标准分词高度压缩时效果最佳。"}}
{"id": "2601.02872", "pdf": "https://arxiv.org/pdf/2601.02872", "abs": "https://arxiv.org/abs/2601.02872", "authors": ["Ziyang Chen", "Xing Wu", "Junlong Jia", "Chaochen Gao", "Qi Fu", "Debing Zhang", "Songlin Hu"], "title": "LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the \"thinking\" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding.", "AI": {"tldr": "LongBench Pro是一个新的双语长上下文基准测试，包含1500个自然长文本样本，覆盖11个主要任务和25个次要任务，支持8k到256k token的输入长度，通过人机协作构建方式平衡质量和可扩展性。", "motivation": "当前长上下文基准测试在可扩展性和真实性之间存在权衡：合成任务无法代表真实世界的复杂性，而完全人工标注成本高昂，难以扩展到极长文本和多样化场景。", "method": "提出人机协作构建流程：前沿LLM生成具有挑战性的问题和参考答案，专家进行严格验证和精炼。包含任务特定指标和多维度分类体系（上下文需求、长度、难度）。", "result": "评估46个长上下文LLM发现：(1)长上下文优化比参数扩展更重要；(2)有效上下文长度通常短于声称长度，存在跨语言不对齐；(3)思考范式主要帮助原生推理训练的模型，混合思考设计提供帕累托权衡。", "conclusion": "LongBench Pro为推进长上下文理解提供了强大的测试平台，揭示了当前模型在长上下文处理中的关键局限性和改进方向。"}}
{"id": "2601.02875", "pdf": "https://arxiv.org/pdf/2601.02875", "abs": "https://arxiv.org/abs/2601.02875", "authors": ["Chen-Han Tsai"], "title": "Revisiting Data Compression with Language Modeling", "categories": ["cs.CL"], "comment": "Preprint", "summary": "In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.", "AI": {"tldr": "该论文探索使用大型语言模型(LLM)进行数据压缩，在enwik9数据集上实现了18%的调整压缩率新SOTA，无需额外模型训练，并验证了LLM在多语言、代码和字节流序列压缩中的竞争力。", "motivation": "虽然已有研究显示LLM在文本和多模态数据压缩方面表现良好，但仍存在实际挑战阻碍其替代传统压缩算法，本研究旨在探索降低调整压缩率的方法。", "method": "使用LLM作为数据压缩器，探索不同方法来实现更低的调整压缩率，重点研究无需额外模型训练的配置方式。", "result": "在enwik9数据集上实现了约18%的调整压缩率，达到新的最先进水平；证明LLM在非英语数据、代码数据和字节流序列压缩中仍具有竞争力。", "conclusion": "LLM在文本主导领域的数据压缩表现出色，通过正确配置也能在非自然文本序列压缩中保持竞争力，为LLM替代传统压缩算法提供了新的可能性。"}}
{"id": "2601.02891", "pdf": "https://arxiv.org/pdf/2601.02891", "abs": "https://arxiv.org/abs/2601.02891", "authors": ["Bach Phan-Tat", "Kris Heylen", "Dirk Geeraerts", "Stefano De Pascale", "Dirk Speelman"], "title": "Transparent Semantic Change Detection with Dependency-Based Profiles", "categories": ["cs.CL"], "comment": null, "summary": "Most modern computational approaches to lexical semantic change detection (LSC) rely on embedding-based distributional word representations with neural networks. Despite the strong performance on LSC benchmarks, they are often opaque. We investigate an alternative method which relies purely on dependency co-occurrence patterns of words. We demonstrate that it is effective for semantic change detection and even outperforms a number of distributional semantic models. We provide an in-depth quantitative and qualitative analysis of the predictions, showing that they are plausible and interpretable.", "AI": {"tldr": "本文提出了一种基于依赖共现模式的语义变化检测方法，该方法在性能上优于多种分布式语义模型，且具有更好的可解释性", "motivation": "虽然基于神经网络的嵌入方法在语义变化检测基准上表现良好，但这些方法往往缺乏透明度，需要更可解释的替代方案", "method": "使用纯粹的依赖共现模式来分析词汇的语义变化，不依赖于神经网络嵌入", "result": "该方法在语义变化检测方面表现有效，甚至优于多种分布式语义模型，预测结果合理且可解释", "conclusion": "依赖共现模式为语义变化检测提供了一种有效且可解释的替代方法，能够产生合理且透明的预测结果"}}
{"id": "2601.02906", "pdf": "https://arxiv.org/pdf/2601.02906", "abs": "https://arxiv.org/abs/2601.02906", "authors": ["Ryan Soh-Eun Shim", "Kwanghee Choi", "Kalvin Chang", "Ming-Hao Hsu", "Florian Eichin", "Zhizheng Wu", "Alane Suhr", "Michael A. Hedderich", "David Harwath", "David R. Mortensen", "Barbara Plank"], "title": "Linear Script Representations in Speech Foundation Models Enable Zero-Shot Transliteration", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual speech foundation models such as Whisper are trained on web-scale data, where data for each language consists of a myriad of regional varieties. However, different regional varieties often employ different scripts to write the same language, rendering speech recognition output also subject to non-determinism in the output script. To mitigate this problem, we show that script is linearly encoded in the activation space of multilingual speech models, and that modifying activations at inference time enables direct control over output script. We find the addition of such script vectors to activations at test time can induce script change even in unconventional language-script pairings (e.g. Italian in Cyrillic and Japanese in Latin script). We apply this approach to inducing post-hoc control over the script of speech recognition output, where we observe competitive performance across all model sizes of Whisper.", "AI": {"tldr": "该论文发现多语言语音模型的激活空间中线性编码了文字系统信息，通过在推理时修改激活向量可以控制语音识别输出的文字系统，甚至实现非常规的语言-文字组合。", "motivation": "多语言语音模型在不同地区变体中使用的文字系统不同，导致语音识别输出存在文字系统的不确定性，需要解决这一问题。", "method": "研究发现文字系统在多语言语音模型的激活空间中呈线性编码，通过在推理时向激活添加特定的文字向量来控制输出文字系统。", "result": "该方法能够在非常规的语言-文字组合中成功诱导文字系统转换，在不同规模的Whisper模型上都表现出竞争力。", "conclusion": "通过修改模型激活空间的文字向量，可以实现对语音识别输出文字系统的后验控制，为解决多语言语音识别中的文字系统不确定性提供了有效方法。"}}
{"id": "2601.02907", "pdf": "https://arxiv.org/pdf/2601.02907", "abs": "https://arxiv.org/abs/2601.02907", "authors": ["Zeyu Gan", "Ruifeng Ren", "Wei Yao", "Xiaolin Hu", "Gengze Xu", "Chen Qian", "Huayi Tang", "Zixuan Gong", "Xinhao Yao", "Pengwei Tang", "Zhenxing Dou", "Yong Liu"], "title": "Beyond the Black Box: Theory and Mechanism of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The rapid emergence of Large Language Models (LLMs) has precipitated a profound paradigm shift in Artificial Intelligence, delivering monumental engineering successes that increasingly impact modern society. However, a critical paradox persists within the current field: despite the empirical efficacy, our theoretical understanding of LLMs remains disproportionately nascent, forcing these systems to be treated largely as ``black boxes''. To address this theoretical fragmentation, this survey proposes a unified lifecycle-based taxonomy that organizes the research landscape into six distinct stages: Data Preparation, Model Preparation, Training, Alignment, Inference, and Evaluation. Within this framework, we provide a systematic review of the foundational theories and internal mechanisms driving LLM performance. Specifically, we analyze core theoretical issues such as the mathematical justification for data mixtures, the representational limits of various architectures, and the optimization dynamics of alignment algorithms. Moving beyond current best practices, we identify critical frontier challenges, including the theoretical limits of synthetic data self-improvement, the mathematical bounds of safety guarantees, and the mechanistic origins of emergent intelligence. By connecting empirical observations with rigorous scientific inquiry, this work provides a structured roadmap for transitioning LLM development from engineering heuristics toward a principled scientific discipline.", "AI": {"tldr": "这篇论文提出了一个基于生命周期的统一分类法，将大语言模型研究分为六个阶段，系统梳理了理论基础和内部机制，为LLM开发从工程启发式向科学原理化转型提供了路线图。", "motivation": "当前LLM领域存在一个关键悖论：尽管实证效果显著，但理论理解仍很初级，这些系统很大程度上被视为'黑箱'，需要解决这种理论碎片化问题。", "method": "提出了一个基于生命周期的分类法，将LLM研究分为数据准备、模型准备、训练、对齐、推理和评估六个阶段，并系统回顾了各阶段的基础理论和内部机制。", "result": "分析了核心理论问题如数据混合的数学论证、不同架构的表征极限、对齐算法的优化动态等，并识别了前沿挑战包括合成数据自改进的理论极限、安全保证的数学边界等。", "conclusion": "通过将实证观察与严谨科学探究相结合，这项工作为LLM开发从工程启发式向原则性科学学科转型提供了结构化路线图。"}}
{"id": "2601.02911", "pdf": "https://arxiv.org/pdf/2601.02911", "abs": "https://arxiv.org/abs/2601.02911", "authors": ["Hyoyeon Lee", "Seth Bullock", "Conor Houghton"], "title": "Image, Word and Thought: A More Challenging Language Task for the Iterated Learning Model", "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": "This is an extended version of a paper accepted for EvoLang2026, it includes additional details of the numerical experiments", "summary": "The iterated learning model simulates the transmission of language from generation to generation in order to explore how the constraints imposed by language transmission facilitate the emergence of language structure. Despite each modelled language learner starting from a blank slate, the presence of a bottleneck limiting the number of utterances to which the learner is exposed can lead to the emergence of language that lacks ambiguity, is governed by grammatical rules, and is consistent over successive generations, that is, one that is expressive, compositional and stable. The recent introduction of a more computationally tractable and ecologically valid semi supervised iterated learning model, combining supervised and unsupervised learning within an autoencoder architecture, has enabled exploration of language transmission dynamics for much larger meaning-signal spaces. Here, for the first time, the model has been successfully applied to a language learning task involving the communication of much more complex meanings: seven-segment display images. Agents in this model are able to learn and transmit a language that is expressive: distinct codes are employed for all 128 glyphs; compositional: signal components consistently map to meaning components, and stable: the language does not change from generation to generation.", "AI": {"tldr": "迭代学习模型通过模拟语言代际传播，证明即使学习者从零开始，通过限制接触的语料数量（瓶颈效应），能够自发形成无歧义、有语法规则且稳定的语言结构。新提出的半监督迭代学习模型成功应用于更复杂的七段显示器图像通信任务，实现了表达性、组合性和稳定性的语言特征。", "motivation": "探索语言传播过程中的约束条件如何促进语言结构的涌现，特别是通过计算上更易处理且生态效度更高的模型来研究更复杂含义的传递。", "method": "采用半监督迭代学习模型，结合监督学习和无监督学习，在自编码器架构中实现。模型应用于七段显示器图像的语言学习任务，测试128种不同符号的通信能力。", "result": "代理能够学习并传播一种表达性语言（所有128个符号都有独特编码）、组合性语言（信号组件一致映射到含义组件）和稳定性语言（代际间保持不变）。", "conclusion": "迭代学习模型有效证明了语言传播瓶颈在促进行为结构化、无歧义和稳定语言系统形成中的关键作用，为理解人类语言进化提供了计算模型支持。"}}
{"id": "2601.02917", "pdf": "https://arxiv.org/pdf/2601.02917", "abs": "https://arxiv.org/abs/2601.02917", "authors": ["Mengze Hong", "Di Jiang", "Jiangtao Wen", "Zhiyang Su", "Yawen Li", "Yanjie Sun", "Guan Wang", "Chen Jason Zhang"], "title": "RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the \"wisdom of the crowd\" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.", "AI": {"tldr": "RAL2M框架通过将LLM重新定位为检索系统中的查询-响应匹配判断器，消除生成幻觉，并提出查询自适应潜在集成策略来减轻判断幻觉。", "motivation": "LLM驱动的服务系统中幻觉是一个主要问题，需要显式知识基础来保证合规性响应。", "method": "提出检索增强学习匹配(RAL2M)框架，将LLM作为查询-响应匹配判断器；采用查询自适应潜在集成策略，显式建模异构模型能力和LLM间的相互依赖关系。", "result": "在大规模基准测试中，该方法有效利用了\"群体智慧\"，显著优于强基线方法。", "conclusion": "该方法为纯生成方法提供了稳健替代方案，并讨论了在将来工作中进一步利用潜在表示的最佳实践和前景方向。"}}
{"id": "2601.02931", "pdf": "https://arxiv.org/pdf/2601.02931", "abs": "https://arxiv.org/abs/2601.02931", "authors": ["Yihua Zhu", "Qianying Liu", "Jiaxin Wang", "Fei Cheng", "Chaoran Liu", "Akiko Aizawa", "Sadao Kurohashi", "Hidetoshi Shimodaira"], "title": "Memorization, Emergence, and Explaining Reversal Failures: A Controlled Study of Relational Semantics in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive LLMs perform well on relational tasks that require linking entities via relational words (e.g., father/son, friend), but it is unclear whether they learn the logical semantics of such relations (e.g., symmetry and inversion logic) and, if so, whether reversal-type failures arise from missing relational semantics or left-to-right order bias. We propose a controlled Knowledge Graph-based synthetic framework that generates text from symmetric/inverse triples, train GPT-style autoregressive models from scratch, and evaluate memorization, logical inference, and in-context generalization to unseen entities to address these questions. We find a sharp phase transition in which relational semantics emerge with sufficient logic-bearing supervision, even in shallow (2-3 layer) models, and that successful generalization aligns with stable intermediate-layer signals. Finally, order-matched forward/reverse tests and a diffusion baseline indicate that reversal failures are primarily driven by autoregressive order bias rather than deficient inversion semantics.", "AI": {"tldr": "该研究通过知识图谱合成框架发现，自回归LLMs在足够逻辑监督下能够学习关系语义，反转失败主要源于自回归顺序偏差而非语义缺陷", "motivation": "探究自回归LLMs是否真正学习关系词（如父子、朋友）的逻辑语义（如对称性和逆逻辑），以及反转类型失败是由于缺失关系语义还是从左到右的顺序偏差", "method": "提出基于知识图谱的合成框架生成对称/逆三元组文本，从头训练GPT风格自回归模型，评估记忆能力、逻辑推理和对未见实体的上下文泛化", "result": "发现关系语义在足够逻辑监督下出现明显的相变现象，即使在浅层（2-3层）模型中也能实现，成功泛化与稳定的中间层信号对齐", "conclusion": "反转失败主要由自回归顺序偏差驱动，而非逆语义缺陷，表明模型确实能够学习关系逻辑语义"}}
{"id": "2601.02972", "pdf": "https://arxiv.org/pdf/2601.02972", "abs": "https://arxiv.org/abs/2601.02972", "authors": ["Nathanaël Carraz Rakotonirina", "Ren Pang", "Neha Anna John", "Michael Bohlke-Schneider", "Momchil Hardalov"], "title": "Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\\% for 8B models and 40\\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\\text{AUC}_{\\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.", "AI": {"tldr": "提出一种多阶段高效推理方法，通过监督微调和强化学习结合自适应长度惩罚，显著减少大语言模型推理时的token数量，在保持精度的同时平均减少28-40%的响应长度。", "motivation": "解决大语言模型在链式推理(CoT)过程中产生的'过度思考'问题，即生成长度不必要的中间token，增加计算成本却不一定提升精度甚至可能降低性能。", "method": "结合监督微调（通过拒绝采样或推理轨迹重新格式化）和强化学习，使用自适应长度惩罚的轻量级奖励函数，惩罚第一个正确答案之后生成的token，仅在有益时鼓励自我验证。", "result": "在7个不同推理任务上平均减少8B模型28%和32B模型40%的响应长度，性能仅下降1.6和2.5个百分点，在Overthinking-Adjusted Accuracy曲线上得分76.6，比基准模型高5分，比次优方法高2.5分。", "conclusion": "该方法概念简单但效果显著，在精度-响应长度权衡方面优于更复杂的现有高效推理方法，实现了更好的计算效率与性能平衡。"}}
{"id": "2601.02933", "pdf": "https://arxiv.org/pdf/2601.02933", "abs": "https://arxiv.org/abs/2601.02933", "authors": ["Vilém Zouhar", "Tom Kocmi"], "title": "Pearmut: Human Evaluation of Translation Made Trivial", "categories": ["cs.CL", "cs.HC"], "comment": "typeset with Typst", "summary": "Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.", "AI": {"tldr": "Pearmut是一个轻量级但功能丰富的人类评估平台，旨在使多语言NLP评估像自动评估一样简单易行，特别专注于机器翻译任务。", "motivation": "人类评估是多语言NLP的黄金标准，但由于现有工具设置复杂、工程和操作开销大，实践中常被自动指标替代。", "method": "平台实现了标准评估协议（DA、ESA、MQM），支持文档级上下文、绝对和对比评估、注意力检查、ESAAI预标注，以及静态和主动学习分配策略。", "result": "Pearmut移除了常见的使用障碍，支持多语言任务评估，并可扩展支持新协议原型开发。", "conclusion": "该平台使可靠的人类评估成为模型开发和诊断的实用常规组件，而不再是偶尔进行的努力。"}}
{"id": "2601.02978", "pdf": "https://arxiv.org/pdf/2601.02978", "abs": "https://arxiv.org/abs/2601.02978", "authors": ["Ruikang Zhang", "Shuo Wang", "Qi Su"], "title": "Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors. Our method employs a contrastive feature retrieval pipeline based on controlled semantic oppositions, combing statistical activation analysis and generation-based validation to distill monosemantic functional features from sparse activation spaces. Using the Big Five personality traits as a case study, we demonstrate that our method enables precise, bidirectional steering of model behavior while maintaining superior stability and performance compared to existing activation steering methods like Contrastive Activation Addition (CAA). We further identify an empirical effect, which we term Functional Faithfulness, whereby intervening on a specific internal feature induces coherent and predictable shifts across multiple linguistic dimensions aligned with the target semantic attribute. Our findings suggest that LLMs internalize deeply integrated representations of high-order concepts, and provide a novel, robust mechanistic path for the regulation of complex AI behaviors.", "AI": {"tldr": "本文提出了一种基于稀疏自编码器的框架，用于检索和控制与高级语言行为相关的可解释内部特征，通过对比特征检索和生成验证实现模型行为的精确双向控制。", "motivation": "当前机械可解释性研究虽然能识别大语言模型的内部特征，但难以将这些特征与复杂行为层面的语义属性可靠控制联系起来。", "method": "采用基于对比语义对立的特征检索流程，结合统计激活分析和基于生成的验证，从稀疏激活空间中提取单义功能特征。以五大人格特质为案例进行研究。", "result": "方法能够实现精确的双向行为控制，相比现有方法（如CAA）具有更好的稳定性和性能，并发现了\"功能忠实性\"效应。", "conclusion": "大语言模型内化了高阶概念的深度集成表征，为复杂AI行为调控提供了新颖而稳健的机械路径。"}}
{"id": "2601.02956", "pdf": "https://arxiv.org/pdf/2601.02956", "abs": "https://arxiv.org/abs/2601.02956", "authors": ["Jeonghyun Park", "Byeongjeong Kim", "Seojin Hwang", "Hwanhee Lee"], "title": "Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion", "categories": ["cs.CL"], "comment": "20 pages, 5 figures, 15 tables", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.", "AI": {"tldr": "该论文揭示了多语言检索增强生成系统对英语的偏好主要是评估基准的结构性偏差所致，而非模型固有偏好，并提出新的去偏度量方法和优化框架", "motivation": "现有mRAG系统表现出对高资源语言（特别是英语）的偏好，但研究发现这种测量结果受到评估基准中结构性先验的严重扭曲", "method": "提出DeLP（去偏语言偏好）度量方法，明确排除结构性混淆因素；基于发现引入DELTA框架，利用单语对齐优化跨语言检索和生成", "result": "使用DeLP分析显示先前报告的英语偏好主要是证据分布的结果；DELTA框架在多种语言上持续优于英语枢轴和mRAG基线方法", "conclusion": "检索器本质上偏好查询和文档语言之间的单语对齐，而非英语偏好；DELTA框架提供了一种轻量高效的mRAG优化方案"}}
{"id": "2601.03014", "pdf": "https://arxiv.org/pdf/2601.03014", "abs": "https://arxiv.org/abs/2601.03014", "authors": ["Junli Liang", "Pengfei Zhou", "Wangqiu Zhou", "Wenjie Qing", "Qi Zhao", "Ziwen Wang", "Qi Song", "Xiangyang Li"], "title": "SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.", "AI": {"tldr": "SentGraph是一个基于句子级图结构的RAG框架，通过显式建模句子间细粒度逻辑关系来解决多跳问答中传统RAG的局限性，在多个基准测试中表现优异。", "motivation": "传统RAG在单跳问答中表现良好，但在需要从多文档结合证据的多跳问答中存在显著限制，现有基于分块的检索方法经常提供不相关和逻辑不连贯的上下文，导致证据链不完整和推理错误。", "method": "提出SentGraph框架：1）离线构建分层句子图，采用修辞结构理论区分核心句和卫星句；2）组织成具有跨文档实体桥接的主题级子图；3）在线检索时执行图引导的证据选择和路径扩展来检索细粒度句子级证据。", "result": "在四个多跳问答基准测试上进行了广泛实验，证明了SentGraph的有效性。", "conclusion": "验证了显式建模句子级逻辑依赖关系对于多跳推理的重要性，SentGraph框架能够有效解决多跳问答中的证据检索和推理问题。"}}
{"id": "2601.02957", "pdf": "https://arxiv.org/pdf/2601.02957", "abs": "https://arxiv.org/abs/2601.02957", "authors": ["Fabian Lukassen", "Christoph Weisser", "Michael Schlee", "Manish Kumar", "Anton Thielmann", "Benjamin Saefken", "Thomas Kneib"], "title": "LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.", "AI": {"tldr": "本文提出了一种结合集成统计方法和大型语言模型的时间序列变点检测框架，提高了检测精度和变点解释性。", "motivation": "解决两个关键问题：1) 单个检测方法因数据特性不同而表现互补，方法选择困难且易产生次优结果；2) 缺乏对检测到的变点的自动化、上下文解释。", "method": "集成十种不同的变点检测算法，并使用LLM驱动的解释管道自动生成上下文叙述，将检测到的变点与现实历史事件关联。对于私有数据，采用检索增强生成(RAG)解决方案。", "result": "相比单个方法，集成方法展现出更优的性能和鲁棒性，并能将原始统计输出转化为分析师和决策者可操作的见解。", "conclusion": "该开源Python框架在金融、政治科学和环境科学等多个领域具有实用价值，为时间序列变点检测提供了准确且可解释的解决方案。"}}
{"id": "2601.03018", "pdf": "https://arxiv.org/pdf/2601.03018", "abs": "https://arxiv.org/abs/2601.03018", "authors": ["Choonghan Kim", "Hyunmin Hwang", "Hangeol Chang", "Jaemin Kim", "Jinse Park", "Jae-Sung Lim", "Jong Chul Ye"], "title": "Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5", "AI": {"tldr": "Dementia-R1是一个基于强化学习的框架，用于从非结构化临床笔记中进行纵向痴呆预后预测，通过冷启动RL策略预训练模型预测临床指标，在真实数据集上达到77.03%的F1分数，7B模型性能媲美GPT-4o。", "motivation": "大型语言模型在临床文本理解上表现良好，但在需要跨多次就诊推理复杂非单调症状轨迹的纵向预测任务（如痴呆预后）中表现不佳，标准监督训练缺乏症状演化的显式标注，直接强化学习受到稀疏二元奖励的限制。", "method": "引入Dementia-R1框架，采用冷启动RL策略，预训练模型从患者历史中预测可验证的临床指标，增强疾病进展推理能力，然后再确定最终临床状态。", "result": "在真实世界非结构化临床数据集上达到77.03%的F1分数，在ADNI基准测试中，7B模型性能与GPT-4o相当，有效捕捉波动的认知轨迹。", "conclusion": "Dementia-R1通过创新的冷启动RL方法成功解决了纵向痴呆预后预测的挑战，证明了其在处理复杂症状轨迹方面的有效性，代码已开源。"}}
{"id": "2601.02965", "pdf": "https://arxiv.org/pdf/2601.02965", "abs": "https://arxiv.org/abs/2601.02965", "authors": ["Phat Tran", "Phuoc Pham", "Hung Trinh", "Tho Quan"], "title": "Low-Resource Heuristics for Bahnaric Optical Character Recognition Improvement", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Bahnar, a minority language spoken across Vietnam, Cambodia, and Laos, faces significant preservation challenges due to limited research and data availability. This study addresses the critical need for accurate digitization of Bahnar language documents through optical character recognition (OCR) technology. Digitizing scanned paper documents poses significant challenges, as degraded image quality from broken or blurred areas introduces considerable OCR errors that compromise information retrieval systems. We propose a comprehensive approach combining advanced table and non-table detection techniques with probability-based post-processing heuristics to enhance recognition accuracy. Our method first applies detection algorithms to improve input data quality, then employs probabilistic error correction on OCR output. Experimental results indicate a substantial improvement, with recognition accuracy increasing from 72.86% to 79.26%. This work contributes valuable resources for Bahnar language preservation and provides a framework applicable to other minority language digitization efforts.", "AI": {"tldr": "本研究提出结合表格检测和概率后处理的OCR方法，将巴拿语的识别准确率从72.86%提升至79.26%，为少数民族语言数字化保存提供有效框架。", "motivation": "巴拿语作为越南、柬埔寨和老挝的少数民族语言，面临因研究不足和数据稀缺而濒危的问题。扫描文档图像质量退化导致OCR错误严重，影响信息检索系统的有效性。", "method": "采用先进的表格和非表格检测技术结合概率后处理启发式方法：首先应用检测算法提升输入数据质量，然后对OCR输出进行概率误差校正。", "result": "实验结果显示识别准确率显著提升，从72.86%提高到79.26%。", "conclusion": "本研究为巴拿语保护提供了宝贵资源，所提出的框架可推广应用于其他少数民族语言的数字化工作，有效解决OCR在退化图像上的识别挑战。"}}
{"id": "2601.03043", "pdf": "https://arxiv.org/pdf/2601.03043", "abs": "https://arxiv.org/abs/2601.03043", "authors": ["Junhao Hu", "Fangze Li", "Mingtao Xu", "Feifan Meng", "Shiju Zhao", "Tiancheng Hu", "Ting Peng", "Anmin Liu", "Wenrui Huang", "Chenxu Liu", "Ziyue Hua", "Tao Xie"], "title": "Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.", "AI": {"tldr": "研究发现稀疏注意力机制在解码阶段会因信息损失导致序列变长，反而增加端到端复杂度（Less is Less现象），为此提出早停算法来平衡信息损失与增益，在推理密集型基准测试中减少90%的token消耗且准确率下降小于2%。", "motivation": "大语言模型部署规模扩大对推理效率提出高要求，解码阶段占主导延迟，现有稀疏注意力方法旨在降低复杂度但可能产生反效果", "method": "通过实证和理论分析发现稀疏注意力的信息损失问题，提出早停算法检测稀疏解码过程中信息损失超过信息增益的阈值", "result": "早停算法显著减少token消耗（高达90%），在多个推理密集型基准测试中仅带来微小准确率下降（<2%）", "conclusion": "稀疏注意力需要谨慎使用以避免Less is Less现象，提出的早停算法有效解决了这一问题，实现了效率与准确性的良好平衡"}}
{"id": "2601.02970", "pdf": "https://arxiv.org/pdf/2601.02970", "abs": "https://arxiv.org/abs/2601.02970", "authors": ["Junseok Kim", "Nakyeong Yang", "Kyungmin Min", "Kyomin Jung"], "title": "Reliability-Aware Adaptive Self-Consistency for Efficient Sampling in LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": "15 pages, 8 figures", "summary": "Self-Consistency improves reasoning reliability through multi-sample aggregation, but incurs substantial inference cost. Adaptive self-consistency methods mitigate this issue by adjusting the sampling budget; however, they rely on count-based stopping rules that treat all responses equally, often leading to unnecessary sampling. We propose Reliability-Aware Adaptive Self-Consistency (ReASC), which addresses this limitation by reframing adaptive sampling from response counting to evidence sufficiency, leveraging response-level confidence for principled information aggregation. ReASC operates in two stages: a single-sample decision stage that resolves instances confidently answerable from a single response, and a reliability-aware accumulation stage that aggregates responses by jointly leveraging their frequency and confidence. Across five models and four datasets, ReASC consistently achieves the best accuracy-cost trade-off compared to existing baselines, yielding improved inference efficiency across model scales from 3B to 27B parameters. As a concrete example, ReASC reduces inference cost by up to 70\\% relative to self-consistency while preserving accuracy on GSM8K using Gemma-3-4B-it.", "AI": {"tldr": "ReASC是一种新的自适应自一致性方法，通过基于证据充分性的响应聚合策略，在保持精度的同时显著降低推理成本。相比传统方法，它利用响应置信度而非简单计数来决定采样停止时机，实现了更好的精度-成本权衡。", "motivation": "传统自一致性方法虽然提高推理可靠性但计算成本高昂，现有的自适应方法基于简单计数停止规则，对所有响应同等对待，导致不必要的采样开销。", "method": "ReASC采用两阶段方法：1)单样本决策阶段处理可单次响应确定的问题；2)可靠性感知聚合阶段，联合利用响应频率和置信度进行信息聚合，基于证据充分性而非简单计数来决定采样。", "result": "在5个模型和4个数据集上的实验表明，ReASC在3B到27B参数规模的模型上都实现了最佳的精度-成本权衡，相比自一致性方法在Gemma-3-4B-it模型上降低70%推理成本的同时保持GSM8K数据集上的精度。", "conclusion": "ReASC通过将自适应采样从响应计数重新定义为证据充分性，提供了一种更有效的推理优化方法，显著提高了大语言模型的推理效率。"}}
{"id": "2601.03066", "pdf": "https://arxiv.org/pdf/2601.03066", "abs": "https://arxiv.org/abs/2601.03066", "authors": ["Janvijay Singh", "Dilek Hakkani-Tür"], "title": "Do LLMs Encode Functional Importance of Reasoning Tokens?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 8 figures, 2 tables", "summary": "Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.", "AI": {"tldr": "论文提出贪婪剪枝方法，通过迭代删除对模型似然影响最小的推理标记来压缩推理链，在保持性能的同时减少计算成本，并发现模型内部存在功能重要性结构。", "motivation": "现有方法通过概率采样、启发式或前沿模型监督来缩短推理链，但无法揭示模型内部是否编码了标记级别的功能重要性信息。", "method": "提出贪婪剪枝方法：基于似然保持的删除过程，迭代移除对指定目标下模型似然影响最小的推理标记，生成长度可控的推理链。", "result": "在蒸馏框架中评估显示，使用剪枝链训练的学生模型在相同推理长度下优于前沿模型监督的压缩基线；分析发现系统性的剪枝模式，注意力分数可预测剪枝排序。", "conclusion": "模型内部确实编码了非平凡的功能重要性结构，贪婪剪枝是有效的诊断和压缩工具，为理解模型推理机制提供了新视角。"}}
{"id": "2601.03089", "pdf": "https://arxiv.org/pdf/2601.03089", "abs": "https://arxiv.org/abs/2601.03089", "authors": ["Xin Huang", "Antoni B. Chan"], "title": "Grad-ELLM: Gradient-based Explanations for Decoder-only LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet their black-box nature raises concerns about transparency and faithfulness. Input attribution methods aim to highlight each input token's contributions to the model's output, but existing approaches are typically model-agnostic, and do not focus on transformer-specific architectures, leading to limited faithfulness. To address this, we propose Grad-ELLM, a gradient-based attribution method for decoder-only transformer-based LLMs. By aggregating channel importance from gradients of the output logit with respect to attention layers and spatial importance from attention maps, Grad-ELLM generates heatmaps at each generation step without requiring architectural modifications. Additionally, we introduce two faithfulneses metrics $π$-Soft-NC and $π$-Soft-NS, which are modifications of Soft-NC/NS that provide fairer comparisons by controlling the amount of information kept when perturbing the text. We evaluate Grad-ELLM on sentiment classification, question answering, and open-generation tasks using different models. Experiment results show that Grad-ELLM consistently achieves superior faithfulness than other attribution methods.", "AI": {"tldr": "提出了Grad-ELLM，一种基于梯度的归因方法，专门针对仅解码器Transformer架构的LLM，通过结合注意力层梯度和注意力图来生成热力图，并引入新的忠实度评估指标。", "motivation": "现有输入归因方法通常是模型无关的，不针对Transformer架构，导致忠实度有限，需要开发专门针对LLM架构的归因方法以提高透明度。", "method": "Grad-ELLM方法：1) 聚合注意力层输出logit梯度的通道重要性；2) 从注意力图中提取空间重要性；3) 在每个生成步骤生成热力图，无需修改模型架构；4) 引入π-Soft-NC和π-Soft-NS两个新的忠实度评估指标。", "result": "在情感分类、问答和开放生成任务上的实验表明，Grad-ELLM相比其他归因方法始终获得更优的忠实度表现。", "conclusion": "Grad-ELLM为基于Transformer的LLM提供了一种有效的梯度归因方法，通过专门的架构感知设计和新的评估指标，显著提升了归因结果的忠实度和可靠性。"}}
{"id": "2601.03103", "pdf": "https://arxiv.org/pdf/2601.03103", "abs": "https://arxiv.org/abs/2601.03103", "authors": ["Soichiro Murakami", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "title": "Who Laughs with Whom? Disentangling Influential Factors in Humor Preferences across User Clusters and LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humor preferences vary widely across individuals and cultures, complicating the evaluation of humor using large language models (LLMs). In this study, we model heterogeneity in humor preferences in Oogiri, a Japanese creative response game, by clustering users with voting logs and estimating cluster-specific weights over interpretable preference factors using Bradley-Terry-Luce models. We elicit preference judgments from LLMs by prompting them to select the funnier response and found that user clusters exhibit distinct preference patterns and that the LLM results can resemble those of particular clusters. Finally, we demonstrate that, by persona prompting, LLM preferences can be directed toward a specific cluster. The scripts for data collection and analysis will be released to support reproducibility.", "AI": {"tldr": "本研究通过聚类分析日本Oogiri游戏中的用户幽默偏好，使用Bradley-Terry-Luce模型估计可解释的偏好因子权重，发现LLMs的幽默偏好可以与特定用户群体相似，并通过角色提示引导LLMs的偏好方向。", "motivation": "由于幽默偏好存在显著的个体和文化差异，使用大语言模型(LLMs)评估幽默变得复杂，需要更好地理解和建模这种异质性。", "method": "使用投票日志对Oogiri游戏用户进行聚类，应用Bradley-Terry-Luce模型估计可解释的偏好因子权重，通过提示LLMs选择更有趣的回答来获取偏好判断。", "result": "发现用户群体展现出不同的偏好模式，LLMs的结果可以与特定群体相似，通过角色提示可以引导LLMs的偏好朝向特定群体。", "conclusion": "LLMs能够模拟特定用户群体的幽默偏好，角色提示是有效引导LLMs偏好的方法，研究支持了LLMs在理解人类幽默偏好异质性方面的潜力。"}}
{"id": "2601.02986", "pdf": "https://arxiv.org/pdf/2601.02986", "abs": "https://arxiv.org/abs/2601.02986", "authors": ["Kwangwook Seo", "Dongha Lee"], "title": "P-Check: Advancing Personalized Reward Model via Learning to Generate Dynamic Checklist", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Recent approaches in personalized reward modeling have primarily focused on leveraging user interaction history to align model judgments with individual preferences. However, existing approaches largely treat user context as a static or implicit conditioning signal, failing to capture the dynamic and multi-faceted nature of human judgment. In this paper, we propose P-Check, a novel personalized reward modeling framework, designed to train a plug-and-play checklist generator that synthesizes dynamic evaluation criteria for guiding the reward prediction. To better align these checklists with personalized nuances, we introduce Preference-Contrastive Criterion Weighting, a training strategy that assigns saliency scores to criteria based on their discriminative power for personalized judgment. We conduct extensive experiments and demonstrate that P-Check not only improves reward accuracy but also enhances downstream personalized generation, and remains robust in OOD scenarios.", "AI": {"tldr": "P-Check是一个新的个性化奖励建模框架，通过动态检查单生成器和偏好对比标准加权策略，提升奖励预测准确性和下游个性化生成任务表现", "motivation": "现有方法将用户上下文视为静态或隐式条件信号，无法捕捉人类判断的动态和多面性特征", "method": "提出P-Check框架，训练即插即用的检查单生成器来合成动态评估标准，并引入偏好对比标准加权策略根据判别力为标准分配显著性分数", "result": "实验表明P-Check不仅提高了奖励准确性，还增强了下游个性化生成能力，并在OOD场景中保持鲁棒性", "conclusion": "P-Check通过动态检查单生成和个性化标准加权，有效解决了现有个性化奖励建模方法的局限性，为个性化AI系统提供了更强大的对齐机制"}}
{"id": "2601.03121", "pdf": "https://arxiv.org/pdf/2601.03121", "abs": "https://arxiv.org/abs/2601.03121", "authors": ["Peiran Li", "Jan Fillies", "Adrian Paschke"], "title": "ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to the main conference of EACL 2026", "summary": "Augmenting toxic language data in a controllable and class-specific manner is crucial for improving robustness in toxicity classification, yet remains challenging due to limited supervision and distributional skew. We propose ToxiGAN, a class-aware text augmentation framework that combines adversarial generation with semantic guidance from large language models (LLMs). To address common issues in GAN-based augmentation such as mode collapse and semantic drift, ToxiGAN introduces a two-step directional training strategy and leverages LLM-generated neutral texts as semantic ballast. Unlike prior work that treats LLMs as static generators, our approach dynamically selects neutral exemplars to provide balanced guidance. Toxic samples are explicitly optimized to diverge from these exemplars, reinforcing class-specific contrastive signals. Experiments on four hate speech benchmarks show that ToxiGAN achieves the strongest average performance in both macro-F1 and hate-F1, consistently outperforming traditional and LLM-based augmentation methods. Ablation and sensitivity analyses further confirm the benefits of semantic ballast and directional training in enhancing classifier robustness.", "AI": {"tldr": "ToxiGAN是一个结合对抗生成和LLM语义引导的类别感知文本增强框架，通过两阶段定向训练和语义锚点技术解决GAN模式坍塌和语义偏移问题，在四个仇恨言论基准测试中表现最佳", "motivation": "毒性语言数据增强需要可控和类别特定的方法以提高分类器鲁棒性，但由于监督有限和分布偏斜而面临挑战", "method": "提出ToxiGAN框架：1）结合对抗生成和LLM语义引导；2）引入两阶段定向训练策略；3）利用LLM生成的中性文本作为语义锚点；4）动态选择中性样本提供平衡指导；5）明确优化毒性样本与中性样本的差异", "result": "在四个仇恨言论基准测试中，ToxiGAN在macro-F1和hate-F1指标上均取得最强平均性能，优于传统和LLM基础的增强方法", "conclusion": "消融实验和敏感性分析证实了语义锚点和定向训练在增强分类器鲁棒性方面的益处，该方法为毒性语言数据增强提供了有效解决方案"}}
{"id": "2601.02989", "pdf": "https://arxiv.org/pdf/2601.02989", "abs": "https://arxiv.org/abs/2601.02989", "authors": ["Hosein Hasani", "Mohammadali Banayeeanzade", "Ali Nafisi", "Sadegh Mohammadian", "Fatemeh Askari", "Mobin Bagherian", "Amirmohammad Izadi", "Mahdieh Soleymani Baghshah"], "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.", "AI": {"tldr": "大语言模型在计数任务中存在系统性限制，研究者提出了一种受System-2认知过程启发的策略，将大计数任务分解为小问题，通过机制分析揭示了计数在模型中的工作原理，使LLM能够超越架构限制实现高精度计数。", "motivation": "大语言模型虽然在复杂数学问题上表现优异，但在计数任务中由于transformer架构的深度限制，处理大规模计数问题时精度会下降，需要解决这一系统性限制。", "method": "提出基于System-2认知过程的测试时策略，将大计数任务分解为小的独立子问题；使用观测性和因果中介分析来理解这种类似System-2策略的底层机制。", "result": "机制分析识别出关键组件：潜在计数在每部分的最终项表示中计算存储，通过专门的注意力头传递到中间步骤，并在最后阶段聚合产生总计数；实验结果表明该策略使LLM能够超越架构限制，在大规模计数任务中实现高精度。", "conclusion": "这项工作提供了LLM中System-2计数机制的深入理解，并提出了一个可推广的方法来改进和理解LLM的推理行为。"}}
{"id": "2601.03136", "pdf": "https://arxiv.org/pdf/2601.03136", "abs": "https://arxiv.org/abs/2601.03136", "authors": ["Selma Wanna", "Agnes Luhtaru", "Jonathan Salfity", "Ryan Barron", "Juston Moore", "Cynthia Matuszek", "Mitch Pryor"], "title": "Limited Linguistic Diversity in Embodied AI Datasets", "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": null, "summary": "Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.", "AI": {"tldr": "本文对多个广泛使用的VLA数据集进行了系统审计，发现这些数据集的语言指令存在高度重复、模板化、结构变化有限的问题，语言多样性不足。", "motivation": "当前VLA模型中使用的数据集语言特征缺乏充分记录，需要系统分析这些数据集实际包含的指令类型和语言多样性。", "method": "从词汇多样性、重复和重叠度、语义相似性、句法复杂性等互补维度对VLA语料库进行量化分析。", "result": "分析显示许多数据集依赖高度重复的模板化指令，结构变化有限，导致指令形式的分布范围狭窄。", "conclusion": "这些发现为当前VLA训练和评估数据的语言信号提供了描述性文档，支持更详细的数据集报告、更原则性的数据集选择以及扩大语言覆盖的有针对性策展或增强策略。"}}
{"id": "2601.02993", "pdf": "https://arxiv.org/pdf/2601.02993", "abs": "https://arxiv.org/abs/2601.02993", "authors": ["Qianchi Zhang", "Hainan Zhang", "Liang Pang", "Hongwei Zheng", "Zhiming Zheng"], "title": "Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "19 pages, 13figures, 8 tables, under review", "summary": "Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.", "AI": {"tldr": "Stable-RAG通过估计检索文档排列敏感性，使用多排列生成、聚类隐藏状态和解码主导推理模式的方法，显著减少大语言模型在RAG中的排列敏感性幻觉问题，提高答案准确性和一致性。", "motivation": "现有RAG方法虽然能减少事实幻觉，但对检索文档的排列顺序敏感，即使包含黄金文档且位置固定，模型输出仍会因文档排列不同而大幅变化，这种排列敏感性尚未得到充分研究和解决。", "method": "提出Stable-RAG方法：1）在多个检索排列下运行生成器；2）对隐藏状态进行聚类；3）从捕获主导推理模式的聚类中心表示解码；4）使用推理结果将幻觉输出对齐到正确答案。", "result": "在三个QA数据集上的实验表明，Stable-RAG相比基线方法显著提高了答案准确性、推理一致性，并在不同数据集、检索器和输入长度下展现出更强的鲁棒泛化能力。", "conclusion": "Stable-RAG有效解决了RAG中的排列敏感性问题，通过多排列推理和聚类方法使模型产生更一致和准确的预测，为提升RAG系统的稳定性和可靠性提供了有效方案。"}}
{"id": "2601.03144", "pdf": "https://arxiv.org/pdf/2601.03144", "abs": "https://arxiv.org/abs/2601.03144", "authors": ["Andrew Shin"], "title": "Self-Verification is All You Need To Pass The Japanese Bar Examination", "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/shinandrew/self_verification", "summary": "Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available.", "AI": {"tldr": "首个无需改变原始题目结构和评分规则就能通过日本司法考试的LLM模型，通过自验证机制和格式忠实监督实现了超越官方及格线的表现", "motivation": "尽管大语言模型快速发展，但在高度专业化和结构化的考试中实现可靠性能仍是重大挑战。日本司法考试要求复杂的法律推理和严格的答案格式，现有分解方法未能在原始考试格式下系统评估", "method": "构建忠实复制考试真实格式和评分标准的新数据集，训练自验证模型，保持原始问题结构和评分规则不变", "result": "模型在实际考试评分标准下超过官方及格分数，是多智能体推理和分解监督等替代策略无法达到的性能水平", "conclusion": "格式忠实监督和一致性验证至关重要，精心设计的单模型方法在专业推理任务中可以胜过更复杂的系统"}}
{"id": "2601.02996", "pdf": "https://arxiv.org/pdf/2601.02996", "abs": "https://arxiv.org/abs/2601.02996", "authors": ["Yihong Liu", "Raoyuan Zhao", "Hinrich Schütze", "Michael A. Hedderich"], "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners", "categories": ["cs.CL"], "comment": "preprint", "summary": "Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.", "AI": {"tldr": "该论文系统研究了大型推理模型在11种语言中的多语言潜在推理能力，发现虽然存在多语言潜在推理现象，但在资源丰富语言中表现更强，低资源语言中较弱，且内部预测演化过程高度一致，呈现出以英语为中心的潜在推理路径模式。", "motivation": "大型推理模型在数学推理任务中表现出色，通常归因于其生成显式思维链解释的能力。然而，研究发现模型在完成文本推理步骤之前就能得出正确答案，表明存在潜在推理现象。虽然这种现象在英语中已有探索，但其在多语言环境中的行为仍不清楚。", "method": "使用基于截断的策略，在只给模型提供部分推理轨迹的情况下，检查正确答案如何出现，从而测量逐步潜在预测形成。此外还进行了表征分析，以了解不同语言间的差异是否反映了不同的内部机制。", "result": "研究结果揭示了多语言潜在推理的明确证据，但表现不均：在资源丰富语言中表现强劲，在低资源语言中较弱，在更难基准测试中普遍较难观察到。尽管表面存在差异，但内部预测演化在跨语言间高度一致，且与英语模式广泛对齐。", "conclusion": "多语言大型推理模型存在以英语为中心的潜在推理路径，尽管在不同语言中表现强度不同，但内部机制具有一致性，这表明模型可能基于英语训练数据形成了统一的推理模式，然后在其他语言中应用相似的内部计算过程。"}}
{"id": "2601.03199", "pdf": "https://arxiv.org/pdf/2601.03199", "abs": "https://arxiv.org/abs/2601.03199", "authors": ["Yang Li", "Han Meng", "Chenan Wang", "Haipeng Chen"], "title": "DIP: Dynamic In-Context Planner For Diffusion Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "4 pages", "summary": "Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \\textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \\textbf{D}ynamic \\textbf{I}n-Context \\textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\\times$ inference speedup over standard inference and 1.17$\\times$ over KV cache-enhanced inference.", "AI": {"tldr": "本文提出了DIP方法，通过动态选择和插入上下文示例来提升扩散语言模型的推理效率，在保持生成质量的同时实现高达12.9倍的推理加速", "motivation": "扩散语言模型在处理长上下文时计算成本高昂，需要解决推理效率问题", "method": "利用扩散生成范式允许动态调整上下文的特性，提出动态上下文规划器(DIP)，在生成过程中动态选择和插入上下文示例", "result": "DIP在保持生成质量的同时，相比标准推理实现了12.9倍加速，相比KV缓存增强推理实现了1.17倍加速", "conclusion": "DIP方法有效解决了扩散语言模型的推理效率问题，为实际应用提供了可行的解决方案"}}
{"id": "2601.03205", "pdf": "https://arxiv.org/pdf/2601.03205", "abs": "https://arxiv.org/abs/2601.03205", "authors": ["Yile Liu", "Yixian Liu", "Zongwei Li", "Yufei Huang", "Xinhua Feng", "Zhichao Hu", "Jinglu Hu", "Jianfeng Yan", "Fengzong Lian", "Yuhong Liu"], "title": "UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 6 figures, 7 tables", "summary": "While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.", "AI": {"tldr": "UltraLogic框架通过代码化方法自动生成大规模、高质量、难度分级的多步推理数据，结合双极浮动奖励机制解决强化学习中的奖励稀疏问题，显著提升大语言模型的复杂推理能力。", "motivation": "大语言模型在多步逻辑推理、规划和验证方面存在瓶颈，缺乏大规模、高质量、难度校准的通用推理数据，需要解决强化学习中的奖励稀疏和负奖励陷阱问题。", "method": "提出UltraLogic框架，通过代码化求解方法将问题逻辑核心与自然语言表达解耦，自动生成高质量数据；包含数百种任务类型和10个难度级别的自动校准管道；引入双极浮动奖励(BFR)机制，使用分级惩罚区分完美回答和存在逻辑缺陷的回答。", "result": "实验证明任务多样性是推理能力提升的主要驱动力，BFR结合难度匹配策略显著提高了训练效率，引导模型达到全局逻辑最优。", "conclusion": "UltraLogic框架成功解决了通用推理数据稀缺和强化学习奖励问题，为提升大语言模型的复杂推理能力提供了有效解决方案，任务多样性和精细化的奖励机制是关键成功因素。"}}
{"id": "2601.03017", "pdf": "https://arxiv.org/pdf/2601.03017", "abs": "https://arxiv.org/abs/2601.03017", "authors": ["Jing Xiong", "Qi Han", "Yunta Hsieh", "Hui Shen", "Huajian Xin", "Chaofan Tao", "Chenyang Zhao", "Hengyuan Zhang", "Taiqiang Wu", "Zhen Zhang", "Haochen Wang", "Zhongwei Wan", "Lingpeng Kong", "Ngai Wong"], "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "AI": {"tldr": "MMFormalizer是首个多模态自动形式化方法，通过整合视觉元素和自适应接地技术，将自然语言数学转换为形式化陈述，能够处理物理世界的多模态特性，包括经典力学、相对论、量子力学和热力学等领域。", "motivation": "传统自动形式化方法面临物理世界多模态特性的挑战，需要从视觉元素推断隐藏约束（如质量或能量），因此需要开发能够整合视觉信息的自动形式化框架。", "method": "提出MMFormalizer方法，通过自适应接地与实体整合，从感知接地原语递归构建形式命题，使用自适应递归终止确保每个抽象都有视觉证据支持，并基于维度或公理接地。", "result": "在PhyX-AF基准测试中，前沿模型如GPT-5和Gemini-3-Pro在编译和语义准确性方面表现最佳，GPT-5在物理推理方面表现优异，几何领域仍是最具挑战性的领域。", "conclusion": "MMFormalizer为统一的多模态自动形式化提供了可扩展框架，弥合了感知和形式推理之间的差距，是首个能够处理经典力学及相关物理领域的多模态自动形式化方法。"}}
{"id": "2601.03232", "pdf": "https://arxiv.org/pdf/2601.03232", "abs": "https://arxiv.org/abs/2601.03232", "authors": ["Kartik Bose", "Abhinandan Kumar", "Raghuraman Soundararajan", "Priya Mudgil", "Samonee Ralmilay", "Niharika Dutta", "Manphool Singhal", "Arun Kumar", "Saugata Sen", "Anurima Patra", "Priya Ghosh", "Abanti Das", "Amit Gupta", "Ashish Verma", "Dipin Sudhakaran", "Ekta Dhamija", "Himangi Unde", "Ishan Kumar", "Krithika Rangarajan", "Prerna Garg", "Rachel Sequeira", "Sudhin Shylendran", "Taruna Yadav", "Tej Pal", "Pankaj Gupta"], "title": "Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between <1B and >=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes.", "AI": {"tldr": "该研究创建了RXL-RADSet放射学报告基准数据集，比较了开源小语言模型与专有模型在RADS分类任务中的表现，发现20-32B参数的大规模SLM在引导提示下可接近专有模型性能。", "motivation": "放射学报告和数据系统(RADS)标准化风险沟通，但从叙述性报告自动分配RADS具有挑战性，因为指南复杂、输出格式限制且缺乏跨RADS框架和模型规模的基准测试。", "method": "创建包含1,600份合成放射学报告的RXL-RADSet数据集，涵盖10种RADS标准和多种模态。使用LLM生成报告并经过放射科医生两阶段验证。评估41个量化SLM和GPT-5.2在固定引导提示下的表现。", "result": "GPT-5.2达到99.8%有效性和81.1%准确性。SLM总体达到96.8%有效性和61.1%准确性，20-32B参数的最佳SLM达到约99%有效性和70%+准确性。引导提示相比零样本提示显著提升性能。", "conclusion": "RXL-RADSet提供了放射科医生验证的多RADS基准，大型SLM在引导提示下可接近专有模型性能，但在高复杂度方案上仍存在差距。"}}
{"id": "2601.03023", "pdf": "https://arxiv.org/pdf/2601.03023", "abs": "https://arxiv.org/abs/2601.03023", "authors": ["Lecheng Gong", "Weimin Fang", "Ting Yang", "Dongjie Tao", "Chunxiao Guo", "Peng Wei", "Bo Xie", "Jinqun Guan", "Zixiao Chen", "Fang Shi", "Jinjie Gu", "Junwei Liu"], "title": "MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items (\"must-ask\" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.", "AI": {"tldr": "MedDialogRubrics是一个新的医学对话AI基准测试，包含5200个合成病例和60000+细粒度评估标准，用于评估大语言模型的多轮诊断能力，发现当前模型仍面临重大挑战", "motivation": "现有的医学大语言模型基准测试和评估框架在信息收集和诊断推理能力评估方面缺乏严谨性，存在隐私和数据治理问题", "method": "使用多智能体系统合成真实患者记录，设计有限原子医学事实的患者代理，采用基于EBM指南的结构化标准生成流程和拒绝采样方法", "result": "当前最先进模型在多个评估维度上都面临重大挑战，需要对话管理架构的改进而不仅仅是基础模型的微调", "conclusion": "医学对话AI的发展需要对话管理架构的进步，MedDialogRubrics为评估和改进医学对话系统提供了重要基准"}}
{"id": "2601.03025", "pdf": "https://arxiv.org/pdf/2601.03025", "abs": "https://arxiv.org/abs/2601.03025", "authors": ["Aarya Khandelwal", "Ritwik Mishra", "Rajiv Ratn Shah"], "title": "LittiChoQA: Literary Texts in Indic Languages Chosen for Question Answering", "categories": ["cs.CL"], "comment": "Submitted to ARR Jan cycle. Targetting AACL 2026", "summary": "Long-context question answering (QA) over literary texts poses significant challenges for modern large language models, particularly in low-resource languages. We address the scarcity of long-context QA resources for Indic languages by introducing LittiChoQA, the largest literary QA dataset to date covering many languages spoken in the Gangetic plains of India. The dataset comprises over 270K automatically generated question-answer pairs with a balanced distribution of factoid and non-factoid questions, generated from naturally authored literary texts collected from the open web. We evaluate multiple multilingual LLMs on non-factoid, abstractive QA, under both full-context and context-shortened settings. Results demonstrate a clear trade-off between performance and efficiency: full-context fine-tuning yields the highest token-level and semantic-level scores, while context shortening substantially improves throughput. Among the evaluated models, Krutrim-2 achieves the strongest performance, obtaining a semantic score of 76.1 with full context. While, in shortened context settings it scores 74.9 with answer paragraph selection and 71.4 with vector-based retrieval. Qualitative evaluations further corroborate these findings.", "AI": {"tldr": "LittiChoQA是最大的印度语言文学问答数据集，包含27万+自动生成的问答对，评估显示全上下文微调效果最佳但效率低，上下文缩短可提升吞吐量，Krutrim-2模型表现最优", "motivation": "解决印度语言长上下文问答资源稀缺的问题，特别是针对恒河平原地区的低资源语言", "method": "从开放网络收集自然创作的文学文本，自动生成27万+问答对（平衡事实性和非事实性问题），评估多语言LLM在完整上下文和缩短上下文设置下的表现", "result": "全上下文微调获得最高分（76.1语义分），上下文缩短显著提升吞吐量，Krutrim-2模型表现最佳，缩短上下文时段落选择得74.9分，向量检索得71.4分", "conclusion": "长上下文问答存在性能与效率的权衡，全上下文效果最好但计算成本高，上下文缩短是实用的折中方案，为低资源语言文学QA提供了重要基准"}}
{"id": "2601.03027", "pdf": "https://arxiv.org/pdf/2601.03027", "abs": "https://arxiv.org/abs/2601.03027", "authors": ["Sindhuja Chaduvula", "Ahmed Y. Radwan", "Azib Farooq", "Yani Ioannou", "Shaina Raza"], "title": "Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning", "categories": ["cs.CL"], "comment": null, "summary": "Preference alignment methods such as RLHF and Direct Preference Optimization (DPO) improve instruction following, but they can also reinforce hallucinations when preference judgments reward fluency and confidence over factual correctness. We introduce F-DPO (Factuality-aware Direct Preference Optimization), a simple extension of DPO that uses only binary factuality labels. F-DPO (i) applies a label-flipping transformation that corrects misordered preference pairs so the chosen response is never less factual than the rejected one, and (ii) adds a factuality-aware margin that emphasizes pairs with clear correctness differences, while reducing to standard DPO when both responses share the same factuality. We construct factuality-aware preference data by augmenting DPO pairs with binary factuality indicators and synthetic hallucinated variants. Across seven open-weight LLMs (1B-14B), F-DPO consistently improves factuality and reduces hallucination rates relative to both base models and standard DPO. On Qwen3-8B, F-DPO reduces hallucination rates by five times (from 0.424 to 0.084) while improving factuality scores by 50 percent (from 5.26 to 7.90). F-DPO also generalizes to out-of-distribution benchmarks: on TruthfulQA, Qwen2.5-14B achieves plus 17 percent MC1 accuracy (0.500 to 0.585) and plus 49 percent MC2 accuracy (0.357 to 0.531). F-DPO requires no auxiliary reward model, token-level annotations, or multi-stage training.", "AI": {"tldr": "F-DPO是一种基于DPO的改进方法，通过引入二进制事实性标签和标签翻转变换，有效减少大语言模型的幻觉问题，在多个模型上显著提升事实准确性。", "motivation": "传统的偏好对齐方法（如RLHF和DPO）虽然能提升指令跟随能力，但可能因为偏好判断更关注流畅性和置信度而非事实正确性，从而强化模型幻觉问题。", "method": "F-DPO在DPO基础上进行两个关键改进：(1)使用标签翻转变换确保被选中的回答不会比被拒绝的回答更不真实；(2)添加事实感知边界，强调具有明显正确性差异的回答对，在两者事实性相同时退化为标准DPO。通过为DPO对添加二进制事实性指标和合成幻觉变体来构建事实感知偏好数据。", "result": "在7个开源LLM（1B-14B）上，F-DPO相比基础模型和标准DPO持续改善事实性并降低幻觉率。Qwen3-8B的幻觉率降低5倍（0.424→0.084），事实性得分提升50%（5.26→7.90）。在TruthfulQA上，Qwen2.5-14B的MC1准确率提升17%（0.500→0.585），MC2准确率提升49%（0.357→0.531）。", "conclusion": "F-DPO是一种简单有效的改进方法，无需辅助奖励模型、token级标注或多阶段训练，就能显著提升大语言模型的事实准确性并减少幻觉问题，具有良好的泛化能力。"}}
{"id": "2601.03034", "pdf": "https://arxiv.org/pdf/2601.03034", "abs": "https://arxiv.org/abs/2601.03034", "authors": ["Jon Atle Gulla", "Peng Liu", "Lemei Zhang"], "title": "NorwAI's Large Language Models: Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.", "AI": {"tldr": "挪威NorLLM团队开发了专门针对挪威语和斯堪的纳维亚语言的大语言模型家族，基于多种Transformer架构，通过大规模预训练和指令调优，为北欧地区提供开源LLM解决方案。", "motivation": "挪威语在自然语言处理领域代表性不足，需要专门针对挪威及斯堪的纳维亚语言开发大语言模型来弥补这一技术差距。", "method": "使用GPT、Mistral、Llama2等Transformer架构，从零开始预训练或持续预训练25B-88.45B词元，采用挪威语扩展分词器和先进的后训练策略。", "result": "开发出指令调优变体模型（如Mistral-7B-Instruct和Mixtral-8x7B-Instruct），展现出强大的助手式能力，适合实际部署。", "conclusion": "这些模型为北欧组织、公司和学生提供了开源的研究和实验资源，填补了挪威语NLP技术的空白，具有重要的实践应用价值。"}}
{"id": "2601.03042", "pdf": "https://arxiv.org/pdf/2601.03042", "abs": "https://arxiv.org/abs/2601.03042", "authors": ["Hexiang Tan", "Wanli Yang", "Junwei Zhang", "Xin Chen", "Rui Tang", "Du Su", "Jingang Wang", "Yuanzhuo Wang", "Fei Sun", "Xueqi Cheng"], "title": "BaseCal: Unsupervised Confidence Calibration via Base Model Signals", "categories": ["cs.CL"], "comment": null, "summary": "Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\\% compared to the best unsupervised baselines.", "AI": {"tldr": "BaseCal方法通过使用基础LLM作为参考来校准后训练LLM的置信度，提出了BaseCal-ReEval和BaseCal-Proj两种无监督解决方案，显著降低了校准误差。", "motivation": "后训练LLM存在严重过度自信问题，而对应的基础LLM通常保持良好校准，这促使研究者利用基础LLM来校准后训练LLM的置信度。", "method": "提出两种方法：1) BaseCal-ReEval：将后训练LLM的响应输入基础LLM获取平均概率作为置信度；2) BaseCal-Proj：训练轻量级投影将后训练LLM的隐藏状态映射回基础LLM状态，再通过基础LLM输出层获得校准后的置信度。", "result": "在五个数据集和三个LLM家族上的实验显示，BaseCal相比最佳无监督基线平均降低了42.90%的预期校准误差(ECE)。", "conclusion": "BaseCal是一种无需人工标注或LLM修改的无监督即插即用解决方案，能有效提高后训练LLM的置信度可靠性。"}}
{"id": "2601.03051", "pdf": "https://arxiv.org/pdf/2601.03051", "abs": "https://arxiv.org/abs/2601.03051", "authors": ["Vidhi Rathore", "Sambu Aneesh", "Himanshu Singh"], "title": "Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Hallucinations can be produced by conversational AI systems, particularly in multi-turn conversations where context changes and contradictions may eventually surface. By representing the entire conversation as a temporal graph, we present a novel graph-based method for detecting dialogue-level hallucinations. Our framework models each dialogue as a node, encoding it using a sentence transformer. We explore two different ways of connectivity: i) shared-entity edges, which connect turns that refer to the same entities; ii) temporal edges, which connect contiguous turns in the conversation. Message-passing is used to update the node embeddings, allowing flow of information between related nodes. The context-aware node embeddings are then combined using attention pooling into a single vector, which is then passed on to a classifier to determine the presence and type of hallucinations. We demonstrate that our method offers slightly improved performance over existing methods. Further, we show the attention mechanism can be used to justify the decision making process. The code and model weights are made available at: https://github.com/sambuaneesh/anlp-project.", "AI": {"tldr": "本文提出了一种基于时序图的对话幻觉检测方法，通过图神经网络和注意力池化机制，在多轮对话中识别幻觉现象，性能略优于现有方法。", "motivation": "对话AI系统在多轮对话中容易产生幻觉问题，特别是当上下文变化和矛盾出现时，需要有效检测对话级别的幻觉。", "method": "将整个对话表示为时序图，每个对话轮次作为节点，使用句子转换器编码。构建两种边连接：共享实体边（连接提及相同实体的轮次）和时间边（连接连续轮次）。通过消息传递更新节点嵌入，使用注意力池化整合信息，最后用分类器检测幻觉类型。", "result": "该方法相比现有方法有轻微的性能提升，注意力机制还能为决策过程提供可解释性。", "conclusion": "基于图神经网络的方法能有效检测多轮对话中的幻觉，注意力机制提供了决策透明度，代码和模型权重已开源。"}}
{"id": "2601.03052", "pdf": "https://arxiv.org/pdf/2601.03052", "abs": "https://arxiv.org/abs/2601.03052", "authors": ["Jianpeng Hu", "Yanzeng Li", "Jialun Zhong", "Wenfa Qi", "Lei Zou"], "title": "Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph", "categories": ["cs.CL"], "comment": null, "summary": "The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.", "AI": {"tldr": "提出基于语义层内部推理图的忠实性幻觉检测方法，通过扩展层间相关性传播算法构建语义级依赖表示，使用小型预训练语言模型框架进行训练和检测，在RAGTruth和Dolly-15k数据集上优于现有基线方法。", "motivation": "基于大语言模型的检索增强生成系统虽然减少了事实性幻觉，但仍存在忠实性幻觉问题。现有检测方法要么忽略模型内部推理过程，要么处理特征粗糙，导致判别器难以学习。", "method": "将层间相关性传播算法从词元级扩展到语义级，基于归因向量构建内部推理图；设计基于小型预训练语言模型的通用框架，利用LLM推理中的依赖关系进行训练和检测，通过阈值动态调整正确样本的通过率。", "result": "实验结果表明，该方法在RAGTruth和Dolly-15k数据集上相比最先进的基线方法取得了更好的整体性能。", "conclusion": "提出的语义级内部推理图方法能有效检测忠实性幻觉，通过更精细的语义级依赖表示和小型模型框架实现了优越的检测性能。"}}
{"id": "2601.03079", "pdf": "https://arxiv.org/pdf/2601.03079", "abs": "https://arxiv.org/abs/2601.03079", "authors": ["Bocheng Chen", "Han Zi", "Xi Chen", "Xitong Zhang", "Kristen Johnson", "Guangliang Liu"], "title": "Learning to Diagnose and Correct Moral Errors: Towards Enhancing Moral Sensitivity in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Moral sensitivity is fundamental to human moral competence, as it guides individuals in regulating everyday behavior. Although many approaches seek to align large language models (LLMs) with human moral values, how to enable them morally sensitive has been extremely challenging. In this paper, we take a step toward answering the question: how can we enhance moral sensitivity in LLMs? Specifically, we propose two pragmatic inference methods that faciliate LLMs to diagnose morally benign and hazardous input and correct moral errors, whereby enhancing LLMs' moral sensitivity. A central strength of our pragmatic inference methods is their unified perspective: instead of modeling moral discourses across semantically diverse and complex surface forms, they offer a principled perspective for designing pragmatic inference procedures grounded in their inferential loads. Empirical evidence demonstrates that our pragmatic methods can enhance moral sensitivity in LLMs and achieves strong performance on representative morality-relevant benchmarks.", "AI": {"tldr": "该论文提出了两种语用推理方法来增强大语言模型的道德敏感性，使其能够诊断道德良性和危险输入并纠正道德错误，在多个道德相关基准测试中表现出色。", "motivation": "尽管现有方法试图让大语言模型与人类道德价值观对齐，但如何使其具备道德敏感性仍然极具挑战性。", "method": "提出了两种基于推理负载的语用推理方法，提供统一的原则性视角来处理语义多样且复杂的道德话语。", "result": "实证证据表明，所提出的语用方法能够有效增强大语言模型的道德敏感性，并在代表性道德基准测试中取得强劲性能。", "conclusion": "语用推理方法为增强大语言模型的道德敏感性提供了一种有效的技术路径，具有统一的处理视角和良好的实践效果。"}}
{"id": "2601.03115", "pdf": "https://arxiv.org/pdf/2601.03115", "abs": "https://arxiv.org/abs/2601.03115", "authors": ["Xiutian Zhao", "Björn Schuller", "Berrak Sisman"], "title": "Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models", "categories": ["cs.CL", "eess.AS"], "comment": "16 pages, 6 figures", "summary": "Emotion is a central dimension of spoken communication, yet, we still lack a mechanistic account of how modern large audio-language models (LALMs) encode it internally. We present the first neuron-level interpretability study of emotion-sensitive neurons (ESNs) in LALMs and provide causal evidence that such units exist in Qwen2.5-Omni, Kimi-Audio, and Audio Flamingo 3. Across these three widely used open-source models, we compare frequency-, entropy-, magnitude-, and contrast-based neuron selectors on multiple emotion recognition benchmarks. Using inference-time interventions, we reveal a consistent emotion-specific signature: ablating neurons selected for a given emotion disproportionately degrades recognition of that emotion while largely preserving other classes, whereas gain-based amplification steers predictions toward the target emotion. These effects arise with modest identification data and scale systematically with intervention strength. We further observe that ESNs exhibit non-uniform layer-wise clustering with partial cross-dataset transfer. Taken together, our results offer a causal, neuron-level account of emotion decisions in LALMs and highlight targeted neuron interventions as an actionable handle for controllable affective behaviors.", "AI": {"tldr": "这篇论文首次对大音频语言模型(LALMs)中情感敏感神经元(ESNs)进行了神经元级别的可解释性研究，揭示了这些模型内部情感编码的机制，并提供了因果证据证明Qwen2.5-Omni、Kimi-Audio和Audio Flamingo 3中存在此类神经元。", "motivation": "情感是语音交流的核心维度，但目前缺乏对现代大音频语言模型如何内部编码情感机制的深入理解。", "method": "在三个广泛使用的开源模型上，比较了基于频率、熵、幅度和对比度的神经元选择器在多个情感识别基准上的表现。使用推理时间干预技术，通过消融和增益放大来研究神经元对情感识别的影响。", "result": "研究发现存在一致的情感特异性特征：消融特定情感选择的神经元会不成比例地降低该情感的识别能力，同时保持其他类别；增益放大则将预测转向目标情感。这些效应随着干预强度系统性地扩展，ESNs表现出非均匀的层级聚类和部分跨数据集迁移。", "conclusion": "研究结果为LALMs中的情感决策提供了因果的神经元级别解释，并强调针对性神经元干预作为可控情感行为的可行操作手段。"}}
{"id": "2601.03134", "pdf": "https://arxiv.org/pdf/2601.03134", "abs": "https://arxiv.org/abs/2601.03134", "authors": ["Xiangzhe Yuan", "Zhenhao Zhang", "Haoming Tang", "Siying Hu"], "title": "The Anatomy of Conversational Scams: A Topic-Based Red Teaming Analysis of Multi-Turn Interactions in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "As LLMs gain persuasive agentic capabilities through extended dialogues, they introduce novel risks in multi-turn conversational scams that single-turn safety evaluations fail to capture. We systematically study these risks using a controlled LLM-to-LLM simulation framework across multi-turn scam scenarios. Evaluating eight state-of-the-art models in English and Chinese, we analyze dialogue outcomes and qualitatively annotate attacker strategies, defensive responses, and failure modes. Results reveal that scam interactions follow recurrent escalation patterns, while defenses employ verification and delay mechanisms. Furthermore, interactional failures frequently stem from safety guardrail activation and role instability. Our findings highlight multi-turn interactional safety as a critical, distinct dimension of LLM behavior.", "AI": {"tldr": "研究发现多轮对话中LLM存在新型诈骗风险，单轮安全评估无法捕捉，通过LLM间模拟实验揭示了诈骗升级模式和防御机制。", "motivation": "随着LLM在多轮对话中获得说服性代理能力，传统单轮安全评估无法有效识别多轮对话诈骗风险，需要系统性研究这些新型风险。", "method": "使用受控的LLM到LLM模拟框架，在英语和中文环境中评估8个最先进模型，分析对话结果并定性标注攻击策略、防御响应和失败模式。", "result": "发现诈骗互动遵循重复升级模式，防御采用验证和延迟机制，交互失败常源于安全护栏激活和角色不稳定性。", "conclusion": "多轮交互安全是LLM行为的一个关键且独特的维度，需要专门的安全评估方法。"}}
{"id": "2601.03135", "pdf": "https://arxiv.org/pdf/2601.03135", "abs": "https://arxiv.org/abs/2601.03135", "authors": ["Aashish Dhawan", "Christopher Driggers-Ellis", "Christan Grant", "Daisy Zhe Wang"], "title": "Improving Indigenous Language Machine Translation with Synthetic Data and Language-Specific Preprocessing", "categories": ["cs.CL"], "comment": null, "summary": "Low-resource indigenous languages often lack the parallel corpora required for effective neural machine translation (NMT). Synthetic data generation offers a practical strategy for mitigating this limitation in data-scarce settings. In this work, we augment curated parallel datasets for indigenous languages of the Americas with synthetic sentence pairs generated using a high-capacity multilingual translation model. We fine-tune a multilingual mBART model on curated-only and synthetically augmented data and evaluate translation quality using chrF++, the primary metric used in recent AmericasNLP shared tasks for agglutinative languages.\n  We further apply language-specific preprocessing, including orthographic normalization and noise-aware filtering, to reduce corpus artifacts. Experiments on Guarani--Spanish and Quechua--Spanish translation show consistent chrF++ improvements from synthetic data augmentation, while diagnostic experiments on Aymara highlight the limitations of generic preprocessing for highly agglutinative languages.", "AI": {"tldr": "本研究通过使用多语言翻译模型生成合成数据来增强美洲土著语言的平行语料库，在Guarani-西班牙语和Quechua-西班牙语翻译中取得chrF++指标的持续改进，但发现通用预处理方法对高度黏着语如Aymara的效果有限。", "motivation": "低资源土著语言缺乏足够的平行语料库来支持有效的神经机器翻译，合成数据生成是解决数据稀缺问题的实用策略。", "method": "使用高容量多语言翻译模型生成合成句子对来增强人工整理的平行数据集，对多语言mBART模型进行微调，并应用语言特定的预处理（包括正字法规范化和噪声感知过滤）。", "result": "在Guarani-西班牙语和Quechua-西班牙语翻译中观察到chrF++指标的持续改进，但对Aymara语言的诊断实验显示通用预处理方法对高度黏着语的效果有限。", "conclusion": "合成数据增强对低资源土著语言机器翻译有效，但需要针对高度黏着语的特定预处理方法来进一步提高性能。"}}
{"id": "2601.03154", "pdf": "https://arxiv.org/pdf/2601.03154", "abs": "https://arxiv.org/abs/2601.03154", "authors": ["Beiduo Chen", "Tiancheng Hu", "Caiqi Zhang", "Robert Litschko", "Anna Korhonen", "Barbara Plank"], "title": "Decoupling the Effect of Chain-of-Thought Reasoning: A Human Label Variation Perspective", "categories": ["cs.CL"], "comment": "19 pages, 10 figures", "summary": "Reasoning-tuned LLMs utilizing long Chain-of-Thought (CoT) excel at single-answer tasks, yet their ability to model Human Label Variation--which requires capturing probabilistic ambiguity rather than resolving it--remains underexplored. We investigate this through systematic disentanglement experiments on distribution-based tasks, employing Cross-CoT experiments to isolate the effect of reasoning text from intrinsic model priors. We observe a distinct \"decoupled mechanism\": while CoT improves distributional alignment, final accuracy is dictated by CoT content (99% variance contribution), whereas distributional ranking is governed by model priors (over 80%). Step-wise analysis further shows that while CoT's influence on accuracy grows monotonically during the reasoning process, distributional structure is largely determined by LLM's intrinsic priors. These findings suggest that long CoT serves as a decisive LLM decision-maker for the top option but fails to function as a granular distribution calibrator for ambiguous tasks.", "AI": {"tldr": "长思维链推理调优的LLM在单答案任务表现出色，但在处理需要捕捉概率模糊性的人类标签变异任务时效果有限。研究发现思维链主要影响最终准确率，而分布排序主要由模型先验决定。", "motivation": "探索推理调优的大型语言模型在需要处理概率模糊性而非解决模糊性的任务（如人类标签变异建模）中的能力，这一领域尚未得到充分研究。", "method": "通过系统性解耦实验，在基于分布的任务上使用Cross-CoT实验，分离推理文本效果与内在模型先验的影响，并进行逐步分析。", "result": "发现解耦机制：思维链改善分布对齐，但最终准确率由思维链内容决定（99%方差贡献），分布排序由模型先验主导（超过80%）。思维链对准确率的影响在推理过程中单调增长，但分布结构主要由LLM内在先验决定。", "conclusion": "长思维链可作为LLM对最优选项的决策制定者，但无法作为模糊任务的细粒度分布校准器。"}}
{"id": "2601.03164", "pdf": "https://arxiv.org/pdf/2601.03164", "abs": "https://arxiv.org/abs/2601.03164", "authors": ["Yu Xinmiao", "Zhang Liwen", "Feng Xiaocheng", "Jiang Yong", "Qin Bing", "Xie Pengjun", "Zhou Jingren"], "title": "WebAnchor: Anchoring Agent Planning to Stabilize Long-Horizon Web Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model(LLM)-based agents have shown strong capabilities in web information seeking, with reinforcement learning (RL) becoming a key optimization paradigm. However, planning remains a bottleneck, as existing methods struggle with long-horizon strategies. Our analysis reveals a critical phenomenon, plan anchor, where the first reasoning step disproportionately impacts downstream behavior in long-horizon web reasoning tasks. Current RL algorithms, fail to account for this by uniformly distributing rewards across the trajectory. To address this, we propose Anchor-GRPO, a two-stage RL framework that decouples planning and execution. In Stage 1, the agent optimizes its first-step planning using fine-grained rubrics derived from self-play experiences and human calibration. In Stage 2, execution is aligned with the initial plan through sparse rewards, ensuring stable and efficient tool usage. We evaluate Anchor-GRPO on four benchmarks: BrowseComp, BrowseComp-Zh, GAIA, and XBench-DeepSearch. Across models from 3B to 30B, Anchor-GRPO outperforms baseline GRPO and First-step GRPO, improving task success and tool efficiency. Notably, WebAnchor-30B achieves 46.0% pass@1 on BrowseComp and 76.4% on GAIA. Anchor-GRPO also demonstrates strong scalability, getting higher accuracy as model size and context length increase.", "AI": {"tldr": "该论文提出了Anchor-GRPO框架，通过两阶段强化学习解决大语言模型在网页信息搜索中的规划瓶颈问题，重点关注首个推理步骤对长程策略的关键影响。", "motivation": "现有强化学习方法在长程网页推理任务中存在问题，首步规划（plan anchor）对下游行为影响巨大，但当前RL算法未能充分考虑这一点，而是均匀分配奖励。", "method": "提出Anchor-GRPO两阶段RL框架：第一阶段通过自对弈经验和人工校准优化首步规划；第二阶段通过稀疏奖励确保执行与初始规划对齐，实现稳定高效的工具使用。", "result": "在BrowseComp、BrowseComp-Zh、GAIA和XBench-DeepSearch四个基准测试中，Anchor-GRPO在3B到30B规模模型上都优于基线方法，WebAnchor-30B在BrowseComp上达到46.0% pass@1，在GAIA上达到76.4%。", "conclusion": "Anchor-GRPO有效解决了长程网页推理中的规划瓶颈问题，展示了良好的可扩展性，随着模型规模和上下文长度增加，准确率进一步提升。"}}
{"id": "2601.03168", "pdf": "https://arxiv.org/pdf/2601.03168", "abs": "https://arxiv.org/abs/2601.03168", "authors": ["Tewodros Kederalah Idris", "Prasenjit Mitra", "Roald Eiselen"], "title": "Can Embedding Similarity Predict Cross-Lingual Transfer? A Systematic Study on African Languages", "categories": ["cs.CL", "cs.LG"], "comment": "13 pages, 1 figure, 19 tables", "summary": "Cross-lingual transfer is essential for building NLP systems for low-resource African languages, but practitioners lack reliable methods for selecting source languages. We systematically evaluate five embedding similarity metrics across 816 transfer experiments spanning three NLP tasks, three African-centric multilingual models, and 12 languages from four language families. We find that cosine gap and retrieval-based metrics (P@1, CSLS) reliably predict transfer success ($ρ= 0.4-0.6$), while CKA shows negligible predictive power ($ρ\\approx 0.1$). Critically, correlation signs reverse when pooling across models (Simpson's Paradox), so practitioners must validate per-model. Embedding metrics achieve comparable predictive power to URIEL linguistic typology. Our results provide concrete guidance for source language selection and highlight the importance of model-specific analysis.", "AI": {"tldr": "本文系统评估了五种嵌入相似性度量方法在跨语言迁移中的预测能力，发现余弦差距和检索类指标能可靠预测迁移成功率，但需按模型分别验证以避免辛普森悖论。", "motivation": "非洲低资源语言的NLP系统构建需要有效的跨语言迁移方法，但缺乏可靠的源语言选择方法指导实践者。", "method": "在816个迁移实验中评估五种嵌入相似性度量方法，涵盖三个NLP任务、三个非洲中心多语言模型和来自四个语系的12种语言。", "result": "余弦差距和检索类指标（P@1、CSLS）能可靠预测迁移成功（ρ=0.4-0.6），而CKA预测能力可忽略（ρ≈0.1）。跨模型聚合时相关性符号会反转（辛普森悖论）。嵌入度量与URIEL语言类型学具有相当的预测能力。", "conclusion": "研究为源语言选择提供了具体指导，强调必须进行模型特定分析，嵌入相似性度量是跨语言迁移中源语言选择的实用工具。"}}
{"id": "2601.03190", "pdf": "https://arxiv.org/pdf/2601.03190", "abs": "https://arxiv.org/abs/2601.03190", "authors": ["Naixin Zhai", "Pengyang Shao", "Binbin Zheng", "Fei Shen", "Long Bai", "Xun Yang"], "title": "Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning", "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning aims to forget sensitive knowledge from Large Language Models (LLMs) while maintaining general utility. However, existing approaches typically treat all tokens in a response indiscriminately and enforce uncertainty over the entire vocabulary. This global treatment results in unnecessary utility degradation and extends optimization to content-agnostic regions. To address these limitations, we propose PALU (Prefix-Aware Localized Unlearning), a framework driven by a local entropy maximization objective across both temporal and vocabulary dimensions. PALU reveals that (i) suppressing the sensitive prefix alone is sufficient to sever the causal generation link, and (ii) flattening only the top-$k$ logits is adequate to maximize uncertainty in the critical subspace. These findings allow PALU to avoid redundant optimization across the full vocabulary and parameter space while minimizing collateral damage to general model performance. Extensive experiments validate that PALU achieves superior forgetting efficacy and utility preservation compared to state-of-the-art baselines.", "AI": {"tldr": "PALU是一个新的机器遗忘框架，通过局部熵最大化在时间和词汇维度上处理敏感知识，仅抑制敏感前缀和top-k logits，在保持模型通用性能的同时实现高效遗忘。", "motivation": "现有方法对响应中的所有token进行无差别处理，导致不必要的性能下降和内容无关区域的优化扩展。", "method": "提出PALU框架，采用局部熵最大化目标，仅抑制敏感前缀和top-k logits来切断因果生成链并最大化关键子空间的不确定性。", "result": "大量实验验证PALU在遗忘效果和性能保持方面优于现有最先进基线方法。", "conclusion": "PALU通过局部化处理避免了全词汇表和参数空间的冗余优化，最小化了对通用模型性能的附带损害。"}}
{"id": "2601.03192", "pdf": "https://arxiv.org/pdf/2601.03192", "abs": "https://arxiv.org/abs/2601.03192", "authors": ["Shengtao Zhang", "Jiaqian Wang", "Ruiwen Zhou", "Junwei Liao", "Yuchen Feng", "Weinan Zhang", "Ying Wen", "Zhiyu Li", "Feiyu Xiong", "Yutao Qi", "Bo Tang", "Muning Wen"], "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory", "categories": ["cs.CL"], "comment": "23 pages, 11 figures", "summary": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.", "AI": {"tldr": "MemRL是一个通过非参数强化学习在情景记忆上实现智能体自我进化的框架，解决了LLM在自我进化中的计算成本高和灾难性遗忘问题，显著优于现有方法。", "motivation": "人类智能的核心是通过构建性情景模拟来掌握新技能，但现有大语言模型在自我进化方面存在困难：微调计算成本高且容易灾难性遗忘，基于记忆的方法依赖被动语义匹配容易检索到噪声。", "method": "MemRL框架将冻结LLM的稳定推理与可塑性记忆分离，采用两阶段检索机制：先通过语义相关性过滤候选，再基于学习的Q值（效用）选择，通过环境反馈以试错方式持续优化效用值。", "result": "在HLE、BigCodeBench、ALFWorld和Lifelong Agent Bench上的大量实验表明，MemRL显著优于最先进的基线方法。", "conclusion": "MemRL有效解决了稳定性-可塑性困境，实现了无需权重更新的持续运行时改进，为智能体的自我进化提供了有效解决方案。"}}
{"id": "2601.03194", "pdf": "https://arxiv.org/pdf/2601.03194", "abs": "https://arxiv.org/abs/2601.03194", "authors": ["Mohammad Zia Ur Rehman", "Sai Kartheek Reddy Kasu", "Shashivardhan Reddy Koppula", "Sai Rithwik Reddy Chirra", "Shwetank Shekhar Singh", "Nagendra Kumar"], "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework", "categories": ["cs.CL"], "comment": "Accepted in the proceedings of AAAI 2026", "summary": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST", "AI": {"tldr": "提出X-MuTeST框架，结合大语言模型的高层语义推理和传统注意力增强技术，为印地语、泰卢固语和英语的仇恨言论检测提供可解释性指导训练，利用人工标注的词汇级理由提升分类性能和可解释性。", "motivation": "社交媒体仇恨言论检测在准确性和可解释性方面面临挑战，特别是对于未被充分研究的印度语言，需要同时提高检测性能和模型解释能力。", "method": "提出X-MuTeST可解释性指导训练框架，结合LLM语义推理和注意力增强技术，通过计算原始文本与n-gram预测概率差异生成解释，并将LLM解释与X-MuTeST解释结合。", "result": "利用人工理由进行训练提升了分类性能和可解释性，结合人工理由和可解释性方法进一步改进了模型注意力。在6,004个印地语、4,492个泰卢固语和6,334个英语样本上进行了评估。", "conclusion": "该工作通过关注资源匮乏语言，推动了跨不同语言环境的仇恨言论检测发展，提供了包含词汇级理由标注的数据集和开源代码。"}}
{"id": "2601.03217", "pdf": "https://arxiv.org/pdf/2601.03217", "abs": "https://arxiv.org/abs/2601.03217", "authors": ["Xinghe Chen", "Naiming Liu", "Shashank Sonkar"], "title": "MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics", "categories": ["cs.CL"], "comment": null, "summary": "Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student's next answer under cross-template rephrasing. Across nine language models (4B-120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10-21%, while providing student step traces improves prediction by 3-15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.", "AI": {"tldr": "MalruleLib是一个将数学学习中的常见错误概念转化为可执行程序的框架，能够生成包含正确和错误推理步骤的双路径追踪，用于评估AI模型在学生错误推理方面的表现。", "motivation": "学生在数学学习中经常出现系统性错误，会重复应用错误但一致的程序。需要一种方法来建模这些错误概念，以便AI系统能够更好地理解学生思维并提供针对性反馈。", "method": "基于67个学习科学和数学教育文献，将记录的误解转化为可执行程序，创建101个错误规则和498个参数化问题模板，生成成对的正误推理路径追踪。", "result": "在9个语言模型(4B-120B)测试中，跨模板错误概念预测准确率从直接解题的66%下降到40%，提供学生步骤追踪可将预测准确率提高3-15%。", "conclusion": "MalruleLib为教育AI提供了基础设施，能够跨情境建模学生程序，支持针对根本误解的诊断和反馈，可生成超过100万个实例用于规模化监督和受控评估。"}}
{"id": "2601.03248", "pdf": "https://arxiv.org/pdf/2601.03248", "abs": "https://arxiv.org/abs/2601.03248", "authors": ["Juntong Ni", "Shiyu Wang", "Ming Jin", "Qi He", "Wei Jin"], "title": "STReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning", "categories": ["cs.CL"], "comment": "preprint, we release our code publicly at https://github.com/LingFengGold/STReasoner", "summary": "Spatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address the gap, we introduce ST-Bench, a benchmark consisting of four core tasks, including etiological reasoning, entity identification, correlation reasoning, and in-context forecasting, developed via a network SDE-based multi-agent data synthesis pipeline. We then propose STReasoner, which empowers LLM to integrate time series, graph structure, and text for explicit reasoning. To promote spatially grounded logic, we introduce S-GRPO, a reinforcement learning algorithm that rewards performance gains specifically attributable to spatial information. Experiments show that STReasoner achieves average accuracy gains between 17% and 135% at only 0.004X the cost of proprietary models and generalizes robustly to real-world data.", "AI": {"tldr": "ST-Bench基准测试和STReasoner模型通过融合时序、图结构和文本信息，以低成本实现显著的时空推理性能提升", "motivation": "现有研究过于注重预测精度而忽视推理能力，时空推理领域发展不足，但在交通网络、电网和疾病传播等高风险决策系统中至关重要", "method": "提出ST-Bench基准(包含四个核心任务)，开发基于网络SDE的多智能体数据合成管道，构建STReasoner模型整合时序、图结构和文本信息，引入S-GRPO强化学习算法奖励空间信息的性能增益", "result": "STReasoner在仅0.004倍专有模型成本下实现17%-135%的平均准确率提升，并在真实数据上表现出强大的泛化能力", "conclusion": "该方法成功解决了时空推理中推理能力不足的问题，为高风险决策系统提供了有效的解决方案，证明了整合多模态信息进行显式推理的可行性"}}
{"id": "2601.03254", "pdf": "https://arxiv.org/pdf/2601.03254", "abs": "https://arxiv.org/abs/2601.03254", "authors": ["Bastien Vanderplaetse", "Xavier Siebert", "Stéphane Dupont"], "title": "Automated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation", "categories": ["cs.CL"], "comment": null, "summary": "The field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, which extracts relevant patterns in messages exchanged by agents trained with two different datasets on the Lewis Game, which is often studied in the context of emergent communication. ASRD helps at the interpretation of the emergent communication by relating the extracted patterns to specific attributes of the input data, thereby considerably simplifying subsequent analysis.", "AI": {"tldr": "本文提出ASRD算法，用于自动检测多智能体系统中涌现语言的语义规则，提高通信策略的可解释性。", "motivation": "当前涌现通信研究缺乏对语言可解释性的关注，需要工具来解释自主智能体开发的通信策略。", "method": "使用ASRD算法分析在Lewis Game中训练的两个不同数据集上智能体交换消息的模式，将模式与输入数据的特定属性关联。", "result": "ASRD能够提取相关模式并简化后续分析。", "conclusion": "ASRD算法显著提升了涌现通信的解释能力，为理解自主智能体开发的通信策略提供了有效工具。"}}
