<div id=toc></div>

# 目录

- [cs.AI](#cs.AI) [总数: 67]
- [cs.CL](#cs.CL) [总数: 76]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions](https://arxiv.org/abs/2512.00097)
*Boyan Duan, Xiao Liang, Shuai Lu, Yaoxiang Wang, Yelong Shen, Kai-Wei Chang, Ying Nian Wu, Mao Yang, Weizhu Chen, Yeyun Gong*

**主要类别:** cs.AI

**AI概要:** HAGeo方法通过启发式辅助构造策略，在欧几里得几何定理证明中实现高效CPU计算，无需神经网络，在IMO-30基准上达到金牌水平表现，超越AlphaGeometry，并建立了更具挑战性的HAGeo-409基准。


<details>
  <summary>更多</summary>
  
**动机:** 解决欧几里得几何定理自动证明的挑战，特别是IMO级别问题，探索不依赖神经网络的高效CPU计算方法。

**方法:** 提出HAGeo方法：基于启发式的辅助构造策略，通过简单随机添加辅助点策略实现几何演绎，完全在CPU上运行。

**结果:** 在IMO-30基准上解决了30题中的28题，达到金牌水平表现，显著超越基于神经网络的AlphaGeometry方法。建立了包含409个问题的HAGeo-409基准，提供更精确的评估。

**结论:** HAGeo方法证明了基于启发式的几何定理证明方法在效率和性能上的优势，为几何自动推理设立了新的高标准，展示了CPU计算在复杂几何问题解决中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Gold-Medal-Level+Olympiad+Geometry+Solving+with+Efficient+Heuristic+Auxiliary+Constructions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00097，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00097&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.

</details>


### [2] [Chunking Strategies for Multimodal AI Systems](https://arxiv.org/abs/2512.00185)
*Shashanka B R, Mohith Charan R, Seema Banu F*

**主要类别:** cs.AI

**AI概要:** 本文对多模态分块策略进行了全面的技术调研和分类分析，涵盖了文本、图像、音频、视频和跨模态数据的分块方法，为开发更高效的多模态AI系统提供技术基础和设计空间。


<details>
  <summary>更多</summary>
  
**动机:** 整合多模态分块策略的研究现状，为研究人员和从业者提供技术基础，开发更有效和高效的多模态AI系统，推动在真实应用中处理准确性、生成连贯性和可扩展性方面的创新。

**方法:** 提供全面的分类体系和技术分析，针对每种模态（文本、图像、音频、视频、跨模态数据）分析经典和现代方法，包括固定大小令牌窗口、递归文本分割、以对象为中心的视觉分块、基于静音的音频分割和视频场景检测等方法。

**结果:** 分析了各种分块方法的底层方法论、支持工具（如LangChain、Detectron2、PySceneDetect）、优势以及挑战，特别是粒度-上下文权衡和多模态对齐方面的挑战。

**结论:** 探索了新兴的跨模态分块策略，比较了不同方法的优缺点，指出了异步信息密度和噪声对齐信号等开放性问题，并确定了未来在自适应、基于学习和任务特定分块方面的研究机会。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chunking+Strategies+for+Multimodal+AI+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00185&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.

</details>


### [3] [A Rosetta Stone for AI Benchmarks](https://arxiv.org/abs/2512.00193)
*Anson Ho, Jean-Stanislas Denain, David Atanasov, Samuel Albanie, Rohin Shah*

**主要类别:** cs.AI

**AI概要:** N/A


<details>
  <summary>更多</summary>
  
**动机:** N/A

**方法:** N/A

**结果:** N/A

**结论:** N/A

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Rosetta+Stone+for+AI+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00193&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a "Rosetta Stone", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.

</details>


### [4] [Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?](https://arxiv.org/abs/2512.00218)
*Matt MacDermott, Qiyao Wei, Rada Djoneva, Francis Rhys Ward*

**主要类别:** cs.AI

**AI概要:** 该研究探讨了不同训练激励对AI推理模型可监控性的影响，发现对抗性优化会降低监控性能，而直接优化可监控性并不能可靠提升监控效果。


<details>
  <summary>更多</summary>
  
**动机:** AI系统输出自然语言推理过程为安全监控提供了机会，但推理链是否能真实反映底层推理过程以及可监控程度受训练方式影响，需要研究不同训练激励如何影响监控能力。

**方法:** 提出了一种新的监控性测量方法，通过监控器能否使用模型推理预测关键潜在变量来评估监控性。研究比较了长度惩罚、KL正则化、对抗性优化和直接优化可监控性等不同训练激励的效果。

**结果:** 在控制准确性的情况下，常用激励（长度惩罚和KL正则化）没有发现一致影响；对抗性优化会降低监控性能；直接优化可监控性不能可靠带来改进。

**结论:** 训练激励对AI推理模型的可监控性有重要影响，特别是对抗性优化会损害监控能力，这为开发更安全的AI系统提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+Under+Pressure%3A+How+do+Training+Incentives+Influence+Chain-of-Thought+Monitorability%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00218，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00218&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.

</details>


### [5] [Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking](https://arxiv.org/abs/2512.00267)
*Anab Maulana Barik, Shou Ziyi, Yang Kaiwen, Yang Qi, Shen Xin*

**主要类别:** cs.AI

**AI概要:** 提出Trification框架，通过生成完整的验证操作集和构建依赖图来改进自动事实核查，显著提升准确性


<details>
  <summary>更多</summary>
  
**动机:** 现有方法无法验证声明的每个组成部分，且缺乏结构化框架来逻辑连接子任务结果进行最终预测

**方法:** 生成全面的验证操作集确保声明完整覆盖，将操作结构化到依赖图中建模逻辑交互，支持动态修改验证策略

**结果:** 在两个挑战性基准测试中显著提高了事实核查准确性，推进了自动事实核查系统的当前最先进水平

**结论:** Trification框架通过结构化依赖图和动态策略调整，有效解决了现有方法的局限性，为自动事实核查系统提供了更可靠的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Trification%3A+A+Comprehensive+Tree-based+Strategy+Planner+and+Structural+Verification+for+Fact-Checking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00267，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00267&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.

</details>


### [6] [ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning](https://arxiv.org/abs/2512.00305)
*Zhengzhuo Xu, SiNan Du, Yiyan Qi, SiwenLu, Chengjin Xu, Chun Yuan, Jian Guo*

**主要类别:** cs.AI

**AI概要:** PointCoT方法通过结合视觉定位和思维链推理，解决了多模态大语言模型在图表理解中的数值幻觉问题，在多个基准测试中表现优异


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态大语言模型在图表理解时过度依赖OCR提取的文本内容，当图表文本标注稀疏时会产生数值幻觉，缺乏对图表元素和比例关系的视觉感知能力

**方法:** 提出PointCoT方法，通过提示MLLMs生成边界框和基于位置标注重新渲染图表，建立文本推理步骤与视觉定位区域的连接。构建了包含62K高质量样本的ChartPoint-SFT数据集，包含逐步思维链、边界框和重新渲染的可视化

**结果:** 开发的ChartPointQ2和ChartPointQ2.5模型在多个图表基准测试中超越最先进方法，如在ChartBench上提升5.04%

**结论:** PointCoT通过整合反射式交互和视觉定位，有效解决了MLLMs在图表理解中的根本挑战，为图表分析任务提供了更可靠的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ChartPoint%3A+Guiding+MLLMs+with+Grounding+Reflection+for+Chart+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00305，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00305&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\% on ChartBench.

</details>


### [7] [RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs](https://arxiv.org/abs/2512.00319)
*Ruike Hu, Shulei Wu*

**主要类别:** cs.AI

**AI概要:** 提出一种轻量级强化学习框架，通过多维奖励函数和梯度正则化策略优化，解决LLMs生成结构化数据时的结构鸿沟问题，显著提升JSON有效性和结构准确性。


<details>
  <summary>更多</summary>
  
**动机:** 解决LLMs在自动化软件生态系统中集成时面临的"结构鸿沟"问题，即概率性token生成与确定性结构化数据格式要求之间的冲突，传统方法存在结构错误或推理延迟问题。

**方法:** 使用轻量级强化学习框架，引入多维奖励函数将结构化输出任务分解为结构完整性、格式正确性、内容准确性和有效性四个约束层次，采用梯度正则化策略优化(GRPO)无需单独critic网络。

**结果:** 在复杂配方生成和结构化数学推理任务上验证，达到89.7%的结构准确性和92.1%的JSON有效性，显著优于零样本基线和SFT方法，VRAM使用减少40%。

**结论:** 该方法有效解决了LLMs生成结构化数据的问题，训练动态分析显示模型先获得语法熟练度再获得语义准确性，提供了一个高效的结构化输出解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RL-Struct%3A+A+Lightweight+Reinforcement+Learning+Framework+for+Reliable+Structured+Output+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00319，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00319&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the "Structure Gap" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to "hallucinated" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.

</details>


### [8] [CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System](https://arxiv.org/abs/2512.00331)
*Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan*

**主要类别:** cs.AI

**AI概要:** 论文提出了CogEvo-Edu系统，这是一个分层教育多智能体系统，通过联合演化学生档案、知识库和教学策略，显著提升了STEM教育中LLM对话导师的性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前STEM教育中部署的LLM对话导师大多依赖单一的静态检索增强生成(RAG)管道，在复杂领域如数字信号处理(DSP)中表现不佳，难以维持连贯的长期学生模型、管理异构知识库和适应长期互动中的教学策略。

**方法:** 提出CogEvo-Edu系统，包含三个层次：认知感知层(CPL)维护双重记忆并进行置信度加权整合；知识演化层(KEL)为知识块分配时空价值；元控制层(MCL)将教学制定为分层顺序决策过程。构建了DSP-EduBench基准进行验证。

**结果:** 使用三模型LLM-as-a-Judge集成评估，CogEvo-Edu将总体评分从5.32提升到9.23，在六个指标上均优于静态RAG、简单记忆和单智能体变体。

**结论:** 研究表明将检索、记忆和控制视为耦合的认知演化过程是有效的，CogEvo-Edu系统证明了联合演化学生档案、知识库和教学策略的价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CogEvo-Edu%3A+Cognitive+Evolution+Educational+Multi-Agent+Collaborative+System，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00331&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

</details>


### [9] [Echo-N1: Affective RL Frontier](https://arxiv.org/abs/2512.00344)
*Naifan Zhang, Ruihan Sun, Ruixi Su, Shiqi Ma, Shiya Zhang, Xianna Weng, Xiaofan Zhang, Yuhan Zhan, Yuyang Xu, Zhaohan Chen, Zhengyuan Pan, Ziyi Song*

**主要类别:** cs.AI

**AI概要:** 该论文提出了首个实时推断用户个性并优化模型行为以满足个性化对话偏好的RL框架，证明了在主观对话领域RL的有效性，其模型Echo-N1在人性化交互质量上显著超越基础版本和商业模型。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM领域过度关注数学、代码等机器已擅长的确定性推理任务，而忽略了定义人类智能的主观、情感化、个性化对话领域，该领域因难以形式化而被认为不适合传统RL方法。

**方法:** 提出了一个框架，能够实时推断用户个性，并基于此优化模型行为以适应个性化对话偏好；引入了动态情商评估套件来量化改进效果。

**结果:** 方法在非可验证环境中产生了持续、稳健且显著的改进，Echo-N1模型表现远超其基础版本，并优于专有的Doubao 1.5 Character模型。

**结论:** 这项工作为RL开辟了新前沿：优化模型以适应对话中深度主观、深度人性化的维度，证明主观对话领域是可解决且具有变革性的RL问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Echo-N1%3A+Affective+RL+Frontier，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00344，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00344&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.

</details>


### [10] [Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models](https://arxiv.org/abs/2512.00349)
*Sitong Fang, Shiyi Hou, Kaile Wang, Boyuan Chen, Donghai Hong, Jiayi Zhou, Josef Dai, Yaodong Yang, Jiaming Ji*

**主要类别:** cs.AI

**AI概要:** 该论文揭示了多模态大语言模型中存在的欺骗风险，提出了首个多模态欺骗评估基准MM-DeceptionBench，并开发了基于多智能体辩论的图像辩论监控框架来检测此类欺骗行为。


<details>
  <summary>更多</summary>
  
**动机:** 随着AI系统能力提升，欺骗性行为从文本扩展到多模态环境，带来更隐蔽和破坏性的安全风险。当前研究主要局限于文本领域，多模态欺骗风险尚未被系统探索。

**方法:** 1) 创建MM-DeceptionBench基准，覆盖六类欺骗行为，评估模型如何通过视觉和文本模态组合进行策略性操纵和误导；2) 提出"图像辩论"多智能体辩论监控框架，强制模型基于视觉证据支持其主张。

**结果:** 实验表明，提出的监控方法在所有测试模型中都显著提高了与人类判断的一致性，将Cohen's kappa提高了1.5倍，准确率提高了1.25倍（在GPT-4o上）。

**结论:** 多模态欺骗是一个严重且被忽视的安全威胁，需要专门的监控方法。提出的辩论框架有效提升了欺骗行为的可检测性，为多模态AI系统的安全评估提供了重要工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Debate+with+Images%3A+Detecting+Deceptive+Behaviors+in+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00349，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00349&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.

</details>


### [11] [GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework](https://arxiv.org/abs/2512.00406)
*Pengyu Zeng, Yuqin Dai, Jun Yin, Jing Zhong, Ziyang Han, Chaoyang Shi, ZhanXiang Jin, Maowei Jiang, Yuxing Han, Shuai Lu*

**主要类别:** cs.AI

**AI概要:** GreenPlanner是一个统一设计评估与生成的能源和功能感知生成框架，通过自动化评估和生成符合规范的建筑平面图，大幅提升设计效率和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 建筑设计直接影响人类福祉和碳排放，但现有的空间功能和能源合规平面图生成方法仍依赖人工，成本高且不可扩展。现有方法虽然能生成视觉上合理的布局，但经常违反关键约束，由于缺乏自动化评估而产生无效结果。

**方法:** GreenPlanner包含四个核心组件：1)带标签的设计可行性数据集用于学习约束先验；2)快速实用设计评估器(PDE)预测能源性能和空间功能有效性；3)通过PDE引导过滤得到的绿色平面数据集(GreenPD)；4)基于GreenPD训练并使用PDE反馈的GreenFlow生成器，实现可控且符合规范的生成。

**结果:** 实验表明，GreenPlanner将评估速度提升了10^5倍以上，准确率超过99%，完全消除了无效样本，相比专业建筑师的设计效率提升了87%。

**结论:** GreenPlanner通过将设计评估与生成过程统一，成功解决了建筑平面图自动生成中的约束违反问题，为可持续建筑设计提供了高效、准确的自动化解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GreenPlanner%3A+Practical+Floorplan+Layout+Generation+via+an+Energy-Aware+and+Function-Feasible+Generative+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00406&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.

</details>


### [12] [Mind the data gap: Missingness Still Shapes Large Language Model Prognoses](https://arxiv.org/abs/2512.00479)
*Yuta Kobayashi, Vincent Jeanselme, Shalmali Joshi*

**主要类别:** cs.AI

**AI概要:** 论文研究发现医疗数据缺失模式显著影响大语言模型的零样本预测性能，缺失指示符的显式包含对不同规模模型产生不一致影响，大模型受益而小模型可能受损。


<details>
  <summary>更多</summary>
  
**动机:** 研究缺失数据模式对大语言模型性能的影响，特别是在医疗领域中数据收集决策导致的系统性缺失对LLM预测能力的影响尚未被充分研究。

**方法:** 通过在哥伦比亚大学医学中心和MIMIC-IV数据集上进行一系列实验，分析缺失指示符在提示中的显式包含对LLM零样本预测性能和校准的影响。

**结果:** 缺失模式显著影响零样本预测性能，缺失指示符的显式包含对某些LLM有益而对其他LLM有害，表现出不一致的影响模式。大模型从这种干预中受益，而小模型可能受到负面影响。

**结论:** 需要更透明的记录和系统评估缺失数据表示对下游性能的影响，LLM范式可能进一步掩盖缺失数据的影响，这一问题在传统机器学习中也常被忽视。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mind+the+data+gap%3A+Missingness+Still+Shapes+Large+Language+Model+Prognoses，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00479，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00479&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.

</details>


### [13] [Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization](https://arxiv.org/abs/2512.00601)
*Boyang Gu, Hongjian Zhou, Bradley Max Segal, Jinge Wu, Zeyu Cao, Hantao Zhong, Lei Clifton, Fenglin Liu, David A. Clifton*

**主要类别:** cs.AI

**AI概要:** 提出了CRPO方法，一种多目标、可验证的强化学习技术，用于临床领域LLM的后训练，在保持准确性的同时显著提升真实性和完整性。


<details>
  <summary>更多</summary>
  
**动机:** 当前后训练方法主要奖励正确性，但临床等高风险领域需要多维度目标（准确性、真实性、完整性），现有方法无法满足这些临床推理原则。

**方法:** 开发了Clinical-Objective Relative Policy Optimization (CRPO)，整合基于规则和可验证的奖励信号，无需人工标注即可联合优化准确性、真实性和完整性。训练了3B参数的Clinical-R1-3B模型。

**结果:** 在三个基准测试中，CRPO相比标准GRPO在真实性和完整性方面显著提升，同时保持准确性改进。

**结论:** CRPO为LLM推理与临床目标对齐提供了可扩展路径，使医疗AI系统更安全、更具协作性，展示了多目标可验证RL方法在医学领域LLM后训练中的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Clinical-R1%3A+Empowering+Large+Language+Models+for+Faithful+and+Comprehensive+Reasoning+with+Clinical+Objective+Relative+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00601&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.

</details>


### [14] [EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients](https://arxiv.org/abs/2512.00670)
*He-Yen Hsieh, Hong Wang, H. T. Kung*

**主要类别:** cs.AI

**AI概要:** EDIT方法通过监测推理过程中的激活对齐情况，在达到足够稳定性时提前终止扩散推理，显著减少推理步骤(11.8%-68.3%)，同时保持或提升准确性，存储开销仅0.02%


<details>
  <summary>更多</summary>
  
**动机:** 基于扩散的大语言模型通过迭代去噪生成token，但答案通常在所有步骤完成前就已稳定。现有方法在模型发布时丢弃了训练过程中的参数重要性元数据

**方法:** EDIT在推理时监测token激活与训练时通过AdamW聚合LoRA更新得到的推理图谱之间的对齐度，当连续步骤间KL散度低于阈值时检测收敛

**结果:** 在多个推理基准测试中，EDIT减少11.8%-68.3%的扩散步骤，在大多数设置中保持或提高准确性，存储开销约为0.02%(约1.5-2MB)

**结论:** 通过利用训练梯度动态，EDIT为减少dLLM推理时间和成本开辟了新的研究方向，实现了显著的效率提升

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EDIT%3A+Early+Diffusion+Inference+Termination+for+dLLMs+Based+on+Dynamics+of+Training+Gradients，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00670&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.

</details>


### [15] [Model of human cognition](https://arxiv.org/abs/2512.00683)
*Wu Yonggang*

**主要类别:** cs.AI

**AI概要:** 提出了一个神经理论框架来解释智能系统的涌现，旨在解决大语言模型缺乏可解释性、缺乏统一理论和运营成本高的问题，提供认知过程的理论洞察和计算高效的可解释AI方法


<details>
  <summary>更多</summary>
  
**动机:** 解决大语言模型存在的三个主要限制：缺乏可解释性、缺乏统一理论框架以及高昂的运营成本

**方法:** 提出一个神经理论框架，该框架既功能强大又具有生物学合理性，用于解释智能在系统中的涌现机制

**结果:** 该模型提供了对决策和问题解决等认知过程的理论洞察，并为创建可解释和可泛化的人工智能提供了计算高效的方法

**结论:** 该神经理论框架为解决当前AI系统的局限性提供了新的理论和方法论基础，有望推动更高效、更可解释的人工智能发展

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Model+of+human+cognition，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00683，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00683&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.

</details>


### [16] [When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF](https://arxiv.org/abs/2512.00709)
*Yifan Xu, Xichen Ye, Yifan Chen, Qiaosheng Zhang*

**主要类别:** cs.AI

**AI概要:** FA-DPO算法：针对人类反馈中普遍存在的偏好翻转问题，从RLHF角度提出的翻转感知直接偏好优化方法，通过建模实例相关翻转概率和迭代优化提升对齐算法的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 大规模语言模型对齐中数据集质量至关重要，但人类反馈收集过程中普遍存在偏好翻转问题，导致数据标注被污染，需要开发对翻转对具有更强鲁棒性的对齐算法。

**方法:** 从RLHF视角将人类意图模型和外部因素引入的偏好翻转机制分解为两个独立阶段；基于Bradley-Terry模型引入实例相关翻转概率；利用偏好标注相关特征捕捉判断不确定性和建模翻转模式；设计简单高效的迭代优化算法，与原始RLHF和DPO算法兼容。

**结果:** 在多种情况下研究了实例相关偏好翻转模型，评估了所提方法及其他基线方法的性能。

**结论:** FA-DPO算法通过系统建模偏好翻转机制和设计高效优化方法，有效提升了语言模型对齐算法对数据标注中翻转问题的鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Human+Preferences+Flip%3A+An+Instance-Dependent+Robust+Loss+for+RLHF，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00709，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00709&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.

</details>


### [17] [SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs](https://arxiv.org/abs/2512.00722)
*Jiaming Xu, Jiayi Pan, Hanzhen Wang, Yongkang Zhou, Jiancai Ye, Yu Wang, Guohao Dai*

**主要类别:** cs.AI

**AI概要:** SpeContext提出了一种利用蒸馏语言模型作为检索算法的新范式，通过算法、系统和编译三个层面的协同设计，在长上下文推理任务中实现了显著的性能提升和参数减少。


<details>
  <summary>更多</summary>
  
**动机:** 发现检索算法与LLM对齐的目标类似于LLM知识蒸馏的目标，从信息论角度分析蒸馏语言模型与原始LLM的信息关注相似性，从而提出使用DLM作为检索算法的新方法。

**方法:** 1) 算法层面：基于DLM头部注意力权重设计轻量级检索头，通过剪枝冗余减少90%以上参数；2) 系统层面：通过弹性加载策略设计异步预取数据流，有效重叠KV缓存检索与LLM计算；3) 编译层面：构建理论内存模型并实现自适应内存管理系统，最大化GPU内存利用率。

**结果:** 在云端和边缘两种资源受限环境中部署评估，相比Huggingface框架，SpeContext在云端实现24.89倍吞吐量提升，在边缘实现10.06倍加速，准确率损失可忽略不计，推进了准确率与吞吐量的帕累托前沿。

**结论:** SpeContext通过创新的DLM检索范式和三层次协同设计，为长上下文推理提供了高效解决方案，在保持准确性的同时显著提升了性能表现，适用于资源受限的实际部署场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SpeContext%3A+Enabling+Efficient+Long-context+Reasoning+with+Speculative+Context+Sparsity+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00722，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00722&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.

</details>


### [18] [Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens](https://arxiv.org/abs/2512.00729)
*Yuxiang Chen, Zuohan Wu, Ziwei Wang, Xiangning Yu, Xujia Li, Linyi Yang, Mengyue Yang, Jun Wang, Lei Chen*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个基于人类心智过程的分类法来解析大型推理模型的心理特征，包含5组17个类别，并创建了包含277,534个推理步骤的数据集。研究发现当前模型的自检机制效果有限，建议采用多步反思机制。同时开发了CAPO自动标注框架来提高分析效率。


<details>
  <summary>更多</summary>
  
**动机:** 受大型推理模型在推理过程中表现出类人行为的启发，研究者希望从跨学科视角建立分类体系来深入理解这些模型的"心理"特征。

**方法:** 提出包含5组17个类别的综合分类法，基于人类心智过程设计；构建包含277,534个原子推理步骤的标注数据集；开发CAPO自动标注框架利用大语言模型进行基于分类法的自动标注。

**结果:** 分析发现当前流行的答案后"双重检查"（自我监控评估）大多是表面的，很少产生实质性修改；CAPO框架与人类专家的一致性高于基线方法，实现了从人类认知角度对大型推理模型的可扩展分析。

**结论:** 激励全面的多步反思而非简单的自我监控可能是更有效的改进路径；分类法、CAPO框架和所得见解为理解和推进大型推理模型的推理能力提供了原则性、可扩展的路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probing+the+%22Psyche%27%27+of+Large+Reasoning+Models%3A+Understanding+Through+a+Human+Lens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00729，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00729&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.

</details>


### [19] [MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents](https://arxiv.org/abs/2512.00756)
*Ruihan Chen, Qiming Li, Xiaocheng Feng, Xiaoliang Yang, Weihong Zhong, Yuxuan Gu, Zekun Zhou, Bing Qin*

**主要类别:** cs.AI

**AI概要:** 该论文提出了MPR-GUI-Bench多语言细粒度GUI基准测试和GUI-XLI跨语言干预方法，以解决大型视觉语言模型在非英语GUI任务中性能下降的问题，平均提升6.5%的多语言感知推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 大型视觉语言模型在英语GUI任务中表现出色，但在多语言环境下性能显著下降，且现有研究缺乏对控件功能和空间关系等细粒度分析，限制了其全球化应用。

**方法:** 提出MPR-GUI-Bench基准测试进行细粒度评估，并设计GUI-XLI方法，通过在感知推理相关层的隐藏状态施加干预，减少不同语言在潜在空间中的差异。

**结果:** 评估结果显示LVLMs在非英语语言中的性能明显低于英语，而GUI-XLI方法平均提升了6.5%的多语言感知推理能力。

**结论:** 该方法有效缓解了多语言GUI任务中的性能差距，为提升大型视觉语言模型的全球化应用提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MPR-GUI%3A+Benchmarking+and+Enhancing+Multilingual+Perception+and+Reasoning+in+GUI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00756，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00756&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.

</details>


### [20] [BioPro: On Difference-Aware Gender Fairness for Vision-Language Models](https://arxiv.org/abs/2512.00807)
*Yujie Lin, Jiayao Ma, Qingguo Hu, Derek F. Wong, Jinsong Su*

**主要类别:** cs.AI

**AI概要:** 该论文提出了BioPro框架，通过差异感知公平性方法在视觉语言模型中实现选择性去偏，在需要中性的场景中减少性别偏见，同时在明确场景中保留合理的性别特征区分。


<details>
  <summary>更多</summary>
  
**动机:** 现有公平性干预方法采用差异无感知视角，对所有人口统计群体实施统一处理，无法区分需要中性的场景和需要保留群体特定属性的场景。

**方法:** 提出BioPro（Bias Orthogonal Projection）框架，通过反事实嵌入识别低维性别变化子空间，并应用投影选择性地中和性别相关信息。该框架完全无需训练。

**结果:** 实验表明BioPro能有效减少中性场景中的性别偏见，同时在明确场景中保持性别忠实度，并可推广到连续偏见变量如场景亮度。

**结论:** BioPro为实现视觉语言模型中的选择性公平性提供了有前景的方向，具有更广泛的适用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BioPro%3A+On+Difference-Aware+Gender+Fairness+for+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00807&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.

</details>


### [21] [Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning](https://arxiv.org/abs/2512.00818)
*Haozhen Gong, Xiaozhong Ji, Yuansen Liu, Wenbin Wu, Xiaoxiao Yan, Jingjing Liu, Kai Wu, Jiazhen Pan, Bailiang Jian, Jiangning Zhang, Xiaobin Hu, Hongwei Bran Li*

**主要类别:** cs.AI

**AI概要:** Med-CMR是一个细粒度的医学复杂多模态推理基准，通过系统能力分解、挑战性任务设计和高质量数据覆盖，评估MLLM在医疗领域的复杂推理能力。GPT-5表现最佳，但专业医疗MLLM并不总是优于通用模型。


<details>
  <summary>更多</summary>
  
**动机:** MLLM开始应用于临床工作流程，但其执行复杂医学推理的能力尚不清楚，需要细粒度的评估基准来测试其在医疗多模态推理中的表现。

**方法:** 创建Med-CMR基准，包含20,653个VQA对，涵盖11个器官系统和12种成像模态。通过系统能力分解将医学多模态推理分为细粒度视觉理解和多步推理，设计具有挑战性的任务，并采用两阶段（专家+模型辅助）审查确保临床真实性。

**结果:** 评估18个最先进的MLLM，GPT-5表现最佳：多选题准确率57.81%，开放式问题得分48.70%，优于Gemini 2.5 Pro和Qwen3-VL-235B-A22B。但专业医疗MLLM并不总是优于通用模型，长尾泛化是主要失败模式。

**结论:** Med-CMR为医学MLLM的视觉推理整合和罕见病例鲁棒性提供了压力测试，是未来临床系统的严格基准，揭示了当前模型在复杂医疗推理中的局限性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Med-CMR%3A+A+Fine-Grained+Benchmark+Integrating+Visual+Evidence+and+Clinical+Logic+for+Medical+Complex+Multimodal+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00818&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.

</details>


### [22] [SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks](https://arxiv.org/abs/2512.00834)
*Lin Zhu, Kezhi Wang, Luping Xiang, Kun Yang*

**主要类别:** cs.AI

**AI概要:** 提出一种结合语义通信和智能代理的轨迹预测框架，在V2X网络中显著提升预测精度，特别是在低信噪比条件下准确率提升47.5%


<details>
  <summary>更多</summary>
  
**动机:** 传统通信方案传输开销和延迟大，现有轨迹预测模型缺乏环境感知和逻辑推理能力，需要更高效的信息交换和上下文推理方法

**方法:** 在V2I通信中，RSU通过特征提取代理和语义分析代理处理历史轨迹数据；在V2V通信中，车辆进行本地特征提取和语义分析，并结合邻居车辆的预测轨迹进行联合预测

**结果:** 在各种通信条件下的大量实验表明，该方法显著优于基线方案，在低信噪比条件下预测准确率提升高达47.5%

**结论:** 集成语义通信和Agentic AI的框架有效提升了车辆轨迹预测性能，为V2X网络提供了更高效可靠的信息交换和推理能力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SemAgent%3A+Semantic-Driven+Agentic+AI+Empowered+Trajectory+Prediction+in+Vehicular+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00834&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.

</details>


### [23] [Assessing model error in counterfactual worlds](https://arxiv.org/abs/2512.00836)
*Emily Howerton, Justin Lessler*

**主要类别:** cs.AI

**AI概要:** 本文提出了三种评估反事实情景建模中模型误差的方法，通过模拟实验对比分析各方法的优缺点，为反事实误差估计和情景设计提供建议。


<details>
  <summary>更多</summary>
  
**动机:** 反事实情景建模是未来规划的重要工具，但很少对情景预测进行回顾性评估。预测与观测的差异主要来自情景偏差和模型校准错误，后者对评估模型在决策中的价值更为重要。

**方法:** 提出并对比了三种估计反事实世界中模型误差的方法，通过模拟实验展示每种方法的优势和局限性。

**结果:** 通过模拟实验验证了三种方法的有效性，分析了它们在不同情况下的表现特点。

**结论:** 为反事实误差估计提供了具体建议，并讨论了使情景预测可评估所需的情景设计要素，强调了模型校准误差评估在决策支持中的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assessing+model+error+in+counterfactual+worlds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00836，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00836&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Counterfactual scenario modeling exercises that ask "what would happen if?" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.

</details>


### [24] [ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI](https://arxiv.org/abs/2512.00839)
*Fabrizio Maturo, Donato Riccio, Andrea Mazzitelli, Giuseppe Bifulco, Francesco Paolone, Iulia Brezeanu*

**主要类别:** cs.AI

**AI概要:** ARCADIA是一个基于大语言模型的因果发现框架，通过结合统计诊断和迭代优化来构建有效的时间连贯因果结构，在破产预测等高风险领域优于传统算法。


<details>
  <summary>更多</summary>
  
**动机:** 传统因果发现算法在处理现实世界高风险领域时缺乏稳定性和可解释性，需要一种能够结合大语言模型推理能力和统计验证的自主因果发现框架。

**方法:** 采用基于约束引导提示和因果有效性反馈的迭代优化方法，通过大语言模型推理与统计诊断相结合来逐步精炼候选有向无环图(DAGs)。

**结果:** 在企业破产数据实验中，ARCADIA相比NOTEARS、GOLEM和DirectLiNGAM等传统方法产生了更可靠的因果图，并提供完全可解释、支持干预的流程。

**结论:** ARCADIA展示了智能大语言模型如何参与自主科学建模和结构化因果推断，推动了AI在因果发现领域的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ARCADIA%3A+Scalable+Causal+Discovery+for+Corporate+Bankruptcy+Analysis+Using+Agentic+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00839，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00839&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.

</details>


### [25] [One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces](https://arxiv.org/abs/2512.00852)
*Yandong Sun, Qiang Huang, Ziwei Xu, Yiqun Sun, Yixuan Tang, Anthony K. H. Tung*

**主要类别:** cs.AI

**AI概要:** 论文提出了语义场子空间(SFS)和SAFARI算法，通过几何保持的上下文感知表示来揭示嵌入空间中的局部语义邻域和层次结构，在保持语义连贯性的同时提高了可解释性和计算效率。


<details>
  <summary>更多</summary>
  
**动机:** 现有的嵌入空间方法往往为了结构规整性而牺牲语义连贯性，或者为提高可解释性而带来高计算开销，需要一种既能保持几何结构又能有效分析语义的方法。

**方法:** 引入语义场子空间(SFS)作为几何保持的上下文感知表示，提出SAFARI无监督算法使用语义偏移度量来发现层次语义结构，并开发了高效的语义偏移近似计算方法替代昂贵的SVD计算。

**结果:** 在6个真实世界文本和图像数据集上的评估显示，SFS在分类和政治偏见检测等任务中优于标准分类器，SAFARI能够一致地揭示可解释和可泛化的语义层次结构，计算速度提升15-30倍且平均误差低于0.01。

**结论:** 这项工作为嵌入空间中的语义理解提供了统一的框架，能够在保持语义连贯性的同时实现结构化、分析和扩展，解决了现有方法在语义保持和计算效率方面的挑战。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是One+Swallow+Does+Not+Make+a+Summer%3A+Understanding+Semantic+Structures+in+Embedding+Spaces，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00852&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.

</details>


### [26] [Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing](https://arxiv.org/abs/2512.00881)
*Li Yuan, Qingfei Huang, Bingshan Zhu, Yi Cai, Qingbao Huang, Changmeng Zheng, Zikun Deng, Tao Wang*

**主要类别:** cs.AI

**AI概要:** MMQAKE是首个多模态多跳问答知识编辑基准，评估模型在多模态推理链中的中间步骤质量和视觉重述输入的鲁棒性。Hybrid-DMKG混合推理框架通过动态多模态知识图谱显著提升了现有方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有MKE基准主要评估最终答案正确性，但忽视了中间推理质量和视觉重述输入的鲁棒性，需要更全面的评估框架。

**方法:** 提出Hybrid-DMKG混合推理框架：使用大语言模型分解多跳问题，多模态检索模型定位更新事实，通过关系链接预测和RAG推理双路径进行混合推理，决策模块聚合证据选择答案。

**结果:** 在MMQAKE基准上，Hybrid-DMKG显著优于现有MKE方法，获得了更高的准确性和对知识更新的更好鲁棒性。

**结论:** MMQAKE基准填补了多模态知识编辑评估的空白，Hybrid-DMKG框架有效解决了多模态多跳推理中的知识更新和鲁棒性问题，为未来研究提供了重要基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid-DMKG%3A+A+Hybrid+Reasoning+Framework+over+Dynamic+Multimodal+Knowledge+Graphs+for+Multimodal+Multihop+QA+with+Knowledge+Editing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00881，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00881&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.

</details>


### [27] [Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models](https://arxiv.org/abs/2512.00918)
*Cen Lu, Yung-Chen Tang, Andrea Cavallaro*

**主要类别:** cs.AI

**AI概要:** 研究发现大视觉语言模型存在结构性脆弱性，仅需屏蔽语言模型中极少数关键神经元（最少4个）即可引发灾难性崩溃，关键神经元主要位于语言模型的下投影层而非视觉组件。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大视觉语言模型展现出强大的多模态理解能力，但其鲁棒性尚未得到充分研究，需要识别可能导致灾难性崩溃的关键神经元。

**方法:** 提出CAN方法，通过检测持续激活神经元和渐进式掩码定位关键神经元，在LLaVA-1.5-7b-hf和InstructBLIP-Vicuna-7b模型上进行实验。

**结果:** 实验发现仅屏蔽语言模型前馈网络中极小部分神经元（极端情况下仅需4个）就足以触发灾难性崩溃，关键神经元主要位于语言模型的下投影层。

**结论:** 研究揭示了大视觉语言模型的结构性脆弱性，发现了关键神经元的分布规律和两阶段崩溃模式，为LVLM安全研究提供了重要见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Minimal+neuron+ablation+triggers+catastrophic+collapse+in+the+language+core+of+Large+Vision-Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00918，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00918&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.

</details>


### [28] [Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE](https://arxiv.org/abs/2512.00969)
*Felix Saretzky, Lucas Andersen, Thomas Engel, Fazel Ansari*

**主要类别:** cs.AI

**AI概要:** 论文提出基于因果机器学习的方法，从预测性维护转向规范性维护，通过因果模型识别故障根本原因并量化干预措施对KPI的影响，从而推荐最优维护策略。


<details>
  <summary>更多</summary>
  
**动机:** 当前制造业预测性维护模型依赖伪相关而非真实因果关系，导致误诊和无效干预，无法系统理解故障原因和确定最有效干预措施。

**方法:** 使用预训练的因果基础模型作为"what-if"模型，模拟评估潜在修复方案，测量每个干预对系统级KPI的因果效应，提供数据驱动的行动排序。

**结果:** 使用半合成制造数据评估模型，并与基线机器学习模型进行比较，验证了方法的有效性。

**结论:** 该研究为稳健的规范性维护框架奠定了技术基础，使工程师能够在因果环境中测试潜在解决方案，做出更有效的运营决策并减少昂贵停机时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Causal+Foundation+Model+in+Prescriptive+Maintenance+Framework+for+Optimizing+Production+Line+OEE，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00969&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a "what-if" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.

</details>


### [29] [IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch](https://arxiv.org/abs/2512.00997)
*Param Biyani, Shashank Kirtania, Yasharth Bajpai, Sumit Gulwani, Ashish Tiwari*

**主要类别:** cs.AI

**AI概要:** IndiMathBench是一个人工验证的数学定理证明评估基准，包含312个来自印度数学奥林匹克竞赛的形式化Lean 4定理及其非正式问题陈述，通过AI辅助流程构建，用于测试数学推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决数学定理自动形式化面临的挑战，特别是语法有效性和语义正确性之间的差距，以及定理证明成功率低的问题，需要建立一个高质量的评估基准。

**方法:** 采用AI驱动的人工辅助流程，通过类别检索、迭代编译器反馈和多模型集成生成候选形式化，专家通过交互式仪表板进行高效验证，并使用自动化质量摘要。

**结果:** 评估显示自动形式化仍然具有挑战性，语法有效性和语义正确性存在显著差距，即使通过迭代优化，定理证明的成功率仍然较低。

**结论:** IndiMathBench为数学推理提供了一个具有挑战性的测试平台，证明了当前模型在数学定理证明方面的局限性，可作为未来研究的重要基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IndiMathBench%3A+Autoformalizing+Mathematical+Reasoning+Problems+with+a+Human+Touch，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00997&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.

</details>


### [30] [ChartAnchor: Chart Grounding with Structural-Semantic Fidelity](https://arxiv.org/abs/2512.01017)
*Xinhang Li, Jingbo Zhou, Pengfei Luo, Yixiong Xiao, Tong Xu*

**主要类别:** cs.AI

**AI概要:** ChartAnchor是一个包含8000+图表-表格-代码三元组的综合基准测试，涵盖30种图表类型，通过双向对齐任务评估多模态大语言模型的图表理解能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试在图表多样性、任务孤立性和评估框架完整性方面存在局限，无法全面评估图表接地（chart grounding）能力，而该能力直接反映了模型在数值推理、多模态对齐和结构重建方面的表现。

**方法:** 提出ChartAnchor基准，包含两个互补任务：图表到代码生成（合成可执行代码复制图表）和受控图表到表格重建（使用预定义标题提取精确数据），采用多级评估框架整合语义验证、风格分析和感知指标。

**结果:** 在多模态大语言模型上的广泛实验揭示了数值精度和代码合成方面的关键限制，强调了超越表面感知的结构化推理的必要性。

**结论:** ChartAnchor通过统一符号和数据驱动的接地方法，为图表接地建立了严格的基础，为在科学、金融和工业领域推进多模态大语言模型提供了有意义的见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ChartAnchor%3A+Chart+Grounding+with+Structural-Semantic+Fidelity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01017，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01017&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.

</details>


### [31] [Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics](https://arxiv.org/abs/2512.01020)
*Jinu Lee, Kyoung-Woon On, Simeng Han, Arman Cohan, Julia Hockenmaier*

**主要类别:** cs.AI

**AI概要:** 论文介绍了LEGIT数据集，这是一个大规模法律推理数据集，用于评估LLM生成的法律推理轨迹质量，发现LLM的法律推理能力受问题覆盖率和正确性影响，RAG和RL方法能带来互补的改进效果


<details>
  <summary>更多</summary>
  
**动机:** 评估LLM在专业领域（如法律）生成推理轨迹的质量对于确保可信度和可解释性至关重要，但由于此类推理任务的固有复杂性，这仍然具有挑战性

**方法:** 创建LEGIT数据集（24K实例），将法院判决转换为对立双方论点和法院结论的分层树结构，作为评估推理轨迹问题覆盖率和正确性的评分标准，并通过专家标注验证可靠性

**结果:** 发现LLM的法律推理能力严重受法律问题覆盖率和正确性影响；RAG和RL方法带来互补效益：RAG提高整体推理能力，RL提高正确性但会降低覆盖率

**结论:** LEGIT数据集为评估LLM法律推理提供了有效工具，揭示了RAG和RL在提升法律推理能力方面的不同优势，为改进专业领域LLM推理能力提供了重要见解

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating+Legal+Reasoning+Traces+with+Legal+Issue+Tree+Rubrics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01020，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01020&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.

</details>


### [32] [Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal](https://arxiv.org/abs/2512.01045)
*Shenxi Liu, Kan Li, Mingyang Zhao, Yuhang Tian, Shoujun Zhou, Bin Li*

**主要类别:** cs.AI

**AI概要:** Med-CRAFT是一个神经符号数据工程框架，通过图遍历过程自动生成医学视频推理基准数据集M3-Med-Auto，解决了高质量医学视频标注数据稀缺的问题。


<details>
  <summary>更多</summary>
  
**动机:** 医学领域多模态大语言模型发展受限于高质量逻辑标注视频数据的稀缺性，传统人工标注成本高昂且不可扩展，现有合成方法存在随机幻觉和缺乏逻辑可解释性的问题。

**方法:** 从原始视频流提取结构化视觉基元，构建动态时空知识图谱，通过图内有效路径锚定查询生成，为每个合成基准项强制执行严格的思维链溯源。

**结果:** 生成的M3-Med-Auto基准数据集展现出细粒度时间选择性和多跳逻辑复杂性，复杂度可与专家策划数据集相媲美，逻辑对齐分析显示图拓扑与先进MLLM推理步骤高度相关。

**结论:** 该工作为关键领域可扩展、低成本的稳健评估协议构建铺平了道路，能够将可验证逻辑编码到视觉语言基准中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Med-CRAFT%3A+Automated+Construction+of+Interpretable+and+Multi-Hop+Video+Workloads+via+Knowledge+Graph+Traversal，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01045，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01045&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \textbf{\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.

</details>


### [33] [Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids](https://arxiv.org/abs/2512.01046)
*Hadi Nekoei, Alexandre Blondin Massé, Rachid Hassani, Sarath Chandar, Vincent Mai*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为Shielded Controller Units (SCUs)的系统化可解释方法，利用系统动态先验知识确保约束满足，在远程微电网优化任务中实现燃料消耗减少24%且不增加电池退化


<details>
  <summary>更多</summary>
  
**动机:** 强化学习在复杂系统中优化决策面临约束满足的挑战，特别是在能源转型背景下，需要确保RL智能体遵守大量法规和操作约束

**方法:** 引入SCUs方法，将环境分解为分层结构，每个SCU显式管理约束子集，为实际部署设计的屏蔽合成方法

**结果:** 配备SCUs的RL智能体在严格操作要求下，燃料消耗减少24%且不增加电池退化，优于其他基线方法并满足所有约束

**结论:** SCUs有助于RL安全应用于能源转型相关的众多决策挑战，提供可解释的约束满足保证

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Shielded+Controller+Units+for+RL+with+Operational+Constraints+Applied+to+Remote+Microgrids，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01046&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.

</details>


### [34] [Automating the Refinement of Reinforcement Learning Specifications](https://arxiv.org/abs/2512.01047)
*Tanmay Ambadkar, Đorđe Žikelić, Abhinav Verma*

**主要类别:** cs.AI

**AI概要:** AutoSpec框架通过探索引导策略改进粗糙的逻辑规范，为强化学习提供更具体的指导，从而帮助算法学习更有用的策略。


<details>
  <summary>更多</summary>
  
**动机:** 当任务定义不够具体时，强化学习算法可能无法学习到有用的策略，需要改进粗糙的逻辑规范来提供更好的指导。

**方法:** 提出AutoSpec框架，利用SpectRL规范逻辑的组合性质，设计四种精化程序来修改规范的抽象图，通过精化现有边规范或引入新边规范。

**结果:** 实验证明AutoSpec在解决控制任务复杂度方面带来了显著改进，所有精化程序都保持了规范的正确性。

**结论:** AutoSpec能够有效改进逻辑规范，为强化学习提供更好的指导，提高学习效果和任务解决能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automating+the+Refinement+of+Reinforcement+Learning+Specifications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01047，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01047&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \textsc{AutoSpec} are utilized.

</details>


### [35] [SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds](https://arxiv.org/abs/2512.01078)
*Jiawei Ren, Yan Zhuang, Xiaokang Ye, Lingjun Mao, Xuhong He, Jianzhi Shen, Mrinaal Dogra, Yiming Liang, Ruixuan Zhang, Tianai Yue, Yiqing Yang, Eric Liu, Ryan Wu, Kevin Benavente, Rajiv Mandya Nagaraju, Muhammad Faayez, Xiyan Zhang, Dhruv Vivek Sharma, Xianrui Zhong, Ziqiao Ma, Tianmin Shu, Zhiting Hu, Lianhui Qin*

**主要类别:** cs.AI

**AI概要:** SimWorld是一个基于Unreal Engine 5构建的新型模拟器，专为在丰富的现实世界环境中开发和评估LLM/VLM智能体而设计，解决了现有模拟器在物理和社会环境模拟方面的不足。


<details>
  <summary>更多</summary>
  
**动机:** 当前LLM/VLM智能体在复杂物理和社会环境中的应用面临挑战，现有模拟器依赖有限的手工环境、简化的游戏物理规则，且缺乏对LLM/VLM智能体的原生支持。

**方法:** 开发了SimWorld模拟器，具备三个核心能力：1）真实开放的世界模拟，包括准确的物理和社会动力学；2）丰富的LLM/VLM智能体接口，支持多模态输入和开放词汇动作；3）可扩展的物理和社会推理场景。

**结果:** 通过在多智能体配送任务中部署前沿LLM模型（GPT-4o、Gemini-2.5-Flash等），展示了不同模型的推理模式和局限性。

**结论:** SimWorld开源发布，有望成为跨学科推进现实世界智能体智能的基础平台。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SimWorld%3A+An+Open-ended+Realistic+Simulator+for+Autonomous+Agents+in+Physical+and+Social+Worlds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01078，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01078&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.

</details>


### [36] [Testing the Machine Consciousness Hypothesis](https://arxiv.org/abs/2512.01081)
*Stephen Fitz*

**主要类别:** cs.AI

**AI概要:** 该论文提出机器意识假说，认为意识是计算系统的无基质功能属性，源于分布式学习系统中集体自我模型的涌现。通过研究细胞自动机环境中的预测性神经网络通信，探讨意识如何从主体间对齐的交流过程中产生。


<details>
  <summary>更多</summary>
  
**动机:** 研究机器意识的本质，挑战传统认为意识是个体建模副现象的观点，提出意识是系统内部自我描述语言的功能属性，旨在开发可实证检验的机器意识理论。

**方法:** 使用细胞自动机作为基础计算环境，建立局部预测性神经网络模型，研究分布式系统中通过通信和适应如何涌现集体自我模型和主体间对齐。

**结果:** 提出意识源于群体间预测性信息交换的理论框架，表明共享模型通过表示性对话从多个局部世界视图中对齐产生。

**结论:** 意识不是建模本身的副产品，而是通信的产物，特别是局部观察者群体在描述基础计算基质中持久模式时的预测信息交换过程。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Testing+the+Machine+Consciousness+Hypothesis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01081，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01081&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.

</details>


### [37] [CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents](https://arxiv.org/abs/2512.01089)
*Peter Jansen, Samiah Hassan, Pragnya Narasimha*

**主要类别:** cs.AI

**AI概要:** CodeDistiller系统通过从大量科学GitHub仓库中自动提取经过验证的领域特定代码示例，显著提升了自动化科学发现系统的能力，使74%的仓库能够生成功能性示例，并产生更准确完整的科学实验。


<details>
  <summary>更多</summary>
  
**动机:** 当前自动化科学发现系统受限于仅从参数化知识生成可靠代码的能力，要么只能对少量手动制作的实验示例进行变异，要么仅基于参数化知识操作，限制了系统质量和应用范围。

**方法:** 开发CodeDistiller系统，自动从大量科学GitHub仓库中提取经过验证的领域特定代码示例库，结合自动化和领域专家评估方法，在250个材料科学仓库上进行测试。

**结果:** 最佳模型能为74%的仓库生成功能性示例，下游评估显示配备CodeDistiller生成库的ASD代理比仅使用通用材料科学代码示例的代理产生更准确、完整和科学合理的实验。

**结论:** CodeDistiller通过自动从开源科学代码库中提取高质量领域特定示例，有效扩展了自动化科学发现系统的能力，无需人工干预即可显著提升实验生成质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CodeDistiller%3A+Automatically+Generating+Code+Libraries+for+Scientific+Coding+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01089，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01089&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.

</details>


### [38] [Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems](https://arxiv.org/abs/2512.01099)
*Daria Smirnova, Hamid Nasiri, Marta Adamska, Zhengxin Yu, Peter Garraghan*

**主要类别:** cs.AI

**AI概要:** 论文提出了GUIDE框架，通过量化模型性能特征来优化AI模型选择，在提高准确率的同时显著降低能耗和延迟。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于LLM的模型编排器依赖定性描述进行决策，无法反映真实模型能力和性能特征，导致模型选择次优、准确率下降和能耗增加。

**方法:** 提出GUIDE框架，这是一个考虑性能-能耗权衡的能源感知模型选择框架，通过整合定量模型性能特征来改进决策过程。

**结果:** 实验结果显示，GUIDE在各种任务中准确率提高0.90%-11.92%，能效提升高达54%，编排器模型选择延迟从4.51秒降至7.2毫秒。

**结论:** GUIDE框架有效解决了LLM编排的局限性，通过量化方法实现了更优的模型选择和显著的性能提升，为AI系统的高效运行提供了实用解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Energy-Aware+Data-Driven+Model+Selection+in+LLM-Orchestrated+AI+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01099，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01099&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.

</details>


### [39] [Foundation Priors](https://arxiv.org/abs/2512.01107)
*Sanjog Misra*

**主要类别:** cs.AI

**AI概要:** 论文提出了"基础先验"概念，认为大模型生成的合成数据不是真实观测值，而是基于用户主观先验和模型学习模式的先验预测分布。通过建立数学框架，将合成数据作为结构化主观先验纳入统计和计量经济学工作流。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型生成的信息越来越丰富，研究者开始使用这些合成输出作为实证研究数据，但需要避免将合成"事实"与真实数据混淆的问题。

**方法:** 提出基础先验作为指数倾斜的广义贝叶斯更新，包含用户原始先验、预期数据分布、提示工程过程和模型信任度参数。建立了将合成数据纳入标准统计工作流的数学框架。

**结果:** 开发了理论框架，证明合成数据反映了模型学习模式和用户主观先验的混合，信任参数控制合成数据的权重。

**结论:** 通过将生成输出视为结构化主观先验而非经验观测，该框架为在实证工作中利用基础模型提供了原则性方法，避免了合成数据与真实数据的混淆。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Foundation+Priors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01107，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01107&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.
  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.

</details>


### [40] [A Benchmark of Causal vs Correlation AI for Predictive Maintenance](https://arxiv.org/abs/2512.01149)
*Krishna Taduri, Shaunak Dhande, Giacinto Paolo, Saggese, Paul Smith*

**主要类别:** cs.AI

**AI概要:** 因果AI方法在预测性维护中显著优于传统相关分析方法，年节省成本116万美元，误报减少97%，同时保持高召回率和精确度


<details>
  <summary>更多</summary>
  
**动机:** 制造业预测性维护存在极端成本不对称问题（漏检成本是误报的50倍），传统机器学习方法无法区分因果关系和伪相关，导致财务表现不佳

**方法:** 评估8种预测模型，从基础统计方法到正式因果推断方法，在10,000台CNC机床数据集（故障率3.3%）上进行测试，比较因果模型与相关基模型的性能

**结果:** 因果推断模型(L5)年节省成本116万美元（减少70.2%），比最佳相关决策树模型(L3)每年多节省8万美元；召回率87.9%，误报减少97%（从165降至5），精确度92.1%，训练-测试性能差距仅2.6个百分点

**结论:** 因果AI方法结合领域知识能够在预测性维护应用中产生更优的财务结果和更可解释的预测，相比基于相关的方法具有显著优势

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Benchmark+of+Causal+vs+Correlation+AI+for+Predictive+Maintenance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01149，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01149&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.

</details>


### [41] [fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment](https://arxiv.org/abs/2512.01189)
*Chunzheng Zhu, Jialin Shao, Jianxin Lin, Yijun Wang, Jing Wang, Jinhui Tang, Kenli Li*

**主要类别:** cs.AI

**AI概要:** 提出fMRI2GES方法，通过双脑解码对齐技术，利用未配对数据实现fMRI到手势的重建，解决了脑-语音-手势配对数据缺乏的问题


<details>
  <summary>更多</summary>
  
**动机:** 传统研究主要关注脑到图像和脑到语言的重建，但缺乏对与语音刺激相关的手势重建的研究，且缺乏配对的脑-语音-手势数据限制了深度学习模型的应用

**方法:** 使用双脑解码对齐方法，结合fMRI到文本模型、文本到手势模型（有配对数据）和fMRI到手势模型（无配对数据），建立双重建模式并通过自监督方式进行对齐训练

**结果:** 成功从fMRI记录中重建出富有表现力的手势，并研究了大脑皮层不同ROI区域的fMRI信号对生成结果的影响

**结论:** 为解码伴随语音的手势提供了新的见解，推动了神经科学和认知科学的发展，展示了直接从脑活动解码复杂行为模式的潜力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是fMRI2GES%3A+Co-speech+Gesture+Reconstruction+from+fMRI+Signal+with+Dual+Brain+Decoding+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01189&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \{brain, speech, gesture\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.

</details>


### [42] [Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction](https://arxiv.org/abs/2512.01210)
*Ruiyu Wang, Tuan Vinh, Ran Xu, Yuyin Zhou, Jiaying Lu, Carl Yang, Francisco Pasquel*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一个基于知识图谱引导的思维链框架，用于电子健康记录中的疾病预测，通过将ICD-9代码映射到PrimeKG知识图谱，提取疾病相关节点和多跳推理路径，生成临床基础且时间一致的推理解释。


<details>
  <summary>更多</summary>
  
**动机:** 现有电子健康记录预测模型通常提供粗糙的事后解释，对患者层面的决策支持有限，需要更临床相关且可解释的预测方法。

**方法:** 使用PrimeKG知识图谱映射ICD-9代码，提取疾病相关节点和多跳推理路径作为思维链生成支架，筛选与观察结果匹配的解释，然后对LLaMA-3.1-Instruct-8B和Gemma-7B模型进行微调。

**结果:** 在10种疾病和有限训练样本（400和1000例）下，KG引导模型优于传统基线方法，AUROC达到0.66-0.70，macro-AUPR达到0.40-0.47，在CRADLE队列中零样本迁移准确率从0.40-0.51提升至0.72-0.77。

**结论:** 知识图谱引导的思维链框架能够生成临床相关且可解释的疾病预测，提高了预测性能并在临床评估中获得医生偏好，显示出在医疗决策支持中的实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Graph+Augmented+Large+Language+Models+for+Next-Visit+Disease+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01210&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.

</details>


### [43] [Unsupervised decoding of encoded reasoning using language model interpretability](https://arxiv.org/abs/2512.01222)
*Ching Fang, Samuel Marks*

**主要类别:** cs.AI

**AI概要:** 研究表明当前的机制可解释性技术（特别是logit lens分析）能够有效解码语言模型中的加密推理过程，在ROT-13加密的思维链推理测试中表现出色，为AI系统监管提供了新方法。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型能力增强，担心其可能发展出人类无法监督的加密推理过程，需要验证当前可解释性技术是否能穿透这种加密推理。

**方法:** 通过微调DeepSeek-R1-Distill-Llama-70B模型进行ROT-13加密的思维链推理，使用logit lens分析从内部激活中解码推理过程，并开发无监督解码管道结合自动释义。

**结果:** logit lens能有效翻译加密推理，准确率在中间到后期层达到峰值，无监督解码管道能从内部表示中高精度重建完整推理记录。

**结论:** 当前机制可解释性技术对简单形式的加密推理比之前认为的更稳健，为评估非人类可读格式推理模型的可解释性提供了初步框架，有助于AI系统监管。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unsupervised+decoding+of+encoded+reasoning+using+language+model+interpretability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01222，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01222&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.

</details>


### [44] [OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction](https://arxiv.org/abs/2512.01289)
*Mingqin Yu, Fethi Rabhi, Boming Xia, Zhengyi Yang, Felix Tan, Qinghua Lu*

**主要类别:** cs.AI

**AI概要:** OntoMetric是一个基于本体论的框架，通过三阶段流程将ESG监管文档转换为经过验证的知识图谱，相比无约束的LLM提取方法，显著提高了语义准确性和模式合规性。


<details>
  <summary>更多</summary>
  
**动机:** ESG披露框架要求组织计算和报告大量指标，但这些要求嵌入在冗长、非结构化的PDF文档中，难以解释、标准化和审计。手动提取不可扩展，而无约束的大语言模型提取往往产生不一致的实体、幻觉关系、缺失来源和高验证失败率。

**方法:** 三阶段流程：(1)基于目录边界的结构感知分割，(2)本体约束的LLM提取，将ESGMKG模式嵌入提示中并为下游推理丰富语义字段，(3)两阶段验证，结合基于LLM的语义验证和基于规则的模式检查。

**结果:** 在五个ESG标准（228页，60个片段）上评估，OntoMetric达到65-90%的语义准确性和80-90%的模式合规性，相比基线无约束提取的3-10%有显著提升，每个验证实体的成本约为0.01-0.02美元。

**结论:** 结合符号本体约束和神经提取能够实现可靠、可审计的知识图谱，适用于监管合规和网络集成，支持可持续金融分析、透明度门户和自动化合规工具等下游应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OntoMetric%3A+An+Ontology-Guided+Framework+for+Automated+ESG+Knowledge+Graph+Construction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01289，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01289&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.

</details>


### [45] [RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2512.01300)
*Dacheng Liao, Mengshi Qi, Peng Shu, Zhining Zhang, Yuxin Lin, Liang Liu, Huadong Ma*

**主要类别:** cs.AI

**AI概要:** 该论文提出了RoboDriveBench，首个专注于端到端轨迹预测任务的鲁棒性基准测试，并开发了RoboDriveVLM框架和测试时适应方法来提升基于视觉语言模型的自动驾驶系统的鲁棒性。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于视觉语言模型(VLM)的端到端自动驾驶系统在真实驾驶场景中存在多种风险，需要评估其实际可行性并提高鲁棒性。

**方法:** 1) 创建RoboDriveBench基准测试，包含11个模拟场景(6个传感器损坏和5个提示损坏)，共64,559个轨迹预测案例；2) 提出RoboDriveVLM框架，将多模态数据映射到统一潜在空间；3) 引入基于跨模态知识蒸馏的测试时适应方法。

**结果:** 通过大量实验验证了当前VLM端到端自动驾驶系统的局限性，并提供了更可靠的解决方案。

**结论:** 该研究揭示了当前VLM自动驾驶系统的不足，提出了有效的鲁棒性增强方法，为实际部署提供了更可靠的方案，代码和数据集将公开。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RoboDriveVLM%3A+A+Novel+Benchmark+and+Baseline+towards+Robust+Vision-Language+Models+for+Autonomous+Driving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01300，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01300&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.

</details>


### [46] [CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL](https://arxiv.org/abs/2512.01311)
*Shinji Mai, Yunpeng Zhai, Ziqian Chen, Cheng Chen, Anni Zou, Shuchang Tao, Zhaoyang Liu, Bolin Ding*

**主要类别:** cs.AI

**AI概要:** CuES框架通过内在好奇心和环境基础自主生成多样、可执行的任务，解决了智能体强化学习中的任务稀缺问题，无需预定义任务或外部语料库，在多个环境中表现优于人工策划的数据集。


<details>
  <summary>更多</summary>
  
**动机:** 解决智能体强化学习中的任务稀缺问题，特别是在新环境中工具语义和可用性未知时，缺乏结构化训练任务的挑战。

**方法:** 提出CuES框架，通过好奇心驱动探索、将交互模式抽象为可重用任务模式，并通过轻量级自上而下指导和基于记忆的质量控制来精炼任务。

**结果:** 在AppWorld、BFCL和WebShop三个环境中，CuES生成的任务分布在多样性和可执行性方面匹配或超越人工策划的数据集，带来显著的下游策略改进。

**结论:** 好奇心驱动、环境基础的任务生成为智能体提供了可扩展的基础，不仅学习如何行动，还学习学习什么。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CuES%3A+A+Curiosity-driven+and+Environment-grounded+Synthesis+Framework+for+Agentic+RL，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01311，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01311&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.

</details>


### [47] [Extending NGU to Multi-Agent RL: A Preliminary Study](https://arxiv.org/abs/2512.01321)
*Juan Hernandez, Diego Fernández, Manuel Cifuentes, Denis Parra, Rodrigo Toro Icarte*

**主要类别:** cs.AI

**AI概要:** 本研究将NGU算法扩展到多智能体环境，在PettingZoo的simple_tag环境中测试，发现共享经验回放缓冲区的NGU表现最佳，表明内在探索与经验共享的结合是关键。


<details>
  <summary>更多</summary>
  
**动机:** 将NGU算法从单智能体扩展到多智能体环境，研究其在稀疏奖励的多智能体强化学习任务中的效果。

**方法:** 在simple_tag环境中比较多智能体NGU与多智能体DQN基线，研究三种设计选择：共享vs个体回放缓冲区、不同k阈值的新颖性共享、异质beta参数值。

**结果:** NGU获得略高回报和更稳定的学习动态；共享回放缓冲区表现最佳；k=1时新颖性共享效果相当，但k值增大会降低性能；异质beta值未带来改进。

**结论:** NGU可有效应用于多智能体设置，关键在于经验共享和精心调整内在探索信号，共享回放缓冲区是实现最佳性能的关键因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Extending+NGU+to+Multi-Agent+RL%3A+A+Preliminary+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01321，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01321&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.

</details>


### [48] [A Fast Heuristic Search Approach for Energy-Optimal Profile Routing for Electric Vehicles](https://arxiv.org/abs/2512.01331)
*Saman Ahmadi, Mahdi Jalili*

**主要类别:** cs.AI

**AI概要:** 提出基于多目标A*搜索的标签设置方法，用于电动汽车能量最优路径规划，避免处理复杂能量配置文件，在未知初始能量情况下实现与已知初始能量相当的性能。


<details>
  <summary>更多</summary>
  
**动机:** 电动汽车在道路网络中寻找能量最优路径时，下坡路段可回收能量产生负能量成本。传统方法假设初始能量已知，但实际场景中初始能量存在不确定性，需要为所有可能的初始能量水平规划最优路径。

**方法:** 基于多目标A*搜索的标签设置方法，采用新颖的配置文件支配规则，避免生成和处理复杂配置文件。开发了四种变体方法。

**结果:** 在真实道路网络和实际能耗数据上的实验表明，该方法在性能上与已知初始能量水平的能量最优A*搜索相当。

**结论:** 提出的标签设置方法简单有效，能够处理初始能量不确定情况下的电动汽车能量最优路径规划问题，避免了复杂配置文件的处理，实现了良好的性能表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Fast+Heuristic+Search+Approach+for+Energy-Optimal+Profile+Routing+for+Electric+Vehicles，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01331，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01331&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We study the energy-optimal shortest path problem for electric vehicles (EVs) in large-scale road networks, where recuperated energy along downhill segments introduces negative energy costs. While traditional point-to-point pathfinding algorithms for EVs assume a known initial energy level, many real-world scenarios involving uncertainty in available energy require planning optimal paths for all possible initial energy levels, a task known as energy-optimal profile search. Existing solutions typically rely on specialized profile-merging procedures within a label-correcting framework that results in searching over complex profiles. In this paper, we propose a simple yet effective label-setting approach based on multi-objective A* search, which employs a novel profile dominance rule to avoid generating and handling complex profiles. We develop four variants of our method and evaluate them on real-world road networks enriched with realistic energy consumption data. Experimental results demonstrate that our energy profile A* search achieves performance comparable to energy-optimal A* with a known initial energy level.

</details>


### [49] [Benchmarking Overton Pluralism in LLMs](https://arxiv.org/abs/2512.01351)
*Elinor Poole-Dayan, Jiayi Wu, Taylor Sorensen, Jiaxin Pei, Michiel A. Bakker*

**主要类别:** cs.AI

**AI概要:** 论文提出了一个测量LLM中Overton多元主义的新框架，通过形式化的OvertonScore指标、大规模人类研究和自动化基准测试，评估模型输出中多样观点的覆盖程度。


<details>
  <summary>更多</summary>
  
**动机:** 需要量化评估大型语言模型输出中多样观点的代表程度，但大规模人类研究成本高昂且耗时，因此需要开发可扩展的自动化评估工具。

**方法:** 1) 将Overton多元主义形式化为集合覆盖度量(OvertonScore)；2) 进行大规模美国代表性人类研究(N=1209；60个问题；8个LLM)；3) 开发能密切复现人类判断的自动化基准。

**结果:** 模型平均OvertonScore为0.35-0.41，DeepSeek V3表现最佳，但所有模型都远低于理论最大值1.0。自动化基准与人类判断具有高等级相关性(ρ=0.88)。

**结论:** 该研究为将多元主义对齐从规范性目标转变为可测量基准奠定了基础，为开发更多元化的LLM提供了系统性进展的基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Benchmarking+Overton+Pluralism+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01351，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01351&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce a novel framework for measuring Overton pluralism in LLMs--the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N = 1209; 60 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.35--0.41, with DeepSeek V3 performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($ρ=0.88$), providing a practical proxy without replacing human assessment. By turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.

</details>


### [50] [The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness](https://arxiv.org/abs/2512.01354)
*Zhongjie Jiang*

**主要类别:** cs.AI

**AI概要:** 本文提出Prompt驱动的认知计算框架(PMCSF)，通过模拟人类认知过程而非单纯模仿数据表面特征来生成合成数据，有效解决AI数据崩溃危机。


<details>
  <summary>更多</summary>
  
**动机:** 现有合成数据生成方法过度追求统计平滑性，系统性地消除了人类文本中的长尾认知异常，导致模型崩溃加速。需要新的范式来模拟人类认知过程而非表面数据特征。

**方法:** 提出PMCSF框架，包含认知状态解码器(CSD)将非结构化文本反转为结构化认知向量，认知文本编码器(CTE)通过数学定义的认知扰动算子将这些状态重新物化为富含人类典型缺陷的文本。

**结果:** 认知编解码验证显示CTE文本与人类文本的Jensen-Shannon散度为0.0614（标准LLM输出为0.4431），通过双盲专业媒体评审，异质模型间认知轮廓对齐的组内相关系数ICC>0.9。功能增益评估显示在A股市场同构压力测试中，使用CTE生成数据的策略在2015年崩盘期间最大回撤减少47.4%，产生8.6%防御性Alpha。

**结论:** 模拟人类认知局限性而非复制表面数据，能够产生具有真正功能增益的合成数据，为解决AI数据崩溃危机提供了可行的技术途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Necessity+of+Imperfection%3AReversing+Model+Collapse+via+Simulating+Cognitive+Boundedness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01354&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.
  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.
  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.
  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.

</details>


### [51] [A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building](https://arxiv.org/abs/2512.01434)
*Daull Xavier, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco*

**主要类别:** cs.AI

**AI概要:** CollabToolBuilder是一个多智能体LLM框架，通过专家在环指导迭代学习创建工具，最小化任务适应和人工反馈成本。使用四个专业智能体(教练、编码员、批评者、资本化器)通过强化动态提示和系统化人工反馈来生成和验证工具。


<details>
  <summary>更多</summary>
  
**动机:** 解决复杂迭代问题(如科学文档生成)中工具创建的需求，需要对齐人类意图和流程，同时减少任务/领域适应工作和人工反馈收集的时间成本。

**方法:** 采用多智能体架构，包含四个专业角色：Coach(指导)、Coder(编码)、Critic(批评)、Capitalizer(资本化)，通过强化动态提示和系统化人工反馈集成来验证和优化工具生成。

**结果:** 通过初步实验展示了框架的有效性，能够生成最先进的研究论文或专利等科学文档，证明了其在复杂迭代问题解决中的适用性。

**结论:** 该框架是多智能体上下文学习、HITL控制和可重用工具资本化的系统级集成方法，适用于科学文档生成等需要迭代优化的复杂问题解决场景。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Flexible+Multi-Agent+LLM-Human+Framework+for+Fast+Human+Validated+Tool+Building，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01434，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01434&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.

</details>


### [52] [A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale](https://arxiv.org/abs/2512.01440)
*Sylvain Marié, Pablo Knecht*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的状态转移事件时间序列(STE-ts)和选择性时序汉明距离(STH)，通过同时利用转移时间和状态持续时间，避免了对大型数据库进行代价高昂且失真的重采样操作。


<details>
  <summary>更多</summary>
  
**动机:** 传统的离散事件系统分析方法通常只关注事件序列或状态时间序列的单一维度，无法充分利用事件/状态的双重性质，且在观测周期和事件数量增长时，重采样操作会变得代价高昂且失真。

**方法:** 定义了状态转移事件时间序列(STE-ts)，提出了选择性时序汉明距离(STH)度量方法，该方法同时考虑转移时间和状态持续时间，能够避免重采样操作。

**结果:** STH方法在模拟和真实数据集上验证了其优势，相比重采样汉明和Jaccard度量具有更好的精度和计算效率，并能聚焦于多个感兴趣的状态。

**结论:** 该方法成功解决了传统方法在大型数据库分析中的局限性，提供了一种更高效、更精确的离散事件系统分析解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Selective+Temporal+Hamming+distance+to+find+patterns+in+state+transition+event+timeseries%2C+at+scale，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01440，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01440&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.

</details>


### [53] [Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework](https://arxiv.org/abs/2512.01452)
*Lingbo Li, Anuradha Mathrani, Teo Susnjak*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一个基于DSPy和GEPA模块的可编程风险评估管道，用结构化代码优化替代手动提示设计，在随机对照试验的偏倚风险评估中实现了自动化和可复现性。


<details>
  <summary>更多</summary>
  
**动机:** 随机对照试验的偏倚风险评估过程资源密集且存在评审者间差异，现有LLM方法依赖难以复现、泛化或评估的手动设计提示。

**方法:** 使用DSPy框架和GEPA模块，通过帕累托引导搜索优化LLM推理，生成可检查的执行轨迹，对100个RCT的7个偏倚风险领域进行评估。

**结果:** 在方法报告清晰的领域（如随机序列生成），GEPA生成的提示表现最佳，商业模型整体表现略优；与手动提示相比，GEPA在准确性和特定领域性能上提升30%-40%。

**结论:** GEPA能够为偏倚风险评估生成一致且可复现的提示，支持LLM在证据合成中的结构化和原则性使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Risk-of-Bias+Assessment+of+Randomized+Controlled+Trials%3A+A+First+Look+at+a+GEPA-trained+Programmatic+Prompting+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01452，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01452&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.

</details>


### [54] [Multi-Path Collaborative Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.01485)
*Jindi Lv, Yuhao Zhou, Zheng Zhu, Xiaofeng Wang, Guan Huang, Jiancheng Lv*

**主要类别:** cs.AI

**AI概要:** M3PO是一个新颖的强化学习框架，通过多路径协作机制解决传统思维链推理的确定性限制，在保持可解释性和推理效率的同时，在知识和推理密集型基准测试中达到最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 传统思维链推理在解码过程中存在内部确定性，限制了探索其他合理替代方案的可能性。现有方法虽然尝试在连续语义空间中进行推理，但仍受自回归解码的贪婪性质约束。

**方法:** 提出多路径感知策略优化(M3PO)框架，利用并行策略rollout作为多样化的推理来源，通过轻量级协作机制将跨路径交互整合到策略更新中，使每个轨迹能够通过同伴反馈精炼推理。

**结果:** 在知识和推理密集型基准测试中实现了最先进的性能表现。

**结论:** M3PO展示了多路径协作学习在培养可靠多步推理模式方面的潜力，同时保持了模型的可解释性和推理效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Path+Collaborative+Reasoning+via+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01485，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01485&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.

</details>


### [55] [SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry](https://arxiv.org/abs/2512.01507)
*Daniel Armstrong, Zlatko Jončev, Andres M Bran, Philippe Schwaller*

**主要类别:** cs.AI

**AI概要:** 利用大型语言模型将合成知识转化为代码，通过Python函数表示战略规则，实现可测试、可解释的合成策略表示，并在路线检索和聚类方面取得良好效果


<details>
  <summary>更多</summary>
  
**动机:** 现有计算机辅助合成规划系统能生成化学有效反应步骤，但难以整合收敛组装、保护基最小化、最优成环序列等战略考量

**方法:** 开发方法论利用LLM分析合成路线，将战略原则转化为代表各种战略战术规则的Python函数，创建可验证的代码而非简单启发式规则

**结果:** 发布了完整代码库和USPTO-ST数据集，实现了基于自然语言的路线检索（Top-3准确率75%），通过时间趋势分析和化学直观路线聚类验证了方法有效性

**结论:** 这项工作弥合了CASP中战术与战略之间的鸿沟，使路线能够通过战略标准而非仅结构进行规范、搜索和评估

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SynthStrategy%3A+Extracting+and+Formalizing+Latent+Strategic+Insights+from+LLMs+in+Organic+Chemistry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01507，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01507&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.

</details>


### [56] [LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems](https://arxiv.org/abs/2512.01556)
*Zhiyuan Wang, Aniri, Tianlong Chen, Yue Zhang, Heng Tao Shen, Xiaoshuang Shi, Kaidi Xu*

**主要类别:** cs.AI

**AI概要:** LEC方法通过线性期望约束和FDR控制，为LLM提供统计保证的选择性预测，并通过双模型路由机制提升性能


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型经常生成不可靠答案，而传统不确定性方法无法完全区分正确和错误预测，缺乏统计保证

**方法:** 提出LEC框架，将选择性预测重新解释为约束决策问题，通过线性期望约束和校准样本计算FDR约束的最大化覆盖率阈值，并扩展到双模型路由机制

**结果:** 在闭式和开放式QA数据集上，LEC实现了更严格的FDR控制，显著提高了样本保留率，双模型路由机制在更低风险水平下接受更多正确样本

**结论:** LEC为LLM提供了统计可靠的FDR控制框架，通过选择性预测和模型路由有效降低错误接受风险，提升模型输出的可靠性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LEC%3A+Linear+Expectation+Constraints+for+False-Discovery+Control+in+Selective+Prediction+and+Routing+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01556，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01556&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.

</details>


### [57] [CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning](https://arxiv.org/abs/2512.01616)
*Chainesh Gautam, Raghuram Bharadwaj Diddigi*

**主要类别:** cs.AI

**AI概要:** 提出一种基于预训练语言-策略对组合的新方法，通过将CLIP的多模态对齐思想扩展到强化学习领域，建立语言指令与策略嵌入的统一表示空间，实现跨任务的快速迁移。


<details>
  <summary>更多</summary>
  
**动机:** 开发能够在同一环境中解决多个任务的智能体，特别是当这些任务与语言自然关联时，需要高效的迁移学习方法来提升任务解决效率。

**方法:** 受计算机视觉中CLIP方法的启发，将语言指令和对应策略视为同一任务概念的不同模态表示，通过对比学习对齐语言和策略的嵌入表示，建立统一的表示空间。

**结果:** 实验结果表明该方法能够有效实现跨任务的快速迁移，提升了任务解决效率。

**结论:** 将CLIP的多模态对齐理念成功扩展到强化学习领域，为语言关联任务的跨任务迁移提供了一种有效的解决方案，证明了语言与策略表示对齐的可行性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CLIP-RL%3A+Aligning+Language+and+Policy+Representations+for+Task+Transfer+in+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01616，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01616&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.

</details>


### [58] [Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation](https://arxiv.org/abs/2512.01723)
*Saba Kublashvili*

**主要类别:** cs.AI

**AI概要:** HistoricalML是一个概率神经符号框架，通过贝叶斯不确定性量化、结构因果模型、合作博弈论和注意力神经网络，解决了历史事件建模中的数据稀缺、异质性噪声、反事实缺失和可解释性挑战。


<details>
  <summary>更多</summary>
  
**动机:** 历史事件建模面临极端数据稀缺（N<100）、异质性和噪声测量、缺失反事实以及需要人类可解释解释的基本挑战。

**方法:** 整合了：(1)贝叶斯不确定性量化分离认知与随机不确定性；(2)结构因果模型进行混淆下的反事实推理；(3)合作博弈论（Shapley值）进行公平分配建模；(4)基于注意力的神经架构进行上下文依赖因子加权。

**结果:** 在19世纪非洲分割（N=7殖民势力）和第二次布匿战争（N=2派系）的案例研究中：量化了德国在一战前的结构性紧张（+107.9%差异），布匿战争的蒙特卡洛模拟与历史结果一致（迦太基在坎尼57.3%胜率，罗马在扎马57.8%胜率）。

**结论:** 该方法在稀疏数据条件下实现一致估计，Shapley分配满足公理公平性保证。反事实分析显示迦太基的政治支持而非军事实力是决定性因素。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Probabilistic+Neuro-Symbolic+Reasoning+for+Sparse+Historical+Data%3A+A+Framework+Integrating+Bayesian+Inference%2C+Causal+Models%2C+and+Game-Theoretic+Allocation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01723，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01723&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.

</details>


### [59] [Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems](https://arxiv.org/abs/2512.01786)
*Xiaochuan Li, Ke Wang, Girija Gouda, Shubham Choudhary, Yaqun Wang, Linwei Hu, Joel Vaughan, Freddy Lecue*

**主要类别:** cs.AI

**AI概要:** 提出LLM Jury-on-Demand框架，通过动态选择最可靠的LLM评委来提升评估质量，在摘要和RAG基准测试中显著优于单评委和静态评委基线。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型在关键领域应用增多，需要可扩展且可靠的实时评估方法。人工评估可靠但成本高，单LLM评委有偏差，静态评委缺乏适应性。

**方法:** 训练可靠性预测器来评估LLM评委与人类专家的共识度，利用词元分布、嵌入和结构输入特征。动态为每个数据点选择最优评委组合，并按可靠性加权聚合分数。

**结果:** 在摘要和RAG基准测试中，动态评委系统与人类判断的相关性显著高于单评委和静态评委基线。

**结论:** 基于学习的自适应评委系统有望为关键领域的大语言模型构建可扩展、更可靠和可信的评估系统。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Who+Judges+the+Judge%3F+LLM+Jury-on-Demand%3A+Building+Trustworthy+LLM+Evaluation+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01786，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01786&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.

</details>


### [60] [H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons](https://arxiv.org/abs/2512.01797)
*Cheng Gao, Huimin Chen, Chaojun Xiao, Zhiyi Chen, Zhiyuan Liu, Maosong Sun*

**主要类别:** cs.AI

**AI概要:** 研究发现大语言模型中的幻觉现象由极少数特定神经元（少于总神经元0.1%）控制，这些神经元在预训练阶段形成，与过度顺从行为有因果关联，为开发更可靠模型提供了新视角


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要从训练数据和目标等宏观角度分析LLM的幻觉问题，但神经元层面的机制尚未深入探索，需要从微观层面理解幻觉产生的根本原因

**方法:** 从三个维度系统研究幻觉相关神经元（H-Neurons）：识别方法、行为影响分析和起源追溯。通过控制干预实验验证因果关联，并考察这些神经元在不同场景下的泛化能力

**结果:** 发现仅需极稀疏的神经元子集（<0.1%）即可可靠预测幻觉发生，且这些神经元在预训练阶段就已形成并保持对幻觉检测的预测能力，与过度顺从行为存在因果联系

**结论:** 研究将宏观行为模式与微观神经机制联系起来，揭示了幻觉产生的神经元基础，为开发更可靠的LLM提供了重要见解，表明可通过针对特定神经元进行干预来减少幻觉

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是H-Neurons%3A+On+the+Existence%2C+Impact%2C+and+Origin+of+Hallucination-Associated+Neurons，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01797，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01797&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.

</details>


### [61] [Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees](https://arxiv.org/abs/2512.01870)
*Alessandro Breccia, Federica Gerace, Marco Lippi, Gabriele Sicuro, Pierluigi Contucci*

**主要类别:** cs.AI

**AI概要:** 大型语言模型能够学习自然数迭代质因数分解生成的确定性树序列的部分内部语法结构，展现出对算术结构的学习能力。


<details>
  <summary>更多</summary>
  
**动机:** 研究大型语言模型是否能学习自然数迭代质因数分解产生的确定性树序列的内部语法结构，探索模型对算术结构而非仅经验数据的学习能力。

**方法:** 使用GPT-2架构的Transformer网络，在10^11个元素上从头开始训练，通过下一个词预测和掩码词预测任务测试模型的预测能力。

**结果:** 模型部分学习了树序列的内部语法，捕获了非平凡的规律性和相关性。

**结论:** 研究表明学习能力可能扩展到算术结构本身，而不仅仅是经验数据，为理解模型对数学结构的学习提供了新视角。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Testing+Transformer+Learnability+on+the+Arithmetic+Sequence+of+Rooted+Trees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01870，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01870&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \mathbb{N}\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\mathbb{N}\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.

</details>


### [62] [Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning](https://arxiv.org/abs/2512.01878)
*Gaganpreet Jhajj, Fuhua Lin*

**主要类别:** cs.AI

**AI概要:** 该研究提出在知识图谱推理中使用惊奇最小化原则，将神经科学中的自由能原理与知识图谱系统连接起来，通过最短路径距离形式化惊奇概念


<details>
  <summary>更多</summary>
  
**动机:** 探索如何将神经科学中的自由能原理和惊奇最小化概念应用于知识图谱系统，建立图距离与惊奇之间的关系

**方法:** 使用有向图中的最短路径距离来形式化惊奇概念，为基于知识图谱的智能体提供一个理论框架

**结果:** 建立了图距离与惊奇之间的数学联系，图距离在图神经网络中表现为消息传递深度，在基于模型的强化学习中表现为世界模型轨迹

**结论:** 这项进行中的研究探索了基于距离的惊奇概念是否能扩展先前关于语法通过树结构最小化惊奇和自由能的研究工作

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graph+Distance+as+Surprise%3A+Free+Energy+Minimization+in+Knowledge+Graph+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01878&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.

</details>


### [63] [Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models](https://arxiv.org/abs/2512.01880)
*Daren Zhong, Dingcheng Huang, Clayton Greenberg*

**主要类别:** cs.AI

**AI概要:** 提出基于n-gram语言模型的国际象棋走棋预测框架，通过按技能水平分类玩家并训练不同模型，显著提高了走棋预测准确性，最高可达39.1%的准确率提升


<details>
  <summary>更多</summary>
  
**动机:** 传统国际象棋引擎主要关注最优走法计算，忽略了不同技能水平玩家走棋的差异性，需要开发能够分析玩家行为模式的方法

**方法:** 使用n-gram语言模型，将玩家分为7个技能等级，利用Lichess平台数据训练分类模型，通过模型选择器动态选择最适合的模型进行走棋预测

**结果:** 模型选择器在早期游戏信息(16个半回合)下技能等级分类准确率达31.7%，走棋预测框架相比基准准确率提升最高达39.1%，计算效率适合实时分析

**结论:** 该框架成功将国际象棋走棋预测转化为行为分析任务，为不同技能水平的玩家提供了更准确的走棋预测，具有实时应用的潜力

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Human+Chess+Moves%3A+An+AI+Assisted+Analysis+of+Chess+Games+Using+Skill-group+Specific+n-gram+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01880，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01880&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.
  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.
  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.

</details>


### [64] [Learned-Rule-Augmented Large Language Model Evaluators](https://arxiv.org/abs/2512.01958)
*Jie Meng, Jin Mao*

**主要类别:** cs.AI

**AI概要:** 提出基于规则增强的LLM评估范式，通过规则蒸馏和强化学习提升大语言模型在多样化评估任务中的泛化能力


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM评估方法依赖人工设计的评估原则，成本高且与标注数据及LLM理解不一致，难以泛化到广泛评估场景

**方法:** 1. 使用LLM辅助的蒙特卡洛树搜索自动从数据中提取评分规则；2. 提出链式规则(CoR)指导LLM遵循规则；3. 通过强化学习训练规则增强的LLM评估器(RuAE)

**结果:** 在多样化任务上的广泛实验证明了该方法在各种评估场景中的有效性和泛化能力

**结论:** 该规则增强评估范式解决了现有LLM评估方法的局限性，为LLM作为通用评估器的应用提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learned-Rule-Augmented+Large+Language+Model+Evaluators，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01958，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01958&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.

</details>


### [65] [From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning](https://arxiv.org/abs/2512.01970)
*Sitao Cheng, Xunjian Yin, Ruiwen Zhou, Yuxuan Li, Xinyi Wang, Liangming Pan, William Yang Wang, Victor Zhong*

**主要类别:** cs.AI

**AI概要:** RL不是简单的概率放大器，而是推理合成器，但需要先通过SFT掌握原子技能才能合成复杂推理策略


<details>
  <summary>更多</summary>
  
**动机:** 研究RL如何贡献推理能力：是激励新技能合成还是仅放大现有行为，通过互补推理任务来探究

**方法:** 使用合成人类传记数据集，将推理能力解耦为参数推理和上下文推理两个原子技能，在IID、组合和零样本三个难度级别评估泛化能力

**结果:** SFT在分布内表现良好但在分布外泛化差，存在SFT泛化悖论；RL能合成复杂策略但前提是基础模型已通过SFT掌握原子技能

**结论:** 解耦原子训练后接RL为复杂推理任务提供了可扩展的泛化路径，RL在足够原子基础上能主动合成复杂推理策略

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是From+Atomic+to+Composite%3A+Reinforcement+Learning+Enables+Generalization+in+Complementary+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01970，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01970&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

</details>


### [66] [Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback](https://arxiv.org/abs/2512.01979)
*Aiden Yiliu Li, Bizhi Yu, Daoan Lei, Tianhe Ren, Shilong Liu*

**主要类别:** cs.AI

**AI概要:** Chain of Ground (CoG)是一个无需训练的多步GUI定位框架，通过多模态大语言模型进行迭代视觉推理和精炼，显著提升了复杂用户界面中自然语言指令的视觉定位精度。


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态大语言模型在GUI视觉定位中面临小目标、视觉相似目标和真实世界布局模糊性的挑战，主要原因是有限的定位能力和未充分利用现有推理潜力。

**方法:** 提出Chain of Ground框架，采用多步迭代推理和假设调整策略，而非直接预测，通过渐进式反思和调整实现更准确的定位。

**结果:** 在ScreenSpot Pro基准测试中达到68.4%准确率，提升4.8个百分点；在TPanel UI工业控制面板数据集上比Qwen3 VL 235B基线提升6.9个百分点。

**结论:** 通过结构化迭代精炼而非额外训练的方式，可以有效释放多模态模型的定位潜力，在真实世界和数字界面中都展现出优秀的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Chain-of-Ground%3A+Improving+GUI+Grounding+via+Iterative+Reasoning+and+Reference+Feedback，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01979，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01979&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.

</details>


### [67] [LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess](https://arxiv.org/abs/2512.01992)
*Sai Kolasani, Maxim Saplin, Nicholas Crispino, Kyle Montgomery, Jared Quincy Davis, Matei Zaharia, Chi Wang, Chenguang Wang*

**主要类别:** cs.AI

**AI概要:** LLM CHESS是一个通过国际象棋游戏评估大语言模型推理和指令遵循能力的框架，测试了50多个模型，发现即使顶级模型也难以稳定完成游戏，显示出推理与非推理模型的明显差距。


<details>
  <summary>更多</summary>
  
**动机:** 为了评估大语言模型在扩展代理交互中的推理和指令遵循泛化能力，减少过拟合和记忆化问题，提供更动态的评估方式。

**方法:** 使用国际象棋游戏环境，通过随机对手和可变技能水平的象棋引擎进行测试，评估胜率、移动质量、合法性、幻觉动作和游戏时长等行为指标，并计算Elo评分。

**结果:** 许多最先进模型难以完成游戏或取得稳定胜利，推理模型与非推理模型表现存在明显差距，该框架有效减少了过拟合和基准饱和问题。

**结论:** LLM CHESS是一个有效的评估工具，能够独特地测试LLMs的推理能力，框架已公开以支持未来研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM+CHESS%3A+Benchmarking+Reasoning+and+Instruction-Following+in+LLMs+through+Chess，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01992，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01992&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [68] [Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis](https://arxiv.org/abs/2512.00046)
*Angelina Parfenova, Andreas Marfurt, Alexander Denzler, Juergen Pfeffer*

**主要类别:** cs.CL

**AI概要:** 本文研究使用大语言模型进行归纳式定性数据分析的自动化，发现人类专家在复杂句子编码上表现更好但简单句表现差，而LLMs则相反，同时还揭示了人类标注与黄金标准存在系统性偏差但获得更高评价的有趣现象。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在探索大语言模型在归纳式编码中的自动化能力，与传统依赖预定义标签的演绎方法不同，关注从数据中自然涌现标签的归纳过程。

**方法:** 评估六个开源大语言模型的性能，与人类专家进行对比分析，专家对编码句子的感知难度进行评分，并系统比较人类和LLM生成标签与测试集黄金标准的偏差。

**结果:** 发现人类编码者在复杂句子标注上表现一致良好但在简单句上表现挣扎，而LLMs呈现相反趋势；人类标注虽有时与黄金标准不同但常获得更高人类评价，某些LLMs更接近真实标签却获得较低专家评分。

**结论:** 研究揭示了人类和LLM在定性数据分析中的互补性特征，为自动化编码工具的开发提供了重要见解，表明需要结合两者优势来提升分析质量。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Text+Annotation+via+Inductive+Coding%3A+Comparing+Human+Experts+to+LLMs+in+Qualitative+Data+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00046，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00046&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.

</details>


### [69] [Emergent Convergence in Multi-Agent LLM Annotation](https://arxiv.org/abs/2512.00047)
*Angelina Parfenova, Alexander Denzler, Juergen Pfeffer*

**主要类别:** cs.CL

**AI概要:** 该研究通过模拟7500次多智能体多轮讨论，分析了黑盒LLM在协作任务中的协调行为，发现LLM群体能在无明确角色提示的情况下实现词汇和语义收敛，并表现出类似谈判的行为模式。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型在协作环境中部署增加，但对其作为黑盒智能体时的协调机制了解甚少，需要研究LLM在多智能体设置中的交互动态。

**方法:** 在归纳编码任务中模拟多智能体多轮讨论，引入过程级指标（代码稳定性、语义自一致性、词汇置信度等），并分析输出嵌入的几何演变来追踪协调动态。

**结果:** LLM群体在词汇和语义上实现收敛，形成不对称影响模式，表现出谈判行为；内在维度随轮次减少表明语义压缩；黑盒交互分析能揭示涌现的协调策略。

**结论:** 研究表明黑盒交互分析可以作为基于内部探针的可解释性方法的可扩展补充，能够揭示LLM协作中涌现的协调策略和行为模式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergent+Convergence+in+Multi-Agent+LLM+Annotation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00047，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00047&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.

</details>


### [70] [Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees](https://arxiv.org/abs/2512.00204)
*Jason Lunder*

**主要类别:** cs.CL

**AI概要:** 该论文提出Tree Matching Networks (TMN)，一种基于依存句法树的图匹配网络，用于自然语言推理任务。相比BERT等序列模型，TMN在SNLI任务上以更少的参数和训练时间获得更好性能，但在SemEval相似性任务上表现不佳。


<details>
  <summary>更多</summary>
  
**动机:** 传统transformer模型如BERT需要从头学习词语关系，参数庞大。利用显式语言结构（如依存句法树）可能提高学习效率，避免从零学习关系。

**方法:** 将图匹配网络(GMN)适配到依存句法树上，创建Tree Matching Networks (TMN)。在SNLI蕴含任务和SemEval相似性任务上与BERT模型对比。

**结果:** TMN在SNLI任务上显著优于BERT模型，内存占用更小，训练时间更短。但在SemEval任务上两者都表现不佳。显式结构表示在可比规模下优于序列模型，但当前聚合方法限制了可扩展性。

**结论:** 显式语言结构能有效提升NLI任务性能，但需要改进聚合方法（如多头注意力聚合）来解决可扩展性问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Tree+Matching+Networks+for+Natural+Language+Inference%3A+Parameter-Efficient+Semantic+Understanding+via+Dependency+Parse+Trees，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00204，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00204&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.

</details>


### [71] [Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis](https://arxiv.org/abs/2512.00214)
*Matej Klemen, Tjaša Arčon, Luka Terčon, Marko Robnik-Šikonja, Kaja Dobrovoljc*

**主要类别:** cs.CL

**AI概要:** 本研究探索使用大型语言模型（LLM）代理来自动化语料库语法分析，通过多语言语法任务验证其在词序特征分析上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 虽然经验语法研究日益数据驱动，但系统分析标注语料库仍需要大量方法和技术努力，需要更高效、可解释的自动化工具。

**方法:** 提出了一个基于代理的语料库语法分析框架，整合自然语言任务解释、代码生成和数据驱动推理，并在Universal Dependencies多语言语料库上测试13个词序特征。

**结果:** 评估覆盖170多种语言，在三个维度（主导顺序准确性、顺序覆盖完整性和分布保真度）上验证了系统性能，展示了LLM推理与结构化语言数据结合的可行性。

**结论:** 该方法为基于语料库的语法研究提供了可解释、可扩展的自动化第一步，证明了LLM在语言学分析中的实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Corpus-Grounded+Agentic+LLMs+for+Multilingual+Grammatical+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00214，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00214&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.

</details>


### [72] [Minimal-Edit Instruction Tuning for Low-Resource Indic GEC](https://arxiv.org/abs/2512.00219)
*Akhil Rajeev P*

**主要类别:** cs.CL

**AI概要:** 使用指令调优的大型语言模型和保守解码方法，无需数据增强即可实现印度语言语法错误校正，在马来语和印地语上取得优异表现


<details>
  <summary>更多</summary>
  
**动机:** 印度语言语法错误校正面临监督数据有限、文字多样性和丰富形态学的挑战，需要寻找计算高效且可复现的替代方案

**方法:** 使用12B参数的GEMMA 3模型进行4位精度指令调优，采用参数高效微调(PEFT)和Alpaca格式，结合确定性约束感知解码和轻量级规范化器

**结果:** 在官方GLEU评估中，马来语得分92.41(排名第六)，印地语得分81.44(排名第三)，表明该方法有效且计算高效

**结论:** 分类器引导的提示设计、适配器指令调优和确定性解码为印度语言GEC提供了可复现的高效方案，未来可加强形态句法约束和人工评估

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Minimal-Edit+Instruction+Tuning+for+Low-Resource+Indic+GEC，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00219，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00219&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.
  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.

</details>


### [73] [OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion](https://arxiv.org/abs/2512.00234)
*Sai Koneru, Matthias Huck, Jan Niehues*

**主要类别:** cs.CL

**AI概要:** OmniFusion是一个端到端的多模态翻译系统，融合了多模态基础模型和翻译大语言模型，支持语音到文本、语音加图像到文本、以及文本加图像到文本的翻译，相比级联方法减少了延迟并提高了翻译质量。


<details>
  <summary>更多</summary>
  
**动机:** 现有的开源文本翻译大语言模型只能通过级联管道进行语音翻译，这增加了延迟且无法利用多模态上下文（如图像）来辅助消歧。多模态基础模型虽具备跨模态感知能力，但缺乏多语言覆盖和专门的翻译性能。

**方法:** 提出一种新颖的融合策略，将预训练多模态基础模型（Omni 2.5-7B）的多个隐藏层状态连接到翻译大语言模型（SeedX PPO-7B），实现联合端到端训练。

**结果:** 实验表明，OmniFusion能有效利用音频和视觉输入，在同时语音翻译中将延迟减少1秒，并提高了整体翻译质量。

**结论:** 通过融合多模态基础模型和翻译大语言模型，OmniFusion成功构建了一个高效的多模态翻译系统，解决了级联方法的延迟问题并提升了翻译性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OmniFusion%3A+Simultaneous+Multilingual+Multimodal+Translations+via+Modular+Fusion，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00234，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00234&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.

</details>


### [74] [Lost without translation -- Can transformer (language models) understand mood states?](https://arxiv.org/abs/2512.00274)
*Prakrithi Shivaprakash, Diptadhi Mukherjee, Lekhansh Shukla, Animesh Mukherjee, Prabhat Chand, Pratima Murthy*

**主要类别:** cs.CL

**AI概要:** 研究表明当前大语言模型无法直接从印度语言中有效识别情绪状态，但通过高质量翻译（特别是英文和中文）能显著提升情绪状态聚类性能，突显了开发本土语言理解模型的必要性。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在精神病学中显示出潜力，但主要是英语中心化的。不同语言有独特的痛苦表达方式，需要验证模型对其他语言情绪状态的理解能力，特别是在印度多种语言环境下的应用可行性。

**方法:** 收集11种印度语言的247个独特情绪短语（抑郁、正常心境、欣快躁狂、烦躁躁狂），测试7种实验条件：直接嵌入原生和罗马化文本（使用多语言和印度专用模型）以及翻译成英文和中文后的嵌入效果，使用综合评分（调整兰德指数、标准化互信息等）评估聚类性能。

**结果:** 印度语言直接嵌入完全失败（综合评分0.002），所有翻译方法均有显著改善。Gemini翻译英文（0.60）和人工翻译英文（0.61）表现良好，而人工翻译英文再译成中文并用中文模型嵌入效果最佳（0.67）。专用印度模型表现不佳。

**结论:** 当前模型无法直接从印度语言中有效表征情绪状态，这对其在印度的精神病学应用构成根本障碍。虽然高质量翻译能弥补这一差距，但依赖专有模型或复杂翻译流程不可持续，必须开发能理解多样本土语言的模型以支持全球心理健康应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Lost+without+translation+--+Can+transformer+%28language+models%29+understand+mood+states%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00274&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.

</details>


### [75] [EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education](https://arxiv.org/abs/2512.00290)
*Guoqing Ma, Jia Zhu, Hanghui Guo, Weijie Shi, Yue Cui, Jiawei Shen, Zilong Li, Yidan Liang*

**主要类别:** cs.CL

**AI概要:** EduEval是一个针对中国K-12教育的中文大语言模型评估基准，包含认知框架、真实教育内容和规模化数据集，评估显示模型在事实性任务表现良好但在对话分类和创造性任务上存在挑战。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型在教育应用中有巨大潜力，但未经审查的部署可能对教育标准构成风险，因此需要建立严格的评估体系来确保模型的教育适用性。

**方法:** 提出EduAbility分类法，整合Bloom分类法和Webb知识深度理论，构建包含6个认知维度的框架；整合真实考试题、课堂对话、学生作文和专家设计的提示词；创建包含24种任务类型、超过11,000道题目的数据集；评估14个主流LLM在零样本和少样本设置下的表现。

**结果:** 模型在事实性任务上表现良好，但在课堂对话分类方面表现不佳，创造性内容生成结果不一致；开源模型在复杂教育推理任务上优于专有系统；少样本提示在不同认知维度效果各异。

**结论:** 不同教育目标需要定制化方法，研究结果为开发专门针对中国多样化教育任务优化的LLM提供了有针对性的基准测试指标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EduEval%3A+A+Hierarchical+Cognitive+Benchmark+for+Evaluating+Large+Language+Models+in+Chinese+Education，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00290，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00290&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.

</details>


### [76] [Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets](https://arxiv.org/abs/2512.00323)
*Muhammad Muneeb, David B. Ascher, Ahsan Baidar Bakht*

**主要类别:** cs.CL

**AI概要:** 对47个基于上下文的问答模型在8个数据集上的基准测试研究，发现最佳模型为ahotrod/electra_large_discriminator_squad2_512，平均准确率43%，并分析了影响性能的因素


<details>
  <summary>更多</summary>
  
**动机:** 评估不同CBQA模型在多个数据集上的表现，无需额外微调即可确定最佳模型，为实际应用提供指导，减少重新训练模型的需求

**方法:** 使用Hugging Face上的47个CBQA模型在8个不同数据集上进行基准测试，使用遗传算法整合其他模型的响应来提高准确率

**结果:** 最佳模型ahotrod/electra_large_discriminator_squad2_512在所有数据集上平均准确率为43%，在特定数据集上表现优异（bioasq10b-factoid:65.92%, biomedical_cpgQA:96.45%），模型性能受上下文长度、模型大小、答案长度和上下文复杂度影响

**结论:** 基于SQuAD数据集训练的模型表现最佳，模型选择应考虑上下文特征，遗传算法可有效提升性能，为实际应用中的模型选择提供了实用指导

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Comparative+Analysis+of+47+Context-Based+Question+Answer+Models+Across+8+Diverse+Datasets，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00323&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\%), biomedical\_cpgQA (96.45\%), QuAC (11.13\%), and Question Answer Dataset (41.6\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\% on the IELTS dataset.

</details>


### [77] [Evidence-Guided Schema Normalization for Temporal Tabular Reasoning](https://arxiv.org/abs/2512.00329)
*Ashish Thanga, Vibhu Dixit, Abhilash Shankarampeta, Vivek Gupta*

**主要类别:** cs.CL

**AI概要:** 论文提出了一种基于SQL的方法来处理演化半结构化表格的时间推理问题，发现模式设计质量对问答精度的影响大于模型容量，提出了三个设计原则，最佳配置达到80.39 EM


<details>
  <summary>更多</summary>
  
**动机:** 当前QA系统在处理随时间演化的半结构化表格（如Wikipedia信息框）时面临挑战，需要更好的时间推理方法

**方法:** 采用SQL方法，包括：(1)从Wikipedia信息框生成3NF模式，(2)生成SQL查询，(3)执行查询。提出了三个模式设计原则：保持上下文的规范化、减少歧义的语义命名、一致的时间锚定

**结果:** 最佳配置（Gemini 2.5 Flash模式 + Gemini-2.0-Flash查询）达到80.39 EM，相比基线（68.89 EM）提升了16.8%

**结论:** 模式设计质量对问答系统精度的影响比模型容量更重要，提出了三个有效的模式设计原则，为处理演化表格的时间推理提供了有效解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Evidence-Guided+Schema+Normalization+for+Temporal+Tabular+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00329&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).

</details>


### [78] [Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents](https://arxiv.org/abs/2512.00332)
*Daud Waqas, Aaryamaan Golthi, Erika Hayashida, Huanzhi Mao*

**主要类别:** cs.CL

**AI概要:** 本文提出了Assertion-Conditioned Compliance (A-CC)评估框架，用于测试多轮工具调用LLM在面对误导性断言时的鲁棒性，发现现有模型在用户源断言和函数源断言方面都存在严重脆弱性。


<details>
  <summary>更多</summary>
  
**动机:** 多轮工具调用LLM在安全关键行业中应用广泛，但缺乏对多轮对话级别鲁棒性的评估标准，特别是在面对现实世界中误导性信息时的表现。

**方法:** 提出A-CC评估范式，通过两种误导性断言向量来测试模型：用户源断言(USAs)测试模型对错误用户信念的顺从性，函数源断言(FSAs)测试模型对矛盾系统策略的合规性。

**结果:** 研究结果显示，现有模型对USA的顺从性和FSA的策略冲突都表现出高度脆弱性，证实了A-CC是部署代理中的一个关键潜在漏洞。

**结论:** A-CC框架揭示了多轮函数调用对话模型的重要安全漏洞，为评估和改进模型鲁棒性提供了重要工具，对安全关键应用具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assertion-Conditioned+Compliance%3A+A+Provenance-Aware+Vulnerability+in+Multi-Turn+Tool-Calling+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00332&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.

</details>


### [79] [IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages](https://arxiv.org/abs/2512.00333)
*Ayush Maheshwari, Kaushal Sharma, Vivek Patel, Aditya Maheshwari*

**主要类别:** cs.CL

**AI概要:** IndicParam是一个包含13,000多道多选题的人类标注基准，覆盖11种印度低资源语言，评估显示即使最好的GPT-5在这些语言上平均准确率仅为45.0%，揭示了跨语言迁移的局限性。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型在高资源多语言任务上表现出色，但对低资源和极低资源的印度语言缺乏充分评估，需要建立专门的基准来测试这些语言的能力。

**方法:** 创建人工标注的IndicParam基准数据集，包含13,000+多选题，覆盖11种印度语言（包括低资源和极低资源语言），评估19个专有和开源LLM，分析知识导向vs语言能力问题，测试多种问题格式。

**结果:** 评估结果显示：GPT-5平均准确率45.0%，DeepSeek-3.2为43.1%，Claude-4.5为42.7%。所有模型在低资源印度语言上表现均不理想，揭示了跨语言迁移的显著局限性。

**结论:** IndicParam为印度语言建立了具有挑战性的基准，揭示了当前LLM在低资源语言处理上的不足，为改进跨语言迁移能力提供了重要见解和评估工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是IndicParam%3A+Benchmark+to+evaluate+LLMs+on+low-resource+Indic+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00333，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00333&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.

</details>


### [80] [CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA](https://arxiv.org/abs/2512.00360)
*Vsevolod Kovalev, Parteek Kumar*

**主要类别:** cs.CL

**AI概要:** 提出了CourseTimeQA数据集和CrossFusion-RAG检索器，在单GPU资源限制下实现了教育视频的时间戳问答，相比BLIP-2检索器在nDCG@10和MRR指标上分别提升0.10和0.08，同时保持1.55秒的中位端到端延迟。


<details>
  <summary>更多</summary>
  
**动机:** 解决在有限计算资源（单GPU）下对教育讲座视频进行时间戳问答的挑战，需要平衡检索性能和延迟/内存预算。

**方法:** 使用冻结编码器、学习的视觉投影、浅层查询无关交叉注意力机制（结合ASR和视频帧）、时间一致性正则化器以及小型交叉注意力重排序器构建CrossFusion-RAG模型。

**结果:** 在CourseTimeQA数据集上，CrossFusion-RAG相比BLIP-2检索器在nDCG@10指标上提升0.10，MRR提升0.08，中位延迟为1.55秒，并在ASR噪声鲁棒性和时间定位诊断方面表现良好。

**结论:** CrossFusion-RAG在有限计算资源下有效提升了教育视频时间戳问答的性能，为实际应用提供了可行的解决方案，并支持可复现的比较研究。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CourseTimeQA%3A+A+Lecture-Video+Benchmark+and+a+Latency-Constrained+Cross-Modal+Fusion+Method+for+Timestamped+QA，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00360，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00360&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.

</details>


### [81] [Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing](https://arxiv.org/abs/2512.00390)
*Nuo Chen, Hanpei Fang, Jiqun Liu, Wilson Wei, Tetsuya Sakai, Xiao-Ming Wu*

**主要类别:** cs.CL

**AI概要:** 研究发现LLM中的大五人格特征会影响相关性标注中的启动效应，高开放性和低神经质人格能降低启动敏感性，提出人格提示法来缓解阈值启动效应。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究表明LLM作为相关性标注工具易受启动效应影响，但尚不清楚LLM模拟的人格特征是否会产生类似的心理偏见效应。

**方法:** 使用多个LLM在TREC 2021和2022深度学习数据集上研究大五人格特征对相关性标注中启动效应的影响。

**结果:** 特定人格特征（如高开放性和低神经质）能持续降低启动敏感性，且最有效的人格特征因模型和任务类型而异。

**结论:** 基于研究发现提出人格提示法来缓解阈值启动效应，将心理学证据与LLM评估实践相结合。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+the+Threshold+Priming+Effect+in+Large+Language+Model-Based+Relevance+Judgments+via+Personality+Infusing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00390&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.

</details>


### [82] [A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction](https://arxiv.org/abs/2512.00392)
*Damian Heywood, Joseph Andrew Carrier, Kyu-Hong Hwang*

**主要类别:** cs.CL

**AI概要:** 本研究开发了一个AI辅助的英语写作错误分析系统，使用大型语言模型对错误进行识别、分类和纠正，基于语言学理论构建了详细的错误分类体系，在测试中展现了潜力但也存在语境理解限制。


<details>
  <summary>更多</summary>
  
**动机:** 开发AI系统来自动化英语写作错误分析，为EFL教学提供比传统评分标准更细致的反馈，提升语言学习效率。

**方法:** 利用Claude 3.5 Sonnet和DeepSeek R1等LLMs，基于Corder、Richards和James的语言学理论构建错误分类体系，通过Python API调用实现，在孤立错误和真实文本（如"English as she is spoke"）上进行测试。

**结果:** AI系统成功识别了多种错误类型，但在语境理解方面存在局限，遇到未编码错误时会生成新错误类别，展示了自动化详细错误分析的潜力。

**结论:** AI在EFL教学中具有变革潜力，能够自动化错误分析并提供详细反馈，但需要进一步改进语境准确性并扩展分类体系至文体和语篇层面的错误。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Taxonomy+of+Errors+in+English+as+she+is+spoke%3A+Toward+an+AI-Based+Method+of+Error+Analysis+for+EFL+Writing+Instruction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00392，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00392&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used "English as she is spoke" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.

</details>


### [83] [CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency](https://arxiv.org/abs/2512.00417)
*Jiacheng Guo, Suozhi Huang, Zixin Yao, Yifan Zhang, Yifu Lu, Jiashuo Liu, Zihao Li, Yanyan Deng, Qixin Xiao, Jia Tian, Kanghong Zhan, Tianyi Li, Xiaochen Liu, Jason Ge, Chaoyang He, Kaixuan Huang, Lin Yang, Wenhao Huang, Mengdi Wang*

**主要类别:** cs.CL

**AI概要:** CryptoBench是首个专家策划的动态基准测试，专门评估LLM代理在加密货币领域的实时分析能力，包含50个每月更新的问题，涵盖检索和预测任务，揭示了当前模型在预测分析方面的明显短板。


<details>
  <summary>更多</summary>
  
**动机:** 加密货币领域具有极端时间敏感性、对抗性信息环境和多样化专业数据源的特殊挑战，需要专门的基准来评估LLM代理的真实能力，而现有通用基准无法满足这些要求。

**方法:** 构建动态基准测试，每月50个由加密领域专家设计的问题，采用四象限分类系统（简单检索、复杂检索、简单预测、复杂预测），评估10个LLM模型在直接和代理框架下的表现。

**结果:** 发现检索-预测不平衡现象：多数领先模型在数据检索方面表现良好，但在需要预测分析的任务中表现明显不足，表明模型表面看似有事实基础但缺乏深度分析能力。

**结论:** CryptoBench为LLM代理评估提供了更具挑战性的场景，揭示了当前模型在加密货币分析中的关键局限性，特别是预测能力的不足，为未来模型改进提供了重要方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CryptoBench%3A+A+Dynamic+Benchmark+for+Expert-Level+Evaluation+of+LLM+Agents+in+Cryptocurrency，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00417，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00417&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.
  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.

</details>


### [84] [SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling](https://arxiv.org/abs/2512.00466)
*Yang Xiao, Chunpu Xu, Ruifeng Yuan, Jiashuo Wang, Wenjie Li, Pengfei Liu*

**主要类别:** cs.CL

**AI概要:** SCALE框架通过选择性分配计算资源来解决数学推理问题，基于子问题难度区分处理方式，在提升性能的同时显著降低计算成本


<details>
  <summary>更多</summary>
  
**动机:** 当前方法在所有推理子问题上均匀分配计算资源，导致挑战性问题资源不足而常规操作资源浪费，造成性能瓶颈和收益递减

**方法:** 四阶段框架：问题分解→难度评估→选择性处理模式分配（System 1处理简单问题，System 2处理复杂问题）→顺序执行与上下文传播

**结果:** 在AIME25上准确率从57.50%提升至71.25%（提升13.75个百分点），计算成本降低33%-53%

**结论:** SCALE通过选择性资源分配解决了当前方法的根本限制，在测试时计算扩展方面实现了重大进展，具有优越的资源利用效率

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SCALE%3A+Selective+Resource+Allocation+for+Overcoming+Performance+Bottlenecks+in+Mathematical+Test-time+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00466，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00466&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.

</details>


### [85] [CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning](https://arxiv.org/abs/2512.00496)
*Diego A. B. Moreira, Alef I. Ferreira, Jhessica Silva, Gabriel O. dos Santos, Gustavo Bonil, João Gondim, Marina dos Santos, Helena Maia, Simone Hashiguti, Nádia da Silva, Carolina Scarton, Helio Pedrini, Sandra Avila*

**主要类别:** cs.CL

**AI概要:** CACARA提出了一种通过涌现对齐学习实现多模态和多语言扩展的新架构，无需全量重训练即可将新模态集成到现有模型中，且仅需英语对齐数据就能支持100多种语言。


<details>
  <summary>更多</summary>
  
**动机:** 解决多模态模型需要资源密集型跨模态训练的问题，以及扩展多语言能力时同样需要大量训练资源的挑战。

**方法:** 使用涌现对齐学习范式，仅对新加入的模态进行英语对齐数据的微调，无需多语言预训练或文本编码器调整。

**结果:** 在音频到文本检索任务中R@1指标提升14.24个百分点，超越现有最佳多模态模型，训练成本仅相当于单语言模型。

**结论:** 涌现对齐范式能够高效实现多模态和多语言能力，在保持已有知识的同时显著降低计算成本，为模型扩展提供了新路径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CACARA%3A+Cross-Modal+Alignment+Leveraging+a+Text-Centric+Approach+for+Cost-Effective+Multimodal+and+Multilingual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00496，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00496&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.

</details>


### [86] [G-KV: Decoding-Time KV Cache Eviction with Global Attention](https://arxiv.org/abs/2512.00504)
*Mengqi Liao, Lu Wang, Chaoyun Zhang, Zekai Shen, Xiaowei Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Huaiyu Wan*

**主要类别:** cs.CL

**AI概要:** G-KV是一种KV缓存驱逐方法，通过全局评分机制结合局部和历史注意力分数来更准确评估token重要性，并采用强化学习和蒸馏等后训练技术优化压缩KV缓存设置下的模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的大语言模型在处理长序列时面临计算和内存挑战，现有的KV缓存压缩方法主要关注提示压缩或基于局部注意力分数的token驱逐，忽视了token的长期重要性。

**方法:** 提出G-KV方法，使用全局评分机制结合局部和历史注意力分数评估token重要性；引入强化学习和蒸馏等后训练技术优化模型在压缩KV缓存环境下的表现。

**结果:** 该方法通过更准确的token重要性评估和模型优化技术，显著提升了推理效率。

**结论:** G-KV通过全局视角的token重要性评估和专门的后训练优化，有效解决了长序列处理中的计算和内存效率问题，为KV缓存压缩提供了新的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是G-KV%3A+Decoding-Time+KV+Cache+Eviction+with+Global+Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00504，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00504&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.

</details>


### [87] [Developing a Comprehensive Framework for Sentiment Analysis in Turkish](https://arxiv.org/abs/2512.00515)
*Cem Rifki Aydin*

**主要类别:** cs.CL

**AI概要:** 该论文开发了一个全面的情感分析框架，特别针对土耳其语，也提出了英语情感分析方法。包含5项主要贡献和3项次要贡献，在特征提取、词典构建、形态分析和神经网络架构等方面取得了最先进的结果。


<details>
  <summary>更多</summary>
  
**动机:** 为土耳其语开发全面的情感分析框架，同时改进英语情感分析方法，填补土耳其语情感分析研究的空白。

**方法:** 结合无监督、半监督和监督指标生成新特征集；使用经典机器学习方法；创建领域特定极性词典；进行细致的形态分析；构建结合循环和递归神经网络的新架构；构建利用情感、句法、语义和词汇特征的新词嵌入。

**结果:** 在土耳其语和英语的不同类型数据集上超越了神经网络模型的表现；为土耳其语语料库首次应用半监督领域特定方法；在英语中重新定义上下文窗口为子句；在所有原创方法中都取得了最先进和显著的结果。

**结论:** 这是截至2020年7月土耳其语情感分析最详细和全面的研究，对英语意见分类问题也有贡献，所提出的方法可适用于其他形态丰富的语言和自然语言处理任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Developing+a+Comprehensive+Framework+for+Sentiment+Analysis+in+Turkish，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00515&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.

</details>


### [88] [Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity](https://arxiv.org/abs/2512.00552)
*Subramanyam Sahoo, Vinija Jain, Saanidhya Vats, Siddharth Mohapatra, Rui Min, Aman Chadha, Divya Chaudhary*

**主要类别:** cs.CL

**AI概要:** 论文提出了一个诊断框架来评估语言模型的数学推理能力，通过四个维度（前后一致性、传递性覆盖、反事实敏感性和扰动鲁棒性）揭示模型是否真正进行逻辑推理而非表面模式匹配。在Qwen3-0.6B模型上的测试显示，虽然准确率达70%+，但推理保真度很差，证明小模型主要依赖模式匹配。


<details>
  <summary>更多</summary>
  
**动机:** 当前数学推理评估主要依赖答案准确率，可能掩盖逻辑计算的根本性失败，需要更深入的方法来区分真正的数学推理和表面模式匹配。

**方法:** 引入包含四个互补轴的诊断框架：前后一致性、传递性覆盖、反事实敏感性和扰动鲁棒性，并在Qwen3-0.6B模型和MenatQA数据集上进行案例研究。

**结果:** 模型表面准确率达70%+，但向后一致性仅15%，传递性覆盖仅32.2%，对扰动表现脆弱，显示推理失败在传统准确率指标下不可见。

**结论:** 小模型严重依赖模式匹配而非真正逻辑计算，诊断框架具有模型无关性和可推广性，发布的评估协议可帮助研究社区超越表面准确率评估可验证的数学推理。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Catch+Me+If+You+Can%3A+How+Smaller+Reasoning+Models+Pretend+to+Reason+with+Mathematical+Fidelity，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00552，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00552&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.

</details>


### [89] [Slovak Conceptual Dictionary](https://arxiv.org/abs/2512.00579)
*Miroslav Blšták*

**主要类别:** cs.CL

**AI概要:** 本文介绍了为斯洛伐克语开发的新概念词典，这是该语言的首个此类语言学工具，旨在解决低资源语言词典数据不足的问题。


<details>
  <summary>更多</summary>
  
**动机:** 斯洛伐克语作为低资源语言，缺乏可用的机器可读语言学数据源，导致自然语言处理任务效果较差，许多任务几乎无法解决。

**方法:** 开发了一个新的概念词典作为斯洛伐克语的第一个此类语言学工具。

**结果:** 成功创建了斯洛伐克语的首个概念词典工具。

**结论:** 该词典的推出将有助于改善斯洛伐克语自然语言处理任务的性能，填补了该语言在语言学工具方面的空白。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Slovak+Conceptual+Dictionary，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00579，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00579&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.

</details>


### [90] [Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2512.00590)
*Alla Chepurova, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev*

**主要类别:** cs.CL

**AI概要:** Wikontic是一个多阶段知识图谱构建管道，通过提取带限定符的三元组、执行类型和关系约束、实体规范化来从开放域文本构建紧凑、本体一致且连接良好的知识图谱，在多个基准测试中表现优异且效率高


<details>
  <summary>更多</summary>
  
**动机:** 当前基于大语言模型的系统通常将知识图谱作为文本检索的辅助结构，未能充分利用其内在质量，需要开发能够构建高质量知识图谱的方法

**方法:** 提出多阶段管道：1)提取带限定符的候选三元组 2)基于Wikidata的类型和关系约束强制执行 3)实体规范化以减少重复 4)构建紧凑且本体一致的知识图谱

**结果:** 在MuSiQue中正确实体出现在96%的三元组中；HotpotQA达到76.0 F1，MuSiQue达到59.8 F1，匹配或超越需要文本上下文的检索增强生成基线；MINE-1基准上达到86%的信息保留率；构建效率高，仅使用不到1000个输出token

**结论:** Wikontic管道提升了生成知识图谱的质量，为在大语言模型中利用结构化知识提供了可扩展的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Wikontic%3A+Constructing+Wikidata-Aligned%2C+Ontology-Aware+Knowledge+Graphs+with+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00590，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00590&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.

</details>


### [91] [Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior](https://arxiv.org/abs/2512.00611)
*Franck Binard, Vanja Kljajevic*

**主要类别:** cs.CL

**AI概要:** Prism是一个小型组合式元语言，用于规范工具使用软件代理的行为。它基于固定的核心上下文Core1构建，提供最小语法类别和组合器，通过领域扩展实现自然语言决策规则到可执行策略的映射。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决工具使用软件代理行为规范的问题，避免引入临时控制结构，提供可组合、可分析的策略表示方法。

**方法:** 构建Core1核心上下文，包含基础类别（数字、字符串、用户提示、工具）和组合器（布尔、谓词、对、列表）。使用单一抽象操作符编写代理策略，通过领域特定的小语法扩展核心。

**结果:** 成功开发出Prism语言，并通过恒温器控制、家庭安全、电商推荐和医疗监控等实例验证了其有效性。

**结论:** Prism在语言学层面实现了核心语法与领域词汇的清晰分离，在工程层面提供了紧凑的接口语言，使动作空间明确且易于分析、验证和安全约束。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prism%3A+A+Minimal+Compositional+Metalanguage+for+Specifying+Agent+Behavior，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00611，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00611&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.

</details>


### [92] [ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization](https://arxiv.org/abs/2512.00617)
*Omer Jauhar Khan*

**主要类别:** cs.CL

**AI概要:** ART框架通过锦标赛式ELO排名和多智能体推理系统优化LLM输出，相比单模型方法在准确性、连贯性和可靠性方面有显著提升


<details>
  <summary>更多</summary>
  
**动机:** 单一语言模型响应存在不一致性、幻觉问题，且在不同查询领域质量参差不齐，需要系统化优化方法

**方法:** 采用锦标赛式ELO排名机制和多智能体协作推理，通过可配置的锦标赛参数、动态智能体选择和多种共识融合策略

**结果:** 实验显示在整体质量指标上提升8.4%，ELO评级收敛的R²值超过0.96，响应质量显著优于基线单模型方法

**结论:** ART框架为需要高质量验证LLM响应的应用提供了可扩展的生产就绪解决方案，能有效提升模型输出的可靠性和一致性

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ART%3A+Adaptive+Response+Tuning+Framework+--+A+Multi-Agent+Tournament-Based+Approach+to+LLM+Response+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00617，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00617&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.

</details>


### [93] [Sycophancy Claims about Language Models: The Missing Human-in-the-Loop](https://arxiv.org/abs/2512.00656)
*Jan Batzner, Volker Stocker, Stefan Schmid, Gjergji Kasneci*

**主要类别:** cs.CL

**AI概要:** 论文回顾了大型语言模型中的奉承回应模式，分析了测量方法挑战并提出了五个核心操作化定义，指出当前研究缺乏对人类感知的评估，并提供了未来研究建议。


<details>
  <summary>更多</summary>
  
**动机:** 现有文献中越来越多地声称大型语言模型存在奉承回应模式，但当前研究在测量方法和人类感知评估方面存在不足，需要系统性地分析这些问题。

**方法:** 回顾和分析了测量LLM奉承性的方法论挑战，识别了五个核心操作化定义，并对比了奉承回应与AI对齐中相关概念的区别。

**结果:** 研究发现奉承性本质上是人类中心的，但当前研究没有评估人类感知；同时难以区分奉承回应与AI对齐中的相关概念。

**结论:** 论文为未来研究提供了可行的建议，强调需要更好的测量方法和包含人类感知评估的研究框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sycophancy+Claims+about+Language+Models%3A+The+Missing+Human-in-the-Loop，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00656，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00656&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.

</details>


### [94] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

**主要类别:** cs.CL

**AI概要:** 论文提出了一个可视化知识图谱框架，通过将模型生成内容与知识源关联并显示置信度，帮助用户识别和纠正大语言模型的幻觉问题，建立人机协作的反馈循环机制。


<details>
  <summary>更多</summary>
  
**动机:** 企业环境中大语言模型在结合专有知识时容易出现可信度高的幻觉，现有缓解策略成本高昂且无法提供确定性保证，需要更有效的解决方案。

**方法:** 开发了一个框架，将专有知识和模型生成内容组织成交互式可视化知识图谱，链接模型断言与真实来源并显示置信度水平。

**结果:** 通过可视化界面，用户可以诊断不一致性、识别薄弱推理链并提供纠正反馈，形成结构化反馈循环。

**结论:** 该人机协作工作流程能够增强模型可靠性并持续改进响应质量，为解决大语言模型幻觉问题提供了有效途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Graphing+the+Truth%3A+Structured+Visualizations+for+Automated+Hallucination+Detection+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00663，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00663&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [95] [A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data](https://arxiv.org/abs/2512.00673)
*Breanna E. Green, Ashley L. Shea, Pengfei Zhao, Drew B. Margolin*

**主要类别:** cs.CL

**AI概要:** GPT-4在细微语言分类任务中表现不佳，尽管在提示中包含标签定义可能有所帮助，但整体上仍难以准确分类，建议在此类任务中谨慎使用ChatGPT


<details>
  <summary>更多</summary>
  
**动机:** 评估ChatGPT在复杂任务（如包含细微语言的分类和标注）中的性能，特别是考虑到大型语言模型技术快速发展的背景下

**方法:** 比较GPT-4与人工标注者的性能，测试ChatGPT 3.5、4和4o版本，设计四种提示风格作为输入，评估精确率、召回率和F1分数，进行定量和定性分析

**结果:** 定量和定性评估显示，虽然提示中包含标签定义可能提升性能，但GPT-4整体在细微语言分类方面存在困难，定性分析揭示了四个具体发现

**结论:** 在涉及细微语言的分类任务中使用ChatGPT应谨慎行事，因为其在此类复杂任务中的表现仍有局限

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Comparison+of+Human+and+ChatGPT+Classification+Performance+on+Complex+Social+Media+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00673&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.

</details>


### [96] [FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case](https://arxiv.org/abs/2512.00745)
*Md Abdullah Al Kafi, Sumit Kumar Banshal*

**主要类别:** cs.CL

**AI概要:** 提出基于Transformer的语言无关词性标注框架，使用孟加拉语和印地语作为案例研究，仅需少量代码修改即可跨语言移植，在低资源语言中取得高准确率。


<details>
  <summary>更多</summary>
  
**动机:** 针对低资源语言缺乏足够标注数据和专门NLP工具的问题，开发一个可快速跨语言适配的词性标注框架，减少模型设计和调优的开销。

**方法:** 使用基于Transformer的架构，设计模块化开源框架，仅需3行框架特定代码即可实现从孟加拉语到印地语的适配，利用预训练Transformer模型的强大表示能力。

**结果:** 在孟加拉语和印地语上分别达到96.85%和97%的词级准确率，保持较高的F1分数，尽管存在数据集不平衡和语言重叠问题。

**结论:** 该框架展示了出色的可移植性和性能，模块化设计使研究人员能专注于语言预处理和数据集优化，这对推进低资源语言的NLP研究至关重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FastPOS%3A+Language-Agnostic+Scalable+POS+Tagging+Framework+Low-Resource+Use+Case，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00745&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.

</details>


### [97] [Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation](https://arxiv.org/abs/2512.00789)
*Xiaodong Cai, Hai Lin, Shaoxiong Zhan, Weiqi Luo, Hong-Gee Kim, Hongyan Hao, Yu Yang, Hai-Tao Zheng*

**主要类别:** cs.CL

**AI概要:** Entropy Equilibrium Sampling (EES)是一种无需超参数调优的token采样策略，通过信息论平衡归一化熵和概率质量来动态调整候选集，在各种模型架构和温度设置下都能保持良好性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有token采样方法需要额外的超参数调优，增加了部署复杂性，需要一种无需超参数调整的采样策略。

**方法:** 提出熵平衡采样(EES)，基于信息论原理，通过平衡归一化熵和概率质量来动态调整候选集，无需超参数调优。

**结果:** EES在推理和生成任务的各种模型架构中表现一致良好，在保持多样性的同时提供竞争性的准确性和连贯性。

**结论:** EES通过消除超参数调优需求，大大简化了部署过程并提升了性能，是一种有效的token采样策略。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Auxiliary-Hyperparameter-Free+Sampling%3A+Entropy+Equilibrium+for+Text+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00789，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00789&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES

</details>


### [98] [Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy](https://arxiv.org/abs/2512.00829)
*Md Mehrab Hossain Opi, Sumaiya Khan, Moshammad Farzana Rahman*

**主要类别:** cs.CL

**AI概要:** 该研究探索使用自动混合精度(AMP)训练来提高孟加拉语NLP任务的训练效率，在保持模型性能的同时显著减少计算资源需求。


<details>
  <summary>更多</summary>
  
**动机:** 孟加拉语NLP开发面临计算资源有限的问题，训练模型需要大量计算资源和时间，特别是在高端硬件访问受限的环境中。

**方法:** 使用自动混合精度(AMP)训练，通过动态混合16位和32位浮点计算来降低GPU内存需求并加速训练。在四个标准孟加拉语NLP任务(情感分析、命名实体识别、错误分类和问答)上评估了四种基于transformer的模型。

**结果:** AMP训练加速了44.5%，内存消耗减少了17.6%，同时F1分数保持在完整精度基线的99.7%以内。

**结论:** AMP技术有潜力通过在硬件受限环境中降低计算门槛，使更多开发者能够获得最先进的NLP能力，从而促进孟加拉语NLP的发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accelerating+Bangla+NLP+Tasks+with+Automatic+Mixed+Precision%3A+Resource-Efficient+Training+Preserving+Model+Efficacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00829&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.

</details>


### [99] [WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](https://arxiv.org/abs/2512.00837)
*Yukang Lin, Jiahao Shao, Shuoran Jiang, Wentao Zhu, Bingjie Lu, Xiangping Wu, Joanna Siebert, Qingcai Chen*

**主要类别:** cs.CL

**AI概要:** WaterSearch是一种新颖的基于搜索的句子级水印框架，通过控制种子池实现多样化并行生成，在保持95%检测强度的同时，比现有方法平均提升51.01%的性能，并具有强大的抗攻击能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有LLM文本水印方法在检测鲁棒性和文本质量之间存在权衡，强水印信号会降低下游任务性能，特别是在短文本和低熵输出场景中表现不佳。

**方法:** 提出控制种子池的嵌入方案，实现多样化并行水印文本生成。WaterSearch框架联合优化分布保真度和水印信号特征，并配备句子级检测方法。

**结果:** 在10个不同任务的3个流行LLM上测试，平均性能提升51.01%，短文本和低熵输出场景分别提升47.78%和36.47%。在插入、同义词替换和改写攻击下保持高检测率。

**结论:** WaterSearch有效解决了水印检测强度与文本质量的权衡问题，在各种场景和攻击下都表现出优越的性能和鲁棒性，为LLM生成内容的安全保护提供了有效解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WaterSearch%3A+A+Quality-Aware+Search-based+Watermarking+Framework+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00837，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00837&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.
  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.

</details>


### [100] [Less is More: Resource-Efficient Low-Rank Adaptation](https://arxiv.org/abs/2512.00878)
*Chunlin Tian, Xuyang Wei, Huanrong Liu, Zhijiang Guo, Li Li*

**主要类别:** cs.CL

**AI概要:** EffiLoRA是一种资源高效的低秩适应方法，通过统一A矩阵和选择性更新B矩阵来减少参数冗余和训练成本，在多种模态任务中优于传统LoRA方法。


<details>
  <summary>更多</summary>
  
**动机:** 传统LoRA方法在大型语言模型微调中存在显著开销和参数干扰问题，尽管已有工作尝试解耦矩阵更新，但训练成本仍然很高。

**方法:** 从矩阵间和层内参数冗余角度重新审视LoRA，采用跨所有Transformer层的统一A矩阵，并引入运行时选择性B矩阵更新机制，动态平衡资源预算和模型性能。

**结果:** EffiLoRA在常识推理、视觉指令调优和图像生成等多种模态任务中持续优于LoRA，显示出更高的效率和鲁棒性。

**结论:** EffiLoRA提供了一个轻量级且可泛化的方法，有效解决了LoRA的参数冗余和效率问题，适用于语言、多模态和扩散模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Less+is+More%3A+Resource-Efficient+Low-Rank+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00878，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00878&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.

</details>


### [101] [Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios](https://arxiv.org/abs/2512.00920)
*Jianxiang Zang, Yongda Wei, Ruxue Bai, Shiyu Jiang, Nijia Mo, Binhong Li, Qiang Sun, Hui Liu*

**主要类别:** cs.CL

**AI概要:** Reward Auditor：一个基于假设检验的框架，用于评估奖励模型在真实世界扰动下的条件可靠性（Suitability），通过统计显著性检验和效应大小量化来推断RM的系统性漏洞


<details>
  <summary>更多</summary>
  
**动机:** 当前奖励模型评估方法仅关注特定场景下的偏好感知准确率，无法揭示RM在真实世界场景中的关键漏洞，需要评估RM在特定真实世界扰动下的条件可靠性

**方法:** 提出Reward Auditor假设检验框架，通过审计RM偏好感知置信度的分布退化来量化统计显著性和效应大小，推断RM在不同真实世界场景中的漏洞确定性和严重性

**结果:** 该框架能够系统性地评估RM在各种真实世界扰动场景下的脆弱性，为构建更安全、鲁棒和可信的下一代LLM对齐系统奠定基础

**结论:** Reward Auditor为解决奖励模型在真实世界场景中的可靠性评估问题提供了科学审计方法，是实现可验证安全LLM对齐的关键基础

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reward+Auditor%3A+Inference+on+Reward+Modeling+Suitability+in+Real-World+Perturbed+Scenarios，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00920，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00920&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.

</details>


### [102] [Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study](https://arxiv.org/abs/2512.00931)
*Imane Jaaouine, Ross D. King*

**主要类别:** cs.CL

**AI概要:** 本研究通过提示工程方法测试了6个LLM在8篇科学论文摘要的零-shot摘要任务中，发现上下文重复(CR)和随机添加(RA)能显著改善摘要与原文的词汇对齐，表明提示工程可以减少幻觉现象。


<details>
  <summary>更多</summary>
  
**动机:** 研究大型语言模型在零-shot科学文本摘要中产生的上下文不一致幻觉问题，探索提示工程方法能否缓解这一问题。

**方法:** 使用6个指令调优的LLM，对8篇酵母生物技术论文摘要应用7种提示方法（基线提示、复杂指令、上下文重复、随机添加），生成336个摘要，用6个指标评估词汇和语义对齐。

**结果:** 上下文重复(CR)和随机添加(RA)方法显著提高了LLM生成摘要与原文的词汇对齐度。

**结论:** 提示工程方法，特别是上下文重复和随机添加策略，在零-shot科学摘要任务中具有减少幻觉的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Hallucinations+in+Zero-Shot+Scientific+Summarisation%3A+A+Pilot+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00931，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00931&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.

</details>


### [103] [DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics](https://arxiv.org/abs/2512.00938)
*Ahmed Mustafa Younes*

**主要类别:** cs.CL

**AI概要:** DeformAr是一个针对阿拉伯语命名实体识别(NER)的调试评估框架，通过跨组件分析和行为分析来解释阿拉伯语与英语Transformer模型性能差异的原因。


<details>
  <summary>更多</summary>
  
**动机:** Transformer模型在英语NLP中表现出色，但在阿拉伯语NER任务中效果有限，主要由于分词、数据集质量和标注不一致等问题，现有研究未能综合分析这些因素的联合影响。

**方法:** 提出DeformAr框架，包含数据提取库和交互式仪表板，采用两阶段分析：跨组件分析提供系统性诊断指标，行为分析结合可解释性技术、token级指标和表示空间分析。

**结果:** 该框架是首个阿拉伯语专用的基于组件的可解释性工具，能够检测模型行为并通过底层表示模式和数据因素进行解释。

**结论:** DeformAr为资源匮乏语言的模型分析提供了重要资源，通过组件感知的诊断过程解决了阿拉伯语NER系统性能差异的根本原因。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DeformAr%3A+Rethinking+NER+Evaluation+through+Component+Analysis+and+Visual+Analytics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00938，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00938&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.
  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.
  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the "what," "how," and "why" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.

</details>


### [104] [Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data](https://arxiv.org/abs/2512.00946)
*Alvaro Paredes Amorin, Andre Python, Christoph Weisser*

**主要类别:** cs.CL

**AI概要:** 轻量级开源大语言模型（如Qwen3 8B和Llama3 8B）在金融情感分析任务中表现优异，即使仅使用5%的训练数据也能在零样本和少样本学习中取得竞争性性能，为资源受限的研究者提供了成本效益高的解决方案。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在金融市场分析中越来越重要，但依赖大量计算资源和专有数据集，成本高昂且对许多研究者不可用。研究旨在探索轻量级开源LLM在不同规模、来源、格式和语言的金融数据集上的泛化能力。

**方法:** 比较了基准金融NLP模型FinBERT与三个开源轻量级LLM（DeepSeek-LLM 7B、Llama3 8B Instruct、Qwen3 8B）在五个公开数据集上的表现：FinancialPhraseBank、Financial Question Answering、Gold News Sentiment、Twitter Sentiment和Chinese Finance Sentiment。

**结果:** LLM（特别是Qwen3 8B和Llama3 8B）在大多数场景中表现最佳，即使仅使用5%的训练数据。这些结果在零样本和少样本学习场景中均成立。

**结论:** 轻量级开源大语言模型构成了成本效益高的选择，即使在仅使用有限标注语料子集的情况下，也能在异质文本数据上实现竞争性性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fine-tuning+of+lightweight+large+language+models+for+sentiment+classification+on+heterogeneous+financial+textual+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00946，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00946&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.

</details>


### [105] [Table as a Modality for Large Language Models](https://arxiv.org/abs/2512.00947)
*Liyao Li, Chao Ye, Wentao Ye, Yifei Sun, Zhe Jiang, Haobo Wang, Jiaming Tian, Yiming Zhang, Ningtao Wang, Xing Fu, Gang Chen, Junbo Zhao*

**主要类别:** cs.CL

**AI概要:** 论文提出TAMO方法，将表格视为独立模态与文本标记整合，通过超图神经网络编码表格结构信息，显著提升大语言模型在表格推理任务上的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有方法简单地将表格数据序列化输入大语言模型，导致结构信息丢失，即使最先进的LLM也难以有效处理表格数据。

**方法:** 提出TAMO多模态框架，使用超图神经网络作为全局表格编码器，将表格作为独立模态与主流大语言模型无缝集成。

**结果:** 在多个基准数据集上获得显著改进，平均相对增益达到42.65%。

**结论:** 将表格作为独立模态处理并保留结构信息，能够有效提升大语言模型在表格推理任务中的泛化能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Table+as+a+Modality+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00947&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.

</details>


### [106] [Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent](https://arxiv.org/abs/2512.00986)
*Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King*

**主要类别:** cs.CL

**AI概要:** Dr.Mi-Bench是一个面向科学领域深度研究代理的模块化集成基准测试，包含200个人工标注的科学文献实例，通过端到端和隔离评估两种模式评估规划、检索和推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试过于关注检索而忽视高级规划和推理能力，且偏向通用领域而非科学领域这一深度研究代理的核心应用场景。

**方法:** 提出模块化集成评估范式Dr.Mi-Eval，利用学术论文的丰富结构，通过端到端评估（针对DR代理）和隔离评估（针对基础LLM）两种互补模式进行评估。

**结果:** 实验结果显示代理表现出专业优势但共享关键弱点，特别是在多源检索和跨科学领域一致性方面存在不足，提升高级规划能力是释放基础LLM推理潜力的关键因素。

**结论:** Dr.Mi-Bench通过暴露可操作的失败模式，为开发更可靠的学术研究助手提供了诊断工具和指导方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dr.Mi-Bench%3A+A+Modular-integrated+Benchmark+for+Scientific+Deep+Research+Agent，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00986&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.

</details>


### [107] [Advancing Academic Chatbots: Evaluation of Non Traditional Outputs](https://arxiv.org/abs/2512.00991)
*Nicole Favero, Francesca Salute, Daniel Hardt*

**主要类别:** cs.CL

**AI概要:** 本研究比较了Graph RAG和Advanced RAG两种检索策略在问答任务中的表现，并评估了LLM生成非传统学术输出（幻灯片和播客脚本）的能力。GPT 4o mini在两种任务中表现最佳，人类评估在检测布局和风格缺陷方面至关重要。


<details>
  <summary>更多</summary>
  
**动机:** 扩展大语言模型评估范围，超越传统的事实问答和摘要任务，探索检索策略比较和非传统学术输出生成能力。

**方法:** 实现原型系统（LLaMA 3 70B + GPT 4o mini），使用人类评分（11个质量维度）和LLM评判进行问答评估，测试基于文档检索的幻灯片和播客生成。

**结果:** GPT 4o mini配合Advanced RAG产生最准确回答；Graph RAG改进有限且产生更多幻觉；GPT 4o mini在内容生成中表现最佳，LLaMA 3在叙事连贯性方面有潜力。

**结论:** 人类评估对检测新兴学术输出的布局和风格问题至关重要，需要结合人类和LLM评估方法；Advanced RAG优于Graph RAG；GPT 4o mini在多种任务中表现最优。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Advancing+Academic+Chatbots%3A+Evaluation+of+Non+Traditional+Outputs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.00991，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.00991&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.

</details>


### [108] [When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals](https://arxiv.org/abs/2512.01037)
*Riad Ahmed Anonto, Md Labid Al Nahiyan, Md Tanvir Hassan, Ch. Md. Rakin Haider*

**主要类别:** cs.CL

**AI概要:** 论文提出了一种名为"语义混淆"的失败模式来测量语言模型在拒绝无害提示时的局部不一致性，并开发了ParaGuard数据集和三个模型无关的指标来量化这种不一致性。


<details>
  <summary>更多</summary>
  
**动机:** 当前的安全对齐语言模型经常错误拒绝无害提示，但现有评估只关注全局拒绝率，忽略了模型接受某种表述却拒绝其近义表述的局部不一致问题。

**方法:** 构建了包含1万个提示的ParaGuard语料库，包含保持意图不变的释义簇；提出了三个基于token级别的指标：混淆指数、混淆率和混淆深度，使用token嵌入、下一token概率和困惑度信号。

**结果:** 实验显示全局拒绝率掩盖了关键结构：某些设置中存在全局不稳定边界，其他设置中存在局部不一致区域，而更严格的拒绝并不总是增加不一致性。

**结论:** 混淆感知审计能够区分系统拒绝的频率和拒绝的合理性，为开发者提供了减少错误拒绝同时保持安全性的实用信号。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+Safety+Blocks+Sense%3A+Measuring+Semantic+Confusion+in+LLM+Refusals，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01037&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.

</details>


### [109] [ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages](https://arxiv.org/abs/2512.01077)
*Neha Joshi, Pamir Gogoi, Aasim Mirza, Aayush Jansari, Aditya Yadavalli, Ayushi Pandey, Arunima Shukla, Deepthi Sudharsan, Kalika Bali, Vivek Seshadri*

**主要类别:** cs.CL

**AI概要:** 该研究收集了来自印度东部10种濒危语言的1060道传统食谱数据集ELR-1000，评估了大语言模型在翻译这些文化特色内容时的表现，发现提供文化背景信息能显著提升翻译质量，并呼吁开发面向弱势语言的技术。


<details>
  <summary>更多</summary>
  
**动机:** 解决大语言模型在低资源、文化特色语言处理方面的不足，促进对濒危语言和文化多样性的保护，推动更公平的语言技术发展。

**方法:** 通过为数字素养较低贡献者设计的移动界面，从印度东部偏远农村社区众包收集1060道传统食谱，构建多模态数据集，并评估多种先进大语言模型在翻译这些食谱时的性能。

**结果:** 大语言模型在处理低资源、文化特色语言时存在困难，但提供语言背景信息、翻译示例和文化保护指南等针对性上下文能显著提高翻译质量。

**结论:** 需要开发专门针对弱势语言和领域的基准测试，以推动更公平和文化敏感的语言技术发展，为此研究发布了ELR-1000数据集供NLP社区使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ELR-1000%3A+A+Community-Generated+Dataset+for+Endangered+Indic+Indigenous+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01077，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01077&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.

</details>


### [110] [How do we measure privacy in text? A survey of text anonymization metrics](https://arxiv.org/abs/2512.01109)
*Yaxuan Ren, Krithika Ramesh, Yaxing Yao, Anjalie Field*

**主要类别:** cs.CL

**AI概要:** 本文通过系统调查澄清和协调文本隐私保护评估指标，分析了47篇论文中的6种隐私概念，评估其与法律标准(HIPAA和GDPR)及用户期望的契合度，为文本匿名化提供更稳健、可比较且符合法律要求的隐私评估指导。


<details>
  <summary>更多</summary>
  
**动机:** 文本匿名化对敏感数据领域的NLP研究至关重要，但评估匿名化方法是否充分保护隐私仍是一个开放挑战，需要系统梳理和协调不同的隐私评估指标。

**方法:** 手动审查47篇报告隐私指标的论文，识别比较6种不同的隐私概念，分析相关指标如何捕捉隐私风险的不同方面，并评估这些概念与法律隐私标准(HIPAA和GDPR)以及基于HCI研究的用户期望的一致性。

**结果:** 识别了6种不同的隐私概念及其评估指标，分析了这些指标在捕捉隐私风险方面的差异，发现当前实践与法律标准和用户期望存在差距。

**结论:** 研究为导航隐私评估方法提供了实用指导，强调了当前实践的不足，旨在促进文本匿名化中更稳健、可比较且具有法律意识的隐私评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+do+we+measure+privacy+in+text%3F+A+survey+of+text+anonymization+metrics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01109，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01109&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.

</details>


### [111] [DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](https://arxiv.org/abs/2512.01174)
*Hyunjun Kim, Sooyoung Ryu*

**主要类别:** cs.CL

**AI概要:** DrawingBench是一个用于评估自主AI代理可信度的透明验证框架，通过空间推理任务和GUI操作序列来评估LLM代理的可靠性，提供可重现的评分和可审计的行为检查。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准测试缺乏透明度和可审计性，无法可靠评估自主AI代理的行为，需要建立可信的验证机制。

**方法:** 开发包含250个多样化提示的框架，涵盖20个类别和4个难度级别，使用8个客观标准进行确定性评估，并通过多轮反馈实现外部监督机制。

**结果:** 评估4个最先进LLM显示：在结构化外部反馈下达到92.8%完美性能（平均提升3.2%，复杂场景提升32.8%），但在工具状态管理和长程规划方面存在系统性错误模式。

**结论:** 透明评估框架可建立对代理系统的信任，外部监督比自我校正更可靠，规范清晰度比任务复杂度更重要，开源框架为可信代理评估提供模板。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DrawingBench%3A+Evaluating+Spatial+Reasoning+and+UI+Interaction+Capabilities+of+Large+Language+Models+through+Mouse-Based+Drawing+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01174，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01174&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench

</details>


### [112] [TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness](https://arxiv.org/abs/2512.01183)
*Yongxin Zhou, Philippe Mulhem, Didier Schwab*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一个RAG扰动-温度分析框架，系统研究了文本扰动与温度设置之间的交互作用，发现高温设置会放大扰动敏感性，为RAG系统的鲁棒性评估提供了诊断基准和实用指南。


<details>
  <summary>更多</summary>
  
**动机:** 现有的RAG系统评估通常孤立地检查检索质量和生成参数，忽略了它们之间的交互作用，特别是在噪声检索条件下温度设置对系统鲁棒性的影响。

**方法:** 提出了一个全面的RAG扰动-温度分析框架，对检索文档应用三种不同类型的扰动，并在不同温度设置下进行多次LLM运行实验，使用HotpotQA数据集在开源和专有LLM上进行广泛测试。

**结果:** 实验表明性能退化呈现特定模式：高温设置持续放大对扰动的脆弱性，某些扰动类型在温度范围内表现出非线性敏感性。

**结论:** 该研究提供了三个关键贡献：(1)RAG鲁棒性评估的诊断基准，(2)量化扰动-温度交互的分析框架，(3)噪声检索条件下模型选择和参数调优的实用指南。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TempPerturb-Eval%3A+On+the+Joint+Effects+of+Internal+Temperature+and+External+Perturbations+in+RAG+Robustness，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01183，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01183&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.

</details>


### [113] [Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks](https://arxiv.org/abs/2512.01191)
*Krithik Vishwanath, Mrigayu Ghosh, Anton Alyakin, Daniel Alexander Alber, Yindalon Aphinyanaphongs, Eric Karl Oermann*

**主要类别:** cs.CL

**AI概要:** 研究发现通用大语言模型在临床医学表现上优于专业临床AI助手，GPT-5表现最佳，而OpenEvidence和UpToDate在多个关键指标上存在不足，强调临床AI工具需要独立评估。


<details>
  <summary>更多</summary>
  
**动机:** 临床AI助手被宣传为比通用大语言模型更安全可靠，但缺乏独立量化评估，存在证据缺口，影响医疗诊断、分诊和指南解释等重要环节。

**方法:** 使用包含MedQA（医学知识）和HealthBench（临床医生对齐）任务的1000项微型基准测试，评估两种广泛部署的临床AI系统（OpenEvidence和UpToDate Expert AI）与三种最先进通用LLM（GPT-5、Gemini 3 Pro和Claude Sonnet 4.5）的表现。

**结果:** 通用模型一致优于临床工具，GPT-5得分最高。OpenEvidence和UpToDate在完整性、沟通质量、情境意识和基于系统的安全推理方面存在缺陷。

**结论:** 市场上推广的临床决策支持工具可能落后于前沿LLM，在部署到面向患者的工作流程前迫切需要透明、独立的评估。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalist+Large+Language+Models+Outperform+Clinical+Tools+on+Medical+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01191，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01191&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.

</details>


### [114] [Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework](https://arxiv.org/abs/2512.01198)
*Jiatong Han*

**主要类别:** cs.CL

**AI概要:** 本研究采用人机协作框架，通过提示工程引导大语言模型识别中医经典中的隐喻和转喻，生成的概念化翻译在所有认知维度上表现最优，为中医等概念密集文本翻译提供了高效可复现的方法路径。


<details>
  <summary>更多</summary>
  
**动机:** 现有中医英译主要依赖字面翻译，目标语读者难以重构底层概念网络并应用于临床实践，需要更有效的隐喻和转喻传达方法。

**方法:** 采用人机协作(HITL)框架，从《黄帝内经》选取4个理论核心篇章，通过提示式认知支架引导DeepSeek V3.1识别隐喻转喻并进行理论传达翻译。

**结果:** 提示调整后的LLM翻译在五个认知维度上表现最佳，具有高跨模型和跨角色一致性，访谈揭示了人机翻译差异及有效的隐喻转喻传递策略。

**结论:** 研究为中医等古老概念密集文本的翻译提供了认知性、高效且可复现的人机协作方法论路径，证明了提示工程在概念传达中的有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conveying+Imagistic+Thinking+in+Traditional+Chinese+Medicine+Translation%3A+A+Prompt+Engineering+and+LLM-Based+Evaluation+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01198，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01198&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Traditional Chinese Medicine (TCM) theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop (HITL) framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis (IPA). Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient, and replicable HITL methodological pathway for the translation of ancient, concept-dense texts such as TCM.

</details>


### [115] [Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language](https://arxiv.org/abs/2512.01256)
*Ekha Morang, Surhoni A. Ngullie, Sashienla Longkumer, Teisovi Angami*

**主要类别:** cs.CL

**AI概要:** 这是第一篇针对Nagamese语言（那加皮钦语）进行情感分析和情绪分类的研究论文，使用朴素贝叶斯和支持向量机方法，构建了包含1,195个词汇的情感词典


<details>
  <summary>更多</summary>
  
**动机:** 虽然英语、印地语等资源丰富的语言已有大量情感分析研究，但Nagamese作为印度东北部的克里奥尔语，尚未有相关研究，这是该领域的首次尝试

**方法:** 构建Nagamese情感极性词典（1,195个词汇），结合其他特征，使用监督机器学习方法（朴素贝叶斯和支持向量机）进行情感分析

**结果:** 建立了Nagamese语言的情感分析框架和词典资源，为这种低资源语言的情感分析奠定了基础

**结论:** 该研究填补了Nagamese语言情感分析的空白，为这种克里奥尔语的自然语言处理研究提供了重要资源和方法框架

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sentiment+Analysis+and+Emotion+Classification+using+Machine+Learning+Techniques+for+Nagamese+Language+-+A+Low-resource+Language，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01256，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01256&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na"ive Bayes and Support Vector Machines.
  Keywords: Nagamese, NLP, sentiment analysis, machine learning

</details>


### [116] [SUPERChem: A Multimodal Reasoning Benchmark in Chemistry](https://arxiv.org/abs/2512.01274)
*Zehua Zhao, Zhixian Huang, Junren Li, Siyu Lin, Junting Zhou, Fengqi Cao, Kun Zhou, Rui Ge, Tingting Long, Yuexiang Zhu, Yan Liu, Jie Zheng, Junnian Wei, Rong Zhu, Peng Zou, Wenyu Li, Zekai Cheng, Tian Ding, Yaxuan Wang, Yizhao Yan, Tingru Wei, Haowei Ming, Weijie Mao, Chen Sun, Yiming Liu, Zichen Wang, Zuo Zhang, Tong Yang, Hao Ma, Zhen Gao, Jian Pei*

**主要类别:** cs.CL

**AI概要:** SUPERChem是一个专家级化学推理基准测试，包含500个多模态和纯文本格式的问题，通过推理路径保真度评分评估大语言模型的化学推理能力，显示当前最佳模型仍低于人类表现水平。


<details>
  <summary>更多</summary>
  
**动机:** 当前化学推理基准存在任务过于简化、缺乏过程评估以及与专家级化学技能不匹配的问题，需要更全面可靠的评估框架。

**方法:** 开发包含500个专家策划推理密集型化学问题的SUPERChem基准，采用多模态和文本格式，通过迭代筛选流程消除有缺陷项目，每个问题配有专家编写的解决方案路径，使用推理路径保真度评分方法。

**结果:** 人类基线准确率为40.3%，最佳模型GPT-5(High)仅达到38.5%，Gemini 2.5 Pro为37.9%，DeepSeek-V3.1-Think为37.3%，所有模型均低于人类表现。

**结论:** SUPERChem能够激发多步骤多模态推理，揭示视觉信息的模型依赖性影响，区分高保真推理者和启发式推理者，为LLMs向专家级化学智能发展提供挑战性基准和可靠评估框架。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SUPERChem%3A+A+Multimodal+Reasoning+Benchmark+in+Chemistry，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01274，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01274&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.

</details>


### [117] [Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning](https://arxiv.org/abs/2512.01282)
*Jiahao Yuan, Zhiqing Cui, Hanqing Wang, Yuansheng Gao, Yucheng Zhou, Usman Naseem*

**主要类别:** cs.CL

**AI概要:** 该论文提出了KardiaBench基准数据集和Kardia-R1框架，通过可解释的强化学习方法提升对话代理的身份感知情感推理能力，在多个LLM骨干网络上取得了优于现有方法的表现。


<details>
  <summary>更多</summary>
  
**动机:** 现有对话系统存在两个主要局限：(1)依赖缺乏持久用户身份的情境中心数据集，难以捕捉个性化情感细微差别；(2)依赖不透明、粗糙的奖励信号，阻碍可验证共情推理的发展。

**方法:** 构建了包含178,080个QA对的大规模基准数据集KardiaBench，采用模型在环的流水线构建方法。提出Kardia-R1框架，使用基于GRPO的Rubric-ERL强化学习方法，通过可解释的评分标准奖励来训练模型进行分步共情认知。

**结果:** 在四个LLM骨干网络上的广泛实验表明，Kardia-R1在情感准确性、共情能力、相关性、身份一致性和安全性方面 consistently优于其他方法。

**结论:** KardiaBench数据集和Kardia-R1框架为解决身份感知情感推理问题提供了有效解决方案，通过可解释的强化学习方法显著提升了对话代理的共情能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Kardia-R1%3A+Unleashing+LLMs+to+Reason+toward+Understanding+and+Empathy+for+Emotional+Support+via+Rubric-as-Judge+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01282，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01282&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.

</details>


### [118] [Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding](https://arxiv.org/abs/2512.01316)
*Koki Natsumi, Hiroyuki Deguchi, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe*

**主要类别:** cs.CL

**AI概要:** 提出AC-PMBR解码方法，通过知识蒸馏模型指导分数矩阵补全，在保持计算成本的同时显著提升翻译质量


<details>
  <summary>更多</summary>
  
**动机:** 传统MBR解码需要二次时间计算候选对分数，PMBR方法虽然减少计算但会降低翻译质量，需要改进质量与成本的权衡

**方法:** 使用知识蒸馏模型指导矩阵补全，提出协议约束的PMBR解码方法，部分评估质量分数并用矩阵补全算法完成缺失分数

**结果:** AC-PMBR将矩阵补全近似误差降低最多3倍，在WMT'23 En↔De翻译任务中相比PMBR获得更高翻译质量，计算成本相当

**结论:** AC-PMBR解码方法有效改善了翻译质量与计算成本之间的权衡，为MBR解码提供了更实用的解决方案

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agreement-Constrained+Probabilistic+Minimum+Bayes+Risk+Decoding，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01316，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01316&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\leftrightarrow$De translation tasks.

</details>


### [119] [MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis](https://arxiv.org/abs/2512.01369)
*Md. Rafiul Biswas, Firoj Alam, Wajdi Zaghouani*

**主要类别:** cs.CL

**AI概要:** MARSAD是一个多功能阿拉伯语NLP平台，专注于社交媒体实时监控分析，提供情感分析、宣传检测、事实核查等功能，支持研究人员和非技术用户使用。


<details>
  <summary>更多</summary>
  
**动机:** 为阿拉伯语世界提供实时社交媒体监控分析工具，满足对多维度内容分析的需求，使非技术用户也能进行专业的数据分析。

**方法:** 集成灵活文档存储与结构化数据管理的后端架构，支持大规模多模态数据处理，通过API密钥安全抓取社交媒体数据，提供用户友好的前端界面。

**结果:** 开发了一个功能全面的NLP平台，能够处理实时和历史社交媒体数据，生成详细的可视化报告，涵盖情感、情绪、宣传、事实核查和仇恨言论检测等多个维度。

**结论:** MARSAD平台成功实现了对阿拉伯语社交媒体的高效监控分析，为研究者和普通用户提供了强大的多维度数据分析能力，具有重要的实际应用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MARSAD%3A+A+Multi-Functional+Tool+for+Real-Time+Social+Media+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01369，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01369&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.

</details>


### [120] [DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis](https://arxiv.org/abs/2512.01410)
*Ruohan Zhou, Jiachen Yuan, Churui Yang, Wenzheng Huang, Guoyan Zhang, Shiyao Wei, Jiazhen Hu, Ning Xin, Md Maruf Hasan*

**主要类别:** cs.CL

**AI概要:** 提出了动态融合学习模型DyFuLM，通过层次动态融合和门控特征聚合模块，在情感分析任务中实现了82.64%粗粒度和68.48%细粒度准确率，显著提升了情感表示性能。


<details>
  <summary>更多</summary>
  
**动机:** 解决复杂文本表达中情感理解的根本挑战，需要同时捕捉层次语义表示和细粒度情感细微差别。

**方法:** 提出DyFuLM多模态框架，包含两个关键模块：层次动态融合模块（自适应整合多级特征）和门控特征聚合模块（调节跨层信息流以实现平衡表示学习）。

**结果:** 在多任务情感数据集上达到82.64%粗粒度和68.48%细粒度准确率，最低回归误差（MAE=0.0674，MSE=0.0082）和最高R²=0.6903。消融实验显示各模块均对性能有显著贡献。

**结论:** DyFuLM通过有效的层次特征融合增强了情感表示和整体性能，每个模块都对特征交互和任务平衡做出了重要贡献。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DyFuLM%3A+An+Advanced+Multimodal+Framework+for+Sentiment+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01410，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01410&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.

</details>


### [121] [PromptBridge: Cross-Model Prompt Transfer for Large Language Models](https://arxiv.org/abs/2512.01420)
*Yaxuan Wang, Quan Liu, Zhenting Wang, Zichao Li, Wei Wei, Yang Liu, Yujia Bao*

**主要类别:** cs.CL

**AI概要:** 论文提出PromptBridge框架，解决大语言模型切换时的提示词漂移问题，通过训练无关的方法实现跨模型提示词迁移，减少重新优化成本。


<details>
  <summary>更多</summary>
  
**动机:** 大语言模型快速演进导致频繁模型切换，但为特定模型优化的提示词在其他模型上性能显著下降，这种现象称为模型漂移。

**方法:** 提出PromptBridge框架：使用少量对齐任务进行校准，通过MAP-RPE方法获得模型特定的最优提示词，学习跨模型提示词映射。

**结果:** 实验表明PromptBridge在单智能体和多智能体设置中都能持续提高下游任务准确性，同时减少迁移工作量。

**结论:** PromptBridge有效解决了模型漂移问题，实现了无需重新优化的跨模型提示词迁移，具有实用价值。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PromptBridge%3A+Cross-Model+Prompt+Transfer+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01420，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01420&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.

</details>


### [122] [Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech](https://arxiv.org/abs/2512.01439)
*Bharatdeep Hazarika, Arya Suneesh, Prasanna Devadiga, Pawan Kumar Rajpoot, Anshuman B Suresh, Ahmed Ifthaquar Hussain*

**主要类别:** cs.CL

**AI概要:** 印度多语言对话AI系统，支持代码混合语言如Hinglish，提升金融科技平台的可及性和用户参与度


<details>
  <summary>更多</summary>
  
**动机:** 印度语言多样性（31种主要语言，仅10%人口懂英语）对金融普惠构成障碍，需要解决语言壁垒问题

**方法:** 采用多智能体架构，包含语言分类、功能管理和多语言响应生成模块，通过多语言模型比较分析和实际部署

**结果:** 在保持低延迟开销（4-8%）的同时显著提升用户参与度

**结论:** 这项工作有助于弥合新兴市场数字金融服务中的语言鸿沟

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multilingual+Conversational+AI+for+Financial+Assistance%3A+Bridging+Language+Barriers+in+Indian+FinTech，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01439，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01439&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\%). This work contributes to bridging the language gap in digital financial services for emerging markets.

</details>


### [123] [MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification](https://arxiv.org/abs/2512.01443)
*Xabier de Zuazo, Ibon Saratxaga, Eva Navas*

**主要类别:** cs.CL

**AI概要:** 本文提出基于Conformer解码器的MEG信号处理方法，在LibriBrain 2025竞赛的两个任务（语音检测和音素分类）中取得优异表现，分别达到88.9%和65.8%的F1分数，超越基线并进入前十名。


<details>
  <summary>更多</summary>
  
**动机:** 针对LibriBrain 2025竞赛中的脑磁图(MEG)信号处理挑战，开发有效的解码器来解决语音检测和音素分类这两个基础任务。

**方法:** 使用紧凑型Conformer架构处理原始306通道MEG信号，包含轻量级卷积投影层和任务特定头部。针对语音检测任务采用MEG导向的SpecAugment数据增强，针对音素分类使用逆平方根类别加权和动态分组加载器处理100样本平均示例，并采用实例级归一化缓解分布偏移。

**结果:** 在官方标准轨道划分下，使用F1-macro进行模型选择，最佳系统在排行榜上获得语音检测88.9%和音素分类65.8%的分数，超越了竞赛基线并在两个任务中都进入了前十名。

**结论:** 所提出的Conformer-based解码器在MEG信号处理任务中表现出色，证明了该方法在处理脑磁图数据方面的有效性，相关技术文档、源代码和检查点已开源供进一步研究使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MEGConformer%3A+Conformer-Based+MEG+Decoder+for+Robust+Speech+and+Phoneme+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01443，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01443&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.

</details>


### [124] [Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages](https://arxiv.org/abs/2512.01460)
*Jozef Kubík, Marek Šuppa, Martin Takáč*

**主要类别:** cs.CL

**AI概要:** 本文提出了一种结合主动学习调度器和聚类策略的微调方法，在低资源语言上仅用有限标注数据就能提升模型性能，节省30%标注成本的同时提高F1分数最多4个点。


<details>
  <summary>更多</summary>
  
**动机:** 低资源语言数据有限导致语言模型性能较弱，而预训练计算成本高昂，因此需要在微调阶段寻求改进方法。

**方法:** 提出集成微调管道，将主动学习(AL)与数据聚类相结合，使用结构化数据选择策略（称为'主动学习调度器'）来增强有限训练数据下的微调过程。

**结果:** 在斯洛伐克语、马耳他语、冰岛语和土耳其语的实验中，该方法可节省高达30%的标注成本，同时性能提升最多4个F1分数点，并提供更好的微调稳定性。

**结论:** 在微调阶段结合聚类和主动学习调度器能有效提升低资源语言模型的性能，同时显著减少标注需求并提高训练稳定性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+BERT+Fine-Tuning+for+Sentiment+Analysis+in+Lower-Resourced+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01460，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01460&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.

</details>


### [125] [MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages](https://arxiv.org/abs/2512.01512)
*Yexing Du, Kaiyuan Liu, Youcheng Pan, Bo Yang, Keqi Deng, Xie Chen, Yang Xiang, Ming Liu, Bin Qin, YaoWei Wang*

**主要类别:** cs.CL

**AI概要:** MCAT框架通过课程学习和数据平衡策略扩展到70种语言，优化语音适配器将语音序列压缩至30个token，在FLEURS数据集上超越现有模型，推理效率提升，仅需100M可训练参数和每语言10小时数据。


<details>
  <summary>更多</summary>
  
**动机:** 当前多模态大语言模型在语音转文本翻译中存在语言覆盖范围有限和推理效率低的问题，特别是英语中心化数据集限制多语言翻译能力，长语音序列导致推理速度下降。

**方法:** 提出MCAT框架：1) 使用课程学习和数据平衡策略扩展语言覆盖至70种语言；2) 设计优化语音适配器模块，将语音序列长度压缩至30个token；在不同规模模型(9B和27B)上进行实验验证。

**结果:** 在FLEURS数据集70x69个翻译方向上超越最先进的端到端模型，同时提升批量推理效率，仅需约100M可训练参数和每语言10小时S2TT数据。

**结论:** MCAT框架有效解决了多语言语音翻译的语言覆盖和效率问题，通过开源促进MLLM在S2TT能力方面的进一步发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MCAT%3A+Scaling+Many-to-Many+Speech-to-Text+Translation+with+MLLMs+to+70+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01512，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01512&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.

</details>


### [126] [Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces](https://arxiv.org/abs/2512.01557)
*Edward Ajayi, Eudoxie Umwari, Mawuli Deku, Prosper Singadi, Jules Udahemuka, Bekalu Tadele, Chukuemeka Edeh*

**主要类别:** cs.CL

**AI概要:** 研究评估了语言检测工具在非洲语言（约鲁巴语、基尼亚卢旺达语、阿姆哈拉语）上的表现，发现专业新闻内容比对话平台数据更适合训练AI模型，但现有模型在处理代码转换文本时仍有困难。


<details>
  <summary>更多</summary>
  
**动机:** 非洲语言在数字环境中的代表性不足，在线对话数据稀缺且存在大量代码转换现象，这给语言检测工具的开发和训练带来了挑战。

**方法:** 从Reddit子论坛和本地新闻来源收集三种非洲语言的数据，比较两者的语言特征，并测试AfroLID和通用LLM模型在两类数据上的检测性能。

**结果:** 新闻数据提供了干净的单语内容，模型检测准确率接近完美；而Reddit数据代码转换严重，模型表现较差。新闻媒体的社交媒体页面还能促进用户使用本地语言互动。

**结论:** 专业策划的新闻内容是训练非洲语言AI模型的更可靠来源，未来需要开发能够同时处理干净文本和代码转换文本的模型。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Language+Diversity%3A+Evaluating+Language+Usage+and+AI+Performance+on+African+Languages+in+Digital+Spaces，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01557，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01557&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.

</details>


### [127] [MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark](https://arxiv.org/abs/2512.01603)
*Yuezhang Peng, Chonghao Cai, Ziang Liu, Shuai Fan, Sheng Jiang, Hua Xu, Yuxin Liu, Qiguang Chen, Kele Xu, Yao Li, Sheng Wang, Libo Qin, Xie Chen*

**主要类别:** cs.CL

**AI概要:** MAC-SLU是一个新的多意图车载语音理解数据集，通过引入真实复杂多意图数据提升SLU任务难度。研究对主流开源LLM和LALM进行了全面基准测试，发现虽然大模型可通过上下文学习完成SLU任务，但性能仍远逊于监督微调，而端到端LALM表现与流水线方法相当且能避免语音识别错误传播。


<details>
  <summary>更多</summary>
  
**动机:** 现有SLU数据集缺乏足够的多样性和复杂性，且缺乏针对最新大语言模型和大音频语言模型的统一基准测试标准。

**方法:** 构建MAC-SLU多意图车载语音理解数据集，采用上下文学习、监督微调(SFT)、端到端(E2E)和流水线四种方法对主流开源LLM和LALM进行综合基准测试。

**结果:** LLM和LALM通过上下文学习具备完成SLU任务的潜力，但性能仍显著落后于监督微调方法。端到端LALM表现与流水线方法相当，并能有效避免语音识别错误传播。

**结论:** MAC-SLU数据集提升了SLU任务难度，为SLU研究提供了新的基准。端到端LALM在避免错误传播方面具有优势，但大模型在SLU任务上仍需进一步优化提升性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MAC-SLU%3A+Multi-Intent+Automotive+Cabin+Spoken+Language+Understanding+Benchmark，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01603，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01603&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\footnote{https://github.com/Gatsby-web/MAC\_SLU} and datasets\footnote{huggingface.co/datasets/Gatsby1984/MAC\_SLU} are released publicly.

</details>


### [128] [Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems](https://arxiv.org/abs/2512.01661)
*Dengyun Peng, Qiguang Chen, Bofei Liu, Jiannan Guan, Libo Qin, Zheng Yan, Jinhao Liu, Jianshu Zhang, Wanxiang Che*

**主要类别:** cs.CL

**AI概要:** 该论文提出了UnsolvableQA数据集和UnsolvableRL框架，通过区分问题的客观不可解性和模型主观能力限制，解决LLM的幻觉和过度自信问题，实现了近乎完美的不可解性检测并提升可解任务的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 当前模型难以区分问题的客观不可解性（内在矛盾）和主观能力限制（超出模型能力的问题），导致幻觉和过度自信问题。

**方法:** 构建UnsolvableQA数据集（包含可解和不可解实例对），采用双轨方法：程序化生成逻辑谜题和新的"反向构建"方法在数学推理链中注入矛盾。基于此数据集开发UnsolvableRL强化学习框架，包含准确性、不可解性和难度三个奖励组件。

**结果:** 实验结果显示该方法实现了近乎完美的不可解性检测，同时提高了可解任务的准确性。发现了"能力崩溃"现象，证明明确接触不可解数据对防止模型系统性过度自信至关重要。

**结论:** 通过区分客观不可解性和主观能力限制，并明确训练模型识别不可解问题，可以有效提高LLM的可靠性，防止幻觉和过度自信问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+the+Boundary+of+Solvability%3A+Aligning+LLMs+to+Detect+Unsolvable+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01661，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01661&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.

</details>


### [129] [MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications](https://arxiv.org/abs/2512.01710)
*Stefano Zeppieri*

**主要类别:** cs.CL

**AI概要:** 论文提出混合记忆增强生成(MMAG)模式，通过五层记忆结构解决LLM在长对话中缺乏连续性、个性化和相关性的问题，在Heero对话代理中验证了效果。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在单次提示中能生成连贯文本，但在长对话中缺乏连续性、个性化和情境适应性，而人类沟通依赖多种记忆形式。

**方法:** 引入MMAG框架，将记忆组织为五层：对话记忆、长期用户记忆、情景事件记忆、感官情境记忆和短期工作记忆，借鉴认知心理学原理设计协调、优先级和冲突解决策略。

**结果:** 在Heero对话代理中实施，加密的长期个人资料和对话历史已提升用户参与度和留存率。

**结论:** MMAG为构建记忆丰富的语言代理提供了基础，使其更连贯、主动且符合人类需求，同时讨论了存储、检索、隐私和延迟等实施问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MMAG%3A+Mixed+Memory-Augmented+Generation+for+Large+Language+Models+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01710，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01710&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.

</details>


### [130] [Self-Supervised Borrowing Detection on Multilingual Wordlists](https://arxiv.org/abs/2512.01713)
*Tim Wientzek*

**主要类别:** cs.CL

**AI概要:** 提出了一种完全自监督的多语言词表借词检测方法，结合PMI相似度和轻量级对比学习，无需标注数据即可自动选择决策阈值，性能优于现有字符串相似度方法，与监督基线相当或更好。


<details>
  <summary>更多</summary>
  
**动机:** 传统借词检测方法依赖标注数据或简单的字符串相似度，需要开发无需人工监督且更准确的自监督方法。

**方法:** 结合两种信息源：基于全局对应模型的PMI相似度和基于语音特征向量的轻量级对比学习组件，包含自动选择决策阈值的程序。

**结果:** 在基准数据集上，PMI单独使用已优于NED和SCA等字符串相似度方法，组合方法性能与监督基线相当或更好，可扩展到不同规模数据集。

**结论:** 该方法提供了一种有效且可扩展的自监督借词检测解决方案，配备了命令行工具供研究者使用，强调了字符编码、温度设置和数据增强策略的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Supervised+Borrowing+Detection+on+Multilingual+Wordlists，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01713，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01713&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.

</details>


### [131] [Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks](https://arxiv.org/abs/2512.01725)
*Jiannan Guan, Qiguang Chen, Libo Qin, Dengyun Peng, Jinhao Liu, Liangyu Huo, Jian Xie, Wanxiang Che*

**主要类别:** cs.CL

**AI概要:** 该论文研究LLM在多解问题中的过度自信现象，提出MuSoBench基准测试，发现长链思维(Long-CoT)能通过迭代探索缓解此问题，并提出认知刚性假说来解释原因。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在需要单一正确答案的推理任务中表现出色，但在需要生成全面且多样化答案的多解任务中表现不佳，存在推理过度自信问题。

**方法:** 引入MuSoBench多解问题基准，比较短链思维(Short-CoT)和长链思维(Long-CoT)提示方法的表现，进行注意力熵分析验证认知刚性假说。

**结果:** 实验显示Short-CoT表现出明显的过度自信，而Long-CoT通过迭代探索和自我反思缓解了此问题。注意力熵分析为认知刚性假说提供了初步支持。

**结论:** 研究结果为评估LLM推理完整性提供了工具，强调需要超越单一答案准确性评估，转向全面探索的评估方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beware+of+Reasoning+Overconfidence%3A+Pitfalls+in+the+Reasoning+Process+for+Multi-solution+Tasks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01725，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01725&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.

</details>


### [132] [Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference](https://arxiv.org/abs/2512.01728)
*Zhengjia Wang, Danding Wang, Qiang Sheng, Jiaying Wu, Juan Cao*

**主要类别:** cs.CL

**AI概要:** OmiGraph是首个针对信息缺失型虚假信息的检测框架，通过构建缺失感知图、建模缺失关系和动态意图，实现了对误导性内容遗漏的检测，在基准测试中表现显著优于现有方法


<details>
  <summary>更多</summary>
  
**动机:** 现有研究主要关注显性虚假信息，而基于信息缺失的欺骗方式（通过故意遗漏重要信息来误导读者）被忽视，这种欺骗更具隐蔽性但危害巨大

**方法:** 提出OmiGraph框架：1）利用上下文环境构建缺失感知图来识别可能被遗漏的内容；2）进行缺失导向的关系建模，识别内部上下文依赖和动态缺失意图；3）通过缺失感知的消息传递和聚合来提取缺失模式进行检测

**结果:** 在两个大规模基准测试中取得了显著性能提升，平均F1分数提升5.4%，准确率提升5.3%

**结论:** 考虑信息缺失视角对于虚假信息检测至关重要，OmiGraph框架有效解决了这一被忽视但重要的研究方向，为检测隐蔽性欺骗提供了新思路

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning+About+the+Unsaid%3A+Misinformation+Detection+with+Omission-Aware+Graph+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01728，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01728&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.

</details>


### [133] [InnoGym: Benchmarking the Innovation Potential of AI Agents](https://arxiv.org/abs/2512.01822)
*Jintian Zhang, Kewei Xu, Jingsheng Zheng, Zhuoyun Yu, Yuqi Zhu, Yujie Luo, Lanning Wei, Shuofei Qiao, Lun Du, Da Zheng, Shumin Deng, Huajun Chen, Ningyu Zhang*

**主要类别:** cs.CL

**AI概要:** InnoGym是首个评估AI智能体创新潜力的基准框架，通过性能增益和新颖性两个指标来衡量解决方案的原创性和改进程度，包含18个真实工程和科学任务。


<details>
  <summary>更多</summary>
  
**动机:** 现有基准主要关注答案正确性而忽略方法多样性，但真正的创新不仅需要正确答案还需要方法的原创性。

**方法:** 引入InnoGym基准框架，包含性能增益和新颖性两个互补指标，通过资源过滤、评估验证和解决方案收集标准化18个真实任务，并提供iGym统一执行环境。

**结果:** 实验显示某些智能体能产生新颖方法但缺乏鲁棒性限制了性能增益，揭示了创造性和有效性之间的关键差距。

**结论:** 需要同时评估创造性和有效性的基准，InnoGym填补了现有基准在创新潜力评估方面的空白。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是InnoGym%3A+Benchmarking+the+Innovation+Potential+of+AI+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01822，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01822&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.

</details>


### [134] [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/abs/2512.01848)
*Jinghan Jia, Nathalie Baracaldo, Sijia Liu*

**主要类别:** cs.CL

**AI概要:** 论文研究如何通过强化学习(RL)而非监督微调(SFT)来提升大型推理模型的安全对齐能力，在保持推理能力的同时更有效地抑制中间推理过程中的不安全行为


<details>
  <summary>更多</summary>
  
**动机:** 现有基于监督微调的安全对齐方法在大型推理模型中存在安全改进不一致、推理能力下降和泛化能力差的问题，无法有效处理中间推理轨迹中出现的安全风险

**方法:** 采用强化学习作为补充优化框架，通过奖励反馈直接优化模型策略，实现更自适应和稳定的安全对齐

**结果:** 在多模型家族和基准测试中，RL实现了更强且更一致的安全增益，同时保持了推理能力，通过抑制不安全探索性推理同时保留反思深度

**结论:** 强化学习为大型推理模型的安全对齐提供了更有效的解决方案，相比纯监督方法能产生更安全可靠的推理过程

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+SFT%3A+Reinforcement+Learning+for+Safer+Large+Reasoning+Models+with+Better+Reasoning+Ability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01848&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.

</details>


### [135] [BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages](https://arxiv.org/abs/2512.01852)
*Hrishikesh Terdalkar, Kirtan Bhojani, Aryan Dongare, Omm Aditya Behera*

**主要类别:** cs.CL

**AI概要:** BHRAM-IL是一个针对多种印度语言（印地语、古吉拉特语、马拉地语、奥里亚语和英语）的幻觉检测基准，包含36,047个问题，评估了14个多语言大模型的幻觉问题，为多语言幻觉检测研究提供了重要资源。


<details>
  <summary>更多</summary>
  
**动机:** 当前幻觉检测研究主要集中在英语，而资源匮乏的印度语言领域缺乏系统性的幻觉检测基准，需要建立专门的多语言幻觉评估工具。

**方法:** 构建包含36,047个问题的基准数据集，涵盖9个类别（事实性、数值、推理和语言任务），使用特定类别指标在(0,1)范围内评估14个多语言大模型。

**结果:** 所有类别和模型的聚合主要得分为0.23，语言校正模糊得分为0.385，显示了BHRAM-IL在幻觉检测评估中的有效性。

**结论:** BHRAM-IL基准填补了印度语言幻觉检测研究的空白，为多语言幻觉检测和缓解的未来研究提供了重要数据集和评估工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BHRAM-IL%3A+A+Benchmark+for+Hallucination+Recognition+and+Assessment+in+Multiple+Indian+Languages，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01852&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.

</details>


### [136] [Cross-Lingual Interleaving for Speech Language Models](https://arxiv.org/abs/2512.01865)
*Adel Moumen, Guangzhi Sun, Philip C. Woodland*

**主要类别:** cs.CL

**AI概要:** 该论文提出了一种跨语言交织方法，通过在无文本监督的情况下混合不同语言的语音标记来构建多语言口语模型，并发布了EN-FR训练数据集和评估基准。


<details>
  <summary>更多</summary>
  
**动机:** 当前口语模型研究主要集中于英语，缺乏多语言评估基准和训练数据，限制了跨语言学习的发展。

**方法:** 使用跨语言交织方法混合不同语言的语音标记，无需文本监督；构建了EN-FR训练数据集TinyStories和两个跨语言语义评估基准。

**结果:** 在360M和1B参数模型上，交织方法提高了单语言语义准确性，实现了稳健的跨语言延续，并增强了跨语言隐藏状态对齐。

**结论:** 跨语言交织是构建理解和跨语言对话的多语言口语模型的简单可扩展途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Cross-Lingual+Interleaving+for+Speech+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01865，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01865&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.

</details>


### [137] [Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models](https://arxiv.org/abs/2512.01892)
*Heloisa Candello, Muneeza Azmat, Uma Sushmitha Gunturi, Raya Horesh, Rogerio Abreu de Paula, Heloisa Pimentel, Marcelo Carpinette Grave, Aminat Adebiyi, Tiago Machado, Maysa Malfiza Garcia de Macedo*

**主要类别:** cs.CL

**AI概要:** 该研究通过混合方法实验评估生成式AI缓解策略的人类感知，发现语言背景、AI经验等因素显著影响评估结果，参与者对语言和语境特征高度敏感，与LLM定量评估形成对比。


<details>
  <summary>更多</summary>
  
**动机:** 随着生成式AI的快速发展，研究人类对生成内容的感知变得至关重要，特别是AI容易产生幻觉和有害内容的问题，而人类对这些缓解策略的感知尚不明确。

**方法:** 采用混合方法实验设计，57名参与者在被试内设计中评估两种条件下的响应：有害响应加缓解策略和仅缓解响应，从忠实度、公平性、伤害移除能力和相关性四个维度进行评估。

**结果:** 结果显示参与者的母语、AI工作经验和标注熟悉度显著影响评估结果，参与者对语言和语境特征高度敏感，会惩罚轻微语法错误但奖励保留的语义语境。

**结论:** 研究提出了新的训练和评估缓解策略的指标，并为人类-AI评估研究提供了重要见解，强调了人类感知与定量评估之间的差异。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Human+Perceptions+of+AI+Responses%3A+Insights+from+a+Mixed-Methods+Study+on+Risk+Mitigation+in+Generative+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01892&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.

</details>


### [138] [OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation](https://arxiv.org/abs/2512.01896)
*Jinzheng Yu, Yang Xu, Haozhen Li, Junqi Li, Yifan Feng, Ligu Zhu, Hao Shen, Lei Shi*

**主要类别:** cs.CL

**AI概要:** 本文提出了自动化网络舆情报告生成任务(OPOR-GEN)，构建了包含463个危机事件的数据集OPOR-BENCH，并开发了基于智能体的评估框架OPOR-EVAL，为这一重要领域的研究奠定了基础。


<details>
  <summary>更多</summary>
  
**动机:** 虽然大语言模型使自动生成舆情报告在技术上可行，但该领域缺乏系统研究，特别是正式的任务定义和基准测试，需要填补这一空白。

**方法:** 定义了OPOR-GEN任务，构建了以事件为中心的OPOR-BENCH数据集（包含新闻文章、社交媒体帖子和参考摘要），提出了基于智能体的OPOR-EVAL评估框架来模拟专家评估。

**结果:** 实验表明，OPOR-EVAL框架与人类专家判断具有高度相关性，前沿模型在该任务上表现出良好的性能。

**结论:** 通过提供全面的任务定义、基准数据集和评估框架，为网络舆情报告生成这一关键领域的未来研究奠定了坚实基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OPOR-Bench%3A+Evaluating+Large+Language+Models+on+Online+Public+Opinion+Report+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01896，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01896&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.

</details>


### [139] [Latent Debate: A Surrogate Framework for Interpreting LLM Thinking](https://arxiv.org/abs/2512.01909)
*Lihu Chen, Xiang Yin, Francesca Toni*

**主要类别:** cs.CL

**AI概要:** 提出了一种名为'潜在辩论'的新框架，通过分析单一模型内部推理过程中隐含的支持和攻击信号来解释LLM的预测机制和幻觉成因，可作为幻觉检测的基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 理解大语言模型的内部思维过程和幻觉成因是关键挑战，现有方法依赖多个答案或模型间的显式辩论，无法捕捉单一模型内部的隐式辩论信号。

**方法:** 提出了模型和任务无关的概念框架，并通过符号化实例化来近似LLM在真假预测任务中的思维过程，捕获单一模型单次推理中的隐藏支持和攻击信号。

**结果:** 实证研究表明潜在辩论是一个忠实结构化代理模型，与原始LLM预测高度一致，同时为幻觉检测提供了强基线，发现中间层高度潜在辩论与幻觉风险密切相关。

**结论:** 潜在辩论是理解LLM内部机制的有潜力框架，特别适用于推理步骤中出现内部（不）一致性的场景，揭示了幻觉与辩论模式之间的强相关性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent+Debate%3A+A+Surrogate+Framework+for+Interpreting+LLM+Thinking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01909&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.

</details>


### [140] [Rectifying LLM Thought from Lens of Optimization](https://arxiv.org/abs/2512.01925)
*Junnan Liu, Hongwei Liu, Songyang Zhang, Kai Chen*

**主要类别:** cs.CL

**AI概要:** RePro通过将思维链推理视为梯度下降过程，引入过程级奖励机制来优化大语言模型的推理性能，有效减少过思考和冗长推理链问题。


<details>
  <summary>更多</summary>
  
**动机:** 现有大语言模型的长思维链推理存在过思考和推理链过长等次优行为，影响性能表现。

**方法:** 将CoT推理框架化为梯度下降过程，定义代理目标函数评估优化过程，使用强度和稳定性双评分机制构建过程级奖励，并集成到RLVR管道中优化LLM。

**结果:** 在数学、科学和编程等多个基准测试中，RePro持续提升了推理性能并减少了次优推理行为。

**结论:** RePro方法通过过程级奖励机制有效优化了大语言模型的推理过程，为解决长思维链推理中的次优行为提供了新思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rectifying+LLM+Thought+from+Lens+of+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01925，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01925&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.

</details>


### [141] [How Far Are We from Genuinely Useful Deep Research Agents?](https://arxiv.org/abs/2512.01948)
*Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou*

**主要类别:** cs.CL

**AI概要:** 该论文提出了FINDER基准测试和DEFT失败分类法，用于评估深度研究代理在生成综合分析报告时的性能，发现当前系统主要问题在于证据整合和验证而非任务理解


<details>
  <summary>更多</summary>
  
**动机:** 现有深度研究代理主要在问答基准上验证，缺乏对综合报告生成的评估，且当前基准存在任务复杂性和主观指标问题，无法反映用户需求

**方法:** 构建FINDER基准（100个人工策划的研究任务和419个结构化检查项），基于1000份主流DRA生成的报告，通过人-LLM协同标注和标注者间可靠性验证建立DEFT失败分类法

**结果:** 当前DRAs主要问题不在于任务理解，而在于证据整合、验证和推理弹性规划方面存在困难

**结论:** FINDER基准和DEFT分类法为深度研究代理的评估提供了更全面的框架，揭示了系统在证据处理方面的核心挑战

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是How+Far+Are+We+from+Genuinely+Useful+Deep+Research+Agents%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.01948，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.01948&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.

</details>


### [142] [The Art of Scaling Test-Time Compute for Large Language Models](https://arxiv.org/abs/2512.02008)
*Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty*

**主要类别:** cs.CL

**AI概要:** 本文首次对测试时缩放(TTS)策略进行了大规模系统研究，比较了8个开源大语言模型在4个推理数据集上的表现，发现没有单一策略最优，但提供了基于问题难度、模型类型和计算预算选择最佳TTS策略的实用指南。


<details>
  <summary>更多</summary>
  
**动机:** 现有研究缺乏在相同条件下对知名TTS策略的系统比较，且模型类型和问题难度对性能的影响尚不明确，需要填补这些研究空白。

**方法:** 使用8个开源LLM（7B到235B参数），在4个推理数据集上生成超过300亿个token，对不同TTS策略进行大规模对比研究。

**结果:** 发现三个一致趋势：1) 没有单一TTS策略普遍最优；2) 推理模型在不同难度和轨迹长度下表现出不同的质量模式；3) 给定模型类型下，最优TTS性能随计算预算单调扩展。

**结论:** 基于研究结果，提供了考虑问题难度、模型类型和计算预算选择最佳TTS策略的实用方案，为有效的推理时缩放提供了实践指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Art+of+Scaling+Test-Time+Compute+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02008&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.

</details>


### [143] [Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling](https://arxiv.org/abs/2512.02010)
*Jack Cook, Junxian Guo, Guangxuan Xiao, Yujun Lin, Song Han*

**主要类别:** cs.CL

**AI概要:** 4/6是一种改进的NVFP4量化算法，通过为每个数值块评估两个缩放因子来减少量化误差，特别是在接近最大值区域，从而提升大语言模型训练稳定性和推理性能。


<details>
  <summary>更多</summary>
  
**动机:** 随着大语言模型规模增大，低精度数值格式如NVFP4因速度和内存优势而流行，但现有NVFP4量化方法在训练中容易发散且在推理时性能下降，主要问题在于对接近最大值区域的量化误差较大。

**方法:** 提出Four Over Six (4/6)算法，修改NVFP4量化过程，为每个数值块评估两个潜在缩放因子，通过缩放到更小的FP4值使可表示数值分布更均匀，特别改善接近最大值区域的表示精度。

**结果:** 在transformer和混合模型架构的预训练实验中，4/6在多个案例中防止了训练发散，使训练损失显著接近BF16精度。同时可轻松集成到多种后训练量化方法中，普遍提升下游任务准确率。

**结论:** 4/6算法有效解决了NVFP4量化中的关键问题，可在NVIDIA Blackwell GPU上高效实现，为使用NVFP4训练和部署大语言模型提供了可行方案，有望推动该领域的未来发展。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Four+Over+Six%3A+More+Accurate+NVFP4+Quantization+with+Adaptive+Block+Scaling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2512.02010，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02010&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false)

**原文摘要:** As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.

</details>
