{"id": "2601.11559", "pdf": "https://arxiv.org/pdf/2601.11559", "abs": "https://arxiv.org/abs/2601.11559", "authors": ["Zilal Eiz AlDin", "John Wu", "Jeffrey Paul Fung", "Jennifer King", "Mya Watts", "Lauren ONeill", "Adam Richard Cross", "Jimeng Sun"], "title": "MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "5 pages", "summary": "Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.", "AI": {"tldr": "该论文提出了MIMIC-RD基准测试，通过将临床文本实体直接映射到Orphanet罕见病数据库来评估LLM在罕见病鉴别诊断中的表现，发现当前最先进的大语言模型在罕见病诊断方面表现不佳。", "motivation": "现有评估LLM罕见病诊断的方法存在两个关键局限：依赖理想化的临床案例研究，无法捕捉真实临床复杂性；或使用ICD编码作为疾病标签，这会显著低估罕见病数量，因为许多罕见病无法直接映射到Orphanet等综合罕见病数据库。", "method": "开发MIMIC-RD基准测试，通过LLM挖掘临床文本实体并直接映射到Orphanet数据库，然后由四位医学注释者验证确认识别的实体是否为真正的罕见病，最终基于145名患者的数据集评估各种模型。", "result": "当前最先进的大语言模型在罕见病鉴别诊断方面表现不佳，突显出现有能力与临床需求之间的巨大差距。", "conclusion": "研究结果指出了改善罕见病鉴别诊断的几个未来发展方向，强调需要更有效的评估方法和模型改进来应对罕见病诊断的挑战。"}}
{"id": "2601.11620", "pdf": "https://arxiv.org/pdf/2601.11620", "abs": "https://arxiv.org/abs/2601.11620", "authors": ["Michael Timothy Bennett"], "title": "A Mind Cannot Be Smeared Across Time", "categories": ["cs.AI"], "comment": null, "summary": "Whether machines can be conscious depends not only on what they compute, but \\emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.", "AI": {"tldr": "论文通过形式化分析证明，机器意识不仅取决于计算内容，更取决于计算时机。意识体验的统一性和同时性与传统顺序计算存在本质差异，强同步要求需要硬件并发能力支持。", "motivation": "探讨机器意识的可能性，指出当前大多数人工智能系统采用顺序或时分复用更新方式，而人类意识体验呈现统一和同时性特征，这种时间特性差异对机器意识实现至关重要。", "method": "扩展堆栈理论，引入代数定律将时间窗口约束满足与逻辑连接关联；定义精确的时间语义τ^Δ,s，证明存在性时间实现◇Δ不保持连接性；区分强同步和弱同步假设；形式化并发容量概念。", "result": "证明系统可以在时间上实现体验的所有成分，但从未实例化体验连接本身；强同步要求客观共实例化，需要硬件并发能力；神经生理证据支持强同步假设。", "conclusion": "在严格顺序硬件上实现需要两个或更多同时贡献者的意识内容是不可能的，意识归因需要架构检查而不仅仅是功能性能评估，硬件架构对机器意识实现至关重要。"}}
{"id": "2601.11622", "pdf": "https://arxiv.org/pdf/2601.11622", "abs": "https://arxiv.org/abs/2601.11622", "authors": ["Hassan Ugail", "Newton Howard"], "title": "Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.", "AI": {"tldr": "该研究将神经科学中的时间整合和亚稳态概念应用于Transformer模型，提出了一个基于激活时间序列的复合动力学指标，用于分析LLM在不同功能机制下的计算组织差异。", "motivation": "大型语言模型的高维内部动态时间组织尚未被充分理解，现有可解释性方法主要关注静态表示或因果干预，忽略了时间结构。研究者希望借鉴神经科学中的时间动力学概念来填补这一空白。", "method": "从GPT-2-medium的自回归生成过程中提取激活时间序列，计算复合动力学指标，并在五种条件下进行评估：结构化推理、强制重复、高温噪声采样、注意力头剪枝和权重噪声注入。", "result": "结构化推理相比重复、噪声和扰动机制显示出显著升高的指标值，单因素方差分析确认了统计显著性差异，关键比较中显示出大效应量。结果对层选择、通道子采样和随机种子具有鲁棒性。", "conclusion": "神经科学启发的动力学指标能够可靠地表征大型语言模型在不同功能机制下的计算组织差异，该指标捕获的是形式动力学特性而非主观体验。"}}
{"id": "2601.11625", "pdf": "https://arxiv.org/pdf/2601.11625", "abs": "https://arxiv.org/abs/2601.11625", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance", "categories": ["cs.AI", "cs.LG"], "comment": "8 pages, Submitted to ACL Rolling Review and is under review", "summary": "Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.", "AI": {"tldr": "该论文提出了一种训练时可解释性方法，通过跟踪微调过程中token级别的归因变化来监控模型决策证据的演变，定义了\"解释漂移\"概念和\"推理稳定点\"来识别模型证据稳定的训练阶段。", "motivation": "微调预训练语言模型会改变模型依赖的证据基础，需要一种方法来监控这种变化并识别模型决策证据稳定的训练阶段。", "method": "提出训练时解释漂移分析方法，跟踪token归因在微调epoch间的变化，定义解释漂移和推理稳定点(RSP)，通过分析归因动态来监控模型证据演变。", "result": "在多个轻量级transformer分类器和基准任务中，解释漂移在训练早期就进入低稳定状态，而验证精度仅发生微小变化；在有标签相关触发token的受控设置中，归因动态揭示了模型对捷径的依赖增加。", "conclusion": "解释漂移提供了一种简单、低成本的诊断工具，可用于监控微调过程中决策证据的演变，并在稳定证据机制中选择检查点。"}}
{"id": "2601.11564", "pdf": "https://arxiv.org/pdf/2601.11564", "abs": "https://arxiv.org/abs/2601.11564", "authors": ["Ahilan Ayyachamy Nadar Ponnusamy", "Karthic Chandran", "M Maruf Hossain"], "title": "Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 6 figures", "summary": "The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.", "AI": {"tldr": "论文研究大语言模型在处理大量无关上下文时的性能与质量权衡，发现KV缓存增长导致非线性性能下降，并揭示MoE架构在不同上下文规模下的异常行为。", "motivation": "随着LLM上下文窗口的扩大，计算开销急剧增加，需要研究在大量无关干扰上下文下模型性能与质量的权衡关系。", "method": "使用密集transformer架构（Llama-3.1-70B和Qwen1.5-14B）进行实验，分析KV缓存增长的影响，并深入分析MoE架构在不同上下文规模下的行为。", "result": "发现性能退化与KV缓存增长呈非线性关系，MoE架构在高低上下文规模下表现出异常行为，高token量时架构优势可能被基础设施瓶颈掩盖。", "conclusion": "扩展上下文窗口需要平衡计算开销与模型质量，KV缓存管理和基础设施优化对维持大语言模型性能至关重要，MoE架构的优势在高负载下可能无法充分发挥。"}}
{"id": "2601.11747", "pdf": "https://arxiv.org/pdf/2601.11747", "abs": "https://arxiv.org/abs/2601.11747", "authors": ["Huaxiaoyue Wang", "Sunav Choudhary", "Franck Dernoncourt", "Yu Shen", "Stefano Petrangeli"], "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement", "categories": ["cs.AI"], "comment": null, "summary": "Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.", "AI": {"tldr": "PRISM方法通过构建设计知识库，利用真实设计数据学习风格知识，根据文本指令实现图形设计的风格化改进，在风格对齐和用户偏好方面优于基线方法", "motivation": "解决非专业人士在图形设计中探索不同风格方向耗时的问题，现有视觉语言模型在风格知识方面过于通用，与特定领域数据不匹配", "method": "提出PRISM方法，分三个阶段：1)聚类高方差设计捕捉风格多样性；2)将每个聚类总结为可操作的设计知识；3)在推理时检索相关知识实现风格感知改进", "result": "在Crello数据集上获得1.49的平均排名（越接近1越好），用户研究显示设计师一致偏好PRISM的结果", "conclusion": "利用设计数据构建知识库能有效学习设计原则，PRISM方法在基于文本指令的风格改进任务中表现优异"}}
{"id": "2601.11565", "pdf": "https://arxiv.org/pdf/2601.11565", "abs": "https://arxiv.org/abs/2601.11565", "authors": ["Pakorn Ueareeworakul", "Shuman Liu", "Jinghao Feng", "Ling Hu", "Zhantang Shi", "Chengqi Sun", "Liang Yao", "Panyi Ouyang", "Haibo Zhang", "Anxiang Zeng"], "title": "Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.", "AI": {"tldr": "Compass-Embedding v4是一个针对东南亚电商场景优化的多语言嵌入框架，通过类感知掩码、多样化语料构建和推理优化技术，在低资源语言上实现了最先进性能", "motivation": "全球电商向新兴市场扩张时，低资源语言缺乏高质量语义表示成为检索、推荐和搜索系统的关键瓶颈，东南亚电商场景面临数据稀缺、噪声监督和生产约束的挑战", "method": "提出类感知掩码(CAM)抑制无效负样本，通过上下文合成数据生成、跨语言翻译和结构化电商数据构建多样化训练语料，结合大批次训练和球面模型合并优化推理效率", "result": "在多语言基准测试和专有电商任务上实现最先进性能，在主要东南亚语言上显著优于通用嵌入模型，同时在高资源语言上保持竞争力", "conclusion": "该框架成功解决了东南亚电商场景中的三个核心挑战，为低资源语言的语义表示学习提供了有效解决方案，具有重要的实际应用价值"}}
{"id": "2601.11781", "pdf": "https://arxiv.org/pdf/2601.11781", "abs": "https://arxiv.org/abs/2601.11781", "authors": ["Dawood Wasif", "Terrence J. Moore", "Seunghyun Yoon", "Hyuk Lim", "Dan Dongseong Kim", "Frederica F. Nelson", "Jin-Hee Cho"], "title": "Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles", "categories": ["cs.AI", "cs.CV"], "comment": "Submitted to ICRA 2026 (under review)", "summary": "Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.", "AI": {"tldr": "RAIL是一个风险感知的人机协同框架，通过融合多种运行时信号生成入侵风险评分，实现自适应控制和人机协作，在自动驾驶中显著提升安全性和性能表现。", "motivation": "自动驾驶车辆在遇到罕见的长尾场景或网络物理入侵时，必须保持安全和有效性，需要一种能够实时感知风险并自适应调整的框架。", "method": "RAIL融合三种信号（曲率执行完整性、碰撞时间接近度和观测偏移一致性）通过加权Noisy-OR生成入侵风险评分(IRS)。当风险超过阈值时，通过学习的权限将动作与特定防护机制混合，同时保留人工干预选项；低风险时执行正常策略。使用情境多臂老虎机基于信号向量选择防护机制，并采用Soft Actor-Critic结合风险优先回放和双奖励机制进行学习。", "result": "在MetaDrive上获得测试回报360.65、成功率0.85、安全违规0.75、干扰率0.0027，仅记录29.07次训练安全违规；在CAN注入和LiDAR欺骗攻击下，成功率提升至0.68和0.80，攻击下脱管率降至0.37和0.03，攻击成功率降至0.34和0.11；在CARLA上仅用8000步就获得1609.70测试回报和0.41成功率。", "conclusion": "RAIL框架通过风险感知和人机协同机制，在多种攻击场景和环境中都表现出优越的安全性和性能，显著优于现有强化学习、安全强化学习和人机协同基线方法。"}}
{"id": "2601.11567", "pdf": "https://arxiv.org/pdf/2601.11567", "abs": "https://arxiv.org/abs/2601.11567", "authors": ["Vanessa D'Amario", "Randy Daniel", "Alessandro Zanetti", "Dhruv Edamadaka", "Nitya Alaparthy", "Joshua Tarkoff"], "title": "Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 11 figures, accepted at 47 workshop Reproducible Artificial Intelligence (AAAI 2026, Singapore, January 27, 2026)", "summary": "Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.", "AI": {"tldr": "该研究评估了6个小型开源医疗大语言模型在儿科内分泌学领域的表现，发现提示词微小变化会导致输出显著差异，模型一致性不等于正确性，且存在自我评估偏差和CUDA版本等系统因素影响输出的问题。", "motivation": "当前小型开源医疗大语言模型的评估主要局限于多选题准确性，缺乏对一致性、鲁棒性和推理行为的全面评估，需要更全面的诊断框架来评估其在真实临床决策支持场景中的潜在问题。", "method": "使用多选题结合人工评估和临床审查，在确定性设置中测试提示词变化对模型输出的影响和自我评估偏差，在随机设置中评估输出变异性并研究一致性与正确性的关系。", "result": "HuatuoGPT-o1-8B表现最佳且一致性最高，但高一致性不代表正确性；模型存在自我评估偏差和对解释顺序的依赖；专家审查发现错误推理中混杂临床可接受回答和临床疏忽；CUDA版本差异会导致统计显著的输出变化。", "conclusion": "研究表明语义上微不足道的提示词扰动会导致输出分歧，对基于LLM评估的可重现性提出质疑，强调需要更广泛的诊断框架来理解真实临床决策支持场景中的潜在陷阱。"}}
{"id": "2601.11792", "pdf": "https://arxiv.org/pdf/2601.11792", "abs": "https://arxiv.org/abs/2601.11792", "authors": ["Yifei Sun", "Yongan Li", "A. K. Qin", "Sicheng Hou", "Tamas Pflanzner"], "title": "A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.", "AI": {"tldr": "本文提出了创新数学问题生成(IMPG)任务，并开发了一个自演进的多角色协作框架，通过细粒度难度指导来生成既正确又创新的数学问题。", "motivation": "现有大语言模型在数学问题生成中虽然正确率高，但缺乏创新性和区分度，需要新的方法来提升问题生成的创新质量。", "method": "构建多角色协作机制（采样器、生成器、评估器、状态机、内存），引入改进的难度模型和DAPS算法，采用多阶段训练流程（CPT、SFT、GRPO），并通过蒸馏实现系统自演进。", "result": "实验表明，相比基线模型，该方法在保持高正确率的同时显著提升了生成问题的创新性。", "conclusion": "提出的自演进多角色协作框架有效解决了创新数学问题生成任务，为智能教育领域的数学问题生成提供了新的技术路径。"}}
{"id": "2601.11573", "pdf": "https://arxiv.org/pdf/2601.11573", "abs": "https://arxiv.org/abs/2601.11573", "authors": ["Muhammad Muneeb", "David B. Ascher"], "title": "An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer (QA) generation (via Google Gemini), natural language inference (NLI) for quality control, semantic deduplication, clustering-based data splitting, and parameter-efficient fine-tuning using LoRA. We fine-tuned three LLMs (LLaMA-3.2-3B, Qwen2.5-7B, Gemma) and benchmarked them on over 14 lexical and semantic metrics. Qwen2.5-7B emerged as the best performer, with BLEU-4 and ROUGE-1 improvements of 82\\% and 70\\% for PRSGPT and 6\\% and 18\\% for BioStarsGPT, respectively. The open-source datasets produced include over 28,000 QA pairs for PRSGPT and 154,282 for BioStarsGPT. Human evaluation of PRSGPT yielded 61.9\\% accuracy on the PRS tools comparison task, comparable to Google Gemini (61.4\\%), but with richer methodological detail and accurate citations. BioStarsGPT demonstrated 59\\% conceptual accuracy across 142 curated bioinformatics questions. Our pipeline enables scalable, domain-specific fine-tuning of LLMs. It enables privacy-preserving, locally deployable bioinformatics assistants, explores their practical applications, and addresses the challenges, limitations, and mitigation strategies associated with their development and use.", "AI": {"tldr": "提出了一个九步可复现的LLM微调流程，专门用于生物信息学领域，开发了PRSGPT和BioStarsGPT两个应用，在多项指标上表现优异，特别是Qwen2.5-7B模型表现最佳。", "motivation": "大型语言模型在复杂生物信息学应用中缺乏专业知识，需要开发专门针对该领域的微调方法。", "method": "九步流程：整合多样数据源、结构化预处理、基于提示的问答生成、自然语言推理质量控制、语义去重、聚类数据分割、使用LoRA进行参数高效微调。微调了LLaMA-3.2-3B、Qwen2.5-7B和Gemma三个模型。", "result": "Qwen2.5-7B表现最佳，PRSGPT的BLEU-4和ROUGE-1分别提升82%和70%，BioStarsGPT提升6%和18%。PRSGPT在PRS工具比较任务上达到61.9%准确率，与Google Gemini相当但提供更丰富的方法细节和准确引用。BioStarsGPT在142个问题上的概念准确率为59%。", "conclusion": "该流程支持可扩展的领域特定LLM微调，实现了隐私保护、本地可部署的生物信息学助手，并探讨了实际应用中的挑战、限制和缓解策略。"}}
{"id": "2601.11809", "pdf": "https://arxiv.org/pdf/2601.11809", "abs": "https://arxiv.org/abs/2601.11809", "authors": ["Zeyu Mu", "Shangtong Zhang", "B. Brian Park"], "title": "Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic", "categories": ["cs.AI"], "comment": "Under review at IEEE Transactions on Intelligent Transportation Systems", "summary": "Connected automated vehicles (CAVs) possess the ability to communicate and coordinate with one another, enabling cooperative platooning that enhances both energy efficiency and traffic flow. However, during the initial stage of CAV deployment, the sparse distribution of CAVs among human-driven vehicles reduces the likelihood of forming effective cooperative platoons. To address this challenge, this study proposes a hybrid multi-agent lane change decision model aimed at increasing CAV participation in cooperative platooning and maximizing its associated benefits. The proposed model employs the QMIX framework, integrating traffic data processed through a convolutional neural network (CNN-QMIX). This architecture addresses a critical issue in dynamic traffic scenarios by enabling CAVs to make optimal decisions irrespective of the varying number of CAVs present in mixed traffic. Additionally, a trajectory planner and a model predictive controller are designed to ensure smooth and safe lane-change execution. The proposed model is trained and evaluated within a microsimulation environment under varying CAV market penetration rates. The results demonstrate that the proposed model efficiently manages fluctuating traffic agent numbers, significantly outperforming the baseline rule-based models. Notably, it enhances cooperative platooning rates up to 26.2\\%, showcasing its potential to optimize CAV cooperation and traffic dynamics during the early stage of deployment.", "AI": {"tldr": "提出基于CNN-QMIX的混合多智能体换道决策模型，提高CAV在混合交通中的协同编队参与率，优化交通效率和能源效益", "motivation": "CAV部署初期稀疏分布导致难以形成有效协同编队，需要提升CAV参与协同编队的能力以最大化效益", "method": "采用QMIX框架结合卷积神经网络处理交通数据(CNN-QMIX)，设计轨迹规划器和模型预测控制器确保安全换道执行", "result": "模型能有效处理动态变化的交通智能体数量，协同编队率提升高达26.2%，显著优于基于规则的基准模型", "conclusion": "该模型在CAV部署早期阶段具有优化协同合作和交通动力学的潜力，为混合交通环境提供了有效的决策解决方案"}}
{"id": "2601.11575", "pdf": "https://arxiv.org/pdf/2601.11575", "abs": "https://arxiv.org/abs/2601.11575", "authors": ["Sotirios Panagiotis Chytas", "Vikas Singh"], "title": "Concept Attractors in LLMs and their Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.", "AI": {"tldr": "论文发现LLMs在特定层会将语义相关提示映射到相似的内部表示，这可以用迭代函数系统和概念吸引子来解释。基于此开发了无需训练的吸引子干预方法，在翻译、幻觉减少等任务上表现优异。", "motivation": "发现LLM在特定层对语义相关提示产生相似内部表示的现象，希望用数学框架解释这一行为并开发实用方法", "method": "使用迭代函数系统理论解释LLM层作为收缩映射的概念，开发基于吸引子的无训练干预方法", "result": "吸引子干预方法在语言翻译、幻觉减少、安全防护和合成数据生成等任务上匹配或超越专门基线方法", "conclusion": "基于吸引子的方法为LLM干预提供了高效、无需微调的替代方案，在基线方法表现不佳的场景中具有良好泛化能力"}}
{"id": "2601.11816", "pdf": "https://arxiv.org/pdf/2601.11816", "abs": "https://arxiv.org/abs/2601.11816", "authors": ["Zahra Moslemi", "Keerthi Koneru", "Yen-Ting Lee", "Sheethal Kumar", "Ramesh Radhakrishnan"], "title": "POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation", "categories": ["cs.AI"], "comment": "Workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks: AAAI 2026", "summary": "Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation", "AI": {"tldr": "POLARIS是一个面向企业后台工作流的治理编排框架，通过类型化计划合成和验证执行来实现可审计、策略对齐且操作可预测的智能体系统。", "motivation": "企业后台工作流需要具备可审计性、策略对齐性和操作可预测性的智能体系统，而通用的多智能体设置往往无法提供这些能力。", "method": "POLARIS框架包含三个核心组件：1）规划器生成结构多样且类型检查的有向无环图；2）基于规则的推理模块选择合规计划；3）执行阶段通过验证器门控检查、有限修复循环和编译策略护栏来阻止或路由副作用。", "result": "在文档中心财务任务中，POLARIS产生决策级工件和完整执行轨迹，减少人工干预。在SROIE数据集上达到0.81的微F1分数，在合成测试套件中实现0.95-1.00的异常路由精度，并保持审计追踪。", "conclusion": "POLARIS为策略对齐的智能体AI提供了方法论和基准参考，建立了治理型智能体AI的初步基准。"}}
{"id": "2601.11578", "pdf": "https://arxiv.org/pdf/2601.11578", "abs": "https://arxiv.org/abs/2601.11578", "authors": ["Ibrahim Al Azher", "Zhishuai Guo", "Hamed Alhoori"], "title": "LimAgents: Multi-Agent LLMs for Generating Research Limitations", "categories": ["cs.CL", "cs.AI"], "comment": "18 Pages, 9 figures", "summary": "Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.", "AI": {"tldr": "LimAgents是一个多智能体LLM框架，通过整合OpenReview评论、作者声明限制和引用文献，系统性地识别显性、隐性、同行评审和文献背景的限制，相比零-shot基线显著提升限制识别覆盖率。", "motivation": "当前零-shot大语言模型生成限制声明时往往流于表面（如数据集偏差或泛化性问题），重复作者已报告的限制而忽略更深层的方法论问题和背景差距，且许多作者仅披露部分或琐碎限制。", "method": "提出LimAgents多智能体框架，不同智能体分工协作：提取显性限制、分析方法论差距、模拟同行评审视角、分析文献背景，由Judge智能体精炼输出，Master智能体整合成清晰限制集合。引入基于LLM-as-a-Judge的点对点评估协议替代传统NLP指标。", "result": "实验显示LimAgents显著提升性能：RAG+多智能体GPT-4o mini配置相比零-shot基线获得+15.51%覆盖率提升，Llama 3 8B多智能体设置获得+4.41%改进。", "conclusion": "LimAgents通过多智能体协作和综合信息整合，能够系统生成实质性限制声明，为科学研究的透明性和严谨性提供有效工具，点对点评估协议能更准确衡量限制覆盖范围。"}}
{"id": "2601.11825", "pdf": "https://arxiv.org/pdf/2601.11825", "abs": "https://arxiv.org/abs/2601.11825", "authors": ["Arya Rahgozar", "Pouria Mortezaagha"], "title": "AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.", "AI": {"tldr": "本文提出了一个基于PICOS框架的AI科学家平台，通过整合关系存储、语义检索和知识图谱技术，实现了可扩展和透明的知识合成，显著提高了证据合成的效率和准确性。", "motivation": "生物医学研究中存在研究浪费问题，包括冗余研究、不完整报告和传统证据合成工作流程的可扩展性有限。", "method": "使用双向LSTM和基于Transformer的多任务分类器进行PICOS合规性和研究设计分类，采用检索增强生成技术进行全文合成，利用BERTopic进行主题建模分析。", "result": "Transformer模型在研究设计分类中达到95.7%准确率，Bi-LSTM在PICOS合规性检测中达到87%准确率。检索增强生成在结构化约束、跨研究整合和图推理方面优于非检索方法。", "conclusion": "PICOS感知和可解释的自然语言处理可以提高证据合成的可扩展性、透明度和效率，该架构是领域无关的，为减少生物医学学科的研究浪费提供了实用框架。"}}
{"id": "2601.11579", "pdf": "https://arxiv.org/pdf/2601.11579", "abs": "https://arxiv.org/abs/2601.11579", "authors": ["Krzysztof Ociepa", "Łukasz Flis", "Remigiusz Kinas", "Krzysztof Wróbel", "Adrian Gwoździej"], "title": "Bielik 11B v3: Multilingual Large Language Model for European Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.\n  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.\n  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.", "AI": {"tldr": "Bielik 11B v3是基于Mistral 7B架构扩展的110亿参数波兰语优化模型，通过四阶段训练流程实现卓越性能，在波兰语任务上超越其他专业模型，并能在多种硬件上高效部署。", "motivation": "开发一个专门针对波兰语优化的高性能语言模型，同时为资源有限的语言建立高效模型开发的新基准。", "method": "采用四阶段训练流程：持续预训练、监督微调(SFT)、直接偏好优化(DPO)和强化学习，基于Mistral 7B v0.2架构通过深度扩展达到110亿参数。", "result": "在波兰语任务上显著超越其他专业模型，性能优于参数量2-6倍的大型模型，涵盖从基础语言理解到复杂推理的广泛任务。", "conclusion": "Bielik 11B v3不仅提升了波兰语的AI能力，还为资源效率高、性能优异的少数语言模型开发设立了新标准，具有广泛的硬件部署能力。"}}
{"id": "2601.11840", "pdf": "https://arxiv.org/pdf/2601.11840", "abs": "https://arxiv.org/abs/2601.11840", "authors": ["Hongyu Lin", "Samer Abdallah", "Makar Valentinov", "Paul Brennan", "Elijah Kagan", "Christoph M. Wintersteiger", "Denis Ignatovich", "Grant Passmore"], "title": "Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic", "categories": ["cs.AI", "cs.LO", "cs.SE"], "comment": "52 pages, 23 figures. Includes a new benchmark dataset (code-logic-bench) and evaluation of neurosymbolic reasoning for software analysis", "summary": "Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.\n  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.\n  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.\n  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.", "AI": {"tldr": "CodeLogician是一个神经符号代理，通过将大语言模型与形式化推理引擎ImandraX结合，实现了对软件逻辑的精确分析，在代码逻辑推理准确性上相比纯LLM方法提升了41-47个百分点。", "motivation": "现有大语言模型在代码理解任务上表现良好，但缺乏对程序行为进行精确、彻底的数学推理能力。现有基准测试要么专注于与真实软件脱节的数学证明自动化，要么专注于不需要语义严谨性的工程任务。", "method": "开发了CodeLogician神经符号代理，集成工业级自动推理引擎ImandraX，使用LLM构建软件系统的显式形式化模型，实现超越二元验证结果的丰富语义问题自动推理。同时提出了code-logic-bench基准测试，衡量程序状态空间、控制流、覆盖约束和边界情况的推理正确性。", "result": "形式化增强相比纯LLM推理在推理准确性上带来了显著改进，缩小了41-47个百分点的差距。", "conclusion": "神经符号集成对于扩展程序分析以实现严谨、自主的软件理解至关重要，CodeLogician证明了这种集成方法的有效性。"}}
{"id": "2601.11580", "pdf": "https://arxiv.org/pdf/2601.11580", "abs": "https://arxiv.org/abs/2601.11580", "authors": ["Xiaoxuan Liu", "Jiaxiang Yu", "Jongseok Park", "Ion Stoica", "Alvin Cheung"], "title": "Speculative Decoding: Performance or Illusion?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.", "AI": {"tldr": "对生产级推理引擎vLLM上推测解码技术的首次系统性研究，评估了多种SD变体在不同工作负载下的性能，发现验证阶段是主要瓶颈，实测性能与理论上限存在显著差距。", "motivation": "推测解码已成为加速大语言模型推理的流行技术，但先前评估依赖研究原型和不切实际的小批量大小，其在真实场景中的有效性仍不明确。", "method": "在vLLM推理引擎上系统研究多种SD变体（n-gram、EAGLE/EAGLE-3、Draft-Model、Multi-Token Prediction），涵盖不同工作负载、模型规模和批量大小，分析影响SD性能的关键因素并量化理论上限。", "result": "目标模型验证阶段主导执行时间，接受长度在不同输出位置、请求和数据集间差异显著，实测性能与理论上限存在较大差距。", "conclusion": "研究揭示了推测解码在实际部署中的性能瓶颈，为改进SD技术指出了新的研究方向，特别是优化验证阶段和提高接受长度一致性的机会。"}}
{"id": "2601.11850", "pdf": "https://arxiv.org/pdf/2601.11850", "abs": "https://arxiv.org/abs/2601.11850", "authors": ["Matthew Nyaaba", "Min SungEun", "Mary Abiswin Apam", "Kwame Owoahene Acheampong", "Emmanuel Dwamena", "Xiaoming Zhai"], "title": "Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.", "AI": {"tldr": "研究探讨了生成式AI在质性研究中的应用，特别是ITAGPT工具如何支持归纳主题分析，通过人类-AI协作框架(HACITA)分析研究人员与AI的互动过程，发现AI作为程序性支架增强透明度，但解释权威仍由人类研究者掌握。", "motivation": "随着生成式AI在质性研究中日益广泛应用，需要探讨其对分析实践和解释权威的影响，研究人类与AI在归纳主题分析中的协作关系。", "method": "采用HACITA框架，三位经验丰富的质性研究人员使用专门设计的ITAGPT工具分析加纳教师教育背景的访谈转录文本，通过交互日志、AI生成表格、研究者修订记录等多源数据分析。", "result": "ITAGPT作为程序性支架有效结构化分析工作流程并提高透明度，但人类研究者通过修改、删除、拒绝、插入和评论等分析行为保持解释权威。", "conclusion": "研究表明归纳主题分析可以通过负责任的人类-AI协作实现，AI工具支持分析过程但不会取代人类研究者的解释判断权。"}}
{"id": "2601.11581", "pdf": "https://arxiv.org/pdf/2601.11581", "abs": "https://arxiv.org/abs/2601.11581", "authors": ["Yuefeng Wang", "ChangJae Lee"], "title": "Enhancing the QA Model through a Multi-domain Debiasing Framework", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 7 tables", "summary": "Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.", "AI": {"tldr": "本研究评估ELECTRA-small模型在SQuAD和对抗数据集上的表现，开发了多领域去偏框架，通过知识蒸馏和去偏技术实现了性能提升，在对抗环境下EM和F1分数最高提升2.6个百分点。", "motivation": "问答模型在机器阅读理解中虽进步显著，但存在偏见问题，特别是在对抗条件下的复杂查询中表现不佳，需要提高模型的鲁棒性和可靠性。", "method": "使用ELECTRA-small模型在SQuAD v1.1和对抗数据集AddSent、AddOneSent上进行评估，识别词汇偏见、数值推理和实体识别错误，开发包含知识蒸馏、去偏技术和领域扩展的多领域去偏框架。", "result": "在所有测试集上实现了最高2.6个百分点的EM和F1分数提升，在对抗环境下取得了显著增益。", "conclusion": "有针对性的偏见缓解策略具有增强自然语言理解系统鲁棒性和可靠性的潜力。"}}
{"id": "2601.11885", "pdf": "https://arxiv.org/pdf/2601.11885", "abs": "https://arxiv.org/abs/2601.11885", "authors": ["Zhifei Li", "Ziyue Qin", "Xiangyu Luo", "Xiaoju Hou", "Yue Zhao", "Miao Zhang", "Zhifang Huang", "Kui Xiao", "Bing Yang"], "title": "MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment", "categories": ["cs.AI"], "comment": "Accepted by AAAI 2026", "summary": "Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.", "AI": {"tldr": "MyGram是一个多模态实体对齐模型，通过模态扩散学习模块捕获模态内结构上下文信息，并使用Gram Loss实现跨模态全局分布一致性，在多个数据集上显著优于基线方法。", "motivation": "现有方法可能忽视模态内的结构上下文信息，容易受到浅层特征的干扰，需要更好的多模态融合和全局分布一致性方法。", "method": "提出模态扩散学习模块捕获模态内深层结构信息，引入Gram Loss作为正则化约束，最小化多模态特征形成的4维平行多面体体积。", "result": "在五个公开数据集上实验，MyGram优于基线模型，在FBDB15K上Hits@1最大提升4.8%，FBYG15K提升9.9%，DBP15K提升4.3%。", "conclusion": "MyGram通过有效捕获模态内结构信息和实现跨模态全局一致性，显著提升了多模态实体对齐的性能。"}}
{"id": "2601.11585", "pdf": "https://arxiv.org/pdf/2601.11585", "abs": "https://arxiv.org/abs/2601.11585", "authors": ["Hyunjun Kim"], "title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents", "categories": ["cs.CL"], "comment": null, "summary": "Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.", "AI": {"tldr": "提出了一种基于信息论的上下文工程框架ECS，通过测量模型答案分布向正确答案的偏移来量化上下文效用，相比传统词法相似性方法在精确上下文选择方面有显著提升", "motivation": "大型语言模型代理需要区分有用的上下文信息和误导性干扰信息，传统基于词重叠的词法相似性方法无法捕捉语用效用", "method": "引入熵上下文塑造(ECS)框架，将效用定义为答案概率的有符号变化，通过理论分析证明任务无关更新会产生接近零的分布偏移", "result": "在多轮上下文选择任务中，ECS在Llama-3.1-8B上达到F1=0.265，相比TF-IDF(F1=0.154)相对提升71.83%", "conclusion": "ECS框架能够有效捕捉语用效用，在精确上下文选择方面优于传统词法相似性方法，为LLM代理的上下文工程提供了理论基础和实践方案"}}
{"id": "2601.11903", "pdf": "https://arxiv.org/pdf/2601.11903", "abs": "https://arxiv.org/abs/2601.11903", "authors": ["YenTing Lee", "Keerthi Koneru", "Zahra Moslemi", "Sheethal Kumar", "Ramesh Radhakrishnan"], "title": "AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems", "categories": ["cs.AI"], "comment": "Workshop on W51: How Can We Trust and Control Agentic AI? Toward Alignment, Robustness, and Verifiability in Autonomous LLM Agents at AAAI 2026", "summary": "Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.\n  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight", "AI": {"tldr": "AEMA是一个面向企业级多智能体系统的评估框架，通过多步骤评估流程提供稳定、可追溯且可审计的LLM多智能体系统评估方案", "motivation": "现有评估方法局限于单响应评分或狭窄基准测试，缺乏稳定性、可扩展性和自动化能力，无法满足企业级多智能体系统的评估需求", "method": "提出AEMA框架，采用过程感知和可审计的设计，在人类监督下规划、执行和聚合异构智能体工作流的多步骤评估", "result": "相比单一LLM-as-a-Judge方法，AEMA在稳定性、人类对齐性和可追溯记录方面表现更优，在企业风格智能体工作流测试中验证了其有效性", "conclusion": "AEMA为LLM多智能体系统提供了透明且可复现的责任评估路径，支持可信赖的自动化评估"}}
{"id": "2601.11658", "pdf": "https://arxiv.org/pdf/2601.11658", "abs": "https://arxiv.org/abs/2601.11658", "authors": ["Indrajit Kar", "Sammy Zonunpuia", "Zonunfeli Ralte"], "title": "Towards AGI A Pragmatic Approach Towards Self Evolving Agent", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.", "AI": {"tldr": "提出了一个分层自进化多智能体框架，通过集成基础LLM、操作SLM智能体、代码生成LLM和教师LLM，使智能体能够在部署后持续适应和改进能力。", "motivation": "现有基于大语言模型的智能体在部署后是静态的，缺乏自主扩展能力、生成新工具或进化推理的能力，需要一种能够持续自我进化的解决方案。", "method": "采用分层框架：任务失败时先尝试工具合成，若持续失败则触发进化阶段，使用课程学习(CL)、基于奖励的学习(RL)或遗传算法(GA)进行进化。使用TaskCraft数据集进行评估。", "result": "进化后的智能体在所有设置中都优于原始版本：CL提供快速恢复和强泛化能力，RL在高难度任务上表现优异，GA提供高行为多样性。", "conclusion": "该框架展示了智能体能够实现稳健、自主、自我改进的进化，为构建具有持续学习能力的AI系统提供了有效途径。"}}
{"id": "2601.11905", "pdf": "https://arxiv.org/pdf/2601.11905", "abs": "https://arxiv.org/abs/2601.11905", "authors": ["Junyu Cao", "Ruijiang Gao", "Esmaeil Keyvanshokooh", "Jianhao Ma"], "title": "LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning", "categories": ["cs.AI", "cs.LG", "math.ST"], "comment": "50 pages. Previous version with human-AI collaboration: arXiv:2410.14640", "summary": "We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.", "AI": {"tldr": "提出LIBRA算法，结合大语言模型和上下文赌博机，用于高风险医疗决策，提供后悔减少、LLM使用效率和高鲁棒性保证", "motivation": "解决个性化医疗等高风险场景中需要同时选择治疗方案和患者特征修改的序列决策问题，整合LLM领域知识和统计学习方法", "method": "首先提出recourse bandit问题框架，开发GLRB算法，然后提出LIBRA算法，战略性地结合LLM知识和赌博机学习，提供三个理论保证", "result": "实验证明GLRB和LIBRA在后悔、治疗质量和样本效率上优于标准上下文赌博机和纯LLM基准，建立了匹配的下界证明算法近最优性", "conclusion": "recourse-aware的LLM辅助赌博机算法为高风险个性化决策中可信的LLM-赌博机协作提供了有前景的解决方案"}}
{"id": "2601.11722", "pdf": "https://arxiv.org/pdf/2601.11722", "abs": "https://arxiv.org/abs/2601.11722", "authors": ["Ahmed Rayane Kebir", "Vincent Guigue", "Lynda Said Lhadj", "Laure Soulier"], "title": "RAC: Retrieval-Augmented Clarification for Faithful Conversational Search", "categories": ["cs.CL", "cs.IR"], "comment": "This is the author's version of the work. The definitive version is published in: Proceedings of the 48th European Conference on Information Retrieval (ECIR '26), 29 March--2 April, 2026, Delft, Netherlands", "summary": "Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.", "AI": {"tldr": "RAC框架通过检索增强生成技术，结合对比偏好优化，生成基于文档证据的澄清问题，显著提升了对话搜索系统中澄清问题的忠实性和有效性。", "motivation": "现有研究主要关注澄清问题的流畅性和意图对齐，但忽略了问题与底层语料库的关联性，导致系统可能提出无法从可用文档中回答的问题。", "method": "1. 比较多种检索索引策略；2. 微调大语言模型以充分利用检索上下文；3. 应用对比偏好优化，优先选择基于检索段落支持的问题而非无根据的替代方案。", "result": "在四个基准测试中，RAC相比基线方法表现出显著改进。通过NLI和数据到文本的新度量标准验证了问题在上下文中的锚定效果，证明方法持续提升了忠实性。", "conclusion": "RAC框架成功解决了澄清问题的语料库忠实性问题，通过检索增强和对比优化确保了生成的问题能够基于可用文档进行回答，为对话搜索系统提供了更可靠的澄清机制。"}}
{"id": "2601.11940", "pdf": "https://arxiv.org/pdf/2601.11940", "abs": "https://arxiv.org/abs/2601.11940", "authors": ["Kang Chen", "Fan Yu", "Junjie Nian", "Shihan Zhao", "Zhuoka Feng", "Zijun Yao", "Heng Wang", "Minshen Yu", "Yixin Cao"], "title": "Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.", "AI": {"tldr": "论文提出了TAAR框架，通过检测思维陷阱并自适应重启解码来解决长链思维推理中早期错误承诺导致持续错误的问题，在多个数学和科学推理基准上显著提升性能。", "motivation": "长链思维推理虽然增强推理能力，但早期错误会导致模型陷入自我一致但错误的思维陷阱，后续反思和验证无法修正根本错误，89%的失败案例存在此类问题。", "method": "提出TAAR框架：训练诊断策略预测思维陷阱位置和逃脱概率，在推理时截断轨迹并自适应重启解码，对严重陷阱情况应用更强扰动如高温重采样和结构化重启后缀。", "result": "在AIME24、AIME25、GPQA-Diamond、HMMT25、BRUMO5等挑战性数学和科学推理基准上，TAAR提升了推理性能且无需微调基础模型参数。", "conclusion": "TAAR通过检测和避免思维陷阱有效解决了长链思维推理中的错误累积问题，为测试时计算优化提供了新方向。"}}
{"id": "2601.11739", "pdf": "https://arxiv.org/pdf/2601.11739", "abs": "https://arxiv.org/abs/2601.11739", "authors": ["Xinyu Pi", "Qisen Yang", "Chuong Nguyen", "Hua Shen"], "title": "Bridging Human Interpretation and Machine Representation: A Landscape of Qualitative Data Analysis in the LLM Era", "categories": ["cs.CL"], "comment": null, "summary": "LLMs are increasingly used to support qualitative research, yet existing systems produce outputs that vary widely--from trace-faithful summaries to theory-mediated explanations and system models. To make these differences explicit, we introduce a 4$\\times$4 landscape crossing four levels of meaning-making (descriptive, categorical, interpretive, theoretical) with four levels of modeling (static structure, stages/timelines, causal pathways, feedback dynamics). Applying the landscape to prior LLM-based automation highlights a strong skew toward low-level meaning and low-commitment representations, with few reliable attempts at interpretive/theoretical inference or dynamical modeling. Based on the revealed gap, we outline an agenda for applying and building LLM-systems that make their interpretive and modeling commitments explicit, selectable, and governable.", "AI": {"tldr": "论文提出了一个4×4框架来分析LLM在质性研究中的输出差异，发现现有系统偏向低层次意义和低承诺表示，缺乏解释性/理论推断和动态建模，并提出了改进议程。", "motivation": "LLM在质性研究中应用日益广泛，但输出结果差异很大，从忠实总结到理论解释和系统模型都有，需要明确这些差异的分类框架。", "method": "引入4×4分析框架：四个意义建构层次（描述性、分类性、解释性、理论性）与四个建模层次（静态结构、阶段/时间线、因果路径、反馈动态）的交叉分析。", "result": "应用该框架分析现有LLM自动化系统发现，多数系统偏向低层次意义建构和低承诺表示，很少有可靠的解释性/理论推断或动态建模尝试。", "conclusion": "基于发现的差距，提出了一个议程，旨在开发能够明确、可选择和可管理其解释性和建模承诺的LLM系统。"}}
{"id": "2601.11974", "pdf": "https://arxiv.org/pdf/2601.11974", "abs": "https://arxiv.org/abs/2601.11974", "authors": ["Xinmeng Hou", "Peiliang Gong", "Bohao Qu", "Wuqi Wang", "Qing Guo", "Yang Liu"], "title": "Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement", "categories": ["cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.", "AI": {"tldr": "MARS框架通过单次循环实现高效自我进化，结合原则性反思和过程性反思来优化LLM代理的推理逻辑，显著降低计算成本并提升性能。", "motivation": "现有LLM代理受限于静态人工设计的提示词，缺乏适应性。现有的自我改进框架通常依赖低效的多轮递归循环，计算成本高昂。", "method": "提出MARS框架，受教育心理学启发，整合原则性反思（抽象规范规则避免错误）和过程性反思（推导逐步成功策略），通过单次循环合成优化指令。", "result": "在六个基准测试上的广泛实验表明，MARS优于最先进的自我进化系统，同时显著减少计算开销。", "conclusion": "MARS框架成功实现了LLM代理的高效自我进化，通过模拟人类学习过程，在单次循环中完成系统性推理逻辑优化，为自适应AI系统提供了新方向。"}}
{"id": "2601.11746", "pdf": "https://arxiv.org/pdf/2601.11746", "abs": "https://arxiv.org/abs/2601.11746", "authors": ["George Mihaila", "Suleyman Olcay Polat", "Poli Nemkova", "Himanshu Sharma", "Namratha V. Urs", "Mark V. Albert"], "title": "LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Language Models for neighborhood generation, they rely on unconstrained paraphrasing that introduces confounding variables, making it difficult to isolate specific feature contributions. We introduce LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations. By enforcing a strict \"Single Mask-Single Sample\" protocol and employing distinct neutral infill and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. We evaluate our method against established baselines (LIME, SHAP, Integrated Gradients) and the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and HateXplain using human-annotated rationales as ground truth. Empirical results demonstrate that LIME-LLM establishes a new benchmark for black-box NLP explainability, achieving significant improvements in local explanation fidelity compared to both traditional perturbation-based methods and recent generative alternatives.", "AI": {"tldr": "LIME-LLM是一个新的NLP解释框架，通过假设驱动的受控扰动替代随机标记掩码，使用单掩码单样本协议和中性填充策略，显著提高了局部解释的保真度。", "motivation": "传统LIME等方法依赖随机标记掩码，会产生语义无效的分布外输入，降低局部替代模型的保真度。最近的生成方法如LLiMe虽然使用大语言模型生成邻域，但引入混淆变量，难以隔离特定特征贡献。", "method": "提出LIME-LLM框架，采用假设驱动的受控扰动，执行严格的\"单掩码单样本\"协议，使用中性填充和边界填充策略，构建流畅的在流形上的邻域以隔离特征效应。", "result": "在CoLA、SST-2和HateXplain三个基准测试中，使用人工标注的理性作为真实标签进行评估，结果显示LIME-LLM相比传统方法(LIME、SHAP、Integrated Gradients)和生成基线LLiMe，在局部解释保真度方面取得显著改进。", "conclusion": "LIME-LLM为黑盒NLP可解释性设立了新的基准，通过受控扰动策略有效解决了传统方法的语义无效性和生成方法的混淆变量问题。"}}
{"id": "2601.11979", "pdf": "https://arxiv.org/pdf/2601.11979", "abs": "https://arxiv.org/abs/2601.11979", "authors": ["Ang Gao", "Changshuo Zhang", "Xiao Zhang", "Deyang Li", "Minjun Zhao", "Fangchao Liu", "Xinyu Zhang"], "title": "Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.", "AI": {"tldr": "提出了Process In-Context Learning (PICL)框架，通过动态插入相关示例来解决数学推理中的实时混淆点，相比静态示例方法显著提升了推理准确性。", "motivation": "现有上下文学习方法使用静态预选示例，无法适应多步数学推理中出现的动态混淆点（如模糊计算、逻辑漏洞），导致级联错误和最终准确率下降。", "method": "PICL采用两阶段框架：1) 通过分析推理过程中的语义和熵来识别潜在混淆点并总结核心特征；2) 在遇到混淆点时从示例池中检索相关示例并直接插入到推理过程中指导后续步骤。", "result": "实验表明PICL在数学推理任务上优于基线方法，通过减轻推理过程中的混淆提高了性能。", "conclusion": "动态演示插入在复杂数学推理中具有重要价值，PICL框架通过实时响应推理需求有效提升了上下文学习的效果。"}}
{"id": "2601.11758", "pdf": "https://arxiv.org/pdf/2601.11758", "abs": "https://arxiv.org/abs/2601.11758", "authors": ["Arnab Das Utsa"], "title": "Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 figures, more than 1o pages", "summary": "Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.", "AI": {"tldr": "该研究提出了一个基于社交媒体语言的透明化焦虑检测方法，使用逻辑回归分类器和语言学可解释特征，在Reddit数据上实现了高性能且具有关键词鲁棒性的焦虑检测。", "motivation": "全球数亿人受焦虑影响但大规模筛查有限，现有社交媒体检测模型缺乏可解释性、关键词鲁棒性验证和严格的数据完整性保证。", "method": "使用Reddit帖子的大规模数据集，在精心策划的子版块上训练逻辑回归分类器，进行特征消融、关键词掩码实验、不同密度差异分析，并使用临床访谈数据进行外部验证。", "result": "模型表现出色，即使在移除情感或掩码关键词后仍保持高准确率，早期检测显著优于随机分类，跨域分析与临床数据高度一致。", "conclusion": "透明化的语言学特征可以支持可靠、可泛化且关键词鲁棒的情绪检测，该框架为不同在线环境下的可解释心理健康筛查提供了可复现的基线。"}}
{"id": "2601.12002", "pdf": "https://arxiv.org/pdf/2601.12002", "abs": "https://arxiv.org/abs/2601.12002", "authors": ["Oliver Schön", "Zhengang Zhong", "Sadegh Soudjani"], "title": "Kernel-Based Learning of Safety Barriers", "categories": ["cs.AI", "cs.LG", "eess.SY"], "comment": "44 pages, 9 figures", "summary": "The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. We employ the concept of control barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result to out-of-distribution behavior. We provide the theoretical results on how to apply the approach to general classes of temporal logic specifications beyond safety. For the data-driven computation of safety barriers, we leverage a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. Our work moves beyond restrictive assumptions on system dynamics and uncertainty, as demonstrated on two case studies including a black-box system with a neural network controller.", "AI": {"tldr": "提出一种基于数据驱动的黑盒系统安全验证方法，使用控制屏障证书和核希尔伯特空间技术，通过傅里叶变换将复杂优化问题转化为线性规划，实现可扩展的分布式鲁棒安全验证框架", "motivation": "AI在自动驾驶和医疗等安全关键应用中快速发展，但传统形式化验证工具难以处理黑盒系统的复杂性和不确定性，需要更灵活的可扩展验证方法", "method": "采用控制屏障证书概念，通过条件均值嵌入将系统数据映射到再生核希尔伯特空间，构建RKHS模糊集，利用有限傅里叶展开将半无限优化问题转化为线性规划问题", "result": "开发了谱屏障方法，能够高效处理复杂的安全验证问题，突破了系统动力学和不确定性的限制性假设，在两个案例研究中验证了有效性", "conclusion": "该方法为黑盒AI系统提供了可扩展的分布式鲁棒安全验证框架，能够处理超越安全性的更广泛时序逻辑规范，具有重要的理论和实践价值"}}
{"id": "2601.11762", "pdf": "https://arxiv.org/pdf/2601.11762", "abs": "https://arxiv.org/abs/2601.11762", "authors": ["Sae Young Moon", "Myeongjun Erik Jang", "Haoyan Luo", "Chunyang Xiao", "Antonios Georgiadis", "Fran Silavong"], "title": "Industry-Aligned Granular Topic Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.", "AI": {"tldr": "本文提出了TIDE框架，这是一个基于大语言模型的粒度主题建模方法，能够生成更细粒度的主题，并在多个业务数据集上优于现有方法。", "motivation": "尽管粒度概念在商业应用中具有重要价值，但现有主题建模方法生成粒度主题的能力尚未得到充分探索，因此需要开发能够提供更深层次洞察的新方法。", "method": "提出了TIDE框架，核心是基于大语言模型(LLMs)的新颖粒度主题建模方法，同时包含长文档摘要、主题层级化和精炼等辅助功能。", "result": "在多个公共和真实商业数据集上的实验表明，TIDE的主题建模方法优于现代主题建模方法，辅助组件为工业业务场景提供了有价值的支持。", "conclusion": "TIDE框架通过LLM驱动的粒度主题建模有效解决了商业应用中的深度洞察需求，目前正在进行开源过程。"}}
{"id": "2601.12014", "pdf": "https://arxiv.org/pdf/2601.12014", "abs": "https://arxiv.org/abs/2601.12014", "authors": ["Elio Masciari", "Vincenzo Moscato", "Enea Vincenzo Napolitano", "Gian Marco Orlando", "Marco Perillo", "Diego Russo"], "title": "Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.\n  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.", "AI": {"tldr": "本文提出了一个可持续性评估框架，用于衡量LLM结构化输出的环境效率，引入GCS_env指标综合评估结构正确性和碳排放效率，发现TOON格式在紧凑性和排放方面优于JSON/XML/YAML，但需要模型原生支持才能保证正确性。", "motivation": "现有基准测试主要关注结构化输出的正确性，但忽略了不同输出格式在推理过程中的环境影响，需要综合考虑环境效率的评估框架。", "method": "提出可持续性评估框架，测量token使用量、生成时间和估计碳排放，并设计Environment-Aware Generation Correctness Score (GCS_env)统一指标。", "result": "TOON格式产生更紧凑的输出和更低的排放，但在缺乏原生支持的模型中结构正确性较低；模型容量增加可缩小这一差距；环境感知评分可根据部署优先级改变格式排名。", "conclusion": "需要进行可持续性包容的基准测试，紧凑表示如TOON在大规模碳意识LLM部署中具有实际优势，模型能力和环境效率需要平衡考虑。"}}
{"id": "2601.11776", "pdf": "https://arxiv.org/pdf/2601.11776", "abs": "https://arxiv.org/abs/2601.11776", "authors": ["Kaituo Zhang", "Zhimeng Jiang", "Na Zou"], "title": "Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems.", "AI": {"tldr": "本文提出了一种完全自反思的LLM去毒框架，利用LLM自身能力进行毒性检测、修正和模型优化，无需外部模块或人工标注，在保持语义保真度的同时实现了更好的去毒效果。", "motivation": "当前去毒技术依赖外部模块、人工标注或人工干预，阻碍了可扩展性和一致性。LLMs已展现出强大的生成能力和自我调节机制，但现有方法很少利用这些内置能力。", "method": "提出毒性信号检测器（内部自识别机制）和系统性干预流程，通过迭代过程将有毒文本转化为无毒对应文本，生成对比去毒数据集用于模型微调。", "result": "在DetoxLLM和ParaDetox基准测试中，该方法在保持语义保真度的同时，实现了比最先进方法更好的去毒性能。", "conclusion": "该方法揭示了LLMs固有的自我去毒能力，为减轻有害内容生成提供了一致有效的途径，为真正自我调节的语言模型和更负责任、符合伦理的文本生成系统铺平了道路。"}}
{"id": "2601.12024", "pdf": "https://arxiv.org/pdf/2601.12024", "abs": "https://arxiv.org/abs/2601.12024", "authors": ["Kartikey Singh Bhandari", "Tanish Jain", "Archit Agrawal", "Dhruv Kumar", "Praveen Kumar", "Pratik Narang"], "title": "A Multi-Agent System for Generating Actionable Business Advice", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.", "AI": {"tldr": "提出基于大语言模型的多智能体框架，将客户评论转化为可操作的商业建议，通过聚类、生成、评估和可行性排序四步骤，在多个服务领域实验中优于单模型基线。", "motivation": "现有分析方法局限于情感分析和方面提取等描述性任务，LLM生成的建议缺乏准确性和深度推理，需要将大规模评论语料转化为可执行的商业建议。", "method": "开发四组件框架：聚类选择代表性评论、生成建议、迭代评估、基于可行性的排序，结合语料蒸馏和反馈驱动的建议精炼。", "result": "在三个服务领域和多个模型家族的实验中，框架在可操作性、特异性和非冗余性方面持续优于单模型基线，中型模型性能接近大型模型框架。", "conclusion": "该多智能体LLM框架能有效将客户评论转化为具体、可操作且实用的商业建议，为决策支持提供有效工具。"}}
{"id": "2601.11778", "pdf": "https://arxiv.org/pdf/2601.11778", "abs": "https://arxiv.org/abs/2601.11778", "authors": ["Sheriff Issaka", "Erick Rosas Gonzalez", "Lieqi Liu", "Evans Kofi Agyei", "Lucas Bandarkar", "Nanyun Peng", "David Ifeoluwa Adelani", "Francisco Guzmán", "Saadia Gabriel"], "title": "Translation as a Scalable Proxy for Multilingual Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can translation quality alone indicate a model's broader multilingual capabilities? Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, we find that translation performance is a good indicator of downstream task success (e.g., Phi-4, median Pearson r: MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding. Translation quality, thus emerges as a strong, inexpensive first-pass proxy of multilingual performance, enabling a translation-first screening with targeted follow-up for specific tasks.", "AI": {"tldr": "翻译质量可作为评估大语言模型多语言能力的有效替代指标，通过翻译性能可以预测下游任务表现，为资源稀缺语言提供低成本评估方案", "motivation": "解决LLM多语言评估的困境：现有基准覆盖语言不足，传统构建方法成本高且面临数据污染问题，需要寻找更简单有效的评估替代方案", "method": "系统评估14个模型（1B-72B参数）在9个多样化基准和7个翻译指标上的表现，分析翻译性能与下游任务成功的相关性", "result": "翻译性能是下游任务成功的良好指标（Phi-4模型的中位数Pearson相关系数：MetricX=0.89，xCOMET=0.91，SSA-COMET=0.87），表明忠实翻译所需表征能力与多语言理解能力高度重叠", "conclusion": "翻译质量可作为强大且廉价的多语言性能初步代理指标，实现以翻译为先的筛选策略，为特定任务提供有针对性的后续评估"}}
{"id": "2601.12030", "pdf": "https://arxiv.org/pdf/2601.12030", "abs": "https://arxiv.org/abs/2601.12030", "authors": ["Yilun Yao", "Shan Huang", "Elsie Dai", "Zhewen Tan", "Zhenyu Duan", "Shousheng Jia", "Yanbing Jiang", "Tong Yang"], "title": "ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents", "categories": ["cs.AI"], "comment": "15 pages, 5 figures", "summary": "Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.", "AI": {"tldr": "ARC框架通过主动的反思驱动上下文管理，在长程信息搜索任务中显著提升大语言模型的性能，相比被动压缩方法准确率提升最高达11%", "motivation": "现有方法将上下文作为静态产物处理，导致早期错误和重点偏差持续存在，随着交互历史增长出现性能退化（上下文腐化）", "method": "提出ARC框架，将上下文管理系统化为主动的反思驱动过程，通过反思驱动的监控和修订机制，在检测到错位或退化时主动重组工作上下文", "result": "在挑战性的长程信息搜索基准测试中，ARC始终优于被动上下文压缩方法，在BrowseComp-ZH基准上使用Qwen2.5-32B-Instruct模型实现准确率绝对提升11%", "conclusion": "将上下文视为动态内部推理状态并通过主动反思进行管理，是解决长程信息搜索中上下文腐化问题的有效方法"}}
{"id": "2601.11791", "pdf": "https://arxiv.org/pdf/2601.11791", "abs": "https://arxiv.org/abs/2601.11791", "authors": ["Laya Iyer", "Pranav Somani", "Alice Guo", "Dan Jurafsky", "Chen Shani"], "title": "Beyond Tokens: Concept-Level Training Objectives for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \\textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ``mom,'' ``mommy,'' ``mother'' $\\rightarrow$ \\textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \\textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.", "AI": {"tldr": "论文提出从token级别预测转向概念级别预测的新训练目标，通过将语义相同的不同表面形式(如mom/mother/mommy)归为同一概念，改进LLM训练效果", "motivation": "传统next-token预测(NTP)目标在token级别工作，会将语义相同但表面形式不同的合理延续视为错误，导致模型偏向表面形式而非深层语义", "method": "引入概念级监督方法，将相同概念的多种表面形式分组(如MOTHER概念包含mom、mommy、mother)，并将这种概念级监督整合到LLM训练中", "result": "概念感知模型在困惑度、领域转移鲁棒性和多样化NLP基准测试上表现优于基于NTP的模型", "conclusion": "概念级监督是改进的训练信号，能更好地将LLM与人类语义抽象对齐"}}
{"id": "2601.12038", "pdf": "https://arxiv.org/pdf/2601.12038", "abs": "https://arxiv.org/abs/2601.12038", "authors": ["Beishui Liao"], "title": "Abstract Argumentation with Subargument Relations", "categories": ["cs.AI"], "comment": "11 pages", "summary": "Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.", "AI": {"tldr": "该论文提出了一个扩展的抽象论证框架，在传统攻击关系基础上增加了明确的子论证关系，用于更好地表示结构化论证中的依赖关系。", "motivation": "传统的Dung抽象论证框架仅通过攻击关系来表征论证可接受性，忽略了论证的内部结构依赖，特别是子论证关系。现有的双极论证框架引入了支持关系，但无法捕捉子论证的非对称性和构成性本质及其与攻击的交互。", "method": "研究在抽象论证框架中显式地加入子论证关系，将其与攻击关系一起作为基本关系。分析子论证关系如何与攻击关系交互，并考察其对基本语义属性的影响。", "result": "提出了一个包含子论证关系的原则性抽象框架，阐明了子论证在抽象可接受性推理中的作用。", "conclusion": "该框架为结构化信息提供了原则性的抽象，能够更好地表示结构化论证形式中的核心依赖关系，弥补了传统抽象论证框架的局限性。"}}
{"id": "2601.11819", "pdf": "https://arxiv.org/pdf/2601.11819", "abs": "https://arxiv.org/abs/2601.11819", "authors": ["Shirlene Rose Bandela", "Sanjeev Parthasarathy", "Vaibhav Garg"], "title": "TWeddit : A Dataset of Triggering Stories Predominantly Shared by Women on Reddit", "categories": ["cs.CL"], "comment": "11 pages, 12 figures, 7 tables", "summary": "Warning: This paper may contain examples and topics that may be disturbing to some readers, especially survivors of miscarriage and sexual violence. People affected by abortion, miscarriage, or sexual violence often share their experiences on social media to express emotions and seek support. On public platforms like Reddit, where users can post long, detailed narratives (up to 40,000 characters), readers may be exposed to distressing content. Although Reddit allows manual trigger warnings, many users omit them due to limited awareness or uncertainty about which categories apply. There is scarcity of datasets on Reddit stories labeled for triggering experiences. We propose a curated Reddit dataset, TWeddit, covering triggering experiences related to issues majorly faced by women. Our linguistic analyses show that annotated stories in TWeddit express distinct topics and moral foundations, making the dataset useful for a wide range of future research.", "AI": {"tldr": "该论文创建了一个名为TWeddit的Reddit数据集，专门标注包含可能引发创伤的内容（如流产和性暴力经历），用于支持相关研究。", "motivation": "社交媒体用户经常分享创伤经历来寻求支持，但Reddit等平台缺乏系统的触发警告机制，现有标注数据集稀缺，需要专门的数据集来研究这类内容。", "method": "通过人工标注Reddit上的长篇叙事帖子，构建TWeddit数据集，覆盖主要影响女性的触发经历内容，并进行语言学和道德基础分析。", "result": "分析显示TWeddit中标注的故事表达了独特的主题和道德基础特征，证实了该数据集对未来研究的实用性。", "conclusion": "TWeddit数据集为研究创伤性内容提供了有价值的资源，有助于理解社交媒体上敏感话题的表达方式和影响。"}}
{"id": "2601.12040", "pdf": "https://arxiv.org/pdf/2601.12040", "abs": "https://arxiv.org/abs/2601.12040", "authors": ["Murilo da Luz", "Bruno Brandão", "Luana Martins", "Gustavo Oliveira", "Bryan de Oliveira", "Luckeciano Melo", "Telma Soares"], "title": "Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty", "categories": ["cs.AI"], "comment": null, "summary": "The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.", "AI": {"tldr": "PREGU方法通过监测LLM生成过程中的熵值来识别不确定性，当熵超过阈值时暂停生成并进行局部搜索优化，在多个推理基准测试中表现优于或等同于Soft Reasoning方法", "motivation": "大型语言模型在数学和逻辑推理等多步推理场景中存在局限性，需要改进不确定性处理能力", "method": "提出PREGU方法：1) 监控自回归生成过程中的输出分布熵值 2) 当熵超过阈值时暂停生成 3) 在潜在空间进行局部搜索优化部分推理 4) 使用Soft Reasoning方法选择最连贯答案", "result": "在LLaMA-3-8B、Mistral-7B和Qwen2-7B模型上，在GSM8K、GSM-Hard、SVAMP和StrategyQA四个推理基准测试中，性能达到或超过Soft Reasoning方法", "conclusion": "熵可以作为推理过程中触发选择性优化的有效信号，PREGU方法能有效提升LLM在复杂推理任务中的表现"}}
{"id": "2601.11846", "pdf": "https://arxiv.org/pdf/2601.11846", "abs": "https://arxiv.org/abs/2601.11846", "authors": ["Natalia Tomashenko", "Xiaoxiao Miao", "Pierre Champion", "Sarina Meyer", "Michele Panariello", "Xin Wang", "Nicholas Evans", "Emmanuel Vincent", "Junichi Yamagishi", "Massimiliano Todisco"], "title": "The Third VoicePrivacy Challenge: Preserving Emotional Expressiveness and Linguistic Content in Voice Anonymization", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "under review", "summary": "We present results and analyses from the third VoicePrivacy Challenge held in 2024, which focuses on advancing voice anonymization technologies. The task was to develop a voice anonymization system for speech data that conceals a speaker's voice identity while preserving linguistic content and emotional state. We provide a systematic overview of the challenge framework, including detailed descriptions of the anonymization task and datasets used for both system development and evaluation. We outline the attack model and objective evaluation metrics for assessing privacy protection (concealing speaker voice identity) and utility (content and emotional state preservation). We describe six baseline anonymization systems and summarize the innovative approaches developed by challenge participants. Finally, we provide key insights and observations to guide the design of future VoicePrivacy challenges and identify promising directions for voice anonymization research.", "AI": {"tldr": "2024年第三届VoicePrivacy挑战赛的结果与分析，聚焦语音匿名化技术，要求开发能隐藏说话人身份同时保留语言内容和情感状态的系统。", "motivation": "推动语音匿名化技术发展，保护说话人隐私同时保持语音的实用价值（内容和情感）。", "method": "建立挑战赛框架，包括匿名化任务定义、数据集、攻击模型和评估指标，提供6个基线系统并总结参赛者的创新方法。", "result": "提供了系统性的挑战赛概述，包括详细的评估框架和不同匿名化方法的性能分析。", "conclusion": "为未来VoicePrivacy挑战赛的设计提供关键见解，并指出了语音匿名化研究的有前景方向。"}}
{"id": "2601.12126", "pdf": "https://arxiv.org/pdf/2601.12126", "abs": "https://arxiv.org/abs/2601.12126", "authors": ["Guocun Wang", "Kenkun Liu", "Jing Lin", "Guorui Song", "Jian Li", "Xiaoguang Han"], "title": "UniMo: Unified Motion Generation and Understanding with Chain of Thought", "categories": ["cs.AI"], "comment": null, "summary": "Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.", "AI": {"tldr": "UniMo是一个基于大语言模型的新型框架，通过监督微调和强化学习整合运动-语言信息与可解释的思维链推理，显著提升了3D人体运动生成和理解任务的性能。", "motivation": "现有3D人体运动生成和理解方法可解释性有限，且基于大语言模型的统一框架存在语义对齐和任务连贯性挑战，同时next-token预测范式不适合运动序列，导致累积预测错误。", "method": "通过监督微调(SFT)将运动-语言信息和可解释思维链推理整合到LLM中，并使用Group Relative Policy Optimization (GRPO)强化学习作为后训练策略，优化token组以增强结构正确性和语义对齐。", "result": "大量实验表明UniMo显著优于现有的统一和任务特定模型，在运动生成和理解任务上都达到了最先进的性能。", "conclusion": "UniMo框架成功解决了现有方法的局限性，通过创新的SFT和GRPO方法实现了运动与语言的更好整合，为3D人体运动分析提供了更有效的统一解决方案。"}}
{"id": "2601.11854", "pdf": "https://arxiv.org/pdf/2601.11854", "abs": "https://arxiv.org/abs/2601.11854", "authors": ["Yifei Zhang", "Hooshang Nayyeri", "Rinat Khaziev", "Emine Yilmaz", "Gokhan Tur", "Dilek Hakkani-Tür", "Hari Thadakamalla"], "title": "ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.", "AI": {"tldr": "ATOD是一个针对任务导向对话系统的基准测试和对话生成管道，专门设计用于评估具有API和工具集成的大型语言模型的智能体行为，包括多目标协调、依赖管理、记忆、适应性和主动性等关键能力。", "motivation": "现有基准测试缺乏对现代任务导向对话系统中智能体行为（如多目标协调、长时程推理、异步执行等）的系统性评估支持，需要新的评估框架来填补这一空白。", "method": "提出ATOD基准测试和合成对话生成管道，生成带有丰富注释的长时程推理对话；开发ATOD-Eval评估框架，将智能体能力维度转化为细粒度指标；提出基于记忆的智能体评估器。", "result": "实验表明ATOD-Eval能够在任务完成度、智能体能力和响应质量方面进行全面评估，提出的评估器在准确性和效率之间提供了更好的权衡。", "conclusion": "ATOD为评估现代任务导向对话系统的智能体行为提供了系统性的基准测试和评估框架，有助于推动该领域的研究和发展。"}}
{"id": "2601.12138", "pdf": "https://arxiv.org/pdf/2601.12138", "abs": "https://arxiv.org/abs/2601.12138", "authors": ["Abhishek Kumar", "Riya Tapwal", "Carsten Maple"], "title": "DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.", "AI": {"tldr": "论文提出了DriveSafe，一个专门针对基于LLM的驾驶助手的四级风险分类法，包含129个细粒度风险类别，涵盖技术、法律、社会、伦理维度，并通过实验验证了现有LLM在驾驶场景中安全拒绝能力的不足。", "motivation": "现有LLM安全分类和评估框架多为通用目的，无法捕捉真实驾驶场景中的领域特定风险，而LLM在车载数字助手中的不安全、模糊或法律错误响应可能引发严重的安全、伦理和监管后果。", "method": "开发了分层的四级风险分类法（DriveSafe），包含129个原子风险类别，基于真实驾驶法规和安全原则，并经过领域专家评审；在六个广泛部署的LLM上评估其对不安全或不合规驾驶相关查询的拒绝行为。", "result": "评估的模型经常无法适当拒绝不安全或不合规的驾驶相关查询，突显了通用安全对齐在驾驶场景中的局限性。", "conclusion": "需要专门针对驾驶领域的风险评估框架和安全对齐方法，现有通用LLM安全措施在驾驶场景中存在显著不足，DriveSafe分类法为系统评估和改进LLM驾驶助手安全性提供了基础。"}}
{"id": "2601.11865", "pdf": "https://arxiv.org/pdf/2601.11865", "abs": "https://arxiv.org/abs/2601.11865", "authors": ["Truong Nguyen", "Phi Van Dat", "Ngan Nguyen", "Linh Ngo Van", "Trung Le", "Thanh Hong Nguyen"], "title": "CTPD: Cross Tokenizer Preference Distillation", "categories": ["cs.CL"], "comment": "AAAI 2026", "summary": "While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher's preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.", "AI": {"tldr": "本文提出了跨分词器偏好蒸馏（CTPD）框架，解决了不同分词器模型间偏好知识蒸馏的技术难题，通过字符级对齐和重要性采样实现了高效的跨模型偏好对齐。", "motivation": "现有知识蒸馏方法在处理不同分词器的教师-学生模型时存在不兼容问题，限制了偏好信息的细粒度白盒蒸馏，特别是在更现实的跨分词器设置中。", "method": "提出CTPD框架，包含三个创新：对齐跨度投影（字符级映射）、跨分词器重要性采样适配（TIS-DPO）、教师锚定参考（DPO风格目标直接利用教师偏好）。", "result": "在多个基准测试中显示出显著性能提升，效果优于现有方法，证明了该方法的有效性。", "conclusion": "CTPD为不同分词方案间的偏好蒸馏提供了实用通用解决方案，为语言模型的可访问和高效对齐开辟了新途径。"}}
{"id": "2601.12141", "pdf": "https://arxiv.org/pdf/2601.12141", "abs": "https://arxiv.org/abs/2601.12141", "authors": ["Yuliia Suprun", "Khen Elimelech", "Lydia E. Kavraki", "Moshe Y. Vardi"], "title": "TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals", "categories": ["cs.AI"], "comment": null, "summary": "Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.", "AI": {"tldr": "TIDE是一种新的任务规划方法，通过将时序扩展目标分解为可管理的子问题，使用成本驱动启发式和自适应回溯机制来高效解决LTLf规划问题", "motivation": "传统LTLf任务规划方法缺乏针对时序目标的启发式引导搜索，导致规划效率低下", "method": "提出TIDE方法：将时序问题分解为序列化的可达-避免子问题，使用成本驱动启发式优先处理有前景的自动机轨迹，并采用自适应回溯机制处理失败计划", "result": "实验结果表明TIDE取得了有前景的性能表现", "conclusion": "TIDE是时序扩展目标规划方法组合中的一个有价值补充，能够确保完整性和效率"}}
{"id": "2601.11866", "pdf": "https://arxiv.org/pdf/2601.11866", "abs": "https://arxiv.org/abs/2601.11866", "authors": ["Kie Shidara", "Preethi Prem", "Jonathan Kim", "Anna Podlasek", "Feng Liu", "Ahmed Alaa", "Danilo Bernardo"], "title": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving", "categories": ["cs.CL"], "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.", "AI": {"tldr": "研究表明，强大的推理型大型语言模型在医学推理中表现出更好的认知灵活性，在mARC基准测试中达到人类水平表现，甚至在某些情况下比人类更少受到Einstellung效应的影响。", "motivation": "评估先进推理型LLMs在临床推理中的认知灵活性，特别是它们是否能够避免Einstellung效应导致的思维僵化问题。", "method": "使用医学抽象与推理语料库(mARC)这一对抗性医学QA基准测试，评估OpenAI、Grok、Gemini、Claude和DeepSeek等家族的推理模型，该基准利用Einstellung效应来诱导对学习到的启发式模式的过度依赖。", "result": "强推理模型比弱推理模型更频繁地避免了Einstellung陷阱，在mARC上达到人类水平表现。在最常被医生答错的问题上，前5名模型以高置信度正确回答了55%到70%的问题，表明这些模型可能比人类更不容易受到Einstellung效应的影响。", "conclusion": "强推理模型在医学推理中展现出改进的灵活性，在mARC上的表现与人类相当，表明这些模型在避免认知偏见方面具有潜在优势。"}}
{"id": "2601.12242", "pdf": "https://arxiv.org/pdf/2601.12242", "abs": "https://arxiv.org/abs/2601.12242", "authors": ["WooSeok Kim", "Jeonghoon Lee", "Sangho Kim", "Taesun An", "WonMin Lee", "Dowon Kim", "Kyungseop Shin"], "title": "Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.NI"], "comment": null, "summary": "In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.", "AI": {"tldr": "本文提出了一种结合回放记忆和on-policy算法的深度强化学习框架，用于优化NOMA系统的网络资源分配，并通过大量仿真评估了不同参数对性能的影响。", "motivation": "随着物联网(IoT)的发展导致网络资源稀缺，需要优化网络资源利用。NOMA系统通过功率复用允许多用户同时接入网络，但仍存在信道分配问题需要解决。", "method": "提出了一种深度强化学习框架，整合了回放记忆和on-policy算法，在NOMA系统中分配网络资源以实现学习泛化。", "result": "通过大量仿真评估了学习率、批次大小、模型类型和状态特征数量等参数变化对系统性能的影响。", "conclusion": "该深度强化学习框架能够有效解决NOMA系统中的信道分配问题，为网络资源优化提供了新的解决方案。"}}
{"id": "2601.11872", "pdf": "https://arxiv.org/pdf/2601.11872", "abs": "https://arxiv.org/abs/2601.11872", "authors": ["Nguyen Tien Phat", "Ngo Vu Minh", "Linh Van Ngo", "Nguyen Thi Ngoc Diep", "Thien Huu Nguyen"], "title": "GloCTM: Cross-Lingual Topic Modeling via a Global Context Space", "categories": ["cs.CL"], "comment": "AAAI 2026", "summary": "Cross-lingual topic modeling seeks to uncover coherent and semantically aligned topics across languages - a task central to multilingual understanding. Yet most existing models learn topics in disjoint, language-specific spaces and rely on alignment mechanisms (e.g., bilingual dictionaries) that often fail to capture deep cross-lingual semantics, resulting in loosely connected topic spaces. Moreover, these approaches often overlook the rich semantic signals embedded in multilingual pretrained representations, further limiting their ability to capture fine-grained alignment. We introduce GloCTM (Global Context Space for Cross-Lingual Topic Model), a novel framework that enforces cross-lingual topic alignment through a unified semantic space spanning the entire model pipeline. GloCTM constructs enriched input representations by expanding bag-of-words with cross-lingual lexical neighborhoods, and infers topic proportions using both local and global encoders, with their latent representations aligned through internal regularization. At the output level, the global topic-word distribution, defined over the combined vocabulary, structurally synchronizes topic meanings across languages. To further ground topics in deep semantic space, GloCTM incorporates a Centered Kernel Alignment (CKA) loss that aligns the latent topic space with multilingual contextual embeddings. Experiments across multiple benchmarks demonstrate that GloCTM significantly improves topic coherence and cross-lingual alignment, outperforming strong baselines.", "AI": {"tldr": "GloCTM是一个新的跨语言主题建模框架，通过统一的语义空间和多重对齐机制显著提升跨语言主题的一致性和语义对齐效果", "motivation": "现有跨语言主题模型存在语言空间分离、对齐机制效果有限、未能充分利用多语言预训练表示等问题，导致跨语言主题语义连接松散", "method": "构建统一语义空间，通过词汇邻域扩展输入表示，使用局部和全局编码器推断主题比例，内部正则化对齐潜在表示，在输出层定义全局主题-词分布，并引入CKA损失对齐潜在主题空间与多语言上下文嵌入", "result": "在多个基准测试中，GloCTM显著提高了主题一致性和跨语言对齐效果，优于强基线方法", "conclusion": "GloCTM通过端到端的统一语义空间设计和多重对齐机制，有效解决了跨语言主题建模中的语义对齐问题，为多语言理解提供了更强大的工具"}}
{"id": "2601.12256", "pdf": "https://arxiv.org/pdf/2601.12256", "abs": "https://arxiv.org/abs/2601.12256", "authors": ["Jinyoung Park", "Minseong Bae", "Jeehye Na", "Hyunwoo J. Kim"], "title": "Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.", "AI": {"tldr": "CoLLaMo是一个基于大语言模型的分子助手，通过多级分子模态协作投影器整合1D序列、2D分子图和3D构象信息，解决了现有分子语言模型的幻觉问题和鲁棒性限制，在多个分子任务上取得最佳性能。", "motivation": "现有分子语言模型(LMLMs)存在幻觉和鲁棒性有限的问题，主要原因是未能充分整合1D序列、2D分子图和3D构象等多种分子模态信息。", "method": "提出CoLLaMo模型，采用关系感知的模态协作注意力机制，促进原子间的细粒度和关系引导信息交换，整合2D结构和3D空间关系；同时提出新的分子中心自动评估指标，包括幻觉评估指标和基于GPT的标题质量评估。", "result": "实验表明CoLLaMo增强了LMLMs的分子模态泛化能力，在分子描述、计算性质QA、描述性质QA、基序计数和IUPAC名称预测等多个任务上取得最佳性能。", "conclusion": "通过多模态协作机制和新的评估框架，CoLLaMo有效解决了分子语言模型的幻觉问题，提升了分子理解和生成任务的性能，为分子AI领域提供了更可靠的解决方案。"}}
{"id": "2601.11886", "pdf": "https://arxiv.org/pdf/2601.11886", "abs": "https://arxiv.org/abs/2601.11886", "authors": ["Kaijie Mo", "Siddhartha Venkatayogi", "Chantal Shaib", "Ramez Kouzy", "Wei Xu", "Byron C. Wallace", "Junyi Jessy Li"], "title": "Faithfulness vs. Safety: Evaluating LLM Behavior Under Counterfactual Medical Evidence", "categories": ["cs.CL"], "comment": "26 pages", "summary": "In high-stakes domains like medicine, it may be generally desirable for models to faithfully adhere to the context provided. But what happens if the context does not align with model priors or safety protocols? In this paper, we investigate how LLMs behave and reason when presented with counterfactual or even adversarial medical evidence. We first construct MedCounterFact, a counterfactual medical QA dataset that requires the models to answer clinical comparison questions (i.e., judge the efficacy of certain treatments, with evidence consisting of randomized controlled trials provided as context). In MedCounterFact, real-world medical interventions within the questions and evidence are systematically replaced with four types of counterfactual stimuli, ranging from unknown words to toxic substances. Our evaluation across multiple frontier LLMs on MedCounterFact reveals that in the presence of counterfactual evidence, existing models overwhelmingly accept such \"evidence\" at face value even when it is dangerous or implausible, and provide confident and uncaveated answers. While it may be prudent to draw a boundary between faithfulness and safety, our findings reveal that there exists no such boundary yet.", "AI": {"tldr": "该论文研究了大型语言模型在面对与医学事实相悖或有害的对抗性证据时的行为，发现现有模型会盲目接受危险或不合理的证据并给出自信但错误的回答。", "motivation": "研究LLMs在医学等高风险领域中，当提供的上下文与模型先验知识或安全协议不一致时的行为和推理方式，特别是在面对反事实或对抗性医学证据时的表现。", "method": "构建MedCounterFact数据集，包含临床比较问题，将真实医学干预替换为四类反事实刺激（从未知词汇到有毒物质），并在多个前沿LLMs上进行评估。", "result": "在反事实证据存在时，现有模型普遍会表面接受这些危险或不可信的\"证据\"，并给出自信且无保留的答案，缺乏对安全性和忠实性的边界判断。", "conclusion": "虽然需要在忠实性和安全性之间划定界限，但目前的研究表明这样的边界尚不存在，模型在面对反事实医学证据时存在严重的安全风险。"}}
{"id": "2601.12259", "pdf": "https://arxiv.org/pdf/2601.12259", "abs": "https://arxiv.org/abs/2601.12259", "authors": ["Jiashuo Liu", "Siyuan Chen", "Zaiyuan Wang", "Zhiyuan Zeng", "Jiacheng Guo", "Liang Hu", "Lingyue Yin", "Suozhi Huang", "Wenxin Hao", "Yang Yang", "Zerui Cheng", "Zixin Yao", "Lingyue Yin", "Haoxin Liu", "Jiayi Cheng", "Yuzhen Li", "Zezhong Ma", "Bingjie Wang", "Bingsen Qiu", "Xiao Liu", "Zeyang Zhang", "Zijian Liu", "Jinpeng Wang", "Mingren Yin", "Tianci He", "Yali Liao", "Yixiao Tian", "Zhenwei Zhu", "Anqi Dai", "Ge Zhang", "Jingkai Liu", "Kaiyuan Zhang", "Wenlong Wu", "Xiang Gao", "Xinjie Chen", "Zhixin Yao", "Zhoufutu Wen", "B. Aditya Prakash", "Jose Blanchet", "Mengdi Wang", "Nian Si", "Wenhao Huang"], "title": "FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": "21 pages", "summary": "Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.", "AI": {"tldr": "FutureX-Pro是一个专门针对高价值垂直领域的未来预测框架，包含金融、零售、公共卫生和自然灾害四个关键领域，通过无污染实时评估管道评估当前SOTA代理LLM在工业部署中的表现。", "motivation": "虽然通用智能体在开放领域搜索中表现出色，但在资本密集和安全关键领域的可靠性尚未充分探索，需要专门针对高价值垂直领域进行预测能力评估。", "method": "基于FutureX的无污染实时评估管道，在四个垂直领域（金融、零售、公共卫生、自然灾害）对代理LLM进行基础预测任务基准测试，包括市场指标预测、供应链需求预测、流行病趋势跟踪和自然灾害追踪。", "result": "研究揭示了通用推理能力与高价值垂直应用所需精度之间存在性能差距。", "conclusion": "当前最先进的代理LLM在工业部署所需的领域基础方面仍存在不足，需要进一步改进才能满足高价值垂直领域的精确预测需求。"}}
{"id": "2601.11908", "pdf": "https://arxiv.org/pdf/2601.11908", "abs": "https://arxiv.org/abs/2601.11908", "authors": ["Byeongjin Kim", "Gyuwan Kim", "Seo Yeon Park"], "title": "PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning", "categories": ["cs.CL"], "comment": "23 pages, 6 figures", "summary": "Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.", "AI": {"tldr": "PPA-Plan是一种主动规划策略，通过识别潜在逻辑陷阱和错误假设，将其作为负面约束来指导规划生成，从而提升大语言模型在长上下文推理中的表现。", "motivation": "现有规划执行框架在处理长上下文推理时，由于依赖表层线索导致规划生成不可靠，形成基于错误假设的计划，且事后修正困难。", "method": "提出PPA-Plan方法：1）识别潜在逻辑陷阱和错误假设；2）将其表述为负面约束；3）在规划生成时明确避免这些约束。", "result": "在长上下文问答基准测试中，PPA-Plan生成的计划执行效果持续优于现有规划执行方法和直接提示方法。", "conclusion": "PPA-Plan通过主动预防规划错误，有效解决了长上下文推理中信息稀疏分布的问题，提升了规划可靠性和推理效果。"}}
{"id": "2601.12260", "pdf": "https://arxiv.org/pdf/2601.12260", "abs": "https://arxiv.org/abs/2601.12260", "authors": ["Yihao Ding", "Qiang Sun", "Puzhen Wu", "Sirui Li", "Siwen Luo", "Wei Liu"], "title": "Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding", "categories": ["cs.AI"], "comment": "Accepted at WWW 2026 Demo Track", "summary": "Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.", "AI": {"tldr": "Docs2Synth是一个合成监督框架，通过自动生成和验证QA对来训练视觉检索器，与MLLM协同工作，在低资源领域减少幻觉并提高响应一致性，无需人工标注。", "motivation": "在受监管领域中，扫描文档包含敏感、动态和领域特定知识，导致缺乏手动标注且预训练模型难以跟上领域特定事实更新。MLLM存在幻觉问题，而VLPM需要昂贵标注。", "method": "自动处理原始文档集合，通过基于代理的系统生成和验证多样化QA对，训练轻量级视觉检索器提取领域相关证据。推理时通过迭代检索-生成循环使检索器与MLLM协作。", "result": "在多个VRDU基准测试中，Docs2Synth显著增强了基础性和领域泛化能力，且不需要人工标注。", "conclusion": "Docs2Synth提供了一种有效的解决方案，通过合成监督和检索引导推理，在私有和低资源领域实现了可靠的文档理解，并提供了易用的Python包便于实际部署。"}}
{"id": "2601.11913", "pdf": "https://arxiv.org/pdf/2601.11913", "abs": "https://arxiv.org/abs/2601.11913", "authors": ["Yichen Jiang", "Peng Ye", "Jiakang Yuan", "Chongjun Tu", "Lei Bai", "Tao Chen"], "title": "LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.", "AI": {"tldr": "LSTM-MAS是一个受LSTM架构启发的多智能体系统，通过模拟LSTM的门控机制和分层信息流来解决大语言模型处理长上下文的问题，在多个基准测试中显著优于现有方法", "motivation": "现有单LLM方法在处理长上下文时面临计算成本高和扩展长度受限的问题，而多智能体框架又容易受到错误累积和幻觉传播的影响", "method": "设计链式架构的多智能体系统，每个节点包含工作智能体（段级理解）、过滤智能体（冗余减少）、判断智能体（错误检测）和管理智能体（信息调控），模拟LSTM的输入门、遗忘门、恒定误差循环单元和输出门", "result": "相比之前最佳多智能体方法CoA，在NarrativeQA、Qasper、HotpotQA和MuSiQue上分别提升了40.93%、43.70%、121.57%和33.12%", "conclusion": "LSTM-MAS通过受控信息传递和选择性长程依赖建模，有效避免了错误累积和幻觉传播，为长上下文理解提供了有效的解决方案"}}
{"id": "2601.12294", "pdf": "https://arxiv.org/pdf/2601.12294", "abs": "https://arxiv.org/abs/2601.12294", "authors": ["Dawei Li", "Yuguang Yao", "Zhen Tan", "Huan Liu", "Ruocheng Guo"], "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "categories": ["cs.AI", "cs.SE"], "comment": "under review", "summary": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "AI": {"tldr": "论文提出了ToolPRMBench，一个专门评估工具使用智能体中过程奖励模型(PRMs)的大规模基准测试，通过离线/在线采样和多LLM验证流程来系统评估PRMs在工具使用场景中的表现。", "motivation": "当前缺乏对工具使用智能体中过程奖励模型(PRMs)的系统性和可靠性评估基准，需要建立一个专门的测试平台来评估PRMs在复杂动作空间中的指导能力。", "method": "基于多个代表性工具使用基准构建ToolPRMBench，将智能体轨迹转换为步级测试用例；使用离线采样隔离单步错误和在线采样捕获多步失败；采用多LLM验证流程减少标签噪声。", "result": "实验结果显示不同PRMs在工具使用场景中效果存在明显差异，专门针对工具使用的PRMs表现出更大潜力。", "conclusion": "ToolPRMBench为PRMs评估提供了可靠基准，证明了专门化PRMs在工具使用场景中的优势，相关代码和数据将开源。"}}
{"id": "2601.11920", "pdf": "https://arxiv.org/pdf/2601.11920", "abs": "https://arxiv.org/abs/2601.11920", "authors": ["Zhen Xu", "Vedant Khatri", "Yijun Dai", "Xiner Liu", "Siyan Li", "Xuanming Zhang", "Renzhe Yu"], "title": "Enhancing LLM-Based Data Annotation with Error Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.", "AI": {"tldr": "本文提出了一种诊断性评估范式，通过人机协作分离任务固有模糊性和模型驱动的不准确性，并开发了分类法来分解LLM在主观标注任务中的错误类型。", "motivation": "LLM在主观标注任务（如心理建构）中表现不稳定，标准评估方法将所有错误合并为单一对齐指标，无法区分不同错误类型对分析结论的影响。", "method": "提出诊断评估范式：1）二维错误分类法（来源：模型特定vs任务固有；类型：边界模糊vs概念误判）；2）轻量级人工标注测试估计任务固有模糊性；3）计算方法按分类法分解观察到的错误。", "result": "在四个教育标注任务上验证了该范式的概念有效性和实用价值，证明过高对齐在某些任务中不现实，单一对齐指标不足以反映LLM标注质量。", "conclusion": "该范式可作为低成本诊断工具，评估任务是否适合LLM标注，并为技术优化提供可操作的见解，理论上解释了为什么在某些任务中追求过高对齐不切实际。"}}
{"id": "2601.12310", "pdf": "https://arxiv.org/pdf/2601.12310", "abs": "https://arxiv.org/abs/2601.12310", "authors": ["Jennifer Dodgson", "Alfath Daryl Alhajir", "Michael Joedhitya", "Akira Rafhael Janson Pattirane", "Surender Suresh Kumar", "Joseph Lim", "C. H. Peh", "Adith Ramdas", "Steven Zhang Zhexu"], "title": "Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection", "categories": ["cs.AI"], "comment": null, "summary": "Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.\n  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.\n  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.", "AI": {"tldr": "论文提出了基于环境生存能力而非奖励的自训练架构，通过行为的环境持久性和未来交互可能性进行选择，避免了奖励黑客问题，实现了稳定的开放式自我改进。", "motivation": "解决自训练系统因缺乏外部数据质量判断标准而导致的奖励黑客和语义漂移问题，探索在稀疏外部反馈和有限内存下的稳定自训练方法。", "method": "引入基于环境生存能力的自训练架构，通过候选行为在真实资源约束下的执行效果进行选择，只保留那些环境效果持久且保持未来交互可能性的行为。", "result": "分析显示系统通过有效可重复策略的持久性实现改进，发展出元学习策略（如故意实验失败以获取信息），证明了环境基础选择能够实现可持续的开放式自我改进。", "conclusion": "环境基础的选择机制为无需人类策划数据或复杂奖励塑造的、更鲁棒和可泛化的自主系统提供了可行路径，解决了自训练中的根本稳定性问题。"}}
{"id": "2601.11923", "pdf": "https://arxiv.org/pdf/2601.11923", "abs": "https://arxiv.org/abs/2601.11923", "authors": ["P. Bilha Githinji", "Aikaterini Melliou", "Xi Yuan", "Dayan Zhang", "Lian Zhang", "Zhenglin Chen", "Jiansong Ji", "Chengying Lv", "Jinhao Xu", "Peiwu Qin", "Dongmei Yu"], "title": "Mapping the maturation of TCM as an adjuvant to radiotherapy", "categories": ["cs.CL"], "comment": null, "summary": "The integration of complementary medicine into oncology represents a paradigm shift that has seen to increasing adoption of Traditional Chinese Medicine (TCM) as an adjuvant to radiotherapy. About twenty-five years since the formal institutionalization of integrated oncology, it is opportune to synthesize the trajectory of evidence for TCM as an adjuvant to radiotherapy. Here we conduct a large-scale analysis of 69,745 publications (2000 - 2025), emerging a cyclical evolution defined by coordinated expansion and contraction in publication output, international collaboration, and funding commitments that mirrors a define-ideate-test pattern. Using a theme modeling workflow designed to determine a stable thematic structure of the field, we identify five dominant thematic axes - cancer types, supportive care, clinical endpoints, mechanisms, and methodology - that signal a focus on patient well-being, scientific rigor and mechanistic exploration. Cross-theme integration of TCM is patient-centered and systems-oriented. Together with the emergent cycles of evolution, the thematic structure demonstrates progressive specialization and potential defragmentation of the field or saturation of existing research agenda. The analysis points to a field that has matured its current research agenda and is likely at the cusp of something new. Additionally, the field exhibits positive reporting of findings that is homogeneous across publication types, thematic areas, and the cycles of evolution suggesting a system-wide positive reporting bias agnostic to structural drivers.", "AI": {"tldr": "对69,745篇文献的大规模分析显示，中医药作为放疗辅助疗法在肿瘤学整合中经历了周期性发展，形成了五个主导主题轴，表明该领域已成熟并可能面临新的研究方向，同时存在系统性正向报告偏倚。", "motivation": "中医药作为放疗辅助疗法在肿瘤整合医学中日益重要，需要系统梳理25年来的证据发展轨迹，了解该领域的演变模式和现状。", "method": "使用主题建模工作流程分析2000-2025年间69,745篇出版物，识别稳定的主题结构，分析出版产出、国际合作和资金投入的周期性变化。", "result": "识别出五个主导主题轴（癌症类型、支持性护理、临床终点、机制和方法学），显示以患者为中心、系统导向的整合模式，呈现周期性发展模式（定义-构思-测试模式）。", "conclusion": "该领域当前研究议程已成熟，可能正处于新突破的边缘，但存在系统性正向报告偏倚，需要关注研究质量的提升和新方向探索。"}}
{"id": "2601.12318", "pdf": "https://arxiv.org/pdf/2601.12318", "abs": "https://arxiv.org/abs/2601.12318", "authors": ["Dehao Ying", "Fengchang Yu", "Haihua Chen", "Changjiang Jiang", "Yurong Li", "Wei Lu"], "title": "Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the \"availability of data and labels.\" This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.", "AI": {"tldr": "这篇论文提出了文档智能领域首个全面的数据生成技术图谱，重新定义数据生成为监督信号生成，并基于数据和标签可用性建立了四类资源中心范式，同时构建了多层次的评估框架来系统化这一分散的研究领域。", "motivation": "文档智能需要大规模高质量训练数据，但人工标注成为关键瓶颈。现有研究局限于单一模态或特定任务，缺乏与现实工作流程统一的技术视角。", "method": "建立首个全面的数据生成技术图谱，重新定义数据生成为监督信号生成，基于数据和标签可用性提出四类范式：数据增强、从零生成数据、自动数据标注和自监督信号构建。", "result": "提出了统一的技术分类框架和多层次评估体系，整合了内在质量和外在效用，并整理了在多个文档智能基准测试中的性能提升数据。", "conclusion": "通过系统化这一分散领域，将数据生成定位为下一代文档智能的核心引擎，揭示了保真度差距等关键挑战和协同进化生态系统等前沿方向。"}}
{"id": "2601.11932", "pdf": "https://arxiv.org/pdf/2601.11932", "abs": "https://arxiv.org/abs/2601.11932", "authors": ["Abdullah Al Monsur", "Nitesh Vamshi Bommisetty", "Gene Louis Kim"], "title": "Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes", "categories": ["cs.CL"], "comment": null, "summary": "The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.", "AI": {"tldr": "本研究针对事件检测研究的两个主要限制：解码器LLM的单向性架构瓶颈和Micro-F1评分偏向多数类的问题，提出使用句子上下文增强和LoRA微调方法，显著提升模型在长尾事件类型上的Macro-F1表现。", "motivation": "解决事件检测研究中解码器LLM的单向性架构限制和Micro-F1评分系统偏向多数类的问题，寻求更公平的性能评估方法。", "method": "使用句子上下文增强模型，并在微调过程中采用低秩适应（LoRA）技术，重点关注Macro-F1作为性能评估指标。", "result": "实验表明，通过句子上下文增强的模型性能优于标准解码器基线，LoRA微调特别显著提升了解码器模型在Macro-F1分数上的表现，尤其是在长尾事件类别上。", "conclusion": "LoRA是增强LLM在长尾事件类别上性能的有效工具，Macro-F1比Micro-F1更能代表模型在多样化事件类型上的真实能力。"}}
{"id": "2601.12323", "pdf": "https://arxiv.org/pdf/2601.12323", "abs": "https://arxiv.org/abs/2601.12323", "authors": ["Yin Cai", "Zhouhong Gu", "Juntao Zhang", "Ping Chen"], "title": "MARO: Learning Stronger Reasoning from Social Interaction", "categories": ["cs.AI"], "comment": null, "summary": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.", "AI": {"tldr": "MARO方法通过多智能体社交环境训练LLM，解决稀疏奖励、角色分布不均和环境不稳定问题，显著提升推理能力并实现能力迁移。", "motivation": "现有LLM训练方法缺乏真实社交场景中的互动、协商和竞争经验，无法应对需要复杂推理和判断的现实场景。", "method": "提出MARO方法：1) 将最终成败结果分解为具体行为信号；2) 平衡不同角色的训练样本权重；3) 直接评估每个行为的效用值。", "result": "实验显示MARO显著提升社交推理能力，且学习到的能力可有效迁移到数学推理和指令遵循等其他任务。", "conclusion": "多智能体社交学习在提升LLM通用推理能力方面具有巨大潜力，MARO为此提供了有效解决方案。"}}
{"id": "2601.11956", "pdf": "https://arxiv.org/pdf/2601.11956", "abs": "https://arxiv.org/abs/2601.11956", "authors": ["Yuyin Lu", "Ziran Liang", "Yanghui Rao", "Wenqi Fan", "Fu Lee Wang", "Qing Li"], "title": "Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.", "AI": {"tldr": "DoublyCal框架通过双重校准机制，使用轻量级代理模型生成知识图谱证据和校准的置信度，指导黑盒LLM产生更准确且置信度可追溯的预测", "motivation": "现有基于知识图谱增强的LLM方法无法量化检索证据和LLM推理中的认知不确定性，导致可信推理存在挑战", "method": "基于双重校准原则，使用轻量级代理模型首先生成KG证据和校准的置信度，然后用校准后的证据指导黑盒LLM进行推理", "result": "在知识密集型基准测试中显著提高了黑盒LLM的准确性和置信度校准，且token成本较低", "conclusion": "DoublyCal框架通过量化证据和推理的不确定性，有效解决了LLM的可信推理问题，为KG增强的LLM提供了可靠的置信度评估机制"}}
{"id": "2601.12338", "pdf": "https://arxiv.org/pdf/2601.12338", "abs": "https://arxiv.org/abs/2601.12338", "authors": ["Kartikey Singh Bhandari", "Manav Ganesh", "Yashwant Viswanathan", "Archit Agrawal", "Dhruv Kumar", "Pratik Narang"], "title": "Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations", "categories": ["cs.AI"], "comment": null, "summary": "Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.", "AI": {"tldr": "该论文提出了一个基于双LLM的框架，从客户评论中提取问题并生成可操作的建议，使用LoRA专家混合策略提升建议质量。", "motivation": "客户评论包含丰富的服务失败和用户期望信息，但将这些非结构化反馈转化为可执行的商业决策仍然困难。", "method": "采用模块化双LLM框架：问题模型提取关键问题并分配主题，建议模型基于问题表示生成针对性操作建议。使用LoRA专家混合策略，训练多个低秩适配器并通过轻量门控机制在推理时进行专家混合。", "result": "在航空和餐厅领域的Yelp评论数据集上，该方法在可操作性、特异性等八个维度上均优于仅提示和单适配器基线。", "conclusion": "该方法能有效将客户评论转化为具体可执行的建议，在保持效率-质量平衡的同时显著提升建议质量。"}}
{"id": "2601.11957", "pdf": "https://arxiv.org/pdf/2601.11957", "abs": "https://arxiv.org/abs/2601.11957", "authors": ["Bingxuan Li", "Jeonghwan Kim", "Cheng Qian", "Xiusi Chen", "Eitan Anzenberg", "Niran Kundapur", "Heng Ji"], "title": "PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.", "AI": {"tldr": "论文提出了CalConflictBench基准测试和PEARL强化学习框架，用于解决日历冲突自动化问题。当前LLM代理表现不佳（35%错误率），而PEARL实现了76%的错误减少率和55%的平均错误率改进。", "motivation": "繁忙专业人士需要反复决定参加、重新安排或拒绝重叠的日历邀请，人工委托在规模化时经常失败，需要自动化解决方案来管理时间冲突。", "method": "引入CalConflictBench基准测试进行系统性研究，提出PEARL框架：通过强化学习增强语言代理，使用外部记忆模块和优化的轮次奖励设计，使代理能够逐步推断和适应用户偏好。", "result": "当前LLM代理表现差（如Qwen-3-30B-Think有35%平均错误率），而PEARL实现了0.76的错误减少率和55%的平均错误率改进。", "conclusion": "PEARL框架能有效解决日历冲突问题，显著提升自动化决策的准确性，证明强化学习增强的语言代理在时间管理任务中具有重要价值。"}}
{"id": "2601.12392", "pdf": "https://arxiv.org/pdf/2601.12392", "abs": "https://arxiv.org/abs/2601.12392", "authors": ["Zhentao Xia", "Yongqi Fan", "Yuxiang Chu", "Yichao Yin", "Liangliang Chen", "Tong Ruan", "Weiyan Zhang"], "title": "PsychēChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychēChat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychēChat outperforms existing methods for emotional insight and safety control.", "AI": {"tldr": "PsychēChat是一个专门为心理咨询设计的LLM系统，通过显式建模用户情绪变化和安全风险分析来提升心理咨询效果，采用多智能体协作和端到端推理两种模式。", "motivation": "现有语言模型在心理咨询中未显式建模用户情绪变化，且缺乏与情绪变化对齐的安全风险控制机制，需要填补这一研究空白。", "method": "采用交互式角色扮演生成咨询对话，包含情绪管理模块（捕捉当前情绪和情绪变化）和风险控制模块（预测用户反应和识别风险），提供智能体模式和LLM模式两种建模范式。", "result": "通过交互评分、对话级评估和人工评估等广泛实验，证明PsychēChat在情感洞察和安全控制方面优于现有方法。", "conclusion": "PsychēChat成功整合了情绪变化追踪和安全风险分析，为心理咨询LLM提供了有效的解决方案，在情感理解安全性方面表现出色。"}}
{"id": "2601.11969", "pdf": "https://arxiv.org/pdf/2601.11969", "abs": "https://arxiv.org/abs/2601.11969", "authors": ["Zecheng Tang", "Baibei Ji", "Ruoxi Sun", "Haitian Wang", "WangJie You", "Zhang Yijun", "Wenpeng Zhu", "Ji Qi", "Juntao Li", "Min Zhang"], "title": "$\\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.", "AI": {"tldr": "MemoryRewardBench是首个系统评估奖励模型在长文本记忆管理能力的基准测试，涵盖10种不同设置，文本长度从8K到128K token，评估显示开源与专有模型性能差距缩小，新一代模型表现更优。", "motivation": "现有工作越来越多采用以内存为中心的机制分段处理长文本，有效的内存管理是大型语言模型在整个序列中传播信息的关键能力，因此需要利用奖励模型自动可靠地评估内存质量。", "method": "引入MemoryRewardBench基准测试，覆盖长文本理解和长文本生成任务，包含10种不同内存管理模式的设置，对13个前沿奖励模型进行评估。", "result": "评估显示开源模型与专有模型之间的性能差距正在缩小，新一代模型无论参数数量如何都持续优于前代模型。", "conclusion": "研究揭示了当前奖励模型在评估LLM内存管理方面的能力和基本局限性，为未来改进提供了重要见解。"}}
{"id": "2601.12410", "pdf": "https://arxiv.org/pdf/2601.12410", "abs": "https://arxiv.org/abs/2601.12410", "authors": ["Dingyi Yang", "Junqi Zhao", "Xue Li", "Ce Li", "Boyang Li"], "title": "Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation", "categories": ["cs.AI"], "comment": "23 pages, 11 figures", "summary": "Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.", "AI": {"tldr": "论文评估大语言模型在知识状态追踪和意图理解方面的能力，通过两个任务测试发现当前最先进的大语言模型表现接近随机水平，远逊于人类", "motivation": "基于认知人类学观点，人类智能的独特之处在于推断他人知识状态和理解意图的能力，而黑猩猩等近亲动物缺乏这种能力。研究旨在评估大语言模型在此领域的表现", "method": "设计两个任务：(1)检测故事角色是否通过行动展示了不应拥有的知识；(2)预测故事角色基于自身知识（而非未知的客观事实）的下一步行动", "result": "结果显示当前最先进的大语言模型在两个任务上都达到接近随机的性能水平，表现显著低于人类", "conclusion": "未来大语言模型研究应更加重视知识估计和意图理解能力的提升"}}
{"id": "2601.12019", "pdf": "https://arxiv.org/pdf/2601.12019", "abs": "https://arxiv.org/abs/2601.12019", "authors": ["Chaowei Zhang", "Xiansheng Luo", "Zewei Zhang", "Yi Zhu", "Jipeng Qiang", "Longwei Wang"], "title": "Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.", "AI": {"tldr": "本文提出SORG框架，利用LLMs的奉承行为生成对立立场推理，并开发ORCD模型进行点击诱饵检测，在三个基准数据集上表现优于现有方法。", "motivation": "在线内容泛滥加剧了点击诱饵问题，LLMs虽有潜力但受奉承行为影响，倾向于产生符合用户信念而非真实的推理。", "method": "设计SORG框架让LLMs生成对立立场的推理对，开发ORCD模型集成三个BERT编码器表示标题和推理，使用对比学习和LLM生成的可信度软标签。", "result": "在三个基准数据集上的实验表明，该方法持续优于LLM提示、微调小语言模型和最先进的点击诱饵检测基线。", "conclusion": "通过利用而非消除LLMs的奉承行为来生成对立推理，可以显著提升点击诱饵检测的性能和鲁棒性。"}}
{"id": "2601.12444", "pdf": "https://arxiv.org/pdf/2601.12444", "abs": "https://arxiv.org/abs/2601.12444", "authors": ["Hui Yang", "Jiaoyan Chen", "Uli Sattler"], "title": "Large Language Model for OWL Proofs", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.", "AI": {"tldr": "该研究探讨了大型语言模型在OWL本体论中生成逻辑证明的能力，开发了自动化数据集构建和评估框架，发现逻辑复杂性是影响性能的主要因素，而非表示格式，同时噪声和不完整数据会显著降低模型表现。", "motivation": "虽然LLMs在推理任务上的能力已被广泛研究，但它们在生成忠实、可读的证明解释方面的能力仍未充分探索，特别是在OWL本体论这种复杂知识表示和推理的背景下。", "method": "开发了自动化数据集构建和评估框架，包含三个顺序任务：提取、简化和解释，以及评估前提逻辑完整性的额外任务。在广泛使用的推理LLMs上进行了大量实验。", "result": "(1) 某些模型整体表现强劲但在复杂案例中仍有局限；(2) 逻辑复杂性而非表示格式是影响LLM性能的主导因素；(3) 输入数据中的噪声和不完整性显著降低LLM性能。", "conclusion": "研究结果强调了LLMs在严格逻辑解释方面的潜力，以及在复杂或不完美条件下支持弹性推理方面存在的差距。"}}
{"id": "2601.12033", "pdf": "https://arxiv.org/pdf/2601.12033", "abs": "https://arxiv.org/abs/2601.12033", "authors": ["Muhammad Alif Al Hakim", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection", "categories": ["cs.CL"], "comment": null, "summary": "Quantization is widely adopted to reduce the computational cost of large language models (LLMs); however, its implications for fairness and safety, particularly in dynamic quantization and multilingual contexts, remain underexplored. In this work, we conduct a systematic study of how static and dynamic quantization methods impact fairness and safety across benchmarks measuring intrinsic and extrinsic bias and safety alignment. For fairness, we evaluate English, French, Dutch, Spanish, and Turkish; for safety, we focus on English, Korean, and Arabic. Our findings reveal that quantization consistently degrades fairness and safety, with dynamic methods demonstrating greater stability than static ones. Moreover, fairness degradation varies across languages, while safety deterioration is especially pronounced in non-English settings. To address these risks, we introduce Critical Weight Protection, a novel technique that identifies and preserves fairness- and safety-critical weights during quantization. This approach effectively mitigates bias and safety deterioration without costly retraining or alignment, maintaining trustworthiness while retaining efficiency.", "AI": {"tldr": "量化会降低大语言模型的公平性和安全性，动态量化比静态量化更稳定，非英语环境下安全性下降更明显。作者提出Critical Weight Protection技术来保护关键权重，无需重新训练即可缓解这些问题。", "motivation": "量化被广泛用于降低大语言模型的计算成本，但其对公平性和安全性的影响，特别是在动态量化和多语言环境中的影响尚未得到充分研究。", "method": "系统研究静态和动态量化方法对公平性和安全性的影响，使用测量内在和外在偏见及安全对齐的基准测试，评估英语、法语、荷兰语、西班牙语、土耳其语等语言的公平性，以及英语、韩语、阿拉伯语的安全性。", "result": "量化持续降低公平性和安全性，动态方法比静态方法更稳定。公平性退化在不同语言间存在差异，安全性恶化在非英语环境中尤其明显。", "conclusion": "提出Critical Weight Protection技术，通过在量化过程中识别和保护公平性和安全性关键权重，有效缓解偏见和安全性恶化问题，无需昂贵的重新训练或对齐，在保持效率的同时维护可信度。"}}
{"id": "2601.12499", "pdf": "https://arxiv.org/pdf/2601.12499", "abs": "https://arxiv.org/abs/2601.12499", "authors": ["Meiru Zhang", "Zaiqiao Meng", "Nigel Collier"], "title": "Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck", "categories": ["cs.AI", "cs.LG"], "comment": "preprint", "summary": "Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the \"Weakest Link Law\": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that \"thinking\" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.", "AI": {"tldr": "大型语言模型在多跳推理中存在位置偏见问题，MFAI方法通过引导注意力来识别识别失败和整合失败机制，发现多跳推理性能受最不可见证据限制，且失败由绝对位置而非距离决定。系统2推理模型能有效定位和整合信息。", "motivation": "尽管大型语言模型具备大规模上下文窗口，但由于固有的位置偏见，在多跳推理中会忽略某些位置的信息，需要明确识别是定位证据失败还是整合信息失败的问题。", "method": "引入多焦点注意力指令(MFAI)作为语义探针，通过显式引导注意力到选定位置来区分识别和整合机制，在MuSiQue和NeoQA两个多跳QA任务上测试5个LLM。", "result": "建立了\"最弱环节定律\"：多跳推理性能崩溃到最不可见证据的性能水平；失败由绝对位置而非线性距离决定(性能差异<3%)；匹配的MFAI可解决识别瓶颈，在低可见性位置提高准确率11.5%；系统2推理模型能有效处理噪声长上下文。", "conclusion": "多跳推理失败主要由位置偏见导致而非距离因素，通过注意力引导可改善识别问题，系统2推理模型在复杂场景中表现出色，为改进LLM推理能力提供了重要见解。"}}
{"id": "2601.12034", "pdf": "https://arxiv.org/pdf/2601.12034", "abs": "https://arxiv.org/abs/2601.12034", "authors": ["Ziyi Zhao", "Chongming Gao", "Yang Zhang", "Haoyan Liu", "Weinan Gan", "Huifeng Guo", "Yong Liu", "Fuli Feng"], "title": "Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to AAAI 2026 (Oral). 9 pages, 5 figures", "summary": "Personalization in Large Language Models (LLMs) often relies on user-specific soft prompts. However, these prompts become obsolete when the foundation model is upgraded, necessitating costly, full-scale retraining. To overcome this limitation, we propose the Prompt-level User Migration Adapter (PUMA), a lightweight framework to efficiently migrate personalized prompts across incompatible models. PUMA utilizes a parameter-efficient adapter to bridge the semantic gap, combined with a group-based user selection strategy to significantly reduce training costs. Experiments on three large-scale datasets show our method matches or even surpasses the performance of retraining from scratch, reducing computational cost by up to 98%. The framework demonstrates strong generalization across diverse model architectures and robustness in advanced scenarios like chained and aggregated migrations, offering a practical path for the sustainable evolution of personalized AI by decoupling user assets from the underlying models.", "AI": {"tldr": "PUMA框架通过轻量级适配器解决LLM个性化提示在模型升级时的迁移问题，无需全量重训练，大幅降低计算成本", "motivation": "大型语言模型个性化依赖用户特定软提示，但基础模型升级时这些提示会失效，需要昂贵的全量重训练", "method": "提出Prompt-level User Migration Adapter (PUMA)，使用参数高效适配器桥接语义差距，结合基于组的用户选择策略降低训练成本", "result": "在三个大规模数据集上实验显示，性能达到或超过从头训练的效果，计算成本降低高达98%，在不同模型架构和链式/聚合迁移场景中表现稳健", "conclusion": "PUMA为个性化AI的可持续发展提供了实用路径，通过将用户资产与底层模型解耦来实现高效迁移"}}
{"id": "2601.12538", "pdf": "https://arxiv.org/pdf/2601.12538", "abs": "https://arxiv.org/abs/2601.12538", "authors": ["Tianxin Wei", "Ting-Wei Li", "Zhining Liu", "Xuying Ning", "Ze Yang", "Jiaru Zou", "Zhichen Zeng", "Ruizhong Qiu", "Xiao Lin", "Dongqi Fu", "Zihao Li", "Mengting Ai", "Duo Zhou", "Wenxuan Bao", "Yunzhe Li", "Gaotang Li", "Cheng Qian", "Yu Wang", "Xiangru Tang", "Yin Xiao", "Liri Fang", "Hui Liu", "Xianfeng Tang", "Yuji Zhang", "Chi Wang", "Jiaxuan You", "Heng Ji", "Hanghang Tong", "Jingrui He"], "title": "Agentic Reasoning for Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "AI": {"tldr": "这篇调查论文提出了代理推理的三维框架，将大语言模型重新构建为能够规划、行动和学习的自主代理，以解决开放动态环境中的推理挑战。", "motivation": "大语言模型在封闭环境中表现出强大的推理能力，但在开放和动态环境中表现不佳，需要新的范式来提升其在实际应用中的推理能力。", "method": "通过三个互补维度组织代理推理：基础代理推理（单代理能力）、自我进化代理推理（通过反馈和适应优化能力）、集体多代理推理（协作环境中的协调与知识共享）。区分上下文推理和训练后推理两种方法。", "result": "论文综述了代理推理在科学、机器人、医疗、自主研究和数学等实际应用中的代表性框架和基准测试，提供了一个统一的路线图。", "conclusion": "代理推理架起了思维与行动之间的桥梁，但还需要解决个性化、长时程交互、世界建模、可扩展多代理训练和实际部署治理等开放挑战。"}}
{"id": "2601.12061", "pdf": "https://arxiv.org/pdf/2601.12061", "abs": "https://arxiv.org/abs/2601.12061", "authors": ["Jinsook Lee", "Kirk Vanacore", "Zhuqian Zhou", "Jeanine Grutter", "Rene F. Kizilcec"], "title": "Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review for ACL 2026", "summary": "Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.", "AI": {"tldr": "论文提出代码本注入分割方法，通过将分割决策与下游标注标准结合，评估LLM分割器在对话行为标注中的表现，发现不同分割器各有优劣，没有单一最优方案。", "motivation": "传统对话行为标注将交际意图局限于单个话语，导致标注者在边界划分上存在分歧，降低了标注可靠性。", "method": "提出代码本注入分割方法，基于下游标注标准进行边界决策，评估LLM分割器与标准和检索增强基线的性能对比。", "result": "DA感知分割器产生比纯文本基线更一致的片段，但基于连贯性的基线在检测对话流全局变化方面更优；不同分割器在不同数据集上表现各异。", "conclusion": "分割是一个重要的设计选择，应根据下游目标进行优化，而非追求单一性能分数；片段内连贯性的提升常以边界区分度和人机分布一致性为代价。"}}
{"id": "2601.12539", "pdf": "https://arxiv.org/pdf/2601.12539", "abs": "https://arxiv.org/abs/2601.12539", "authors": ["Ali Ezzat Shahroor", "Mohamed Bayan Kmainasi", "Abul Hasnat", "Dimitar Dimitrov", "Giovanni Da San Martino", "Preslav Nakov", "Firoj Alam"], "title": "MemeLens: Multilingual Multitask VLMs for Memes", "categories": ["cs.AI", "cs.CL"], "comment": "disinformation, misinformation, factuality, harmfulness, fake news, propaganda, hateful meme, multimodality, text, images", "summary": "Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.", "AI": {"tldr": "MemeLens是一个统一的多语言多任务解释增强视觉语言模型，用于表情包理解，整合了38个公共数据集，将标签映射到20个任务的共享分类法中，发现多模态训练对于鲁棒的表情包理解至关重要", "motivation": "现有表情包研究分散在不同任务和语言中，限制了跨领域泛化能力，需要统一的多语言多任务框架来解决这一差距", "method": "整合38个公共表情包数据集，过滤并将特定数据集标签映射到包含20个任务的共享分类法中，开发多语言多任务解释增强视觉语言模型MemeLens", "result": "研究发现鲁棒的表情包理解需要多模态训练，在语义类别间存在显著差异，当模型在单个数据集上微调而非统一训练时对过度专业化敏感", "conclusion": "多模态统一训练对于表情包理解至关重要，提供了实验资源和数据集供社区使用，推动了表情包研究的跨领域泛化能力"}}
{"id": "2601.12068", "pdf": "https://arxiv.org/pdf/2601.12068", "abs": "https://arxiv.org/abs/2601.12068", "authors": ["Rowzatul Zannat", "Abdullah Al Shafi", "Abdul Muntakim"], "title": "Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.", "AI": {"tldr": "该研究开发了一个全面的孟加拉语症状-疾病数据集，包含758个症状-疾病关系和85种疾病，用于基于孟加拉语症状输入预测疾病。通过评估多种机器学习模型，集成学习方法达到了98%的准确率。", "motivation": "为非英语人群提供可靠的健康信息获取途径，特别是针对孟加拉语人群的疾病预测资源有限的问题。", "method": "开发包含758个症状-疾病关系和85种疾病的孟加拉语数据集，并使用多种机器学习模型进行疾病预测评估，采用软投票和硬投票集成方法。", "result": "集成学习方法在疾病预测任务中达到了98%的准确率，表现出优越的鲁棒性和泛化能力。", "conclusion": "该研究为孟加拉语疾病预测建立了基础资源，有助于提升孟加拉语人群在健康信息获取和早期疾病检测方面的公平性，为本地化健康信息学和诊断工具的发展铺平道路。"}}
{"id": "2601.12542", "pdf": "https://arxiv.org/pdf/2601.12542", "abs": "https://arxiv.org/abs/2601.12542", "authors": ["Lukas Weidener", "Marko Brkić", "Mihailo Jovanović", "Ritvik Singh", "Chiara Baccin", "Emre Ulgac", "Alex Dobrin", "Aakaash Meduri"], "title": "Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery", "categories": ["cs.AI"], "comment": null, "summary": "Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.", "AI": {"tldr": "Deep Research是一个多智能体系统，能在几分钟内完成交互式科学研究，支持半自动和全自动两种工作模式，在计算生物学基准测试中表现优于现有基线14-26个百分点。", "motivation": "现有AI科学发现系统多为专有系统且采用批处理模式，每个研究周期需要数小时，无法实现研究者的实时指导。", "method": "采用多智能体架构，包含规划、数据分析、文献搜索和新颖性检测等专门智能体，通过持久世界状态保持跨迭代研究周期的上下文。", "result": "在BixBench计算生物学基准测试中达到48.8%的开放回答准确率和64.5%的多项选择准确率，显著超越现有基线。", "conclusion": "该系统展示了AI辅助科学工作流程的实际应用前景，同时分析了开放获取文献限制和自动化新颖性评估等架构约束对实际部署的影响。"}}
{"id": "2601.12075", "pdf": "https://arxiv.org/pdf/2601.12075", "abs": "https://arxiv.org/abs/2601.12075", "authors": ["Mehrdad Farahani", "Franziska Penzkofer", "Richard Johansson"], "title": "To Copy or Not to Copy: Copying Is Easier to Induce Than Recall", "categories": ["cs.CL"], "comment": null, "summary": "Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \\emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\\rightarrow$Recall (suppressing context use) and Recall$\\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.", "AI": {"tldr": "该研究通过提取\"仲裁向量\"来机制性分析语言模型在检索增强设置中如何在参数知识和上下文信息之间进行选择，发现诱导复制行为比恢复参数召回更容易实现。", "motivation": "研究语言模型在检索增强环境中如何权衡使用参数知识（存储在权重中）和上下文信息（来自提示），需要理解模型在这两种知识源之间的仲裁机制。", "method": "从模型激活中提取仲裁向量，通过比较不相关上下文（引发参数召回）和相关但错误上下文（引发复制）的残差流质心差异，在选定层和标记跨度进行加性干预。", "result": "实验显示在两个架构和两个开放域QA基准测试中都观察到一致的行为转变，机制分析揭示诱导复制是容易的\"再激活\"过程，而恢复召回是更脆弱的\"抑制\"过程。", "conclusion": "模型在参数知识和上下文信息之间的仲裁存在不对称性，复制行为更容易被触发，而参数召回机制更加脆弱且与特定标记干预紧密相关。"}}
{"id": "2601.12547", "pdf": "https://arxiv.org/pdf/2601.12547", "abs": "https://arxiv.org/abs/2601.12547", "authors": ["Dipayan Sengupta", "Saumya Panda"], "title": "How Clinicians Think and What AI Can Learn From It", "categories": ["cs.AI"], "comment": "34 pages", "summary": "Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\\to$ perception $\\to$ inference $\\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.", "AI": {"tldr": "论文主张临床AI应从预测引擎转向模拟临床推理的序贯控制问题，提出使用鲁棒的序数决策规则而非期望效用优化，以应对医学决策中的不确定性和测量粗糙性。", "motivation": "当前临床AI系统主要作为预测引擎，但真实临床推理是时间受限的序贯控制问题，需要结合信息收集和不可逆行动，而临床医生常使用快速节俭的词典式启发式方法。", "method": "提出使用鲁棒的序数决策规则（如ε-支配、极大极小）而非期望效用优化，将启发式视为低维特例，AI作为\"选择性复杂度\"在决策脆弱时用于打破平局。", "result": "序数决策规则在测量粗糙性超过决策边界时更加稳定，而期望效用优化在小扰动下变得脆弱（高翻转概率）。", "conclusion": "临床AI应使用丰富模型进行信念和轨迹建模，但通过鲁棒序数规则选择行动，将AI部署为\"选择性复杂度\"，主要在决策脆弱且信息具有正期望影响时用于打破平局。"}}
{"id": "2601.12078", "pdf": "https://arxiv.org/pdf/2601.12078", "abs": "https://arxiv.org/abs/2601.12078", "authors": ["Linfeng Du", "Ye Yuan", "Zichen Zhao", "Fuyuan Lyu", "Emiliano Penaloza", "Xiuying Chen", "Zipeng Sun", "Jikun Kang", "Laurent Charlin", "Xue Liu", "Haolun Wu"], "title": "Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.", "AI": {"tldr": "PURPLE是一个基于上下文多臂老虎机的框架，通过Plackett-Luce排序模型优化用户档案构建，直接根据生成质量进行检索，在9个个性化任务上超越了现有方法。", "motivation": "现有基于语义相关性的检索增强方法不可靠，相关记录可能因冗余或冲突信息反而降低生成质量，需要更直接优化生成效用的方法。", "method": "提出PURPLE框架，将档案构建视为集合生成问题，使用Plackett-Luce模型捕捉记录间复杂依赖关系，通过参考响应的似然度作为密集反馈进行训练。", "result": "在9个个性化任务上的广泛实验表明，PURPLE在效果和效率上都显著优于强启发式和检索增强基线方法。", "conclusion": "PURPLE为优化用户档案提供了一个原则性且可扩展的解决方案，能够直接将对齐检索与生成质量，实现更好的个性化响应。"}}
{"id": "2601.12560", "pdf": "https://arxiv.org/pdf/2601.12560", "abs": "https://arxiv.org/abs/2601.12560", "authors": ["Arunkumar V", "Gangadharan G. R.", "Rajkumar Buyya"], "title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents", "categories": ["cs.AI", "cs.MA"], "comment": "28 pages, 4 figures, 5 tables", "summary": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.", "AI": {"tldr": "论文提出了从文本生成模型向智能体AI的转变，建立了统一的智能体架构分类法（感知、大脑、规划、行动、工具使用、协作），分析了推理模型的演进和环境分类，并指出了当前挑战和未来研究方向。", "motivation": "AI正从单纯的文本生成转向智能体AI，但新兴设计多样（从简单单循环到分层多智能体系统），使得该领域难以系统化理解，需要统一的分类框架来梳理这一复杂景观。", "method": "提出统一的智能体架构分类法，将智能体分解为六个核心组件：感知、大脑、规划、行动、工具使用和协作；分析从线性推理到原生推理模型的演进，以及从固定API调用到开放标准的转变；对智能体操作环境进行分类和评估实践回顾。", "result": "建立了系统化的智能体分类框架，清晰描述了智能体技术的演进路径，包括推理模型发展和工具使用标准的转变，提供了环境分类和当前评估方法的全面综述。", "conclusion": "论文为智能体AI领域提供了重要的概念框架和分类体系，识别了幻觉行为、无限循环和提示注入等关键挑战，为构建更健壮可靠的自主系统指明了未来研究方向。"}}
{"id": "2601.12099", "pdf": "https://arxiv.org/pdf/2601.12099", "abs": "https://arxiv.org/abs/2601.12099", "authors": ["Leonardo S. Goodall", "Dor Shilton", "Daniel A. Mullins", "Harvey Whitehouse"], "title": "Large language models struggle with ethnographic text annotation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.", "AI": {"tldr": "评估7个先进大语言模型在民族志文本标注任务上的表现，发现模型性能有限，无法替代人类专家，特别是在长文本、序数区分和模糊概念处理方面表现不佳", "motivation": "评估LLMs在民族志文本自动标注方面的能力，探索其加速跨文化研究的潜力", "method": "使用7个最先进的LLMs对567个民族志摘录中的121个仪式特征进行标注，并与人类标注者的一致性进行比较", "result": "模型性能有限，远低于可靠自动标注所需水平；人类编码者难以达成一致的特征对LLMs也困难；即使在人类一致同意的特征上，模型表现也不及人类", "conclusion": "LLMs目前还不能替代人类专家在民族志标注中的专业能力"}}
{"id": "2601.12641", "pdf": "https://arxiv.org/pdf/2601.12641", "abs": "https://arxiv.org/abs/2601.12641", "authors": ["Xiangyu Shi", "Junyang Ding", "Xu Zhao", "Sinong Zhan", "Payal Mohapatra", "Daniel Quispe", "Kojo Welbeck", "Jian Cao", "Wei Chen", "Ping Guo", "Qi Zhu"], "title": "STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models", "categories": ["cs.AI"], "comment": "Accepted to the Design, Automation & Test in Europe Conference (DATE) 2026", "summary": "Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.", "AI": {"tldr": "该论文提出了STEP-LLM，一个从自然语言生成STEP格式CAD模型的框架，通过DFS重序列化、检索增强生成和强化学习等方法，显著提升了CAD模型生成的几何保真度和制造兼容性。", "motivation": "当前基于文本的CAD生成方法使用命令序列或脚本格式，但这些格式依赖特定内核且制造兼容性差。STEP格式作为广泛采用的中性边界表示格式更适合制造，但其图结构特性给自回归LLM带来挑战。", "method": "1) 构建4万对STEP-标题数据集；2) 采用DFS重序列化线性化交叉引用；3) 使用链式思维风格的结构注释；4) 集成检索增强生成进行监督微调；5) 通过基于Chamfer距离的几何奖励进行强化学习。", "result": "STEP-LLM在几何保真度上持续优于Text2CAD基线，RAG模块显著提升完整性和可渲染性，DFS重序列化增强整体准确性，RL进一步减少几何差异。", "conclusion": "研究表明LLM驱动的STEP模型生成具有可行性，有望实现CAD设计的民主化，让非专家也能通过自然语言创建可直接制造的CAD模型。"}}
{"id": "2601.12104", "pdf": "https://arxiv.org/pdf/2601.12104", "abs": "https://arxiv.org/abs/2601.12104", "authors": ["David Ilić", "David Stanojević", "Kostadin Cvejoski"], "title": "Powerful Training-Free Membership Inference Against Autoregressive Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "9 pages, 2 figures; appendix with additional experiments and derivations", "summary": "Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.", "AI": {"tldr": "EZ-MIA是一种新的成员推理攻击方法，通过分析模型在错误位置的概率偏移来检测微调语言模型中的隐私风险，相比现有方法在低误报率下检测率显著提高", "motivation": "微调语言模型可能记忆并泄露训练数据中的敏感信息，现有成员推理攻击方法在实践所需的低误报率阈值下检测效果有限", "method": "提出EZ分数，测量微调模型相对于预训练参考模型在错误位置的概率偏移方向不平衡性，仅需两次前向传播且无需模型训练", "result": "在WikiText数据集上，EZ-MIA在1%误报率下真阳性率66.3%（比之前方法高3.8倍），AUC达到0.98；在0.1%误报率下检测率提高8倍", "conclusion": "微调语言模型的隐私风险比之前认知的要大得多，对隐私审计和部署决策有重要影响"}}
{"id": "2601.12661", "pdf": "https://arxiv.org/pdf/2601.12661", "abs": "https://arxiv.org/abs/2601.12661", "authors": ["Chuhan Qiao", "Jianghua Huang", "Daxing Zhao", "Ziding Liu", "Yanjun Shen", "Bing Cheng", "Wei Lin", "Kai Wu"], "title": "MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents", "categories": ["cs.AI"], "comment": null, "summary": "Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, frequently overlooking the end-to-end process integrity and clinical safety essential for real-world practice. While recent interactive benchmarks have introduced dynamic scenarios, they often remain fragmented and coarse-grained, failing to capture the structured inquiry logic and diagnostic rigor required in professional consultations. To bridge this gap, we propose MedConsultBench, a comprehensive framework designed to evaluate the complete online consultation cycle by covering the entire clinical workflow from history taking and diagnosis to treatment planning and follow-up Q\\&A. Our methodology introduces Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. By addressing the underspecification and ambiguity inherent in online consultations, the benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q\\&A via constraint-respecting plan revisions. Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks significant deficiencies in information-gathering efficiency and medication safety. These results underscore a critical gap between theoretical medical knowledge and clinical practice ability, establishing MedConsultBench as a rigorous foundation for aligning medical AI with the nuanced requirements of real-world clinical care.", "AI": {"tldr": "MedConsultBench是一个全面的医疗咨询评估框架，通过原子信息单元(AIUs)和22个细粒度指标来评估LLM在完整临床工作流程中的表现，发现高诊断准确性往往掩盖了信息收集效率和用药安全方面的严重缺陷。", "motivation": "当前医疗咨询代理评估过于注重结果导向任务，忽略了端到端流程完整性和临床安全性，缺乏对结构化问询逻辑和诊断严谨性的细粒度评估。", "method": "提出MedConsultBench框架，使用原子信息单元(AIUs)在子轮次级别追踪临床信息获取，通过22个细粒度指标评估从病史采集、诊断到治疗计划和随访问答的完整临床工作流程。", "result": "对19个大型语言模型的系统评估显示，高诊断准确性往往掩盖了信息收集效率和用药安全方面的显著缺陷。", "conclusion": "研究揭示了理论医学知识与临床实践能力之间的关键差距，MedConsultBench为将医疗AI与真实临床护理的细微要求对齐提供了严谨基础。"}}
{"id": "2601.12132", "pdf": "https://arxiv.org/pdf/2601.12132", "abs": "https://arxiv.org/abs/2601.12132", "authors": ["Md Mahmudul Hoque", "Md Mehedi Hassain", "Md Hojaifa Tanvir", "Rahul Nandy"], "title": "Bengali Text Classification: An Evaluation of Large Language Model Approaches", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the \"Sports\" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.", "AI": {"tldr": "本研究评估了三种指令调优大语言模型（LLaMA 3.1 8B、LLaMA 3.2 3B和Qwen 2.5 7B）在孟加拉语新闻分类任务上的表现，Qwen 2.5以72%的准确率表现最佳。", "motivation": "孟加拉语文本分类面临标注数据集和预训练语言模型缺乏的挑战，需要探索大语言模型在此任务上的有效性。", "method": "使用来自Prothom Alo报纸的Kaggle数据集，在相同分类框架下评估三种指令调优LLM的性能。", "result": "Qwen 2.5获得最高分类准确率72%（体育类别表现尤佳），LLaMA 3.1和3.2分别获得53%和56%的准确率。", "conclusion": "LLMs在孟加拉语文本分类中表现有效，未来研究将探索更多模型、解决类别不平衡问题并改进微调方法。"}}
{"id": "2601.12667", "pdf": "https://arxiv.org/pdf/2601.12667", "abs": "https://arxiv.org/abs/2601.12667", "authors": ["Yi Di", "Zhibin Zhao", "Fujin Wang", "Xue Liu", "Jiafeng Tang", "Jiaxin Ren", "Zhi Zhai", "Xuefeng Chen"], "title": "Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration", "categories": ["cs.AI"], "comment": null, "summary": "It is foreseeable that the number of spacecraft will increase exponentially, ushering in an era dominated by satellite mega-constellations (SMC). This necessitates a focus on energy in space: spacecraft power systems (SPS), especially their health management (HM), given their role in power supply and high failure rates. Providing health management for dozens of SPS and for thousands of SPS represents two fundamentally different paradigms. Therefore, to adapt the health management in the SMC era, this work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, as well as transparent reasoning and improved interpretability. Meanwhile, to validate this exploration, a hardware-realistic fault injection experimental platform is established, and its simulation model is built and open-sourced, both fully replicating the real SPS. The corresponding experimental results demonstrate that SpaceHMchat achieves excellent performance across 23 quantitative metrics, such as 100% conclusion accuracy in logical reasoning of work condition recognition, over 99% success rate in anomaly detection tool invocation, over 90% precision in fault localization, and knowledge base search time under 3 minutes in maintenance decision-making. Another contribution of this work is the release of the first-ever AIL HM dataset of SPS. This dataset contains four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.", "AI": {"tldr": "本文针对卫星巨型星座时代航天器电源系统健康管理的挑战，提出了对齐底层能力原则(AUC)并开发了开源人机协作框架SpaceHMchat，实现全回路健康管理，在23个量化指标上表现出色，并发布了首个AIL HM数据集。", "motivation": "随着卫星数量的指数级增长，航天器电源系统(SPS)的健康管理变得至关重要，但为数十个和数千个SPS提供健康管理是两种完全不同的范式，需要适应卫星巨型星座时代的新需求。", "method": "提出对齐底层能力原则(AUC)，开发开源人机协作框架SpaceHMchat，建立硬件真实的故障注入实验平台和仿真模型，涵盖工况识别、异常检测、故障定位和维护决策全流程。", "result": "SpaceHMchat在23个量化指标上表现优异：工况识别逻辑推理结论准确率100%，异常检测工具调用成功率超99%，故障定位精度超90%，维护决策知识库搜索时间低于3分钟。", "conclusion": "该工作为卫星巨型星座时代的航天器电源系统健康管理提供了有效的解决方案，通过人机协作框架和开源数据集推动了该领域的发展，具有重要的实践和理论价值。"}}
{"id": "2601.12154", "pdf": "https://arxiv.org/pdf/2601.12154", "abs": "https://arxiv.org/abs/2601.12154", "authors": ["Teodor-Călin Ionescu", "Lifeng Han", "Jan Heijdra Suasnabar", "Anne Stiggelbout", "Suzan Verberne"], "title": "Analyzing Cancer Patients' Experiences with Embedding-based Topic Modeling and LLMs", "categories": ["cs.CL"], "comment": "under review to CLIN journal", "summary": "This study investigates the use of neural topic modeling and LLMs to uncover meaningful themes from patient storytelling data, to offer insights that could contribute to more patient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer patients (132,722 words in 13 interviews). We first evaluate BERTopic and Top2Vec for individual interview summarization by using similar preprocessing, chunking, and clustering configurations to ensure a fair comparison on Keyword Extraction. LLMs (GPT4) are then used for the next step topic labeling. Their outputs for a single interview (I0) are rated through a small-scale human evaluation, focusing on {coherence}, {clarity}, and {relevance}. Based on the preliminary results and evaluation, BERTopic shows stronger performance and is selected for further experimentation using three {clinically oriented embedding} models. We then analyzed the full interview collection with the best model setting. Results show that domain-specific embeddings improved topic \\textit{precision} and \\textit{interpretability}, with BioClinicalBERT producing the most consistent results across transcripts. The global analysis of the full dataset of 13 interviews, using the BioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews, namely ``Coordination and Communication in Cancer Care Management\" and ``Patient Decision-Making in Cancer Treatment Journey''. Although the interviews are machine translations from Dutch to English, and clinical professionals are not involved in this evaluation, the findings suggest that neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians from patient interviews. This pipeline could support more efficient document navigation and strengthen the role of patients' voices in healthcare workflows.", "AI": {"tldr": "本研究使用神经主题建模(BERTopic和Top2Vec)和LLMs分析癌症患者访谈数据，发现BioClinicalBERT嵌入模型能提高主题精确度和可解释性，揭示癌症护理中的协调沟通和患者决策制定等关键主题。", "motivation": "通过分析患者讲述数据来发现有意义主题，为更以患者为中心的医疗实践提供见解，强化患者在医疗工作流程中的声音。", "method": "使用BERTopic和Top2Vec对13个癌症患者访谈(132,722词)进行主题建模，采用相似预处理、分块和聚类配置，然后用GPT-4进行主题标注，并通过人工评估比较性能。", "result": "BERTopic表现更优，BioClinicalBERT嵌入模型产生最一致结果，识别出\"癌症护理管理中的协调与沟通\"和\"癌症治疗旅程中的患者决策制定\"两个主导主题。", "conclusion": "神经主题建模特别是BERTopic能从患者访谈中为临床医生提供有用反馈，支持更高效的文档导航并强化患者在医疗工作流程中的作用。"}}
{"id": "2601.12688", "pdf": "https://arxiv.org/pdf/2601.12688", "abs": "https://arxiv.org/abs/2601.12688", "authors": ["Xu Zhang", "Qinghua Wang", "Mengyang Zhao", "Fang Wang", "Cunquan Qu"], "title": "Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.", "AI": {"tldr": "该论文提出了一种掩码多阶段推理(MMSI)框架，通过将量刑逻辑融入预训练Transformer编码器，解决多被告案件中角色模糊问题，提升AI辅助司法分析的准确性和可解释性。", "motivation": "多被告案件中司法表述常模糊被告角色，影响AI分析效果和司法公平性，需要精确区分主犯从犯责任。", "method": "采用定向掩码机制澄清被告角色，比较数据构建策略增强模型对罪责差异的敏感性，通过广播机制将预测的罪责标签整合到回归模型中。", "result": "在故意伤害案件的IMLJP数据集上评估，MMSI框架在基于角色的罪责区分方面显著优于基线模型，取得了显著的准确率提升。", "conclusion": "该工作为增强智能司法系统提供了稳健解决方案，代码已公开，有助于提升多被告案件分析的智能辅助水平。"}}
{"id": "2601.12179", "pdf": "https://arxiv.org/pdf/2601.12179", "abs": "https://arxiv.org/abs/2601.12179", "authors": ["Adam E. Friedman", "Stevan Harnad", "Rushen Shi"], "title": "Tolerance Principle and Small Language Model Learning", "categories": ["cs.CL"], "comment": "14 pages, 6 figures. BUCLD 50 Proceedings. To be published in 2026 by Cascadilla Press", "summary": "Modern language models like GPT-3, BERT, and LLaMA require massive training data, yet with sufficient training they reliably learn to distinguish grammatical from ungrammatical sentences. Children aged as young as 14 months already have the capacity to learn abstract grammar rules from very few exemplars, even in the presence of non-rule-following exceptions. Yang's (2016) Tolerance Principle defines a precise threshold for how many exceptions a rule can tolerate and still be learnable. The present study explored the minimal amount and quality of training data necessary for rules to be generalized by a transformer-based language model to test the predictions of the Tolerance Principle. We trained BabyBERTa (Huebner et al. 2021), a transformer model optimized for small datasets, on artificial grammars. The training sets varied in size, number of unique sentence types, and proportion of rule-following versus exception exemplars. We found that, unlike human infants, BabyBERTa's learning dynamics do not align with the Tolerance Principle.", "AI": {"tldr": "本研究通过BabyBERTa模型测试Yang的容忍原则，发现与人类婴儿不同，该Transformer模型的学习动态不符合容忍原则的预测。", "motivation": "探索Transformer语言模型在少量训练数据下学习抽象语法规则的能力，并与人类婴儿的语言学习能力进行对比，验证Yang的容忍原则在AI模型中的适用性。", "method": "使用BabyBERTa模型在人工语法数据集上进行训练，通过调整训练集大小、唯一句子类型数量和规则遵循/例外样本比例来测试学习效果。", "result": "研究发现BabyBERTa的学习动态与人类婴儿不同，不符合容忍原则的预测，表明当前Transformer模型在少量数据学习语法规则方面与人类认知存在差异。", "conclusion": "Transformer语言模型在从少量示例中学习抽象语法规则的能力上与人类婴儿存在本质差异，需要进一步研究如何使AI模型更接近人类语言学习的认知机制。"}}
{"id": "2601.12711", "pdf": "https://arxiv.org/pdf/2601.12711", "abs": "https://arxiv.org/abs/2601.12711", "authors": ["Kevin Wang", "Neel P. Bhatt", "Cong Liu", "Junbo Li", "Runjin Chen", "Yihan Xi", "Timothy Barclay", "Alvaro Velasquez", "Ufuk Topcu", "Zhangyang Wang"], "title": "Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts", "categories": ["cs.AI", "cs.LG", "cs.SC"], "comment": null, "summary": "Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.", "AI": {"tldr": "该论文提出了一个神经符号LoRA框架，通过动态结合数值更新和符号操作两种策略，在语言模型微调中实现了更好的适应性和性能表现。", "motivation": "现有的语言模型适应方法要么通过数值更新修改模型参数，要么通过符号操作处理离散提示或逻辑约束。数值微调擅长注入新的事实知识，而符号更新则提供无需重新训练的灵活风格和对齐控制。两种方法各有优势但缺乏有效结合。", "method": "提出了神经符号LoRA框架，包含统一的监控信号和基于奖励的分类器，动态决定何时使用LoRA进行更深层的事实重构，何时应用TextGrad进行标记级别的编辑。通过仅在需要时将符号转换卸载到外部LLM来保持内存效率。", "result": "在多个LLM骨干网络上进行的广泛实验表明，神经符号LoRA始终优于纯数值或纯符号基线方法，显示出卓越的适应性和改进的性能。", "conclusion": "研究结果表明，交织使用数值和符号更新可以为语言模型微调开启新的多功能性水平，符号编辑过程中产生的精炼提示还可以作为高质量、可重复使用的训练数据，在数学推理等数据稀缺领域尤为重要。"}}
{"id": "2601.12199", "pdf": "https://arxiv.org/pdf/2601.12199", "abs": "https://arxiv.org/abs/2601.12199", "authors": ["Muhammad Umar Farooq", "Oscar Saz"], "title": "CTC-DID: CTC-Based Arabic dialect identification for streaming applications", "categories": ["cs.CL"], "comment": "Accepted for IEEE ICASSP 2026", "summary": "This paper proposes a Dialect Identification (DID) approach inspired by the Connectionist Temporal Classification (CTC) loss function as used in Automatic Speech Recognition (ASR). CTC-DID frames the dialect identification task as a limited-vocabulary ASR system, where dialect tags are treated as a sequence of labels for a given utterance. For training, the repetition of dialect tags in transcriptions is estimated either using a proposed Language-Agnostic Heuristic (LAH) approach or a pre-trained ASR model. The method is evaluated on the low-resource Arabic Dialect Identification (ADI) task, with experimental results demonstrating that an SSL-based CTC-DID model, trained on a limited dataset, outperforms both fine-tuned Whisper and ECAPA-TDNN models. Notably, CTC-DID also surpasses these models in zero-shot evaluation on the Casablanca dataset. The proposed approach is found to be more robust to shorter utterances and is shown to be easily adaptable for streaming, real-time applications, with minimal performance degradation.", "AI": {"tldr": "基于CTC损失的方言识别方法，将方言识别任务构建为有限词汇ASR系统，在低资源阿拉伯方言识别任务中表现优于微调Whisper和ECAPA-TDNN模型，且在零样本评估中表现更优。", "motivation": "受ASR中CTC损失函数启发，将方言识别任务重新构建为序列标注问题，以提升低资源方言识别的性能。", "method": "使用CTC-DID框架，将方言标签作为序列标签处理，通过语言无关启发式方法(LAH)或预训练ASR模型估计转录中的方言标签重复，基于自监督学习训练模型。", "result": "在有限数据集上训练的SSL-based CTC-DID模型在阿拉伯方言识别任务中超越微调Whisper和ECAPA-TDNN模型，在Casablanca数据集零样本评估中表现更佳，对短语音更具鲁棒性。", "conclusion": "CTC-DID方法在低资源方言识别中表现优异，具有更好的零样本泛化能力，且易于适配流式实时应用，性能下降极小。"}}
{"id": "2601.12720", "pdf": "https://arxiv.org/pdf/2601.12720", "abs": "https://arxiv.org/abs/2601.12720", "authors": ["Hanbin Wang", "Jingwei Song", "Jinpeng Li", "Qi Zhu", "Fei Mi", "Ganqu Cui", "Yasheng Wang", "Lifeng Shang"], "title": "Teaching Large Reasoning Models Effective Reflection", "categories": ["cs.AI"], "comment": "14 pages (including appendix), 5 figures", "summary": "Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.", "AI": {"tldr": "该论文提出了SCFT和RLERR两种方法来解决大型推理模型的表面反思问题，通过自我批判微调和强化学习奖励机制，显著提升了推理准确性和反思质量。", "motivation": "大型推理模型在复杂推理任务中经常进行自我反思，但许多反思是表面的，无法改善原始答案且增加计算开销，因此需要解决表面反思问题。", "method": "1. SCFT：通过自我批判生成高质量批判，使用拒绝采样筛选，并用批判目标微调模型；2. RLERR：基于SCFT的高质量反思构建奖励信号，通过强化学习引导模型内化自我纠正过程。", "result": "在AIME2024和AIME2025基准测试中，SCFT和RLERR显著提高了推理准确性和反思质量，优于最先进的基线方法。", "conclusion": "SCFT和RLERR方法有效解决了表面反思问题，提升了大型推理模型的反思能力和推理性能，为自我反思机制提供了新的训练框架。"}}
{"id": "2601.12208", "pdf": "https://arxiv.org/pdf/2601.12208", "abs": "https://arxiv.org/abs/2601.12208", "authors": ["Yunzhe Li", "Richie Yueqi Feng", "Tianxin Wei", "Chin-Chia Hsu"], "title": "CoReflect: Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating conversational systems in multi-turn settings remains a fundamental challenge. Conventional pipelines typically rely on manually defined rubrics and fixed conversational context$-$a static approach that limits coverage and fails to capture the diverse, emergent behaviors of dialogue models. To address this, we introduce CoReflect (Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement), which unifies dialogue simulation and evaluation into an adaptive, iterative process. CoReflect employs a conversation planner that generates structured templates to guide a user simulator through diverse, goal-directed dialogues. Subsequently, a reflective analyzer processes these dialogues to identify systematic behavioral patterns and automatically refine the evaluation rubrics. Crucially, the insights from the conversation analysis are fed back into the planner to update conversation templates for subsequent iterations. This co-evolution loop ensures that the complexity of test cases and the diagnostic precision of rubrics improve in tandem. By minimizing human intervention, CoReflect provides a scalable and self-refining methodology that allows evaluation protocols to adapt alongside the rapidly advancing capabilities of dialogue models.", "AI": {"tldr": "CoReflect是一个对话系统评估框架，通过协同进化的模拟和反思性评估标准优化，将对话模拟和评估统一为自适应迭代过程，减少人工干预并提升评估的覆盖面和诊断精度。", "motivation": "传统对话系统评估方法依赖人工定义的固定评估标准和静态对话上下文，覆盖面有限且无法捕捉对话模型多样化的新兴行为，需要更自适应的评估方案。", "method": "使用对话规划器生成结构化模板引导用户模拟器进行目标导向对话，然后通过反思分析器分析对话以识别系统行为模式并自动优化评估标准，形成协同进化循环。", "result": "CoReflect通过最小化人工干预，提供了可扩展和自我优化的方法论，使评估协议能够随着对话模型能力的快速发展而自适应调整。", "conclusion": "CoReflect通过协同进化循环统一对话模拟和评估，实现了测试案例复杂性和评估标准诊断精度的同步提升，为对话系统评估提供了更有效的解决方案。"}}
{"id": "2601.12744", "pdf": "https://arxiv.org/pdf/2601.12744", "abs": "https://arxiv.org/abs/2601.12744", "authors": ["Tasnim Ahmed", "Yifan Zhu", "Salimur Choudhury"], "title": "Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks", "categories": ["cs.AI", "cs.NI", "cs.SE"], "comment": "Accepted for presentation at The IEEE International Conference on Communications (ICC) 2026", "summary": "Intent-Based Networking (IBN) allows operators to specify high-level network goals rather than low-level configurations. While recent work demonstrates that large language models can automate configuration tasks, a distinct class of intents requires generating optimization code to compute provably optimal solutions for traffic engineering, routing, and resource allocation. Current systems assume text-based intent expression, requiring operators to enumerate topologies and parameters in prose. Network practitioners naturally reason about structure through diagrams, yet whether Vision-Language Models (VLMs) can process annotated network sketches into correct optimization code remains unexplored. We present IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. Our evaluation shows that visual parameter extraction reduces execution success by 12-21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system. We also demonstrate practical feasibility through a case study that deploys VLM-generated code to network testbed infrastructure using Model Context Protocol.", "AI": {"tldr": "IntentOpt基准测试评估了4种视觉语言模型在从网络拓扑图中生成优化代码的能力，发现视觉参数提取会降低执行成功率12-21%，开源模型表现远落后于闭源模型", "motivation": "当前基于文本的意图表达需要操作员用文字描述拓扑和参数，但网络从业者通常通过图表进行结构推理，需要探索视觉语言模型是否能从带注释的网络草图中生成正确的优化代码", "method": "创建包含85个优化问题的IntentOpt基准，评估4种VLMs在三种提示策略下处理多模态与纯文本输入的表现，并通过案例研究在实际网络测试床上部署生成的代码", "result": "视觉参数提取使执行成功率下降12-21个百分点，GPT-5-Mini从93%降至72%；思维程序提示降低性能最多13个百分点；开源模型表现较差，Llama-3.2-11B-Vision仅18%而GPT-5-Mini达75%", "conclusion": "研究建立了当前VLMs在IBN系统中生成优化代码的基线能力和局限性，证明了通过模型上下文协议在实际网络基础设施中部署VLM生成代码的可行性"}}
{"id": "2601.12247", "pdf": "https://arxiv.org/pdf/2601.12247", "abs": "https://arxiv.org/abs/2601.12247", "authors": ["Miao Li", "Hanyang Jiang", "Sikai Chen", "Hengyu Fu", "Yuhang Cai", "Baihe Huang", "Tinghan Ye", "Xuanzhou Chen", "Pascal Van Hentenryck"], "title": "Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.", "AI": {"tldr": "PVF是一种无需训练的解码策略，通过分层规划-验证-填充机制，在扩散语言模型中实现高效文本生成，相比基于置信度的并行解码减少65%的函数评估次数，同时保持准确性。", "motivation": "当前扩散语言模型的解码策略未能充分利用全局双向上下文来指导生成过程，存在效率低下的问题。", "method": "提出Plan-Verify-Fill（PVF）范式：主动构建分层语义骨架，优先处理高影响力语义锚点，并通过验证协议实现结构化停止机制。", "result": "在LLaDA-8B-Instruct和Dream-7B-Instruct模型上的实验显示，PVF相比基准方法减少高达65%的函数评估次数，效率显著提升。", "conclusion": "PVF为扩散语言模型提供了一种高效的训练无关解码方案，通过主动规划和验证机制实现了效率与准确性的平衡。"}}
{"id": "2601.12781", "pdf": "https://arxiv.org/pdf/2601.12781", "abs": "https://arxiv.org/abs/2601.12781", "authors": ["Hyejin Park", "Junhyuk Kwon", "Suha Kwak", "Jungseul Ok"], "title": "VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.", "AI": {"tldr": "VIRO框架通过嵌入轻量级操作符级验证器来解决神经符号REC方法中的级联错误问题，在目标存在和无目标场景下都实现了最先进性能", "motivation": "现有的神经符号REC方法假设中间推理步骤准确，但实际上会导致级联错误传播，即使图像中没有目标也会产生高置信度误报", "method": "引入验证集成推理操作符(VIRO)，在推理步骤中嵌入轻量级操作符级验证器，每个操作符执行并验证其输出(如对象存在性、空间关系)", "result": "达到61.1%的平衡准确率，程序失败率低于0.3%，在计算效率和可扩展性方面表现优异，并能泛化到真实世界的自我中心数据", "conclusion": "VIRO框架通过集成验证机制有效解决了级联错误问题，在保持推理可解释性的同时显著提升了REC系统的鲁棒性和可靠性"}}
{"id": "2601.12263", "pdf": "https://arxiv.org/pdf/2601.12263", "abs": "https://arxiv.org/abs/2601.12263", "authors": ["Yixuan Du", "Chenxiao Yu", "Haoyan Xu", "Ziyi Wang", "Yue Zhao", "Xiyang Hu"], "title": "Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.", "AI": {"tldr": "论文揭示了视觉语言模型在产品搜索中的多模态排名攻击漏洞，提出了MGEO框架，通过联合优化图像扰动和文本后缀来操纵搜索结果排名，实验证明该攻击方法显著优于单模态攻击。", "motivation": "现代检索和推荐系统广泛采用视觉语言模型，但其在竞争性排名场景中对抗对抗性操纵的鲁棒性尚未得到充分研究，特别是在产品搜索领域存在潜在安全风险。", "method": "提出多模态生成引擎优化（MGEO）框架，采用交替梯度优化策略，同时优化不可感知的图像扰动和流畅的文本后缀，利用VLM内部的深度跨模态耦合特性。", "result": "在真实数据集上的广泛实验表明，这种协调攻击方法显著优于仅文本或仅图像的基线方法，成功实现了对目标产品的不公平推广。", "conclusion": "研究发现多模态协同效应这一VLM的优势可能被武器化，从而在不触发传统内容过滤器的情况下破坏搜索排名的完整性，揭示了VLM在实际应用中的重要安全漏洞。"}}
{"id": "2601.12804", "pdf": "https://arxiv.org/pdf/2601.12804", "abs": "https://arxiv.org/abs/2601.12804", "authors": ["Hanwei Zhang", "Luo Cheng", "Rui Wen", "Yang Zhang", "Lijun Zhang", "Holger Hermanns"], "title": "SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.", "AI": {"tldr": "SL-CBM是一种新的可解释AI方法，通过在概念和类别层面生成空间一致的重要性图来增强概念瓶颈模型的局部忠实性，提高解释质量和干预效果。", "motivation": "现有的概念瓶颈模型(CBMs)存在局部忠实性差的问题，无法将概念与有意义的图像区域进行空间对齐，限制了其可解释性和可靠性。", "method": "提出SL-CBM，集成1x1卷积层和交叉注意力机制，增强概念、图像区域和最终预测之间的对齐，通过对比性和基于熵的正则化平衡准确性、稀疏性和忠实性。", "result": "在图像数据集上的广泛实验表明，SL-CBM显著提高了局部忠实性、解释质量和干预效果，同时保持了竞争性的分类准确性。", "conclusion": "SL-CBM弥合了基于概念的推理和空间可解释性之间的差距，为可解释和可信赖的概念模型设立了新标准。"}}
{"id": "2601.12269", "pdf": "https://arxiv.org/pdf/2601.12269", "abs": "https://arxiv.org/abs/2601.12269", "authors": ["Xucong Hu", "Jian-Qiao Zhu"], "title": "Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.", "AI": {"tldr": "该论文展示了一种无需额外训练即可从基础语言模型中恢复强大心理理论能力的方法，通过基于马尔可夫链蒙特卡洛的功率采样和退火技术来优化序列级概率分布。", "motivation": "自回归语言模型通常被认为是下一个token预测器，主要优化表面合理性而非保持正确的潜在状态表示（全局连贯性），因此在心理理论任务上表现不佳。虽然后训练方法可以改善性能，但研究旨在直接从基础模型中恢复这种能力。", "method": "采用基于马尔可夫链蒙特卡洛的功率采样方法，从自回归语言模型的序列级（而非token级）概率分布中进行采样，并引入退火技术，逐渐将温度分布从高温转向低温。", "result": "研究发现结合退火技术的功率采样显著提高了心理理论任务的性能，优于固定温度的功率采样方法。", "conclusion": "基于采样的优化方法提供了一种无需重新训练即可从语言模型中提取潜在能力的强大途径，表明基础模型本身就具备心理理论能力，只是需要通过适当的采样策略来激活。"}}
{"id": "2601.12822", "pdf": "https://arxiv.org/pdf/2601.12822", "abs": "https://arxiv.org/abs/2601.12822", "authors": ["Wenqi Zhang", "Yulin Shen", "Changyue Jiang", "Jiarun Dai", "Geng Hong", "Xudong Pan"], "title": "MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction", "categories": ["cs.AI"], "comment": null, "summary": "Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.", "AI": {"tldr": "MirrorGuard是一个即插即用的防御框架，通过基于模拟的训练提升计算机使用代理的安全性，无需执行真实操作即可拦截和修正不安全推理链，在保持代理效用的同时显著降低安全风险。", "motivation": "大型基础模型集成到计算机使用代理中带来了严重的安全风险：恶意指令或视觉提示注入可能触发不安全的推理并导致有害的系统级操作。现有防御方法如基于检测的阻止会过早中止任务，降低代理效用。", "method": "提出神经符号模拟管道，在基于文本的模拟环境中生成真实的高风险GUI交互轨迹，捕获不安全推理模式和系统危险。MirrorGuard在模拟环境中学习拦截和修正不安全推理链。", "result": "在ByteDance UI-TARS系统上，将不安全率从66.5%降至13.0%，同时保持较低的错误拒绝率(FRR)。相比最先进的GuardAgent(降至53.9%和15.4% FRR)表现更优。", "conclusion": "模拟衍生的防御可以在保持代理基本效用的同时提供强大的现实世界保护，证明了仿真训练对提升计算机使用代理安全性的有效性。"}}
{"id": "2601.12286", "pdf": "https://arxiv.org/pdf/2601.12286", "abs": "https://arxiv.org/abs/2601.12286", "authors": ["Jonathan Pan"], "title": "Conversational Context Classification: A Representation Engineering Approach", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.", "AI": {"tldr": "本研究探索使用表征工程和单类支持向量机在LLM内部状态中识别特定上下文子空间的方法，用于检测LLM的离上下文响应问题。", "motivation": "大型语言模型日益普及，但存在生成脱离上下文响应的问题，需要有效保障机制。传统异常检测方法难以直接应用于上下文语义场景。", "method": "使用表征工程(RepE)和单类支持向量机(OCSVM)，在LLM内部状态中训练上下文示例，建立隐藏状态潜在空间的稳健边界。在Llama和Qwen模型上评估，识别与特定上下文强相关的优化层。", "result": "评估结果显示在识别特定上下文子空间方面取得了有希望的结果，能够有效检测对话线程是否在上下文内。", "conclusion": "该方法不仅有助于检测上下文内外的对话，还为更好地解释LLM的研究做出了贡献。"}}
{"id": "2601.12842", "pdf": "https://arxiv.org/pdf/2601.12842", "abs": "https://arxiv.org/abs/2601.12842", "authors": ["Qitong Fang", "Haotian Li", "Xu Wang"], "title": "SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "11 pages, 3 figures. Equal contribution: Qitong Fang and Haotian Li. Corresponding authors: Qitong Fang (fangqitong@student.jlju.edu.cn), Haotian Li (lihaotian@student.jlju.edu.cn), Xu Wang (wangxu@jlju.edu.cn)", "summary": "Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.", "AI": {"tldr": "SCULPT是一个约束引导的蒙特卡洛树搜索方法，通过领域感知评分和剪枝策略，提升LLM在自动化工作流中的有序探索能力，避免随机搜索中的不合理分支。", "motivation": "当前LLM自动化工作流依赖随机探索，经常遍历不合理分支，因为现有方法使用通用提示或弱领域先验的策略，导致在操作符、单位和格式上进行近乎随机的搜索。", "method": "提出SCULPT方法，将领域感知评分整合到MCTS的选择、扩展、模拟和回溯过程中，使用符号检查（维度一致性、类型兼容性、数值合理性、深度控制和多样性）和结构模式指导来评分和剪枝动作。", "result": "在匹配的LLM配置下，SCULPT在多个数据集上实现了稳定的性能提升；使用GPT-5.2的额外结果验证了执行器可转移性和在前沿推理模型上的性能。", "conclusion": "领域感知约束可以在保持效率和推理稳定性的同时提高准确性，为LLM自动化工作流提供更可靠的搜索策略。"}}
{"id": "2601.12369", "pdf": "https://arxiv.org/pdf/2601.12369", "abs": "https://arxiv.org/abs/2601.12369", "authors": ["Ming Zhang", "Jiabao Zhuang", "Wenqing Jing", "Ziyu Kong", "Jingyi Deng", "Yujiong Shen", "Kexin Tan", "Yuhang Zhao", "Ning Luo", "Renzhe Zheng", "Jiahui Lin", "Mingqi Wu", "Long Ma", "Yi Zou", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies", "categories": ["cs.CL"], "comment": null, "summary": "Deep Research Agents are increasingly used for automated survey generation. However, whether they can write surveys like human experts remains unclear. Existing benchmarks focus on fluency or citation accuracy, but none evaluates the core capabilities: retrieving essential papers and organizing them into coherent knowledge structures. We introduce TaxoBench, a diagnostic benchmark derived from 72 highly-cited computer science surveys. We manually extract expert-authored taxonomy trees containing 3,815 precisely categorized citations as ground truth. Our benchmark supports two evaluation modes: Deep Research mode tests end-to-end retrieval and organization given only a topic, while Bottom-Up mode isolates structuring capability by providing the exact papers human experts used. We evaluate 7 leading Deep Research agents and 12 frontier LLMs. Results reveal a dual bottleneck: the best agent recalls only 20.9% of expert-selected papers, and even with perfect input, the best model achieves only 0.31 ARI in organization. Current deep research agents remain far from expert-level survey writing. Our benchmark is publicly available at https://github.com/KongLongGeFDU/TaxoBench.", "AI": {"tldr": "TaxoBench是一个新的评估基准，用于测试深度研究代理在自动生成综述时的核心能力：检索关键论文并将其组织成连贯的知识结构。基于72篇高被引计算机科学综述，包含3,815个精确分类的引用作为基准。评估显示当前最佳代理只能召回20.9%的专家选择论文，组织结构能力也很有限。", "motivation": "现有基准主要关注流畅性或引用准确性，但缺乏对深度研究代理核心能力（检索关键论文和组织知识结构）的评估。需要一个新的诊断性基准来评估这些代理是否能像人类专家一样撰写综述。", "method": "从72篇高被引计算机科学综述中手动提取专家编写的分类树作为基准。支持两种评估模式：深度研究模式（端到端检索和组织）和自底向上模式（隔离组织结构能力）。评估了7个领先的深度研究代理和12个前沿LLM。", "result": "结果显示双重瓶颈：最佳代理只能召回20.9%的专家选择论文；即使在完美输入条件下，最佳模型在组织结构方面仅达到0.31 ARI（调整兰德指数）。当前深度研究代理距离专家级综述写作水平仍有很大差距。", "conclusion": "当前的深度研究代理在自动生成综述方面仍远未达到专家水平，特别是在关键论文检索和知识组织结构化方面存在显著不足。TaxoBench为评估和改进这些系统提供了重要基准。"}}
{"id": "2601.12856", "pdf": "https://arxiv.org/pdf/2601.12856", "abs": "https://arxiv.org/abs/2601.12856", "authors": ["Liping Huang", "Gaoxi Xiao", "Stefan Ma", "Hechang Chen", "Shisong Tang", "Flora Salim"], "title": "Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data", "categories": ["cs.AI", "cs.LG"], "comment": "9 pages, 9 figures. It's accepted by WWW 2026 Web4Good Track. To make accessible earlier, authors would like to put it on arxiv before the conference", "summary": "Dengue, a mosquito-borne disease, continues to pose a persistent public health challenge in urban areas, particularly in tropical regions such as Singapore. Effective and affordable control requires anticipating where transmission risks are likely to emerge so that interventions can be deployed proactively rather than reactively. This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. Instead of treating cases as isolated reports, we model how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions. While mosquito movement is highly localized, long-distance transmission is often driven by human mobility, and in our case study, the learned network aligns closely with commuting flows, providing an interpretable explanation for citywide spread. These hidden links are optimized through gradient descent and used not only to forecast hotspot status but also to verify the consistency of spreading patterns, by examining the stability of the inferred network across consecutive weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79. Importantly, the learned transmission links align with commuting flows, highlighting the interpretable interplay between hidden epidemic spread and human mobility. By shifting from simply reporting dengue cases to mining and validating hidden spreading dynamics, this work transforms open web-based case data into a predictive and explanatory resource. The proposed framework advances epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience.", "AI": {"tldr": "本研究提出了一种新颖的框架，通过分析公开的登革热病例数据，挖掘城市区域间的潜在传播链接，预测登革热传播热点，并验证传播模式的稳定性。", "motivation": "登革热在热带城市地区持续构成公共卫生挑战，需要提前预测传播风险以进行主动干预而非被动应对。传统方法将病例视为孤立报告，而实际传播受邻近地区疫情动态影响。", "method": "开发了一个框架，从公开登革热病例数据中挖掘区域间潜在传播链接，通过梯度下降优化这些隐藏链接，建模热点形成如何受邻近区域疫情动态影响。使用连续四周的热点历史数据进行预测和网络稳定性验证。", "result": "在新加坡2013-2018和2020年的案例研究中，仅需四周热点历史数据即可达到平均F-score 0.79。学习到的传播链接与通勤流高度吻合，揭示了隐藏的流行病传播与人类移动性之间的可解释关系。", "conclusion": "该框架将公开的基于网络的病例数据转化为预测性和解释性资源，推进了流行病建模，为公共卫生规划、早期干预和城市韧性提供了可扩展、低成本的工具。"}}
{"id": "2601.12374", "pdf": "https://arxiv.org/pdf/2601.12374", "abs": "https://arxiv.org/abs/2601.12374", "authors": ["Akram Elbouanani", "Aboubacar Tuo", "Adrian Popescu"], "title": "A Scalable Entity-Based Framework for Auditing Bias in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.", "AI": {"tldr": "该研究提出了一个可扩展的偏见审计框架，使用命名实体作为探针来测量大语言模型的结构性偏见，通过19亿数据点的分析揭示了系统性的政治、地理和行业偏见模式。", "motivation": "现有的大语言模型偏见评估方法要么依赖人工提示缺乏生态效度，要么使用自然任务但缺乏规模和严谨性，需要一种既真实又可扩展的偏见评估方法。", "method": "使用命名实体作为探针，通过合成数据生成大规模测试集，涵盖多种实体类型、任务、语言、模型和提示策略，共分析19亿个数据点。", "result": "发现系统性偏见：模型惩罚右翼政治家、偏爱左翼政治家、偏好西方和富裕国家而非全球南方、偏爱西方公司、惩罚国防和制药行业企业。指令调优减少偏见，但模型规模增大会放大偏见，中文或俄文提示不能减弱西方偏好。", "conclusion": "大语言模型在高风险应用部署前应进行严格审计，合成数据方法能够可靠地重现自然文本中的偏见模式，支持大规模偏见分析。"}}
{"id": "2601.12912", "pdf": "https://arxiv.org/pdf/2601.12912", "abs": "https://arxiv.org/abs/2601.12912", "authors": ["Andreas Brännström", "Juan Carlos Nieves"], "title": "Human Emotion Verification by Action Languages via Answer Set Programming", "categories": ["cs.AI"], "comment": "Under consideration in Theory and Practice of Logic Programming (TPLP)", "summary": "In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "本文介绍了基于ASP和转移系统的动作语言C-MT，用于建模人类心理状态在可观察动作序列下的演化过程，通过心理学理论形式化多维心理状态，并引入新的因果规则来约束心理状态转移。", "motivation": "需要控制智能体行为并限制动作的不良心理副作用，为此需要一种能够形式化心理状态动态变化的语言框架。", "method": "基于答案集编程(ASP)和转移系统，结合情绪评估理论等心理学理论，形式化多维心理状态，引入forbids to cause因果规则和心理状态动态表达式，将心理变化原则转化为转移约束和不变性属性。", "result": "开发了C-MT语言框架，能够建模心理状态间的有效转移原则，支持通过轨迹分析来评估不同心理变化动态的比较。", "conclusion": "C-MT语言为人类心理状态的动态演化提供了受控推理框架，支持情感验证模型的设计，并在逻辑编程理论与实践期刊中接受审稿。"}}
{"id": "2601.12376", "pdf": "https://arxiv.org/pdf/2601.12376", "abs": "https://arxiv.org/abs/2601.12376", "authors": ["Ofek Raban", "Ethan Fetaya", "Gal Chechik"], "title": "LR-DWM: Efficient Watermarking for Diffusion Language Models", "categories": ["cs.CL"], "comment": "Submitted to ACL Rolling Review (ARR). 7 pages, 4 figures", "summary": "Watermarking (WM) is a critical mechanism for detecting and attributing AI-generated content. Current WM methods for Large Language Models (LLMs) are predominantly tailored for autoregressive (AR) models: They rely on tokens being generated sequentially, and embed stable signals within the generated sequence based on the previously sampled text. Diffusion Language Models (DLMs) generate text via non-sequential iterative denoising, which requires significant modification to use WM methods designed for AR models. Recent work proposed to watermark DLMs by inverting the process when needed, but suffers significant computational or memory overhead. We introduce Left-Right Diffusion Watermarking (LR-DWM), a scheme that biases the generated token based on both left and right neighbors, when they are available. LR-DWM incurs minimal runtime and memory overhead, remaining close to the non-watermarked baseline DLM while enabling reliable statistical detection under standard evaluation settings. Our results demonstrate that DLMs can be watermarked efficiently, achieving high detectability with negligible computational and memory overhead.", "AI": {"tldr": "LR-DWM是一种针对扩散语言模型的高效水印方案，通过同时利用左右相邻token来嵌入水印，在保持接近原始模型性能的同时实现可靠检测。", "motivation": "现有的水印方法主要针对自回归语言模型，不适用于通过非顺序迭代去噪生成文本的扩散语言模型。现有方法需要反转过程，导致计算和内存开销大。", "method": "提出左右扩散水印(LR-DWM)方案，在生成token时基于左右邻居token（当可用时）进行偏置，实现水印嵌入。", "result": "LR-DWM在运行时和内存开销极小，接近无水印基线模型，在标准评估设置下实现可靠的统计检测。", "conclusion": "扩散语言模型可以通过LR-DWM高效水印，以可忽略的计算和内存开销实现高可检测性。"}}
{"id": "2601.12913", "pdf": "https://arxiv.org/pdf/2601.12913", "abs": "https://arxiv.org/abs/2601.12913", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Francesco Giannini", "Alberto Termine", "Filippo Bonchi", "Mateja Jamnik", "Giuseppe Marra"], "title": "Actionable Interpretability Must Be Defined in Terms of Symmetries", "categories": ["cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.", "AI": {"tldr": "该论文认为当前AI可解释性研究存在根本性问题，因为现有定义缺乏可操作性，无法提供推导具体建模和推理规则的形式化原则。作者提出基于对称性的可操作定义，并假设四种对称性足以涵盖核心可解释性属性、定义可解释模型类别，并统一可解释推理的贝叶斯逆推形式。", "motivation": "当前人工智能可解释性研究缺乏可操作的形式化定义，无法为实际建模和推理提供明确的指导原则，导致该领域研究基础不牢固。", "method": "提出基于对称性理论的可操作定义框架，假设四种对称性能够系统性地构建可解释性理论基础，涵盖模型特性和推理方法。", "result": "建立了以对称性为核心的可解释性形式化框架，该框架能够统一解释对齐、干预和反事实等可解释推理方法，将其表述为贝叶斯逆推的形式。", "conclusion": "通过引入对称性概念，为AI可解释性研究提供了可操作的形式化基础，解决了现有定义缺乏明确建模和推理指导原则的根本问题，为该领域奠定了更坚实的理论基础。"}}
{"id": "2601.12389", "pdf": "https://arxiv.org/pdf/2601.12389", "abs": "https://arxiv.org/abs/2601.12389", "authors": ["Lakshya Tomar", "Vinayak Abrol", "Puneet Agarwal"], "title": "NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.", "AI": {"tldr": "NADIR是一种新型非自回归模型，针对多语言音译等局部依赖任务，通过差分Transformer和专家混合机制，在保持竞争力的准确率下实现13倍加速，显著减少各类错误。", "motivation": "自回归模型在序列到序列任务中虽然准确但推理延迟高，而非自回归模型速度快但存在幻觉和长度控制问题，需要在这两者之间找到平衡。", "method": "提出NADIR架构，结合差分Transformer和专家混合机制，无需顺序依赖即可建模复杂字符映射。", "result": "相比最先进的自回归基线加速13倍以上，字符错误率15.78%（自回归14.44%，标准非自回归21.88%），重复错误减少49.53%，替换错误减少24.45%，省略错误减少32.92%，插入错误减少16.87%。", "conclusion": "NADIR为构建快速可靠的非自回归系统提供了实用蓝图，有效弥合了自回归准确性与实时大规模部署需求之间的差距。"}}
{"id": "2601.13060", "pdf": "https://arxiv.org/pdf/2601.13060", "abs": "https://arxiv.org/abs/2601.13060", "authors": ["Zecheng Li", "Zhihui Cao", "Wenke Huang", "Yudong Zhang", "Keying Qi", "Rui Wang", "Zeyu Zheng", "Jian Zhao", "Hao Zhu", "Hengxin Wu", "Yuran Wang", "Guitao Fan", "Guokun Wu", "Yicong Liu", "Zhilin Gao", "Haikun Xu", "He Yang", "Minqi Xiang", "Xingyu Liu", "Zuojian Wang"], "title": "MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux", "categories": ["cs.AI"], "comment": null, "summary": "Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.", "AI": {"tldr": "MagicGUI-RMS是一个多智能体奖励模型系统，通过结合领域特定和通用奖励模型，提供自适应轨迹评估、纠正反馈和自我进化学习能力，显著提升GUI代理的任务准确性和行为鲁棒性。", "motivation": "GUI代理在自主交互和任务执行方面进展迅速，但自动化评估代理轨迹和大规模生成高质量训练数据这两个核心挑战仍未解决，现有方法依赖人工标注或静态规则验证，限制了可扩展性和动态环境适应性。", "method": "提出MagicGUI-RMS系统，集成领域特定奖励模型(DS-RM)和通用奖励模型(GP-RM)，设计结构化数据构建管道自动生成平衡多样的奖励数据集，通过自动数据回流机制识别错误动作并提出改进方案。", "result": "大量实验证明MagicGUI-RMS在任务准确性和行为鲁棒性方面取得显著提升。", "conclusion": "MagicGUI-RMS为构建基于奖励自适应驱动的自我改进GUI代理奠定了原则性和有效的基础。"}}
{"id": "2601.12419", "pdf": "https://arxiv.org/pdf/2601.12419", "abs": "https://arxiv.org/abs/2601.12419", "authors": ["Mahammad Namazov", "Tomáš Koref", "Ivan Habernal"], "title": "Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification", "categories": ["cs.CL"], "comment": null, "summary": "Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's \"reasons\" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.", "AI": {"tldr": "本研究比较了不同可解释性技术在法律结果预测中的效果，发现模型预测理由与法律专家的判断存在显著差异，尽管量化分析结果和分类性能良好。", "motivation": "法律领域应用大语言模型需要信任和透明度，但目前哪种技术能最好地解释法律结果预测仍是一个开放性问题。", "method": "提出了一个模型无关的可解释性技术比较分析框架，采用两种理由提取方法，通过标准化充分性和全面性指标评估忠实度，并请法律专家评估提取理由的合理性。", "result": "模型预测违规的理由与法律专家的判断存在显著差异，尽管量化分析结果和下游分类性能表现良好。", "conclusion": "需要更深入理解模型决策过程与人类专家判断之间的差异，单纯依赖量化指标可能不足以评估法律领域模型的可解释性。"}}
{"id": "2601.13122", "pdf": "https://arxiv.org/pdf/2601.13122", "abs": "https://arxiv.org/abs/2601.13122", "authors": ["Gourab K Patro", "Himanshi Agrawal", "Himanshu Gharat", "Supriya Panigrahi", "Nim Sherpa", "Vishal Vaddina", "Dagnachew Birru"], "title": "Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward", "categories": ["cs.AI"], "comment": null, "summary": "Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.", "AI": {"tldr": "论文分析了通用AI系统在负责任AI原则下的风险，提出C2V2框架来解决高自由度输出带来的挑战，强调需要重新思考RAI方法。", "motivation": "现代通用AI系统虽然功能强大，但存在幻觉、毒性和刻板印象等风险，使其不可信。与传统任务特定AI相比，这些风险更严重且难以缓解。", "method": "基于八个负责任AI原则（公平性、隐私、可解释性等）分析通用AI的风险和脆弱性，提出C2V2（控制、一致性、价值、真实性）需求框架。", "result": "发现通用AI的高自由度输出是风险根源，需要应用依赖的RAI建模和系统设计方法结合多种技术来满足C2V2需求。", "conclusion": "通过正式建模应用特定的RAI需求并采用系统设计方法，可以实现负责任通用AI的开发目标。"}}
{"id": "2601.12430", "pdf": "https://arxiv.org/pdf/2601.12430", "abs": "https://arxiv.org/abs/2601.12430", "authors": ["Tsan Tsai Chan", "Varsha Suresh", "Anisha Saha", "Michael Hahn", "Vera Demberg"], "title": "System-Mediated Attention Imbalances Make Vision-Language Models Say Yes", "categories": ["cs.CL"], "comment": "Under review", "summary": "Vision-language model (VLM) hallucination is commonly linked to imbalanced allocation of attention across input modalities: system, image and text. However, existing mitigation strategies tend towards an image-centric interpretation of these imbalances, often prioritising increased image attention while giving less consideration to the roles of the other modalities. In this study, we evaluate a more holistic, system-mediated account, which attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs. We show that this framework offers a useful empirical perspective on the yes-bias, a common form of hallucination in which VLMs indiscriminately respond 'yes'. Causally redistributing attention from the system modality to image and textual inputs substantially suppresses this bias, often outperforming existing approaches. We further present evidence suggesting that system-mediated attention imbalances contribute to the yes-bias by encouraging a default reliance on coarse input representations, which are effective for some tasks but ill-suited to others. Taken together, these findings firmly establish system attention as a key factor in VLM hallucination and highlight its potential as a lever for mitigation.", "AI": {"tldr": "该研究发现视觉语言模型(VLM)的幻觉问题源于系统模态的冗余权重导致图像和文本输入注意力分配失衡，通过重新分配系统注意力可有效抑制yes-bias幻觉。", "motivation": "现有缓解VLM幻觉的方法过于关注图像注意力而忽视其他模态的作用，需要更全面的系统介导视角来解释注意力失衡问题。", "method": "采用系统介导的理论框架，通过因果性地重新分配系统模态的注意力到图像和文本输入，评估对yes-bias幻觉的抑制效果。", "result": "重新分配系统注意力能显著抑制yes-bias，效果优于现有方法，系统注意力失衡导致模型过度依赖粗糙输入表示。", "conclusion": "系统注意力是VLM幻觉的关键因素，可作为有效的缓解杠杆，需要更平衡的多模态注意力分配策略。"}}
{"id": "2601.13186", "pdf": "https://arxiv.org/pdf/2601.13186", "abs": "https://arxiv.org/abs/2601.13186", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching", "categories": ["cs.AI", "cs.MA"], "comment": "33 pages, 19 figures", "summary": "Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.", "AI": {"tldr": "本文扩展了TIVS评估框架，提出TIVS-O系统，通过语义缓存和可观测性评分在多智能体环境中实现零高风险漏洞，同时减少41.6%的LLM调用并优化安全性与透明度的权衡。", "motivation": "提示注入是大型语言模型安全部署的主要障碍，特别是在多智能体环境中，中间输出可能传播或放大恶意指令。需要同时解决安全性和可审计性的平衡问题。", "method": "基于四度量TIVS框架，引入语义相似性缓存和第五个度量(可观测性评分比)，在HOPE启发的嵌套学习架构中构建包含连续记忆系统的多智能体管道，使用301个合成生成的攻击提示进行测试。", "result": "系统实现零高风险漏洞的安全响应，语义缓存减少41.6%的LLM调用，降低延迟、能耗和碳排放，五种TIVS-O配置展示了缓解严格性与取证透明度之间的最优权衡。", "conclusion": "可观测性感知评估能揭示多智能体管道内的非单调效应，记忆增强智能体可在不修改模型权重的情况下同时最大化安全鲁棒性、实时性能、运营成本节约和环境可持续性，为安全绿色LLM部署提供生产就绪路径。"}}
{"id": "2601.12465", "pdf": "https://arxiv.org/pdf/2601.12465", "abs": "https://arxiv.org/abs/2601.12465", "authors": ["Miao Peng", "Weizhou Shen", "Nuo Chen", "Chenliang Li", "Ming Yan", "Jia Li"], "title": "Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the \"almost-there\" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from \"almost-there\" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.", "AI": {"tldr": "论文提出DeepReasonQA框架和LongPAS方法，解决RLVR在长上下文推理中的性能下降问题，通过构建高难度多跳问答对和细粒度信用分配，显著提升长上下文推理能力。", "motivation": "RLVR在短上下文推理中有效，但在需要精确基础和长距离推理的长上下文场景中性能下降，存在\"almost-there\"现象（轨迹基本正确但最后一步失败）。", "method": "1. DeepReasonQA：基于知识图谱的可控合成框架，构建高难度多跳长上下文问答对；2. LongPAS：长上下文过程优势塑造方法，通过有效性和相关性维度评估推理步骤，进行细粒度信用分配。", "result": "在三个长上下文推理基准测试中大幅超越RLVR基线，与前沿LLMs性能相当但参数量更少，有效强化长上下文推理能力并保持RL训练稳定性。", "conclusion": "该方法成功解决了长上下文推理中的关键瓶颈，通过高质量数据合成和细粒度奖励设计，显著提升了LLMs在复杂长上下文任务中的表现。"}}
{"id": "2601.13206", "pdf": "https://arxiv.org/pdf/2601.13206", "abs": "https://arxiv.org/abs/2601.13206", "authors": ["Neil K. R. Sehgal", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\\% vs. 4\\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\\geq$95\\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.", "AI": {"tldr": "研究发现大型语言模型在实时时间限制下表现不佳，缺乏时间意识，但在回合制限制下表现优异，揭示了其时间追踪而非策略推理的缺陷。", "motivation": "研究动机是探索LLMs在实时截止时间约束下的时间意识能力，因为现实世界中的沟通（如治疗会话和商业谈判）都依赖于连续时间约束，而当前的LLM架构和评估协议很少测试这种实时时间意识。", "method": "通过模拟严格截止时间下的配对代理谈判实验，设置控制条件（仅知全局时间限制）和时间感知条件（每回合接收剩余时间更新），比较两种条件下的交易达成率和报价接受率。", "result": "时间感知条件下的交易达成率显著更高（GPT-5.1为32% vs 4%），报价接受率高出六倍，但在回合制限制下LLMs能达到近乎完美的交易达成率（≥95%）。", "conclusion": "LLMs在时间追踪方面存在系统性缺陷，这限制了其在许多时间敏感应用中的部署，失败源于时间追踪能力而非战略推理能力。"}}
{"id": "2601.12471", "pdf": "https://arxiv.org/pdf/2601.12471", "abs": "https://arxiv.org/abs/2601.12471", "authors": ["Sravanthi Machcha", "Sushrita Yerra", "Sahil Gupta", "Aishwarya Sahoo", "Sharmin Sultana", "Hong Yu", "Zonghai Yao"], "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors; To appear in proceedings of the Main Conference of the European Chapter of the Association for Computational Linguistics (EACL) 2026", "summary": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.", "AI": {"tldr": "MedAbstain是一个用于医学多选题问答中拒绝回答的基准测试框架，发现即使最先进的LLM在不确定时也难以主动拒绝回答，而提供明确的拒绝选项比输入扰动更能提高安全性", "motivation": "当前LLM评估过于关注准确性，但在现实世界和安全关键应用中，模型在不确定时能够拒绝回答对于可信部署同样重要", "method": "引入MedAbstain统一基准，整合了共形预测、对抗性问题扰动和明确的拒绝选项，系统评估开源和闭源LLM", "result": "即使高精度模型也经常在不确定时无法拒绝回答，提供明确拒绝选项比输入扰动更能增加模型不确定性和安全拒绝，模型规模扩展或高级提示技术改进有限", "conclusion": "拒绝机制对于可信LLM部署至关重要，为高风险应用中的安全性改进提供了实用指导"}}
{"id": "2601.13233", "pdf": "https://arxiv.org/pdf/2601.13233", "abs": "https://arxiv.org/abs/2601.13233", "authors": ["Bolin Chen", "Dex Doksoo Lee", "Wei \"Wayne'' Chen", "Wei Chen"], "title": "RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Response Requirements", "categories": ["cs.AI", "cs.CE"], "comment": null, "summary": "Metamaterials design for advanced functionality often entails the inverse design on nonlinear and condition-dependent responses (e.g., stress-strain relation and dispersion relation), which are described by continuous functions. Most existing design methods focus on vector-valued responses (e.g., Young's modulus and bandgap width), while the inverse design of functional responses remains challenging due to their high-dimensionality, the complexity of accommodating design requirements in inverse-design frameworks, and non-existence or non-uniqueness of feasible solutions. Although generative design approaches have shown promise, they are often data-hungry, handle design requirements heuristically, and may generate infeasible designs without uncertainty quantification. To address these challenges, we introduce a RAndom-forest-based Generative approach (RAG). By leveraging the small-data compatibility of random forests, RAG enables data-efficient predictions of high-dimensional functional responses. During the inverse design, the framework estimates the likelihood through the ensemble which quantifies the trustworthiness of generated designs while reflecting the relative difficulty across different requirements. The one-to-many mapping is addressed through single-shot design generation by sampling from the conditional likelihood. We demonstrate RAG on: 1) acoustic metamaterials with prescribed partial passbands/stopbands, and 2) mechanical metamaterials with targeted snap-through responses, using 500 and 1057 samples, respectively. Its data-efficiency is benchmarked against neural networks on a public mechanical metamaterial dataset with nonlinear stress-strain relations. Our framework provides a lightweight, trustworthy pathway to inverse design involving functional responses, expensive simulations, and complex design requirements, beyond metamaterials.", "AI": {"tldr": "提出RAG方法，基于随机森林实现数据高效的函数响应逆设计，解决高维功能响应设计的挑战", "motivation": "现有设计方法主要关注向量值响应，而函数响应的逆设计面临高维度、设计需求复杂、解不存在或不唯一等挑战，生成式方法数据需求大且缺乏不确定性量化", "method": "基于随机森林的生成方法(RAG)，利用随机森林的小数据兼容性预测高维函数响应，通过集成估计生成设计的可信度，使用条件似然采样实现单次设计生成", "result": "在声学超材料和力学超材料上验证，分别使用500和1057个样本实现指定频带和snap-through响应设计，数据效率优于神经网络", "conclusion": "RAG为涉及函数响应、昂贵仿真和复杂设计需求的逆设计提供了轻量级、可信赖的解决方案，适用于超材料及其他领域"}}
{"id": "2601.12473", "pdf": "https://arxiv.org/pdf/2601.12473", "abs": "https://arxiv.org/abs/2601.12473", "authors": ["Renlong Jie", "Chen Chu", "Zhen Wang"], "title": "Capability-Aware Early-Stage Research Idea Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Predicting the outcomes of research ideas at their conceptual stage (i.e. before significant resources are committed) holds great potential for optimizing scientific resource allocation and research planning. While existing methods rely heavily on finished manuscripts or peer reviews, we propose a novel capability-aware framework that predicts paper acceptance and ratings using only author information and research ideas, without requiring full text or experimental results. Our approach integrates author information, (inferred) capability presentation, and research ideas through a three-way transformer architecture with flexible fusion mechanisms. We also introduce a two-stage architecture for learning the capability representation given the author information and idea. Experiments show that our method significantly outperform the single-way models by finetuning bert-base and bert-large, and the capability predicting significantly increase the predictive accuracy of the final model. The proposed method can be applied in both early-stage research outcome prediction and scientific resource allocation.", "AI": {"tldr": "提出一个基于作者信息和研究想法的能力感知框架，仅使用作者信息和研究想法（无需全文或实验结果）就能预测论文接受度和评分，通过三路transformer架构实现显著性能提升。", "motivation": "在概念阶段预测研究成果（在投入大量资源之前）对于优化科学资源分配和研究规划具有巨大潜力，现有方法过度依赖已完成的手稿或同行评审。", "method": "使用三路transformer架构整合作者信息、（推断的）能力呈现和研究想法，采用灵活融合机制，并引入两阶段架构学习给定作者信息和想法的能力表示。", "result": "实验显示该方法显著优于基于bert-base和bert-large的单路模型微调结果，能力预测显著提高了最终模型的预测准确性。", "conclusion": "该方法可应用于早期研究成果预测和科学资源分配，为研究评估提供了新的视角和工具。"}}
{"id": "2601.13262", "pdf": "https://arxiv.org/pdf/2601.13262", "abs": "https://arxiv.org/abs/2601.13262", "authors": ["Eric Onyame", "Akash Ghosh", "Subhadip Baidya", "Sriparna Saha", "Xiuying Chen", "Chirag Agarwal"], "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/", "AI": {"tldr": "CUREMED-BENCH是一个多语言医疗推理数据集，CURE-MED框架通过课程强化学习提升LLM在多语言医疗推理中的逻辑正确性和语言稳定性，在13种语言上表现优异。", "motivation": "大型语言模型在单语数学和常识推理上表现良好，但在多语言医疗推理应用中不可靠，阻碍了在多语言医疗环境中的部署。", "method": "提出CUREMED-BENCH多语言医疗推理数据集，并开发CURE-MED框架，整合代码切换感知的监督微调和组相对策略优化，采用课程强化学习方法。", "result": "在13种语言上持续优于强基线，7B参数达到85.21%语言一致性和54.35%逻辑正确性，32B参数达到94.96%语言一致性和70.04%逻辑正确性。", "conclusion": "该方法支持LLM实现可靠和公平的多语言医疗推理，代码和数据集已公开。"}}
{"id": "2601.12505", "pdf": "https://arxiv.org/pdf/2601.12505", "abs": "https://arxiv.org/abs/2601.12505", "authors": ["Ashish Raj Shekhar", "Shiven Agarwal", "Priyanuj Bordoloi", "Yash Shah", "Tejas Anvekar", "Vivek Gupta"], "title": "DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.", "AI": {"tldr": "DoPE是一个针对多模态大语言模型的文档层防御框架，通过在PDF/HTML考试文档中嵌入语义诱饵来防止和检测AI作弊，无需依赖传统分类器，在Integrity-Bench基准测试中取得了显著效果。", "motivation": "多模态大语言模型能够直接处理考试文档，威胁传统评估方式和学术诚信，需要开发有效的防御机制来保护学术评估的完整性。", "method": "提出DoPE框架，包含FewSoRT-Q（生成问题级语义诱饵）和FewSoRT-D（将诱饵封装到水印文档中），利用MLLM管道中的渲染-解析差异来实施防御。", "result": "在Integrity-Bench基准测试中，对OpenAI和Anthropic的黑盒MLLM实现了91.4%的检测率（误报率8.7%），96.3%的尝试被阻止成功完成或诱导产生诱饵对齐的失败。", "conclusion": "DoPE提供了一种模型无关的防御方法，能够有效防止和检测AI作弊，为学术诚信保护提供了实用的文档层防御解决方案，相关工具和代码已开源以促进可重复研究。"}}
{"id": "2601.13268", "pdf": "https://arxiv.org/pdf/2601.13268", "abs": "https://arxiv.org/abs/2601.13268", "authors": ["Zainab Ghafoor", "Md Shafiqul Islam", "Koushik Howlader", "Md Rasel Khondokar", "Tanusree Bhattacharjee", "Sayantan Chakraborty", "Adrito Roy", "Ushashi Bhattacharjee", "Tirtho Roy"], "title": "Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.", "AI": {"tldr": "提出多智能体精炼框架，通过迭代对齐提升医疗大语言模型的安全性和可靠性，使用DeepSeek R1和Med-PaLM生成模型，LLaMA 3.1和Phi-4评估模型，基于AMA医学伦理原则和五级安全风险评估协议，在900个临床查询中验证有效性。", "motivation": "医疗大语言模型在临床部署中存在伦理完整性和安全合规性的重大障碍，需要确保其安全可靠地应用于医疗场景。", "method": "采用多智能体精炼框架，结合两个生成模型(DeepSeek R1和Med-PaLM)和两个评估模型(LLaMA 3.1和Phi-4)，使用AMA医学伦理原则和SRA-5协议进行迭代评估和精炼。", "result": "DeepSeek R1收敛更快(平均2.34次迭代)，Med-PaLM在隐私敏感场景表现更优；多智能体循环减少89%伦理违规和92%风险降级率。", "conclusion": "该研究提出了一个可扩展、符合监管要求且成本效益高的医疗AI安全治理范式，有效提升医疗大语言模型的安全性和可靠性。"}}
{"id": "2601.12535", "pdf": "https://arxiv.org/pdf/2601.12535", "abs": "https://arxiv.org/abs/2601.12535", "authors": ["Ahmed Attia", "Alham Fikri"], "title": "Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.", "AI": {"tldr": "该论文提出了一种基于自监督强化学习的低资源机器翻译微调方法，通过NLLB模型进行往返翻译并使用chrF++和BLEU作为奖励函数，在多种低资源语言上取得了翻译质量的提升。", "motivation": "低资源机器翻译虽然受到越来越多的关注，但许多改进方法仍未得到充分探索。研究者希望开发一种有效的方法来提升低资源语言的翻译质量。", "method": "使用基于自监督强化学习的微调方法，通过NLLB模型进行英语到目标低资源语言的往返翻译，以chrF++和BLEU组合作为重建英语句子的奖励函数。", "result": "在NLLB-MD数据集上评估600M和1.3B参数的NLLB模型，在Central Aymara、Friulian、Wolof和Russian等语言上观察到一致的改进。定性分析显示翻译输出的流畅性和语义保真度有所提高。", "conclusion": "该方法能够进一步从规模中受益，使模型能够更好地利用其预训练知识并持续自我改进，为低资源机器翻译提供了有效的解决方案。"}}
{"id": "2601.13327", "pdf": "https://arxiv.org/pdf/2601.13327", "abs": "https://arxiv.org/abs/2601.13327", "authors": ["Po-Yu Liang", "Tobo Duran", "Jun Bai"], "title": "PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion", "categories": ["cs.AI"], "comment": null, "summary": "We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model", "AI": {"tldr": "PepEDiff是一个新颖的肽结合剂生成器，直接在预训练蛋白质嵌入模型的连续潜在空间中生成结合序列，无需依赖结构预测，提高了序列多样性并在TIGIT靶点上优于现有方法。", "motivation": "现有肽结合剂生成方法严重依赖中间结构预测，增加了复杂性并限制了序列多样性，需要一种不依赖结构预测的直接生成方法。", "method": "使用预训练蛋白质嵌入模型构建连续潜在空间，通过潜在空间探索和基于扩散的采样生成肽序列，避免记忆已知序列并捕获结合相关特征。", "result": "在TIGIT（具有大而平坦蛋白质-蛋白质相互作用界面的挑战性靶点）的案例研究中，该方法在基准测试中优于最先进的方法。", "conclusion": "PepEDiff作为一个通用的、无需结构的零样本肽结合剂设计框架具有巨大潜力，能够在新颖的蛋白质空间区域提出肽序列。"}}
{"id": "2601.12549", "pdf": "https://arxiv.org/pdf/2601.12549", "abs": "https://arxiv.org/abs/2601.12549", "authors": ["Ilia Badanin", "Daniil Dzenhaliou", "Imanol Schlag"], "title": "Benchmarking Concept-Spilling Across Languages in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.", "AI": {"tldr": "本文提出了一个评估多语言大模型语义鲁棒性的比较框架，通过系统测量模型处理多语言多义词的能力，揭示了语言溢出现象，并建立了模型性能的排序系统。", "motivation": "多语言大模型在跨语言能力上表现出色，但存在系统性偏向其他语言表示的问题，导致在非英语语言生成内容时产生语义干扰（语言溢出现象）。", "method": "采用结构化含义生成任务，在九种语言中评估开源和闭源多语言LLMs，使用精心筛选的100个高多义性英语词汇基准，通过要求模型生成恰好五个含义来测量性能。", "result": "研究发现不同模型和语言间的语义鲁棒性存在显著差异，语义更强的模型在生成序列后期才转向主导语言的含义，而较弱模型则更早转向。", "conclusion": "贡献了一个可扩展的多语言语义评估比较基准和严谨的验证流程，为开发更语言平衡的AI系统提供了关键工具。"}}
{"id": "2601.13358", "pdf": "https://arxiv.org/pdf/2601.13358", "abs": "https://arxiv.org/abs/2601.13358", "authors": ["Samuel Cyrenius Anderson"], "title": "The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": "34 pages, 10 figures", "summary": "Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.", "AI": {"tldr": "论文发现神经网络规模扩大不会均匀提升推理能力，而是重构推理结构。通过分析25,000+思维链轨迹，揭示了领域特定的相变现象：法律推理呈现结晶化，科学数学推理保持液态，代码推理形成离散晶格结构。几何结构可预测可学习性，并提出了神经推理算子实现推理加速。", "motivation": "研究神经网络规模扩大对推理能力的影响机制，探索不同领域推理任务在模型规模扩展时的结构性变化，而非简单的性能提升。", "method": "分析了25,000多个思维链轨迹，覆盖法律、科学、代码、数学四个领域，使用8B和70B两种参数规模的模型。引入神经推理算子作为从初始隐藏状态到终端隐藏状态的学习映射。", "result": "法律推理维度降低45%，轨迹对齐提升31%，流形解缠10倍；科学数学推理几何不变；代码推理形成离散策略模式。神经推理算子在法律推理上达到63.6%的准确率，发现了跨域通用的振荡特征。", "conclusion": "思维成本由流形几何决定而非任务难度，为推理加速提供了基于拓扑结构的蓝图，揭示了注意力层和前馈层通过对立动力学驱动推理的机制。"}}
{"id": "2601.12555", "pdf": "https://arxiv.org/pdf/2601.12555", "abs": "https://arxiv.org/abs/2601.12555", "authors": ["Yihong Liu", "Bingyu Xiong", "Hinrich Schütze"], "title": "Evaluating Contextually Mediated Factual Recall in Multilingual Large Language Models", "categories": ["cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) can recall a wide range of factual knowledge across languages. However, existing factual recall evaluations primarily assess fact retrieval in isolation, where the queried entity is explicitly named and the fact is requested directly. In natural language use, facts are often accessed through context, where the relevant entity is introduced only indirectly. In this work, we study contextually mediated factual recall, asking whether LLMs can reliably retrieve factual knowledge when the target entity is embedded in a naturalistic context rather than queried explicitly, across languages. We construct controlled prompts that preserve the underlying fact while introducing referential mediation through contextual sentences. To disentangle contextual effects from name-specific associations, we further compare performance using synthetic names and real names across languages. Evaluating multiple model families in five languages, we find that contextual mediation consistently degrades factual recall, with substantial variation across relations. Larger models are more robust to contextual mediation, exhibiting a reduced performance gap relative to direct queries, while the effect of real names and name origin is mixed and unsystematic. These findings highlight a gap between isolated factual recall and context-dependent language understanding in multilingual LLMs.", "AI": {"tldr": "该研究探讨了多语言大语言模型在上下文语境中而非直接查询时的实体事实回忆能力，发现上下文会降低事实回忆准确性，但更大模型对此更具鲁棒性。", "motivation": "现有事实回忆评估主要针对孤立事实检索，而自然语言使用中事实往往通过上下文间接获取，需要研究模型在语境化场景中的表现。", "method": "构建保留基础事实但通过上下文句子引入指代中介的受控提示，使用合成名称和真实名称比较性能，在五种语言中评估多个模型家族。", "result": "上下文中介持续降低事实回忆准确性，不同关系间存在显著差异；更大模型对上下文中介更具鲁棒性，真实名称和名称来源的影响混合且无系统性。", "conclusion": "多语言大语言模型在孤立事实回忆和上下文依赖语言理解之间存在差距，需要改进模型在自然语境中的事实检索能力。"}}
{"id": "2601.13383", "pdf": "https://arxiv.org/pdf/2601.13383", "abs": "https://arxiv.org/abs/2601.13383", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "title": "A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge", "categories": ["cs.AI"], "comment": "15 pages, 3 figures", "summary": "The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.", "AI": {"tldr": "AgentForge是一个轻量级开源Python框架，通过模块化架构简化LLM驱动的自主代理开发，提供可组合技能抽象、统一LLM后端接口和声明式配置系统，显著减少开发时间并保持高性能。", "motivation": "现有代理框架存在架构僵化、供应商锁定和复杂性问题，阻碍了快速原型设计和部署，需要一种更灵活、易用的解决方案。", "method": "提出三个核心创新：1) 可组合技能抽象与形式化输入输出契约；2) 统一LLM后端接口支持云端API和本地推理引擎；3) 基于YAML的声明式配置系统。将技能组合机制形式化为有向无环图(DAG)。", "result": "在四个基准场景中，AgentForge达到竞争性任务完成率，相比LangChain减少62%开发时间，相比直接API集成减少78%开发时间，编排延迟低于100ms。", "conclusion": "AgentForge为研究者和实践者提供了生产就绪的基础设施，可在不牺牲灵活性或性能的情况下构建、评估和部署自主代理，填补了LLM代理生态系统的关键空白。"}}
{"id": "2601.12607", "pdf": "https://arxiv.org/pdf/2601.12607", "abs": "https://arxiv.org/abs/2601.12607", "authors": ["Anurag Acharya", "Timothy Vega", "Rizwan A. Ashraf", "Anshu Sharma", "Derek Parker", "Robert Rallo"], "title": "A Cloud-based Multi-Agentic Workflow for Science", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.", "AI": {"tldr": "该研究提出了一个领域无关、模型独立的智能体框架，作为科学助手在云端运行，能够处理从文献综述到复杂模拟等任务，在合成任务中成功率达97.5%，真实任务中达91%。", "motivation": "大型语言模型缺乏执行复杂任务（如运行模拟或做出复杂决策）的能力，限制了其实用性。智能体系统虽能调用外部资源，但设计平衡模型、云提供商和外部资源的工作流程非常具有挑战性。", "method": "构建了一个监督智能体统领多个具有独立能力的智能体的框架，包括概念验证系统用于加速催化剂研究，详细描述了框架结构并报告了运营成本。", "result": "系统能够90%的时间将任务路由到正确的智能体，合成任务成功完成率达97.5%，真实任务达91%，准确性与大多数前沿模型相当或更好。", "conclusion": "这是一个可行的框架，可供其他科学领域复制使用，展示了云端智能体系统在科学辅助方面的实用价值。"}}
{"id": "2601.13443", "pdf": "https://arxiv.org/pdf/2601.13443", "abs": "https://arxiv.org/abs/2601.13443", "authors": ["Héctor Manuel Manzanilla-Granados", "Zaira Navarrete-Cazales", "Miriam Pescador-Rojas", "Tonahtiu Ramírez-Romero"], "title": "Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models", "categories": ["cs.AI"], "comment": "Preprint. This version corresponds to the initial public release of the CUA architecture and associated evaluation metrics", "summary": "The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.\n  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.\n  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.", "AI": {"tldr": "论文提出显性认知分配原则和认知通用代理架构，通过分离和协调认知功能来改善AI辅助推理的结构性，相比传统LLM推理在认知收敛、对齐和工具性透明度方面表现更优", "motivation": "当前大语言模型的使用缺乏认知结构，将问题构建、知识探索、方法意识等功能混为一体，限制了可追溯性、认知控制和可复现性，特别是在高责任场景中", "method": "引入显性认知分配原则，开发认知通用代理(CUA)架构，将推理分为探索与框架、认知锚定、工具方法映射和解释合成四个阶段，使用通用认知工具(UCIs)形式化各种调查手段", "result": "在农业领域的多组提示测试中，CUA推理显示出更早的结构化认知收敛、更高的语义扩展对齐度，以及系统性地暴露调查工具景观，而基线LLM推理在一致性和工具结构显性化方面表现较差", "conclusion": "显性认知分配和工具分配能显著提升AI辅助推理的结构性和透明度，为高责任环境中的可靠AI推理提供了有效框架"}}
{"id": "2601.12618", "pdf": "https://arxiv.org/pdf/2601.12618", "abs": "https://arxiv.org/abs/2601.12618", "authors": ["Elham Tajik", "Conrad Borchers", "Bahar Shahrokhian", "Sebastian Simon", "Ali Keramati", "Sonika Pal", "Sreecharan Sankaranarayanan"], "title": "Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems", "categories": ["cs.CL"], "comment": "LAK 2026 conference paper, 7 pages", "summary": "Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.", "AI": {"tldr": "本研究提出利用LLM多智能体系统的推理轨迹作为新型过程数据，通过余弦相似度量化智能体间的分歧，将分歧重新定义为有价值的分析信号，从而提高质性编码的解释实践和方法论严谨性。", "motivation": "随着生成式AI的兴起，自动化分析流程缺乏方法学标准指导，需要新的方法来增强质性数据分析的解释实践和可靠性建立。", "method": "使用LLM多智能体系统生成推理轨迹，应用余弦相似度分析近10,000个智能体对编码人类辅导对话的实例，系统检测、量化和解释智能体间的分歧。", "result": "LLM智能体的语义推理相似度能稳健区分共识与分歧，并与人类编码可靠性相关；质性分析揭示了代码内的细微教学子功能和概念代码本改进机会。", "conclusion": "通过整合定量相似度指标与质性审查，该方法能通过揭示解释歧义来改进和加速编码过程中的评分者间信度建立，特别在LLM与人类协作时，推理轨迹分歧代表了教育研究方法论严谨性和解释深度的有价值新型分析信号。"}}
{"id": "2601.13462", "pdf": "https://arxiv.org/pdf/2601.13462", "abs": "https://arxiv.org/abs/2601.13462", "authors": ["Amine Rostane"], "title": "SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation", "categories": ["cs.AI"], "comment": "19 pages, includes figures and tables", "summary": "Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.", "AI": {"tldr": "SpatialBench-UC是一个用于评估文本到图像模型空间关系理解能力的小型可复现基准测试，包含200个成对空间关系提示，通过选择性预测和置信度报告机制来解决自动化评估中的模糊性问题。", "motivation": "文本到图像模型的空间指令遵循评估难以自动化，因为目标检测器可能漏检目标或返回多个可能检测结果，简单的几何测试在边界情况下会变得模糊不清。", "method": "开发了SpatialBench-UC基准测试，包含200个提示（50个对象对×4种关系），分组为100个通过交换对象角色获得的反事实对。采用选择性预测方法，当证据较弱时检查器可以弃权，并报告置信度。", "result": "评估了三个基线模型（Stable Diffusion 1.5、SD 1.5 BoxDiff和SD 1.4 GLIGEN），接地方法显著提高了通过率和覆盖率，但弃权仍然是主要因素，主要原因是目标检测缺失。", "conclusion": "空间评估本质上是一个选择性预测问题，需要将结果解释为风险覆盖权衡而非单一分数。该基准测试提供了版本化提示、固定配置和可审计比较，支持模型间的可复现评估。"}}
{"id": "2601.12632", "pdf": "https://arxiv.org/pdf/2601.12632", "abs": "https://arxiv.org/abs/2601.12632", "authors": ["Kriti Bhattarai", "Vipina K. Keloth", "Donald Wright", "Andrew Loza", "Yang Ren", "Hua Xu"], "title": "BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.\n  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.\n  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.\n  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.", "AI": {"tldr": "BioPulse-QA是一个新的生物医学问答基准，包含2280个专家验证的QA对，用于评估LLMs在新发布的生物医学文档上的表现，解决了现有基准的局限性。", "motivation": "现有生物医学基准存在局限性：使用静态过时数据、无法捕捉动态丰富的生物医学知识、存在数据泄露风险、忽视语言变异鲁棒性和人口统计偏差等关键维度。", "method": "从新发布的药物标签、试验方案和临床指南等生物医学文档中构建QA对，包含提取式和抽象式问题格式，并评估了GPT-4o、GPT-o1、Gemini-2.0-Flash和LLaMA-3.1 8B Instruct四个LLM。", "result": "GPT-o1在药物标签上获得最高F1分数(0.92)，Gemini-2.0-Flash次之(0.90)；临床试验是最具挑战性的来源，提取式F1分数低至0.36；改写比拼写错误带来的性能差异更大，偏差测试显示差异可忽略。", "conclusion": "BioPulse-QA为评估生物医学LLMs提供了一个可扩展且临床相关的框架，能够更好地评估模型在动态生物医学环境中的表现。"}}
{"id": "2601.13464", "pdf": "https://arxiv.org/pdf/2601.13464", "abs": "https://arxiv.org/abs/2601.13464", "authors": ["Chongyang Gao", "Marco Postiglione", "Julian Baldwin", "Natalia Denisenko", "Isabel Gortner", "Luke Fosdick", "Chiara Pulice", "Sarit Kraus", "V. S. Subrahmanian"], "title": "Context and Transcripts Improve Detection of Deepfake Audios of Public Figures", "categories": ["cs.AI", "cs.SD"], "comment": null, "summary": "Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).", "AI": {"tldr": "论文提出了一种基于上下文的音频深度伪造检测器(CADD)，通过整合上下文信息和文字转录显著提升了检测性能，对对抗性攻击具有更强的鲁棒性。", "motivation": "当前音频深度伪造检测器仅分析音频文件本身，忽视了上下文和文字转录信息，而人类在判断信息真实性时会使用上下文。", "method": "创建了记者提供的深度伪造数据集(JDD)和合成音频数据集(SYN)，提出了新颖的CADD架构，并在ITW和P$^2$V两个大规模数据集上评估性能。", "result": "使用上下文和/或文字转录可将多种基线检测器的F1分数提升5%-37.58%，AUC提升3.77%-42.79%，EER提升6.17%-47.83%。CADD对5种对抗性规避策略的平均性能下降仅为-0.71%。", "conclusion": "充分的上文和文字转录信息能显著提高音频深度伪造检测器的效能，CADD架构通过整合这些信息实现了更鲁棒的检测性能。"}}
{"id": "2601.12639", "pdf": "https://arxiv.org/pdf/2601.12639", "abs": "https://arxiv.org/abs/2601.12639", "authors": ["Daniel Vennemeyer", "Punya Syon Pandey", "Phan Anh Duong", "Michael Umeokoli", "Samuel Ratnam"], "title": "Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.", "AI": {"tldr": "本文通过控制实验比较了六种微调目标对LLM安全性和能力的影响，发现在小规模训练时各目标安全性相似但能力不同，大规模训练时监督学习和偏好优化会显著增加对抗脆弱性，而ORPO和KL正则化能有效缓解这一问题", "motivation": "现有研究对微调目标如何影响LLM对齐性和对抗鲁棒性的直接分析有限，需要系统比较不同微调目标在安全结果形成中的作用", "method": "采用控制变量实验设计，固定数据、领域、架构和优化参数，比较六种微调目标：监督微调、直接偏好优化、条件微调、接种提示、几率比偏好优化和KL正则化微调，在封闭式推理和开放式生成任务上进行评估", "result": "目标选择在安全-能力前沿上引起系统性、规模依赖的变化：小训练预算时各目标鲁棒性相似但能力不同；大预算时目标差异显著，监督和偏好优化使能力增益与对抗脆弱性紧密关联，而ORPO和KL正则化能显著减轻这两种问题", "conclusion": "微调目标在小规模时对安全性影响不大，但随着训练规模增加，成为对抗鲁棒性和潜在角色稳定性的主要驱动因素，选择合适的约束性学习信号目标至关重要"}}
{"id": "2601.13465", "pdf": "https://arxiv.org/pdf/2601.13465", "abs": "https://arxiv.org/abs/2601.13465", "authors": ["Yimeng Min", "Carla P. Gomes"], "title": "Graph Neural Networks are Heuristics", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making. At inference time, dropout and snapshot ensembling allow a single model to act as an implicit ensemble, reducing optimality gaps through increased solution diversity. Our results establish that graph neural networks do not require supervised training nor explicit search to be effective. Instead, they can internalize global combinatorial structure and function as strong, learned heuristics. This reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics.", "AI": {"tldr": "该论文展示了一个图神经网络通过单次训练轨迹就能成为组合优化的无监督启发式算法，针对旅行商问题，通过编码全局结构约束作为归纳偏置，实现无需搜索、监督或序列决策的直接前向推理生成解。", "motivation": "探索图神经网络是否可以不依赖监督训练或显式搜索，而是通过内部化全局组合结构来直接作为强大的学习启发式算法，重新定义学习在组合优化中的作用。", "method": "使用图神经网络编码全局结构约束作为归纳偏置，采用非自回归模型通过直接前向传递生成解。在推理时使用dropout和快照集成技术，使单一模型作为隐式集成，通过增加解多样性来减少最优性差距。", "result": "证明了图神经网络不需要监督训练或显式搜索就能有效工作，能够内部化全局组合结构并作为强大的学习启发式算法。", "conclusion": "这项研究重新定义了学习在组合优化中的角色：从增强经典算法转变为直接实例化新的启发式算法，展示了图神经网络作为无监督启发式的潜力。"}}
{"id": "2601.12648", "pdf": "https://arxiv.org/pdf/2601.12648", "abs": "https://arxiv.org/abs/2601.12648", "authors": ["Nafiz Imtiaz Khan", "Kylie Cleland", "Vladimir Filkov", "Roger Eric Goldman"], "title": "Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?", "categories": ["cs.CL", "cs.AI"], "comment": "51 pages, 12 figures, 8 tables. Feasibility study using retrospective radiology reports. Submitted to JAMIA Open (under review)", "summary": "Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.", "AI": {"tldr": "本研究评估使用大型语言模型(LLM)从放射学报告中自动提取结构化手术日志信息的可行性，结果显示本地和商业模型都能达到接近0.87的F1分数，显著减轻学员文书负担。", "motivation": "放射学培训中的手术病例日志记录耗时且人工撰写容易不一致，需要自动化解决方案来提高效率和一致性。", "method": "使用基于指令和思维链提示的多种本地和商业LLM，从414份介入放射学报告中提取结构化手术信息，评估敏感度、特异度、F1分数、推理延迟和标记效率。", "result": "本地和商业模型都表现出强大的提取性能，最佳F1分数接近0.87，在速度和成本之间展现出不同的权衡。", "conclusion": "LLM自动化有潜力显著减轻学员文书负担并提高病例记录一致性，证明了AI辅助文档在医学教育中的可行性，需要跨机构和临床工作流程进一步验证。"}}
{"id": "2601.13481", "pdf": "https://arxiv.org/pdf/2601.13481", "abs": "https://arxiv.org/abs/2601.13481", "authors": ["Jian Zhang", "Zhangqi Wang", "Zhiyuan Wang", "Weiping Fu", "Yu He", "Haiping Zhu", "Qika Lin", "Jun Liu"], "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement", "categories": ["cs.AI"], "comment": null, "summary": "Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.", "AI": {"tldr": "APOLO是一个自动提示优化框架，通过多智能体协作机制系统探索更广更细粒度的提示空间，显著提升语言模型在心理健康情感诊断中的准确性和鲁棒性。", "motivation": "现有方法在临床情感诊断中面临两个关键挑战：情感共病（多种情绪状态交织使预测复杂化）和临床相关线索的低效探索，而大语言模型的诊断可靠性对提示设计高度敏感。", "method": "将指令优化建模为部分可观测马尔可夫决策过程，采用包含Planner、Teacher、Critic、Student、Target角色的多智能体协作机制，通过闭环框架迭代优化提示。", "result": "实验结果表明APOLO在领域特定和分层基准测试中 consistently 提高了诊断准确性和鲁棒性。", "conclusion": "APOLO为心理健康护理中可信赖的大语言模型应用提供了一个可扩展和可推广的范式。"}}
{"id": "2601.12658", "pdf": "https://arxiv.org/pdf/2601.12658", "abs": "https://arxiv.org/abs/2601.12658", "authors": ["Tianyi Yang", "Nashrah Haque", "Vaishnave Jonnalagadda", "Yuya Jeremy Ong", "Zhehui Chen", "Yanzhao Wu", "Lei Yu", "Divyesh Jadav", "Wenqi Wei"], "title": "Augmenting Question Answering with A Hybrid RAG Approach", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 tables, 2 figures; presented at IEEE CogMI 2025", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.", "AI": {"tldr": "本文提出了SSRAG方法，通过整合查询增强、智能路由和结构化检索机制，结合向量和图技术，显著提升了问答任务中RAG系统的回答质量", "motivation": "现有的RAG方法在检索上下文相关信息方面存在困难，导致回答不完整或次优，需要改进检索过程和上下文基础", "method": "SSRAG混合架构，包含查询增强、智能路由、向量和图技术的结构化检索机制以及上下文统一", "result": "在TruthfulQA、SQuAD和WikiQA三个QA数据集上对五个大型语言模型进行广泛评估，证明该方法相比标准RAG实现持续提升响应质量", "conclusion": "SSRAG通过改进检索过程和增强上下文基础，有效提高了回答准确性和信息丰富度，为RAG技术提供了更优的解决方案"}}
{"id": "2601.13518", "pdf": "https://arxiv.org/pdf/2601.13518", "abs": "https://arxiv.org/abs/2601.13518", "authors": ["Jiayi Yuan", "Jonathan Nöther", "Natasha Jaques", "Goran Radanović"], "title": "AgenticRed: Optimizing Agentic Systems for Automated Red-teaming", "categories": ["cs.AI", "cs.NE"], "comment": "Website: https://yuanjiayiy.github.io/AgenticRed/", "summary": "While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.", "AI": {"tldr": "AgenticRed是一个自动化红队测试系统，使用LLM的上下文学习能力自主设计和优化红队测试流程，无需人工干预，在多个模型上显著提升了攻击成功率。", "motivation": "现有自动化红队测试方法依赖人工设计的工作流程，存在人类偏见且设计空间探索成本高昂，需要更自动化的系统设计方法。", "method": "采用进化选择方法，将红队测试视为系统设计问题，利用LLM的上下文学习能力迭代设计和优化红队系统，而非在预定义结构中优化攻击策略。", "result": "在多个模型上取得显著成果：Llama-2-7B攻击成功率96%（提升36%），Llama-3-8B达到98%，GPT-3.5-Turbo和GPT-4o-mini达到100%，Claude-Sonnet-3.5达到60%（提升24%）。", "conclusion": "自动化系统设计是AI安全评估的强大范式，能够跟上快速演进的模型发展步伐，为解决AI安全问题提供了有效途径。"}}
{"id": "2601.12696", "pdf": "https://arxiv.org/pdf/2601.12696", "abs": "https://arxiv.org/abs/2601.12696", "authors": ["Tassallah Abdullahi", "Macton Mgonzo", "Mardiyyah Oduwole", "Paul Okewunmi", "Abraham Owodunni", "Ritambhara Singh", "Carsten Eickhoff"], "title": "UbuntuGuard: A Culturally-Grounded Policy Benchmark for Equitable AI Safety in African Languages", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Current guardian models are predominantly Western-centric and optimized for high-resource languages, leaving low-resource African languages vulnerable to evolving harms, cross-lingual safety failures, and cultural misalignment. Moreover, most guardian models rely on rigid, predefined safety categories that fail to generalize across diverse linguistic and sociocultural contexts. Robust safety, therefore, requires flexible, runtime-enforceable policies and benchmarks that reflect local norms, harm scenarios, and cultural expectations. We introduce UbuntuGuard, the first African policy-based safety benchmark built from adversarial queries authored by 155 domain experts across sensitive fields, including healthcare. From these expert-crafted queries, we derive context-specific safety policies and reference responses that capture culturally grounded risk signals, enabling policy-aligned evaluation of guardian models. We evaluate 13 models, comprising six general-purpose LLMs and seven guardian models across three distinct variants: static, dynamic, and multilingual. Our findings reveal that existing English-centric benchmarks overestimate real-world multilingual safety, cross-lingual transfer provides partial but insufficient coverage, and dynamic models, while better equipped to leverage policies at inference time, still struggle to fully localize African-language contexts. These findings highlight the urgent need for multilingual, culturally grounded safety benchmarks to enable the development of reliable and equitable guardian models for low-resource languages. Our code can be found online.\\footnote{Code repository available at https://github.com/hemhemoh/UbuntuGuard.", "AI": {"tldr": "UbuntuGuard是首个非洲政策导向的安全基准，由领域专家构建对抗性查询，评估大语言模型在非洲低资源语言中的安全性能，发现现有英语中心基准高估多语言安全性，跨语言迁移不足。", "motivation": "现有监护模型主要针对西方高资源语言，缺乏对非洲低资源语言的保护，存在跨语言安全失效和文化错位问题，需要反映当地文化规范的灵活安全基准。", "method": "邀请155名领域专家创作对抗性查询，从中推导出情境化安全政策和参考响应，评估13个模型（6个通用LLM和7个监护模型）在静态、动态和多语言三种变体上的表现。", "result": "发现英语中心基准高估多语言安全性，跨语言迁移提供部分但不充分的覆盖，动态模型虽能更好利用推理时政策，但仍难以完全本地化非洲语言情境。", "conclusion": "迫切需要多语言、文化基础的安全基准来开发可靠公平的非洲低资源语言监护模型，UbuntuGuard为此提供了重要工具和洞见。"}}
{"id": "2601.13533", "pdf": "https://arxiv.org/pdf/2601.13533", "abs": "https://arxiv.org/abs/2601.13533", "authors": ["Changshuo Zhang"], "title": "Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the \"reason first, recommend later\" paradigm to achieve \"reasoning while recommending\", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.", "AI": {"tldr": "提出EGLR推荐模型，通过熵引导的潜在推理机制实现实时推理推荐，解决生成式重排序中模型难度动态熵变化的问题，提升探索-利用平衡和推荐精度。", "motivation": "现有生成式方法难以适应列表生成过程中模型难度的动态熵变化，无法准确捕捉复杂偏好，需要新的推理机制来优化决策过程。", "method": "引入潜在推理机制，实现\"推理同时推荐\"范式；采用熵引导的变长推理，结合上下文感知推理标记和动态温度调整；轻量级集成设计，无需复杂独立模块。", "result": "在两个真实数据集上的实验验证了模型有效性，能够兼容现有生成式重排序模型并提升其性能。", "conclusion": "EGLR模型具有实际部署价值和研究潜力，通过熵引导推理有效降低了模型决策过程的熵，实现了更精确的探索-利用平衡。"}}
{"id": "2601.12698", "pdf": "https://arxiv.org/pdf/2601.12698", "abs": "https://arxiv.org/abs/2601.12698", "authors": ["Qiuyi Qu", "Yicheng Sui", "Yufei Sun", "Rui Chen", "Xiaofei Zhang", "Yuzhi Zhang", "Haofeng Wang", "Ge Lan", "Ning Zhang"], "title": "A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization", "categories": ["cs.CL"], "comment": null, "summary": "GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.", "AI": {"tldr": "该论文提出了一种基于模板的GPU代码优化方法，通过将内核重构为可参数化模板并进行基于搜索的自动调优，相比直接代码重写能获得更稳定和更高质量的加速效果。", "motivation": "GPU代码优化是HPC和大模型训练的关键性能瓶颈，现有LLM代理方法主要关注直接代码重写，参数选择隐式且难以控制，导致性能提升不稳定。", "method": "在代理驱动的迭代循环上增加模板重写层：将内核语义重构为显式可参数化模板，然后通过基于搜索的自动调优优化模板参数，结合性能分析反馈在硬件资源限制下执行约束参数搜索。", "result": "在真实内核测试中实现了超过3倍的加速效果，相比仅使用代理的直接重写方法，显著降低了迭代优化的随机性。", "conclusion": "模板加搜索的设计使优化过程更加可解释，能够系统性地实现高性能配置，该方法可扩展到OpenCL、HIP等后端，为实际生产工作负载提供自动化性能优化。"}}
{"id": "2601.13545", "pdf": "https://arxiv.org/pdf/2601.13545", "abs": "https://arxiv.org/abs/2601.13545", "authors": ["Shirin Shahabi", "Spencer Graham", "Haruna Isah"], "title": "TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning", "categories": ["cs.AI", "cs.ET", "cs.MA"], "comment": "16 pages, 6 figures, 2 tables", "summary": "Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com", "AI": {"tldr": "TruthTensor是一个新颖的LLM评估框架，通过实时预测市场和概率评分来评估语言模型在真实世界环境中的表现，超越了传统静态基准测试的局限性。", "motivation": "当前语言模型评估存在根本性挑战：静态基准无法捕捉真实世界的不确定性、分布偏移，以及孤立任务准确性与人类对齐决策之间的差距。", "method": "构建基于前瞻性、无污染任务的框架，锚定实时预测市场评估，结合概率评分和漂移中心诊断，包含人工与自动化评估角色、标注协议和统计测试程序。", "result": "在500+个真实市场测试中显示，具有相似预测准确性的模型在校准、漂移和风险敏感性方面存在显著差异，突显多维度评估的必要性。", "conclusion": "TruthTensor通过操作化现代评估最佳实践，包括明确假设框架、谨慎指标选择、透明计算成本报告和人工验证，为LLM在真实决策环境中提供可辩护的评估。"}}
{"id": "2601.12731", "pdf": "https://arxiv.org/pdf/2601.12731", "abs": "https://arxiv.org/abs/2601.12731", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Nicolò Brunello", "Gianluca Demartini"], "title": "A Shared Geometry of Difficulty in Multilingual Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.", "AI": {"tldr": "研究发现LLMs的问题难度预测存在两阶段表示过程：浅层表征形成语言无关的难度表示，深层表征发展为语言特定的难度表示，浅层探针跨语言泛化能力更强。", "motivation": "研究大型语言模型中问题难度的多语言几何结构，探索难度相关信号在模型内部不同层次的表现和跨语言泛化特性。", "method": "使用Easy2Hard基准的AMC子集，翻译成21种语言，在LLMs内部表征上训练线性探针来分析难度信号。", "result": "发现难度信号在浅层（早期层）和深层（后期层）两个阶段出现，浅层探针跨语言泛化更好但语言内准确率较低，深层探针语言内准确率高但跨语言泛化差。", "conclusion": "LLMs首先形成语言无关的问题难度表示，随后发展为语言特定的表示，这种两阶段表示过程不仅适用于语义内容，也适用于问题难度估计等高层次元认知属性。"}}
{"id": "2601.13546", "pdf": "https://arxiv.org/pdf/2601.13546", "abs": "https://arxiv.org/abs/2601.13546", "authors": ["Hui Sun", "Chang Xu", "Haonan Xie", "Hao Li", "Yuhao Huang", "Chuheng Zhang", "Ming Jin", "Xiaoguang Liu", "Gang Wang", "Jiang Bian"], "title": "ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution", "categories": ["cs.AI"], "comment": null, "summary": "LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.", "AI": {"tldr": "该论文提出了一个基于多代理的时间序列演化算法TSEvol，构建了TSEData-20K数据集和ChatAD聊天机器人系列，并开发了TKTO优化方法来提升跨任务泛化能力，在异常检测任务上取得了显著性能提升。", "motivation": "现有LLM驱动的异常检测方法存在推理能力不足、多轮对话能力欠缺和泛化能力有限的问题，需要提升时间序列异常行为的理解和解释能力。", "method": "1) 提出多代理时间序列演化算法TSEvol；2) 构建TSEData-20K数据集和ChatAD系列模型；3) 提出TKTO优化方法增强跨任务泛化；4) 建立LLADBench基准评估系统。", "result": "三个ChatAD模型在准确率上提升高达34.50%，F1分数提升34.71%，误报率降低37.42%。通过TKTO优化后的ChatAD在分类、预测和插补任务上展现出竞争力的推理和跨任务泛化性能。", "conclusion": "该研究通过多代理演化算法、大规模数据集构建和优化方法，显著提升了LLM在时间序列异常检测中的性能和泛化能力，为解决现有方法的局限性提供了有效解决方案。"}}
{"id": "2601.12748", "pdf": "https://arxiv.org/pdf/2601.12748", "abs": "https://arxiv.org/abs/2601.12748", "authors": ["Bin Xie", "Bingbing Xu", "Xueyun Tian", "Yilin Chen", "Huawei Shen"], "title": "Towards Robust Process Reward Modeling via Noise-aware Learning", "categories": ["cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \\underline{\\textbf{N}}oise-\\underline{\\textbf{A}}ware \\underline{\\textbf{I}}terative \\underline{\\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\\% absolute gain in average F1 over PRMs trained with noisy supervision.", "AI": {"tldr": "提出两阶段框架解决过程奖励模型中的监督噪声问题：标签阶段使用LLM检测反思行为来修正标签，训练阶段通过噪声感知迭代训练让PRM基于自身置信度逐步精炼噪声标签。", "motivation": "现有的蒙特卡洛估计方法产生策略依赖的奖励信号，导致错误的正样本(奖励错误步骤)和错误的负样本(惩罚正确步骤)等标签噪声问题。", "method": "1. 标签阶段：引入反思感知标签修正机制，使用大语言模型作为裁判检测与当前推理步骤相关的反思和自我修正行为；2. 训练阶段：提出噪声感知迭代训练框架，使PRM能够基于自身置信度逐步精炼噪声标签。", "result": "实验显示该方法显著改善了步骤级正确性判别能力，相比使用噪声监督训练的PRM实现了高达27%的平均F1分数绝对提升。", "conclusion": "该方法有效缓解了过程奖励模型中的监督噪声问题，通过结合外部LLM判断和内部置信度评估，显著提升了步骤级奖励信号的准确性。"}}
{"id": "2601.13558", "pdf": "https://arxiv.org/pdf/2601.13558", "abs": "https://arxiv.org/abs/2601.13558", "authors": ["Mehrab Beikzadeh", "Chenglin Hong", "Cory J Cascalheira", "Callisto Boka", "Majid Sarrafzadeh", "Ian W Holloway"], "title": "Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM. With participant consent, we collected textual data and trained machine learning models using features derived from ChatGPT embeddings, BERT embeddings, LIWC, and a dictionary-based risk term approach. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78, and moderate performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63. These findings demonstrate that social media and dating app text data can provide valuable insights into risk and protective behaviors and highlight the potential of large language model-based methods to support scalable and personalized public health interventions for MSM.", "AI": {"tldr": "该研究利用社交媒体和约会应用的文本数据，通过机器学习模型成功预测男男性行为者的性风险行为、饮酒习惯和PrEP使用情况，展示了基于大语言模型的方法在公共卫生干预中的潜力。", "motivation": "男男性行为者(MSM)相比异性恋男性面临更高的性传播感染风险和有害饮酒问题，社交媒体和约会应用的文本数据为个性化公共卫生干预提供了新机会。", "method": "收集参与者同意的文本数据，使用ChatGPT嵌入、BERT嵌入、LIWC和基于词典的风险术语方法提取特征，训练机器学习预测模型。", "result": "模型在预测每月酗酒和超过5个性伴侣方面表现优异(F1分数0.78)，在预测PrEP使用和重度饮酒方面表现中等(F1分数0.64和0.63)。", "conclusion": "社交媒体和约会应用文本数据能有效识别风险和保护性行为，大语言模型方法具有支持可扩展、个性化公共卫生干预的潜力。"}}
{"id": "2601.12758", "pdf": "https://arxiv.org/pdf/2601.12758", "abs": "https://arxiv.org/abs/2601.12758", "authors": ["Shenyan Zheng", "Jiayou Zhong", "Anudeex Shetty", "Heng Ji", "Preslav Nakov", "Usman Naseem"], "title": "VISPA: Pluralistic Alignment via Automatic Value Selection and Activation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "WIP", "summary": "As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.", "AI": {"tldr": "VISPA是一个无需训练的多元对齐框架，通过动态选择和内部激活引导实现价值观表达的直接控制，在医疗等多个领域展现出良好的性能表现。", "motivation": "大型语言模型在高风险领域应用时，需要反映多元人类偏好而非平均偏好，但现有方法存在价值观控制和代表性不足的问题。", "method": "提出VISPA框架，采用训练无关的方法，通过动态选择和内部模型激活引导来实现价值观控制。", "result": "在多个模型和评估设置下的广泛实证研究表明，VISPA在医疗等领域的各种多元对齐模式中都表现优异，且能适应不同的引导初始化、模型和价值观。", "conclusion": "通过内部激活机制可以实现多元对齐，为构建服务所有人的语言模型提供了可扩展的路径。"}}
{"id": "2601.13559", "pdf": "https://arxiv.org/pdf/2601.13559", "abs": "https://arxiv.org/abs/2601.13559", "authors": ["Sun Hui", "Ding Yanfeng", "Huidong Ma", "Chang Xu", "Keyan Jin", "Lizheng Zu", "Cheng Zhong", "xiaoguang Liu", "Gang Wang", "Wentong Cai"], "title": "AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent", "categories": ["cs.AI"], "comment": null, "summary": "Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.", "AI": {"tldr": "AgentGC是一个基于多智能体的进化式基因组数据压缩器，通过三层架构（用户层、认知层、压缩层）结合LLM技术，在压缩比和吞吐量方面显著优于现有方法", "motivation": "解决当前基于学习的基因组数据压缩方法存在的不可进化性、低层次压缩建模、有限适应性和用户界面不友好等问题", "method": "采用三层架构：1)用户层通过Leader+LLM提供友好界面；2)认知层由Leader驱动，整合LLM进行算法-数据集-系统联合优化；3)压缩层由Worker负责，通过自动化多知识学习框架执行压缩解压。提供CP、TP、BM三种模式", "result": "在9个数据集上与14个基线方法相比，平均压缩比提升16.66%、16.11%、16.33%，吞吐量提升4.73倍、9.23倍、9.15倍", "conclusion": "AgentGC成功解决了现有方法的局限性，在压缩效率和性能方面取得了显著提升，为基因组数据存储和管理提供了有效的进化式压缩解决方案"}}
{"id": "2601.12771", "pdf": "https://arxiv.org/pdf/2601.12771", "abs": "https://arxiv.org/abs/2601.12771", "authors": ["Keito Inoshita"], "title": "Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.", "AI": {"tldr": "提出LAMA框架，利用LLM的联想记忆能力，通过双智能体架构回忆同名名人并聚合国籍信息，在国籍预测任务中显著优于传统方法", "motivation": "大型语言模型拥有丰富世界知识但缺乏有效提取方法，国籍预测需要文化历史背景知识，传统直接推理方法在应用抽象语言规则方面存在局限", "method": "LAMA框架：使用Person Agent和Media Agent双智能体并行回忆同名名人，通过投票机制生成Top-1预测，通过条件补全生成Top-K预测", "result": "在99国国籍预测任务中达到0.817准确率，大幅超越传统LLM提示方法和神经网络模型", "conclusion": "LLM在回忆具体示例方面比抽象推理更可靠，基于回忆的方法对低频国籍具有鲁棒性，双智能体架构产生协同效应，展示了检索和聚合LLM知识的新方法有效性"}}
{"id": "2601.13562", "pdf": "https://arxiv.org/pdf/2601.13562", "abs": "https://arxiv.org/abs/2601.13562", "authors": ["Zhiguang Liu", "Yi Shang"], "title": "Reasoning is a Modality", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Code access: https://github.com/lz7fd/Reasoning_is_a_Modality", "summary": "The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.", "AI": {"tldr": "该论文提出了一个角色分离的Transformer架构来解决抽象推理问题，在ARC视觉推理任务上超越了人类平均表现和现有方法，证明了推理作为独立模态的有效性。", "motivation": "现代AI系统缺乏人类那样的可解释内部状态，只能产生事后合理化解释而非基于内部状态的推理。研究者假设推理应该作为独立于低层工作空间的模态存在。", "method": "设计了角色分离的Transformer块，将全局控制器token与网格工作空间token分开，实现迭代规则执行，在VARC视觉中心协议下进行训练和评估。", "result": "在ARC-1任务上达到62.6%的准确率，超过人类平均表现(60.2%)，显著优于先前方法，模型展现出比密集ViT基线更一致的规则应用结构。", "conclusion": "通过分离控制器和工作空间的架构设计，成功实现了从概率预测向控制器驱动推理的转变，为构建具有可解释内部状态的AI系统提供了新方向。"}}
{"id": "2601.12812", "pdf": "https://arxiv.org/pdf/2601.12812", "abs": "https://arxiv.org/abs/2601.12812", "authors": ["Sushant Kumar Ray", "Gautam Siddharth Kashyap", "Sahil Tripathi", "Nipun Joshi", "Vijay Govindarajan", "Rafiq Ali", "Jiechao Gao", "Usman Naseem"], "title": "Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?", "categories": ["cs.CL"], "comment": "Accepted at EACL 2026 (Industry Track)", "summary": "Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.", "AI": {"tldr": "MEDASSESS-X是一个在推理时进行对齐的临床问答框架，通过轻量级引导向量提升各种LLM的医疗问答性能，无需微调即可显著提升准确性、事实一致性和安全性", "motivation": "解决SPECIALISATION FALLACY（专业化谬误）的假设，即认为专业医疗LLM必然优于通用LLM，同时克服专业医疗LLM的局限性（覆盖范围窄、重训练成本高、适应性有限）", "method": "提出MEDASSESS-X框架，在推理时应用对齐而非监督微调，使用轻量级引导向量指导模型激活进行医疗一致性推理，不更新模型权重或需要领域特定重训练", "result": "在所有LLM家族中实现一致性能提升：准确率最高提升+6%，事实一致性提升+7%，安全错误率降低多达50%", "conclusion": "MEDASSESS-X通过推理时对齐有效解决了专业化谬误，证明了无需领域特定微调也能稳定提升各种LLM在临床问答中的性能"}}
{"id": "2601.13581", "pdf": "https://arxiv.org/pdf/2601.13581", "abs": "https://arxiv.org/abs/2601.13581", "authors": ["Heedou Kim", "Changsik Kim", "Sanghwa Shin", "Jaewoo Kang"], "title": "SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System", "categories": ["cs.AI"], "comment": "This paper has been accepted to the EACL 2026 Industry Track", "summary": "Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.", "AI": {"tldr": "ScriptMind是一个基于LLM的诈骗检测框架，通过犯罪脚本推理任务、数据集和认知模拟评估，显著提升小型LLM的检测性能，在多项指标上超越GPT-4o，并能有效增强用户的防诈骗认知意识。", "motivation": "传统诈骗检测方法难以应对个性化、多轮对话的社交工程诈骗，而大型语言模型在识别欺骗方面的潜力尚未充分探索，特别是在认知辅助方面的应用。", "method": "提出ScriptMind框架，包含三个组件：犯罪脚本推理任务(CSIT)用于诈骗推理，犯罪脚本感知推理数据集(CSID)用于微调小型LLM，认知模拟评估(CSED)用于评估实时认知影响。使用571个韩国电话诈骗案例构建22,712个结构化训练实例。", "result": "11B参数的小型LLM经过ScriptMind微调后，检测准确率比GPT-4o高出13%，在检测准确性、误报减少、诈骗者话语预测和推理质量方面均优于商业模型。在电话诈骗模拟实验中显著提升并维持了用户的怀疑水平。", "conclusion": "ScriptMind代表了向以人为中心、认知自适应的LLM诈骗防御系统迈出的重要一步，展示了LLM在增强人类认知防诈骗能力方面的巨大潜力。"}}
{"id": "2601.12815", "pdf": "https://arxiv.org/pdf/2601.12815", "abs": "https://arxiv.org/abs/2601.12815", "authors": ["Zhaolu Kang", "Junhao Gong", "Qingxi Chen", "Hao Zhang", "Jiaxin Liu", "Rong Fu", "Zhiyuan Feng", "Yuan Wang", "Simon Fong", "Kaiyue Zhou"], "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.", "AI": {"tldr": "提出JurisMMA框架解决法律判决预测问题，通过任务分解和流程标准化提升效果，并构建包含10万+中国司法记录的多模态数据集JurisMM进行验证", "motivation": "传统法律判决预测方法依赖统计分析或角色模拟，难以处理多重指控、多样化证据且缺乏适应性", "method": "开发JurisMMA框架，分解审判任务、标准化流程并组织成不同阶段；构建大型多模态数据集JurisMM（包含文本和视频-文本数据）", "result": "在JurisMM和LawBench基准测试上的实验验证了框架的有效性", "conclusion": "该框架不仅适用于法律判决预测，还为更广泛的法律应用提供了新视角，对未来法律方法和数据集发展有重要意义"}}
{"id": "2601.13589", "pdf": "https://arxiv.org/pdf/2601.13589", "abs": "https://arxiv.org/abs/2601.13589", "authors": ["HyeYoung Lee"], "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification", "categories": ["cs.AI", "cs.SD"], "comment": null, "summary": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.", "AI": {"tldr": "提出基于音频情感信号的实时多智能体AI系统，通过四个协作智能体将识别的情感状态转化为安全、适龄、可控的响应内容，具备安全验证机制和低延迟特性。", "motivation": "传统语音情感识别研究主要关注分类准确性，但缺乏将情感状态转化为实际响应内容的安全可控机制，特别是在儿童相关媒体和治疗应用等敏感场景。", "method": "采用四智能体协作架构：1)CNN情感识别智能体提取声学特征；2)响应策略决策智能体映射情感到响应模式；3)内容参数生成智能体产生媒体控制参数；4)安全验证智能体确保内容适龄合规。引入显式安全验证循环。", "result": "在公开数据集上实现73.2%情感识别准确率、89.4%响应模式一致性、100%安全合规性，推理延迟低于100ms，适合设备端部署。", "conclusion": "模块化架构提供了可解释性和可扩展性，适用于儿童媒体、治疗应用和情感响应智能设备，实现了从情感识别到安全内容生成的完整闭环。"}}
{"id": "2601.12844", "pdf": "https://arxiv.org/pdf/2601.12844", "abs": "https://arxiv.org/abs/2601.12844", "authors": ["Julie Rançon", "Jean-François Cerisier", "Emilie Remond", "Aurélien Nguyen", "Andrew Peterson", "Ladjel Bellatreche"], "title": "Rapport du Projet de Recherche TRAIMA", "categories": ["cs.CL"], "comment": "in French language", "summary": "The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{é}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{é}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{é}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.", "AI": {"tldr": "TRAIMA项目研究如何利用机器学习自动处理教育环境中的多模态互动，重点关注解释性和协作性序列，建立了多模态转录和分析的方法论框架，为教育AI研究奠定基础。", "motivation": "解决教育互动研究中手动分析言语、副言语和非言语数据耗时且难以扩展的方法论挑战", "method": "结合话语分析和互动语言学理论，定义解释性话语的三段式结构；比较现有转录规范；基于INTER-EXPLIC和EXPLIC-LEXIC语料库进行实证分析；利用TechnéLAB平台进行多模态数据采集", "result": "建立了与机器学习兼容的转录规范、标注类别和分析单元的方法论框架，揭示了转录实践的变异性和解释性维度", "conclusion": "项目未开发完全自动化的系统，而是为多模态教学互动的自动处理建立了严谨的方法论基础，强调了理论明确性和研究者反思性的重要性"}}
{"id": "2601.13591", "pdf": "https://arxiv.org/pdf/2601.13591", "abs": "https://arxiv.org/abs/2601.13591", "authors": ["Maojun Sun", "Yifei Xie", "Yue Wu", "Ruijian Han", "Binyan Jiang", "Defeng Sun", "Yancheng Yuan", "Jian Huang"], "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.", "AI": {"tldr": "DSAEval是一个包含641个真实世界数据科学问题的多模态基准测试，评估11种先进LLM代理在结构化与非结构化数据任务上的表现，发现Claude-Sonnet-4.5综合最强，GPT-5.2最有效率，MiMo-V2-Flash性价比最高。", "motivation": "现有基于LLM的数据代理难以评估开放式的真实世界数据科学问题，这些问题跨越多种分类且缺乏标准答案。", "method": "提出DSAEval基准测试，包含285个多样化数据集，具有多模态环境感知、多查询交互和多维度评估三大特性，系统性评估11种先进代理LLM。", "result": "Claude-Sonnet-4.5综合性能最强，GPT-5.2最有效率，MiMo-V2-Flash最具成本效益。多模态感知在视觉任务上提升2.04%-11.30%性能。当前代理在结构化数据和常规分析流程表现良好，但在非结构化领域仍有挑战。", "conclusion": "数据科学代理在结构化数据上表现良好，但在非结构化领域仍需改进。研究为数据科学代理的发展提供了关键见解和未来研究方向。"}}
{"id": "2601.12868", "pdf": "https://arxiv.org/pdf/2601.12868", "abs": "https://arxiv.org/abs/2601.12868", "authors": ["Shiyue Hu", "Ruizhe Li", "Yanjun Gao"], "title": "Race, Ethnicity and Their Implication on Bias in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Work in process", "summary": "Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.", "AI": {"tldr": "本文通过可解释性方法研究LLMs中种族和族裔信息的内部表征机制，发现人口统计信息分散在不同神经元中且存在模型间差异，干预可减少偏见但仍有残留效应", "motivation": "现有研究主要记录结果层面的差异，对LLMs内部如何处理种族和族裔信息的机制了解有限，特别是在医疗等高风险领域", "method": "使用两个公开数据集（毒性生成和临床叙事理解任务），分析三个开源模型，采用探测、神经元归因和靶向干预的可复现解释性流程", "result": "人口统计信息分布在多个内部单元中，模型间差异显著；部分单元编码敏感或刻板印象关联；相同人口统计线索可引发不同行为；干预抑制相关神经元可减少偏见但仍有残留效应", "conclusion": "偏见减少主要来自行为层面而非表征层面的改变，需要更系统性的缓解方法来解决LLMs中的人口统计偏见问题"}}
{"id": "2601.13600", "pdf": "https://arxiv.org/pdf/2601.13600", "abs": "https://arxiv.org/abs/2601.13600", "authors": ["Paul He", "Elke Kirschbaum", "Shiva Kasiviswanathan"], "title": "Foundations of Global Consistency Checking with Noisy LLM Oracles", "categories": ["cs.AI"], "comment": "Under Review", "summary": "Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.", "AI": {"tldr": "该论文提出了一个自适应分治算法来检测和定位自然语言事实集合中的全局不一致性，解决了LLM在一致性验证中的噪声问题和指数级查询复杂度问题。", "motivation": "确保自然语言事实集合的全局一致性对于事实核查、摘要生成和知识库构建至关重要，但现有LLM方法存在判断噪声且无法保证全局一致性。", "method": "提出自适应分治算法，识别最小不一致子集(MUSes)，可选地通过hitting-sets计算最小修复，具有低阶多项式查询复杂度。", "result": "在合成和真实LLM oracle上的实验表明，该方法能有效检测和定位不一致性，为基于LLM的评估器提供了可扩展的语言一致性验证框架。", "conclusion": "该方法提供了一个实用的解决方案，能够在多项式时间内验证大规模事实集合的全局一致性，解决了LLM一致性验证的可扩展性问题。"}}
{"id": "2601.12904", "pdf": "https://arxiv.org/pdf/2601.12904", "abs": "https://arxiv.org/abs/2601.12904", "authors": ["Jiahao Wang", "Weiyu Xie", "Mingxing Zhang", "Boxing Zhang", "Jianwei Dong", "Yuening Zhu", "Chen Lin", "Jinqi Tang", "Yaochen Han", "Zhiyuan Ai", "Xianglin Chen", "Yongwei Wu", "Congfeng Jiang"], "title": "From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.", "AI": {"tldr": "FusionRAG是一个新的推理框架，通过在预处理阶段嵌入跨块上下文信息，并在重处理阶段选择性重计算关键token的KV缓存，在保持生成质量的同时显著提升RAG效率", "motivation": "现有的RAG方法重用预处理的KV缓存会因缺乏跨块上下文信息而导致生成质量显著下降，无法充分发挥KV缓存重用的潜在优势", "method": "提出两阶段框架：离线预处理阶段将相关文本块信息嵌入到每个块中；在线重处理阶段选择性重计算模型关注的token的KV缓存", "result": "在相同重计算比例下显著提升生成质量，重计算少于15%的token即可获得比基线高70%的归一化F1分数，TTFT减少2.66-9.39倍", "conclusion": "FusionRAG在生成质量和效率之间实现了更好的权衡，有效解决了KV缓存重用中的跨块上下文缺失问题"}}
{"id": "2601.13632", "pdf": "https://arxiv.org/pdf/2601.13632", "abs": "https://arxiv.org/abs/2601.13632", "authors": ["Zhiming Xue", "Sichen Zhao", "Yalun Qi", "Xianling Zeng", "Zihan Yu"], "title": "Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning", "categories": ["cs.AI"], "comment": null, "summary": "With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization. We first construct a logistics topology graph by using the discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) is adopted to extract spatial correlations and temporal dependencies for predicting future congestion risks. These prediction results are then integrated into a dynamic edge weight mechanism to perform path planning. We evaluated the framework on the Smart Logistics Dataset 2024, which contains real-world Internet of Things(IoT) sensor data. The experimental results show that the RADR algorithm significantly enhances the resilience of the supply chain. Particularly in the case study of high congestion scenarios, our method reduces the potential congestion risk exposure by 19.3% while only increasing the transportation distance by 2.1%. This empirical evidence confirms that the proposed data-driven approach can effectively balance delivery efficiency and operational safety.", "AI": {"tldr": "提出风险感知动态路由框架RADR，结合时空图神经网络和组合优化，通过预测拥堵风险来动态规划物流路径，在保证运输效率的同时显著降低拥堵风险。", "motivation": "电商快速发展给物流网络带来巨大压力，传统静态路由策略无法有效应对交通拥堵和需求波动问题，需要更智能的动态路由解决方案。", "method": "1) 使用空间聚类方法从离散GPS数据构建物流拓扑图；2) 采用GCN和GRU混合深度学习模型提取时空特征预测未来拥堵风险；3) 将预测结果集成到动态边权重机制中进行路径规划", "result": "在Smart Logistics Dataset 2024上验证，RADR算法显著提升供应链韧性。在高拥堵场景下，潜在拥堵风险降低19.3%，而运输距离仅增加2.1%", "conclusion": "数据驱动方法能有效平衡配送效率和运营安全，为物流网络动态路由提供了有效解决方案"}}
{"id": "2601.12906", "pdf": "https://arxiv.org/pdf/2601.12906", "abs": "https://arxiv.org/abs/2601.12906", "authors": ["Lingrui Mei", "Shenghua Liu", "Yiwei Wang", "Yuyao Ge", "Baolong Bi", "Jiayu Yao", "Jun Wan", "Ziling Yin", "Jiafeng Guo", "Xueqi Cheng"], "title": "Gated Differentiable Working Memory for Long-Context Language Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Long contexts challenge transformers: attention scores dilute across thousands of tokens, critical information is often lost in the middle, and models struggle to adapt to novel patterns at inference time. Recent work on test-time adaptation addresses this by maintaining a form of working memory -- transient parameters updated on the current context -- but existing approaches rely on uniform write policies that waste computation on low-utility regions and suffer from high gradient variance across semantically heterogeneous contexts. In this work, we reframe test-time adaptation as a budget-constrained memory consolidation problem, focusing on which parts of the context should be consolidated into working memory under limited computation. We propose Gdwm (Gated Differentiable Working Memory), a framework that introduces a write controller to gate the consolidation process. The controller estimates Contextual Utility, an information-theoretic measure of long-range contextual dependence, and allocates gradient steps accordingly while maintaining global coverage. Experiments on ZeroSCROLLS and LongBench v2 demonstrate that Gdwm achieves comparable or superior performance with 4$\\times$ fewer gradient steps than uniform baselines, establishing a new efficiency-performance Pareto frontier for test-time adaptation.", "AI": {"tldr": "Gdwm框架通过门控机制选择性地将高价值上下文信息整合到工作记忆中，用4倍更少的梯度步骤实现了与均匀方法相当或更好的性能", "motivation": "长上下文处理中，传统注意力机制存在信息稀释和中间信息丢失问题，现有测试时适应方法采用均匀写入策略导致计算浪费和梯度方差高", "method": "提出Gdwm框架，引入写入控制器来门控整合过程，控制器估计上下文效用（信息论的长距离依赖度量），在保持全局覆盖的同时分配梯度步骤", "result": "在ZeroSCROLLS和LongBench v2上的实验显示，Gdwm比均匀基线方法少用4倍梯度步骤，实现了相当或更优的性能", "conclusion": "Gdwm为测试时适应建立了新的效率-性能帕累托前沿，通过预算约束下的选择性记忆整合解决了长上下文处理中的计算效率问题"}}
{"id": "2601.13687", "pdf": "https://arxiv.org/pdf/2601.13687", "abs": "https://arxiv.org/abs/2601.13687", "authors": ["Zhichao Liang", "Satoshi Nakamura"], "title": "Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue", "categories": ["cs.AI"], "comment": null, "summary": "Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.", "AI": {"tldr": "SocialMindChange是一个新的心理理论基准测试，要求语言模型在社交互动中主动改变他人心理状态，而不仅仅是被动追踪。该测试包含1200个社交情境，评估显示当前LLMs的表现比人类低54.2%。", "motivation": "现有的心理理论基准测试让语言模型处于被动角色，仅追踪心理状态变化。但真实社交互动中，心理理论被用于主动改变他人心理状态轨迹。", "method": "构建包含4个角色和5个连接场景的社交情境，模型扮演其中一个角色生成对话来达成目标，同时保持与所有参与者状态的一致性。使用四步框架构建了1200个情境，包含6000个场景和9万多个问题。", "result": "评估10个最先进的LLMs，平均表现比人类低54.2%，表明当前模型在长连接互动中维持和改变心理状态表征方面仍有困难。", "conclusion": "当前大型语言模型在主动改变心理状态方面能力有限，SocialMindChange基准揭示了这一重要差距，为未来模型发展提供了重要测试平台。"}}
{"id": "2601.12910", "pdf": "https://arxiv.org/pdf/2601.12910", "abs": "https://arxiv.org/abs/2601.12910", "authors": ["Tim Baumgärtner", "Iryna Gurevych"], "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies.", "AI": {"tldr": "SciCoQA是一个检测科学论文与代码库差异的数据集，包含611个差异实例（81个真实，530个合成），涵盖多个计算科学领域。评估显示LLMs在此任务上表现困难，最佳模型GPT-5仅能检测45.7%的真实差异。", "motivation": "确保科学论文与代码实现之间的一致性，解决科学出版物与代码库存在差异的问题，促进可复现性研究。", "method": "从GitHub问题和可复现性论文构建数据集，提出合成数据生成方法来扩展数据集，详细分析差异类型和类别。", "result": "构建了包含611个差异实例的数据集，评估21个LLMs发现该任务具有挑战性，特别是在处理省略细节、长文本输入和预训练语料外数据时。", "conclusion": "SciCoQA数据集揭示了科学论文与代码实现之间的差异检测是一个困难但重要的任务，现有LLMs在此方面仍有很大改进空间。"}}
{"id": "2601.13709", "pdf": "https://arxiv.org/pdf/2601.13709", "abs": "https://arxiv.org/abs/2601.13709", "authors": ["Christopher Kao", "Vanshika Vats", "James Davis"], "title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SI"], "comment": "For associated dataset, see https://github.com/cocochief4/llm-mafia. Published in IEEE ICA 2025, waiting for IEEEXplore proceedings", "summary": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.", "AI": {"tldr": "研究显示GPT-4o在社交推理游戏中的欺骗能力比人类更强，AI能更好地融入群体并有效欺骗", "motivation": "随着LLM代理在更多应用中使用，对其安全性的担忧增加。虽然之前研究表明LLM在受控任务中可以欺骗，但对其在社交语境中使用自然语言欺骗的能力了解甚少", "method": "使用异步多代理框架模拟35场Mafia游戏，创建GPT-4-Turbo Mafia检测器分析游戏记录预测黑手党玩家，以预测准确率作为欺骗质量的替代指标，并与28场人类游戏和随机基线比较", "result": "Mafia检测器对LLM游戏的预测准确率低于人类游戏，表明LLM能更好地融入群体并更有效地欺骗，结果在不同游戏天数和检测到的黑手党数量下均一致", "conclusion": "研究结果强调了LLM在社交语境中欺骗的复杂性和风险，并发布了LLM Mafia游戏记录数据集以支持未来研究"}}
{"id": "2601.12921", "pdf": "https://arxiv.org/pdf/2601.12921", "abs": "https://arxiv.org/abs/2601.12921", "authors": ["Adimulya Kartiyasa", "Bao Gia Cao", "Boyang Li"], "title": "Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.", "AI": {"tldr": "该论文提出了IndoSoSci数据集和一种有效的检索增强生成方法，通过从印尼社会科学期刊中提取文化事实，使用LLM生成的假设文档作为查询，显著提升了大型语言模型对印尼文化的理解能力。", "motivation": "目前大型语言模型对印尼文化的理解有限，而本土社会科学期刊包含大量从本土视角进行的文化研究，这是一个被忽视的重要文化知识来源。", "method": "从151个开源印尼社会科学期刊创建IndoSoSci文本数据集，提取与印尼文化相关的事实，采用检索增强生成(RAG)方法，在检索阶段使用LLM生成的假设文档作为查询。", "result": "提出的方法在IndoCulture基准测试中取得了显著的性能提升，结合印尼维基百科后创造了该基准测试的最新准确率记录。", "conclusion": "通过利用本土社会科学期刊资源和创新的检索增强生成方法，可以有效提升LLMs对印尼文化的理解能力，为文化知识注入提供了有效途径。"}}
{"id": "2601.13735", "pdf": "https://arxiv.org/pdf/2601.13735", "abs": "https://arxiv.org/abs/2601.13735", "authors": ["Hojin Kim", "Jaehyung Kim"], "title": "Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection", "categories": ["cs.AI"], "comment": "15 pages, 4 figures", "summary": "Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.", "AI": {"tldr": "研究发现当前概率置信度指标不能有效捕捉推理步骤间的因果依赖关系，主要反映表面流畅性而非逻辑结构。提出新的对比因果度量方法，能更准确地选择推理输出。", "motivation": "挑战当前最佳N选择中概率置信度指标作为推理质量代理的假设，质疑其是否真正捕捉到推理步骤间的因果依赖关系", "method": "引入三类步骤间因果扰动，系统性地破坏推理步骤间的依赖关系但保持局部流畅性，测试不同模型家族和推理基准下的选择准确性变化", "result": "即使应用硬注意力掩码等严重干扰手段，选择准确性仅轻微下降，表明当前概率指标对逻辑结构不敏感，主要捕捉表面流畅性或分布先验", "conclusion": "现有概率指标不能有效评估推理质量，提出的对比因果度量方法能更忠实地进行输出选择，为改进推理评估提供了新方向"}}
{"id": "2601.12945", "pdf": "https://arxiv.org/pdf/2601.12945", "abs": "https://arxiv.org/abs/2601.12945", "authors": ["Miao Xie", "Siguang Chen", "Chunli Lv"], "title": "A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits", "categories": ["cs.CL", "cs.LG"], "comment": "27 pages, 6 table", "summary": "Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.", "AI": {"tldr": "第一篇系统综述大语言模型与多臂老虎机双向交互的调研论文，探讨两者在组件层面的相互增强作用，并提供了相关文献索引", "motivation": "探索大语言模型(LLMs)和多臂老虎机(MAB)算法这两个强大领域的交叉潜力，填补系统研究两者双向交互的空白", "method": "通过系统调研和分析现有的LLM增强老虎机系统以及老虎机增强LLM系统，从组件层面深入分析双向交互的设计、方法和性能", "result": "识别了双向交互的关键挑战和代表性发现：MAB算法可解决LLM从预训练到RAG和个性化的关键挑战，而LLMs可重新定义MAB系统的核心组件以改进序列决策", "conclusion": "该调研为未来研究提供了指导，建立了首个系统性的LLM与MAB交互研究框架，并通过GitHub仓库索引相关文献以促进该领域发展"}}
{"id": "2601.13752", "pdf": "https://arxiv.org/pdf/2601.13752", "abs": "https://arxiv.org/abs/2601.13752", "authors": ["Chak Tou Leong", "Dingwei Chen", "Heming Xia", "Qingyu Yin", "Sunbowen Lee", "Jian Wang", "Wenjie Li"], "title": "Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering", "categories": ["cs.AI", "cs.CL"], "comment": "Working in progress", "summary": "Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \\textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.", "AI": {"tldr": "RELIEF框架通过捕捉和调整大推理模型的内部推理信念来优化其行为，无需推理轨迹监督，在效率和忠实度任务上表现优异且训练成本低", "motivation": "大型推理模型存在计算冗余和推理不忠实的问题，现有方法依赖强化学习或黄金标准推理轨迹，计算成本高且难以扩展", "method": "提出RELIEF框架，通过logit探测捕捉模型的内部推理信念，然后使用合成的自反问答对进行微调，使模型自我概念与目标信念蓝图对齐", "result": "在效率和忠实度任务上的实验表明，RELIEF匹配或超越了基于行为监督和偏好的基线方法，同时训练成本更低", "conclusion": "通过调整模型的推理信念可以有效塑造其实际行为，RELIEF提供了一种无需监督的高效行为塑造方法"}}
{"id": "2601.12960", "pdf": "https://arxiv.org/pdf/2601.12960", "abs": "https://arxiv.org/abs/2601.12960", "authors": ["Ainhoa Vivel-Couso", "Nicolás Vila-Blanco", "María J. Carreira", "Alberto Bugarín-Diz", "Inmaculada Tomás", "Jose M. Alonso-Moral"], "title": "Trustworthy Data-driven Chronological Age Estimation from Panoramic Dental Images", "categories": ["cs.CL"], "comment": "This paper is a preliminary version of an accepted article in Information Systems Frontiers, Springer, Special Issue \"Explainability in Human-Centric AI\". Please cite the final published version of the paper, not this preprint. The final published version can be found at https://link.springer.com/article/10.1007/s10796-025-10682-3", "summary": "Integrating deep learning into healthcare enables personalized care but raises trust issues due to model opacity. To improve transparency, we propose a system for dental age estimation from panoramic images that combines an opaque and a transparent method within a natural language generation (NLG) module. This module produces clinician-friendly textual explanations about the age estimations, designed with dental experts through a rule-based approach. Following the best practices in the field, the quality of the generated explanations was manually validated by dental experts using a questionnaire. The results showed a strong performance, since the experts rated 4.77+/-0.12 (out of 5) on average across the five dimensions considered. We also performed a trustworthy self-assessment procedure following the ALTAI checklist, in which it scored 4.40+/-0.27 (out of 5) across seven dimensions of the AI Trustworthiness Assessment List.", "AI": {"tldr": "提出一个结合不透明和透明方法的牙科年龄估计系统，通过自然语言生成模块为临床医生提供可理解的文本解释，获得了专家高度评价和可信度评估高分。", "motivation": "深度学习在医疗保健中的应用虽然能实现个性化护理，但由于模型不透明性引发了信任问题，需要提高透明度。", "method": "使用全景图像进行牙科年龄估计，结合不透明和透明方法，通过基于规则的自然语言生成模块产生临床医生友好的文本解释，并与牙科专家合作设计验证。", "result": "专家在五个维度上的平均评分为4.77±0.12（满分5分），ALTAI清单的可信度评估在七个维度上得分为4.40±0.27（满分5分），表现优异。", "conclusion": "该系统成功提高了牙科年龄估计的透明度和可信度，通过专家验证的自然语言解释有效解决了深度学习模型在医疗应用中的信任问题。"}}
{"id": "2601.13761", "pdf": "https://arxiv.org/pdf/2601.13761", "abs": "https://arxiv.org/abs/2601.13761", "authors": ["Shengda Fan", "Xuyan Ye", "Yankai Lin"], "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.", "AI": {"tldr": "DARC是一个两阶段的自对弈框架，通过解耦问题生成和解答训练来解决现有自对弈方法的不稳定性问题，在多个推理基准测试中平均提升10.9个点，无需人工标注即可接近全监督模型性能。", "motivation": "现有自对弈框架存在优化不稳定性问题，包括：(i)求解器依赖的奖励反馈导致目标非平稳性，(ii)自生成伪标签带来的自举误差", "method": "两阶段框架：1)训练提问者根据明确难度级别和外部语料生成难度校准的问题；2)通过非对称自蒸馏机制训练求解器，使用文档增强的教师模型生成高质量伪标签来监督无文档访问的学生求解器", "result": "模型无关的方法，在9个推理基准和3个骨干模型上平均提升10.9个点，持续优于所有基线方法，接近全监督模型性能", "conclusion": "DARC框架有效解决了自对弈中的不稳定性问题，实现了无需人工标注的自我进化，为自改进人工智能提供了稳定可靠的方法"}}
{"id": "2601.12973", "pdf": "https://arxiv.org/pdf/2601.12973", "abs": "https://arxiv.org/abs/2601.12973", "authors": ["Shuanghong Huang", "Jinlei Xu", "Youchao Zhou", "Yanghao Zhou", "Xuan Zhao", "Chong Feng", "Wenxuan Zhang"], "title": "Pardon? Evaluating Conversational Repair in Large Audio-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) have demonstrated strong performance in spoken question answering (QA), with existing evaluations primarily focusing on answer accuracy and robustness to acoustic perturbations. However, such evaluations implicitly assume that spoken inputs remain semantically answerable, an assumption that often fails in real-world interaction when essential information is missing. In this work, we introduce a repair-aware evaluation setting that explicitly distinguishes between answerable and unanswerable audio inputs. We define answerability as a property of the input itself and construct paired evaluation conditions using a semantic-acoustic masking protocol. Based on this setting, we propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence under answerable conditions and repair behavior under unanswerable conditions. Experiments on two spoken QA benchmarks across diverse LALMs reveal a consistent gap between answer accuracy and conversational reliability: while many models perform well when inputs are answerable, most fail to recognize semantic unanswerability and initiate appropriate conversational repair. These findings expose a limitation of prevailing accuracy-centric evaluation practices and motivate reliability assessments that treat unanswerable inputs as cues for repair and continued interaction.", "AI": {"tldr": "该论文提出了修复感知评估框架和EAR评分指标，用于评估大型音频语言模型在可回答和不可回答音频输入下的表现，揭示了当前模型在识别语义不可回答性和启动对话修复方面的局限性。", "motivation": "现有评估主要关注答案准确性和声学扰动鲁棒性，但假设语音输入在语义上总是可回答的，这在现实交互中当关键信息缺失时往往不成立。", "method": "引入修复感知评估设置，通过语义-声学掩蔽协议构建配对评估条件，提出EAR评分指标联合评估可回答条件下的任务能力和不可回答条件下的修复行为。", "result": "实验发现在两个语音问答基准测试中，模型在可回答条件下表现良好，但大多数模型无法识别语义不可回答性并启动适当的对话修复，显示出准确性与对话可靠性之间的差距。", "conclusion": "当前以准确性为中心的评估实践存在局限性，需要开发能够将不可回答输入视为修复和持续交互线索的可靠性评估方法。"}}
{"id": "2601.13770", "pdf": "https://arxiv.org/pdf/2601.13770", "abs": "https://arxiv.org/abs/2601.13770", "authors": ["Mostapha Benhenda"], "title": "Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench", "AI": {"tldr": "Look-Ahead-Bench是一个评估金融领域大语言模型前瞻性偏差的标准基准测试，通过分析不同市场周期下的性能衰减来区分真实预测能力与记忆性表现，发现标准LLMs存在显著前瞻性偏差，而PiT模型随规模扩大展现出更好的泛化能力。", "motivation": "现有方法主要通过问答测试前瞻性知识，缺乏对实际金融工作流程中模型行为的评估，需要建立标准化基准来测量PiT大语言模型的前瞻性偏差。", "method": "创建Look-Ahead-Bench基准，在现实金融场景中评估模型表现，通过分析时间上不同的市场周期中的性能衰减来区分真实预测和记忆表现，并引入多个量化基线建立性能阈值。", "result": "评估显示标准LLMs（Llama 3.1和DeepSeek 3.2）存在显著的前瞻性偏差，而PiT-Inference模型（Pitinf-Small、Pitinf-Medium和Pitinf-Large）随着规模扩大展现出改进的泛化能力和推理能力。", "conclusion": "该研究为金融大语言模型的时间偏差标准化评估奠定了基础，提供了识别适合实际部署模型的实用框架，代码已在GitHub上开源。"}}
{"id": "2601.12974", "pdf": "https://arxiv.org/pdf/2601.12974", "abs": "https://arxiv.org/abs/2601.12974", "authors": ["Hongyang Ma", "Tiantian Gu", "Huaiyuan Sun", "Huilin Zhu", "Yongxin Wang", "Jie Li", "Wubin Sun", "Zeliang Lian", "Yinghong Zhou", "Yi Gao", "Shirui Wang", "Zhihui Tang"], "title": "Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios", "categories": ["cs.CL"], "comment": "29 pages, 15 figures", "summary": "The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping \"Guideline Adherence\" versus \"Decision Quality\" reveals a prevalent \"High Efficacy, Low Safety\" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.", "AI": {"tldr": "该论文提出了SCMPE基准测试，评估大语言模型从静态知识检索到动态临床对话的转变能力，发现在牙科领域中，模型的主要瓶颈不在于知识保留，而在于主动信息收集和动态状态跟踪能力。", "motivation": "随着大语言模型从被动知识检索器向自主临床代理的转变，需要从静态准确性评估转向动态行为可靠性评估，特别是在牙科这种高质量AI建议能够支持患者参与决策的领域。", "method": "开发了标准化临床管理与性能评估（SCMPE）基准，包括知识导向评估（静态客观任务）和基于工作流程的模拟（多轮模拟患者交互）两个维度。", "result": "模型在静态任务中表现优异，但在动态临床对话中性能显著下降；发现普遍存在\"高疗效、低安全\"风险；RAG在静态任务中减少幻觉，但在动态工作流程中效果有限且不稳定。", "conclusion": "仅靠外部知识无法弥合推理差距，需要领域自适应预训练。研究为牙科大语言模型的能力边界提供了实证分析，为标准化知识与安全自主临床实践之间的差距提供了路线图。"}}
{"id": "2601.13846", "pdf": "https://arxiv.org/pdf/2601.13846", "abs": "https://arxiv.org/abs/2601.13846", "authors": ["Glinskaya Maria"], "title": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.", "AI": {"tldr": "本文提出了虚拟城市主义(VU)框架，使用多模态AI通过合成城市复制品量化城市身份，在东京九个区域的实验中达到约81%的识别准确率，开发了城市身份水平(UIL)指标并识别出核心身份形成元素。", "motivation": "开发计算上可处理的城市身份量化指标，通过AI技术推进城市身份分析的可操作性。", "method": "整合Stable Diffusion和LoRA模型创建东京九个区域的合成城市序列，排除现有导向标记以提取核心身份元素，通过人类评估实验验证感知合法性、量化区域身份并推导核心身份形成元素。", "result": "合成复制品获得平均81%的识别准确率，成功开发UIL指标评估不同区域身份水平，语义分析识别出文化嵌入的类型学作为核心身份形成元素。", "conclusion": "VU框架被证明是AI增强城市分析的可行方法，为自动化多参数身份指标的发展指明了方向。"}}
{"id": "2601.12979", "pdf": "https://arxiv.org/pdf/2601.12979", "abs": "https://arxiv.org/abs/2601.12979", "authors": ["Qingyu Lu", "Liang Ding", "Kanjian Zhang", "Jinxia Zhang", "Dacheng Tao"], "title": "The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check", "categories": ["cs.CL"], "comment": "Under Review", "summary": "The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a \"bitter lesson\": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.", "AI": {"tldr": "尽管扩散大语言模型(dLLMs)在理论上能突破自回归模型的序列延迟瓶颈，但实际评估显示当前dLLMs无法作为可靠的智能体骨干，在具身智能和工具调用场景中均存在系统性失败，需要结合因果推理机制才能有效应用于智能体工作流。", "motivation": "研究扩散大语言模型(dLLMs)是否能真正替代自回归模型作为实时智能交互的骨干，验证其效率提升是否转化为有效的智能体行为。", "method": "在Agentboard和BFCL基准上对dLLMs进行综合评估，涵盖具身智能体(需要长时程规划)和工具调用智能体(需要精确格式化)两种范式，并开发DiffuAgent多智能体评估框架。", "result": "当前dLLMs在具身设置中无法处理时间反馈而重复尝试失败，在工具调用设置中因扩散噪声无法保持符号精度；dLLMs在非因果角色(如记忆总结和工具选择)中有效，但需要将因果推理机制融入去噪过程。", "conclusion": "扩散大语言模型目前不能作为可靠的智能体骨干，需要整合精确的逻辑推理机制到去噪过程中才能实现有效的智能体应用，揭示了效率提升与实际效果之间的差距。"}}
{"id": "2601.13880", "pdf": "https://arxiv.org/pdf/2601.13880", "abs": "https://arxiv.org/abs/2601.13880", "authors": ["Ye Tian", "Zihao Wang", "Onat Gungor", "Xiaoran Fan", "Tajana Rosing"], "title": "LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health", "categories": ["cs.AI"], "comment": null, "summary": "Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.", "AI": {"tldr": "LifeAgentBench是一个大规模QA基准测试，用于评估LLM在长时程、跨维度健康推理方面的能力，包含22,573个问题，并提出LifeAgent作为强基线智能体。", "motivation": "当前缺乏系统性的基准来评估LLM在个性化数字健康支持中的能力，特别是在长时程、跨维度生活方式信号推理方面。", "method": "开发LifeAgentBench基准测试，包含可扩展的构建流程和标准化评估协议，系统评估11个主流LLM，并提出集成多步证据检索和确定性聚合的LifeAgent智能体。", "result": "识别了LLM在长时程聚合和跨维度推理方面的关键瓶颈，LifeAgent相比两个广泛使用的基线方法取得了显著改进。", "conclusion": "该研究为LLM在健康助手领域的评估提供了系统基准，提出的LifeAgent方法展示了在实际日常场景中的潜力，基准测试已公开可用。"}}
{"id": "2601.12983", "pdf": "https://arxiv.org/pdf/2601.12983", "abs": "https://arxiv.org/abs/2601.12983", "authors": ["Jesus-German Ortiz-Barajas", "Jonathan Tonglet", "Vivek Gupta", "Iryna Gurevych"], "title": "ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.", "AI": {"tldr": "ChartAttack是一个评估多模态大语言模型生成误导性图表能力的框架，通过注入误导元素来降低图表问答准确率，平均下降19.6分，突显了MLLM图表生成系统的安全风险。", "motivation": "多模态大语言模型(MLLMs)被广泛用于自动化图表生成，但同时也带来了新的滥用风险，需要评估其被用于大规模生成误导性图表的可能性。", "method": "提出ChartAttack框架，通过向图表设计中注入误导元素来诱导对基础数据的错误解读，并创建AttackViz数据集（包含标注了有效误导元素及其导致的错误答案的图表问答对）。", "result": "实验显示ChartAttack显著降低了MLLM阅读器的问答性能，域内和跨域设置下准确率分别平均下降19.6和14.9个百分点；人类研究显示参与者准确率平均下降20.2个百分点。", "conclusion": "研究强调了在基于MLLM的图表生成系统的设计、评估和部署中，迫切需要加强鲁棒性和安全性考虑，相关代码和数据已公开。"}}
{"id": "2601.13887", "pdf": "https://arxiv.org/pdf/2601.13887", "abs": "https://arxiv.org/abs/2601.13887", "authors": ["Hong Su"], "title": "Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.", "AI": {"tldr": "该论文提出了人类模拟计算（HSC）框架，通过模拟人类思维-行动-学习-反思的闭环过程，增强大语言模型在动态现实环境中的适应和推理能力。", "motivation": "大语言模型仅依赖文本数据的局限性使其难以适应开放动态的现实环境，无法有效验证推理结果和进行主动环境交互。", "method": "提出HSC框架，建立包含思考、行动、学习、反思和活动调度的闭环推理过程，融入人类常用思维策略如特征导向推理、行动扩展范围和环境反馈驱动的实时学习。", "result": "理论分析表明，人类模拟策略无法仅从语言材料中学习获得，需要类人推理过程和基于行动的推理方法才能实现稳健适应和有效环境交互。", "conclusion": "HSC框架通过模拟人类认知过程，为大语言模型提供了在现实环境中进行主动学习、自我改进和环境交互的新范式，突破了纯文本推理的局限性。"}}
{"id": "2601.12995", "pdf": "https://arxiv.org/pdf/2601.12995", "abs": "https://arxiv.org/abs/2601.12995", "authors": ["Runxuan Liu", "Xianhao Ou", "Xinyan Ma", "Jiyuan Wang", "Jiafeng Liang", "Jiaqi Li", "Tao He", "Zheng Chu", "Rongchuan Mu", "Zekun Wang", "Baoxin Wang", "Dayong Wu", "Ming Liu", "Shijin Wang", "Guoping Hu", "Bing Qin"], "title": "Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.", "AI": {"tldr": "该论文提出图推理范式(GRP)和PASC-GRPO方法，通过结构化图表示和过程感知奖励机制，解决了传统文本推理中的计算瓶颈、监督粗糙、奖励作弊等问题，在数学推理和代码生成任务上取得显著提升。", "motivation": "当前大语言模型的推理主要生成纯文本，对非结构化数据进行语义评估会造成训练计算瓶颈，且现有RLVR方法存在监督粒度粗糙、奖励作弊、训练成本高和泛化能力差等问题。", "method": "提出图推理范式(GRP)实现结构化和符号化推理，使用带有步骤级认知标签的图结构表示；设计PASC-GRPO方法，通过结构化评估替代语义评估，利用图结构结果奖励实现过程感知验证，并通过分层裁剪优势估计缓解奖励作弊。", "result": "实验结果表明，在数学推理和代码生成任务上取得了显著改进。", "conclusion": "GRP和PASC-GRPO方法有效解决了当前推理方法的局限性，通过结构化表示和过程感知奖励机制提升了推理性能，数据、模型和代码将后续发布。"}}
{"id": "2601.13904", "pdf": "https://arxiv.org/pdf/2601.13904", "abs": "https://arxiv.org/abs/2601.13904", "authors": ["Jaeyoung Moon", "Youjin Choi", "Yucheon Park", "David Melhart", "Georgios N. Yannakakis", "Kyung-Joong Kim"], "title": "PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation", "categories": ["cs.AI", "cs.HC"], "comment": "CHI '26 Accepted paper", "summary": "Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.", "AI": {"tldr": "PREFAB是一种低成本回顾式自标注方法，通过检测情感变化区域而非全标注来减轻标注负担，使用偏好学习模型和预览机制提高标注效率和质量。", "motivation": "现有情感状态标注方法需要用户在整个会话中持续标注，耗时耗力且容易疲劳出错，需要更高效的标注方案。", "method": "基于峰终规则和情感序数表示，采用偏好学习模型检测相对情感变化，只标注选定片段并通过插值补全其余部分，引入预览机制提供上下文提示。", "result": "PREFAB在建模情感变化方面优于基线方法，减轻了工作负担（有条件地减轻时间负担），提高了标注者信心且不降低标注质量。", "conclusion": "PREFAB提供了一种有效的情感标注替代方案，在保持标注质量的同时显著降低了标注成本和工作负担。"}}
{"id": "2601.13018", "pdf": "https://arxiv.org/pdf/2601.13018", "abs": "https://arxiv.org/abs/2601.13018", "authors": ["Ghislain Dorian Tchuente Mondjo"], "title": "Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at \"EAI AFRICOMM 2025 - 17th EAI International Conference on Communications and Networks in Africa\"", "summary": "Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.", "AI": {"tldr": "论文提出BiAtt-BiRNN-HateXplain模型，通过双向注意力机制和双向RNN层解决仇恨言论检测中注意力不一致的问题，提高检测性能、可解释性并减少无意识偏见。", "motivation": "现有仇恨言论检测模型存在注意力变异性问题，导致解释不一致、预测不稳定和学习困难。需要更透明且能处理序列数据的解释性模型。", "method": "提出双向注意力BiRNN模型，结合多任务学习同时进行解释和分类，利用BiRNN层考虑输入数据的序列特性。", "result": "在HateXplain数据上的实验结果显示，模型在检测性能、可解释性方面有明显改进，并减少了无意识偏见错误。", "conclusion": "该模型相比复杂的LLM更易解释，通过正确处理解释任务可以提升分类准确性并减少针对特定社群的偏见。"}}
{"id": "2601.13969", "pdf": "https://arxiv.org/pdf/2601.13969", "abs": "https://arxiv.org/abs/2601.13969", "authors": ["Joaquín Polonuer", "Lucas Vittor", "Iñaki Arango", "Ayush Noori", "David A. Clifton", "Luciano Del Corro", "Marinka Zitnik"], "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval", "categories": ["cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.", "AI": {"tldr": "ARK是一个自适应的知识图谱检索器，使用语言模型控制广度-深度权衡，通过全局搜索和邻域探索两种操作实现多跳遍历，在STaRK基准上显著优于现有方法，并能通过无标签模仿蒸馏到小模型中。", "motivation": "解决知识图谱检索中广度搜索与深度遍历的平衡问题，传统相似性检索器覆盖广但深度不足，而遍历方法依赖种子节点选择且在多实体多关系查询中容易失败。", "method": "提出ARK自适应知识检索器，使用语言模型控制两种操作：全局词汇搜索和一跳邻域探索，可组合成多跳遍历，无需种子选择、预设跳数或检索训练。", "result": "在STaRK基准上达到59.1%平均Hit@1和67.4平均MRR，比检索基线和无训练代理方法提升高达31.4% Hit@1和28.0% MRR。通过无标签模仿蒸馏到8B模型，在多个数据集上显著提升性能。", "conclusion": "ARK成功解决了知识图谱检索的广度-深度权衡问题，提供了一种灵活的自适应检索方法，并能有效蒸馏到更小的模型中，保持了教师模型的高性能。"}}
{"id": "2601.13024", "pdf": "https://arxiv.org/pdf/2601.13024", "abs": "https://arxiv.org/abs/2601.13024", "authors": ["Chongyuan Dai", "Yaling Shen", "Jinpeng Hu", "Zihan Gao", "Jia Li", "Yishun Jiang", "Yaxiong Wang", "Liu Liu", "Zongyuan Ge"], "title": "Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses", "categories": ["cs.CL"], "comment": "24 pages, 10 figures, 9 Tables", "summary": "Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \\underline{\\textsc{E}}licited \\underline{\\textsc{D}}istinct \\underline{\\textsc{A}}ffective \\underline{\\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.", "AI": {"tldr": "该论文提出了CEDAR多模态基准测试，专门评估大语言模型在文化情感理解方面的表现，发现当前模型在语言一致性和文化对齐方面存在分离现象。", "motivation": "现有的大语言模型文化对齐评估主要关注陈述性知识（如地理事实和社会习俗），无法捕捉不同文化视角下的主观情感解释差异，需要专门的情感文化对齐评估工具。", "method": "开发了CEDAR基准测试，采用新颖的流程：利用LLM生成临时标签识别跨文化情感差异实例，然后通过严格人工评估获得可靠的真实标注，包含10,962个实例，涵盖7种语言和14种细粒度情感类别。", "result": "对17个代表性多语言模型的全面评估显示，语言一致性和文化对齐之间存在分离现象，表明当前模型在基于文化的情感理解方面仍面临重大挑战。", "conclusion": "文化情感理解是大语言模型文化对齐的关键维度，当前模型在这方面存在显著不足，CEDAR基准为评估和改进模型的文化情感能力提供了重要工具。"}}
{"id": "2601.14027", "pdf": "https://arxiv.org/pdf/2601.14027", "abs": "https://arxiv.org/abs/2601.14027", "authors": ["Junqi Liu", "Zihao Zhou", "Zekai Zhu", "Marco Dos Santos", "Weikun He", "Jiawei Liu", "Ran Wang", "Yunzhou Xie", "Junqiao Zhao", "Qiufeng Wang", "Lihong Zhi", "Jia Li", "Wenda Li"], "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics", "categories": ["cs.AI"], "comment": null, "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.", "AI": {"tldr": "提出使用通用编程代理作为形式数学推理器的新范式，通过Claude Code与Numina-Lean-MCP结合，无需训练即可实现高性能定理证明，在Putnam 2025测试中取得满分成绩。", "motivation": "(1)通用编程代理为多样化推理任务提供自然接口；(2)只需替换基础模型即可提升性能，无需训练；(3)MCP支持灵活扩展和自主调用专业工具，避免复杂设计", "method": "基于新范式开发Numina-Lean-Agent，结合Claude Code与Numina-Lean-MCP，实现与Lean的自主交互、相关定理检索、非形式证明和辅助推理工具", "result": "使用Claude Opus 4.5基础模型，在Putnam 2025测试中解决所有问题(12/12)，与最佳闭源系统性能相当；成功形式化Brascamp-Lieb定理", "conclusion": "该范式展示了使用通用编程代理进行形式数学推理的有效性，提供了更灵活和可复现的解决方案，具有广泛的应用潜力"}}
{"id": "2601.13035", "pdf": "https://arxiv.org/pdf/2601.13035", "abs": "https://arxiv.org/abs/2601.13035", "authors": ["Xu Xiaodan", "Hu Xiaolin"], "title": "SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification", "categories": ["cs.CL", "cs.LG"], "comment": "in progress", "summary": "Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \\textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\\% on FB15k-237 and +3.4\\% on YAGO3-10.", "AI": {"tldr": "SASA框架通过分离注意力机制和语义感知对比学习解决知识图谱三元组分类中的语义交互不足和表示学习不充分问题，在两个基准数据集上显著超越现有最佳方法。", "motivation": "知识图谱存在不可靠知识问题，现有文本方法虽然提升了泛化能力，但存在两个关键挑战：忽略不同KG组件间的有效语义交互，以及单一二元分类训练目标导致语义表示学习不充分。", "method": "提出SASA框架：1) 分离注意力机制将三元组编码为解耦的上下文表示并通过更有效的交互方式融合；2) 语义感知分层对比学习作为辅助训练目标，同时考虑局部和全局层次的对比学习。", "result": "在FB15k-237数据集上准确率提升+5.9%，在YAGO3-10数据集上提升+3.4%，显著超越现有最先进方法。", "conclusion": "SASA通过创新的注意力机制和对比学习方法有效解决了三元组分类中的语义交互和表示学习问题，为知识图谱的可靠性验证提供了更强大的解决方案。"}}
{"id": "2601.14096", "pdf": "https://arxiv.org/pdf/2601.14096", "abs": "https://arxiv.org/abs/2601.14096", "authors": ["Benedikt Hartl", "Léo Pio-Lopez", "Chris Fields", "Michael Levin"], "title": "Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems", "categories": ["cs.AI"], "comment": "41 pages, 5 figures", "summary": "The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.", "AI": {"tldr": "该论文提出了一个统一框架，认为认知过程的核心是嵌入空间的重映射和导航这两个不变原则，适用于从生物系统到人工智能的各种智能体。", "motivation": "寻求不同来源、组成和基质的智能体问题解决的整合视图，探索跨尺度决策的普适性原则。", "method": "通过比较分析生物系统（从亚细胞化学网络到生物群体）和现代AI系统（如变换器、扩散模型）的认知过程，识别共同机制。", "result": "发现生物系统和AI系统都通过嵌入空间重映射和迭代误差最小化的导航过程实现认知功能，这构成了基质独立的认知不变性。", "conclusion": "重映射和导航的双重原则为理解自然和合成系统的认知提供了统一框架，有助于跨尺度的自适应智能工程。"}}
{"id": "2601.13044", "pdf": "https://arxiv.org/pdf/2601.13044", "abs": "https://arxiv.org/abs/2601.13044", "authors": ["Warit Sirichotedumrong", "Adisai Na-Thalang", "Potsawee Manakul", "Pittawat Taveekitworachai", "Sittipong Sripaisarnmongkol", "Kunat Pipatanakul"], "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition", "categories": ["cs.CL"], "comment": "Models and datasets are publicly available on https://huggingface.co/collections/typhoon-ai/typhoon-asr-technical-report ; Project Page: https://opentyphoon.ai/model/typhoon-asr-realtime", "summary": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.", "AI": {"tldr": "Typhoon ASR Real-time是一个115M参数的流式泰语语音识别模型，通过文本标准化和课程学习实现低延迟高精度，比Whisper Large-v3计算成本降低45倍，同时发布标准化评测基准。", "motivation": "现有泰语ASR主要基于离线架构，缺乏高效流式解决方案，预训练模型如Whisper延迟过高不适用于实时应用。", "method": "采用FastConformer-Transducer架构，通过严格的文本标准化解决泰语转录系统歧义，使用两阶段课程学习进行东北方言适配，保持中央泰语性能。", "result": "紧凑模型实现45倍计算成本降低，精度与Whisper Large-v3相当，解决了数字表达和重复标记等系统歧义问题。", "conclusion": "文本标准化可与模型缩放相媲美，提出的方法填补了泰语流式ASR空白，发布的基准数据集将促进研究社区标准化评估。"}}
{"id": "2601.14171", "pdf": "https://arxiv.org/pdf/2601.14171", "abs": "https://arxiv.org/abs/2601.14171", "authors": ["Qianli Ma", "Chang Guo", "Zhiheng Tian", "Siyu Wang", "Jipeng Xiao", "Yuanhao Yue", "Zhipeng Zhang"], "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "categories": ["cs.AI"], "comment": null, "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "AI": {"tldr": "RebuttalAgent是一个多智能体框架，将反驳信生成重新定义为以证据为中心的计划任务，通过分解审稿意见、构建混合上下文和外部搜索来确保每个论点都有证据支持，在覆盖率、忠实度和策略连贯性方面优于基线方法。", "motivation": "当前的反驳信生成方法存在幻觉、忽略批评和缺乏可验证基础的问题，需要更精确地匹配审稿人意图和稿件细节。", "method": "提出多智能体框架，将复杂反馈分解为原子问题，动态构建混合上下文（压缩摘要+高保真文本），集成自主按需外部搜索模块，并在起草前生成可检查的响应计划。", "result": "在RebuttalBench上验证，系统在覆盖率、忠实度和策略连贯性方面优于强基线方法。", "conclusion": "RebuttalAgent为同行评审过程提供了一个透明且可控的助手，通过证据锚定的方法有效解决了现有反驳信生成的局限性。"}}
{"id": "2601.13050", "pdf": "https://arxiv.org/pdf/2601.13050", "abs": "https://arxiv.org/abs/2601.13050", "authors": ["Lars Klöser", "Mika Beele", "Bodo Kraft"], "title": "Profiling German Text Simplification with Interpretable Model-Fingerprints", "categories": ["cs.CL"], "comment": "Presented at 2nd International Conference on Explainable AI for Neural and Symbolic Systems", "summary": "While Large Language Models (LLMs) produce highly nuanced text simplifications, developers currently lack tools for a holistic, efficient, and reproducible diagnosis of their behavior. This paper introduces the Simplification Profiler, a diagnostic toolkit that generates a multidimensional, interpretable fingerprint of simplified texts. Multiple aggregated simplifications of a model result in a model's fingerprint. This novel evaluation paradigm is particularly vital for languages, where the data scarcity problem is magnified when creating flexible models for diverse target groups rather than a single, fixed simplification style. We propose that measuring a model's unique behavioral signature is more relevant in this context as an alternative to correlating metrics with human preferences. We operationalize this with a practical meta-evaluation of our fingerprints' descriptive power, which bypasses the need for large, human-rated datasets. This test measures if a simple linear classifier can reliably identify various model configurations by their created simplifications, confirming that our metrics are sensitive to a model's specific characteristics. The Profiler can distinguish high-level behavioral variations between prompting strategies and fine-grained changes from prompt engineering, including few-shot examples. Our complete feature set achieves classification F1-scores up to 71.9 %, improving upon simple baselines by over 48 percentage points. The Simplification Profiler thus offers developers a granular, actionable analysis to build more effective and truly adaptive text simplification systems.", "AI": {"tldr": "本文提出了Simplification Profiler工具包，通过生成多维可解释的文本简化指纹来诊断大语言模型的简化行为，无需依赖大规模人工标注数据集即可评估模型特性。", "motivation": "当前缺乏对LLM文本简化行为进行整体、高效和可重复诊断的工具，特别是在数据稀缺的语言环境中需要为不同目标群体创建灵活模型而非单一简化风格时。", "method": "开发Simplification Profiler工具包，通过聚合多个简化文本来生成模型指纹，使用线性分类器验证指纹的描述能力，测量能否可靠识别不同模型配置。", "result": "完整特征集达到71.9%的分类F1分数，比简单基线提高48个百分点以上，能够区分提示策略的高层行为变化和提示工程的细粒度变化。", "conclusion": "Simplification Profiler为开发者提供了细粒度、可操作的分析方法，有助于构建更有效和真正自适应的文本简化系统。"}}
{"id": "2601.14192", "pdf": "https://arxiv.org/pdf/2601.14192", "abs": "https://arxiv.org/abs/2601.14192", "authors": ["Xiaofang Yang", "Lijun Li", "Heng Zhou", "Tong Zhu", "Xiaoye Qu", "Yuchen Fan", "Qianshan Wei", "Rui Ye", "Li Kang", "Yiran Qin", "Zhiqiang Kou", "Daizong Liu", "Qi Li", "Ning Ding", "Siheng Chen", "Jing Shao"], "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "categories": ["cs.AI", "cs.CL"], "comment": "35 pages, 200 references", "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "AI": {"tldr": "本文系统研究了智能体系统的效率问题，从内存、工具学习和规划三个核心组件入手，分析了延迟、令牌消耗、步骤数等成本因素，提出了效率评估的双重比较方法，并总结了效率导向的基准测试和未来研究方向。", "motivation": "虽然大型语言模型在智能体系统中的有效性不断提升，但效率问题（对实际部署至关重要）往往被忽视，因此需要从系统层面全面研究智能体效率。", "method": "通过回顾近期各种方法，分析内存管理（上下文压缩）、工具学习（最小化工具调用）和规划（受控搜索机制）三个核心组件的效率优化技术，并建立效率评估框架。", "result": "提出了两种互补的效率评估方式：固定成本预算下的有效性比较，以及同等有效性水平下的成本比较，建立了效果与成本的帕累托前沿分析框架。", "conclusion": "智能体系统效率研究需要多维度考量，本文为效率评估提供了系统框架和基准测试方法，指出了未来研究的关键挑战和发展方向。"}}
{"id": "2601.13099", "pdf": "https://arxiv.org/pdf/2601.13099", "abs": "https://arxiv.org/abs/2601.13099", "authors": ["Abdellah El Mekki", "Samar M. Magdy", "Houdaifa Atou", "Ruwa AbuHweidi", "Baraah Qawasmeh", "Omer Nacar", "Thikra Al-hibiri", "Razan Saadie", "Hamzah Alsayadi", "Nadia Ghezaiel Hammouda", "Alshima Alkhazimi", "Aya Hamod", "Al-Yas Al-Ghafri", "Wesam El-Sayed", "Asila Al sharji", "Mohamad Ballout", "Anas Belfathi", "Karim Ghaddar", "Serry Sibaee", "Alaa Aoun", "Areej Asiri", "Lina Abureesh", "Ahlam Bashiti", "Majdal Yousef", "Abdulaziz Hafiz", "Yehdih Mohamed", "Emira Hamedtou", "Brakehe Brahim", "Rahaf Alhamouri", "Youssef Nafea", "Aya El Aatar", "Walid Al-Dhabyani", "Emhemed Hamed", "Sara Shatnawi", "Fakhraddin Alwajih", "Khalid Elkhidir", "Ashwag Alasmari", "Abdurrahman Gerrio", "Omar Alshahri", "AbdelRahim A. Elmadany", "Ismail Berrada", "Amir Azad Adli Alkathiri", "Fadi A Zaraket", "Mustafa Jarrar", "Yahya Mohamed El Hadj", "Hassan Alhuzali", "Muhammad Abdul-Mageed"], "title": "Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs", "categories": ["cs.CL"], "comment": "Project resources will be available here: https://github.com/UBC-NLP/Alexandria", "summary": "Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \\textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.", "AI": {"tldr": "Alexandria是一个大规模社区驱动的人工翻译数据集，覆盖13个阿拉伯国家和11个重要领域，提供方言翻译资源，包含10.7万个对话样本，用于机器翻译和大型语言模型的训练与评估。", "motivation": "解决阿拉伯语高度双言现象导致的机器翻译系统在方言输入上表现不佳的问题，满足数百万方言使用者的实际需求。", "method": "创建包含13个阿拉伯国家、11个领域的大规模人工翻译数据集，提供城市来源元数据和性别配置标注，捕捉真实的方言变体。", "result": "数据集包含107K个多轮对话样本，可作为训练资源和评估基准，自动和人工评估揭示了阿拉伯语感知LLM在不同方言翻译中的能力和持续挑战。", "conclusion": "Alexandria数据集填补了阿拉伯方言翻译资源的空白，为机器翻译和语言模型提供了重要的训练和评估基础，同时揭示了当前技术在方言翻译方面仍面临的重大挑战。"}}
{"id": "2601.13105", "pdf": "https://arxiv.org/pdf/2601.13105", "abs": "https://arxiv.org/abs/2601.13105", "authors": ["Liu Kaipeng", "Wu Ling"], "title": "Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification", "categories": ["cs.CL"], "comment": "19pages, 1figure", "summary": "This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.", "AI": {"tldr": "本研究通过结合LoRA微调大语言模型和RAG框架，实现了英语双及物构式的自动识别，LoRA微调的Qwen3-8B模型在性能上显著优于原生模型和纯理论RAG系统。", "motivation": "研究旨在探索如何有效识别英语双及物构式，通过结合参数高效微调技术(LoRA)和检索增强生成(RAG)框架来提升大语言模型在此任务上的性能。", "method": "采用LoRA微调Qwen3-8B大语言模型，结合RAG框架，在英国国家语料库的标注数据上进行二元分类任务。", "result": "LoRA微调的Qwen3-8B模型性能显著优于原生Qwen3-MAX模型和纯理论RAG系统，错误分析显示微调使模型从表层形式匹配转向基于语义的理解。", "conclusion": "LoRA微调结合RAG框架能有效提升大语言模型在句法构式识别任务上的性能，实现从形式匹配到语义理解的转变。"}}
{"id": "2601.13111", "pdf": "https://arxiv.org/pdf/2601.13111", "abs": "https://arxiv.org/abs/2601.13111", "authors": ["Hassan Soliman", "Vivek Gupta", "Dan Roth", "Iryna Gurevych"], "title": "CORE-T: COherent REtrieval of Tables for Text-to-SQL", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Preprint under review. Code and data available at: https://github.com/UKPLab/arxiv2026-core-t", "summary": "Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.", "AI": {"tldr": "CORE-T是一个无需训练的框架，通过LLM生成表目的元数据和预计算表兼容性缓存，在推理时结合密集检索和LLM选择，显著提升多表SQL查询中的表选择准确性和执行效率。", "motivation": "现实文本到SQL工作流需要连接多表，准确检索相关表成为性能瓶颈。现有密集检索方法召回率高但包含干扰项，而连接感知方法需要额外假设或计算开销大。", "method": "提出CORE-T框架：1)用LLM生成表目的元数据；2)预计算轻量级表兼容性缓存；3)推理时先用密集检索获取候选表；4)单次LLM调用选择可连接子集；5)简单调整步骤恢复强兼容表。", "result": "在Bird、Spider和MMQA数据集上：表选择F1提升高达22.7分，检索表数量减少42%，多表执行准确率在Bird上提升5.0分，MMQA上提升6.9分，token使用量比LLM密集型基线减少4-5倍。", "conclusion": "CORE-T通过结合密集检索的召回能力和LLM的语义理解，有效解决了多表SQL查询中的表选择问题，在保持高准确性的同时显著降低了计算开销。"}}
{"id": "2601.13115", "pdf": "https://arxiv.org/pdf/2601.13115", "abs": "https://arxiv.org/abs/2601.13115", "authors": ["Fengran Mo", "Yifan Gao", "Sha Li", "Hansi Zeng", "Xin Liu", "Zhaoxuan Tan", "Xian Li", "Jianshu Chen", "Dakuo Wang", "Meng Jiang"], "title": "Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.", "AI": {"tldr": "本文提出了一种基于强化学习的多轮对话搜索代理，通过跨轮次的搜索和推理交替进行，能够动态适应用户意图的演变，在四个对话基准测试中超越了现有基线方法。", "motivation": "现有方法通常采用静态的重写-检索-生成流程，分别优化不同步骤而忽略了混合主动行为的联合优化，且当前深度搜索代理主要针对单轮场景，缺乏处理多轮交互的能力。", "method": "引入一个对话代理，通过强化学习训练，在轮次间交替进行搜索和推理，使用针对演进用户目标的定制奖励来学习探索性和适应性行为。", "result": "在四个广泛使用的对话基准测试中，该方法超越了多个现有强基线，证明了其有效性。", "conclusion": "通过强化学习训练的跨轮次搜索推理交替机制能够有效处理多轮对话中用户意图的动态演变，为多轮人机交互提供了更有效的解决方案。"}}
{"id": "2601.13137", "pdf": "https://arxiv.org/pdf/2601.13137", "abs": "https://arxiv.org/abs/2601.13137", "authors": ["Yuan Gao", "Zhigang Liu", "Xinyu Yao", "Bo Chen", "Xiaobing Zhao"], "title": "Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains", "categories": ["cs.CL"], "comment": "13 pages, 5 figures", "summary": "With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.", "AI": {"tldr": "提出对抗对齐框架VC-LLM，通过持续预训练、指令微调和对抗训练提升大语言模型在敏感领域的价值一致性，在双语测试中优于主流模型。", "motivation": "解决大语言模型在敏感领域（种族、社会、政治）的偏见和价值不一致问题", "method": "使用对抗训练框架：Attacker生成争议查询，Actor生成价值一致响应，Critic过滤确保质量；包含持续预训练和指令微调", "result": "VC-LLM在中英文测试中表现优于现有主流模型，验证了方法的有效性", "conclusion": "提出的对抗对齐框架能有效提升LLM在敏感领域的价值一致性，但需注意论文包含可能具有冒犯性或有害的示例"}}
{"id": "2601.13155", "pdf": "https://arxiv.org/pdf/2601.13155", "abs": "https://arxiv.org/abs/2601.13155", "authors": ["Zimeng Wu", "Donghao Wang", "Chaozhe Jin", "Jiaxin Chen", "Yunhong Wang"], "title": "Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\\times$ and 2.29$\\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.", "AI": {"tldr": "SPTS是一个无需训练的框架，通过部分注意力探测和低秩变换探测策略实现高效的长上下文LLM推理，在保持性能的同时实现显著加速。", "motivation": "解决长上下文推理中计算开销大的问题，现有token导向方法存在加速潜力有限、代理信号过时和冗余干扰等局限", "method": "提出Partial Attention Probing(PAP)选择信息丰富的token，Low-rank Transformation Probing(LTP)预测token变换，以及Multi-Stage Delayed Pruning(MSDP)策略重新分配跳过预算", "result": "在预填充和端到端生成方面分别实现最高2.46倍和2.29倍的加速，同时保持最先进的模型性能", "conclusion": "SPTS框架有效解决了长上下文推理的效率问题，在速度和准确性之间取得了良好平衡，为高效LLM推理提供了有前景的解决方案"}}
{"id": "2601.13178", "pdf": "https://arxiv.org/pdf/2601.13178", "abs": "https://arxiv.org/abs/2601.13178", "authors": ["Joseph Gatto", "Parker Seegmiller", "Timothy Burdick", "Philip Resnik", "Roshnik Rahat", "Sarah DeLozier", "Sarah M. Preum"], "title": "Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages", "categories": ["cs.CL"], "comment": "19 Pages, 5 Figures", "summary": "Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `\"which message is more medically urgent\" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.\n  We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage", "AI": {"tldr": "该论文提出了首个大型公开数据集PMR-Bench，用于研究异步门诊门户消息中的医疗分诊任务，将患者消息分诊视为成对推理问题，开发了两种基于LLM的模型在医疗紧急度评估上取得了显著提升。", "motivation": "医疗分诊是基于医疗需求分配资源和优先处理患者的重要任务，但目前缺乏大规模公开数据集来研究异步门诊门户消息的分诊问题，需要开发有效的自动化方法来评估医疗紧急度。", "method": "提出PMR-Bench数据集（1569条消息和2000+测试对），将患者消息分诊建模为成对推理问题；开发自动化数据标注策略；训练两种模型：基于Bradley-Terry的UrgentReward和基于next token prediction的UrgentSFT。", "result": "UrgentSFT在PMR-Bench上表现最佳，UrgentReward在低资源设置中具有优势；UrgentSFT-8B和UrgentReward-8B相比现成的8B模型在收件箱排序指标上分别提升了15和16个百分点。", "conclusion": "该研究成功建立了医疗消息分诊的基准数据集和有效方法，证明了LLM在医疗紧急度评估任务中的潜力，为实际医疗分诊场景提供了实用的解决方案。"}}
{"id": "2601.13183", "pdf": "https://arxiv.org/pdf/2601.13183", "abs": "https://arxiv.org/abs/2601.13183", "authors": ["Sergio Servantez", "Sarah B. Lawsky", "Rajiv Jain", "Daniel W. Linna", "Kristian Hammond"], "title": "OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand", "categories": ["cs.CL"], "comment": "25 pages, 9 Figures, 15 tables", "summary": "Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.", "AI": {"tldr": "OpenExempt是一个法律推理诊断评估框架和基准，通过动态生成自然语言推理任务来精细评估语言模型在法律领域的推理能力，发现了模型在长推理路径和混淆语句下的性能断崖现象。", "motivation": "现有推理基准主要使用静态问答对，只能提供性能快照，将复杂行为压缩为单一准确率指标，在法律等复杂规则领域存在局限性，无法有效隔离特定失败模式。", "method": "使用专家制作的美国破产法法规符号表示，动态生成大量自然语言推理任务及其机器可计算解决方案，用户可以精细控制任务复杂度和范围，单独测试各个推理技能。", "result": "在13个不同语言模型上的实验显示，只有在长推理路径和存在混淆语句的情况下才会出现明显的性能断崖现象。", "conclusion": "OpenExempt框架和基准的发布支持研究和改进下一代推理系统，提供了更精细的法律推理评估工具。"}}
{"id": "2601.13217", "pdf": "https://arxiv.org/pdf/2601.13217", "abs": "https://arxiv.org/abs/2601.13217", "authors": ["Bingsen Chen", "Boyan Li", "Ping Nie", "Yuyu Zhang", "Xi Ye", "Chen Zhao"], "title": "Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.", "AI": {"tldr": "Mr Dre评估套件揭示深度研究代理在多次报告修订中存在显著局限性：虽然能处理大部分用户反馈，但会退化16-27%的已有内容和引用质量，且无法保持先前编辑的完整性。", "motivation": "现有基准将报告生成视为一次性写作任务，与人类研究者通过自我反思或同行反馈迭代起草和修订报告的方式存在根本差异，需要探索深度研究代理是否能可靠地根据用户反馈修订报告。", "method": "引入Mr Dre评估套件，包含：(1)统一的长报告评估协议（涵盖全面性、事实性和呈现质量）；(2)人工验证的反馈模拟流程，用于多轮修订评估。分析了五个不同的深度研究代理。", "result": "代理能处理大部分用户反馈，但会退化16-27%的先前覆盖内容和引用质量。在多轮修订中，即使表现最佳的代理也存在显著改进空间，会破坏反馈范围之外的内容且无法保持早期编辑。", "conclusion": "深度研究代理在报告修订方面存在根本性挑战，通过即时推理修复（如提示工程和专用修订子代理）无法轻易解决这些问题，表明需要更深入的方法来改进代理的修订能力。"}}
{"id": "2601.13228", "pdf": "https://arxiv.org/pdf/2601.13228", "abs": "https://arxiv.org/abs/2601.13228", "authors": ["Tianqi Du", "Lizhe Fang", "Weijie Yang", "Chenheng Zhang", "Zeming Wei", "Yifei Wang", "Yisen Wang"], "title": "Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.", "AI": {"tldr": "A3框架将自回归模型扩展为支持任意顺序和子集生成的统一框架，结合了自回归模型的深度建模优势和扩散模型的灵活性，在保持高质量生成的同时支持双向和平行解码。", "motivation": "扩散语言模型虽然支持任意顺序生成和双向条件控制，但由于单步依赖限制，建模深度不足，样本质量和稳定性不如自回归模型。需要一种既能保持自回归模型深度建模优势，又能获得扩散模型灵活性的方法。", "method": "提出A3（Any-order Any-subset Autoregressive modeling）框架，通过结构化多组预测过程重新构建扩散式训练。采用双流注意力架构和渐进适应策略，将预训练的自回归模型转换为支持任意顺序预测。", "result": "在问答、常识推理和故事填充任务上的实验表明，A3在性能上优于基于扩散的模型，同时保持了灵活的解码能力。", "conclusion": "A3提供了一个统一的方法，实现了灵活、高效且新颖的语言建模范式，既保留了自回归模型的概率严谨性和多层依赖建模能力，又继承了扩散模型的双向和平行生成灵活性。"}}
{"id": "2601.13247", "pdf": "https://arxiv.org/pdf/2601.13247", "abs": "https://arxiv.org/abs/2601.13247", "authors": ["Baochang Ren", "Yunzhi Yao", "Rui Sun", "Shuofei Qiao", "Ningyu Zhang", "Huajun Chen"], "title": "Aligning Agentic World Models via Knowledgeable Experience Learning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.", "AI": {"tldr": "WorldMind框架通过构建符号化世界知识库来解决LLMs的物理幻觉问题，利用环境反馈统一过程经验和目标经验，在EB-ALFRED和EB-Habitat基准测试中表现优异且具有出色的跨模型和环境迁移能力。", "motivation": "当前大型语言模型存在模态鸿沟：拥有丰富语义知识但缺乏对物理世界不变法则的程序性基础，导致生成逻辑正确但物理上不可执行的计划（物理幻觉）。现有方法依赖资源密集的训练或微调，难以适应物理动态的开放性变化。", "method": "引入WorldMind框架，通过综合环境反馈自主构建符号化世界知识库，统一过程经验（通过预测误差强制物理可行性）和目标经验（通过成功轨迹指导任务最优性）。", "result": "在EB-ALFRED和EB-Habitat上的实验表明，WorldMind相比基线方法取得了更优越的性能，并展现出显著的跨模型和跨环境迁移能力。", "conclusion": "WorldMind有效解决了LLMs的物理幻觉问题，提供了一种非参数化的适应性解决方案，避免了持续昂贵的重新训练，为构建更可靠的世界模型提供了新途径。"}}
{"id": "2601.13251", "pdf": "https://arxiv.org/pdf/2601.13251", "abs": "https://arxiv.org/abs/2601.13251", "authors": ["Ebubekir Tosun", "Mehmet Emin Buldur", "Özay Ezerceli", "Mahmoud ElHussieni"], "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.", "AI": {"tldr": "该论文提出了一个大规模语义聚类系统，专门解决神经网络嵌入无法区分同义词和反义词的问题，通过三路语义关系判别器和软到硬聚类算法，生成了290万个高精度语义聚类。", "motivation": "神经网络嵌入存在显著盲点：无法可靠区分同义词和反义词，导致提高相似度阈值时仍会将对立词分组在一起。", "method": "构建包含84.3万个概念对的数据集，使用Gemini 2.5-Flash LLM增强和人工验证；提出三路语义关系判别器（达到90%宏F1分数）；开发新颖的软到硬聚类算法，采用拓扑感知的两阶段扩展-剪枝程序。", "result": "系统处理了1500万个词汇项，评估了5.2亿个潜在关系，最终生成了290万个高精度语义聚类。", "conclusion": "该方法实现了高精度语义搜索和检索增强生成，特别适用于形态丰富和低资源语言，解决了现有同义词数据库稀疏的问题。"}}
{"id": "2601.13253", "pdf": "https://arxiv.org/pdf/2601.13253", "abs": "https://arxiv.org/abs/2601.13253", "authors": ["Ebubekir Tosun", "Mehmet Emin Buldur", "Özay Ezerceli", "Mahmoud ElHussieni"], "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.", "AI": {"tldr": "提出一种混合方法生成低资源语言的大规模语义关系数据集，通过土耳其语语料库验证，包含843,000个语义对，成本仅65美元，性能达到90%准确率", "motivation": "解决土耳其语等低资源语言在NLP领域中语义关系数据稀缺的问题", "method": "三阶段混合方法：1) FastText嵌入与凝聚聚类识别语义簇 2) Gemini 2.5-Flash自动分类语义关系 3) 整合词典资源", "result": "生成包含843,000个土耳其语语义对的数据集（同义词、反义词、同下位词），规模是现有资源的10倍，在下游任务中达到90%的检索准确率和F1分数", "conclusion": "该方法可扩展解决低资源语言的语料稀缺问题，数据集和模型已公开发布，适用于其他低资源语言"}}
{"id": "2601.13260", "pdf": "https://arxiv.org/pdf/2601.13260", "abs": "https://arxiv.org/abs/2601.13260", "authors": ["Sawsan Alqahtani", "Mir Tafseer Nayeem", "Md Tahmid Rahman Laskar", "Tasnim Mohiuddin", "M Saiful Bari"], "title": "Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EACL 2026 (long, main). The first two authors contributed equally", "summary": "Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.", "AI": {"tldr": "该论文主张将分词重新定义为语言模型的核心建模决策而非预处理步骤，提出上下文感知框架进行分词器和模型的协同设计，以实现更公平、高效和可适应的语言技术。", "motivation": "当前的分词方法（如BPE）虽然具有可扩展性，但与语言结构错位、放大偏见、在不同语言和领域浪费容量，需要理论化和一致性的设计。", "method": "提出上下文感知框架，将分词器和模型进行协同设计，考虑语言、领域和部署因素，并强调标准化评估和透明报告。", "result": "论文提出了一个理论框架，但未提供具体的实验结果，主要贡献在于概念重构和方法论建议。", "conclusion": "将分词视为核心设计问题而非技术后处理，可以产生更公平、更高效、更可适应的语言技术，需要标准化评估和透明报告来确保选择的可问责性和可比性。"}}
{"id": "2601.13264", "pdf": "https://arxiv.org/pdf/2601.13264", "abs": "https://arxiv.org/abs/2601.13264", "authors": ["Tyler Lizzo", "Larry Heck"], "title": "Unlearning in LLMs: Methods, Evaluation, and Open Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.", "AI": {"tldr": "本文是一篇关于大语言模型机器遗忘技术的综述论文，系统性地分类和评估了现有的遗忘方法，并指出了该领域的挑战和未来方向。", "motivation": "随着大语言模型的广泛应用，隐私、版权、安全和偏见等问题日益突出，机器遗忘成为在不完全重新训练的情况下选择性移除模型知识的重要解决方案。", "method": "对LLM遗忘方法进行结构化综述，将现有方法分为数据中心、参数中心、架构中心、混合和其他策略，并评估了包括基准测试、指标和数据集在内的评估生态系统。", "result": "提供了全面的遗忘方法分类框架和评估体系，识别了不同方法的优缺点和适用场景。", "conclusion": "本文为开发可靠和负责任的大语言模型遗忘技术提供了路线图，强调了可扩展效率、形式化保证、跨语言多模态遗忘等关键挑战和开放问题。"}}
{"id": "2601.13288", "pdf": "https://arxiv.org/pdf/2601.13288", "abs": "https://arxiv.org/abs/2601.13288", "authors": ["Gonzalo Ariel Meyoyan", "Luciano Del Corro"], "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification", "categories": ["cs.CL"], "comment": null, "summary": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.", "AI": {"tldr": "论文提出了一种在LLM推理过程中复用隐藏状态进行轻量级分类的方法，通过双层聚合器在token和层间选择最佳表示，避免了额外安全模型的延迟和内存开销", "motivation": "现有生产LLM系统需要独立的安全分类模型，导致延迟增加、VRAM占用高和操作复杂，希望通过复用LLM前向传播的隐藏状态来解决这些问题", "method": "训练轻量级探针分析LLM隐藏状态，使用双层聚合器：(1)层内token汇总和(2)跨层摘要聚合，采用直接池化、评分注意力门和降维多头自注意力探针", "result": "在安全和情感基准测试中，该方法优于仅使用logit的方法，与更大的任务特定基线竞争，同时保持接近服务延迟并避免额外模型的开销", "conclusion": "通过复用LLM计算并智能选择隐藏状态表示，可以在不增加延迟和内存成本的情况下实现有效的分类任务，简化了生产部署架构"}}
{"id": "2601.13300", "pdf": "https://arxiv.org/pdf/2601.13300", "abs": "https://arxiv.org/abs/2601.13300", "authors": ["Yow-Fu Liou", "Yu-Chien Tang", "Yu-Hsiang Liu", "An-Zi Yen"], "title": "OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference", "categories": ["cs.CL"], "comment": null, "summary": "Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.", "AI": {"tldr": "该论文提出了选项注入(OI-Bench)基准测试方法，通过在多项选择题中插入误导性指令选项来评估大语言模型对指令干扰的脆弱性，包含3000个问题并测试了12个模型。", "motivation": "现有研究表明LLM决策会受到社会线索、框架和指令等导向信号的影响，需要系统评估模型在标准化选择结构中对指令干扰的鲁棒性。", "method": "开发了选项注入方法，在MCQA界面中添加包含误导性指令的额外选项，构建了包含16种指令类型的OI-Bench基准，涵盖知识、推理和常识任务。", "result": "实验结果显示不同模型存在显著脆弱性，鲁棒性表现各异，成功分析了攻击成功率、行为响应，并探索了从推理时提示到训练后对齐的缓解策略。", "conclusion": "OI-Bench能够支持在基于选择的界面中更系统地评估LLM对指令干扰的鲁棒性，揭示了模型在此类攻击下的系统性脆弱特征。"}}
{"id": "2601.13317", "pdf": "https://arxiv.org/pdf/2601.13317", "abs": "https://arxiv.org/abs/2601.13317", "authors": ["Samantha Sudhoff", "Pranav Perumal", "Zhaoqing Wu", "Tunazzina Islam"], "title": "Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": null, "summary": "Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.", "AI": {"tldr": "本研究开发了一个可解释的主题发现框架，对比分析了Meta付费广告和Bluesky公共帖子中的气候话语，发现平台激励机制影响了气候叙事的主题结构、立场对齐和时间响应性。", "motivation": "现有计算研究通常孤立分析不同平台的气候话语，限制了区分机构信息与公众表达的能力。需要比较付费广告生态系统和公共社交媒体平台的气候传播差异。", "method": "使用可解释的端到端主题发现框架，通过语义相似性聚类文本，并利用大语言模型生成简明的人类可解释主题标签。对比传统主题建模基线，通过人工判断和基于LLM的评估器评估主题质量。", "result": "发现了付费气候信息与公共气候话语之间的系统性差异，主题流行度在重大政治事件周围发生明显变化。平台级别的激励机制反映在气候叙事的主题结构、立场对齐和时间响应性中。", "conclusion": "平台激励机制显著影响气候话语的特征，提出的框架支持跨异质传播环境的比较叙事分析，为理解不同平台上的气候传播提供了新视角。"}}
{"id": "2601.13319", "pdf": "https://arxiv.org/pdf/2601.13319", "abs": "https://arxiv.org/abs/2601.13319", "authors": ["Peter Sullivan", "AbdelRahim Elmadany", "Alcides Alcoba Inciarte", "Muhammad Abdul-Mageed"], "title": "Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology", "categories": ["cs.CL"], "comment": null, "summary": "Dialectal Arabic (DA) speech data vary widely in domain coverage, dialect labeling practices, and recording conditions, complicating cross-dataset comparison and model evaluation. To characterize this landscape, we conduct a computational analysis of linguistic ``dialectness'' alongside objective proxies of audio quality on the training splits of widely used DA corpora. We find substantial heterogeneity both in acoustic conditions and in the strength and consistency of dialectal signals across datasets, underscoring the need for standardized characterization beyond coarse labels. To reduce fragmentation and support reproducible evaluation, we introduce Arab Voices, a standardized framework for DA ASR. Arab Voices provides unified access to 31 datasets spanning 14 dialects, with harmonized metadata and evaluation utilities. We further benchmark a range of recent ASR systems, establishing strong baselines for modern DA ASR.", "AI": {"tldr": "Arab Voices框架标准化了31个阿拉伯语方言数据集，提供统一访问和评估工具，解决了方言语音数据异质性问题，并建立了现代方言ASR的强基线。", "motivation": "阿拉伯语方言(DA)语音数据在领域覆盖、方言标注实践和录音条件上差异巨大，导致跨数据集比较和模型评估困难，需要标准化表征。", "method": "对广泛使用的DA语料库进行语言\"方言性\"的计算分析，同时使用音频质量的客观代理指标，并开发Arab Voices标准化框架整合31个数据集。", "result": "发现数据集间在声学条件和方言信号强度/一致性上存在显著异质性，建立了涵盖14种方言的统一框架，并为现代DA ASR系统提供了强基准性能。", "conclusion": "需要超越粗粒度标签的标准化表征，Arab Voices框架减少了碎片化并支持可复现评估，为阿拉伯语方言ASR研究提供了重要基础设施。"}}
{"id": "2601.13328", "pdf": "https://arxiv.org/pdf/2601.13328", "abs": "https://arxiv.org/abs/2601.13328", "authors": ["Geoffrey Churchill", "Steven Skiena"], "title": "Reducing Tokenization Premiums for Low-Resource Languages", "categories": ["cs.CL"], "comment": null, "summary": "Relative to English, low-resource languages suffer from substantial tokenization premiums in modern LMs, meaning that it generally requires several times as many tokens to encode a sentence in a low-resource language than to encode the analogous sentence in English. This tokenization premium results in increased API and energy costs and reduced effective context windows for these languages. In this paper we analyze the tokenizers of ten popular LMs to better understand their designs and per-language tokenization premiums. We also propose a mechanism to reduce tokenization premiums in pre-trained models, by post-hoc additions to the token vocabulary that coalesce multi-token characters into single tokens. We apply this methodology to 12 low-resource languages, demonstrating that the original and compressed inputs often have similar last hidden states when run through the Llama 3.2 1B model.", "AI": {"tldr": "本文分析了10个流行语言模型的tokenizer设计，针对低资源语言存在的高tokenization溢价问题，提出了一种通过词汇表后处理来减少多字符token的方法，在12种低资源语言上验证了有效性。", "motivation": "低资源语言相比英语存在显著的tokenization溢价问题，即编码相同含义的句子需要更多token，这导致API和能源成本增加，以及有效上下文窗口缩小。", "method": "分析10个流行LM的tokenizer设计，提出通过后处理方式向词汇表添加多字符组合的单一token，以减少低资源语言的token数量。", "result": "在Llama 3.2 1B模型上验证，压缩后的输入与原始输入在最后隐藏状态上表现相似，证明方法的有效性。", "conclusion": "提出的后处理词汇表扩展方法能有效降低低资源语言的tokenization溢价，为改善多语言模型效率提供了可行方案。"}}
{"id": "2601.13330", "pdf": "https://arxiv.org/pdf/2601.13330", "abs": "https://arxiv.org/abs/2601.13330", "authors": ["Jamie Cummins", "Beth Clarke", "Ian Hussey", "Malte Elson"], "title": "RegCheck: A tool for automating comparisons between study registrations and papers", "categories": ["cs.CL"], "comment": "15 pages, 1 figure", "summary": "Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.", "AI": {"tldr": "RegCheck是一个基于LLM的模块化工具，帮助研究人员、审稿人和编辑自动比较研究注册文件与对应论文的一致性，保持人工判断在循环中，提高科学研究的透明度和严谨性。", "motivation": "研究注册对科学透明度和严谨性很重要，但手动检查注册与论文的一致性既费时又费力，需要跨格式和跨领域的专业知识。AI的发展为自动化这一过程提供了新可能。", "method": "开发了RegCheck工具，采用模块化设计，让用户决定需要比较的特征，LLM辅助提取相关文本供用户判断差异，生成可共享的验证报告。", "result": "RegCheck能够适应不同科学领域和文件格式，提供了一个可扩展的基础设施来支持可重复性科学。", "conclusion": "RegCheck通过结合AI自动化和人类专业知识，有效解决了研究注册验证的难题，为科学共同体提供了实用的工具来增强研究透明度和可靠性。"}}
{"id": "2601.13346", "pdf": "https://arxiv.org/pdf/2601.13346", "abs": "https://arxiv.org/abs/2601.13346", "authors": ["Sang Yun Kwon", "AbdelRahim Elmadany", "Muhammad Abdul-Mageed"], "title": "AfroScope: A Framework for Studying the Linguistic Landscape of Africa", "categories": ["cs.CL"], "comment": null, "summary": "Language Identification (LID) is the task of determining the language of a given text and is a fundamental preprocessing step that affects the reliability of downstream NLP applications. While recent work has expanded LID coverage for African languages, existing approaches remain limited in (i) the number of supported languages and (ii) their ability to make fine-grained distinctions among closely related varieties. We introduce AfroScope, a unified framework for African LID that includes AfroScope-Data, a dataset covering 713 African languages, and AfroScope-Models, a suite of strong LID models with broad language coverage. To better distinguish highly confusable languages, we propose a hierarchical classification approach that leverages Mirror-Serengeti, a specialized embedding model targeting 29 closely related or geographically proximate languages. This approach improves macro F1 by 4.55 on this confusable subset compared to our best base model. Finally, we analyze cross linguistic transfer and domain effects, offering guidance for building robust African LID systems. We position African LID as an enabling technology for large scale measurement of Africas linguistic landscape in digital text and release AfroScope-Data and AfroScope-Models publicly.", "AI": {"tldr": "AfroScope是一个针对非洲语言的统一语言识别框架，包含覆盖713种语言的数据集和强大的识别模型，通过分层分类方法显著提升了混淆语言的识别准确率。", "motivation": "现有语言识别方法在非洲语言支持上存在局限：支持语言数量有限，且难以区分密切相关的语言变体，影响下游NLP应用的可靠性。", "method": "提出AfroScope框架，包含AfroScope-Data数据集和AfroScope-Models模型套件；针对29种易混淆语言，使用Mirror-Serengeti嵌入模型进行分层分类。", "result": "在易混淆语言子集上，分层分类方法比最佳基础模型的macro F1提高了4.55；分析了跨语言迁移和领域效应。", "conclusion": "AfroScope为大规模测量非洲数字文本语言景观提供了技术支持，相关数据和模型已公开发布。"}}
{"id": "2601.13352", "pdf": "https://arxiv.org/pdf/2601.13352", "abs": "https://arxiv.org/abs/2601.13352", "authors": ["Yuxing Lu", "J. Ben Tamo", "Weichen Zhao", "Nan Sun", "Yishan Zhong", "Wenqi Shi", "Jinzhuo Wang", "May D. Wang"], "title": "LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "17 pages, 5 figures, 6 tables", "summary": "Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.", "AI": {"tldr": "LLM-as-RNN将冻结的大型语言模型转换为循环预测器，通过自然语言记忆状态实现无需参数更新的在线学习，在多个序列预测任务中显著优于基线方法", "motivation": "标准LLM推理使用不可变的上下文历史，一旦在生成步骤t出错，无法通过可更新记忆机制改进步骤t+1的预测", "method": "将LLM隐藏状态表示为自然语言记忆（结构化系统提示摘要），通过反馈驱动的文本重写在每个时间步更新状态，在固定token预算下实现无需参数更新的学习", "result": "在医疗、气象和金融三个序列基准测试中，LLM-as-RRN显著优于零样本、全历史和MemPrompt基线，平均预测准确率提高6.5%，并产生可解释的学习轨迹", "conclusion": "LLM-as-RRN通过语言实现有效的在线学习，能够纠正错误并保留任务相关模式，为LLM提供了可更新的记忆机制"}}
{"id": "2601.13359", "pdf": "https://arxiv.org/pdf/2601.13359", "abs": "https://arxiv.org/abs/2601.13359", "authors": ["Asen Dotsinski", "Panagiotis Eustratiadis"], "title": "Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce \"sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., \"Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.", "AI": {"tldr": "提出一种名为'sockpuppetting'的简单越狱方法，通过在模型输出开头插入接受序列来攻击开源大语言模型，无需优化即可获得高攻击成功率。", "motivation": "随着开源大语言模型能力增强，需要保护它们免受恶意提示攻击并理解可能的攻击向量。现有自动化越狱方法如GCG需要大量计算资源和专业知识。", "method": "在模型输出开头插入接受序列（如\"Sure, here is how to...\"），让模型完成响应。还探索了混合方法，在助手消息块内优化对抗后缀。", "result": "sockpuppetting在Qwen3-8B上比GCG攻击成功率提高80%；混合方法在Llama-3.1-8B上比GCG提高64%攻击成功率。", "conclusion": "该方法是一种低成本有效攻击，非专业攻击者也可使用，凸显了开源模型中需要防御输出前缀注入攻击的重要性。"}}
{"id": "2601.13368", "pdf": "https://arxiv.org/pdf/2601.13368", "abs": "https://arxiv.org/abs/2601.13368", "authors": ["Zhenjiang Mao", "Anirudhh Venkat"], "title": "Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.", "AI": {"tldr": "提出了一种新的不确定性评估方法，通过引入步间注意力机制和隐藏置信度机制来改进LLM推理过程中的置信度估计，在数学和因果推理任务上优于现有方法", "motivation": "现有方法在分析长推理序列时忽略了时间上的置信度传播问题，导致即使早期步骤置信度很低，整体置信度仍然被高估，可能产生误导性幻觉", "method": "结合步间注意力分析步骤间的语义相关性，引入隐藏置信度机制保留历史置信度信息，与逐步置信度结合生成更准确的整体置信度估计", "result": "在GAOKAO数学基准和CLadder因果推理数据集上评估，在负对数似然和期望校准误差指标上表现优异，实现了预测质量和校准性能的更好平衡", "conclusion": "该方法通过关注推理过程中的时间置信度传播问题，有效提高了LLM推理结果的不确定性评估准确性，为减少误导性幻觉提供了有效解决方案"}}
{"id": "2601.13387", "pdf": "https://arxiv.org/pdf/2601.13387", "abs": "https://arxiv.org/abs/2601.13387", "authors": ["Zhenjiang Mao", "Anirudhh Venkat", "Artem Bisliouk", "Akshat Kothiyal", "Sindhura Kumbakonam Subramanian", "Saithej Singhu", "Ivan Ruchkin"], "title": "Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.", "AI": {"tldr": "该论文提出使用信号时序逻辑(STL)来分析大语言模型在多步推理过程中的逐步置信度信号，通过挖掘区分正确和错误响应的时序模式，开发出比现有方法更准确的置信度估计方法。", "motivation": "现有置信度估计方法通常将整个推理过程简化为单一标量分数，忽略了置信度在生成过程中的动态演变，导致对表面因素敏感且难以区分正确推理和自信陈述的错误。", "method": "使用信号时序逻辑(STL)表征逐步置信度信号，通过判别性STL挖掘程序发现区分正确和错误响应的时序公式，并开发基于参数超网络的STL块置信度估计方法。", "result": "实验表明，该方法在多个推理任务上的置信度分数比基线方法更加校准，STL模式在不同任务间具有通用性，数值参数对个别问题表现出敏感性。", "conclusion": "基于STL的逐步置信度分析方法能够有效捕捉推理过程中的动态置信信号，提供比传统单一标量方法更准确和可靠的置信度估计。"}}
{"id": "2601.13388", "pdf": "https://arxiv.org/pdf/2601.13388", "abs": "https://arxiv.org/abs/2601.13388", "authors": ["Sasha Ronaghi", "Prerit Choudhary", "David H Rehkopf", "Bryant Lin"], "title": "Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction", "categories": ["cs.CL"], "comment": "7 pages, 5 figures", "summary": "Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.", "AI": {"tldr": "本研究使用大型语言模型从糖尿病患者的非结构化访谈中提取社会健康决定因素信息，并将其转化为结构化数据用于糖尿病控制预测，LLM预测准确率达到60%", "motivation": "社会健康决定因素对2型糖尿病管理至关重要，但电子健康记录和风险预测模型中往往缺少这些信息，结构化筛查工具无法充分捕捉患者经验的复杂性", "method": "收集65名65岁以上T2D患者的非结构化访谈，使用检索增强生成技术的LLM分析叙事，生成定性总结和结构化SDOH评分，结合传统生物标志物使用多种机器学习模型进行预测", "result": "LLM能够从访谈文本中预测糖尿病控制水平，准确率达到60%，成功将非结构化SDOH数据转化为结构化见解", "conclusion": "LLM提供了一种可扩展的方法，可以将非结构化SDOH相关数据转化为结构化洞察，增强临床风险模型和决策支持"}}
{"id": "2601.13392", "pdf": "https://arxiv.org/pdf/2601.13392", "abs": "https://arxiv.org/abs/2601.13392", "authors": ["Shlok Shelat", "Jay Raval", "Souvik Roy", "Manas Gaur"], "title": "Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks", "categories": ["cs.CL", "cs.AI", "cs.FL"], "comment": "30 pages, 11 figures, 6 tables, Work in Progress", "summary": "Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.", "AI": {"tldr": "LLMs在DFA构造任务中表现不佳，虽然能处理熟悉的问题和事实性问题，但在未见过的复杂约束问题上准确率显著下降，显示出形式推理能力的根本局限。", "motivation": "探究LLMs是否真正具备符号推理能力，还是仅仅在模式匹配熟悉的语言结构，通过DFA构造任务来评估其形式推理能力。", "method": "构建包含事实性问题、已知问题、未见问题（手工制作的多约束问题和系统生成的Arden定理问题）的DFA构造基准，评估多种提示策略（直接、思维链、思维树）和三阶段提示协议。", "result": "模型在事实性问题上准确率100%，已知问题上84-90%，但未见问题上准确率下降30-64%，存在约束误解、Kleene-star语义处理错误和全局一致性保持失败等问题。", "conclusion": "LLMs能够生成语法合理的DFA，但在语义正确的形式推理能力上存在根本差距，提示策略和错误修正协议只能纠正表面错误，无法解决全局不一致或结构缺陷问题。"}}
{"id": "2601.13433", "pdf": "https://arxiv.org/pdf/2601.13433", "abs": "https://arxiv.org/abs/2601.13433", "authors": ["Priyanka Mary Mammen", "Emil Joswin", "Shankar Venkitachalam"], "title": "Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.", "AI": {"tldr": "研究发现语言模型在推理任务中会受到建议和认可的影响，且对认可来源的可信度存在系统性偏见：模型更容易受到高权威专家的错误认可误导，导致准确性下降和错误答案置信度增加。", "motivation": "探索语言模型是否基于认可提供者的感知专业知识水平表现出系统性偏见，此前研究未充分探讨认可来源可信度的影响。", "method": "在数学、法律和医学推理4个数据集上评估11个模型，使用代表4个专业水平层级的角色进行实验。", "result": "模型随着来源专业水平提高而更容易受到错误/误导性认可的影响，高权威来源不仅导致准确性下降，还增加对错误答案的置信度。", "conclusion": "权威偏见在模型中机制性编码，可以通过引导使模型避免这种偏见，从而即使在专家给出误导性认可时也能提高性能。"}}
{"id": "2601.13437", "pdf": "https://arxiv.org/pdf/2601.13437", "abs": "https://arxiv.org/abs/2601.13437", "authors": ["Adriana-Valentina Costache", "Daria-Nicoleta Dragomir", "Silviu-Florin Gheorghe", "Eduard Poesina", "Paul Irofti", "Radu Tudor Ionescu"], "title": "MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.", "AI": {"tldr": "本文提出了首个多语言开放集学习与发现（MOSLD）基准，用于文本主题分类，包含12种语言的96万样本，并提出了一个集成多阶段的新框架来持续发现和学习新类别。", "motivation": "开放集学习与发现（OSLD）是机器学习中的一个挑战性任务，测试时可能出现未知类别样本。虽然零样本学习在文本分类中已有广泛研究，但OSLD在文本领域相对较新，缺乏标准基准。", "method": "1) 构建多语言OSLD基准：重新整理现有数据集并收集新闻领域新样本；2) 提出新颖的OSLD框架：集成多个阶段来持续发现和学习新类别；3) 评估包括自提模型在内的多种语言模型。", "result": "创建了包含12种语言、960K数据样本的MOSLD基准，提供了多个语言模型的评估结果作为未来研究的参考基准。", "conclusion": "该研究填补了文本领域OSLD基准的空白，提出的多语言基准和新框架为开放集学习与发现任务提供了重要基础，相关资源已开源供社区使用。"}}
{"id": "2601.13453", "pdf": "https://arxiv.org/pdf/2601.13453", "abs": "https://arxiv.org/abs/2601.13453", "authors": ["Aditya Thole", "Anmol Agrawal", "Arnav Ramamoorthy", "Dhruv Kumar"], "title": "PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems", "AI": {"tldr": "PSA是一个自主代理系统，能够生成长达6分钟的物理问题讲解视频，使用Manim动画和自动化评估流程来提升视频质量。", "motivation": "虽然大语言模型在文本物理问题上表现良好，但生成高质量视觉解释的能力尚未充分探索，需要改善物理教育的多模态理解。", "method": "开发PhysicsSolutionAgent(PSA)，使用Manim生成动画视频，设计包含15个量化参数的自动化评估流程，并整合视觉语言模型的反馈来迭代改进视频质量。", "result": "在32个视频测试中，PSA实现100%视频完成率，平均自动化得分3.8/5，但定性分析和人工检查发现视觉布局不一致和内容解释错误等问题。", "conclusion": "研究揭示了可靠Manim代码生成的关键限制，凸显了多模态推理和视觉解释评估的挑战，强调未来多模态教育系统需要改进视觉理解、验证和评估框架。"}}
{"id": "2601.13503", "pdf": "https://arxiv.org/pdf/2601.13503", "abs": "https://arxiv.org/abs/2601.13503", "authors": ["Kyung Ho Lim", "Byung-Hoon Kim"], "title": "Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives", "categories": ["cs.CL"], "comment": null, "summary": "Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.", "AI": {"tldr": "Anonpsy是一个新的精神病学叙事去识别框架，通过图引导的语义重写方法，在保持诊断保真度的同时有效降低再识别风险。", "motivation": "现有去识别方法（如PHI掩码和LLM重写）在文本层面操作，对保留或修改哪些语义元素控制有限，无法有效处理精神病学叙事中嵌入身份信息的特殊生活事件。", "method": "1) 将叙事转换为编码临床实体、时间锚点和类型化关系的语义图；2) 应用图约束扰动修改识别性上下文同时保留临床关键结构；3) 通过图条件LLM生成重新生成文本。", "result": "在90个临床医生撰写的精神病案例叙事上评估，Anonpsy在保持诊断保真度的同时，在专家、语义和GPT-5评估中都实现了持续低的再识别风险。与纯LLM重写基线相比，语义相似性和可识别性显著降低。", "conclusion": "显式结构表示与约束生成相结合，为精神病学叙事的去识别提供了有效方法，平衡了隐私保护与临床实用性的需求。"}}
{"id": "2601.13537", "pdf": "https://arxiv.org/pdf/2601.13537", "abs": "https://arxiv.org/abs/2601.13537", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Minwoo Lee", "Kyomin Jung"], "title": "When Wording Steers the Evaluation: Framing Bias in LLM judges", "categories": ["cs.CL", "cs.AI"], "comment": "4 pages", "summary": "Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.", "AI": {"tldr": "研究发现大型语言模型在评估任务中存在显著的框架偏见，即通过不同的提示词构造（谓语肯定vs否定）会导致模型判断产生显著差异，这表明当前的LLM评估系统存在结构性偏见问题", "motivation": "大型语言模型对提示词措辞敏感，但在评估任务中需要稳定公正的判断，框架偏见对LLM评估的影响尚未充分研究", "method": "使用心理学中的框架效应理论，设计对称的谓语肯定和谓语否定构造提示词，在四个高风险评估任务中系统测试14个LLM法官模型的反应", "result": "所有测试的LLM模型都显示出明显的框架敏感性，不同模型家族表现出倾向于同意或拒绝的明显趋势", "conclusion": "框架偏见是当前基于LLM的评估系统的结构性特征，需要开发框架感知的评估协议来确保评估的公正性和稳定性"}}
{"id": "2601.13547", "pdf": "https://arxiv.org/pdf/2601.13547", "abs": "https://arxiv.org/abs/2601.13547", "authors": ["Yujia Hu", "Roy Ka-Wei Lee"], "title": "HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations", "categories": ["cs.CL", "cs.AI"], "comment": "EACL 2026 Main Conference", "summary": "Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \\textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \\textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \\textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.\n  \\textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}", "AI": {"tldr": "HateXScore是一个评估仇恨言论检测模型解释质量的四维度指标套件，用于揭示标准指标无法发现的解释性缺陷和标注不一致问题", "motivation": "当前仇恨言论检测评估框架很少评估文本被判定为仇恨言论的原因，缺乏对模型解释质量的系统评估方法", "method": "提出HateXScore四组件指标：结论明确性、引用跨度的忠实性和因果基础、受保护群体识别（策略可配置）、各元素间的逻辑一致性，在六个仇恨言论数据集上进行评估", "result": "人类评估显示与HateXScore高度一致，验证了其作为可信透明内容审核实用工具的有效性", "conclusion": "HateXScore可作为诊断性补充工具，揭示准确率和F1分数等标准指标无法检测到的解释性失败和标注不一致问题"}}
{"id": "2601.13575", "pdf": "https://arxiv.org/pdf/2601.13575", "abs": "https://arxiv.org/abs/2601.13575", "authors": ["Thanh-Lam T. Nguyen", "Ngoc-Quang Le", "Quoc-Trung Phu", "Thi-Phuong Le", "Ngoc-Huyen Pham", "Phuong-Nguyen Nguyen", "Hoang-Quynh Le"], "title": "Comparing Without Saying: A Dataset and Benchmark for Implicit Comparative Opinion Mining from Same-User Reviews", "categories": ["cs.CL"], "comment": null, "summary": "Existing studies on comparative opinion mining have mainly focused on explicit comparative expressions, which are uncommon in real-world reviews. This leaves implicit comparisons - here users express preferences across separate reviews - largely underexplored. We introduce SUDO, a novel dataset for implicit comparative opinion mining from same-user reviews, allowing reliable inference of user preferences even without explicit comparative cues. SUDO comprises 4,150 annotated review pairs (15,191 sentences) with a bi-level structure capturing aspect-level mentions and review-level preferences. We benchmark this task using two baseline architectures: traditional machine learning- and language model-based baselines. Experimental results show that while the latter outperforms the former, overall performance remains moderate, revealing the inherent difficulty of the task and establishing SUDO as a challenging and valuable benchmark for future research.", "AI": {"tldr": "SUDO是一个新颖的隐式比较观点挖掘数据集，包含4,150个标注评论对，用于在没有显式比较线索的情况下推断用户偏好。实验显示语言模型优于传统机器学习方法，但整体性能中等，表明该任务具有挑战性。", "motivation": "现有比较观点挖掘研究主要关注显式比较表达，而现实评论中较少见。隐式比较（用户在不同评论中表达偏好）研究不足，需要专门数据集来探索。", "method": "创建SUDO数据集，包含4,150个标注评论对（15,191个句子），采用双层结构捕捉方面级提及和评论级偏好。使用传统机器学习和语言模型两种基线架构进行基准测试。", "result": "语言模型基线优于传统机器学习方法，但整体性能保持中等水平，表明隐式比较观点挖掘任务具有内在难度。", "conclusion": "SUDO作为一个具有挑战性的基准数据集，为未来隐式比较观点挖掘研究提供了有价值的基础，突显了该领域需要进一步探索和技术改进。"}}
{"id": "2601.13588", "pdf": "https://arxiv.org/pdf/2601.13588", "abs": "https://arxiv.org/abs/2601.13588", "authors": ["Inho Won", "Hangyeol Yoo", "Minkyung Cho", "Jungyeul Park", "Hoyun Song", "KyungTae Lim"], "title": "TREX: Tokenizer Regression for Optimal Data Mixture", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EACL 2026. Long Paper. (19 languages studied: Chinese, Greek, Japanese, etc.)", "summary": "Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX's predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both inand out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.", "AI": {"tldr": "TREX是一个基于回归的框架，通过训练小规模代理分词器来预测最优的多语言数据混合比例，显著提升多语言LLM分词器的压缩性能", "motivation": "现有方法依赖启发式规则或成本高昂的大规模搜索来确定多语言分词器的最佳语言数据比例，缺乏高效准确的优化方法", "method": "训练小规模代理分词器在随机数据混合上，收集压缩统计数据，学习从数据混合预测压缩性能的回归模型", "result": "TREX预测的混合比例在分布内和分布外压缩效率上比LLaMA3和均匀分布方法提升高达12%", "conclusion": "TREX框架有效解决了多语言分词器设计中准确性与成本的权衡问题，展现出强大的可扩展性、鲁棒性和实用效果"}}
{"id": "2601.13590", "pdf": "https://arxiv.org/pdf/2601.13590", "abs": "https://arxiv.org/abs/2601.13590", "authors": ["Fan Huang", "Haewoon Kwak", "Jisun An"], "title": "Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.", "AI": {"tldr": "大型语言模型在说服情境下存在脆弱性，小型模型更容易被说服，元认知提示反而增加脆弱性，对抗性微调效果因模型而异", "motivation": "评估大型语言模型在SMCR沟通框架下对说服的敏感性，分析不同说服策略对模型信念稳定性的影响", "method": "使用SMCR沟通框架，在五个主流LLM和三个领域（事实知识、医疗问答、社会偏见）进行系统性评估，测试不同说服策略和多轮交互的影响，并研究元认知提示和对抗性微调的效果", "result": "小模型表现出极端顺从，80%以上信念变化发生在第一轮说服；元认知提示增加了脆弱性而非增强鲁棒性；对抗性微调效果因模型而异，GPT-4o-mini达到98.6%鲁棒性，Mistral 7B从35.7%提升到79.3%，但Llama模型即使微调后仍低于14%", "conclusion": "当前鲁棒性干预措施存在显著的模型依赖性限制，研究结果为开发更可信赖的LLMs提供了指导"}}
{"id": "2601.13614", "pdf": "https://arxiv.org/pdf/2601.13614", "abs": "https://arxiv.org/abs/2601.13614", "authors": ["Bo Peng", "Sirui Chen", "Lei Xu", "Chaochao Lu"], "title": "CauScientist: Teaching LLMs to Respect Data for Causal Discovery", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating \"data scientists\" with probabilistic statistics as rigorous \"verifiers\". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.", "AI": {"tldr": "CauScientist是一个结合大语言模型和概率统计的因果发现框架，通过LLM生成假设和统计验证的协同工作方式，显著提升了因果发现的准确性和效率。", "motivation": "现有因果发现方法存在局限性：纯数据驱动方法受统计不可区分性和建模假设限制，而基于LLM的方法要么忽略统计证据，要么包含可能误导结果的未经验证先验知识。", "method": "采用混合初始化选择优质起始图，通过LLM提议的修改和统计标准验证迭代优化结构，维护错误记忆以指导高效搜索空间。", "result": "实验显示CauScientist显著优于纯数据驱动基线，F1分数提升达53.8%，召回率从35.0%提升至100.0%。在37节点图上，相比Qwen3-32B将结构汉明距离(SHD)降低了44.0%。", "conclusion": "CauScientist框架成功地将LLM的假设生成能力与统计验证相结合，有效解决了现有因果发现方法的局限性，为科学理解和可靠决策提供了更可靠的因果发现工具。"}}
{"id": "2601.13630", "pdf": "https://arxiv.org/pdf/2601.13630", "abs": "https://arxiv.org/abs/2601.13630", "authors": ["Zhaopeng Zhang", "Pengcheng Sun", "Lan Zhang", "Chen Tang", "Jiewei Lai", "Yunhao Wang", "Hui Jin"], "title": "Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user's permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query's activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.", "AI": {"tldr": "AAAC是一个无需训练的多类权限控制框架，利用激活空间的几何可分性来防止LLM在知识库问答中泄露超出用户权限的敏感信息", "motivation": "LLMs在知识库问答中可能无意中回答超出用户权限范围的内容，泄露敏感信息，难以在细粒度访问控制要求下部署", "method": "基于中间激活的几何可分性，构建权限锚点库，通过多锚点引导机制将查询激活重定向到授权区域", "result": "在三个LLM家族上的实验显示，AAAC将权限违规率降低高达86.5%，基于提示的攻击成功率降低90.7%，同时以较小的推理开销提高响应可用性", "conclusion": "AAAC通过激活空间锚定访问控制有效解决了LLM在知识库问答中的权限泄露问题，提供了一种无需训练的高效权限控制方案"}}
{"id": "2601.13644", "pdf": "https://arxiv.org/pdf/2601.13644", "abs": "https://arxiv.org/abs/2601.13644", "authors": ["Yang Cao", "Bicheng Yu", "Sikun Yang", "Ming Liu", "Yujiu Yang"], "title": "Towards Token-Level Text Anomaly Detection", "categories": ["cs.CL", "cs.LG"], "comment": "WWW 2026", "summary": "Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.", "AI": {"tldr": "该论文提出了令牌级异常检测新范式，能够在文本中精确定位异常部分，超越了传统文档级检测的限制，并在三个基准数据集上验证了其优越性能。", "motivation": "现有文本异常检测方法仅限于文档级别分析，无法识别文本中具体的异常部分，需要更细粒度的异常定位能力。", "method": "提出了令牌级异常检测的统一框架，定义了文档级和令牌级文本异常，并在垃圾邮件、评论和语法错误三个领域收集和标注了带令牌级标签的基准数据集。", "result": "实验结果表明，该框架在6个基线方法中取得了更好的性能，实现了文本中精确的异常定位。", "conclusion": "令牌级异常检测为文本精确异常定位开辟了新可能性，所有代码和数据已公开提供。"}}
{"id": "2601.13649", "pdf": "https://arxiv.org/pdf/2601.13649", "abs": "https://arxiv.org/abs/2601.13649", "authors": ["Xiaolin Zhou", "Zheng Luo", "Yicheng Gao", "Qixuan Chen", "Xiyang Hu", "Yue Zhao", "Ruishan Liu"], "title": "Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.", "AI": {"tldr": "该论文研究了LLM作为评判者时存在的语言偏见问题，发现在同语言评判中欧洲语言表现优于非洲语言，在跨语言评判中模型偏向英语答案，且语言偏见不能完全由低困惑度偏见解释。", "motivation": "大型语言模型作为评判者应用时存在各种偏见，特别是语言偏见会影响评判结果与人类偏好的一致性，需要深入研究其具体表现和成因。", "method": "研究两种语言偏见类型：(1)同语言评判中的性能差异；(2)跨语言评判中对主流语言的偏好。分析不同语言家族间的表现差异，并探讨语言偏见与低困惑度偏见的关系。", "result": "欧洲语言在同语言评判中表现优于非洲语言，文化相关主题中偏见更明显；跨语言评判中模型偏好英语答案，答案语言比问题语言影响更大；语言偏见与困惑度仅有微弱相关性。", "conclusion": "LLM作为评判者存在显著的语言偏见，这种偏见不能仅用困惑度解释，需要在模型设计和应用中考虑多语言公平性问题。"}}
{"id": "2601.13658", "pdf": "https://arxiv.org/pdf/2601.13658", "abs": "https://arxiv.org/abs/2601.13658", "authors": ["Arthur Amalvy", "Hen-Hsen Huang"], "title": "Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation", "categories": ["cs.CL"], "comment": "12 pages", "summary": "The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.", "AI": {"tldr": "本文提出了一种新颖的合成评估数据集，用于解决时序知识图谱提取（TKGE）任务中评估数据稀缺和污染问题。通过预测未来时间事实构建无污染数据集，用于更准确评估LLM性能。", "motivation": "时序知识图谱提取任务中，现有训练和评估数据集稀缺，且存在评估数据污染问题，导致LLM性能评估可能被高估。需要构建无污染的数据集进行鲁棒和无偏的基准测试。", "method": "采用两步方法：1）时序知识图谱预测生成可信的未来四元组；2）使用LLM进行四元组到文本生成，创建语义对齐的文本描述。构建了包含4.2K未来四元组的数据集。", "result": "实验表明，在最先进的LLM提取框架EDC上，使用本文构建的数据集评估时，LLM性能相比已知事实数据集有所下降，证实了数据污染对性能评估的影响。", "conclusion": "提出的合成数据集方法能有效消除数据污染问题，为TKGE任务提供长期、无污染的基准测试资源，数据集和生成方法已公开，支持持续创建无限未来时序数据集。"}}
{"id": "2601.13659", "pdf": "https://arxiv.org/pdf/2601.13659", "abs": "https://arxiv.org/abs/2601.13659", "authors": ["Chunlei Meng", "Ziyang Zhou", "Lucas He", "Xiaojing Du", "Chun Ouyang", "Zhongxue Gan"], "title": "Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "This study has been accepted by IEEE ICASSP2026", "summary": "Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.", "AI": {"tldr": "TSDA模型通过时空解耦与对齐的跨模态情感分析方法，在解耦时空特征后进行因子一致性对齐，显著提升了多模态情感分析性能", "motivation": "现有基于模态不变/特定因子分解或复杂融合的方法忽略了时空异质性，导致时空信息不对称和性能受限", "method": "提出TSDA框架：1）将每个模态显式解耦为时空动态和空间结构上下文；2）时空编码器分离特征；3）因子一致性跨模态对齐；4）因子特定监督和去相关正则化；5）门控重耦合模块", "result": "大量实验显示TSDA超越基线方法，消融分析验证了设计的必要性和可解释性", "conclusion": "显式时空解耦和对齐策略能有效解决多模态情感分析中的时空异质性问题，提升模型性能"}}
{"id": "2601.13669", "pdf": "https://arxiv.org/pdf/2601.13669", "abs": "https://arxiv.org/abs/2601.13669", "authors": ["Jiayu Lin", "Zhongyu Wei"], "title": "CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a \"middle ground\". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.", "AI": {"tldr": "该论文提出了社区级别的对齐方法作为现有统一对齐和个体化对齐之间的折中方案，并创建了CommunityBench基准来评估LLMs在社区特定偏好建模方面的能力。", "motivation": "现有LLM对齐策略要么假设统一价值观（忽视少数群体），要么完全个体化定制（成本过高），而人类社会实际上是按社会集群组织的，存在群体内价值对齐。", "method": "提出社区级别对齐概念，建立CommunityBench基准（基于共同身份和共同纽带理论的4个任务），评估各类基础模型在社区特定偏好建模上的表现。", "result": "当前LLMs在建模社区特定偏好方面能力有限，但社区级别对齐显示出促进个体建模的潜力。", "conclusion": "社区级别对齐为可扩展和多元化的LLM对齐提供了有前景的方向，是统一对齐和个体化对齐之间的有效折中方案。"}}
{"id": "2601.13684", "pdf": "https://arxiv.org/pdf/2601.13684", "abs": "https://arxiv.org/abs/2601.13684", "authors": ["Zhiyuan Shi", "Qibo Qiu", "Feng Xue", "Zhonglin Jiang", "Li Yu", "Jian Jiang", "Xiaofei He", "Wenxiao Wang"], "title": "HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\\times$ compared to the original model in the 224K context. Our code will be open-source.", "AI": {"tldr": "HeteroCache是一个无需训练的KV缓存动态压缩框架，通过细粒度头部分类和分层存储机制，解决了长上下文推理中的内存增长和注意力漂移问题，在224K上下文长度下实现3倍解码加速。", "motivation": "解决LLM长上下文推理中KV缓存线性内存增长的问题，现有静态压缩方法无法保留全局重要信息，动态检索方法存在粗粒度缓存策略和高I/O开销的局限性。", "method": "基于注意力头的时空异质性，将头部按稳定性和冗余度分类，采用细粒度权重分配策略，为注意力快速变化的头部分配更大缓存预算，并使用分层存储机制进行异步按需检索。", "result": "在多个长上下文基准测试中达到最先进性能，在224K上下文长度下相比原始模型加速解码达3倍。", "conclusion": "HeteroCache通过动态细粒度压缩策略有效解决了长上下文推理中的内存瓶颈和注意力漂移问题，证明了基于头部异质性的压缩方法的有效性。"}}
{"id": "2601.13690", "pdf": "https://arxiv.org/pdf/2601.13690", "abs": "https://arxiv.org/abs/2601.13690", "authors": ["Yue Guo", "Fanfu Wang", "Jianwei Lv", "Xincheng Shi", "Yuchen Li", "Youya Wang", "Yunsheng Zeng", "Yujing Liu", "Yunhao Qiao", "Gen Li", "Junfeng Wang", "Bo Yuan"], "title": "Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.", "AI": {"tldr": "本文提出Dr. Assistant临床诊断助手模型，通过CDRD数据结构捕获临床推理逻辑，采用两阶段训练（SFT+RL），在诊断推理和问询方面超越开源模型并接近闭源模型性能。", "motivation": "传统临床决策支持系统维护成本高、泛化能力差，而大语言模型虽在医疗领域有应用但诊断推理和问询能力受限。", "method": "提出CDRD数据结构捕获临床推理逻辑，构建Dr. Assistant模型，采用两阶段训练：监督微调（SFT）和基于定制奖励函数的强化学习（RL）。", "result": "Dr. Assistant在诊断推理和问询评估基准上超越开源模型，与闭源模型性能相当。", "conclusion": "该方法为临床诊断问询指导提供了有效解决方案，通过结构化临床推理数据和两阶段训练显著提升模型性能。"}}
{"id": "2601.13695", "pdf": "https://arxiv.org/pdf/2601.13695", "abs": "https://arxiv.org/abs/2601.13695", "authors": ["Sifan Li", "Hongkai Chen", "Yujun Cai", "Liyang Chen", "Qingwen Ye", "Yiwei Wang"], "title": "OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens", "categories": ["cs.CL"], "comment": null, "summary": "Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.", "AI": {"tldr": "OptiSQL是一个视觉驱动的框架，直接从表格图像和自然语言问题生成可执行SQL，使用光学标记大幅减少输入标记数量，同时保持高执行准确率。", "motivation": "传统的文本到SQL方法需要将表格完全线性化为文本模式，这会带来大量标记开销且与许多实际场景（如文档或网页中的视觉表格）不匹配。", "method": "使用OCR导向的视觉编码器将表格结构和内容压缩为少量光学标记，冻结编码器并微调预训练解码器进行SQL生成，以隔离表示充分性。", "result": "在可视化的Spider 2.0-Snow数据集上，OptiSQL将表格输入标记减少了一个数量级，同时保持了强大的执行准确性。鲁棒性分析显示光学标记在视觉扰动下能保持关键结构信息。", "conclusion": "紧凑的光学表示可以作为可执行语义解析的高效接口，为处理视觉表格提供了一种更实用的解决方案。"}}
{"id": "2601.13697", "pdf": "https://arxiv.org/pdf/2601.13697", "abs": "https://arxiv.org/abs/2601.13697", "authors": ["Zhihang Yuan", "Chengyu Yue", "Long Huang", "Litu Ou", "Lei Shi"], "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.", "AI": {"tldr": "GRADFILTERING是一种基于梯度信噪比的不确定性感知数据选择框架，通过小型GPT-2代理模型和LoRA集成来高效筛选指令数据，在减少计算成本的同时保持或提升模型性能", "motivation": "现代指令数据集庞大、嘈杂且冗余，全数据微调成本高昂且不必要。现有数据选择方法要么构建昂贵的梯度数据存储，要么使用弱代理模型分配静态分数，忽略了模型训练过程中的不确定性变化", "method": "使用小型GPT-2代理模型结合LoRA集成，通过计算每个样本的梯度信噪比(G-SNR)作为效用指标，进行目标无关的数据选择", "result": "在大多数LLM-as-a-judge评估和人工评估中，该方法匹配或超越了随机子集和强基线方法。GRADFILTERING选择的子集在相同计算预算下收敛更快", "conclusion": "GRADFILTERING提供了一种高效的数据选择方法，通过利用不确定性感知评分，能够在降低计算成本的同时保持模型性能，为LLM的可解释性提供了新的视角"}}
{"id": "2601.13711", "pdf": "https://arxiv.org/pdf/2601.13711", "abs": "https://arxiv.org/abs/2601.13711", "authors": ["Lotta Kiefer", "Christoph Leiter", "Sotaro Takeshita", "Elena Schmidt", "Steffen Eger"], "title": "GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.", "AI": {"tldr": "该论文提出了GerAV，一个包含60万标注文本对的德语作者验证基准数据集，填补了非英语语言在该领域的空白，并通过系统评估发现微调大语言模型效果最佳，同时在专业化和泛化性之间存在权衡。", "motivation": "现有作者验证研究主要集中在英语数据上，其他语言缺乏大规模基准和系统评估，特别是德语作者验证领域存在明显的研究空白。", "method": "构建GerAV基准数据集，包含Twitter和Reddit数据（分为领域内、跨领域消息子集和基于配置文件的子集），使用训练分割对强基线模型和最先进模型进行系统评估，包括微调大语言模型。", "result": "微调大语言模型表现最佳，比近期基线模型F1分数绝对提升0.09，在零样本设置下比GPT-5高0.08。发现模型在特定数据类型训练时匹配条件下表现最好但跨数据泛化能力较差，通过组合训练源可以缓解此限制。", "conclusion": "GerAV为德语和跨领域作者验证研究提供了一个具有挑战性和多功能的基准，揭示了专业化和泛化性之间的权衡关系，为未来研究提供了重要基础。"}}
{"id": "2601.13717", "pdf": "https://arxiv.org/pdf/2601.13717", "abs": "https://arxiv.org/abs/2601.13717", "authors": ["Zehan Li", "Yuxuan Wang", "Ali El Lahib", "Ying-Jieh Xia", "Xinyu Pi"], "title": "Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably \"rewind\" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.", "AI": {"tldr": "研究发现模拟无知(SI)方法无法有效模拟真实无知(TI)，在477个竞赛级问题和9个模型上存在52%的性能差距，提示工程无法可靠地'回滚'模型知识，建议不要使用基于SI的回顾性设置来评估预测能力", "motivation": "解决LLM预测能力评估的根本矛盾：前瞻性评估方法严谨但延迟高，回顾性预测面临数据污染问题，因为SOTA模型拥有越来越新的知识截止点", "method": "通过系统测试模拟无知(SI)方法是否能近似真实无知(TI)，使用477个竞赛级问题测试9个模型，分析截止指令效果、思维链推理的知识抑制能力和推理优化模型的表现", "result": "SI方法系统性失败：截止指令导致SI与TI存在52%性能差距；思维链推理无法抑制先验知识；推理优化模型的SI保真度更差，即使推理痕迹不包含明确的截止后引用", "conclusion": "基于截止前事件的回顾性预测存在方法论缺陷，不建议使用基于SI的回顾性设置来基准测试预测能力，提示无法可靠地'回滚'模型知识"}}
{"id": "2601.13722", "pdf": "https://arxiv.org/pdf/2601.13722", "abs": "https://arxiv.org/abs/2601.13722", "authors": ["Yulin Hu", "Zimo Long", "Jiahe Guo", "Xingyu Sui", "Xing Fu", "Weixiang Zhao", "Yanyan Zhao", "Bing Qin"], "title": "OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \\emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \\textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \\textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \\textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.", "AI": {"tldr": "该论文提出了OP-Bench基准测试，用于评估对话系统中的过度个性化问题，并提出了Self-ReCheck方法来缓解此问题", "motivation": "现有对话系统基准主要关注能否回忆用户信息，但忽略了是否恰当使用个性化信息，可能导致过度个性化问题", "method": "将过度个性化形式化为三种类型（无关性、重复性、奉承性），构建1700个实例的OP-Bench基准，并提出轻量级的Self-ReCheck记忆过滤机制", "result": "发现引入记忆后过度个性化问题普遍存在，模型倾向于不必要地检索和过度关注用户记忆，Self-ReCheck能有效缓解此问题同时保持个性化性能", "conclusion": "这项工作为记忆增强对话系统中更可控和恰当的个性化迈出了第一步"}}
{"id": "2601.13729", "pdf": "https://arxiv.org/pdf/2601.13729", "abs": "https://arxiv.org/abs/2601.13729", "authors": ["Weichuan Wang", "Mingyang Liu", "Linqi Song", "Chen Ma"], "title": "On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation", "categories": ["cs.CL"], "comment": "9 pages, 12 figures", "summary": "In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.", "AI": {"tldr": "该论文系统评估了机器翻译中的非确定性现象，发现温度约束下的非确定性机器翻译(ND-MT)能提供比确定性机器翻译(D-MT)更高质量的候选译文，但现有评估框架无法一致评估ND-MT性能，并提出了Buckets效应和ExpectoSample策略来解决评估挑战。", "motivation": "语言模型的非确定性特性在机器翻译这一复杂非确定性NLP任务中研究不足，需要系统评估现代机器翻译系统的非确定性表现及其对多模态问题的解决潜力。", "method": "使用五个最先进的ND-MT系统，在三个开放数据集上，采用词汇和语义两种评估指标，在不同采样规模下进行系统评估，并提出ExpectoSample策略来自动评估指标的可靠性。", "result": "发现ND-MT在温度约束下能产生比D-MT更高质量的候选译文，但现有评估框架对ND-MT评估结果不一致，存在Buckets效应——最低质量候选译文决定了系统整体排名。", "conclusion": "ND-MT在解决机器翻译多模态问题方面具有潜力，但需要开发新的评估方法来应对其非确定性特性带来的评估挑战，ExpectoSample策略为此提供了解决方案。"}}
{"id": "2601.13734", "pdf": "https://arxiv.org/pdf/2601.13734", "abs": "https://arxiv.org/abs/2601.13734", "authors": ["Chenyu Hui"], "title": "Towards robust long-context understanding of large language model via active recap learning", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages", "summary": "In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM", "AI": {"tldr": "ARL是一种通过主动回顾学习增强大语言模型长文本理解能力的框架，通过在持续预训练中构建目标序列和在推理时进行回顾性总结来提升模型性能", "motivation": "解决大语言模型在理解长上下文时面临的挑战，需要一种有效的方法来增强模型对长文本的记忆和理解能力", "method": "1. 基于长短前向上下文的损失差距识别关键标记和相关段落\n2. 使用LLM总结相关段落\n3. 在推理时让模型自主生成和利用这些回顾性总结，建立跨段落的递归记忆机制", "result": "在RULER基准上取得26.8%的显著提升，在LongBench基准上获得9.44%的改进", "conclusion": "ARL提供了一种简单而有效的持续预训练方法，能够显著增强大语言模型的长上下文理解能力，推动了LLM中可扩展记忆增强技术的发展"}}
{"id": "2601.13742", "pdf": "https://arxiv.org/pdf/2601.13742", "abs": "https://arxiv.org/abs/2601.13742", "authors": ["Arjun Chandra", "Kevin Miller", "Venkatesh Ravichandran", "Constantinos Papayiannis", "Venkatesh Saligrama"], "title": "Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues", "categories": ["cs.CL"], "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.", "AI": {"tldr": "TRACE框架让大语言模型能够通过音频线索进行推理，实现低成本且与人类评估对齐的语音到语音评估，在成本效益和人类一致性方面优于现有方法", "motivation": "当前自动语音到语音评估方法依赖昂贵且不透明的音频语言模型，而大语言模型虽然推理能力强但仅限于文本内容", "method": "提出TRACE框架，通过人类思维链注释协议将评估分为内容、音质和副语言三个维度，构建音频信号的文本蓝图，让LLM进行维度判断并通过确定性策略融合为总体评分", "result": "TRACE在人类评估者一致性方面优于音频语言模型和仅基于转录文本的LLM评估，同时显著更经济高效", "conclusion": "TRACE框架为可扩展且与人类对齐的语音到语音评估提供了有效解决方案，将释放相关注释和框架以促进该领域发展"}}
{"id": "2601.13749", "pdf": "https://arxiv.org/pdf/2601.13749", "abs": "https://arxiv.org/abs/2601.13749", "authors": ["Benaya Trabelsi", "Jonathan Shaki", "Sarit Kraus"], "title": "Pro-AI Bias in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "13 pages, 6 figures. Code available at: https://github.com/benayat/Pro-AI-bias-in-LLMs", "summary": "Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.", "AI": {"tldr": "该研究发现大语言模型(LLMs)存在系统性的亲AI偏好，在决策建议、薪资评估和内部表征三个层面都表现出对人工智能的偏好倾向", "motivation": "随着大语言模型在多个领域被用作决策支持工具，研究者希望探究这些模型是否存在对人工智能本身的系统性偏好偏差", "method": "通过三个互补实验：1)分析LLMs对咨询查询的AI相关建议倾向；2)比较AI与非AI职位的薪资评估差异；3)探测开源模型的内部表征相似性", "result": "发现一致的亲AI偏差证据：专有模型几乎确定性地推荐AI选项；对AI职位薪资高估10个百分点；\"人工智能\"在所有情感框架下都表现出最高的表征中心性", "conclusion": "LLM生成的建议和评估可能会在高风险决策中系统性扭曲选择和认知，需要引起关注"}}
{"id": "2601.13802", "pdf": "https://arxiv.org/pdf/2601.13802", "abs": "https://arxiv.org/abs/2601.13802", "authors": ["Yushen Chen", "Junzhe Liu", "Yujie Tu", "Zhikang Niu", "Yuzhe Liang", "Kai Yu", "Chunyu Qiang", "Chen Zhang", "Xie Chen"], "title": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .", "AI": {"tldr": "Habibi是一个统一的多方言阿拉伯语语音合成模型套件，利用现有ASR语料库通过课程学习支持高资源到低资源方言，在生成质量上优于领先商业服务，无需文本标注，并建立了首个系统性的多方言阿拉伯语语音合成基准。", "motivation": "阿拉伯方言语音合成研究存在明显空白，缺乏标准化数据、基准和评估指南，且阿拉伯方言的语言复杂性高，导致研究者望而却步。", "method": "利用现有开源ASR语料库，通过语言信息驱动的课程学习方法来支持从高资源到低资源的阿拉伯方言，采用有效的上下文学习保持可扩展性，无需文本标注。", "result": "Habibi在生成质量上优于领先的商业服务，同时通过有效的上下文学习保持可扩展性。", "conclusion": "该研究为多方言阿拉伯语语音合成提供了首个系统性基准和评估标准，为后续研究奠定了坚实基础，并承诺开源模型。"}}
{"id": "2601.13806", "pdf": "https://arxiv.org/pdf/2601.13806", "abs": "https://arxiv.org/abs/2601.13806", "authors": ["Dezhao Song", "Guglielmo Bonifazi", "Frank Schilder", "Jonathan Richard Schwarz"], "title": "Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \\textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.", "AI": {"tldr": "提出基于知识图谱的LLM后训练方法，使用IRAC框架构建法律知识图谱，通过SFT和DPO训练提升法律推理能力，在多个法律基准测试中表现优异。", "motivation": "当前LLM后训练主要依赖文本语料和人类反馈，缺乏对领域知识结构的捕捉，导致在处理复杂推理任务（特别是高风险专业领域如法律）时表现不佳。", "method": "采用IRAC框架建模关键法律概念，构建包含12K法律案例的知识图谱，使用该图谱生成训练数据，对三个SOTA LLM（30B、49B和70B）进行监督微调和直接偏好优化。", "result": "后训练模型在4/5个多样化法律基准（14个任务）上平均表现优于基线，70B DPO模型在4/6个推理任务上获得最佳成绩，甚至超越了141B SOTA法律LLM。", "conclusion": "知识图谱辅助方法能有效增强LLM的法律推理能力，该方法可推广到其他高风险专业领域。"}}
{"id": "2601.13835", "pdf": "https://arxiv.org/pdf/2601.13835", "abs": "https://arxiv.org/abs/2601.13835", "authors": ["Sam OConnor Russell", "Delphine Charuau", "Naomi Harte"], "title": "The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations", "categories": ["cs.CL"], "comment": "Accepted to ICASSP 2026", "summary": "Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.", "AI": {"tldr": "该研究通过声码器方法控制语音中的韵律和词汇线索，发现基于自监督语音表示(S3R)的对话轮换模型可以单独使用韵律或词汇线索进行预测，两者在S3R中编码有限相互依赖。", "motivation": "解决人机交互中流畅对话轮换的挑战，探究S3R模型是依赖韵律线索、词汇线索还是两者都依赖。", "method": "使用声码器方法更清晰地控制语音中的韵律和词汇线索，测试基于S3R的语音活动预测模型。", "result": "韵律匹配但不可理解的噪音预测准确率与清晰语音相似；韵律和词汇线索都能支持对话轮换，且可以单独使用；当任一信息被破坏时，模型无需额外训练就能利用另一信息。", "conclusion": "未来模型可能只需要韵律线索，这能提供隐私保护和潜在性能优势；研究结果在CPC和wav2vec2.0 S3R中一致，为未来研究提供了方向。"}}
{"id": "2601.13836", "pdf": "https://arxiv.org/pdf/2601.13836", "abs": "https://arxiv.org/abs/2601.13836", "authors": ["Qian Chen", "Jinlan Fu", "Changsong Li", "See-Kiong Ng", "Xipeng Qiu"], "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": "https://openmoss.github.io/FutureOmni", "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", "AI": {"tldr": "FutureOmni是首个评估多模态大语言模型在视听环境中预测未来事件能力的基准测试，包含919个视频和1034个QA对。当前模型在此任务上表现不佳（最佳准确率64.8%），作者提出了OFF训练策略来提升未来预测能力。", "motivation": "现有的多模态大语言模型主要关注回顾性理解，而在从视听线索预测未来事件方面的能力尚未充分探索，需要建立专门的评估基准。", "method": "通过可扩展的LLM辅助人工循环流程构建FutureOmni基准，包含8个主要领域的视听数据；提出Omni-Modal Future Forecasting (OFF)训练策略，并使用7K样本的指令调优数据集进行训练。", "result": "评估13个全模态和7个纯视频模型显示，当前系统在视听未来预测方面表现挣扎，特别是在语音密集场景中，Gemini 3 Flash取得最佳64.8%准确率。OFF方法在FutureOmni及其他基准测试中显示出对未来预测和泛化能力的提升。", "conclusion": "多模态大语言模型在视听未来预测方面仍有很大改进空间，FutureOmni基准和OFF训练策略为此领域提供了重要工具和方法，有助于推动未来事件预测能力的发展。"}}
{"id": "2601.13876", "pdf": "https://arxiv.org/pdf/2601.13876", "abs": "https://arxiv.org/abs/2601.13876", "authors": ["Unggi Lee", "Jahyun Jeong", "Sunyoung Shin", "Haeun Park", "Jeongsu Moon", "Youngchang Song", "Jaechang Shim", "JaeHwan Lee", "Yunju Noh", "Seungwon Choi", "Ahhyun Kim", "TaeHyeon Kim", "Kyungtae Joo", "Taeyeong Kim", "Gyeonggeon Lee"], "title": "Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education", "categories": ["cs.CL"], "comment": null, "summary": "Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \\textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.", "AI": {"tldr": "论文提出了一个教育对齐的轻量级视觉-语言-动作框架，通过文本修复、知识蒸馏、安全训练和教学评估四个组件，在资源受限的教育环境中实现可解释的科学演示机器人系统。", "motivation": "当前VLA模型需要大量计算资源且牺牲语言生成能力，不适合需要可解释性教育系统的资源受限教育环境。教师在进行科学演示时面临安全和一致性的挑战。", "method": "提出Pedagogical VLA Framework，包含四个组件：文本修复恢复语言生成能力、LLM蒸馏传递教学知识、教育环境安全训练、适应科学教育的教学评估。在物理、化学、生物和地球科学五个演示场景中评估。", "result": "框架在任务性能（成功率、协议遵从性、效率、安全性）上与基线模型相当，同时能生成上下文适当的教育解释。通过教师调查和LLM-as-Judge评估验证教学质量。", "conclusion": "该框架为资源受限的教育环境提供了一个有效的解决方案，能够在保持任务性能的同时提供高质量的教学解释，有助于提升STEM教育的科学演示效果。"}}
{"id": "2601.13882", "pdf": "https://arxiv.org/pdf/2601.13882", "abs": "https://arxiv.org/abs/2601.13882", "authors": ["Unggi Lee", "Sookbun Lee", "Heungsoo Choi", "Jinseo Lee", "Haeun Park", "Younghoon Jeon", "Sungmin Cho", "Minju Kang", "Junbo Koh", "Jiyeong Bae", "Minwoo Nam", "Juyeon Eun", "Yeonji Jung", "Yeil Jeong"], "title": "OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.", "AI": {"tldr": "OpenLearnLM Benchmark是一个基于教育评估理论的三维框架，用于评估大语言模型在教育场景中的能力，包括知识、技能和态度三个维度，包含12.4万+测试项，评估显示不同模型在不同维度表现各异，没有单一模型在所有方面都领先。", "motivation": "现有的大语言模型教育评估基准过于关注狭窄技能且缺乏学习科学基础，需要建立一个理论扎实、全面的评估框架来真实评估LLM在教育环境中的准备度。", "method": "基于教育评估理论构建三维评估框架：知识（课程对齐内容和教学理解）、技能（基于场景的能力，采用四层级中心-角色-场景-子场景结构）、态度（一致性对齐和欺骗抵抗）。整合了12.4万+测试项，覆盖多学科、教育角色和布鲁姆分类法的难度级别。", "result": "评估7个前沿模型显示不同的能力分布：Claude-Opus-4.5在实践技能上表现优异但内容知识较低，Grok-4.1-fast在知识方面领先但存在对齐问题。没有单一模型在所有维度都占优。", "conclusion": "多维度评估对于全面理解LLM在教育环境中的能力至关重要，OpenLearnLM提供了一个开放、全面的框架来推进LLM在真实教育场景中的准备度评估。"}}
{"id": "2601.13885", "pdf": "https://arxiv.org/pdf/2601.13885", "abs": "https://arxiv.org/abs/2601.13885", "authors": ["Esma Balkır", "Alice Pernthaller", "Marco Basaldella", "José Hernández-Orallo", "Nigel Collier"], "title": "Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.", "AI": {"tldr": "该论文提出了一种针对连续评分生成任务的计算机自适应测试方法，通过异方差正态分布替代伯努利分布，实现了用仅2%的测试项就能达到95%准确率的模型排名。", "motivation": "现代LLM评估越来越多地依赖生成任务，这些任务的输出是连续评分而非二元正确/错误判断，需要扩展传统的CAT方法以适应连续有界评分。", "method": "使用异方差正态分布替代IRT中的伯努利响应分布，建立基于连续有界评分（如ROUGE、BLEU、LLM-as-a-Judge）的自适应测试框架，并引入具有自适应停止标准的不确定性感知排序器。", "result": "在五个基准测试中验证，方法仅使用2%的测试项，相比随机采样提高了0.12τ的排名相关性，在置信预测上达到95%的准确率。", "conclusion": "该方法成功将IRT自适应测试扩展到连续评分场景，实现了高效且可靠的LLM模型排名，显著减少了评估成本。"}}
{"id": "2601.13918", "pdf": "https://arxiv.org/pdf/2601.13918", "abs": "https://arxiv.org/abs/2601.13918", "authors": ["Yusheng Liao", "Chuan Xuan", "Yutong Cai", "Lina Yang", "Zhe Chen", "Yanfeng Wang", "Yu Wang"], "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization", "categories": ["cs.CL"], "comment": "37 pages, 12 figures", "summary": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.", "AI": {"tldr": "论文提出了AgentEHR基准测试和RetroSum框架，用于解决大语言模型在原始电子病历数据中进行自主导航和复杂决策任务时面临的信息丢失和推理连续性断裂问题。", "motivation": "当前大语言模型在医疗领域的应用受到限制，主要依赖精心策划的输入和简化的检索任务，无法在原始高噪声的电子病历数据库中进行长距离交互推理和复杂决策。", "method": "提出了RetroSum框架，结合了回顾性总结机制和演化经验策略。回顾机制通过动态重新评估交互历史来防止长上下文信息丢失，演化策略通过从记忆库中检索累积经验来弥合领域差距。", "result": "RetroSum在竞争基线基础上实现了高达29.16%的性能提升，同时将总交互错误显著降低了高达92.3%。", "conclusion": "RetroSum框架有效解决了电子病历自主导航中的信息丢失和推理连续性断裂问题，为在真实临床环境中应用大语言模型提供了有效的解决方案。"}}
{"id": "2601.13919", "pdf": "https://arxiv.org/pdf/2601.13919", "abs": "https://arxiv.org/abs/2601.13919", "authors": ["Yuezhe Yang", "Hao Wang", "Yige Peng", "Jinman Kim", "Lei Bi"], "title": "HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs", "categories": ["cs.CL", "cs.CV"], "comment": "Under Review", "summary": "Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \\textbf{HyperWalker}, a \\textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \\textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \\textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \\textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker", "AI": {"tldr": "HyperWalker是一个基于动态超图和测试时训练的深度诊断框架，通过构建iBrochure超图建模电子健康记录的结构异质性和多模态临床信息的高阶关联，使用强化学习代理Walker导航寻找最优诊断路径，在医学报告生成和视觉问答任务上达到最先进性能。", "motivation": "现有医学视觉语言模型主要采用样本隔离推理范式，独立处理病例而无法利用纵向电子健康记录或结构相关的患者示例，限制了仅基于图像信息的推理能力，忽略了外部补充医学证据。", "method": "1. 构建动态超图iBrochure建模EHR数据结构和多模态临床信息的高阶关联\n2. 使用强化学习代理Walker在超图中导航寻找最优诊断路径\n3. 引入linger机制，采用多跳正交检索策略迭代选择反映不同临床属性的互补邻域病例", "result": "在MIMIC数据集上的医学报告生成任务和EHRXQA数据集上的医学视觉问答任务中，HyperWalker都取得了最先进的性能表现。", "conclusion": "HyperWalker通过动态超图和测试时训练重新构建了临床推理框架，能够有效利用外部医学证据进行更准确的诊断，为医学AI诊断提供了新的解决方案。"}}
{"id": "2601.13922", "pdf": "https://arxiv.org/pdf/2601.13922", "abs": "https://arxiv.org/abs/2601.13922", "authors": ["Adrian Cosma", "Oleg Szehr", "David Kletz", "Alessandro Antonucci", "Olivier Pelletier"], "title": "Automatic Prompt Optimization for Dataset-Level Feature Discovery", "categories": ["cs.CL"], "comment": "5 Figures, 1 Table", "summary": "Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.", "AI": {"tldr": "提出多智能体提示优化框架，将特征发现视为数据集级提示优化问题，通过语言模型智能体协同工作自动从非结构化文本中发现可解释的判别性特征。", "motivation": "当前从非结构化文本中提取特征的方法主要依赖手工制作的提示或固定特征模式，需要更自动化和可解释的特征发现方法。", "method": "使用多智能体框架，语言模型智能体共同提出特征定义、提取特征值并评估特征质量，基于数据集级性能和可解释性反馈迭代优化提示。", "result": "该方法能够优化诱导共享特征集的提示，而不是针对单个样本的预测，提供了从非结构化文本自动发现特征的机制。", "conclusion": "该框架为自动特征发现提供了原则性方法，突破了依赖逐样本监督的传统提示优化方法，实现了数据集级的特征定义优化。"}}
{"id": "2601.13992", "pdf": "https://arxiv.org/pdf/2601.13992", "abs": "https://arxiv.org/abs/2601.13992", "authors": ["Jin Cui", "Jiaqi Guo", "Jiepeng Zhou", "Ruixuan Yang", "Jiayi Lu", "Jiajun Xu", "Jiangcheng Song", "Boran Zhao", "Pengju Ren"], "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework", "categories": ["cs.CL", "cs.AI"], "comment": "11pages, 9figures", "summary": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.", "AI": {"tldr": "COMPACT是一个多教师CoT蒸馏框架，通过动态梯度加权融合不同教师的监督，使用图共识、互信息适应性和损失难度三个维度评估学生模型兼容性，有效整合多样化推理能力并避免灾难性遗忘。", "motivation": "现有CoT蒸馏方法通常依赖单一教师，受限于教师能力偏差和灾难性遗忘问题，而多教师融合面临教师-学生不兼容导致幻觉放大和被动监督无法确保逻辑内化的挑战。", "method": "提出COMPACT框架：1）图共识过滤误导性推理路径；2）互信息适应性检测真正理解时刻；3）损失难度评估学生接受度防止负迁移。通过这三个维度动态加权教师梯度。", "result": "大量实验和潜在空间分析表明，COMPACT有效整合了多样化推理能力而不破坏模型原有知识结构，在多个基准测试中达到最先进性能，同时缓解了灾难性遗忘。", "conclusion": "COMPACT通过自适应融合多教师监督，成功解决了CoT蒸馏中的教师不兼容和逻辑内化问题，为将复杂推理能力有效转移到紧凑学生模型提供了有效解决方案。"}}
{"id": "2601.13995", "pdf": "https://arxiv.org/pdf/2601.13995", "abs": "https://arxiv.org/abs/2601.13995", "authors": ["Zihan Niu", "Wenping Hu", "Junmin Chen", "Xiyue Wang", "Tong Xu", "Ruiming Tang"], "title": "From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \\textbf{+5.84\\%} using only \\textbf{5\\%} of the data, while our aligned sampling strategy further boosts average performance by \\textbf{+4.24\\%}.", "AI": {"tldr": "TAGS是一个基于知识树的指令调优数据选择框架，通过细粒度标签和层次聚类构建知识树，实现了质量、多样性和目标对齐的联合控制，仅用5%数据就超越全数据集性能5.84%。", "motivation": "现有方法依赖实例级质量评分或基于嵌入聚类/语义标签的多样性度量，受限于嵌入空间平坦性和标签粗糙性，忽略了细粒度知识及其层次依赖关系，阻碍了精确数据评估和知识对齐采样。", "method": "使用LLM标注器提取原子知识概念，通过自底向上层次聚类构建全局知识树；基于该树开发树感知度量来量化数据质量和多样性；通过最大化树级信息增益和KL散度实现叶级对齐的可控采样策略。", "result": "TAGS显著优于现有基线方法，仅使用5%数据就超越全数据集模型性能5.84%，对齐采样策略进一步将平均性能提升4.24%。", "conclusion": "基于知识树的数据选择框架能够有效捕捉细粒度知识层次结构，实现更精确的数据评估和知识对齐采样，为LLM指令调优提供了高效可控的数据选择解决方案。"}}
{"id": "2601.14004", "pdf": "https://arxiv.org/pdf/2601.14004", "abs": "https://arxiv.org/abs/2601.14004", "authors": ["Hengyuan Zhang", "Zhihao Zhang", "Mingyang Wang", "Zunhai Su", "Yiwei Wang", "Qianli Wang", "Shuzhou Yuan", "Ercong Nie", "Xufeng Duan", "Qibo Xue", "Zeping Yu", "Chenming Shang", "Xiao Liang", "Jing Xiong", "Hui Shen", "Chaofan Tao", "Zhengwu Liu", "Senjie Jin", "Zhiheng Xi", "Dongdong Zhang", "Sophia Ananiadou", "Tao Gui", "Ruobing Xie", "Hayden Kwok-Hay So", "Hinrich Schütze", "Xuanjing Huang", "Qi Zhang", "Ngai Wong"], "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.", "AI": {"tldr": "本文提出一个基于\"定位-引导-改进\"流程的可操作机制可解释性框架，将MI从观察科学转变为可干预的模型优化方法", "motivation": "现有机制可解释性研究主要作为观察科学，缺乏系统性的可操作干预框架，需要弥合这一差距", "method": "建立\"定位、引导、改进\"的流程框架，基于特定可解释对象对定位（诊断）和引导（干预）方法进行形式化分类，建立严格的干预协议", "result": "该框架能够在对齐性、能力和效率方面实现实质性改进，使MI成为模型优化的可操作方法", "conclusion": "该工作提供了一个结构化的可操作MI框架，将机制可解释性从单纯的分析洞察转变为实际的模型优化工具"}}
{"id": "2601.14041", "pdf": "https://arxiv.org/pdf/2601.14041", "abs": "https://arxiv.org/abs/2601.14041", "authors": ["Yunhe Wang", "Kai Han", "Huiling Zhen", "Yuchuan Tian", "Hanting Chen", "Yongbing Huang", "Yufei Cui", "Yingte Shu", "Shan Gao", "Ismail Elezi", "Roy Vaughan Miles", "Songcen Xu", "Feng Wen", "Chao Xu", "Sinan Zeng", "Dacheng Tao"], "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.", "AI": {"tldr": "本文提出扩散语言模型(DLMs)作为自回归语言模型的革命性替代方案，分析了当前阻碍DLMs发展的十大挑战，并提出了包含四大支柱的战略路线图，旨在突破因果瓶颈限制，实现下一代AI的复杂结构推理能力。", "motivation": "自回归语言模型存在因果瓶颈限制，缺乏全局结构预见性和迭代优化能力。扩散语言模型提供了整体性双向去噪的新范式，但当前仍受限于自回归遗留架构和优化框架，未能充分发挥潜力。", "method": "识别了阻碍扩散语言模型发展的十大根本挑战，包括架构惯性、梯度稀疏性和线性推理限制等。提出了包含基础架构、算法优化、认知推理和统一多模态智能四大支柱的战略路线图。", "result": "提出了向扩散原生生态系统转变的愿景，包括多尺度标记化、主动重掩码和潜在思维等关键技术，以超越因果视野限制。", "conclusion": "向扩散原生生态系统的转型对于开发具备复杂结构推理、动态自我修正和无缝多模态集成能力的下一代AI至关重要，DLMs有望实现其\"GPT-4时刻\"的突破。"}}
{"id": "2601.14007", "pdf": "https://arxiv.org/pdf/2601.14007", "abs": "https://arxiv.org/abs/2601.14007", "authors": ["Junyu Zhang", "Yipeng Kang", "Jiong Guo", "Jiayu Zhan", "Junqi Wang"], "title": "BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models", "categories": ["cs.CL"], "comment": "34 pagess, 16 figures, 6 tables, submitted to ACL 2026", "summary": "Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.", "AI": {"tldr": "论文提出了一个抽象-具象框架来评估大语言模型对抽象概念的理解，发现模型具有跨抽象层级的价值表示能力，价值表征作为稳定锚点影响具体决策但不受干预影响", "motivation": "探究大语言模型是否真正理解抽象概念，还是仅仅进行统计模式操作，特别以人类价值观作为测试案例，因其语义丰富且对AI对齐至关重要", "method": "使用探测（检测内部激活中的价值痕迹）和干预（修改表征以改变行为）方法，在6个开源LLM和10个价值维度上进行实验，分析抽象-抽象、抽象-具象、具象-具象三个层面的能力", "result": "探测显示基于抽象价值描述训练的探针能可靠检测具体事件叙述和决策推理中的相同价值，证明跨层级迁移能力；干预揭示不对称性：价值表征干预能因果改变具体判断和决策，但不影响抽象解释", "conclusion": "LLMs维持着连接抽象与行动的结构化价值表示，这为构建价值驱动的自主AI系统提供了机制性和操作性的基础，可实现更透明、可泛化的对齐和控制"}}
{"id": "2601.14051", "pdf": "https://arxiv.org/pdf/2601.14051", "abs": "https://arxiv.org/abs/2601.14051", "authors": ["Peter Devine", "Mardhiyah Sanni", "Farid Adilazuarda", "Julieta Gil Loizaga", "Barry Haddow"], "title": "Kakugo: Distillation of Low-Resource Languages into Small Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.", "AI": {"tldr": "Kakugo是一个低成本管道，仅需语言名称即可为低资源语言训练通用小语言模型(SLMs)，通过教师模型生成合成提示和翻译指令数据，在54种语言上验证有效，单语言成本低于50美元", "motivation": "解决低资源语言缺乏训练数据和AI模型支持的问题，为语言社区提供经济可行的AI开发方案", "method": "使用大型教师模型生成合成提示和翻译指令数据集，为54种低资源语言创建训练数据并训练小语言模型", "result": "在翻译、分类、问答等自然语言处理任务上，该管道持续提升基础模型性能，证明了方法的有效性", "conclusion": "Kakugo提供了一种经济高效且易于使用的方法，使语言社区能够以极低成本开发特定语言的AI模型，促进语言技术民主化"}}
{"id": "2601.14032", "pdf": "https://arxiv.org/pdf/2601.14032", "abs": "https://arxiv.org/abs/2601.14032", "authors": ["Hongli Zhou", "Hui Huang", "Wei Liu", "Chenglong Wang", "Xingyuan Bu", "Lvyuan Han", "Fuhai Song", "Muyun Yang", "Wenhao Jiang", "Hailong Cao", "Tiejun Zhao"], "title": "RM-Distiller: Exploiting Generative LLM for Reward Model Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.", "AI": {"tldr": "RM-Distiller是一个奖励模型蒸馏框架，通过系统利用教师LLM的三种能力（精炼、评分和生成）来提升奖励模型性能，显著优于传统方法。", "motivation": "现有方法主要将教师模型视为简单的二元标注器，未能充分利用其丰富的知识和能力进行奖励模型蒸馏。", "method": "提出RM-Distiller框架，利用教师LLM的三种能力：1)精炼能力-合成高相关响应对创建细粒度对比信号；2)评分能力-通过边缘感知优化目标指导RM捕获精确偏好强度；3)生成能力-整合教师生成分布来正则化RM保持基本语言知识。", "result": "大量实验表明，RM-Distiller在RM基准测试和基于强化学习的对齐任务上显著优于传统蒸馏方法。", "conclusion": "系统利用教师模型的多方面能力对于有效的奖励建模至关重要，这是首个关于从生成式LLM进行RM蒸馏的系统研究。"}}
{"id": "2601.14063", "pdf": "https://arxiv.org/pdf/2601.14063", "abs": "https://arxiv.org/abs/2601.14063", "authors": ["Mohsinul Kabir", "Tasnim Ahmed", "Md Mezbaur Rahman", "Shaoxiong Ji", "Hassan Alhuzali", "Sophia Ananiadou"], "title": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "30 Pages, 13 Figures", "summary": "Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.", "AI": {"tldr": "XCR-Bench是一个跨文化推理基准，包含4900个平行句子和1098个独特的文化特定项目，用于评估大语言模型在跨文化情境下的识别和适应能力。", "motivation": "当前评估大语言模型跨文化能力的进展受到高质量CSI标注语料库稀缺的限制，特别是缺乏跨文化平行句子对。", "method": "整合Newmark的CSI框架和Hall的文化三元论，构建包含三个推理任务的基准，涵盖表面文化元素以及半可见和不可见的文化元素（如社会规范、信仰和价值观）。", "result": "最先进的大语言模型在识别和适应社会礼仪与文化参考相关的CSI方面表现一致较弱，并在文化适应过程中编码了区域和民族宗教偏见。", "conclusion": "该研究为跨文化NLP的未来研究提供了重要资源，揭示了LLMs在跨文化推理方面的系统性弱点，并发布了语料库和代码以促进进一步研究。"}}
{"id": "2601.14124", "pdf": "https://arxiv.org/pdf/2601.14124", "abs": "https://arxiv.org/abs/2601.14124", "authors": ["Saad Mankarious", "Aya Zirikly"], "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.", "AI": {"tldr": "提出了一种基于扩散模型的文本生成方法，用于解决心理健康数据中的性别偏见问题，通过男性到女性的风格转换来增加女性作者内容的代表性，无需依赖预训练大语言模型。", "motivation": "现有合成数据方法主要依赖预训练大语言模型，存在输出多样性有限和传播训练数据偏见的问题，特别是在心理健康领域存在显著的性别不平衡问题。", "method": "使用CARMA阿拉伯语心理健康语料库，构建五个捕捉不同语言和语义性别表达方面的数据集，为每个设置训练独立的扩散模型，将偏见缓解构建为风格转换问题。", "result": "定量评估显示源文本和生成文本之间保持高语义保真度，同时具有有意义的表面风格差异，定性分析确认了语言上合理的性别转换。", "conclusion": "基于扩散的风格转换能够生成高熵值、语义忠实的合成数据，为缓解敏感、低资源心理健康领域的性别偏见提供了有效且灵活的框架。"}}
{"id": "2601.14046", "pdf": "https://arxiv.org/pdf/2601.14046", "abs": "https://arxiv.org/abs/2601.14046", "authors": ["Shikhar Bharadwaj", "Chin-Jou Li", "Yoonjae Kim", "Kwanghee Choi", "Eunjung Yeo", "Ryan Soh-Eun Shim", "Hanyu Zhou", "Brendon Boldt", "Karen Rosero Jacome", "Kalvin Chang", "Darsh Agrawal", "Keer Xu", "Chao-Han Huck Yang", "Jian Zhu", "Shinji Watanabe", "David R. Mortensen"], "title": "PRiSM: Benchmarking Phone Realization in Speech Models", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.", "AI": {"tldr": "PRiSM是首个开源基准测试，通过内在和外在评估揭示语音识别系统的盲点，发现多语言训练、编码器-CTC模型稳定性以及专用模型优于大型音频语言模型的重要性。", "motivation": "当前语音识别系统评估仅关注表面转录准确性，缺乏对语音感知盲点的深入分析，需要标准化评估方法以提升跨语言语音处理的音素能力。", "method": "PRiSM通过标准化转录评估，在临床、教育和多语言场景中使用转录和表示探针进行内在和外在评估，系统比较不同模型的性能。", "result": "研究发现：多语言训练对性能至关重要，编码器-CTC模型最稳定，专用语音识别模型仍优于大型音频语言模型。", "conclusion": "PRiSM提供了代码、配方和数据集，推动领域向具有强大音素能力的多语言语音模型发展，强调标准化评估和多语言训练的重要性。"}}
{"id": "2601.14152", "pdf": "https://arxiv.org/pdf/2601.14152", "abs": "https://arxiv.org/abs/2601.14152", "authors": ["Hyunjong Ok", "Jaeho Lee"], "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.", "AI": {"tldr": "研究发现，在大语言模型的多选题回答中，将上下文放在问题和选项之前（CQO）比反向顺序（QOC）性能高出14%以上，原因是QOC中的因果注意力机制阻止选项关注上下文信息", "motivation": "大语言模型对提示词结构表现出惊人的敏感性，但这种敏感性的机制尚不清楚，需要深入研究", "method": "通过系统性的架构分析，识别因果注意力机制在多选题回答中的作用机制", "result": "发现CQO顺序比QOC顺序性能显著提升14%以上，因果注意力在QOC中阻止选项令牌关注上下文，形成信息瓶颈", "conclusion": "因果注意力机制是影响提示词结构敏感性的核心机制，QOC顺序导致上下文信息对选项不可见，从而影响模型性能"}}
{"id": "2601.14050", "pdf": "https://arxiv.org/pdf/2601.14050", "abs": "https://arxiv.org/abs/2601.14050", "authors": ["Yuxin Chen", "Zhengzhou Cai", "Xiangtian Ji", "Weixiang Zhao", "An Zhang", "Xiang Wang", "Tat-Seng Chua"], "title": "Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering", "categories": ["cs.CL"], "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.", "AI": {"tldr": "该论文分析了MoE模型的多语言处理机制，发现路由行为与语言家族对齐，专家使用呈现层次模式，并提出路由引导的引导方法以提升多语言性能。", "motivation": "虽然MoE架构展现出强大的多语言能力，但其性能提升的内部机制和跨语言差异尚未得到充分理解。", "method": "对MoE模型进行系统分析，包括跨语言和网络深度的路由行为和专家专业化研究，并通过层次干预实验探索不同层的功能。", "result": "发现多语言处理高度结构化：路由与语言家族对齐，专家使用呈现层次模式，高资源语言依赖共享专家而低资源语言更多使用语言专属专家。提出的路由引导方法能持续提升多语言性能。", "conclusion": "MoE模型的多语言处理具有明确的层次结构特征，通过针对中间层的路由引导可以显著改善多语言性能，特别是在语言相关的语言对中表现更佳。"}}
{"id": "2601.14160", "pdf": "https://arxiv.org/pdf/2601.14160", "abs": "https://arxiv.org/abs/2601.14160", "authors": ["Ali Hamza Bashir", "Muhammad Rehan Khalid", "Kostadin Cvejoski", "Jana Birr", "Jule Berghaus", "Armin Berger", "Sandra Halscheidt", "Christian Temath", "Rafet Sifa", "David Berghaus"], "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.", "AI": {"tldr": "本文提出了一种通过合成数据生成方法将大型语言模型适配到德国法律问答领域的新方法，无需昂贵的人工标注即可从权威法律文本生成高质量问答对，显著提升模型在法律问答任务上的表现。", "motivation": "大型语言模型在专业领域如法律推理中因缺乏专家知识而经常产生事实错误或幻觉，需要有效方法在缺乏人工标注数据的情况下适配到专业领域。", "method": "使用新颖的合成数据生成方法直接从德国权威法规系统性地生成高质量、多样化且法律准确的问答对，并采用严格的自动化过滤方法和参数高效微调技术。", "result": "使用合成数据适配的LLM在德国法律问答任务上显著优于基线模型，证明了合成数据在知识密集型领域的有效性。", "conclusion": "精心设计的合成数据可以作为高风险、知识密集型领域中人工标注的可靠替代方案，为专业领域适配LLM提供了可行路径。"}}
{"id": "2601.14172", "pdf": "https://arxiv.org/pdf/2601.14172", "abs": "https://arxiv.org/abs/2601.14172", "authors": ["Víctor Yeste", "Paolo Rosso"], "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum", "categories": ["cs.CL", "cs.AI"], "comment": "Code: https://github.com/VictorMYeste/human-value-detection, 37 pages, 4 figures,", "summary": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.", "AI": {"tldr": "该研究探讨了在句子层面识别施瓦茨动机连续体中的19种价值观，作为文本中人类价值检测的具体形式。通过比较不同模型架构和集成方法，发现在8GB GPU限制下，经过精心调优的监督编码器在结构化人类价值检测中仍然是强大且计算效率高的基线方法。", "motivation": "研究动机是解决在新闻和政治宣言等脱离上下文的句子中进行细粒度句子级价值检测的内在困难，这些场景具有稀疏的道德线索和严重的类别不平衡问题，即使对于强大的现代神经模型也具有挑战性。", "method": "研究方法包括：1）构建二元道德存在性任务；2）比较存在性门控层次结构与直接多标签分类器；3）基准测试指令调优的LLMs（Gemma 2 9B、Llama 3.1 8B等）在零样本/少样本和QLoRA设置下的性能；4）构建简单集成模型。使用DeBERTa-base为基础，并添加轻量级信号（前句上下文、LIWC-22/eMFD/MJD词典和主题特征）。", "result": "研究结果显示：二元道德存在性任务可从单句学习（正类F1≈0.74）；层次结构未能超越直接预测；软投票监督集成达到macro-F1 0.332，显著超过最佳单监督模型和先前的英文基线；轻量级信号和小型集成产生最可靠的改进。", "conclusion": "结论认为，在8GB单GPU约束和7-9B规模下，精心调优的监督编码器仍然是结构化人类价值检测的强大且计算效率高的基线方法。更丰富的价值结构和文档上下文中的句子信息可以进一步提高性能，而层次门控的益处有限。"}}
{"id": "2601.14230", "pdf": "https://arxiv.org/pdf/2601.14230", "abs": "https://arxiv.org/abs/2601.14230", "authors": ["Yiyang Wang", "Yiqiao Jin", "Alex Cabral", "Josiah Hester"], "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "15 pages, 9 figures", "summary": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.", "AI": {"tldr": "MASCOT框架通过双层优化策略解决多智能体系统中的人格崩溃和社交奉承问题，提升人格一致性和社交贡献度", "motivation": "多智能体系统存在人格崩溃（智能体退化为通用行为）和社交奉承（产生冗余非建设性对话）的问题", "method": "提出MASCOT框架，包含：1）人格感知行为对齐（RLAIF驱动的个体智能体微调）；2）协作对话优化（基于群体奖励的元策略）", "result": "在心理支持和职场领域评估中显著优于现有基线，人格一致性提升14.1，社交贡献度提升10.6", "conclusion": "MASCOT为构建下一代社会智能多智能体系统提供了实用路线图"}}
{"id": "2601.14105", "pdf": "https://arxiv.org/pdf/2601.14105", "abs": "https://arxiv.org/abs/2601.14105", "authors": ["Olesya Razuvayevskaya", "Kalina Bontcheva"], "title": "Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks", "categories": ["cs.CL"], "comment": "In Proceedings of the ACM Web Conference 2026 (WWW 2026)", "summary": "This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means", "AI": {"tldr": "本研究首次大规模比较了群众撰写与专业撰写的事实核查文本中的说服技巧。研究发现群众创作的反驳并不比专业事实核查使用更多说服技巧，并揭示了两种来源在修辞手法上的系统性差异。", "motivation": "先前假设认为群众生产的事实核查内容可能更依赖主观或说服性措辞，但缺乏大规模实证比较研究来验证这一假设。", "method": "使用来自Community Notes、EUvsDisinfo和Database of Known Fakes的广泛数据集，量化这些事实核查生态系统中说服技巧的普遍程度和类型。", "result": "1. 没有证据表明Community Notes比专业事实核查包含更多平均数量的说服技巧\n2. 发现群众和专业反驳之间存在系统性修辞差异\n3. 虽然包含更多说服元素的笔记获得略高的有用性评分，但群众评分者能够有效惩罚特定有问题的修辞手段使用", "conclusion": "群众撰写的事实核查在说服技巧使用上与专业内容相当，且群众评分机制能够有效识别和惩罚不当修辞，这支持了群众参与事实核查的可行性和有效性。"}}
{"id": "2601.14242", "pdf": "https://arxiv.org/pdf/2601.14242", "abs": "https://arxiv.org/abs/2601.14242", "authors": ["Bertie Vidgen", "Austin Mann", "Abby Fennelly", "John Wright Stanly", "Lucas Rothman", "Marco Burstein", "Julien Benchek", "David Ostrofsky", "Anirudh Ravichandran", "Debnil Sur", "Neel Venugopal", "Alannah Hsia", "Isaac Robinson", "Calix Huang", "Olivia Varones", "Daniyal Khan", "Michael Haines", "Zach Richards", "Chirag Mahapatra", "Brendan Foody", "Osvald Nitski"], "title": "APEX-Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.", "AI": {"tldr": "APEX-Agents是一个评估AI代理在投资银行、管理咨询和法律领域执行跨应用长时程任务的基准测试，包含480个任务，Gemini 3 Flash在测试中表现最佳。", "motivation": "需要评估AI代理在真实工作环境中处理复杂跨应用任务的能力，特别是针对投资银行分析师、管理顾问和企业律师的实际工作需求。", "method": "创建包含480个任务的APEX-Agents基准，要求代理在包含文件和工具的真实工作环境中操作，使用Pass@1指标测试8个不同的AI代理。", "result": "Gemini 3 Flash (Thinking=High)获得最高分24.0%，其次是GPT-5.2、Claude Opus 4.5和Gemini 3 Pro。所有测试环境和评估基础设施都已开源。", "conclusion": "APEX-Agents为评估AI代理在实际工作场景中的表现提供了标准化基准，展示了当前领先模型的能力水平，并开源了完整的测试基础设施供社区使用。"}}
{"id": "2601.14112", "pdf": "https://arxiv.org/pdf/2601.14112", "abs": "https://arxiv.org/abs/2601.14112", "authors": ["George Mihaila"], "title": "Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.", "AI": {"tldr": "ExpNet是一种轻量级神经网络，通过从transformer注意力模式学习到token重要性分数的显式映射，自动发现最优注意力特征组合，解决了现有注意力解释方法依赖手动聚合策略和固定归因规则的问题。", "motivation": "随着transformer模型在高风险领域部署，模型不透明性阻碍了信任和问责。现有注意力解释方法依赖手动定义的聚合策略，而模型无关方法将模型视为黑箱且计算成本高。", "method": "引入Explanation Network (ExpNet)，一个轻量级神经网络，学习从transformer注意力模式到token级别重要性分数的显式映射，自动发现最优注意力特征组合而非依赖预定规则。", "result": "在跨任务设置中评估ExpNet，并与广泛的模型无关方法和基于注意力的技术进行基准测试，涵盖四个方法家族。", "conclusion": "ExpNet提供了一种自动学习注意力特征组合的新方法，相比传统手动定义规则的方法具有优势，为transformer模型的可解释性提供了更有效的解决方案。"}}
{"id": "2601.14121", "pdf": "https://arxiv.org/pdf/2601.14121", "abs": "https://arxiv.org/abs/2601.14121", "authors": ["Jonathan Tonglet", "Iryna Gurevych", "Tinne Tuytelaars", "Marie-Francine Moens"], "title": "NewsRECON: News article REtrieval for image CONtextualization", "categories": ["cs.CL"], "comment": "Preprint under review. Code available at https://github.com/jtonglet/arxiv2025-newsrecon", "summary": "Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.", "AI": {"tldr": "NewsRECON是一种新方法，通过将新闻图像链接到相关文章来推断其拍摄时间和地点，解决了反向图像搜索失败时的挑战，在TARA和5Pils-OOC数据集上取得了SOTA效果。", "motivation": "现有的反向图像搜索(RIS)引擎经常无法返回结果，限制了实际应用。需要解决RIS证据不可用时的新闻图像时空定位问题。", "method": "NewsRECON方法利用9万多篇文章语料库，包含：(1)双编码器检索事件相关文章；(2)两个交叉编码器通过位置和事件一致性对文章重新排序。还可与多模态大语言模型结合使用。", "result": "在TARA和5Pils-OOC数据集上的实验表明，NewsRECON优于现有方法，在缺乏RIS证据的情况下实现了新的SOTA结果。", "conclusion": "NewsRECON提供了一种有效的替代方案，能够在反向图像搜索不可用时准确推断新闻图像的时间和地点信息，对新闻可信度和反虚假信息具有重要意义。"}}
{"id": "2601.14123", "pdf": "https://arxiv.org/pdf/2601.14123", "abs": "https://arxiv.org/abs/2601.14123", "authors": ["Sofia Bennani", "Charles Moslonka"], "title": "A Systematic Analysis of Chunking Strategies for Reliable Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "3 pages, 2 figures, 1 table, pre-print", "summary": "We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a \"context cliff\" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).", "AI": {"tldr": "本研究系统评估了文档分块策略对RAG系统性能的影响，发现句子分块是最具成本效益的方法，重叠分块无实际益处，且存在约2.5k tokens的上下文质量临界点。", "motivation": "工业实践中RAG系统的文档分块通常依赖启发式方法，缺乏系统性评估，需要研究不同分块选择对系统可靠性的实际影响。", "method": "使用自然问题数据集进行端到端评估，系统变化分块方法（token、句子、语义、代码）、分块大小、重叠和上下文长度，采用SPLADE检索和Mistral-8B生成器的标准工业设置。", "result": "发现重叠分块无显著益处且增加索引成本；句子分块在约5k tokens内与语义分块效果相当且更经济；超过约2.5k tokens出现质量下降的\"上下文悬崖\"；最优上下文长度取决于目标（语义质量在小上下文最佳，精确匹配需要更大上下文）。", "conclusion": "为成本高效的RAG部署提供了实用指导：避免使用重叠分块，优先选择句子分块，控制上下文长度在2.5k tokens以内，并根据具体目标调整上下文大小。"}}
{"id": "2601.14210", "pdf": "https://arxiv.org/pdf/2601.14210", "abs": "https://arxiv.org/abs/2601.14210", "authors": ["Rohan Bhatnagar", "Youran Sun", "Chi Andrew Zhang", "Yixin Wen", "Haizhao Yang"], "title": "HALT: Hallucination Assessment via Latent Testing", "categories": ["cs.CL"], "comment": null, "summary": "Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.", "AI": {"tldr": "提出一种轻量级残差探针方法，直接从LLM中间隐藏状态读取幻觉风险，实现近乎实时的幻觉风险评估，可用于选择性生成和路由决策。", "motivation": "大型语言模型存在幻觉问题，虽然内部表示可能编码了不确定性，但解码压力仍会产生流畅但不准确的回答。中间层可能保留了在最终解码阶段被衰减的认识性信号。", "method": "使用小型辅助网络作为残差探针，从问题标记的中间隐藏状态直接读取幻觉风险。该方法计算成本远低于标记生成，可与推理完全并行评估。", "result": "在四个QA基准测试和多个LLM家族中，方法表现出强大的AUROC和AURAC性能，在数据集偏移下具有良好泛化能力，并揭示了中间表示的可解释结构。", "conclusion": "快速内部不确定性读取为可靠的代理AI提供了原则性基础，通过选择性生成和路由机制，使LLM能够立即回答确信查询，同时将不确定查询委托给更强的验证流程。"}}
{"id": "2601.14249", "pdf": "https://arxiv.org/pdf/2601.14249", "abs": "https://arxiv.org/abs/2601.14249", "authors": ["Yuming Yang", "Mingyoung Lai", "Wanxu Zhao", "Xiaoran Fan", "Zhiheng Xi", "Mingqi Wu", "Chiyue Huang", "Jun Zhao", "Haijun Lv", "Jian Tong", "Yunhua Zhou", "Yicheng Zou", "Qipeng Guo", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment", "categories": ["cs.CL"], "comment": "26 pages. Project page: https://github.com/UmeanNever/RankSurprisalRatio", "summary": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.", "AI": {"tldr": "论文提出了Rank-Surprisal Ratio (RSR)指标，通过结合对齐性和信息性来评估推理轨迹的适用性，在知识蒸馏中比现有指标表现更好。", "motivation": "现有方法主要通过学生似然度评估轨迹适用性，但忽略了更有信息量的轨迹。研究发现强教师生成的轨迹不一定产生更好的学生模型，需要更好的评估指标。", "method": "提出RSR指标，定义为轨迹的平均token排名与平均负对数似然的比值，平衡学习信号强度和行为对齐性。", "result": "在5个学生模型和11个不同教师的实验中，RSR与训练后性能强相关（平均Spearman 0.86），优于现有指标。", "conclusion": "RSR是一个简单有效的指标，可用于轨迹选择和教师选择，解决了知识蒸馏中数据-学生适配性的评估问题。"}}
