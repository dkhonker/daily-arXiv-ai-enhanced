<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 88]
- [cs.AI](#cs.AI) [总数: 13]
- [stat.ML](#stat.ML) [总数: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [MedSyn: Enhancing Diagnostics with Human-AI Collaboration](https://arxiv.org/abs/2506.14774)
*Burcu Sayin, Ipek Baris Schlicht, Ngoc Vo Hong, Sara Allievi, Jacopo Staiano, Pasquale Minervini, Andrea Passerini*

**主要类别:** cs.LG

**AI概要:** 提出了一种混合的人工智能框架MedSyn，通过医生与大型语言模型（LLMs）之间的多步骤互动对话来改进诊断和治疗决策。模拟的医患-LLM交互表明开源LLMs作为医生助手具有潜力。


<details>
  <summary>更多</summary>
  
**动机:** 临床决策过程复杂且受到认知偏见、信息不全以及病例模糊性的影响。虽然大型语言模型显示出支持临床决策的前景，但它们通常的一次性或有限互动使用方式可能忽略了现实世界医疗实践中的复杂性。

**方法:** 设计了一个名为MedSyn的混合人机框架，让医生和大型语言模型能够进行多步骤、互动式的对话，以精确诊断并制定治疗方案。这种框架不同于静态的决策支持工具，它允许动态交流，医生可以质疑LLM的建议，同时LLM也能提供不同的视角。研究通过模拟的医生-LLM交互来评估开源LLMs作为医生助手的可能性。

**结果:** 结果显示，开源的大型语言模型在实际中作为医生助手展现出潜力。

**结论:** MedSyn框架有潜力成为提高诊断准确性和病人结果的有效工具。未来的工作将涉及真实的医生互动，以进一步验证MedSyn在诊断准确性及病人结果方面的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MedSyn%3A+Enhancing+Diagnostics+with+Human-AI+Collaboration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14774，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14774&send_immediately=true&force_search=false)

**原文摘要:** Clinical decision-making is inherently complex, often influenced by cognitive
biases, incomplete information, and case ambiguity. Large Language Models
(LLMs) have shown promise as tools for supporting clinical decision-making, yet
their typical one-shot or limited-interaction usage may overlook the
complexities of real-world medical practice. In this work, we propose a hybrid
human-AI framework, MedSyn, where physicians and LLMs engage in multi-step,
interactive dialogues to refine diagnoses and treatment decisions. Unlike
static decision-support tools, MedSyn enables dynamic exchanges, allowing
physicians to challenge LLM suggestions while the LLM highlights alternative
perspectives. Through simulated physician-LLM interactions, we assess the
potential of open-source LLMs as physician assistants. Results show open-source
LLMs are promising as physician assistants in the real world. Future work will
involve real physician interactions to further validate MedSyn's usefulness in
diagnostic accuracy and patient outcomes.

</details>


### [2] [Two-dimensional Parallel Tempering for Constrained Optimization](https://arxiv.org/abs/2506.14781)
*Corentin Delacour, M Mahmudul Hasan Sajeeb, Joao P. Hespanha, Kerem Y. Camsari*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种二维并行回火算法(2D-PT)，它通过在惩罚强度上增加第二维度的副本，解决了Ising机器中软约束过强导致混合缓慢或过弱无法保证可行性的难题。实验表明该方法在图稀疏化和Wishart实例中具有优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 在机器学习和优化领域，采样Boltzmann概率分布非常重要，促使了像Ising机这样的硬件加速器的设计。然而，在实际应用中，由于软约束的存在，要么导致混合过程变慢，要么无法确保解的可行性。

**方法:** 研究者们引入了一个二维并行回火算法（2D-PT），该算法在原有的基础上添加了一个额外维度的复制品，用来调节惩罚力度。这种方案确保了最终复制品中的约束满足条件，并且类似于低温下的低能量状态。

**结果:** 2D-PT改进了高度受限副本中的混合效率，并消除了对惩罚强度进行显式调整的需求。对于带有复制约束的图稀疏化问题，2D-PT实现了接近理想的混合效果，Kullback-Leibler散度以O(1/t)的速度衰减。当应用于稀疏化的Wishart实例时，与相同数量副本的传统PT相比，2D-PT提供了数量级上的加速。

**结论:** 所提出的二维并行回火算法能够广泛地应用于受约束的Ising问题，并且可以部署到现有的Ising机器上，为解决此类问题提供了一个有效的途径。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Two-dimensional+Parallel+Tempering+for+Constrained+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14781&send_immediately=true&force_search=false)

**原文摘要:** Sampling Boltzmann probability distributions plays a key role in machine
learning and optimization, motivating the design of hardware accelerators such
as Ising machines. While the Ising model can in principle encode arbitrary
optimization problems, practical implementations are often hindered by soft
constraints that either slow down mixing when too strong, or fail to enforce
feasibility when too weak. We introduce a two-dimensional extension of the
powerful parallel tempering algorithm (PT) that addresses this challenge by
adding a second dimension of replicas interpolating the penalty strengths. This
scheme ensures constraint satisfaction in the final replicas, analogous to
low-energy states at low temperature. The resulting two-dimensional parallel
tempering algorithm (2D-PT) improves mixing in heavily constrained replicas and
eliminates the need to explicitly tune the penalty strength. In a
representative example of graph sparsification with copy constraints, 2D-PT
achieves near-ideal mixing, with Kullback-Leibler divergence decaying as
O(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of
magnitude speedup over conventional PT with the same number of replicas. The
method applies broadly to constrained Ising problems and can be deployed on
existing Ising machines.

</details>


### [3] [Integrating Dynamical Systems Learning with Foundational Models: A Meta-Evolutionary AI Framework for Clinical Trials](https://arxiv.org/abs/2506.14782)
*Joseph Geraci, Bessi Qorri, Christian Cumbaa, Mike Tsay, Paul Leonczyk, Luca Pani*

**主要类别:** cs.LG

**AI概要:** 本文介绍了两种AI系统：一个是大规模语言模型DeepSeek-V3，另一个是为小规模临床试验数据设计的NetraAI框架。NetraAI结合了收缩映射、信息几何和进化算法来识别预测性患者群体，并通过一个元进化层（LLM Strategist）与大型语言模型协作，以指导发现过程。在案例研究中，NetraAI仅使用少数特征就显著提高了分类器的表现。


<details>
  <summary>更多</summary>
  
**动机:** 为了处理小规模临床试验数据集，需要一种稳定且可解释的AI方法。本文动机在于开发一种新的自适应和自我反思的人工智能，它能够加速临床发现，并且提供可靠和可解释的知识。

**方法:** NetraAI是一种基于动力系统的方法，它利用收缩映射、信息几何以及进化算法来识别具有预测性的患者子群。该方法还包括一个伪时间嵌入和长程记忆机制，用于探索高阶特征交互，并通过内部进化的循环选择出简洁且可解释的变量组合（称为“人格”）。此外，还引入了一个大语言模型作为策略家，充当元进化层次，观察输出结果，优先考虑有前景的变量，注入领域知识，并评估稳健性。

**结果:** 在精神分裂症、抑郁症和胰腺癌等案例研究中，NetraAI发现了小而效应量大的亚人群，这些发现将弱基线模型(AUC ~0.50-0.68)转化为近乎完美的分类器，只使用了几个特征。

**结论:** NetraAI位于动力系统、信息几何和进化学习的交汇点，符合诸如LeCun的联合嵌入预测架构(JEPA)等新兴的概念级推理范式。通过重视可靠性和可解释性，NetraAI提供了一种新型的自适应和自我反思的人工智能，旨在加速临床发现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Integrating+Dynamical+Systems+Learning+with+Foundational+Models%3A+A+Meta-Evolutionary+AI+Framework+for+Clinical+Trials，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14782，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14782&send_immediately=true&force_search=false)

**原文摘要:** Artificial intelligence (AI) has evolved into an ecosystem of specialized
"species," each with unique strengths. We analyze two: DeepSeek-V3, a
671-billion-parameter Mixture of Experts large language model (LLM)
exemplifying scale-driven generality, and NetraAI, a dynamical system-based
framework engineered for stability and interpretability on small clinical trial
datasets. We formalize NetraAI's foundations, combining contraction mappings,
information geometry, and evolutionary algorithms to identify predictive
patient cohorts. Features are embedded in a metric space and iteratively
contracted toward stable attractors that define latent subgroups. A
pseudo-temporal embedding and long-range memory enable exploration of
higher-order feature interactions, while an internal evolutionary loop selects
compact, explainable 2-4-variable bundles ("Personas").
  To guide discovery, we introduce an LLM Strategist as a meta-evolutionary
layer that observes Persona outputs, prioritizes promising variables, injects
domain knowledge, and assesses robustness. This two-tier architecture mirrors
the human scientific process: NetraAI as experimentalist, the LLM as theorist,
forming a self-improving loop.
  In case studies (schizophrenia, depression, pancreatic cancer), NetraAI
uncovered small, high-effect-size subpopulations that transformed weak baseline
models (AUC ~0.50-0.68) into near-perfect classifiers using only a few
features. We position NetraAI at the intersection of dynamical systems,
information geometry, and evolutionary learning, aligned with emerging
concept-level reasoning paradigms such as LeCun's Joint Embedding Predictive
Architecture (JEPA). By prioritizing reliable, explainable knowledge, NetraAI
offers a new generation of adaptive, self-reflective AI to accelerate clinical
discovery.

</details>


### [4] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales, Arturo Jaramillo, Heli Ricalde Guerrero*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的基于粒子的变分推断方法——分支斯坦变分梯度下降（BSVGD），它通过引入随机分支机制来探索状态空间，适用于多模态分布。本文提供了理论收敛性保证，并通过数值实验验证了算法的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 为了处理多模态分布问题，作者扩展了经典的斯坦变分梯度下降（SVGD）算法，旨在提高对状态空间的探索能力。

**方法:** 提出了Branched Stein Variational Gradient Descent (BSVGD) 方法，该方法在SVGD的基础上加入了随机分支机制，以促进对状态空间更广泛的探索。

**结果:** 提供了关于分布收敛性的理论保障，并通过数值实验展示了BSVGD与SVGD相比，在样本间的Wasserstein距离和计算时间上的表现。

**结论:** BSVGD作为SVGD的一种改进版，通过随机分支机制增强了对于复杂多模态分布的适应性，且在实验中证实了其有效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Branching+Stein+Variational+Gradient+Descent+for+sampling+multimodal+distributions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.13916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.13916&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel particle-based variational inference method designed to
work with multimodal distributions. Our approach, referred to as Branched Stein
Variational Gradient Descent (BSVGD), extends the classical Stein Variational
Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism
that encourages the exploration of the state space. In this work, a theoretical
guarantee for the convergence in distribution is presented, as well as
numerical experiments to validate the suitability of our algorithm. Performance
comparisons between the BSVGD and the SVGD are presented using the Wasserstein
distance between samples and the corresponding computational times.

</details>


### [5] [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
*Mohamed Masry, Mohamed Amen, Mohamed Elzyat, Mohamed Hamed, Norhan Magdy, Maram Khaled*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种名为ETS的框架，该框架结合了脑电图(EEG)和同步眼动追踪数据，以解决开放词汇文本生成和感知语言的情感分类两个关键任务。模型在EEG到文本解码上BLEU和Rouge得分较高，在基于EEG的三元情感分类上F1分数提高了10%，表现优于监督基线，并且能够处理来自不同被试者和来源的数据。


<details>
  <summary>更多</summary>
  
**动机:** 使用非侵入性脑电图(EEG)从大脑活动中解码自然语言是神经科学和机器学习中的一个重大挑战，特别是在开放词汇场景下，传统方法难以应对噪声和可变性。先前的研究在小范围封闭词汇表上取得了高准确率，但在开放词汇上仍存在困难。

**方法:** 提出了一种名为ETS的新框架，它将EEG与同步的眼动追踪数据结合起来，旨在提高开放词汇表中自然语言解码的性能，并执行情绪分类。

**结果:** ETS框架在EEG到文本解码的任务上获得了较高的BLEU和Rouge评分，在基于EEG的三元情感分类上F1分数提高了10%。

**结论:** 提出的ETS模型在开放词汇表EEG到文本解码和情感分类方面表现出色，超过了监督基线，并且对于跨受试者和数据源具有良好的适应性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ETS%3A+Open+Vocabulary+Electroencephalography-To-Text+Decoding+and+Sentiment+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14783，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14783&send_immediately=true&force_search=false)

**原文摘要:** Decoding natural language from brain activity using non-invasive
electroencephalography (EEG) remains a significant challenge in neuroscience
and machine learning, particularly for open-vocabulary scenarios where
traditional methods struggle with noise and variability. Previous studies have
achieved high accuracy on small-closed vocabularies, but it still struggles on
open vocabularies. In this study, we propose ETS, a framework that integrates
EEG with synchronized eye-tracking data to address two critical tasks: (1)
open-vocabulary text generation and (2) sentiment classification of perceived
language. Our model achieves a superior performance on BLEU and Rouge score for
EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment
classification, which significantly outperforms supervised baselines.
Furthermore, we show that our proposed model can handle data from various
subjects and sources, showing great potential for high performance open
vocabulary eeg-to-text system.

</details>


### [6] [Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation](https://arxiv.org/abs/2506.14784)
*Emre Yilmaz, Philipp Bekemeyer*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于迁移学习的方法，通过表面压力数据预测来流参数（如攻角和来流速度），并展示了该方法在领域适应、任务适应以及噪声数据影响下的表现。


<details>
  <summary>更多</summary>
  
**动机:** 传统测量方法可能因传感器故障导致挑战，而基于表面压力数据的数据驱动预测模型可以作为替代方案。为了实现实时学习，并克服数据分布变化及新预测任务适应的挑战。

**方法:** 首先离线训练一个卷积神经网络(ConvNet)模型用于核心预测任务，然后冻结除输出节点前选定层之外的所有权重，最后通过对这些层进行再训练来执行迁移学习。

**结果:** 结果表明，该方法在适应数据分布变化、领域扩展和任务更新方面具有潜力，但对于噪声数据的应用效果不明显。

**结论:** 所提出的迁移学习方法为风洞测试和飞行操作中的来流参数预测提供了一个有效的解决方案，特别是在处理不同数据分布和新的预测任务时。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Onflow+Parameters+Using+Transfer+Learning+for+Domain+and+Task+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14784，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14784&send_immediately=true&force_search=false)

**原文摘要:** Determining onflow parameters is crucial from the perspectives of wind tunnel
testing and regular flight and wind turbine operations. These parameters have
traditionally been predicted via direct measurements which might lead to
challenges in case of sensor faults. Alternatively, a data-driven prediction
model based on surface pressure data can be used to determine these parameters.
It is essential that such predictors achieve close to real-time learning as
dictated by practical applications such as monitoring wind tunnel operations or
learning the variations in aerodynamic performance of aerospace and wind energy
systems. To overcome the challenges caused by changes in the data distribution
as well as in adapting to a new prediction task, we propose a transfer learning
methodology to predict the onflow parameters, specifically angle of attack and
onflow speed. It requires first training a convolutional neural network
(ConvNet) model offline for the core prediction task, then freezing the weights
of this model except the selected layers preceding the output node, and finally
executing transfer learning by retraining these layers. A demonstration of this
approach is provided using steady CFD analysis data for an airfoil for i)
domain adaptation where transfer learning is performed with data from a target
domain having different data distribution than the source domain and ii) task
adaptation where the prediction task is changed. Further exploration on the
influence of noisy data, performance on an extended domain, and trade studies
varying sampling sizes and architectures are provided. Results successfully
demonstrate the potential of the approach for adaptation to changing data
distribution, domain extension, and task update while the application for noisy
data is concluded to be not as effective.

</details>


### [7] [Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size](https://arxiv.org/abs/2506.15025)
*Soufiane Hayou, Liyuan Liu*

**主要类别:** cs.LG

**AI概要:** 本文研究了词汇量大小对大型语言模型（LLM）训练动态的影响，指出当词汇量增加时，训练动态会在μP（最大更新参数化）和大词汇量（LV）模式之间过渡。在LV模式下，最佳的嵌入学习率与隐藏层学习率的比例应大致为Θ(√宽度)，而非μP预测的Θ(宽度)。通过实验验证了理论，并展示了使用建议的学习率调整规则预训练10亿参数模型的好处。


<details>
  <summary>更多</summary>
  
**动机:** 先前的研究表明，尽管μP方法在实践中取得了令人印象深刻的结果，但当应用于大型语言模型（LLMs）时，观察到的情况并不一致。μP理论的一个局限是它假设输入维度（即词汇量大小）在考虑宽度趋向无穷时保持固定，这在实际中是不现实的，因为通常词汇量要远大于模型宽度。因此，本研究旨在分析词汇量大小如何影响训练动态，并提出一种新的适用于大词汇量情况下的最优缩放规则。

**方法:** 进行了理论分析以研究词汇量大小对训练动态的影响，定义了一个新的“大词汇量（LV）”模式，该模式描述了随着词汇量增大，训练动态会从μP模式向LV模式转变的现象。基于理论分析，提出了一个不同于μP所预测的新比例规则，即在LV模式下，最佳嵌入学习率与隐藏层学习率之比应约为Θ(√宽度)。此外，还通过一系列实验来验证提出的理论，并且从头开始预训练了一个10亿参数的模型来展示建议的缩放规则所带来的好处。

**结果:** 理论分析表明，在大词汇量（LV）模式下，为了优化训练效果，嵌入学习率与隐藏层学习率之间的理想比例应该接近于Θ(√宽度)，这一发现与文献中报告的经验结果非常接近，但与μP所预测的Θ(宽度)不同。通过实验验证了这个理论，并且证明了按照建议的比例规则设置学习率可以提高预训练过程中的性能。

**结论:** 研究表明，随着词汇量的增加，训练动态逐渐从μP模式转变为一个新的LV模式，在LV模式下需要采用不同的学习率缩放规则。具体来说，对于具有较大词汇量的语言模型，嵌入学习率与隐藏层学习率的理想比例大约为Θ(√宽度)。这些发现有助于改进大规模语言模型预训练过程中的效率和效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Embedding+Learning+Rate+in+LLMs%3A+The+Effect+of+Vocabulary+Size，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15025，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15025&send_immediately=true&force_search=false)

**原文摘要:** Pretraining large language models is a costly process. To make this process
more efficient, several methods have been proposed to optimize model
architecture/parametrization and hardware use. On the parametrization side,
$\mu P$ (Maximal Update Parametrization) parametrizes model weights and
learning rate (LR) in a way that makes hyperparameters (HPs) transferable with
width (embedding dimension): HPs can be tuned for a small model and used for
larger models without additional tuning. While $\mu$P showed impressive results
in practice, recent empirical studies have reported conflicting observations
when applied to LLMs. One limitation of the theory behind $\mu$P is the fact
that input dimension (vocabulary size in LLMs) is considered fixed when taking
the width to infinity. This is unrealistic since vocabulary size is generally
much larger than width in practice. In this work, we provide a theoretical
analysis of the effect of vocabulary size on training dynamics, and
subsequently show that as vocabulary size increases, the training dynamics
\emph{interpolate between the $\mu$P regime and another regime that we call
Large Vocab (LV) Regime}, where optimal scaling rules are different from those
predicted by $\mu$P. Our analysis reveals that in the LV regime, the optimal
embedding LR to hidden LR ratio should roughly scale as $\Theta(\sqrt{width})$,
surprisingly close to the empirical findings previously reported in the
literature, and different from the $\Theta(width)$ ratio predicted by $\mu$P.
We conduct several experiments to validate our theory, and pretrain a 1B model
from scratch to show the benefit of our suggested scaling rule for the
embedding LR.

</details>


### [8] [PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series](https://arxiv.org/abs/2506.14786)
*Haobo Li, Eunseo Jung, Zixin Chen, Zhaowei Wang, Yueya Wang, Huamin Qu, Alexis Kai Hon Lau*

**主要类别:** cs.LG

**AI概要:** 提出了一种轻量级方法——物理信息位置编码（PIPE），用于将物理信息嵌入视觉语言模型中，以提高多模态时间序列预测的准确性和对齐效果，特别是在卫星图像数据集上的台风强度预测方面表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 现有的多模态方法主要集中在利用文本数据辅助时间序列预测，而忽略了时间序列数据集中存在的视觉数据，并且难以有效捕捉视觉数据中的物理信息，如卫星图像的时间和地理空间背景。

**方法:** 提出了物理信息位置编码（PIPE），该方法包括两个关键创新：(1) 物理信息位置索引方案，用于将物理特性映射到位置ID；(2) 变体-频率位置编码机制，在嵌入空间中编码物理变量的频率信息及标记的顺序信息。

**结果:** 通过在最具代表性和最大的开源卫星图像数据集上进行实验，PIPE 在深度学习预测和气候领域方法两方面均取得了最先进性能，其中台风强度预测比先前工作提高了12%。

**结论:** PIPE 通过保持物理信息和顺序信息显著提升了多模态对齐与预测准确性，在多个基准测试中展现了优越性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PIPE%3A+Physics-Informed+Position+Encoding+for+Alignment+of+Satellite+Images+and+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14786，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14786&send_immediately=true&force_search=false)

**原文摘要:** Multimodal time series forecasting is foundational in various fields, such as
utilizing satellite imagery and numerical data for predicting typhoons in
climate science. However, existing multimodal approaches primarily focus on
utilizing text data to help time series forecasting, leaving the visual data in
existing time series datasets untouched. Furthermore, it is challenging for
models to effectively capture the physical information embedded in visual data,
such as satellite imagery's temporal and geospatial context, which extends
beyond images themselves. To address this gap, we propose physics-informed
positional encoding (PIPE), a lightweight method that embeds physical
information into vision language models (VLMs). PIPE introduces two key
innovations: (1) a physics-informed positional indexing scheme for mapping
physics to positional IDs, and (2) a variant-frequency positional encoding
mechanism for encoding frequency information of physical variables and
sequential order of tokens within the embedding space. By preserving both the
physical information and sequential order information, PIPE significantly
improves multimodal alignment and forecasting accuracy. Through the experiments
on the most representative and the largest open-sourced satellite image
dataset, PIPE achieves state-of-the-art performance in both deep learning
forecasting and climate domain methods, demonstrating superiority across
benchmarks, including a 12% improvement in typhoon intensity forecasting over
prior works. Our code is provided in the supplementary material.

</details>


### [9] [Muon Optimizes Under Spectral Norm Constraints](https://arxiv.org/abs/2506.15054)
*Lizhang Chen, Jonathan Li, Qiang Liu*

**主要类别:** cs.LG

**AI概要:** 本文通过将Muon优化器置于Lion-$\mathcal{K}$优化器家族中，提供了Muon的理论分析，并揭示了其隐式正则化效应。


<details>
  <summary>更多</summary>
  
**动机:** 尽管Muon优化器在实证表现上展示了很好的性能，但它的理论基础尚未被充分理解。本文旨在填补这一空白。

**方法:** 文章将Muon优化器与Lion-$\mathcal{K}$族中的优化器相联系，并且特别指出当使用核范数时，Muon等同于Lion-$\mathcal{K}$。基于Lion-$\mathcal{K}$的理论结果，文章进一步证明了Muon（带有解耦权重衰减）实际上是在解决一个对权重矩阵谱范数施加约束的优化问题。

**结果:** 研究表明，Muon优化器具有隐式的正则化效果，它通过解耦权重衰减来隐式地限制权重矩阵的谱范数。此外，通过对凸映射$\mathcal{K}$的选择进行变化，可以自然地推广到更广泛的隐式正则化和约束优化算法。

**结论:** 通过将Muon置于Lion-$\mathcal{K}$框架下，本文不仅阐明了Muon优化器的隐式正则化效应，还为进一步探索更多种类的隐式正则化和约束优化算法提供了可能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Muon+Optimizes+Under+Spectral+Norm+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15054，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15054&send_immediately=true&force_search=false)

**原文摘要:** The pursuit of faster optimization algorithms remains an active and important
research direction in deep learning. Recently, the Muon optimizer [JJB+24] has
demonstrated promising empirical performance, but its theoretical foundation
remains less understood. In this paper, we bridge this gap and provide a
theoretical analysis of Muon by placing it within the Lion-$\mathcal{K}$ family
of optimizers [CLLL24]. Specifically, we show that Muon corresponds to
Lion-$\mathcal{K}$ when equipped with the nuclear norm, and we leverage the
theoretical results of Lion-$\mathcal{K}$ to establish that Muon (with
decoupled weight decay) implicitly solves an optimization problem that enforces
a constraint on the spectral norm of weight matrices. This perspective not only
demystifies the implicit regularization effects of Muon but also leads to
natural generalizations through varying the choice of convex map $\mathcal{K}$,
allowing for the exploration of a broader class of implicitly regularized and
constrained optimization algorithms.

</details>


### [10] [Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems](https://arxiv.org/abs/2506.14787)
*Funing Li, Yuan Tian, Ruben Noortwyck, Jifeng Zhou, Liming Kuang, Robert Schulz*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于深度强化学习的框架，用于解决多深度存储系统中异构物品配置的检索问题。通过图表示法和结合了图神经网络与Transformer模型的新神经网络架构来优化检索延迟。


<details>
  <summary>更多</summary>
  
**动机:** 在现代工业和物流环境中，快速配送服务的发展增加了对高效且高密度存储系统的需求。虽然多深度自主车辆存储和检索系统（AVS/RS）可以提高存储密度，但这些系统在进行检索操作时会遇到通道堵塞的问题。传统方法是将具有相同特征的物品储存在同一通道内，但这限制了系统的灵活性和适应性。

**方法:** 研究者们提出了一个基于深度强化学习的框架，旨在处理包含不同特性的物品的多深度存储系统的检索问题。每件物品都关联有一个特定的到期日，目标是最小化总延迟。为此引入了一种基于图的状态表示方法，该方法结合了物品属性与多深度仓库的局部拓扑结构。此外，设计了一种新的神经网络架构，它结合了图神经网络（GNN）和Transformer模型。GNN负责编码拓扑信息及物品特定信息至直接可访问物品的嵌入向量，而Transformer则将这些嵌入映射为全局优先级分配。

**结果:** 广泛的数值实验表明，所提出的神经网络架构优于启发式方法，并且训练后的代理在优化检索延迟方面表现出色。

**结论:** 本研究开发了一种创新的方法来改进多深度存储系统中的检索效率问题，通过使用深度强化学习技术以及先进的神经网络架构实现了显著的效果提升。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Topology-Aware+and+Highly+Generalizable+Deep+Reinforcement+Learning+for+Efficient+Retrieval+in+Multi-Deep+Storage+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14787，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14787&send_immediately=true&force_search=false)

**原文摘要:** In modern industrial and logistics environments, the rapid expansion of fast
delivery services has heightened the demand for storage systems that combine
high efficiency with increased density. Multi-deep autonomous vehicle storage
and retrieval systems (AVS/RS) present a viable solution for achieving greater
storage density. However, these systems encounter significant challenges during
retrieval operations due to lane blockages. A conventional approach to mitigate
this issue involves storing items with homogeneous characteristics in a single
lane, but this strategy restricts the flexibility and adaptability of
multi-deep storage systems.
  In this study, we propose a deep reinforcement learning-based framework to
address the retrieval problem in multi-deep storage systems with heterogeneous
item configurations. Each item is associated with a specific due date, and the
objective is to minimize total tardiness. To effectively capture the system's
topology, we introduce a graph-based state representation that integrates both
item attributes and the local topological structure of the multi-deep
warehouse. To process this representation, we design a novel neural network
architecture that combines a Graph Neural Network (GNN) with a Transformer
model. The GNN encodes topological and item-specific information into
embeddings for all directly accessible items, while the Transformer maps these
embeddings into global priority assignments. The Transformer's strong
generalization capability further allows our approach to be applied to storage
systems with diverse layouts. Extensive numerical experiments, including
comparisons with heuristic methods, demonstrate the superiority of the proposed
neural network architecture and the effectiveness of the trained agent in
optimizing retrieval tardiness.

</details>


### [11] [Neural Canonical Polyadic Factorization for Traffic Analysis](https://arxiv.org/abs/2506.15079)
*Yikai Hou, Peng Tang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种神经规范多项式分解(NCPF)模型，该模型结合了低秩张量代数和深度表示学习，以实现鲁棒的交通数据填补。NCPF通过将CP分解嵌入到神经架构中，并利用分层特征融合机制来明确建模多线性交互作用，从而在六个城市交通数据集上展示了其优于现有六种最先进基线方法的表现。


<details>
  <summary>更多</summary>
  
**动机:** 当前智能交通系统依赖于准确的时空交通分析来优化城市流动性和基础设施韧性，然而传感器故障和异构感知差距导致的数据缺失严重阻碍了可靠的交通建模。

**方法:** 论文提出了一种神经规范多项式分解（NCPF）模型，它将低秩张量代数与深度表示学习相结合，用于鲁棒的交通数据插补。NCPF创新地通过可学习的嵌入投影将CP分解嵌入神经结构中，其中稀疏的交通张量被编码为跨道路段、时间间隔和移动度量的密集潜在因子。采用层次特征融合机制利用哈达玛积显式建模多重线性交互，并且堆叠的多层感知器层非线性地细化这些表征以捕捉复杂的时空耦合。

**结果:** 在六个城市交通数据集上的广泛评估表明，NCPF的表现优于六种最先进的基线方法。

**结论:** 通过统一CP分解的可解释因子分析与神经网络的非线性表达能力，NCPF为高维交通数据插补提供了一个原则性强而又灵活的方法，为下一代交通运输数字孪生和自适应交通控制系统提供了关键支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Canonical+Polyadic+Factorization+for+Traffic+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15079，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15079&send_immediately=true&force_search=false)

**原文摘要:** Modern intelligent transportation systems rely on accurate spatiotemporal
traffic analysis to optimize urban mobility and infrastructure resilience.
However, pervasive missing data caused by sensor failures and heterogeneous
sensing gaps fundamentally hinders reliable traffic modeling. This paper
proposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes
low-rank tensor algebra with deep representation learning for robust traffic
data imputation. The model innovatively embeds CP decomposition into neural
architecture through learnable embedding projections, where sparse traffic
tensors are encoded into dense latent factors across road segments, time
intervals, and mobility metrics. A hierarchical feature fusion mechanism
employs Hadamard products to explicitly model multilinear interactions, while
stacked multilayer perceptron layers nonlinearly refine these representations
to capture complex spatiotemporal couplings. Extensive evaluations on six urban
traffic datasets demonstrate NCPF's superiority over six state-of-the-art
baselines. By unifying CP decomposition's interpretable factor analysis with
neural network's nonlinear expressive power, NCPF provides a principled yet
flexible approaches for high-dimensional traffic data imputation, offering
critical support for next-generation transportation digital twins and adaptive
traffic control systems.

</details>


### [12] [AZT1D: A Real-World Dataset for Type 1 Diabetes](https://arxiv.org/abs/2506.14789)
*Saman Khamesian, Asiful Arefeen, Bithika M. Thompson, Maria Adela Grando, Hassan Ghasemzadeh*

**主要类别:** cs.LG

**AI概要:** 介绍了AZT1D数据集，该数据集包含来自25名1型糖尿病患者的详细信息，包括连续血糖监测、胰岛素泵和给药数据、碳水化合物摄入量以及设备模式等，这些数据支持了人工智能和机器学习应用的发展以改善临床决策和个人化护理。


<details>
  <summary>更多</summary>
  
**动机:** 由于公开可用的提供详尽且全面患者数据的数据集稀缺，限制了1型糖尿病管理中数据驱动方法的进步。为了解决这一问题，研究者提出了一个新的数据集。

**方法:** 收集了25名使用自动化胰岛素输送系统(AID)的1型糖尿病患者的数据，包括连续血糖监测(CGM)数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量及设备模式（常规、睡眠和运动），每位患者的数据采集周期为6至8周。

**结果:** 生成了一个名为AZT1D的数据集，它提供了详细的自然状态下的数据，特别是关于推注胰岛素投递的具体细节，这在现有数据集中是很少见的。

**结论:** 通过提供丰富的自然主义数据，AZT1D数据集能够支持广泛的人工智能与机器学习应用，旨在提高1型糖尿病患者的临床决策和个人化护理水平。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AZT1D%3A+A+Real-World+Dataset+for+Type+1+Diabetes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14789，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14789&send_immediately=true&force_search=false)

**原文摘要:** High quality real world datasets are essential for advancing data driven
approaches in type 1 diabetes (T1D) management, including personalized therapy
design, digital twin systems, and glucose prediction models. However, progress
in this area has been limited by the scarcity of publicly available datasets
that offer detailed and comprehensive patient data. To address this gap, we
present AZT1D, a dataset containing data collected from 25 individuals with T1D
on automated insulin delivery (AID) systems. AZT1D includes continuous glucose
monitoring (CGM) data, insulin pump and insulin administration data,
carbohydrate intake, and device mode (regular, sleep, and exercise) obtained
over 6 to 8 weeks for each patient. Notably, the dataset provides granular
details on bolus insulin delivery (i.e., total dose, bolus type, correction
specific amounts) features that are rarely found in existing datasets. By
offering rich, naturalistic data, AZT1D supports a wide range of artificial
intelligence and machine learning applications aimed at improving clinical
decision making and individualized care in T1D.

</details>


### [13] [Interpretability and Generalization Bounds for Learning Spatial Physics](https://arxiv.org/abs/2506.15199)
*Alejandro Francisco Queiruga, Theo Gutman-Solo, Shuai Jiang*

**主要类别:** cs.LG

**AI概要:** 本文通过将数值分析的严谨性应用于机器学习，量化了不同ML技术在解决1D泊松微分方程时的准确性，并发现模型泛化到真实物理方程的能力并不总是得到保证。此外，还提出了一个新的基于格林函数表示的科学模型机制解释方法，以及一种新的交叉验证技术来度量物理系统中的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管机器学习（ML）在科学问题上的应用前景广阔，但对于科学应用来说，实际的定量准确性至关重要。本研究旨在通过严格的方法来评估不同的ML技术在解决基础的一维泊松微分方程时的准确性。

**方法:** 采用数值分析的方法对微分方程进行严格的处理，具体来说是通过对训练动态的分析和推导最优参数来确定白盒微分方程发现方法和黑盒线性模型的最佳参数。同时，还理论和实证地展示了各种模型在有限数据离散化和受限训练数据子空间下的泛化边界和收敛率。

**结果:** 证明了在探索的所有情况下，模型对真实物理方程的泛化能力并不能得到保证。出乎意料的是，不同类别的模型可以表现出相反的泛化行为。另外，从黑盒模型的权重中提取出了格林函数表示法，为科学模型提供了一种新的机制可解释性视角。

**结论:** 研究表明，对于给定的数据集，即使使用了多种ML技术，模型也不一定能够很好地泛化至真实的物理方程。此外，提出了一种新的基于格林函数的解释方式和一种针对物理系统的新型交叉验证技术。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Interpretability+and+Generalization+Bounds+for+Learning+Spatial+Physics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15199，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15199&send_immediately=true&force_search=false)

**原文摘要:** While there are many applications of ML to scientific problems that look
promising, visuals can be deceiving. For scientific applications, actual
quantitative accuracy is crucial. This work applies the rigor of numerical
analysis for differential equations to machine learning by specifically
quantifying the accuracy of applying different ML techniques to the elementary
1D Poisson differential equation. Beyond the quantity and discretization of
data, we identify that the function space of the data is critical to the
generalization of the model. We prove generalization bounds and convergence
rates under finite data discretizations and restricted training data subspaces
by analyzing the training dynamics and deriving optimal parameters for both a
white-box differential equation discovery method and a black-box linear model.
The analytically derived generalization bounds are replicated empirically.
Similar lack of generalization is empirically demonstrated for deep linear
models, shallow neural networks, and physics-specific DeepONets and Neural
Operators. We theoretically and empirically demonstrate that generalization to
the true physical equation is not guaranteed in each explored case.
Surprisingly, we find that different classes of models can exhibit opposing
generalization behaviors. Based on our theoretical analysis, we also
demonstrate a new mechanistic interpretability lens on scientific models
whereby Green's function representations can be extracted from the weights of
black-box models. Our results inform a new cross-validation technique for
measuring generalization in physical systems. We propose applying it to the
Poisson equation as an evaluation benchmark of future methods.

</details>


### [14] [Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting](https://arxiv.org/abs/2506.14790)
*Tianxiang Zhan, Ming Jin, Yuanpeng He, Yuxuan Liang, Yong Deng, Shirui Pan*

**主要类别:** cs.LG

**AI概要:** 本文提出了连续进化池(CEP)，一种针对时间序列中重复出现的概念漂移问题的解决方案，通过存储不同概念的预测器实例来保留所有概念知识，并在概念重现时充分利用这些知识。实验表明，CEP能有效保留不同概念的知识，在线预测中对于重复出现的概念显著提高了预测结果。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决时间序列分析中常见的重复概念漂移问题，其中以前观察到的数据模式会随时间重新出现，导致在线预测准确度下降。现有方法虽然尝试延缓遗忘过程但可能导致部分已学知识丢失，且忽视了知识保持机制的探索。

**方法:** 提出了一种名为连续进化池(CEP)的新方法，该方法通过为不同的概念存储预测器的不同实例来保存所有的概念知识。当遇到测试样本时，首先选择最近的预测器然后从其邻近样本学习特征，称为检索过程；如果邻近样本不足，则表示出现了新概念，此时将基于当前最接近样本发展出新的模型加入池中以储存新概念的知识。同时设有淘汰机制定期清理过时知识，保证预测效果。

**结果:** 通过对不同架构模型和八个真实数据集进行实验验证，证明了CEP能够有效地保留不同概念的知识。特别地，在处理具有重复概念的在线预测场景时，CEP显著提升了预测结果的质量。

**结论:** CEP提供了一个有效的框架用于处理时间序列中的重复概念漂移问题，通过独特的存储与检索机制以及动态更新策略，成功实现了对过往概念的有效记忆与利用，从而大大改善了面对重复概念时的预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Continuous+Evolution+Pool%3A+Taming+Recurring+Concept+Drift+in+Online+Time+Series+Forecasting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14790，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14790&send_immediately=true&force_search=false)

**原文摘要:** Recurring concept drift, a type of concept drift in which previously observed
data patterns reappear after some time, is one of the most prevalent types of
concept drift in time series. As time progresses, concept drift occurs and
previously encountered concepts are forgotten, thereby leading to a decline in
the accuracy of online predictions. Existing solutions employ parameter
updating techniques to delay forgetting; however, this may result in the loss
of some previously learned knowledge while neglecting the exploration of
knowledge retention mechanisms. To retain all conceptual knowledge and fully
utilize it when the concepts recur, we propose the Continuous Evolution Pool
(CEP), a pooling mechanism that stores different instances of forecasters for
different concepts. Our method first selects the forecaster nearest to the test
sample and then learns the features from its neighboring samples - a process we
refer to as the retrieval. If there are insufficient neighboring samples, it
indicates that a new concept has emerged, and a new model will evolve from the
current nearest sample to the pool to store the knowledge of the concept.
Simultaneously, the elimination mechanism will enable outdated knowledge to be
cleared to ensure the prediction effect of the forecasters. Experiments on
different architectural models and eight real datasets demonstrate that CEP
effectively retains the knowledge of different concepts. In the scenario of
online forecasting with recurring concepts, CEP significantly enhances the
prediction results.

</details>


### [15] [Warping and Matching Subsequences Between Time Series](https://arxiv.org/abs/2506.15452)
*Simiao Lin, Wannes Meert, Pieter Robberechts, Hendrik Blockeel*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的技术，简化了扭曲路径以突出、量化和可视化关键转换（位移、压缩、幅度差异），从而增强时间序列比较中的可解释性。


<details>
  <summary>更多</summary>
  
**动机:** 在进行时间序列的聚类和分类等任务时，虽然弹性距离度量允许变形提供了一个稳健的定量比较，但缺乏基于它们的定性比较。传统的可视化专注于点对点对齐，并不能传达子序列层面更广泛的结构关系。这种局限性使得难以理解一个时间序列相对于另一个时间序列是如何以及在哪里发生偏移、加速或减速的。

**方法:** 本文提出了一种新技术，通过简化变形路径来强调、量化并可视化关键变换（如位移、压缩、振幅差异）。该方法旨在提供更清晰的时间序列之间子序列匹配表示，进而提高时间序列对比的可解释性。

**结果:** 该方法能够有效地突出、量化并可视化时间序列之间的关键转换，包括位移、压缩及振幅差异等现象。这不仅有助于用户更好地理解时间序列间的关系，也增强了时间序列比较过程中的可解释性。

**结论:** 本文介绍的新技术为时间序列比较提供了一种增强的定性分析手段，通过对变形路径的简化处理，可以更清楚地展示出不同时间序列间的结构性关系及其变化模式，从而大大提高了这一领域工作的可解释性和实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Warping+and+Matching+Subsequences+Between+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15452，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15452&send_immediately=true&force_search=false)

**原文摘要:** Comparing time series is essential in various tasks such as clustering and
classification. While elastic distance measures that allow warping provide a
robust quantitative comparison, a qualitative comparison on top of them is
missing. Traditional visualizations focus on point-to-point alignment and do
not convey the broader structural relationships at the level of subsequences.
This limitation makes it difficult to understand how and where one time series
shifts, speeds up or slows down with respect to another. To address this, we
propose a novel technique that simplifies the warping path to highlight,
quantify and visualize key transformations (shift, compression, difference in
amplitude). By offering a clearer representation of how subsequences match
between time series, our method enhances interpretability in time series
comparison.

</details>


### [16] [Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout](https://arxiv.org/abs/2506.14793)
*Aditya Ravuri, Neil D. Lawrence*

**主要类别:** cs.LG

**AI概要:** 通过在蛋白质语言模型的特征层和变换器之间添加一个dropout层，并采用类似蒙特卡洛dropout的方法平均输出，可以提高对蛋白质关键标量属性的零样本预测性能，即使模型最初没有用dropout进行训练。


<details>
  <summary>更多</summary>
  
**动机:** 研究者希望提高蛋白质语言模型（如ESM2）对于蛋白质关键标量属性（适应性）的零样本预测能力，而无需重新训练或微调模型。

**方法:** 在蛋白质语言模型的特征/嵌入层和变换器之间插入一个dropout层，并且在推理时使用类似于蒙特卡洛dropout的技术来平均输出结果，以提升ProteinGym数据集子集上的零样本表现。

**结果:** 实验表明，在推断过程中加入dropout层并采用类似蒙特卡洛dropout的方法平均输出，能够提高蛋白质关键标量属性的零样本预测性能。此外，即使模型初始训练时未使用dropout，该方法也有效，且不需对PLM进行再训练或微调。

**结论:** 向蛋白质语言模型中引入dropout层，并在推理时应用类似蒙特卡洛dropout的方法，可以增强模型对于蛋白质关键标量属性的零样本预测能力，其中0.1的dropout率似乎对所有模型都有效。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Protein+Language+Model+Zero-Shot+Fitness+Predictions+are+Improved+by+Inference-only+Dropout，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14793，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14793&send_immediately=true&force_search=false)

**原文摘要:** Protein Language Models (PLMs) such as ESM2 have been shown to be capable of
zero-shot prediction of critical scalar properties of proteins (fitness). In
this work, we show that injecting a dropout layer at inference time between a
PLM's featurizer/embedding layer and its transformer, and averaging its output
akin to Monte-Carlo dropout increases zero-shot performance on a subset of the
ProteinGym dataset. This is the case even when the model was not trained with
dropouts to begin with, and does not require retraining or finetuning of the
PLM. A dropout of 0.1 seems performant across all models.

</details>


### [17] [LIT-LVM: Structured Regularization for Interaction Terms in Linear Predictors using Latent Variable Models](https://arxiv.org/abs/2506.15492)
*Mohammadreza Nemati, Zhipeng Huang, Kevin S. Xu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为LIT-LVM的新方法，通过假设交互项系数具有近似的低维结构，并用低维空间中的潜在向量表示每个特征，从而在高维环境中准确估计线性预测器中交互项的系数。该方法相比弹性网络和因子分解机，在模拟和真实数据上表现出更好的预测准确性，特别是在交互项数量远高于样本数量时。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们想要解决在线性预测器中精确估计交互项系数的问题。他们认为不同交互项的系数具有近似的低维结构，希望通过利用这种结构来提高预测准确性并减少过拟合。

**方法:** 提出的方法是LIT-LVM，它将每个特征映射到一个低维空间中的潜在向量，以此来表示交互项系数的低维结构。这种方法可以看作是一种结构化的正则化方法，能够进一步减轻在高维环境下的过拟合问题。

**结果:** 实验结果表明，LIT-LVM在多种模拟和真实数据集上都比弹性网络和因子分解机有更高的预测精度，尤其是在交互项的数量相对于样本数量较高时。此外，LIT-LVM还提供了有用的特征之间的低维潜在关系表示，有助于可视化和分析这些关系。

**结论:** LIT-LVM作为一种新的方法，能够有效地估计线性预测器中的交互项系数，并且在高维设置下提供优于现有方法的预测性能。同时，它还能为特征提供有价值的低维表示以支持进一步的分析。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LIT-LVM%3A+Structured+Regularization+for+Interaction+Terms+in+Linear+Predictors+using+Latent+Variable+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15492，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15492&send_immediately=true&force_search=false)

**原文摘要:** Some of the simplest, yet most frequently used predictors in statistics and
machine learning use weighted linear combinations of features. Such linear
predictors can model non-linear relationships between features by adding
interaction terms corresponding to the products of all pairs of features. We
consider the problem of accurately estimating coefficients for interaction
terms in linear predictors. We hypothesize that the coefficients for different
interaction terms have an approximate low-dimensional structure and represent
each feature by a latent vector in a low-dimensional space. This
low-dimensional representation can be viewed as a structured regularization
approach that further mitigates overfitting in high-dimensional settings beyond
standard regularizers such as the lasso and elastic net. We demonstrate that
our approach, called LIT-LVM, achieves superior prediction accuracy compared to
elastic net and factorization machines on a wide variety of simulated and real
data, particularly when the number of interaction terms is high compared to the
number of samples. LIT-LVM also provides low-dimensional latent representations
for features that are useful for visualizing and analyzing their relationships.

</details>


### [18] [Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors](https://arxiv.org/abs/2506.14794)
*Henrik Klagges, Robert Dahlke, Fabian Klemm, Benjamin Merkel, Daniel Klingmann, David A. Reiss, Dan Zecha*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种名为"Assembly-of-Experts"(AoE)的新方法，用于从现有的混合专家模型中创建功能强大的子模型。通过改变来自父模型权重的比例，可以观察到AoE子模型的一些特性逐渐变化，而其他行为特征则突然出现。使用这种方法，研究者们构建了一个名为DeepSeek R1T "Chimera"的671B开放权重混合模型，该模型结合了DeepSeek的V3-0324和R1型号变体的优点，并且不需要任何微调或蒸馏就能表现出令人惊讶的紧凑有序推理能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地利用在预训练模型上的巨大投资，并提高计算效率，即解决在大型语言模型（LLM）预训练过程中计算一个8位权重需要$10^{13}$-$10^{15}$ FLOPs的问题。

**方法:** 开发了新的"Assembly-of-Experts"(AoE)构造方法，允许单独插值模型权重张量以增强或抑制父模型的语义特征。通过调整从父模型获取权重的比例来观察子模型属性的变化。

**结果:** 几乎每个生成的模型都是功能性且有能力的，这使得搜索模型空间变得直接。特别是，所构建的DeepSeek R1T "Chimera"模型在没有进行任何微调或知识蒸馏的情况下，能够接近R1级别的智能水平，同时使用大约少40%的输出标记，速度接近V3。

**结论:** AoE方法证明了其在无需额外训练的情况下，能够有效组合不同专家模型的优势，创造出具有高效性能的新模型。此外，它还展示了可以通过调整父模型权重比例来控制新模型特性的潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Assembly+of+Experts%3A+Linear-time+construction+of+the+Chimera+LLM+variants+with+emergent+and+adaptable+behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14794，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14794&send_immediately=true&force_search=false)

**原文摘要:** Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM
during pretraining is extremely expensive and seems inefficient. To better
leverage the huge investments made into pretrained models, we develop the new
"Assembly-of-Experts" (AoE) construction method to create capable child
variants of existing Mixture-of-Experts parent models in linear time. Model
weight tensors get interpolated individually, allowing to enhance or suppress
semantic features of the parents.
  Varying the proportion of weights taken from the parent models, we observe
some properties of the AoE child model changing gradually, while other
behavioral traits emerge with a sharp transition. Surprisingly, nearly every
generated model is functional and capable, which makes searching the model
space straightforward.
  We construct the DeepSeek R1T "Chimera", a 671B open-weights hybrid model
combining DeepSeek's V3-0324 and R1 model variants. The child inherits only the
routed expert tensors of R1, but still achieves about R1-level intelligence. At
the same time, it uses about 40\% fewer output tokens, close to V3 speed.
Constructed without any fine-tuning or distillation, the Chimera exhibits
surprisingly compact, orderly reasoning compared to its parent models.

</details>


### [19] [A Simplified Analysis of SGD for Linear Regression with Weight Averaging](https://arxiv.org/abs/2506.15535)
*Alexandru Meterez, Depen Morwani, Costin-Andrei Oncescu, Jingfeng Wu, Cengiz Pehlevan, Sham Kakade*

**主要类别:** cs.LG

**AI概要:** 本文通过简化分析，利用基础线性代数工具重新得到了与先前研究相同偏差和方差的边界，从而为随机梯度下降(SGD)在线性回归中的优化提供了更易于理解的方法。


<details>
  <summary>更多</summary>
  
**动机:** 为了更好地理解和优化广泛使用的随机梯度下降算法在过参数化模型中的表现，特别是针对线性回归问题时，现有分析方法需要处理正半定矩阵上的算子，这使得分析变得复杂。本文旨在提供一个基于简单线性代数工具的新分析途径，来简化这一过程。

**方法:** 作者采用了一种简化的分析方法，该方法基于基本线性代数工具，避免了对正半定(PSD)矩阵上算子的操作，同时能够重现之前研究中给出的偏差-方差分解风险界限。

**结果:** 研究结果表明，即使不依赖复杂的矩阵运算，也能获得与之前工作相同的偏差和方差边界。这使得对于SGD在线性回归中性能的理解变得更加容易，并为进一步探讨小批量处理和学习率调度奠定了基础。

**结论:** 本研究表明，通过对SGD在线性回归中的应用进行简化分析，可以更容易地掌握其优化机制，并有望促进对实际模型训练过程中mini-batching及学习率调整策略的研究，进而提高模型训练效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是A+Simplified+Analysis+of+SGD+for+Linear+Regression+with+Weight+Averaging，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15535，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15535&send_immediately=true&force_search=false)

**原文摘要:** Theoretically understanding stochastic gradient descent (SGD) in
overparameterized models has led to the development of several optimization
algorithms that are widely used in practice today. Recent work
by~\citet{zou2021benign} provides sharp rates for SGD optimization in linear
regression using constant learning rate, both with and without tail iterate
averaging, based on a bias-variance decomposition of the risk. In our work, we
provide a simplified analysis recovering the same bias and variance bounds
provided in~\citep{zou2021benign} based on simple linear algebra tools,
bypassing the requirement to manipulate operators on positive semi-definite
(PSD) matrices. We believe our work makes the analysis of SGD on linear
regression very accessible and will be helpful in further analyzing
mini-batching and learning rate scheduling, leading to improvements in the
training of realistic models.

</details>


### [20] [Bound by semanticity: universal laws governing the generalization-identification tradeoff](https://arxiv.org/abs/2506.14797)
*Marco Nurisso, Jesseba Fernando, Raj Deshpande, Alan Perotti, Raja Marjieh, Steven M. Frankland, Richard L. Lewis, Taylor W. Webb, Declan Campbell, Francesco Vaccarino, Jonathan D. Cohen, Giovanni Petri*

**主要类别:** cs.LG

**AI概要:** 研究揭示了智能系统在表示输入时面临的泛化与识别之间的基本权衡，并得出了一个独立于输入空间几何形状的普遍帕累托前沿公式，该公式描述了正确泛化概率和识别概率。实验表明，这一理论不仅适用于简单的ReLU网络，也适用于更复杂的卷积神经网络和最先进的视觉-语言模型。


<details>
  <summary>更多</summary>
  
**动机:** 探讨智能系统如何平衡其内部表示的结构化（以支持广泛的泛化）和选择性（以保持输入身份）。通过研究这种权衡的根本限制，来理解有限语义分辨率对深度网络和大脑表征能力的影响。

**方法:** 通过数学推导得出关于模型间表示相似度与泛化及识别概率之间关系的封闭形式表达式；分析扩展至噪声异质空间及多输入场景；使用最小ReLU网络进行端到端训练验证理论预测；并在更复杂的卷积神经网络和视觉-语言模型上检验理论的普适性。

**结果:** 发现了一个不依赖于输入空间几何特性的普遍帕累托前沿，它将泛化与识别的概率关联起来；观察到了随着输入数量增加处理能力急剧下降的现象以及非单调最优解的存在；实验证明即使是在复杂模型中，有限分辨率相似性也是一个基本的信息约束。

**结论:** 本研究表明，在不同类型的神经网络中都存在由有限分辨率相似性导致的基本信息约束，这对理解深层网络乃至人脑如何平衡泛化能力和个体识别能力具有重要意义。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bound+by+semanticity%3A+universal+laws+governing+the+generalization-identification+tradeoff，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14797，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14797&send_immediately=true&force_search=false)

**原文摘要:** Intelligent systems must deploy internal representations that are
simultaneously structured -- to support broad generalization -- and selective
-- to preserve input identity. We expose a fundamental limit on this tradeoff.
For any model whose representational similarity between inputs decays with
finite semantic resolution $\varepsilon$, we derive closed-form expressions
that pin its probability of correct generalization $p_S$ and identification
$p_I$ to a universal Pareto front independent of input space geometry.
Extending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs
predicts a sharp $1/n$ collapse of multi-input processing capacity and a
non-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end
reproduces these laws: during learning a resolution boundary self-organizes and
empirical $(p_S,p_I)$ trajectories closely follow theoretical curves for
linearly decaying similarity. Finally, we demonstrate that the same limits
persist in two markedly more complex settings -- a convolutional neural network
and state-of-the-art vision-language models -- confirming that
finite-resolution similarity is a fundamental emergent informational
constraint, not merely a toy-model artifact. Together, these results provide an
exact theory of the generalization-identification trade-off and clarify how
semantic resolution shapes the representational capacity of deep networks and
brains alike.

</details>


### [21] [ss-Mamba: Semantic-Spline Selective State-Space Model](https://arxiv.org/abs/2506.14802)
*Zuochen Ye*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的基础模型ss-Mamba，该模型通过在选择性状态空间建模框架内整合语义感知嵌入和自适应样条基时间编码来提高时间序列预测。


<details>
  <summary>更多</summary>
  
**动机:** 为了提高时间序列预测的准确性、鲁棒性和可解释性，并且降低计算复杂度，使之成为传统基于Transformer模型的一种灵活而高效的替代方案。

**方法:** ss-Mamba模型结合了Mamba选择性状态空间模型、语义索引嵌入（从预训练语言模型初始化）以及基于样条的Kolmogorov-Arnold网络（KAN），以捕捉复杂的季节性和非平稳时间效应。

**结果:** 广泛的实验评估表明，ss-Mamba提供了更高的准确率、鲁棒性和可解释性，同时将计算复杂度从二次降低到线性时间。

**结论:** ss-Mamba作为一款新颖的基础模型，在时间序列预测方面表现出色，具有较高的性能和较低的计算成本，可以作为基于Transformer架构的有效替代品。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ss-Mamba%3A+Semantic-Spline+Selective+State-Space+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14802，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14802&send_immediately=true&force_search=false)

**原文摘要:** We propose ss-Mamba, a novel foundation model that enhances time series
forecasting by integrating semantic-aware embeddings and adaptive spline-based
temporal encoding within a selective state-space modeling framework. Building
upon the recent success of Transformer architectures, ss-Mamba adopts the Mamba
selective state space model as an efficient alternative that achieves
comparable performance while significantly reducing computational complexity
from quadratic to linear time. Semantic index embeddings, initialized from
pretrained language models, allow effective generalization to previously unseen
series through meaningful semantic priors. Additionally, spline-based
Kolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex
seasonalities and non-stationary temporal effects, providing a powerful
enhancement over conventional temporal feature encodings. Extensive
experimental evaluations confirm that ss-Mamba delivers superior accuracy,
robustness, and interpretability, demonstrating its capability as a versatile
and computationally efficient alternative to traditional Transformer-based
models in time-series forecasting.

</details>


### [22] [Heavy-Ball Momentum Method in Continuous Time and Discretization Error Analysis](https://arxiv.org/abs/2506.14806)
*Bochen Lyu, Xiaojing Zhang, Fangyi Zheng, He Wang, Zheng Wang, Zhanxing Zhu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一个连续时间近似模型，即分段连续微分方程，用于离散的Heavy-Ball (HB) 动量方法，并显式地考虑了离散化误差。该模型能够控制任意阶步长的离散化误差，并被应用于发现方向平滑性的隐式正则化以及研究对角线性网络中HB动量方法的隐式偏差。


<details>
  <summary>更多</summary>
  
**动机:** 尽管动量在基于梯度的优化方法中起着关键作用，但由于离散化误差导致原始离散动力学和连续时间近似之间的差距尚未得到全面解决。为了填补这一空白，提供额外的理论工具，本研究专注于HB动量方法的连续时间近似并着重于离散化误差。

**方法:** 设计了一个一阶分段连续微分方程，并加入多个反向项以显式考虑离散化误差。通过这种方法，研究人员构建了一个可以控制任意阶步长离散化误差的HB动量方法的连续时间模型。

**结果:** 提供了HB动量方法的一个连续时间模型，允许控制任意阶步长的离散化误差。此外，还利用此模型发现了方向平滑性的新隐式正则化，并探讨了对角线性网络中HB动量方法的隐式偏差。这些理论发现得到了数值实验的支持。

**结论:** 该工作为HB动量方法建立了更精确的连续时间近似，并且这种近似有助于理解动量方法的隐式正则化效应及其在深度学习中的潜在应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Heavy-Ball+Momentum+Method+in+Continuous+Time+and+Discretization+Error+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14806，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14806&send_immediately=true&force_search=false)

**原文摘要:** This paper establishes a continuous time approximation, a piece-wise
continuous differential equation, for the discrete Heavy-Ball (HB) momentum
method with explicit discretization error. Investigating continuous
differential equations has been a promising approach for studying the discrete
optimization methods. Despite the crucial role of momentum in gradient-based
optimization methods, the gap between the original discrete dynamics and the
continuous time approximations due to the discretization error has not been
comprehensively bridged yet. In this work, we study the HB momentum method in
continuous time while putting more focus on the discretization error to provide
additional theoretical tools to this area. In particular, we design a
first-order piece-wise continuous differential equation, where we add a number
of counter terms to account for the discretization error explicitly. As a
result, we provide a continuous time model for the HB momentum method that
allows the control of discretization error to arbitrary order of the step size.
As an application, we leverage it to find a new implicit regularization of the
directional smoothness and investigate the implicit bias of HB for diagonal
linear networks, indicating how our results can be used in deep learning. Our
theoretical findings are further supported by numerical experiments.

</details>


### [23] [PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models](https://arxiv.org/abs/2506.14808)
*Jenny Schmalfuss, Nadine Chang, Vibashan VS, Maying Shen, Andres Bruhn, Jose M. Alvarez*

**主要类别:** cs.LG

**AI概要:** 本文研究了视觉语言模型(VLMs)对不同提示的敏感性，通过引入PARC框架来分析VLMs在面对提示变化时的表现。结果显示VLMs在视觉领域表现出与大型语言模型相似的提示敏感性，并且发现InternVL2家族的模型特别稳健。


<details>
  <summary>更多</summary>
  
**动机:** 鉴于视觉语言模型（VLMs）集成了已知对提示敏感的大规模语言模型（LLMs），因此确定VLMs是否继承了这种对于不同提示的不稳定性至关重要。

**方法:** 开发了名为PARC（Prompt Analysis via Reliability and Calibration）的VLM提示敏感性分析框架，它基于三个支柱：1）语言和视觉领域的合理提示变化；2）带有内置保证的新颖模型可靠性分数；3）允许跨数据集和提示范围进行提示变化分析的校准步骤。

**结果:** 研究表明VLMs在视觉领域反映了LLM的语言提示敏感性，最具破坏性的变化会改变预期的答案。在评估的22个模型中，InternVL2系列的模型表现得尤为稳健。还发现了提示敏感性与训练数据之间的联系迹象。

**结论:** 该研究提供了一个系统的方法来评估VLMs对提示变化的敏感性，并指出了某些模型如InternVL2系列的相对稳健性。此外，还揭示了提示敏感性可能与训练数据有关。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PARC%3A+A+Quantitative+Framework+Uncovering+the+Symmetries+within+Vision+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14808，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14808&send_immediately=true&force_search=false)

**原文摘要:** Vision language models (VLMs) respond to user-crafted text prompts and visual
inputs, and are applied to numerous real-world problems. VLMs integrate visual
modalities with large language models (LLMs), which are well known to be
prompt-sensitive. Hence, it is crucial to determine whether VLMs inherit this
instability to varying prompts. We therefore investigate which prompt
variations VLMs are most sensitive to and which VLMs are most agnostic to
prompt variations. To this end, we introduce PARC (Prompt Analysis via
Reliability and Calibration), a VLM prompt sensitivity analysis framework built
on three pillars: (1) plausible prompt variations in both the language and
vision domain, (2) a novel model reliability score with built-in guarantees,
and (3) a calibration step that enables dataset- and prompt-spanning prompt
variation analysis. Regarding prompt variations, PARC's evaluation shows that
VLMs mirror LLM language prompt sensitivity in the vision domain, and most
destructive variations change the expected answer. Regarding models,
outstandingly robust VLMs among 22 evaluated models come from the InternVL2
family. We further find indications that prompt sensitivity is linked to
training data. The code will be at https://github.com/NVlabs/PARC.

</details>


### [24] [Intelligent Routing for Sparse Demand Forecasting: A Comparative Evaluation of Selection Strategies](https://arxiv.org/abs/2506.14810)
*Qiwen Zhang*

**主要类别:** cs.LG

**AI概要:** 论文提出了一种Model-Router框架，能够根据每种产品独特的需求模式动态选择最合适的预测模型（包括经典、机器学习和深度学习方法），尤其适用于稀疏和间歇性需求的预测。通过使用InceptionTime作为路由器，该方法在Favorita大规模数据集上的实验表明，相较于单一模型基准，其预测准确率提高了11.8%（NWRMSLE指标），同时推理速度提升了4.67倍。


<details>
  <summary>更多</summary>
  
**动机:** 供应链中稀疏和间歇性的需求预测是一个关键挑战，因为频繁出现的零需求期会妨碍传统模型的准确性，并影响库存管理。

**方法:** 研究提出了一个Model-Router框架，该框架可以基于每个产品的独特需求模式从经典方法、机器学习(ML)以及深度学习(DL)方法中动态地选择最适合的预测模型。比较了基于规则、LightGBM和InceptionTime三种不同的路由器方式，以学习如何为不同需求类型（平滑型、块状型或间歇型）分配适当的预测策略。

**结果:** 实验结果表明，在Favorita的大规模数据集上，采用深度学习(Inception Time)作为路由器的方法相比强大的单模态基准，能够提高高达11.8% (NWRMSLE) 的预测精度，并且推理时间快了4.67倍。

**结论:** 通过智能适应性AI优化现代供应链运营，所提出的Model-Router框架显著提高了预测精度，从而有望大幅减少缺货情况与过剩库存造成的浪费。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Intelligent+Routing+for+Sparse+Demand+Forecasting%3A+A+Comparative+Evaluation+of+Selection+Strategies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14810，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14810&send_immediately=true&force_search=false)

**原文摘要:** Sparse and intermittent demand forecasting in supply chains presents a
critical challenge, as frequent zero-demand periods hinder traditional model
accuracy and impact inventory management. We propose and evaluate a
Model-Router framework that dynamically selects the most suitable forecasting
model-spanning classical, ML, and DL methods for each product based on its
unique demand pattern. By comparing rule-based, LightGBM, and InceptionTime
routers, our approach learns to assign appropriate forecasting strategies,
effectively differentiating between smooth, lumpy, or intermittent demand
regimes to optimize predictions. Experiments on the large-scale Favorita
dataset show our deep learning (Inception Time) router improves forecasting
accuracy by up to 11.8% (NWRMSLE) over strong, single-model benchmarks with
4.67x faster inference time. Ultimately, these gains in forecasting precision
will drive substantial reductions in both stockouts and wasteful excess
inventory, underscoring the critical role of intelligent, adaptive Al in
optimizing contemporary supply chain operations.

</details>


### [25] [Self-Composing Policies for Scalable Continual Reinforcement Learning](https://arxiv.org/abs/2506.14811)
*Mikel Malagón, Josu Ceberio, Jose A. Lozano*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种可增长的模块化神经网络架构，能够自然地避免连续强化学习中的灾难性遗忘和干扰问题，并且在标准连续控制和视觉问题实验中表现出比其他方法更好的知识迁移和性能。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决连续强化学习过程中出现的灾难性遗忘和干扰问题，同时保持模型的可塑性和扩展性。

**方法:** 引入了一种新的、可增长的模块化神经网络结构，每个模块允许选择性地结合先前策略及其内部策略，从而加速当前任务的学习过程。此外，与以往增长型神经网络方法不同的是，本研究提出的方案参数数量随任务数量线性增长，不会因规模扩大而牺牲灵活性。

**结果:** 在基准连续控制和视觉问题上的实验证明，该方法相较于其他替代方法实现了更高效的知识转移以及更高的性能表现。

**结论:** 所提方法通过一种新颖的可增长模块化设计，在连续强化学习中有效克服了灾难性遗忘的问题，并且随着任务数量的增加，它仍能维持良好的适应性和性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Composing+Policies+for+Scalable+Continual+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14811，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14811&send_immediately=true&force_search=false)

**原文摘要:** This work introduces a growable and modular neural network architecture that
naturally avoids catastrophic forgetting and interference in continual
reinforcement learning. The structure of each module allows the selective
combination of previous policies along with its internal policy, accelerating
the learning process on the current task. Unlike previous growing neural
network approaches, we show that the number of parameters of the proposed
approach grows linearly with respect to the number of tasks, and does not
sacrifice plasticity to scale. Experiments conducted in benchmark continuous
control and visual problems reveal that the proposed approach achieves greater
knowledge transfer and performance than alternative methods.

</details>


### [26] [Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks](https://arxiv.org/abs/2506.14813)
*Yuxuan Jiang, Ziming Zhou, Boyu Xu, Beijie Liu, Runhui Xu, Peng Huang*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为TRAINCHECK的框架，该框架能够主动检测深度学习模型训练过程中的隐性错误，并为调试提供帮助。


<details>
  <summary>更多</summary>
  
**动机:** 深度学习模型的训练过程复杂，容易出现难以察觉和诊断的隐性错误。

**方法:** TRAINCHECK自动推断出针对深度学习训练定制的不变量，并使用这些不变量来主动检测训练过程中的隐性错误。

**结果:** 在重现20个具有不同根本原因的真实世界隐性训练错误时，TRAINCHECK能够在单次训练迭代中成功检测到18个错误，并且还发现了流行训练库中导致隐性错误的6个未知漏洞。

**结论:** TRAINCHECK 为深度学习训练过程中的隐性错误提供了一种有效的主动检查方法，并且在实际应用中证明了其有效性和实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Training+with+Confidence%3A+Catching+Silent+Errors+in+Deep+Learning+Training+with+Automated+Proactive+Checks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14813，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14813&send_immediately=true&force_search=false)

**原文摘要:** Training deep learning (DL) models is a complex process, making it prone to
silent errors that are challenging to detect and diagnose. This paper presents
TRAINCHECK, a framework that takes a proactive checking approach to address
silent training errors. TRAINCHECK automatically infers invariants tailored for
DL training. It uses these invariants to proactively detect silent errors
during the training process while providing debugging help. To evaluate
TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root
causes. TRAINCHECK successfully detects 18 errors within a single training
iteration. It also uncovers 6 unknown bugs in popular training libraries that
lead to silent errors.

</details>


### [27] [Predicting Anthropometric Body Composition Variables Using 3D Optical Imaging and Machine Learning](https://arxiv.org/abs/2506.14815)
*Gyaneshwar Agrahari, Kiran Bist, Monika Pandey, Jacob Kapita, Zachary James, Jackson Knox, Steven Heymsfield, Sophia Ramirez, Peter Wolenski, Nadejda Drenska*

**主要类别:** cs.LG

**AI概要:** 该研究提出了一种基于3D光学图像生物标志物的统计和机器学习模型，作为双能X射线吸收测定法（DXA）扫描的一种替代方法，用于预测人体成分变量。使用了半监督的p-Laplacian回归模型，在数据有限的情况下也能够取得较好的预测结果。


<details>
  <summary>更多</summary>
  
**动机:** 由于DXA扫描成本高且耗时，研究人员寻求一种更经济、快捷的方法来准确预测人体组成变量如附肢瘦体重（ALM）、体脂百分比（BFP）和骨矿物质密度（BMD），这些是早期诊断多种慢性疾病的关键。

**方法:** 研究者采用了从3D光学图像中获得的生物标志物（身高、体积、左小腿周长等）并应用了统计与机器学习模型，包括半监督学习的p-Laplacian回归模型以及监督学习算法如支持向量回归（SVR）和最小二乘支持向量回归。

**结果:** p-Laplacian模型在训练数据占总数据10%的情况下，对于ALM、BMD和BFP分别达到了约13%、10%和20%的误差率；而SVR对ALM和BMD提供了最佳性能，误差约为8%，当使用80%的数据进行训练时，最小二乘SVR对BFP表现最好，误差率为11%左右。

**结论:** 研究表明，p-Laplacian模型在数据受限的情况下具有成为医疗应用中有前景工具的潜力，特别是在预测人体组成变量方面。此外，对于拥有较多标注数据的情况，某些监督学习算法如SVR表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+Anthropometric+Body+Composition+Variables+Using+3D+Optical+Imaging+and+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14815，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14815&send_immediately=true&force_search=false)

**原文摘要:** Accurate prediction of anthropometric body composition variables, such as
Appendicular Lean Mass (ALM), Body Fat Percentage (BFP), and Bone Mineral
Density (BMD), is essential for early diagnosis of several chronic diseases.
Currently, researchers rely on Dual-Energy X-ray Absorptiometry (DXA) scans to
measure these metrics; however, DXA scans are costly and time-consuming. This
work proposes an alternative to DXA scans by applying statistical and machine
learning models on biomarkers (height, volume, left calf circumference, etc)
obtained from 3D optical images. The dataset consists of 847 patients and was
sourced from Pennington Biomedical Research Center. Extracting patients' data
in healthcare faces many technical challenges and legal restrictions. However,
most supervised machine learning algorithms are inherently data-intensive,
requiring a large amount of training data. To overcome these limitations, we
implemented a semi-supervised model, the $p$-Laplacian regression model. This
paper is the first to demonstrate the application of a $p$-Laplacian model for
regression. Our $p$-Laplacian model yielded errors of $\sim13\%$ for ALM,
$\sim10\%$ for BMD, and $\sim20\%$ for BFP when the training data accounted for
10 percent of all data. Among the supervised algorithms we implemented, Support
Vector Regression (SVR) performed the best for ALM and BMD, yielding errors of
$\sim 8\%$ for both, while Least Squares SVR performed the best for BFP with
$\sim 11\%$ error when trained on 80 percent of the data. Our findings position
the $p$-Laplacian model as a promising tool for healthcare applications,
particularly in a data-constrained environment.

</details>


### [28] [Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints](https://arxiv.org/abs/2506.14821)
*Sunil Kumar, Bowen Zhao, Leo Dirac, Paulina Varshavskaya*

**主要类别:** cs.LG

**AI概要:** 本文通过借鉴Deepseek-r1等方法，利用Group Relative Policy Optimization（GRPO）训练小规模的视觉-语言模型（VLMs），使其能够使用外部工具如放大功能，并通过一系列优化措施，提升了在视觉问题回答任务上的表现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型模型的推理能力近期有了巨大的进步，但视觉-语言模型在处理细节视觉推理时依然存在困难，尤其是在计算资源有限的情况下。

**方法:** 研究者受到类似Deepseek-r1的方法启发，对较小规模的模型采用群组相对策略优化（GRPO）进行训练，以使它们能够使用像缩放这样的外部工具。为了实现最佳效果，采用了GRPO学习、简单的奖励结构、简化的工具调用界面、为工具调用结果分配额外标记以及过代表难以视觉识别的例子的数据混合等综合手段。

**结果:** 与相同规模的基础模型相比，该方法在某些视觉问答（VQA）任务上取得了更好的表现，这得益于从外部工具收集到的详细视觉信息。

**结论:** 通过结合GRPO学习和对外部工具的有效利用，即使是在计算资源受限的情况下，也能够提高视觉-语言模型在复杂视觉推理任务中的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcing+VLMs+to+Use+Tools+for+Detailed+Visual+Reasoning+Under+Resource+Constraints，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14821，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14821&send_immediately=true&force_search=false)

**原文摘要:** Despite tremendous recent advances in large model reasoning ability,
vision-language models (VLMs) still struggle with detailed visual reasoning,
especially when compute resources are limited. To address this challenge, we
draw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale
models with Group Relative Policy Optimization (GRPO) to use external tools
such as zoom. The greatest benefit is obtained with a combination of GRPO
learning, a simple reward structure, a simplified tool-calling interface,
allocating additional tokens to the result of the tool call, and a training
data mix that over-represents visually difficult examples. Compared to
similarly-sized baseline models, our method achieves better performance on some
visual question-answering (VQA) tasks, thanks to the detailed visual
information gathered from the external tool.

</details>


### [29] [FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models](https://arxiv.org/abs/2506.14824)
*Yao Zhang, Hewei Gao, Haokun Chen, Weiguo Li, Yunpu Ma, Volker Tresp*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为FedNano的联邦学习框架，它通过在服务器上集中大型语言模型（LLM）并在客户端引入轻量级模块NanoEdge来解决多模态大语言模型在实际部署中的挑战。这种方法极大地减少了客户端存储需求和通信开销，并且能够处理异构的客户端数据同时保护隐私。实验表明FedNano优于先前的联邦学习基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 多模态大语言模型（MLLMs）虽然在多模态推理和跨模态检索等任务中表现出色，但在现实场景部署时面临分布式多模态数据和严格隐私要求带来的挑战。现有的联邦学习方法假设客户端能够部署完整模型，但这种假设对于大规模MLLMs来说并不适用，因为它们体积庞大并且对通信有高要求。

**方法:** 提出了FedNano，一种新的联邦学习框架，它将LLM集中在服务器端，并为每个客户端引入了轻量级适应模块NanoEdge。NanoEdge采用了特定模态编码器、连接器以及可训练的低秩适应NanoAdapter。这样设计消除了在客户端部署LLM的需求，大幅降低了客户端的存储需求和通信成本。

**结果:** FedNano相比之前的联邦学习基线方法表现更好，它成功地缩小了MLLM规模与联邦学习可行性之间的差距，使得构建可扩展的、去中心化的多模态AI系统成为可能。

**结论:** FedNano通过创新的设计有效解决了多模态大语言模型在联邦学习环境下的部署难题，不仅减少了客户端资源占用还提高了整体性能，在保障隐私的同时促进了多模态AI系统的实用性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedNano%3A+Toward+Lightweight+Federated+Tuning+for+Pretrained+Multimodal+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14824，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14824&send_immediately=true&force_search=false)

**原文摘要:** Multimodal Large Language Models (MLLMs) excel in tasks like multimodal
reasoning and cross-modal retrieval but face deployment challenges in
real-world scenarios due to distributed multimodal data and strict privacy
requirements. Federated Learning (FL) offers a solution by enabling
collaborative model training without centralizing data. However, realizing FL
for MLLMs presents significant challenges, including high computational
demands, limited client capacity, substantial communication costs, and
heterogeneous client data. Existing FL methods assume client-side deployment of
full models, an assumption that breaks down for large-scale MLLMs due to their
massive size and communication demands. To address these limitations, we
propose FedNano, the first FL framework that centralizes the LLM on the server
while introducing NanoEdge, a lightweight module for client-specific
adaptation. NanoEdge employs modality-specific encoders, connectors, and
trainable NanoAdapters with low-rank adaptation. This design eliminates the
need to deploy LLM on clients, reducing client-side storage by 95%, and
limiting communication overhead to only 0.01% of the model parameters. By
transmitting only compact NanoAdapter updates, FedNano handles heterogeneous
client data and resource constraints while preserving privacy. Experiments
demonstrate that FedNano outperforms prior FL baselines, bridging the gap
between MLLM scale and FL feasibility, and enabling scalable, decentralized
multimodal AI systems.

</details>


### [30] [Accurate and Uncertainty-Aware Multi-Task Prediction of HEA Properties Using Prior-Guided Deep Gaussian Processes](https://arxiv.org/abs/2506.14828)
*Sk Md Ahnaf Akif Alvi, Mrinalini Mulukutla, Nicolas Flores, Danial Khatamsaz, Jan Janssen, Danny Perez, Douglas Allaire, Vahid Attari, Raymundo Arroyave*

**主要类别:** cs.LG

**AI概要:** 本文系统地评估了四种主要的代理模型（常规高斯过程、深度高斯过程、用于多输出回归的编码器-解码器神经网络和XGBoost）在AlCoCrCuFeMnNiV高熵合金系统中的拟合性能，发现深度高斯过程在处理材料信息学中常见的异方差性、异位性和不完整数据方面表现出色，并且通过有效地捕捉属性间相关性和输入依赖的不确定性来提高预测精度。


<details>
  <summary>更多</summary>
  
**动机:** 加速高熵合金的发现与优化，特别是在将计算预测与稀疏实验观察相结合时，替代建模技术变得不可或缺。研究旨在评估不同代理模型对关联材料属性的预测能力，以便为材料设计提供强有力且数据高效的工具。

**方法:** 采用常规高斯过程(cGP)、深度高斯过程(DGP)、适用于多输出回归的编码器-解码器神经网络以及XGBoost这四种代理模型，应用于结合了实验和计算属性的混合数据集上，以评估它们在预测如屈服强度、硬度、模量、极限抗拉强度、延伸率及动态和准静态条件下的平均硬度等相互关联材料属性的能力。

**结果:** 结果显示，在处理材料信息学领域常见的异方差、异位及不完整数据问题上，层次化和深度建模方法表现出了优势。特别是基于机器学习先验的深度高斯过程在捕捉属性间相关性和输入依赖型不确定性方面优于其他代理模型，从而提高了预测准确性。

**结论:** 先进的代理模型，尤其是融入了机器学习基础先验知识的深度高斯过程，在处理复杂材料属性预测任务中展现出了显著的优势。这些模型能够有效应对材料信息学中普遍存在的挑战，成为进行稳健而高效材料设计的强大工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Accurate+and+Uncertainty-Aware+Multi-Task+Prediction+of+HEA+Properties+Using+Prior-Guided+Deep+Gaussian+Processes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14828，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14828&send_immediately=true&force_search=false)

**原文摘要:** Surrogate modeling techniques have become indispensable in accelerating the
discovery and optimization of high-entropy alloys(HEAs), especially when
integrating computational predictions with sparse experimental observations.
This study systematically evaluates the fitting performance of four prominent
surrogate models conventional Gaussian Processes(cGP), Deep Gaussian
Processes(DGP), encoder-decoder neural networks for multi-output regression and
XGBoost applied to a hybrid dataset of experimental and computational
properties in the AlCoCrCuFeMnNiV HEA system. We specifically assess their
capabilities in predicting correlated material properties, including yield
strength, hardness, modulus, ultimate tensile strength, elongation, and average
hardness under dynamic and quasi-static conditions, alongside auxiliary
computational properties. The comparison highlights the strengths of
hierarchical and deep modeling approaches in handling heteroscedastic,
heterotopic, and incomplete data commonly encountered in materials informatics.
Our findings illustrate that DGP infused with machine learning-based prior
outperform other surrogates by effectively capturing inter-property
correlations and input-dependent uncertainty. This enhanced predictive accuracy
positions advanced surrogate models as powerful tools for robust and
data-efficient materials design.

</details>


### [31] [Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model](https://arxiv.org/abs/2506.14830)
*Zhizhao Wen, Ruoxin Zhang, Chao Wang*

**主要类别:** cs.LG

**AI概要:** 本研究提出了一种结合多头注意力机制的混合BiGRU-MHA模型，以提高存储设备健康分类的准确性和稳定性。实验结果表明该模型在训练集和测试集上都达到了92%以上的分类准确率，并且具有极小的性能差距，展示了出色的泛化能力。此外，该模型有助于减少数据丢失风险并优化维护成本。


<details>
  <summary>更多</summary>
  
**动机:** 固态硬盘（SSD）健康状态预测对于数据可靠性保证具有关键作用。传统模型在泛化性能方面存在瓶颈，需要一种新的技术方法来解决这一问题。

**方法:** 研究者提出了一个混合BiGRU-MHA模型，该模型结合了双向门控循环单元（BiGRU）网络的时间特征提取能力和多头注意力（MHA）机制的关键信息聚焦能力。BiGRU能够捕捉SSD退化特征中的前后依赖关系，而MHA则动态地分配特征权重，增强了模型对重要健康指标的敏感性。

**结果:** 所提出的模型在训练集上达到了92.70%的分类准确率，在测试集上达到了92.44%的分类准确率，性能差距仅为0.26%，显示了很好的泛化能力。ROC曲线分析显示测试集上的AUC为0.94，证实了模型稳健的二分类性能。

**结论:** 这项工作不仅为SSD健康预测提供了一种新技术方法，而且解决了传统模型的泛化瓶颈问题，为工业级存储系统的预防性维护提供了实用价值。它显著降低了数据丢失的风险，并通过提前发出故障警告帮助优化了维护成本，支持了云计算数据中心和边缘存储环境下的可靠存储系统建设的智能决策。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimization+of+bi-directional+gated+loop+cell+based+on+multi-head+attention+mechanism+for+SSD+health+state+classification+model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14830，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14830&send_immediately=true&force_search=false)

**原文摘要:** Aiming at the critical role of SSD health state prediction in data
reliability assurance, this study proposes a hybrid BiGRU-MHA model that
incorporates a multi-head attention mechanism to enhance the accuracy and
stability of storage device health classification. The model innovatively
integrates temporal feature extraction and key information focusing
capabilities. Specifically, it leverages the bidirectional timing modeling
advantages of the BiGRU network to capture both forward and backward
dependencies of SSD degradation features. Simultaneously, the multi-head
attention mechanism dynamically assigns feature weights, improving the model's
sensitivity to critical health indicators. Experimental results show that the
proposed model achieves classification accuracies of 92.70% on the training set
and 92.44% on the test set, with a minimal performance gap of only 0.26%,
demonstrating excellent generalization ability. Further analysis using the
receiver operating characteristic (ROC) curve shows an area under the curve
(AUC) of 0.94 on the test set, confirming the model's robust binary
classification performance. This work not only presents a new technical
approach for SSD health prediction but also addresses the generalization
bottleneck of traditional models, offering a verifiable method with practical
value for preventive maintenance of industrial-grade storage systems. The
results show the model can significantly reduce data loss risks by providing
early failure warnings and help optimize maintenance costs, supporting
intelligent decision-making in building reliable storage systems for cloud
computing data centers and edge storage environments.

</details>


### [32] [CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration](https://arxiv.org/abs/2506.14843)
*Luca Gherardini, Imre Lengyel, Tunde Peto, Caroline C. W. Klaverd, Magda A. Meester-Smoord, Johanna Maria Colijnd, EYE-RISK Consortium, E3 Consortium, Jose Sousa*

**主要类别:** cs.LG

**AI概要:** 本文介绍了CACTUS，一种用于提高年龄相关性黄斑变性(AMD)阶段分类的综合抽象和分类工具。该工具提供了解释性和灵活性，优于标准机器学习模型，并通过识别关键因素来增强决策制定。


<details>
  <summary>更多</summary>
  
**动机:** 由于医疗数据通常有限或不完整，这会阻碍模型性能。在AMD等疾病中，早期诊断至关重要，因为缺乏有效的治疗方法来逆转病情进展。因此需要一种考虑遗传、饮食、临床和人口统计因素的分类方法。

**方法:** 研究者们提出了一个名为CACTUS（Comprehensive Abstraction and Classification Tool for Uncovering Structures）的工具，旨在改进AMD阶段分类。CACTUS提供了可解释性和灵活性，并且表现优于传统的机器学习模型。

**结果:** CACTUS能够识别重要的特征，这些特征可以与现有的医学知识进行比较。通过去除不太相关或有偏见的数据，创建了一个让临床医生提供反馈并解决偏见的临床场景。

**结论:** CACTUS工具不仅提高了AMD阶段分类的准确性，还增强了决策过程中的信心，并且允许与现有医学知识对比重要特征。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CACTUS+as+a+Reliable+Tool+for+Early+Classification+of+Age-related+Macular+Degeneration，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14843，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14843&send_immediately=true&force_search=false)

**原文摘要:** Machine Learning (ML) is used to tackle various tasks, such as disease
classification and prediction. The effectiveness of ML models relies heavily on
having large amounts of complete data. However, healthcare data is often
limited or incomplete, which can hinder model performance. Additionally, issues
like the trustworthiness of solutions vary with the datasets used. The lack of
transparency in some ML models further complicates their understanding and use.
In healthcare, particularly in the case of Age-related Macular Degeneration
(AMD), which affects millions of older adults, early diagnosis is crucial due
to the absence of effective treatments for reversing progression. Diagnosing
AMD involves assessing retinal images along with patients' symptom reports.
There is a need for classification approaches that consider genetic, dietary,
clinical, and demographic factors. Recently, we introduced the -Comprehensive
Abstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed
at improving AMD stage classification. CACTUS offers explainability and
flexibility, outperforming standard ML models. It enhances decision-making by
identifying key factors and providing confidence in its results. The important
features identified by CACTUS allow us to compare with existing medical
knowledge. By eliminating less relevant or biased data, we created a clinical
scenario for clinicians to offer feedback and address biases.

</details>


### [33] [Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders](https://arxiv.org/abs/2506.14937)
*Luan Gonçalves Miranda, Pedro Ivo da Cruz, Murilo Bellezoni Loiola*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种利用机器学习算法自动定义自编码器在异常检测系统中使用的分离阈值的方法，以提高检测过程的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数字安全机制，如使用自编码器(Anomaly Detection Systems using Autoencoders, AE)的异常检测系统，虽然能够很好地解决数据不平衡等内在问题，但由于AE采用非标准且复杂的分离阈值来对重构误差进行分类，该阈值的定义直接影响了检测过程的表现。

**方法:** 为了改善这一状况，研究者们提议通过几种机器学习算法来自动生成这个阈值，并对K-近邻(K-Nearest Neighbors)、K-均值(K-Means)和支持向量机(Support Vector Machine)这三种算法进行了评估。

**结果:** 研究结果表明，利用机器学习算法自动定义自编码器的分离阈值可以有效地影响和潜在地提升异常检测系统的性能。

**结论:** 本文提出的基于机器学习的自动阈值定义方法为提高基于自编码器的异常检测系统性能提供了新的思路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Determina%C3%A7%C3%A3o+Autom%C3%A1tica+de+Limiar+de+Detec%C3%A7%C3%A3o+de+Ataques+em+Redes+de+Computadores+Utilizando+Autoencoders，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14937，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14937&send_immediately=true&force_search=false)

**原文摘要:** Currently, digital security mechanisms like Anomaly Detection Systems using
Autoencoders (AE) show great potential for bypassing problems intrinsic to the
data, such as data imbalance. Because AE use a non-trivial and nonstandardized
separation threshold to classify the extracted reconstruction error, the
definition of this threshold directly impacts the performance of the detection
process. Thus, this work proposes the automatic definition of this threshold
using some machine learning algorithms. For this, three algorithms were
evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.

</details>


### [34] [Generalized Reference Kernel With Negative Samples For Support Vector One-class Classification](https://arxiv.org/abs/2506.14895)
*Jenni Raitoharju*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种利用负样本改进的广义参考核方法（GRKneg），用于一类支持向量机。该方法在少量负样本的情况下优于标准的一类SVM和二分类SVM。


<details>
  <summary>更多</summary>
  
**动机:** 研究小规模单类分类问题，在有一定数量负样本可用的情况下，如何改善现有的OC-SVM模型。

**方法:** 提出了广义参考核与负样本（GRKneg）的方法，并探讨了选择/生成参考向量的不同方式；推荐了一种针对手头问题的方法。

**结果:** 所提出的方法在使用不同数量的负样本时，始终优于使用径向基函数核的标准OC-SVM。当负样本很少时，该方法也明显优于二分类SVM。

**结论:** 通过使用负样本来改进核函数，提出的GRKneg方法可以在负样本较少的小规模单类分类问题中提供更好的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Reference+Kernel+With+Negative+Samples+For+Support+Vector+One-class+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14895，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14895&send_immediately=true&force_search=false)

**原文摘要:** This paper focuses on small-scale one-class classification with some negative
samples available. We propose Generalized Reference Kernel with Negative
Samples (GRKneg) for One-class Support Vector Machine (OC-SVM). We study
different ways to select/generate the reference vectors and recommend an
approach for the problem at hand. It is worth noting that the proposed method
does not use any labels in the model optimization but uses the original OC-SVM
implementation. Only the kernel used in the process is improved using the
negative data. We compare our method with the standard OC-SVM and with the
binary Support Vector Machine (SVM) using different amounts of negative
samples. Our approach consistently outperforms the standard OC-SVM using Radial
Basis Function kernel. When there are plenty of negative samples, the binary
SVM outperforms the one-class approaches as expected, but we show that for the
lowest numbers of negative samples the proposed approach clearly outperforms
the binary SVM.

</details>


### [35] [Flat Channels to Infinity in Neural Loss Landscapes](https://arxiv.org/abs/2506.14951)
*Flavio Martinelli, Alexander Van Meegen, Berfin Şimşek, Wulfram Gerstner, Johanni Brea*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Flat+Channels+to+Infinity+in+Neural+Loss+Landscapes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14951，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14951&send_immediately=true&force_search=false)

**原文摘要:** The loss landscapes of neural networks contain minima and saddle points that
may be connected in flat regions or appear in isolation. We identify and
characterize a special structure in the loss landscape: channels along which
the loss decreases extremely slowly, while the output weights of at least two
neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight
vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At
convergence, the two neurons implement a gated linear unit:
$a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot
\mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot
\mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these
channels to infinity are asymptotically parallel to symmetry-induced lines of
critical points. Gradient flow solvers, and related optimization methods like
SGD or ADAM, reach the channels with high probability in diverse regression
settings, but without careful inspection they look like flat local minima with
finite parameter values. Our characterization provides a comprehensive picture
of these quasi-flat regions in terms of gradient dynamics, geometry, and
functional interpretation. The emergence of gated linear units at the end of
the channels highlights a surprising aspect of the computational capabilities
of fully connected layers.

</details>


### [36] [Event-Driven Online Vertical Federated Learning](https://arxiv.org/abs/2506.14911)
*Ganyu Wang, Boyu Wang, Bin Gu, Charles Ling*

**主要类别:** cs.LG

**AI概要:** 本文首次识别了在线垂直联邦学习中的挑战，并提出了一种事件驱动的在线VFL框架，该框架使用动态局部遗憾(DLR)来解决非凸模型在非平稳环境下的在线学习问题。实验表明，所提出的框架在处理非平稳数据时比现有的在线VFL框架更稳定，并且显著减少了通信和计算成本。


<details>
  <summary>更多</summary>
  
**动机:** 在线垂直联邦学习（VFL）比离线学习更能适应现实世界场景，但将在线学习集成到VFL中存在挑战，因为VFL的独特性质在于客户端对同一样本持有不相交的特征集。此外，在实际情况下，客户可能不会同步接收到同一实体的不相交特征的数据流。

**方法:** 作者提出了一个事件驱动的在线VFL框架，其中只有部分客户在每个事件期间被激活，而其余客户则被动参与学习过程。此外，他们还引入了动态局部后悔（DLR）的概念，以应对非凸模型在非平稳环境中所带来的在线学习问题。

**结果:** 通过综合的后悔分析，特别是针对非凸条件下的DLR进行了检查。广泛的实验表明，所提框架在非平稳数据条件下比现有在线VFL框架更稳定，同时显著降低了通信和计算成本。

**结论:** 本文介绍了一种新的在线VFL框架，它能够更好地处理非平稳数据，并且相比现有方法具有更好的稳定性和更低的资源消耗。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Event-Driven+Online+Vertical+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14911，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14911&send_immediately=true&force_search=false)

**原文摘要:** Online learning is more adaptable to real-world scenarios in Vertical
Federated Learning (VFL) compared to offline learning. However, integrating
online learning into VFL presents challenges due to the unique nature of VFL,
where clients possess non-intersecting feature sets for the same sample. In
real-world scenarios, the clients may not receive data streaming for the
disjoint features for the same entity synchronously. Instead, the data are
typically generated by an \emph{event} relevant to only a subset of clients. We
are the first to identify these challenges in online VFL, which have been
overlooked by previous research. To address these challenges, we proposed an
event-driven online VFL framework. In this framework, only a subset of clients
were activated during each event, while the remaining clients passively
collaborated in the learning process. Furthermore, we incorporated
\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by
online learning problems with non-convex models within a non-stationary
environment. We conducted a comprehensive regret analysis of our proposed
framework, specifically examining the DLR under non-convex conditions with
event-driven online VFL. Extensive experiments demonstrated that our proposed
framework was more stable than the existing online VFL framework under
non-stationary data conditions while also significantly reducing communication
and computation costs.

</details>


### [37] [Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective](https://arxiv.org/abs/2506.14965)
*Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, Zhiting Hu*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一个名为Guru的RL推理语料库，它覆盖了六个推理领域，并基于此重新审视了RL在大型语言模型推理中的应用。研究结果表明，对于预训练中常见的领域，跨域RL训练能够带来好处；而对于预训练数据中较少见的领域，则需要在领域内进行训练才能取得有意义的表现提升。此外，还发布了两个模型Guru-7B和Guru-32B，在公开可用数据上经过RL训练后达到了最先进性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的强化学习（RL）改进大型语言模型（LLM）推理的努力主要集中在数学和代码上，限制了我们对其在一般推理中更广泛应用潜力的理解。缺乏可靠且可扩展的RL奖励信号是涵盖多样化推理领域的关键挑战。

**方法:** 创建了一个名为Guru的RL推理语料库，包含92K个可验证的例子，跨越六个推理领域：数学、代码、科学、逻辑、模拟和表格。每个领域都通过特定于领域的奖励设计、去重和过滤来构建，以确保对RL训练的可靠性和有效性。使用该语料库系统地重新评估了RL在LLM推理中的既定发现，并观察到不同领域间存在显著差异。

**结果:** 研究揭示了一种更为复杂的模式：在预训练过程中频繁出现的领域（如数学、代码、科学）能轻易从跨域RL训练中获益，而那些在预训练期间接触有限的领域（例如逻辑、模拟和表格）则需要在其所在领域内进行专门训练才能实现有意义的表现增长。这表明RL可能有助于真正技能的获取。另外，提出了两种模型Guru-7B和Guru-32B，它们在公开模型中使用公开可用的数据进行RL训练后表现最佳，比最强基线分别高出7.9%和6.7%。这些模型也有效提高了其基础模型在复杂任务上的Pass@k性能。

**结论:** 本研究表明，针对多样化的推理领域，采用恰当设计的RL奖励信号可以提高大型语言模型的能力。特别是对于那些在预训练阶段暴露不足的领域，针对性的RL训练显得尤为重要。提出的Guru语料库以及Guru-7B和Guru-32B模型不仅展示了RL在促进真实技能获得方面的作用，而且为开放模型提供了新的基准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Reinforcement+Learning+for+LLM+Reasoning+from+A+Cross-Domain+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14965&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning (RL) has emerged as a promising approach to improve
large language model (LLM) reasoning, yet most open efforts focus narrowly on
math and code, limiting our understanding of its broader applicability to
general reasoning. A key challenge lies in the lack of reliable, scalable RL
reward signals across diverse reasoning domains. We introduce Guru, a curated
RL reasoning corpus of 92K verifiable examples spanning six reasoning
domains--Math, Code, Science, Logic, Simulation, and Tabular--each built
through domain-specific reward design, deduplication, and filtering to ensure
reliability and effectiveness for RL training. Based on Guru, we systematically
revisit established findings in RL for LLM reasoning and observe significant
variation across domains. For example, while prior work suggests that RL
primarily elicits existing knowledge from pretrained models, our results reveal
a more nuanced pattern: domains frequently seen during pretraining (Math, Code,
Science) easily benefit from cross-domain RL training, while domains with
limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain
training to achieve meaningful performance gains, suggesting that RL is likely
to facilitate genuine skill acquisition. Finally, we present Guru-7B and
Guru-32B, two models that achieve state-of-the-art performance among open
models RL-trained with publicly available data, outperforming best baselines by
7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We
also show that our models effectively improve the Pass@k performance of their
base models, particularly on complex tasks less likely to appear in pretraining
data. We release data, models, training and evaluation code to facilitate
general-purpose reasoning at: https://github.com/LLM360/Reasoning360

</details>


### [38] [FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning](https://arxiv.org/abs/2506.14929)
*Ganyu Wang, Jinjie Fang, Maxwell J. Ying, Bin Gu, Xi Chen, Boyu Wang, Charles Ling*

**主要类别:** cs.LG

**AI概要:** 本文提出了FedOne框架，一种联邦黑盒离散提示学习方法，旨在与基于云的大型语言模型交互时最大化查询效率。通过理论分析和数值实验，证明了该方法在查询效率方面的显著改进。


<details>
  <summary>更多</summary>
  
**动机:** 先前关于联邦黑盒提示调整的研究忽略了与基于云的大型语言模型服务相关的大量查询成本问题。为了解决这一差距，需要进行理论分析以提高查询效率。

**方法:** 作者提出了一种称为FedOne的策略，该策略通过每轮仅激活一个客户端来优化联邦黑盒提示学习中的查询效率，并在此基础上构建了FedOne框架。

**结果:** 通过数值实验验证了FedOne框架在不同方面均有显著的查询效率提升，这与理论结果一致。

**结论:** FedOne框架提供了一种有效的方法来解决联邦黑盒提示学习中的高查询成本问题，并且在实践中被证明能够显著提高查询效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedOne%3A+Query-Efficient+Federated+Learning+for+Black-box+Discrete+Prompt+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14929，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14929&send_immediately=true&force_search=false)

**原文摘要:** Black-Box Discrete Prompt Learning is a prompt-tuning method that optimizes
discrete prompts without accessing model parameters or gradients, making the
prompt tuning on a cloud-based Large Language Model (LLM) feasible. Adapting
federated learning to BDPL could further enhance prompt tuning performance by
leveraging data from diverse sources. However, all previous research on
federated black-box prompt tuning had neglected the substantial query cost
associated with the cloud-based LLM service. To address this gap, we conducted
a theoretical analysis of query efficiency within the context of federated
black-box prompt tuning. Our findings revealed that degrading FedAvg to
activate only one client per round, a strategy we called \textit{FedOne},
enabled optimal query efficiency in federated black-box prompt learning.
Building on this insight, we proposed the FedOne framework, a federated
black-box discrete prompt learning method designed to maximize query efficiency
when interacting with cloud-based LLMs. We conducted numerical experiments on
various aspects of our framework, demonstrating a significant improvement in
query efficiency, which aligns with our theoretical results.

</details>


### [39] [Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits](https://arxiv.org/abs/2506.14988)
*Tianyi Xu, Jiaxin Liu, Zizhan Zheng*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种多智能体多臂老虎机框架，旨在确保各智能体结果公平的同时最大化系统整体性能。为此引入了一种新颖的探测框架，并为离线和在线场景分别设计了算法。实验表明该方法在公平性和效率上优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于解决多智能体环境下，在对臂奖励信息有限的情况下进行决策的问题，同时保证公平性与系统整体性能。

**方法:** 提出了一个新式的探测框架来策略性地收集选定手臂的信息；利用子模性质设计了一个贪婪探测算法（适用于离线设置）；开发了一种在线情况下实现次线性遗憾并保持公平性的算法。

**结果:** 通过合成数据集和真实世界数据集上的广泛实验验证了所提方法相较于基线方法能够取得更好的公平性和效率表现。

**结论:** 所提出的多智能体多臂老虎机框架及其相关算法能够在保证公平性的同时提高系统整体性能，且经实验证明优于现有方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Fair+Algorithms+with+Probing+for+Multi-Agent+Multi-Armed+Bandits，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14988，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14988&send_immediately=true&force_search=false)

**原文摘要:** We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at
ensuring fair outcomes across agents while maximizing overall system
performance. A key challenge in this setting is decision-making under limited
information about arm rewards. To address this, we introduce a novel probing
framework that strategically gathers information about selected arms before
allocation. In the offline setting, where reward distributions are known, we
leverage submodular properties to design a greedy probing algorithm with a
provable performance bound. For the more complex online setting, we develop an
algorithm that achieves sublinear regret while maintaining fairness. Extensive
experiments on synthetic and real-world datasets show that our approach
outperforms baseline methods, achieving better fairness and efficiency.

</details>


### [40] [Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment](https://arxiv.org/abs/2506.15019)
*Yue Gao*

**主要类别:** cs.LG

**AI概要:** 研究发现，通过确保训练稳定性和临床评分相关性，受控微分方程(CDE)状态表示可以为脓毒症治疗的强化学习(RL)提供有效的策略。稳定的CDE自编码器产生的表示与病情严重程度得分高度相关，并能实现性能优异的RL策略。而不稳定的CDE表示则导致表示质量下降和策略失效。


<details>
  <summary>更多</summary>
  
**动机:** 有效强化学习依赖于从不规则ICU时间序列中学习到稳定且具有临床意义的状态表示，但现有工作忽略了连续表示中的训练不稳定性的关键挑战及其对策略性能的负面影响。

**方法:** 使用受控微分方程(CDE)状态表示来实现强效的RL策略，当满足两个关键因素时：(1) 通过提前停止或稳定方法保证训练稳定性；(2) 通过对临床评分(SOFA, SAPS-II, OASIS)的相关性正则化实施疾病意识表示。

**结果:** 在MIMIC-III脓毒症队列上的实验表明，稳定的CDE自编码器生成的表示与重症评分紧密相关，并支持了表现优秀的RL策略（WIS回报>0.9）。相反，不稳定的CDE表示导致了表示质量降低和策略失败（WIS回报约为0）。

**结论:** 研究结果强调了使用CDE对不规则医学时间序列进行编码的实际指导原则，并突出了顺序表示学习中训练稳定性的必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+CDE+Autoencoders+with+Acuity+Regularization+for+Offline+Reinforcement+Learning+in+Sepsis+Treatment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15019，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15019&send_immediately=true&force_search=false)

**原文摘要:** Effective reinforcement learning (RL) for sepsis treatment depends on
learning stable, clinically meaningful state representations from irregular ICU
time series. While previous works have explored representation learning for
this task, the critical challenge of training instability in sequential
representations and its detrimental impact on policy performance has been
overlooked. This work demonstrates that Controlled Differential Equations (CDE)
state representation can achieve strong RL policies when two key factors are
met: (1) ensuring training stability through early stopping or stabilization
methods, and (2) enforcing acuity-aware representations by correlation
regularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the
MIMIC-III sepsis cohort reveal that stable CDE autoencoder produces
representations strongly correlated with acuity scores and enables RL policies
with superior performance (WIS return $> 0.9$). In contrast, unstable CDE
representation leads to degraded representations and policy failure (WIS return
$\sim$ 0). Visualizations of the latent space show that stable CDEs not only
separate survivor and non-survivor trajectories but also reveal clear acuity
score gradients, whereas unstable training fails to capture either pattern.
These findings highlight practical guidelines for using CDEs to encode
irregular medical time series in clinical RL, emphasizing the need for training
stability in sequential representation learning.

</details>


### [41] [SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models](https://arxiv.org/abs/2506.15021)
*Gyuhak Kim, Sumiran Singh Thakur, Su Min Park, Wei Wei, Yujia Bao*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的监督微调方法SFT-GO，该方法通过根据重要性对token进行分组，并使用最差组损失和标准交叉熵损失的加权组合来优化大型语言模型。实验结果表明，这种方法在各种数据集和基础模型上均优于基线方法。


<details>
  <summary>更多</summary>
  
**动机:** 现有的监督微调（SFT）方法通常将每个训练实例视为一个均匀序列，赋予所有标记相同的权重，而忽略了只有部分标记可能包含关键的任务特定信息这一事实。为了解决这个问题，提出了SFT-GO方法。

**方法:** SFT-GO根据每个样本中令牌的重要性值对其进行分组，并使用最差组损失与标准交叉熵损失的加权组合来优化LLM。这种机制自适应地强调最具挑战性的令牌组，并指导模型更好地处理不同的组分布，从而改善整体学习动态。

**结果:** 经验证据显示，当采用三种不同的令牌分组策略时，使用SFT-GO训练的模型在流行的LLM基准测试中始终优于基线方法。这些改进适用于多种数据集和基础模型。

**结论:** SFT-GO提供了一种有效的方法来改进大型语言模型的监督微调过程，通过理论分析证明了其收敛效率，并且在实证研究中展现了其相对于现有方法的优势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SFT-GO%3A+Supervised+Fine-Tuning+with+Group+Optimization+for+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15021，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15021&send_immediately=true&force_search=false)

**原文摘要:** Supervised fine-tuning (SFT) has become an essential step in tailoring large
language models (LLMs) to align with human expectations and specific downstream
tasks. However, existing SFT methods typically treat each training instance as
a uniform sequence, giving equal importance to all tokens regardless of their
relevance. This overlooks the fact that only a subset of tokens often contains
critical, task-specific information. To address this limitation, we introduce
Supervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that
treats groups of tokens differently based on their importance.SFT-GO groups
tokens in each sample based on their importance values and optimizes the LLM
using a weighted combination of the worst-group loss and the standard
cross-entropy loss. This mechanism adaptively emphasizes the most challenging
token groups and guides the model to better handle different group
distributions, thereby improving overall learning dynamics. We provide a
theoretical analysis of SFT-GO's convergence rate, demonstrating its
efficiency. Empirically, we apply SFT-GO with three different token grouping
strategies and show that models trained with SFT-GO consistently outperform
baseline approaches across popular LLM benchmarks. These improvements hold
across various datasets and base models, demonstrating the robustness and the
effectiveness of our method.

</details>


### [42] [ODD: Overlap-aware Estimation of Model Performance under Distribution Shift](https://arxiv.org/abs/2506.14978)
*Aayush Mishra, Anqi Liu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法，即Overlap-aware Disagreement Discrepancy (ODD)，用于更准确地估计机器学习模型在未见测试域中的错误。该方法通过领域分类器来估算领域重叠，并且相比之前的DIS^2方法，在预测目标性能上表现更好。


<details>
  <summary>更多</summary>
  
**动机:** 先前的工作使用了分歧差异（DIS^2）来推导在分布偏移下的实用误差边界，但这种方法会导致源域和目标域重叠区域的竞争问题。基于这样的观察，作者假设目标域的分歧不应该超过源域在重叠区域的分歧，因此提出了一个新方法来解决这个问题。

**方法:** 作者提出了Overlap-aware Disagreement Discrepancy (ODD) 方法，该方法只在非重叠的目标域中要求有分歧，从而消除了竞争。此外，基于ODD的方法使用领域分类器来估计领域重叠情况。

**结果:** 实验表明，与DIS^2相比，ODD方法提高了整体性能评估的准确性，同时保持了有效性和可靠性。

**结论:** ODD方法为跨域设置下估计机器学习模型的误差提供了一个更为准确和可靠的选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ODD%3A+Overlap-aware+Estimation+of+Model+Performance+under+Distribution+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14978&send_immediately=true&force_search=false)

**原文摘要:** Reliable and accurate estimation of the error of an ML model in unseen test
domains is an important problem for safe intelligent systems. Prior work uses
disagreement discrepancy (DIS^2) to derive practical error bounds under
distribution shifts. It optimizes for a maximally disagreeing classifier on the
target domain to bound the error of a given source classifier. Although this
approach offers a reliable and competitively accurate estimate of the target
error, we identify a problem in this approach which causes the disagreement
discrepancy objective to compete in the overlapping region between source and
target domains. With an intuitive assumption that the target disagreement
should be no more than the source disagreement in the overlapping region due to
high enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).
Maximizing ODD only requires disagreement in the non-overlapping target domain,
removing the competition. Our ODD-based bound uses domain-classifiers to
estimate domain-overlap and better predicts target performance than DIS^2. We
conduct experiments on a wide array of benchmarks to show that our method
improves the overall performance-estimation error while remaining valid and
reliable. Our code and results are available on GitHub.

</details>


### [43] [Sequential Policy Gradient for Adaptive Hyperparameter Optimization](https://arxiv.org/abs/2506.15051)
*Zheng Li, Jerry Cheng, Huanying Helen Gu*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的轻量级在线超参数优化方法——顺序策略梯度建模（SPG），该方法能够以较低的计算成本提高模型性能，并在多个数据集上表现出一致的改进。


<details>
  <summary>更多</summary>
  
**动机:** 传统的强化学习方法由于时间和计算成本过高，阻碍了其在神经架构搜索和超参数优化中的广泛应用。为了解决这个问题，研究者们提出了一个新的解决方案。

**方法:** 受DeepSeek-V3多令牌预测架构启发，研究者们开发了一个名为顺序策略梯度建模（SPG）的新轨迹生成范式。与传统策略梯度方法不同，SPG通过临时模块扩展基础模型，能够在一次前向传递中生成状态-动作（填充后）轨迹。

**结果:** 实验显示，当使用SPG在其原始数据集上重新训练时，模型性能得到提升，并且优于标准的迁移微调。对五个涵盖计算机视觉、自然语言处理以及音频的数据集进行评估后，SPG展示出跨越多种广泛采用模型的一致性改进，实现了+0.2%~7%的性能增益。

**结论:** 研究表明，所提出的SPG方法不仅降低了计算成本，而且对于工业应用来说具有显著的优势，因为它能够跨多个领域和数据集提供一致性的性能改进。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sequential+Policy+Gradient+for+Adaptive+Hyperparameter+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15051，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15051&send_immediately=true&force_search=false)

**原文摘要:** Reinforcement learning is essential for neural architecture search and
hyperparameter optimization, but the conventional approaches impede widespread
use due to prohibitive time and computational costs. Inspired by DeepSeek-V3
multi-token prediction architecture, we propose Sequential Policy Gradient
modeling (SPG), a novel trajectory generation paradigm for lightweight online
hyperparameter optimization. In contrast to conventional policy gradient
methods, SPG extends the base model with temporary modules, enabling it to
generate state-action (padded) trajectories in a single forward pass. Our
experiments demonstrate that models gain performance when retrained with SPG on
their original datasets and also outperform standard transfer fine-tuning. We
evaluate on five datasets spanning computer vision (ImageNet, COCO), natural
language processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial
applicability of SPG. The proposed method demonstrates consistent improvements
across widely adopted models, achieving performance gains of $+0.2\sim7\%$,
with significantly low computational costs. Fully reproducible code and
pre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.

</details>


### [44] [Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks](https://arxiv.org/abs/2506.14986)
*Maxime Usdin, Lito Kriara, Licinio Craveiro*

**主要类别:** cs.LG

**AI概要:** 该研究通过结合基础临床数据和数字生活科学数据，利用先进的模型如基于注意力机制的多模态转换器来预测多发性硬化症（MS）的残疾进展。结果显示，整合数字数据能够提高早期预测性能，而多模态方法相较于单模态在预测准确性上表现更优。


<details>
  <summary>更多</summary>
  
**动机:** 由于疾病异质性的存在，早期预测多发性硬化症(MS)的残疾进展十分具有挑战性。本研究旨在通过使用稀疏的基础线临床数据以及12周的每日数字化Floodlight数据，来预测48周和72周后的残疾情况，试图改善早期预测难题。

**方法:** 采用了最先进的表格及时间序列基础模型(FMs)、定制的基于注意力机制的多模态转换器以及机器学习方法。研究中特别强调了通过高级模型整合数字数据的重要性，以期超越仅依赖临床数据所能达到的表现。

**结果:** 尽管早期预测困难(AUROC 0.63)，但通过先进模型整合数字数据确实提高了相对于单独使用临床数据时的表现。采用Moment FM提供的单模态嵌入的转换器模型取得了最佳结果；然而，在一致性上，自定义的多模态转换器优于其单模态版本，这表明了将临床与数字数据相结合的优势。

**结论:** 研究发现展示了基础模型(FMs)和多模态方法从复杂且多样化的临床和数字生命科学数据中提取预测信号的巨大潜力，这不仅有助于提高对MS疾病的预后准确性，也可能适用于其他复杂疾病的精准医疗。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Early+Prediction+of+Multiple+Sclerosis+Disability+Progression+via+Multimodal+Foundation+Model+Benchmarks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14986&send_immediately=true&force_search=false)

**原文摘要:** Early multiple sclerosis (MS) disability progression prediction is
challenging due to disease heterogeneity. This work predicts 48- and 72-week
disability using sparse baseline clinical data and 12 weeks of daily digital
Floodlight data from the CONSONANCE clinical trial. We employed
state-of-the-art tabular and time-series foundation models (FMs), a custom
multimodal attention-based transformer, and machine learning methods. Despite
the difficulty of early prediction (AUROC 0.63), integrating digital data via
advanced models improved performance over clinical data alone. A transformer
model using unimodal embeddings from the Moment FM yielded the best result, but
our multimodal transformer consistently outperformed its unimodal counterpart,
confirming the advantages of combining clinical with digital data. Our findings
demonstrate the promise of FMs and multimodal approaches to extract predictive
signals from complex and diverse clinical and digital life sciences data (e.g.,
imaging, omics), enabling more accurate prognostics for MS and potentially
other complex diseases.

</details>


### [45] [Singular Value Decomposition on Kronecker Adaptation for Large Language Model](https://arxiv.org/abs/2506.15251)
*Yee Hin Chong, Peng Qu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的参数高效微调策略SoKA，它结合了Kronecker-乘积张量分解和SVD驱动初始化以及谱感知动态秩选择。实验表明，SoKA不仅减少了可训练参数的数量，而且在收敛速度、梯度稳定性方面优于现有的方法，并且在不同的推理任务上表现优异。


<details>
  <summary>更多</summary>
  
**动机:** 大型预训练Transformer模型虽然在各种语言和推理任务中取得了最先进的结果，但完全微调会导致大量的存储、内存和计算开销。当前的参数高效微调（PEFT）方法试图通过仅学习一小部分特定于任务的参数来减轻这些成本，但它们要么引入推理时间延迟（适配器模块），要么遇到次优收敛问题（随机初始化低秩更新），或者依赖于可能不匹配任务复杂性的固定秩选择（基于Kronecker的分解）。

**方法:** 研究者提出了SoKA (SVD on Kronecker Adaptation)，一种新颖的参数高效微调策略，该策略将Kronecker-乘积张量因子化与SVD驱动初始化及谱感知动态秩选择相结合。通过Kronecker-Product SVD (KPSVD) 程序将全权重更新的主要成分提取到紧凑的Kronecker因子中，同时使用能量阈值和拐点标准的自适应秩选择算法去除可以忽略的成分。

**结果:** 实证评估显示，在LLaMA2-7B模型上进行算术推理(GSM8K)、形式数学(MATH)和代码生成(MBPP)时，SoKA只需要0.99M个可训练参数，比LoRA/PiSSA少25%，同时达到了或超过了基线性能。此外，SoKA表现出更快的收敛性和更稳定的梯度。

**结论:** SoKA是一种有效的参数高效微调方法，它能够减少可训练参数数量，同时保持甚至提升模型性能。其特点在于快速收敛和稳定梯度，这使其成为大规模模型适应中的稳健且高效的选择。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Singular+Value+Decomposition+on+Kronecker+Adaptation+for+Large+Language+Model，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15251，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15251&send_immediately=true&force_search=false)

**原文摘要:** Large pre-trained Transformer models achieve state-of-the-art results across
diverse language and reasoning tasks, but full fine-tuning incurs substantial
storage, memory, and computational overhead. Parameter-efficient fine-tuning
(PEFT) methods mitigate these costs by learning only a small subset of
task-specific parameters, yet existing approaches either introduce
inference-time latency (adapter modules), suffer from suboptimal convergence
(randomly initialized low-rank updates), or rely on fixed rank choices that may
not match task complexity (Kronecker-based decompositions).
  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that
combines Kronecker-product tensor factorization with SVD-driven initialization
and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)
procedure extracts principal components of the full weight update into compact
Kronecker factors, while an adaptive rank selection algorithm uses
energy-threshold and elbow-point criteria to prune negligible components.
  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal
mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires
only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or
exceeding baseline performance. Moreover, SoKA exhibits faster convergence and
more stable gradients, highlighting its robustness and efficiency for
large-scale model adaptation.

</details>


### [46] [Unlocking Post-hoc Dataset Inference with Synthetic Data](https://arxiv.org/abs/2506.15271)
*Bihe Zhao, Pratyush Maini, Franziska Boenisch, Adam Dziedzic*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种通过合成生成所需保留数据集的方法来解决数据集推理(DI)方法的局限性，该方法能够准确反映原始分布，并通过后验校准缩小真实和合成数据之间的可能性差距，从而在保持低误报率的同时以高置信度检测出原始训练集。


<details>
  <summary>更多</summary>
  
**动机:** 现有的数据集推理（DI）方法需要一个与疑似泄露的数据集分布相匹配但未参与训练的私有数据集，但在实际中这样的同分布保留数据很难获得，这极大地限制了DI的应用。研究者希望解决这个问题，让数据所有者能够验证未经授权的数据使用情况。

**方法:** 研究者采用了一种新的方法，通过训练一个基于精心设计的后缀完成任务的数据生成器来创建高质量、多样化的合成数据，以准确反映原始分布；并通过事后校准来弥合真实数据与合成数据之间存在的似然性差异。

**结果:** 广泛的实验表明，使用所生成的数据作为保留集，可以使DI以很高的置信度检测到原始训练集，同时保持较低的假阳性率。

**结论:** 这项研究为版权拥有者提供了合法主张数据使用的有力工具，并展示了其方法在现实世界诉讼中的可靠性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlocking+Post-hoc+Dataset+Inference+with+Synthetic+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15271&send_immediately=true&force_search=false)

**原文摘要:** The remarkable capabilities of Large Language Models (LLMs) can be mainly
attributed to their massive training datasets, which are often scraped from the
internet without respecting data owners' intellectual property rights. Dataset
Inference (DI) offers a potential remedy by identifying whether a suspect
dataset was used in training, thereby enabling data owners to verify
unauthorized use. However, existing DI methods require a private set-known to
be absent from training-that closely matches the compromised dataset's
distribution. Such in-distribution, held-out data is rarely available in
practice, severely limiting the applicability of DI. In this work, we address
this challenge by synthetically generating the required held-out set. Our
approach tackles two key obstacles: (1) creating high-quality, diverse
synthetic data that accurately reflects the original distribution, which we
achieve via a data generator trained on a carefully designed suffix-based
completion task, and (2) bridging likelihood gaps between real and synthetic
data, which is realized through post-hoc calibration. Extensive experiments on
diverse text datasets show that using our generated data as a held-out set
enables DI to detect the original training sets with high confidence, while
maintaining a low false positive rate. This result empowers copyright owners to
make legitimate claims on data usage and demonstrates our method's reliability
for real-world litigations. Our code is available at
https://github.com/sprintml/PostHocDatasetInference.

</details>


### [47] [Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation](https://arxiv.org/abs/2506.15309)
*Júlia Vilalta-Mor, Alexis Molina, Laura Ortega Varga, Isaac Filella-Merce, Victor Guallar*

**主要类别:** cs.LG

**AI概要:** 提出了一种结构化的主动学习范式，结合了序列到序列变分自编码器，用于在药物发现中同时优化针对多个治疗靶点的分子。通过平衡化学多样性、分子质量和多靶点亲和力，该方法能够生成针对三种相关冠状病毒主要蛋白酶的结构多样化的泛抑制剂候选物。


<details>
  <summary>更多</summary>
  
**动机:** 在药物发现中，同时针对多个治疗靶点优化分子是一个巨大的挑战，特别是由于稀疏奖励和相互冲突的设计限制。

**方法:** 研究者提出了一个结构化主动学习（AL）范式，它将序列到序列（Seq2Seq）变分自编码器（VAE）集成到了迭代循环中，旨在平衡化学多样性、分子质量以及多靶点亲和力。

**结果:** 在一项概念验证研究中，该方法针对三种相关的冠状病毒主要蛋白酶（SARS-CoV-2, SARS-CoV, MERS-CoV），高效地产生了一组结构多样化的泛抑制剂候选物。

**结论:** 研究表明，在主动学习流程中谨慎地安排化学过滤器的时间和位置可以显著增强有益化学空间的探索，从而将稀疏奖励、多目标药物设计问题转化为可操作的计算任务。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Active+Learning-Guided+Seq2Seq+Variational+Autoencoder+for+Multi-target+Inhibitor+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15309，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15309&send_immediately=true&force_search=false)

**原文摘要:** Simultaneously optimizing molecules against multiple therapeutic targets
remains a profound challenge in drug discovery, particularly due to sparse
rewards and conflicting design constraints. We propose a structured active
learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational
autoencoder (VAE) into iterative loops designed to balance chemical diversity,
molecular quality, and multi-target affinity. Our method alternates between
expanding chemically feasible regions of latent space and progressively
constraining molecules based on increasingly stringent multi-target docking
thresholds. In a proof-of-concept study targeting three related coronavirus
main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently
generated a structurally diverse set of pan-inhibitor candidates. We
demonstrate that careful timing and strategic placement of chemical filters
within this active learning pipeline markedly enhance exploration of beneficial
chemical space, transforming the sparse-reward, multi-objective drug design
problem into an accessible computational task. Our framework thus provides a
generalizable roadmap for efficiently navigating complex polypharmacological
landscapes.

</details>


### [48] [When and How Unlabeled Data Provably Improve In-Context Learning](https://arxiv.org/abs/2506.15329)
*Yingcong Li, Xiangyu Chang, Muti Kara, Xiaofeng Liu, Amit Roy-Chowdhury, Samet Oymak*

**主要类别:** cs.LG

**AI概要:** 该研究探索了在标签缺失的情况下，不同类型的注意力模型如何利用未标记数据进行学习。单层线性注意力模型无法有效利用未标记的数据，而多层或循环的transformers可以通过构建特定形式的估计器来利用这些数据。理论分析表明，这种能力与期望最大化算法有关，并且模型深度对性能有显著影响。实验结果证明，通过循环使用现成的基础模型可以大幅提升半监督表格学习的表现。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于理解在上下文学习中，即使示例带有缺失或不正确的标签，为什么仍能表现出有效性。具体来说，是想弄清楚在一定比例的示例缺少标签时，模型如何能够利用未标记的数据。

**方法:** 研究者采用了一种规范设置，其中示例是从二元高斯混合模型（GMM）中抽取的，并且一部分示例具有缺失标签。他们对单层线性注意力模型和多层或循环变换器进行了全面的理论研究，以了解它们如何处理带有缺失标签的数据。

**结果:** 结果显示，单层线性注意力模型虽然能够恢复最佳的全监督估计量，但完全不能利用未标记数据；相比之下，多层或循环变换器能够通过隐式地构造特定形式的估计器来有效地利用未标记数据。进一步的理论分析揭示了这种能力与期望最大化算法之间的联系，并指出模型深度对于提高性能至关重要。实际应用中，通过循环使用现成的表格基础模型增强了其半监督学习能力。

**结论:** 结论是，多层或循环变换器相较于单层线性注意力模型，在处理带缺失标签的数据时表现更佳，因为它们能够更有效地利用未标记数据。此外，只需少量增加模型深度就能显著改善性能。基于此发现，研究提出了一个简单的方法来增强现有表格基础模型的半监督学习能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是When+and+How+Unlabeled+Data+Provably+Improve+In-Context+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15329，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15329&send_immediately=true&force_search=false)

**原文摘要:** Recent research shows that in-context learning (ICL) can be effective even
when demonstrations have missing or incorrect labels. To shed light on this
capability, we examine a canonical setting where the demonstrations are drawn
according to a binary Gaussian mixture model (GMM) and a certain fraction of
the demonstrations have missing labels. We provide a comprehensive theoretical
study to show that: (1) The loss landscape of one-layer linear attention models
recover the optimal fully-supervised estimator but completely fail to exploit
unlabeled data; (2) In contrast, multilayer or looped transformers can
effectively leverage unlabeled data by implicitly constructing estimators of
the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting
features and partially-observed labels (with missing entries set to zero). We
characterize the class of polynomials that can be expressed as a function of
depth and draw connections to Expectation Maximization, an iterative
pseudo-labeling algorithm commonly used in semi-supervised learning.
Importantly, the leading polynomial power is exponential in depth, so mild
amount of depth/looping suffices. As an application of theory, we propose
looping off-the-shelf tabular foundation models to enhance their
semi-supervision capabilities. Extensive evaluations on real-world datasets
show that our method significantly improves the semisupervised tabular learning
performance over the standard single pass inference.

</details>


### [49] [Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI](https://arxiv.org/abs/2506.15408)
*David Dembinsky, Adriano Lucieri, Stanislav Frolov, Hiba Najjar, Ko Watanabe, Andreas Dengel*

**主要类别:** cs.LG

**AI概要:** 本文通过系统性文献回顾，提出了一个统一的框架VXAI来评估可解释的人工智能（XAI）方法，并对现有的362篇相关出版物进行了分类和总结，旨在为XAI方法提供标准化的评估协议。


<details>
  <summary>更多</summary>
  
**动机:** 当前的AI系统依赖于复杂的黑盒模型，如深度神经网络，这些模型缺乏透明度，导致了信任问题。虽然可解释AI（XAI）提供了人类可以理解的模型行为解释，但这个领域缺少标准的评估流程和合适的评估指标共识。

**方法:** 作者按照PRISMA指南进行系统文献综述，引入了一个名为VXAI的统一评估框架，并将相关的362份出版物贡献归纳为41个功能相似的度量组。此外，还提出了一个三维分类方案，涵盖了解释类型、评估情境性和解释质量要求。

**结果:** 研究结果形成了迄今为止最全面和结构化的VXAI概述，支持系统的度量选择，促进了不同方法之间的比较，并为未来的扩展提供了灵活的基础。

**结论:** VXAI框架为XAI方法的评估提供了标准化手段，有助于提高XAI方法的实用性和可信度，并为后续的研究奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Unifying+VXAI%3A+A+Systematic+Review+and+Framework+for+the+Evaluation+of+Explainable+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15408，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15408&send_immediately=true&force_search=false)

**原文摘要:** Modern AI systems frequently rely on opaque black-box models, most notably
Deep Neural Networks, whose performance stems from complex architectures with
millions of learned parameters. While powerful, their complexity poses a major
challenge to trustworthiness, particularly due to a lack of transparency.
Explainable AI (XAI) addresses this issue by providing human-understandable
explanations of model behavior. However, to ensure their usefulness and
trustworthiness, such explanations must be rigorously evaluated. Despite the
growing number of XAI methods, the field lacks standardized evaluation
protocols and consensus on appropriate metrics. To address this gap, we conduct
a systematic literature review following the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a
unified framework for the eValuation of XAI (VXAI). We identify 362 relevant
publications and aggregate their contributions into 41 functionally similar
metric groups. In addition, we propose a three-dimensional categorization
scheme spanning explanation type, evaluation contextuality, and explanation
quality desiderata. Our framework provides the most comprehensive and
structured overview of VXAI to date. It supports systematic metric selection,
promotes comparability across methods, and offers a flexible foundation for
future extensions.

</details>


### [50] [Reward Models in Deep Reinforcement Learning: A Survey](https://arxiv.org/abs/2506.15421)
*Rui Yu, Shenghua Wan, Yucen Wang, Chen-Xiao Gao, Le Gan, Zongzhang Zhang, De-Chuan Zhan*

**主要类别:** cs.LG

**AI概要:** 本综述全面回顾了深度强化学习文献中的奖励建模技术，包括背景、最新方法分类、应用、评估方法及未来研究方向。


<details>
  <summary>更多</summary>
  
**动机:** 为了引导策略优化，在强化学习中引入了奖励模型作为期望目标的代理，本文旨在系统地总结当前文献中奖励模型的相关工作。

**方法:** 本文首先概述了奖励建模的背景和预备知识，接着介绍了近期基于来源、机制和学习范式的奖励建模方法，并讨论了这些技术的应用以及奖励模型的评价方法。

**结果:** 通过整理和分类，文章为读者提供了关于奖励建模技术的详尽概览，并指出了有前景的研究方向。

**结论:** 这篇综述填补了现有文献对于奖励模型系统性回顾的空白，涵盖了已建立的方法和新兴的技术，强调了奖励建模领域内重要的研究趋势。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reward+Models+in+Deep+Reinforcement+Learning%3A+A+Survey，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15421，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15421&send_immediately=true&force_search=false)

**原文摘要:** In reinforcement learning (RL), agents continually interact with the
environment and use the feedback to refine their behavior. To guide policy
optimization, reward models are introduced as proxies of the desired
objectives, such that when the agent maximizes the accumulated reward, it also
fulfills the task designer's intentions. Recently, significant attention from
both academic and industrial researchers has focused on developing reward
models that not only align closely with the true objectives but also facilitate
policy optimization. In this survey, we provide a comprehensive review of
reward modeling techniques within the deep RL literature. We begin by outlining
the background and preliminaries in reward modeling. Next, we present an
overview of recent reward modeling approaches, categorizing them based on the
source, the mechanism, and the learning paradigm. Building on this
understanding, we discuss various applications of these reward modeling
techniques and review methods for evaluating reward models. Finally, we
conclude by highlighting promising research directions in reward modeling.
Altogether, this survey includes both established and emerging methods, filling
the vacancy of a systematic review of reward models in current literature.

</details>


### [51] [Zero-Shot Reinforcement Learning Under Partial Observability](https://arxiv.org/abs/2506.15446)
*Scott Jeen, Tom Bewley, Jonathan M. Cullen*

**主要类别:** cs.LG

**AI概要:** 研究了部分可观测环境下标准零样本强化学习方法的性能下降，并展示了基于记忆架构的方法在这些条件下是有效的补救措施。


<details>
  <summary>更多</summary>
  
**动机:** 探索当标准零射强化学习方法应用于部分可观测环境时，其性能如何下降，以及基于记忆的架构是否能够有效解决这一问题。

**方法:** 通过在状态、奖励和动态变化部分可观测的领域中评估基于记忆的零射RL方法，并与无记忆基线进行比较。

**结果:** 基于记忆的零射RL方法在部分可观测的条件下比没有记忆的基线表现更好。

**结论:** 对于部分可观测环境中的零样本强化学习任务，采用基于记忆的架构可以提高算法的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Reinforcement+Learning+Under+Partial+Observability，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15446，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15446&send_immediately=true&force_search=false)

**原文摘要:** Recent work has shown that, under certain assumptions, zero-shot
reinforcement learning (RL) methods can generalise to any unseen task in an
environment after reward-free pre-training. Access to Markov states is one such
assumption, yet, in many real-world applications, the Markov state is only
partially observable. Here, we explore how the performance of standard
zero-shot RL methods degrades when subjected to partially observability, and
show that, as in single-task RL, memory-based architectures are an effective
remedy. We evaluate our memory-based zero-shot RL methods in domains where the
states, rewards and a change in dynamics are partially observed, and show
improved performance over memory-free baselines. Our code is open-sourced via:
https://enjeeneer.io/projects/bfms-with-memory/.

</details>


### [52] [HiPreNets: High-Precision Neural Networks through Progressive Training](https://arxiv.org/abs/2506.15064)
*Ethan Mulle, Wei Kang, Qi Gong*

**主要类别:** cs.LG

**AI概要:** Error


<details>
  <summary>更多</summary>
  
**动机:** Error

**方法:** Error

**结果:** Error

**结论:** Error

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HiPreNets%3A+High-Precision+Neural+Networks+through+Progressive+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15064，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15064&send_immediately=true&force_search=false)

**原文摘要:** Deep neural networks are powerful tools for solving nonlinear problems in
science and engineering, but training highly accurate models becomes
challenging as problem complexity increases. Non-convex optimization and
numerous hyperparameters to tune make performance improvement difficult, and
traditional approaches often prioritize minimizing mean squared error (MSE)
while overlooking $L^{\infty}$ error, which is the critical focus in many
applications. To address these challenges, we present a progressive framework
for training and tuning high-precision neural networks (HiPreNets). Our
approach refines a previously explored staged training technique for neural
networks that improves an existing fully connected neural network by
sequentially learning its prediction residuals using additional networks,
leading to improved overall accuracy. We discuss how to take advantage of the
structure of the residuals to guide the choice of loss function, number of
parameters to use, and ways to introduce adaptive data sampling techniques. We
validate our framework's effectiveness through several benchmark problems.

</details>


### [53] [HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models](https://arxiv.org/abs/2506.15065)
*Trishna Chakraborty, Udita Ghosh, Xiaopan Zhang, Fahim Faisal Niloy, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, Chengyu Song*

**主要类别:** cs.LG

**AI概要:** 本文首次系统性地研究了基于大型语言模型（LLMs）的实体代理在场景-任务不一致的情况下执行长期任务时出现的幻觉问题。通过构建一个幻觉探测集，我们发现模型虽然展现出推理能力，但无法解决场景-任务间的不一致性，揭示了处理不可行任务的基本局限。此外，还提供了关于理想模型行为的可操作见解，为开发更稳健可靠的规划策略提供指导。


<details>
  <summary>更多</summary>
  
**动机:** 随着大型语言模型越来越多地被用作实体代理的认知核心，由于用户指令未能与观察到的实际环境相结合而产生的继承性幻觉会导致导航错误等问题。因此，研究者们希望了解幻觉发生的程度、触发它们的不同类型不一致以及当前模型如何响应这些问题。

**方法:** 研究团队建立了一个幻觉探测集，该集合基于现有的基准，并能够诱导出比基础提示高40倍的幻觉率。然后，他们评估了两个模拟环境中的12个模型，以检查这些模型在面对场景-任务不一致时的表现。

**结果:** 研究发现，尽管模型显示了一定的推理能力，但在解决场景-任务之间的不一致方面却显得力不从心，这表明了现有模型在处理不可行任务上存在根本性的局限。

**结论:** 该研究表明，目前的大型语言模型作为实体代理的核心，在遇到场景与任务不匹配的情况时，容易产生幻觉并表现出局限性。为了改善这一点，研究提出了针对每种情况的理想模型行为建议，为将来开发更加健壮和可靠的规划策略提供了有价值的参考。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HEAL%3A+An+Empirical+Study+on+Hallucinations+in+Embodied+Agents+Driven+by+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15065，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15065&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) are increasingly being adopted as the cognitive
core of embodied agents. However, inherited hallucinations, which stem from
failures to ground user instructions in the observed physical environment, can
lead to navigation errors, such as searching for a refrigerator that does not
exist. In this paper, we present the first systematic study of hallucinations
in LLM-based embodied agents performing long-horizon tasks under scene-task
inconsistencies. Our goal is to understand to what extent hallucinations occur,
what types of inconsistencies trigger them, and how current models respond. To
achieve these goals, we construct a hallucination probing set by building on an
existing benchmark, capable of inducing hallucination rates up to 40x higher
than base prompts. Evaluating 12 models across two simulation environments, we
find that while models exhibit reasoning, they fail to resolve scene-task
inconsistencies-highlighting fundamental limitations in handling infeasible
tasks. We also provide actionable insights on ideal model behavior for each
scenario, offering guidance for developing more robust and reliable planning
strategies.

</details>


### [54] [Pixel-level Certified Explanations via Randomized Smoothing](https://arxiv.org/abs/2506.15499)
*Alaa Anani, Tobias Lorenz, Mario Fritz, Bernt Schiele*

**主要类别:** cs.LG

**AI概要:** 本文提出了首个基于随机平滑的认证框架，确保任何黑盒归因方法在像素级别上的鲁棒性，并通过稀疏化和平滑归因图来抵抗ℓ2-有界扰动。同时，文章还提出了三个评估指标以衡量认证鲁棒性、定位和忠实度。


<details>
  <summary>更多</summary>
  
**动机:** 后验归因方法旨在通过突出显示有影响力的输入像素来解释深度学习预测。然而，这些解释非常不稳定：微小且难以察觉的输入扰动可能会大幅改变归因图，同时保持相同的预测结果。这种脆弱性削弱了它们的可信度，因此需要对像素级别的归因分数提供严格的鲁棒性保证。

**方法:** 引入了一个基于随机平滑的认证框架，该框架能够为任何黑盒归因方法提供像素级别的鲁棒性保障。通过将归因图进行稀疏化和平滑处理，任务被重新定义成一个分割问题，并且每个像素的重要性都得到了针对ℓ2-有界扰动的认证。

**结果:** 通过对12种归因方法在5个ImageNet模型上进行了广泛的评估，证明了经过认证的归因是鲁棒的、可解释的并且忠实于原始模型，这使得它们可以可靠地用于下游任务。

**结论:** 提出的方法提高了归因图的鲁棒性，增强了用户对于模型解释的信任，并且为未来的研究提供了新的评估标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pixel-level+Certified+Explanations+via+Randomized+Smoothing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15499，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15499&send_immediately=true&force_search=false)

**原文摘要:** Post-hoc attribution methods aim to explain deep learning predictions by
highlighting influential input pixels. However, these explanations are highly
non-robust: small, imperceptible input perturbations can drastically alter the
attribution map while maintaining the same prediction. This vulnerability
undermines their trustworthiness and calls for rigorous robustness guarantees
of pixel-level attribution scores. We introduce the first certification
framework that guarantees pixel-level robustness for any black-box attribution
method using randomized smoothing. By sparsifying and smoothing attribution
maps, we reformulate the task as a segmentation problem and certify each
pixel's importance against $\ell_2$-bounded perturbations. We further propose
three evaluation metrics to assess certified robustness, localization, and
faithfulness. An extensive evaluation of 12 attribution methods across 5
ImageNet models shows that our certified attributions are robust,
interpretable, and faithful, enabling reliable use in downstream tasks. Our
code is at https://github.com/AlaaAnani/certified-attributions.

</details>


### [55] [Over-squashing in Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2506.15507)
*Ivan Marisca, Jacob Bamberger, Cesare Alippi, Michael M. Bronstein*

**主要类别:** cs.LG

**AI概要:** 本文研究了时空图神经网络（STGNNs）中的信息传播问题，特别是过压缩现象，并揭示了其与静态图神经网络不同的特性。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在静态场景中对图神经网络的信息传播限制进行了广泛的研究，但在处理序列数据的时空图神经网络（STGNNs）中，这一问题仍未得到充分探索。时间维度增加了需要传播的信息量，从而加剧了这个问题。

**方法:** 本文正式定义了时空过压缩问题，并通过分析展示了它与静态情况下的不同特征。作者还证明了遵循时间-空间或先时间后空间处理范式的架构都受到此现象的影响，并提供了理论依据来支持计算效率高的实现。

**结果:** 研究发现，出乎意料的是，卷积型STGNN更倾向于从时间上较远而非较近的点传播信息。此外，研究结果得到了合成和真实世界数据集上的验证，为设计更加有效的模型提供了深入见解。

**结论:** 论文提出了关于时空过压缩问题的新理解，为时空图神经网络的设计提供了原则性的指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Over-squashing+in+Spatiotemporal+Graph+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15507，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15507&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) have achieved remarkable success across various
domains. However, recent theoretical advances have identified fundamental
limitations in their information propagation capabilities, such as
over-squashing, where distant nodes fail to effectively exchange information.
While extensively studied in static contexts, this issue remains unexplored in
Spatiotemporal GNNs (STGNNs), which process sequences associated with graph
nodes. Nonetheless, the temporal dimension amplifies this challenge by
increasing the information that must be propagated. In this work, we formalize
the spatiotemporal over-squashing problem and demonstrate its distinct
characteristics compared to the static case. Our analysis reveals that
counterintuitively, convolutional STGNNs favor information propagation from
points temporally distant rather than close in time. Moreover, we prove that
architectures that follow either time-and-space or time-then-space processing
paradigms are equally affected by this phenomenon, providing theoretical
justification for computationally efficient implementations. We validate our
findings on synthetic and real-world datasets, providing deeper insights into
their operational dynamics and principled guidance for more effective designs.

</details>


### [56] [Towards Reliable Forgetting: A Survey on Machine Unlearning Verification, Challenges, and Future Directions](https://arxiv.org/abs/2506.15115)
*Lulu Xue, Shengshan Hu, Wei Lu, Yan Shen, Dongxu Li, Peijin Guo, Ziqi Zhou, Minghui Li, Yanjun Zhang, Leo Yu Zhang*

**主要类别:** cs.LG

**AI概要:** 本文首次对机器遗忘验证方法进行了系统性综述，提出了基于行为验证和参数验证两大类别的分类体系，并分析了代表性方法的假设、优势与局限性，最后指出了当前研究中的开放问题。


<details>
  <summary>更多</summary>
  
**动机:** 随着隐私保护、安全性和法律合规（如GDPR）的需求不断增长，机器遗忘技术成为确保机器学习模型可控性和法规遵从性的关键手段。然而，在这一领域中，一个基本挑战在于如何有效地验证遗忘操作是否已成功且彻底地执行。尽管在遗忘技术方面的工作日益增多，但验证方法的研究相对较少且较为零散。现有方法缺乏统一的分类体系和系统性的评估框架。

**方法:** 本文通过提出一个分类体系来组织现有的机器遗忘验证技术，将其分为两大主要类别：基于行为的验证和基于参数的验证。接着，对每个类别中的代表性方法进行审查，分析它们的基本假设、优点及局限性，并识别出实际部署中的潜在脆弱性。

**结果:** 文章构建了一个结构化的机器遗忘验证方法综述，提供了不同验证方法之间的对比分析，明确了各自的优势与劣势，并指出了未来研究的方向。

**结论:** 本论文为发展更加稳健、高效且理论基础扎实的机器遗忘验证机制奠定了基础，并针对当前验证研究中的开放问题提出了见解。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Reliable+Forgetting%3A+A+Survey+on+Machine+Unlearning+Verification%2C+Challenges%2C+and+Future+Directions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15115，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15115&send_immediately=true&force_search=false)

**原文摘要:** With growing demands for privacy protection, security, and legal compliance
(e.g., GDPR), machine unlearning has emerged as a critical technique for
ensuring the controllability and regulatory alignment of machine learning
models. However, a fundamental challenge in this field lies in effectively
verifying whether unlearning operations have been successfully and thoroughly
executed. Despite a growing body of work on unlearning techniques, verification
methodologies remain comparatively underexplored and often fragmented. Existing
approaches lack a unified taxonomy and a systematic framework for evaluation.
To bridge this gap, this paper presents the first structured survey of machine
unlearning verification methods. We propose a taxonomy that organizes current
techniques into two principal categories -- behavioral verification and
parametric verification -- based on the type of evidence used to assess
unlearning fidelity. We examine representative methods within each category,
analyze their underlying assumptions, strengths, and limitations, and identify
potential vulnerabilities in practical deployment. In closing, we articulate a
set of open problems in current verification research, aiming to provide a
foundation for developing more robust, efficient, and theoretically grounded
unlearning verification mechanisms.

</details>


### [57] [RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation](https://arxiv.org/abs/2506.15513)
*Le Vu Anh, Nguyen Viet Anh, Mehmet Dik, Luong Van Nghia*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为Retrieval-Path Contamination Scoring (RePCS)的诊断方法，用于检测检索增强生成(RAG)系统中是否依赖于记忆训练数据而非检索到的信息。该方法通过比较仅使用查询的参数路径和同时使用查询及检索上下文的检索增强路径之间的KL散度来实现。


<details>
  <summary>更多</summary>
  
**动机:** 尽管检索增强生成（RAG）策略允许大型语言模型（LLM）利用最新的外部信息更新响应，但这些模型可能仍然会依赖于记忆中的训练数据，并绕过检索到的证据产生污染输出。因此，需要一种能够检测这种行为的方法，而且不需要访问或重新训练模型。

**方法:** 引入了Retrieval-Path Contamination Scoring (RePCS)，这是一种通过对比两种推理路径——纯参数路径（仅基于查询）与检索增强路径（基于查询和检索内容）之间输出分布的Kullback-Leibler(KL)散度来检测潜在的记忆行为的方法。低散度表明检索到的内容影响很小，暗示可能存在记忆现象。

**结果:** 在Prompt-WNQA基准测试上，RePCS达到了0.918的ROC-AUC分数，比之前最强的方法高出6.5个百分点，同时在NVIDIA T4 GPU上的延迟开销低于4.7%。此外，还提供了PAC风格的保证，将KL阈值与用户定义的误报率和漏报率联系起来。

**结论:** RePCS提供了一个轻量级、黑盒式的保障机制，用来验证RAG系统是否真正有意义地利用了检索功能，这在安全至关重要的应用中尤为重要。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RePCS%3A+Diagnosing+Data+Memorization+in+LLM-Powered+Retrieval-Augmented+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15513，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15513&send_immediately=true&force_search=false)

**原文摘要:** Retrieval-augmented generation (RAG) has become a common strategy for
updating large language model (LLM) responses with current, external
information. However, models may still rely on memorized training data, bypass
the retrieved evidence, and produce contaminated outputs. We introduce
Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects
such behavior without requiring model access or retraining. RePCS compares two
inference paths: (i) a parametric path using only the query, and (ii) a
retrieval-augmented path using both the query and retrieved context by
computing the Kullback-Leibler (KL) divergence between their output
distributions. A low divergence suggests that the retrieved context had minimal
impact, indicating potential memorization. This procedure is model-agnostic,
requires no gradient or internal state access, and adds only a single
additional forward pass. We further derive PAC-style guarantees that link the
KL threshold to user-defined false positive and false negative rates. On the
Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result
outperforms the strongest prior method by 6.5 percentage points while keeping
latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,
black-box safeguard to verify whether a RAG system meaningfully leverages
retrieval, making it especially valuable in safety-critical applications.

</details>


### [58] [ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed Machine Learning](https://arxiv.org/abs/2506.15181)
*Bing Liu, Chengcheng Zhao, Li Chai, Peng Cheng, Yaonan Wang*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为ImprovDML的去中心化分布式机器学习框架，该框架在保证隐私保护和抵御拜占庭攻击的同时，实现了高模型精度。


<details>
  <summary>更多</summary>
  
**动机:** 在分布式机器学习中同时解决拜占庭攻击和隐私泄露问题变得非常重要。但通常采用的策略会显著降低模型准确性。

**方法:** 开发了一个名为ImprovDML的去中心化框架，使用一种鲁棒向量共识算法来计算正常代理节点凸包内的点，并引入多元高斯噪声以保护隐私。

**结果:** 提供了非凸环境下的收敛性保证和渐近学习误差界限，这些界限比现有工作中的更紧。此外，采用集中式地理隐私概念进行隐私分析，证明了它在隐私保护和模型准确性之间提供了更好的权衡。

**结论:** 数值模拟验证了理论结果，表明所提出的框架能够在保持隐私和对抗拜占庭攻击的同时，实现较高的模型准确度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ImprovDML%3A+Improved+Trade-off+in+Private+Byzantine-Resilient+Distributed+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15181，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15181&send_immediately=true&force_search=false)

**原文摘要:** Jointly addressing Byzantine attacks and privacy leakage in distributed
machine learning (DML) has become an important issue. A common strategy
involves integrating Byzantine-resilient aggregation rules with differential
privacy mechanisms. However, the incorporation of these techniques often
results in a significant degradation in model accuracy. To address this issue,
we propose a decentralized DML framework, named ImprovDML, that achieves high
model accuracy while simultaneously ensuring privacy preservation and
resilience to Byzantine attacks. The framework leverages a kind of resilient
vector consensus algorithms that can compute a point within the normal
(non-Byzantine) agents' convex hull for resilient aggregation at each
iteration. Then, multivariate Gaussian noises are introduced to the gradients
for privacy preservation. We provide convergence guarantees and derive
asymptotic learning error bounds under non-convex settings, which are tighter
than those reported in existing works. For the privacy analysis, we adopt the
notion of concentrated geo-privacy, which quantifies privacy preservation based
on the Euclidean distance between inputs. We demonstrate that it enables an
improved trade-off between privacy preservation and model accuracy compared to
differential privacy. Finally, numerical simulations validate our theoretical
results.

</details>


### [59] [Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework](https://arxiv.org/abs/2506.15538)
*Laura Kopf, Nils Feldhus, Kirill Bykov, Philine Lou Bommer, Anna Hedström, Marina M. -C. Höhne, Oliver Eberle*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的框架PRISM，用于更准确地描述神经网络特征的多义性，解决了现有方法对神经元单一概念编码假设的局限，并通过基准测试展示了其在语言模型中的优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 当前特征描述方法存在鲁棒性不足和错误假设神经元仅编码单一概念的问题，这限制了特征描述的表现力和捕捉模型内部行为的能力。

**方法:** 开发了一个名为Polysemantic FeatuRe Identification and Scoring Method (PRISM)的新框架，该框架能够为多义性和单一概念特征提供更加细致的描述。

**结果:** 通过与现有方法进行广泛的基准测试比较，证明了PRISM方法能够生成更准确可靠的特征描述，提高了描述质量和捕捉不同概念的能力。

**结论:** PRISM框架克服了以往方法的局限性，提供了更为丰富和准确的神经网络特征描述方式。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Capturing+Polysemanticity+with+PRISM%3A+A+Multi-Concept+Feature+Description+Framework，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15538，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15538&send_immediately=true&force_search=false)

**原文摘要:** Automated interpretability research aims to identify concepts encoded in
neural network features to enhance human understanding of model behavior.
Current feature description methods face two critical challenges: limited
robustness and the flawed assumption that each neuron encodes only a single
concept (monosemanticity), despite growing evidence that neurons are often
polysemantic. This assumption restricts the expressiveness of feature
descriptions and limits their ability to capture the full range of behaviors
encoded in model internals. To address this, we introduce Polysemantic FeatuRe
Identification and Scoring Method (PRISM), a novel framework that captures the
inherent complexity of neural network features. Unlike prior approaches that
assign a single description per feature, PRISM provides more nuanced
descriptions for both polysemantic and monosemantic features. We apply PRISM to
language models and, through extensive benchmarking against existing methods,
demonstrate that our approach produces more accurate and faithful feature
descriptions, improving both overall description quality (via a description
score) and the ability to capture distinct concepts when polysemanticity is
present (via a polysemanticity score).

</details>


### [60] [Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors](https://arxiv.org/abs/2506.15190)
*Jiyi Wang, Jingyang Ke, Bo Dai, Anqi Wu*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于技能的模仿学习（SKIL）框架，用于行为理解。该框架通过表征学习推断出可解释的技能集，并将策略参数化为这些技能的动态混合。在不同任务中验证了其有效性，能够识别可重用的技能组件，学习连续演变的组合策略，并生成超出传统离散模型能力的真实轨迹。


<details>
  <summary>更多</summary>
  
**动机:** 现有的行为分割方法通过对生成过程施加限制性假设来简化动物行为生成的过程，这并不符合实际情况。为了更准确地反映动物行为生成过程，研究者提出了一个新的模仿学习框架。

**方法:** 引入了基于技能的模仿学习（SKIL），这是一种基于强化学习的模仿框架，它通过利用转移概率上的表征学习来推断可解释的技能集，即行为的潜在基础函数，并将策略参数化为这些技能的动态混合。

**结果:** 该方法在简单网格世界、离散迷宫以及自由移动动物的无约束视频上进行了验证。结果显示，它能够识别可复用的技能组成部分，学会持续演化的合成策略，并产生比传统离散模型更真实的轨迹。

**结论:** 通过利用具有组成表示的行为生成建模，所提出的方法提供了一个简洁而有原则的解释，说明复杂的动物行为是如何从基本运动基元的动态组合中产生的。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Task-Agnostic+Skill+Bases+to+Uncover+Motor+Primitives+in+Animal+Behaviors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15190&send_immediately=true&force_search=false)

**原文摘要:** Animals flexibly recombine a finite set of core motor primitives to meet
diverse task demands, but existing behavior-segmentation methods oversimplify
this process by imposing discrete syllables under restrictive generative
assumptions. To reflect the animal behavior generation procedure, we introduce
skill-based imitation learning (SKIL) for behavior understanding, a
reinforcement learning-based imitation framework that (1) infers interpretable
skill sets, i.e., latent basis functions of behavior, by leveraging
representation learning on transition probabilities, and (2) parameterizes
policies as dynamic mixtures of these skills. We validate our approach on a
simple grid world, a discrete labyrinth, and unconstrained videos of freely
moving animals. Across tasks, it identifies reusable skill components, learns
continuously evolving compositional policies, and generates realistic
trajectories beyond the capabilities of traditional discrete models. By
exploiting generative behavior modeling with compositional representations, our
method offers a concise, principled account of how complex animal behaviors
emerge from dynamic combinations of fundamental motor primitives.

</details>


### [61] [Learning Algorithms in the Limit](https://arxiv.org/abs/2506.15543)
*Hristo Papazov, Nicolas Flammarion*

**主要类别:** cs.LG

**AI概要:** 本文研究了通过扩展Gold的归纳推理框架来学习可计算函数的问题，引入了时间约束观察和策略轨迹观察，以更现实的约束条件下来研究一般递归函数的学习性。


<details>
  <summary>更多</summary>
  
**动机:** 作者希望通过添加计算观察和受限输入源来扩展Gold的归纳推理框架，以便在更实际的约束条件下研究一般递归函数的学习性。

**方法:** 研究方法包括引入时间约束观察和策略-轨迹观察，以及建立围绕计算代理观察的形式化框架。

**结果:** 结果显示，仅输入-输出观察不足以学习一般递归函数类，但通过施加计算复杂性限制或补充近似时间约束观察可以克服这一障碍。此外，从策略轨迹学习可计算函数可简化为从输入和输出中学习有理函数。

**结论:** 结论是，即使对于策略-轨迹观察，线性时间可计算函数类也无法存在可计算或多项式质量特征集。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Algorithms+in+the+Limit，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15543，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15543&send_immediately=true&force_search=false)

**原文摘要:** This paper studies the problem of learning computable functions in the limit
by extending Gold's inductive inference framework to incorporate
\textit{computational observations} and \textit{restricted input sources}.
Complimentary to the traditional Input-Output Observations, we introduce
Time-Bound Observations, and Policy-Trajectory Observations to study the
learnability of general recursive functions under more realistic constraints.
While input-output observations do not suffice for learning the class of
general recursive functions in the limit, we overcome this learning barrier by
imposing computational complexity constraints or supplementing with approximate
time-bound observations. Further, we build a formal framework around
observations of \textit{computational agents} and show that learning computable
functions from policy trajectories reduces to learning rational functions from
input and output, thereby revealing interesting connections to finite-state
transducer inference. On the negative side, we show that computable or
polynomial-mass characteristic sets cannot exist for the class of linear-time
computable functions even for policy-trajectory observations.

</details>


### [62] [DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones](https://arxiv.org/abs/2506.15554)
*Akhil Singampalli, Danish Gufran, Sudeep Pasricha*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的领域增量学习框架DAILOC，能够同时解决基于Wi-Fi指纹的室内定位中由时间和设备引起的领域漂移问题。通过实验表明，DAILOC在多个智能手机、建筑物和时间点上显著优于现有方法，平均误差降低了2.74倍，最坏情况下的误差降低了4.6倍。


<details>
  <summary>更多</summary>
  
**动机:** 基于Wi-Fi指纹的室内定位在实际部署中面临着由于设备异质性和室内环境随时间变化导致的领域漂移的重大挑战。现有的方法通常分别处理这些问题，这导致了泛化能力差以及随着时间推移容易出现灾难性遗忘。

**方法:** DAILOC是一个新颖的领域增量学习框架，它采用一种新的解缠策略，利用多级变分自动编码器将领域漂移与位置相关特征分开。此外，还引入了一种新的记忆引导类潜变量对齐机制来应对长时间内的灾难性遗忘效应。

**结果:** 实验结果表明，在多个智能手机、建筑物和时间实例上，DAILOC相比最新的方法显著提高了性能，平均错误率降低了2.74倍，最糟糕情况下的错误率降低了4.6倍。

**结论:** DAILOC作为一个针对基于Wi-Fi指纹室内定位的领域增量学习框架，有效地解决了因时间和设备引起的问题，并且在减少定位误差方面表现出色。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DAILOC%3A+Domain-Incremental+Learning+for+Indoor+Localization+using+Smartphones，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15554，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15554&send_immediately=true&force_search=false)

**原文摘要:** Wi-Fi fingerprinting-based indoor localization faces significant challenges
in real-world deployments due to domain shifts arising from device
heterogeneity and temporal variations within indoor environments. Existing
approaches often address these issues independently, resulting in poor
generalization and susceptibility to catastrophic forgetting over time. In this
work, we propose DAILOC, a novel domain-incremental learning framework that
jointly addresses both temporal and device-induced domain shifts. DAILOC
introduces a novel disentanglement strategy that separates domain shifts from
location-relevant features using a multi-level variational autoencoder.
Additionally, we introduce a novel memory-guided class latent alignment
mechanism to address the effects of catastrophic forgetting over time.
Experiments across multiple smartphones, buildings, and time instances
demonstrate that DAILOC significantly outperforms state-of-the-art methods,
achieving up to 2.74x lower average error and 4.6x lower worst-case error.

</details>


### [63] [Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates](https://arxiv.org/abs/2506.15559)
*Danish Gufran, Sudeep Pasricha*

**主要类别:** cs.LG

**AI概要:** 提出了一种基于逻辑门的框架LogNet，用于解释和增强基于深度学习的室内定位。LogNet能够识别对于每个参考点最具影响力的接入点，并揭示环境噪声如何干扰基于深度学习的定位决策，从而提高长期部署的稳定性。


<details>
  <summary>更多</summary>
  
**动机:** 现有的基于深度学习（DL）的室内定位模型大多作为黑盒模型运行，缺乏可解释性，这限制了我们对模型预测方式的理解以及模型如何响应随时间变化的实际噪声。这种不足影响了我们理解由于环境动态引起的时变效应的能力，也阻碍了为长期可靠性调整模型的可能性。

**方法:** 提出了LogNet，一种新颖的基于逻辑门的框架，旨在解释并增强基于深度学习的室内定位。它通过透明化推理过程来确定哪些接入点（APs）对于每个参考点（RPs）是最具影响力的，并且展示了环境噪声是如何扰乱基于深度学习的定位决策的。

**结果:** 在多个真实世界建筑平面图上的评估表明，在两年的时间变化中，LogNet不仅能够解释深度学习模型的内部行为，而且还能改善性能——与先前基于深度学习的模型相比，降低了1.1倍到2.8倍的定位误差，缩小了3.4倍到43.3倍的模型大小，并减少了1.5倍到3.6倍的延迟。

**结论:** LogNet提供了一种有效的方法来理解和改进基于深度学习的室内定位系统，通过增强模型的可解释性，可以更好地诊断问题并适应长时间内的环境变化，从而提高了系统的稳定性和准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Explainable+Indoor+Localization%3A+Interpreting+Neural+Network+Learning+on+Wi-Fi+Fingerprints+Using+Logic+Gates，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15559&send_immediately=true&force_search=false)

**原文摘要:** Indoor localization using deep learning (DL) has demonstrated strong accuracy
in mapping Wi-Fi RSS fingerprints to physical locations; however, most existing
DL frameworks function as black-box models, offering limited insight into how
predictions are made or how models respond to real-world noise over time. This
lack of interpretability hampers our ability to understand the impact of
temporal variations - caused by environmental dynamics - and to adapt models
for long-term reliability. To address this, we introduce LogNet, a novel logic
gate-based framework designed to interpret and enhance DL-based indoor
localization. LogNet enables transparent reasoning by identifying which access
points (APs) are most influential for each reference point (RP) and reveals how
environmental noise disrupts DL-driven localization decisions. This
interpretability allows us to trace and diagnose model failures and adapt DL
systems for more stable long-term deployments. Evaluations across multiple
real-world building floorplans and over two years of temporal variation show
that LogNet not only interprets the internal behavior of DL models but also
improves performance-achieving up to 1.1x to 2.8x lower localization error,
3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to
prior DL-based models.

</details>


### [64] [Centroid Approximation for Byzantine-Tolerant Federated Learning](https://arxiv.org/abs/2506.15264)
*Mélanie Cambus, Darya Melnyk, Tijana Milentijević, Stefan Schmid*

**主要类别:** cs.LG

**AI概要:** 本文研究了联邦学习在拜占庭行为下的鲁棒性，并提出了在盒子有效性条件下对质心近似的首个下界，同时给出了一种新的算法，在凸有效性条件下达到√2d-近似。


<details>
  <summary>更多</summary>
  
**动机:** 联邦学习允许每个客户端在分布式环境中训练机器学习模型时将数据保留在本地。最近的研究已经确定了输入必须满足的要求以保证训练循环的收敛性。然而，对于联邦学习是否能够抵抗拜占庭式的行为，以及平均/质心和分布式计算中的有效性条件之间的权衡尚未得到充分探索。

**方法:** 通过理论分析，给出了在考虑拜占庭故障情况下质心近似的下界和上界，并提出了一种新算法来实现特定条件下的近似。

**结果:** 研究表明，仅靠各种有效性条件并不能保证良好的平均值近似。此外，即使达到了良好的近似效果，由于潜在的拜占庭异常值，实验设置中可能仍然得不到好的结果。提出的下界为min{(n-t)/t,√d}，其中n是客户端数量，t是拜占庭故障数量的上限，d是机器学习模型的维度。对于n<d的情况提供了一个新的分析，得出了一个上界2min{n,√d}。另外，新算法在凸有效性条件下实现了√2d-近似。

**结论:** 本文提供了关于联邦学习中拜占庭容错性的深入理解，并且证明了在某些条件下，可以实现有效的质心近似。所展示的所有边界也适用于分布式点对点环境，并且这些分析结果得到了联合随机梯度下降和联合平均设置中的实证评估的支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Centroid+Approximation+for+Byzantine-Tolerant+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15264，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15264&send_immediately=true&force_search=false)

**原文摘要:** Federated learning allows each client to keep its data locally when training
machine learning models in a distributed setting. Significant recent research
established the requirements that the input must satisfy in order to guarantee
convergence of the training loop. This line of work uses averaging as the
aggregation rule for the training models. In particular, we are interested in
whether federated learning is robust to Byzantine behavior, and observe and
investigate a tradeoff between the average/centroid and the validity conditions
from distributed computing. We show that the various validity conditions alone
do not guarantee a good approximation of the average. Furthermore, we show that
reaching good approximation does not give good results in experimental settings
due to possible Byzantine outliers. Our main contribution is the first lower
bound of $\min\{\frac{n-t}{t},\sqrt{d}\}$ on the centroid approximation under
box validity that is often considered in the literature, where $n$ is the
number of clients, $t$ the upper bound on the number of Byzantine faults, and
$d$ is the dimension of the machine learning model. We complement this lower
bound by an upper bound of $2\min\{n,\sqrt{d}\}$, by providing a new analysis
for the case $n<d$. In addition, we present a new algorithm that achieves a
$\sqrt{2d}$-approximation under convex validity, which also proves that the
existing lower bound in the literature is tight. We show that all presented
bounds can also be achieved in the distributed peer-to-peer setting. We
complement our analytical results with empirical evaluations in federated
stochastic gradient descent and federated averaging settings.

</details>


### [65] [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606)
*Gabrel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong*

**主要类别:** cs.LG

**AI概要:** 本文探讨了大型语言模型（LLMs）在对齐后仍然可能因为后续微调而失去安全保护的问题，提出了一种无需训练的方法——低秩外推法（LoX），以增强模型的安全鲁棒性。实验表明，LoX能够显著降低良性或恶意微调攻击的成功率，同时保持模型适应新任务的能力。


<details>
  <summary>更多</summary>
  
**动机:** 尽管已经付出了巨大的努力来通过校准提高模型安全性，但对齐后的模型仍可能因随后的微调而使其安全保护失效，即使额外的训练数据看似无害。

**方法:** 提出了一个名为低秩外推（Low-Rank Extrapolation, LoX）的新方法，该方法不涉及进一步训练，而是通过外推已对齐LLM的安全子空间来增强其安全性。

**结果:** 实验证明了LoX的有效性，面对良性或恶意微调攻击时，攻击成功率绝对减少了11%到54%。此外，通过参数的攻击成功率景观研究发现，LoX成功的原因在于它将LLM参数移动到了一个更平坦的区域，从而降低了对扰动的敏感度。

**结论:** LoX是一种有效的、无需训练的方法，可以增强LLM的安全鲁棒性，对抗良性及恶意微调攻击，并且不影响模型执行新任务的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LoX%3A+Low-Rank+Extrapolation+Robustifies+LLM+Safety+Against+Fine-tuning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15606&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have become indispensable in real-world
applications. However, their widespread adoption raises significant safety
concerns, particularly in responding to socially harmful questions. Despite
substantial efforts to improve model safety through alignment, aligned models
can still have their safety protections undermined by subsequent fine-tuning -
even when the additional training data appears benign. In this paper, we
empirically demonstrate that this vulnerability stems from the sensitivity of
safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building
on this insight, we propose a novel training-free method, termed Low-Rank
Extrapolation (LoX), to enhance safety robustness by extrapolating the safety
subspace of an aligned LLM. Our experimental results confirm the effectiveness
of LoX, demonstrating significant improvements in robustness against both
benign and malicious fine-tuning attacks while preserving the model's
adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute
reductions in attack success rates (ASR) facing benign or malicious fine-tuning
attacks. By investigating the ASR landscape of parameters, we attribute the
success of LoX to that the extrapolation moves LLM parameters to a flatter
zone, thereby less sensitive to perturbations. The code is available at
github.com/VITA-Group/LoX.

</details>


### [66] [GFLC: Graph-based Fairness-aware Label Correction for Fair Classification](https://arxiv.org/abs/2506.15620)
*Modar Sulaiman, Kallol Roy*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于图的公平性感知标签校正方法（GFLC），用于在保持数据集人口统计学平等的同时有效纠正标签噪声，实验结果表明该方法在性能与公平性指标之间取得了显著改进。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能系统对社会各个方面的影响力日益增强，机器学习中的公平性对于构建可信系统至关重要。然而，训练和发展去偏技术所使用的数据往往包含有偏见和噪声的标签，这会影响模型性能并导致测试时分类器公平性的失真。

**方法:** 本文介绍的方法是Graph-based Fairness-aware Label Correction (GFLC)，它结合了三个关键组成部分：预测置信度衡量、通过Ricci流优化的图拉普拉斯算子进行的图正则化以及明确的人口统计学平等激励机制。

**结果:** 实验结果证明了所提方法的有效性，并且相较于基线，在性能与公平性指标之间的权衡上显示出显著改善。

**结论:** 本研究提出的GFLC方法为解决带有偏见和噪声标签的数据集中的公平性问题提供了一个有效的解决方案，并且在维持一定水平的模型性能的同时能够提升算法的公平性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GFLC%3A+Graph-based+Fairness-aware+Label+Correction+for+Fair+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15620，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15620&send_immediately=true&force_search=false)

**原文摘要:** Fairness in machine learning (ML) has a critical importance for building
trustworthy machine learning system as artificial intelligence (AI) systems
increasingly impact various aspects of society, including healthcare decisions
and legal judgments. Moreover, numerous studies demonstrate evidence of unfair
outcomes in ML and the need for more robust fairness-aware methods. However,
the data we use to train and develop debiasing techniques often contains biased
and noisy labels. As a result, the label bias in the training data affects
model performance and misrepresents the fairness of classifiers during testing.
To tackle this problem, our paper presents Graph-based Fairness-aware Label
Correction (GFLC), an efficient method for correcting label noise while
preserving demographic parity in datasets. In particular, our approach combines
three key components: prediction confidence measure, graph-based regularization
through Ricci-flow-optimized graph Laplacians, and explicit demographic parity
incentives. Our experimental findings show the effectiveness of our proposed
approach and show significant improvements in the trade-off between performance
and fairness metrics compared to the baseline.

</details>


### [67] [DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for Optimizing Large-Scale EV Charging Infrastructure](https://arxiv.org/abs/2506.15289)
*Chuan Li, Shunyu Zhao, Vincent Gauthier, Hassine Moungla*

**主要类别:** cs.LG

**AI概要:** 为应对电动汽车快速增长的需求，提出了一种名为DOVA-PATBM的地理计算框架，该框架能够整合不同类型地区的充电设施规划，并在美国佐治亚州的应用中展示了其在覆盖范围、可达性和电网适应性方面的改进。


<details>
  <summary>更多</summary>
  
**动机:** 随着电池电动汽车的加速普及，需要既丰富数据又具有地理可扩展性的基础设施规划工具。与大多数先前针对单个城市优化充电位置的研究不同，州级和国家级网络必须调和密集城市核心、依赖汽车的远郊以及电力受限农村走廊之间相互矛盾的需求。

**方法:** DOVA-PATBM是一个地理计算框架，它将不同情境统一在一个管道中，通过层次化的H3网格栅格化异构数据，使用区域标准化图神经网络中心性模型推断交叉口重要性，并叠加一个Voronoi镶嵌以确保每个30公里半径内至少有一个五端口直流快速充电站。

**结果:** 应用于美国佐治亚州时，DOVA-PATBM (i) 将30公里瓷砖覆盖率提高了12个百分点，(ii) 使低收入居民到最近充电站的平均距离减半，(iii) 在所有地方都满足次输电容量限制——同时保持全国规模推广的计算可行性。

**结论:** 研究结果表明，紧密集成的、由图神经网络驱动的多分辨率方法可以弥合学术优化与可部署基础设施政策之间的差距。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DOVA-PATBM%3A+An+Intelligent%2C+Adaptive%2C+and+Scalable+Framework+for+Optimizing+Large-Scale+EV+Charging+Infrastructure，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15289，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15289&send_immediately=true&force_search=false)

**原文摘要:** The accelerating uptake of battery-electric vehicles demands infrastructure
planning tools that are both data-rich and geographically scalable. Whereas
most prior studies optimise charging locations for single cities, state-wide
and national networks must reconcile the conflicting requirements of dense
metropolitan cores, car-dependent exurbs, and power-constrained rural
corridors.
  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,
Adaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework
that unifies these contexts in a single pipeline. The method rasterises
heterogeneous data (roads, population, night lights, POIs, and feeder lines)
onto a hierarchical H3 grid, infers intersection importance with a
zone-normalised graph neural network centrality model, and overlays a Voronoi
tessellation that guarantees at least one five-port DC fast charger within
every 30 km radius. Hourly arrival profiles, learned from loop-detector and
floating-car traces, feed a finite M/M/c queue to size ports under
feeder-capacity and outage-risk constraints. A greedy maximal-coverage
heuristic with income-weighted penalties then selects the minimum number of
sites that satisfy coverage and equity targets.
  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile
coverage by 12 percentage points, (ii) halves the mean distance that low-income
residents travel to the nearest charger, and (iii) meets sub-transmission
headroom everywhere -- all while remaining computationally tractable for
national-scale roll-outs. These results demonstrate that a tightly integrated,
GNN-driven, multi-resolution approach can bridge the gap between academic
optimisation and deployable infrastructure policy.

</details>


### [68] [Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction](https://arxiv.org/abs/2506.15626)
*Vincent Roca, Marc Tommasi, Paul Andrey, Aurélien Bellet, Markus D. Schirmer, Hilde Henon, Laurent Puy, Julien Ramon, Grégory Kuchcinski, Martin Bretzner, Renaud Lopes*

**主要类别:** cs.LG

**AI概要:** 本研究通过使用联邦学习（FL）来评估缺血性中风患者脑龄估计的性能，并探讨其与临床表型和功能结果之间的关联。结果显示，尽管集中式学习提供了最准确的预测，但联邦学习始终优于单站点模型。脑龄与糖尿病等血管危险因素以及中风后恢复显著相关，表明它在中风护理中的预后建模潜力。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在解决训练稳健脑龄模型所需的大数据集问题，这些问题往往受到隐私限制。为了克服这一障碍，该研究探索了联邦学习在不进行数据集中化的情况下对脑龄估计的有效性及其与临床特征和功能性结果的相关性。

**方法:** 研究人员利用来自16个医院中心的1674名中风患者的FLAIR脑部影像，实施了标准机器学习和深度学习模型来进行脑龄估计。实验采用了三种数据管理策略：集中式学习（合并数据）、联邦学习（各站点本地训练）和单一站点学习。

**结果:** 集中式学习提供了最准确的脑龄预测，但联邦学习的表现始终优于单一站点模型。所有模型中，患有糖尿病的患者的脑龄明显更高。此外，脑龄与良好和较差的功能结果之间存在显著差异，并且多变量预测显示了脑龄与中风后恢复之间的重要联系。

**结论:** 联邦学习能够在不需将数据集中化的条件下实现精确的年龄预测。脑龄与血管风险因子及中风后康复之间存在着密切关系，这突显了它在中风照护领域作为预后模型构建工具的巨大潜力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Federated+Learning+for+MRI-based+BrainAGE%3A+a+multicenter+study+on+post-stroke+functional+outcome+prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15626，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15626&send_immediately=true&force_search=false)

**原文摘要:** $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a
neuroimaging biomarker reflecting brain health. However, training robust
BrainAGE models requires large datasets, often restricted by privacy concerns.
This study evaluates the performance of federated learning (FL) for BrainAGE
estimation in ischemic stroke patients treated with mechanical thrombectomy,
and investigates its association with clinical phenotypes and functional
outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients
across 16 hospital centers. We implemented standard machine learning and deep
learning models for BrainAGE estimates under three data management strategies:
centralized learning (pooled data), FL (local training at each site), and
single-site learning. We reported prediction errors and examined associations
between BrainAGE and vascular risk factors (e.g., diabetes mellitus,
hypertension, smoking), as well as functional outcomes at three months
post-stroke. Logistic regression evaluated BrainAGE's predictive value for
these outcomes, adjusting for age, sex, vascular risk factors, stroke severity,
time between MRI and arterial puncture, prior intravenous thrombolysis, and
recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate
predictions, FL consistently outperformed single-site models. BrainAGE was
significantly higher in patients with diabetes mellitus across all models.
Comparisons between patients with good and poor functional outcomes, and
multivariate predictions of these outcomes showed the significance of the
association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data
centralization. The strong association between BrainAGE, vascular risk factors,
and post-stroke recovery highlights its potential for prognostic modeling in
stroke care.

</details>


### [69] [Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance](https://arxiv.org/abs/2506.15305)
*Qingkai Zhang, L. Jeff Hong, Houmin Yan*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于第三方物流(3PL)主导的供应链金融(SCF)的先进信用风险管理框架，通过量化回归生成元模型(QRGMM)来评估风险，并结合深度分解机(DeepFM)处理电子商务销售数据中的复杂协变量交互作用。实验结果表明该模型在信贷风险评估和贷款额度确定方面有效，为提高中小企业(SMEs)融资提供了坚实基础。


<details>
  <summary>更多</summary>
  
**动机:** 跨境电商（CBEC）迅速发展为中小企业带来了巨大机遇，但由于这些企业通常缺乏足够的信用记录，在融资上面临重大挑战。因此，需要一种新的解决方案来帮助中小企业利用其在途库存作为抵押品进行融资。

**方法:** 研究者提出了一个针对3PL主导SCF设计的高级信用风险管理框架，它采用QRGMM对销售分布建模以估计风险，并引入了功能性风险度量公式，用以系统地捕捉不同贷款水平下的多种风险度量间的关系。此外，为了更好地处理电商销售数据中复杂的协变量相互作用，还将QRGMM与DeepFM相结合。

**结果:** 通过合成数据集和真实世界的数据集上的广泛实验验证了所提模型在信用风险评估及贷款规模确定方面的有效性。

**结论:** 本研究表明，通过应用生成式人工智能技术于CBEC SCF风险管理领域，能够显著改善信用实践并增加中小企业的资本获取机会。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Conditional+Generative+Modeling+for+Enhanced+Credit+Risk+Management+in+Supply+Chain+Finance，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15305，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15305&send_immediately=true&force_search=false)

**原文摘要:** The rapid expansion of cross-border e-commerce (CBEC) has created significant
opportunities for small and medium-sized enterprises (SMEs), yet financing
remains a critical challenge due to SMEs' limited credit histories. Third-party
logistics (3PL)-led supply chain finance (SCF) has emerged as a promising
solution, leveraging in-transit inventory as collateral. We propose an advanced
credit risk management framework tailored for 3PL-led SCF, addressing the dual
challenges of credit risk assessment and loan size determination. Specifically,
we leverage conditional generative modeling of sales distributions through
Quantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for
risk estimation. We propose a unified framework that enables flexible
estimation of multiple risk measures while introducing a functional risk
measure formulation that systematically captures the relationship between these
risk measures and varying loan levels, supported by theoretical guarantees. To
capture complex covariate interactions in e-commerce sales data, we integrate
QRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on
synthetic and real-world data validate the efficacy of our model for credit
risk assessment and loan size determination. This study represents a pioneering
application of generative AI in CBEC SCF risk management, offering a solid
foundation for enhanced credit practices and improved SME access to capital.

</details>


### [70] [AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning](https://arxiv.org/abs/2506.15651)
*Tevin Wang, Chenyan Xiong*

**主要类别:** cs.LG

**AI概要:** 介绍了一种名为AutoRule的全自动化方法，它可以从偏好反馈中提取规则并将其转化为基于规则的奖励。该方法在AlpacaEval2.0上实现了长度控制胜率相对提高了28.6%，并且在一个保留的MT-Bench子集上的第二轮表现上获得了6.1%的相对增益。


<details>
  <summary>更多</summary>
  
**动机:** 当前基于规则奖励的方法通常依赖于手动规则工程，而AutoRule旨在提供一种从人类反馈中自动生成规则以改进强化学习的策略。

**方法:** AutoRule通过三个阶段操作：使用推理模型来解释用户偏好、从这些解释的推理链中识别候选规则，并将它们综合成一个统一的规则集。然后利用最终确定的规则集，通过语言模型验证器计算每个输出满足规则的比例，并将此度量作为辅助奖励与学习到的奖励模型一起用于策略优化。

**结果:** 使用AutoRule训练Llama-3-8B模型相比仅使用学习奖励模型的GRPO基线，在AlpacaEval2.0上实现了长度控制胜率28.6%的相对提升，以及在保留的MT-Bench子集上的第二回合表现上取得了6.1%的相对收益。

**结论:** 分析表明，所提取的规则与数据集偏好有良好的一致性，并且相比于纯学习奖励模型，AutoRule在两个周期内表现出更少的奖励漏洞。案例研究还表明，提取出的规则能够捕捉到不同数据集中被重视的独特品质。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoRule%3A+Reasoning+Chain-of-thought+Extracted+Rule-based+Rewards+Improve+Preference+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15651，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15651&send_immediately=true&force_search=false)

**原文摘要:** Rule-based rewards offer a promising strategy for improving reinforcement
learning from human feedback (RLHF), but current approaches often rely on
manual rule engineering. We present AutoRule, a fully automated method for
extracting rules from preference feedback and formulating them into rule-based
rewards. AutoRule extraction operates in three stages: it leverages a reasoning
model to interpret user preferences, identifies candidate rules from the
reasoning chain of these interpretations, and synthesizes them into a unified
rule set. Leveraging the finalized rule set, we employ language-model verifiers
to compute the fraction of rules satisfied by each output, using this metric as
an auxiliary reward alongside the learned reward model during policy
optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\%
relative improvement in length-controlled win rate on AlpacaEval2.0, and a
6.1\% relative gain in second-turn performance on a held-out MT-Bench subset,
compared to a GRPO baseline trained with the same learned reward model but
without the rule-based auxiliary reward. Our analysis confirms that the
extracted rules exhibit good agreement with dataset preference. We find that
AutoRule demonstrates reduced reward hacking compared to a learned reward model
when run over two episodes. Finally, our case study suggests that the extracted
rules capture unique qualities valued in different datasets. The extracted
rules are provided in the appendix, and the code is open-sourced at
https://github.com/cxcscmu/AutoRule.

</details>


### [71] [SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language Models Using Forward-Only Passes](https://arxiv.org/abs/2506.15307)
*Jinglong Luo, Zhuo Zhang, Yehong Zhang, Shiyu Liu, Ye Dong, Xun Zhou, Hui Wang, Yue Yu, Zenglin Xu*

**主要类别:** cs.LG

**AI概要:** 本文提出了SecFwT，这是一种基于安全多方计算（MPC）的框架，旨在高效且隐私保护地对大型语言模型（LLMs）进行微调。它通过仅前向调整范式和使用MPC友好的随机特征注意力机制来解决现有方法中的计算开销问题和softmax注意力机制的效率低下问题。实验结果显示了在效率和隐私保护方面的显著改进。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在多个领域产生了变革，但在医疗保健和金融等敏感领域的应用受到了严格隐私要求导致的可用训练数据稀缺性的限制。尽管基于安全多方计算（MPC）的隐私保护机器学习能够保护模型参数和用户数据，但其应用于大型语言模型时主要限于推理阶段，因为微调过程引入了显著的计算挑战，特别是在隐私保护反向传播和优化器操作方面。

**方法:** 为了克服这些挑战，研究者们提出了SecFwT，这是首个专为高效、隐私保护的大型语言模型微调设计的MPC框架。SecFwT采用了一种只包含前向传播的调节模式，以消除反向传播及优化器相关的计算，并利用MPC友好的随机特征注意机制来近似softmax注意机制，从而大幅减少昂贵的非线性运算和计算复杂度。

**结果:** 实验结果表明，SecFwT在效率和隐私保护方面实现了重大改进，使得针对隐私至关重要的应用场景下的大规模安全微调成为可能。

**结论:** SecFwT作为一个基于MPC的新型框架，成功解决了大型语言模型在隐私保护微调过程中遇到的关键障碍，包括高昂的计算成本以及softmax注意力机制在MPC环境中的低效性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SecFwT%3A+Efficient+Privacy-Preserving+Fine-Tuning+of+Large+Language+Models+Using+Forward-Only+Passes，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15307，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15307&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) have transformed numerous fields, yet their
adaptation to specialized tasks in privacy-sensitive domains, such as
healthcare and finance, is constrained by the scarcity of accessible training
data due to stringent privacy requirements. Secure multi-party computation
(MPC)-based privacy-preserving machine learning offers a powerful approach to
protect both model parameters and user data, but its application to LLMs has
been largely limited to inference, as fine-tuning introduces significant
computational challenges, particularly in privacy-preserving backward
propagation and optimizer operations. This paper identifies two primary
obstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the
substantial computational overhead of backward and optimizer processes, and (2)
the inefficiency of softmax-based attention mechanisms in MPC settings. To
address these challenges, we propose SecFwT, the first MPC-based framework
designed for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a
forward-only tuning paradigm to eliminate backward and optimizer computations
and employs MPC-friendly Random Feature Attention to approximate softmax
attention, significantly reducing costly non-linear operations and
computational complexity. Experimental results demonstrate that SecFwT delivers
substantial improvements in efficiency and privacy preservation, enabling
scalable and secure fine-tuning of LLMs for privacy-critical applications.

</details>


### [72] [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679)
*Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark*

**主要类别:** cs.LG

**AI概要:** 研究发现，稀疏自编码器（SAEs）中频繁激活的密集潜在变量不仅是模型固有的特性，而且通常反映了有意义的模型表示。这些密集潜在变量在语言模型计算中发挥着功能性作用，不应被简单视为训练噪声。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于探索稀疏自编码器（SAE）生成的密集潜在变量是否为训练过程中的不希望出现的人工产物，以及它们是否具有语义意义。

**方法:** 研究人员系统地调查了密集潜在变量的几何结构、功能和起源，并展示了它们不仅持久存在而且经常反映有意义的模型表示。通过展示密集潜在变量倾向于形成重建残差流特定方向的对极对，以及消除它们的子空间如何抑制新密集特征在重新训练的SAE中出现来支持这一观点。此外，引入了一个密集潜在变量分类法，识别与位置跟踪、上下文绑定等功能相关的类别。最后，分析了这些特征在不同层之间的演变。

**结果:** 结果表明，密集潜在变量是残差空间的一个内在属性，它们在语言模型的不同层次上从结构特征转变为语义特征，再到最终层次上的输出导向信号。

**结论:** 结论是密集潜在变量在语言模型计算中扮演着功能性角色，应该被视为重要的组成部分而不是简单的训练噪音。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dense+SAE+Latents+Are+Features%2C+Not+Bugs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15679，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15679&send_immediately=true&force_search=false)

**原文摘要:** Sparse autoencoders (SAEs) are designed to extract interpretable features
from language models by enforcing a sparsity constraint. Ideally, training an
SAE would yield latents that are both sparse and semantically meaningful.
However, many SAE latents activate frequently (i.e., are \emph{dense}), raising
concerns that they may be undesirable artifacts of the training procedure. In
this work, we systematically investigate the geometry, function, and origin of
dense latents and show that they are not only persistent but often reflect
meaningful model representations. We first demonstrate that dense latents tend
to form antipodal pairs that reconstruct specific directions in the residual
stream, and that ablating their subspace suppresses the emergence of new dense
features in retrained SAEs -- suggesting that high density features are an
intrinsic property of the residual space. We then introduce a taxonomy of dense
latents, identifying classes tied to position tracking, context binding,
entropy regulation, letter-specific output signals, part-of-speech, and
principal component reconstruction. Finally, we analyze how these features
evolve across layers, revealing a shift from structural features in early
layers, to semantic features in mid layers, and finally to output-oriented
signals in the last layers of the model. Our findings indicate that dense
latents serve functional roles in language model computation and should not be
dismissed as training noise.

</details>


### [73] [Universal Laboratory Model: prognosis of abnormal clinical outcomes based on routine tests](https://arxiv.org/abs/2506.15330)
*Pavel Karpov, Ilya Petrenkov, Ruslan Raiman*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种将表格建模问题转化为集合翻译问题的方法，利用GPT-like标签列嵌入及其对应值作为源集，目标集为相同类型的嵌入。该方法能够有效处理缺失值，并且无需隐式估计它们。当应用于临床实验室数据时，对于尿酸、血糖、胆固醇偏高以及铁蛋白水平偏低的联合预测，AUC提高了高达8%。


<details>
  <summary>更多</summary>
  
**动机:** 临床实验室结果在任何诊断过程中都是普遍存在的。基于已执行测试的结果来预测未被要求的测试异常值是很有吸引力的，因为这可以让早期诊断变得普及。CBC（全血细胞计数）测试是最广泛使用的临床程序之一，将其与常规生化面板结合使用会产生因患者而异的测试-值对集合，或者说是带有缺失值的数据表。

**方法:** 作者将表格建模问题定义为一个集合翻译问题，其中源集合由GPT-like标签列嵌入及其对应的值组成，目标集合则仅包含同类型的嵌入。这种新方法能够在不隐含地估算缺失值的情况下有效地处理这些缺失值，并将大型语言模型的世界与表格领域连接起来。

**结果:** 通过将此方法应用于临床实验室数据，研究者们实现了对于尿酸、血糖和胆固醇水平升高以及铁蛋白水平降低的联合预测，在AUC指标上达到了最高8%的提升。

**结论:** 提出的表格建模方法作为一种集合翻译问题，成功地解决了缺失值的问题，并且在不需要预估缺失值的前提下提升了临床实验室数据中特定生物标志物异常值的预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Universal+Laboratory+Model%3A+prognosis+of+abnormal+clinical+outcomes+based+on+routine+tests，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15330，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15330&send_immediately=true&force_search=false)

**原文摘要:** Clinical laboratory results are ubiquitous in any diagnosis making.
Predicting abnormal values of not prescribed tests based on the results of
performed tests looks intriguing, as it would be possible to make early
diagnosis available to everyone. The special place is taken by the Common Blood
Count (CBC) test, as it is the most widely used clinical procedure. Combining
routine biochemical panels with CBC presents a set of test-value pairs that
varies from patient to patient, or, in common settings, a table with missing
values. Here we formulate a tabular modeling problem as a set translation
problem where the source set comprises pairs of GPT-like label column embedding
and its corresponding value while the target set consists of the same type
embeddings only. The proposed approach can effectively deal with missing values
without implicitly estimating them and bridges the world of LLM with the
tabular domain. Applying this method to clinical laboratory data, we achieve an
improvement up to 8% AUC for joint predictions of high uric acid, glucose,
cholesterol, and low ferritin levels.

</details>


### [74] [Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations](https://arxiv.org/abs/2506.15337)
*Naoki Matsumura, Yuta Yoshimoto, Yuto Iwasaki, Meguru Yamazaki, Yasufumi Sakai*

**主要类别:** cs.LG

**AI概要:** 提出了一种新的知识蒸馏框架，利用未经微调的预训练神经网络势能作为教师模型来生成包含高能结构的训练数据，从而提高分子动力学模拟的稳定性和准确性。


<details>
  <summary>更多</summary>
  
**动机:** 传统的方法在材料特定模型中进行微调时会增加能量壁垒，使得难以创建包含高能结构的训练数据，这对精确和稳定的分子动力学（MD）模拟至关重要。

**方法:** 提出了一种新知识蒸馏框架，首先使用现成的未经微调的预训练NNP作为教师模型生成训练数据集，然后用较小规模但高精度的密度泛函理论（DFT）数据集对学习者NNP进行微调。

**结果:** 该方法应用于有机（聚乙二醇）和无机（L10GeP2S12）材料，并且与现有方法相比，在再现物理特性方面达到了相当或更优的准确性。此外，与现有的NNP生成方法相比，该方法减少了10倍昂贵的DFT计算量，而没有牺牲准确性。

**结论:** 新提出的知识蒸馏框架能够有效提升分子动力学模拟中的稳定性及准确性，同时显著降低了成本高昂的DFT计算需求。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Knowledge+Distillation+Framework+for+Accelerating+High-Accuracy+Neural+Network-Based+Molecular+Dynamics+Simulations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15337，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15337&send_immediately=true&force_search=false)

**原文摘要:** Neural network potentials (NNPs) offer a powerful alternative to traditional
force fields for molecular dynamics (MD) simulations. Accurate and stable MD
simulations, crucial for evaluating material properties, require training data
encompassing both low-energy stable structures and high-energy structures.
Conventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as
a teacher model to generate training data for a student model. However, in
material-specific models, this fine-tuning process increases energy barriers,
making it difficult to create training data containing high-energy structures.
To address this, we propose a novel KD framework that leverages a
non-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy
landscape facilitates the exploration of a wider range of structures, including
the high-energy structures crucial for stable MD simulations. Our framework
employs a two-stage training process: first, the student NNP is trained with a
dataset generated by the off-the-shelf teacher; then, it is fine-tuned with a
smaller, high-accuracy density functional theory (DFT) dataset. We demonstrate
the effectiveness of our framework by applying it to both organic (polyethylene
glycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving
comparable or superior accuracy in reproducing physical properties compared to
existing methods. Importantly, our method reduces the number of expensive DFT
calculations by 10x compared to existing NNP generation methods, without
sacrificing accuracy.

</details>


### [75] [Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges](https://arxiv.org/abs/2506.15346)
*A. S. Stankevich, I. B. Petrov*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种基于条件Image-to-Image Schrödinger Bridge (cI^2SB) 的新方法，用于改进声波全波形反演（FWI）中的速度模型重建。该方法在生成高保真度样本方面优于先前的条件扩散模型，并且只需要少量神经函数评估即可达到监督学习方法所无法比拟的样本保真度。


<details>
  <summary>更多</summary>
  
**动机:** 传统全波形反演（FWI）方法和其他基于深度学习的解决方案难以生成高分辨率样本，而基于扩散模型的方法虽然能够产生高质量样本，但迭代和随机采样过程以及输出控制的启发式性质限制了其应用。尤其是如何将近似速度模型有效地纳入基于扩散模型的反演方案中仍不明确。

**方法:** 作者引入Schrödinger Bridge来插值真实分布与平滑速度模型之间的差异，并通过扩展Image-to-Image Schrödinger Bridge的概念到条件采样，构建了一个条件Image-to-Image Schrödinger Bridge (cI^2SB) 框架。

**结果:** 实验结果表明，所提出的方法比重新实现的早期工作中的条件扩散模型表现更好，而且仅需几次神经函数评估就能得到比监督学习方法更高的样本保真度。

**结论:** 研究者提出的cI^2SB框架为声波全波形反演提供了更高效、更准确的速度模型重建手段。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Acoustic+Waveform+Inversion+with+Image-to-Image+Schr%C3%B6dinger+Bridges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15346，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15346&send_immediately=true&force_search=false)

**原文摘要:** Recent developments in application of deep learning models to acoustic Full
Waveform Inversion (FWI) are marked by the use of diffusion models as prior
distributions for Bayesian-like inference procedures. The advantage of these
methods is the ability to generate high-resolution samples, which are otherwise
unattainable with classical inversion methods or other deep learning-based
solutions. However, the iterative and stochastic nature of sampling from
diffusion models along with heuristic nature of output control remain limiting
factors for their applicability. For instance, an optimal way to include the
approximate velocity model into diffusion-based inversion scheme remains
unclear, even though it is considered an essential part of FWI pipeline. We
address the issue by employing a Schr\"odinger Bridge that interpolates between
the distributions of ground truth and smoothed velocity models. To facilitate
the learning of nonlinear drifts that transfer samples between distributions we
extend the concept of Image-to-Image Schr\"odinger Bridge
($\text{I}^2\text{SB}$) to conditional sampling, resulting in a conditional
Image-to-Image Schr\"odinger Bridge (c$\text{I}^2\text{SB}$) framework. To
validate our method, we assess its effectiveness in reconstructing the
reference velocity model from its smoothed approximation, coupled with the
observed seismic signal of fixed shape. Our experiments demonstrate that the
proposed solution outperforms our reimplementation of conditional diffusion
model suggested in earlier works, while requiring only a few neural function
evaluations (NFEs) to achieve sample fidelity superior to that attained with
supervised learning-based approach. The supplementary code implementing the
algorithms described in this paper can be found in the repository
https://github.com/stankevich-mipt/seismic_inversion_via_I2SB.

</details>


### [76] [Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership Inference](https://arxiv.org/abs/2506.15349)
*Terrance Liu, Matteo Boglioni, Yiwei Fu, Shengyuan Hu, Pratiksha Thaker, Zhiwei Steven Wu*

**主要类别:** cs.LG

**AI概要:** 本文研究了如何通过结合更强的成员推理攻击（MIA）方法来提高黑盒环境下的一次运行审计效率，特别是在使用DP-SGD训练的CIFAR-10图像分类模型上，利用分位数回归进行MIA，从而在保持计算效率的同时获得更紧致的经验下界。


<details>
  <summary>更多</summary>
  
**动机:** 在实际应用中，尤其是在黑盒环境下，现有的一次运行差分隐私审计方法在经验下界与理论上界之间存在较大差距。因此，研究旨在探索如何通过增强成员推理攻击（MIA）的方法来改善这一情况。

**方法:** 研究者提出了一种新的方法，该方法利用分位数回归来进行成员推理攻击（MIA），并将其应用于黑盒环境中基于DP-SGD训练的CIFAR-10图像分类模型上的一次运行审计。

**结果:** 实验结果显示，所提出的方法能够在保持一次运行审计方法计算效率的同时，对于使用DP-SGD训练的CIFAR-10图像分类模型提供更为紧致的差分隐私保证的经验下界。

**结论:** 结合了分位数回归用于成员推理攻击的新方法，在不牺牲计算效率的前提下，提高了黑盒环境下的差分隐私审计效果。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+One-run+Privacy+Auditing+with+Quantile+Regression-Based+Membership+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15349，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15349&send_immediately=true&force_search=false)

**原文摘要:** Differential privacy (DP) auditing aims to provide empirical lower bounds on
the privacy guarantees of DP mechanisms like DP-SGD. While some existing
techniques require many training runs that are prohibitively costly, recent
work introduces one-run auditing approaches that effectively audit DP-SGD in
white-box settings while still being computationally efficient. However, in the
more practical black-box setting where gradients cannot be manipulated during
training and only the last model iterate is observed, prior work shows that
there is still a large gap between the empirical lower bounds and theoretical
upper bounds. Consequently, in this work, we study how incorporating approaches
for stronger membership inference attacks (MIA) can improve one-run auditing in
the black-box setting. Evaluating on image classification models trained on
CIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes
quantile regression for MIA, achieves tighter bounds while crucially
maintaining the computational efficiency of one-run methods.

</details>


### [77] [Sampling 3D Molecular Conformers with Diffusion Transformers](https://arxiv.org/abs/2506.15378)
*J. Thorben Frank, Winfried Ripken, Gregor Lied, Klaus-Robert Müller, Oliver T. Unke, Stefan Chmiela*

**主要类别:** cs.LG

**AI概要:** 本文提出了DiTMC框架，该框架通过模块化架构适应了将Diffusion Transformers应用于分子构象生成时遇到的挑战。实验表明DiTMC在标准构象生成基准上达到了最先进的精度和物理有效性。


<details>
  <summary>更多</summary>
  
**动机:** Diffusion Transformers在生成建模中表现出色，特别是在图像合成方面，但将其应用于分子时引入了新的挑战，例如离散分子图信息与连续3D几何形状的整合、处理欧几里得对称性以及设计能够跨不同大小和结构的分子通用的条件机制。

**方法:** 作者提出了一种名为DiTMC的框架，该框架通过分离3D坐标处理和原子连接性的条件作用来解决这些挑战，并引入了两种互补的基于图的条件策略，它们可以无缝集成到DiTMC架构中。此外，还结合了不同的注意力机制，包括标准非等变和SO(3)-等变形式，以灵活控制准确性和计算效率之间的权衡。

**结果:** 在标准的构象生成基准测试（GEOM-QM9, -DRUGS, -XL）上的实验表明，DiTMC实现了最先进水平的精确度和物理有效性。结果强调了架构选择和对称先验如何影响样本质量和效率。

**结论:** 研究结果突出了架构选择和对称先验对样本质量和效率的影响，并为大规模分子结构生成模型提供了有希望的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sampling+3D+Molecular+Conformers+with+Diffusion+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15378，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15378&send_immediately=true&force_search=false)

**原文摘要:** Diffusion Transformers (DiTs) have demonstrated strong performance in
generative modeling, particularly in image synthesis, making them a compelling
choice for molecular conformer generation. However, applying DiTs to molecules
introduces novel challenges, such as integrating discrete molecular graph
information with continuous 3D geometry, handling Euclidean symmetries, and
designing conditioning mechanisms that generalize across molecules of varying
sizes and structures. We propose DiTMC, a framework that adapts DiTs to address
these challenges through a modular architecture that separates the processing
of 3D coordinates from conditioning on atomic connectivity. To this end, we
introduce two complementary graph-based conditioning strategies that integrate
seamlessly with the DiT architecture. These are combined with different
attention mechanisms, including both standard non-equivariant and
SO(3)-equivariant formulations, enabling flexible control over the trade-off
between between accuracy and computational efficiency. Experiments on standard
conformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC
achieves state-of-the-art precision and physical validity. Our results
highlight how architectural choices and symmetry priors affect sample quality
and efficiency, suggesting promising directions for large-scale generative
modeling of molecular structures. Code available at
https://github.com/ML4MolSim/dit_mc.

</details>


### [78] [Global Ground Metric Learning with Applications to scRNA data](https://arxiv.org/abs/2506.15383)
*Damin Kühn, Michael T. Schaub*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的方法来学习共享度量空间上任意分布的全局地面度量，该方法仅需要分布级别的类别标签进行训练，能够提高最优传输距离计算的准确性，并在嵌入、聚类和分类任务中表现更优。


<details>
  <summary>更多</summary>
  
**动机:** 传统的地面度量要么是预定义的，如欧几里得距离，要么是通过监督方式利用标记数据学习得到的。然而，预定义的度量通常不能考虑到数据中不同特征内在结构和变化的重要性，现有的地面度量学习监督方法往往不能跨多个类别泛化，或者局限于支持集相同的分布。为了解决这些限制，本文提出了一个新方法。

**方法:** 所提出的方法提供了一个类似于全局度量的距离，但只需要分布级别的类别标签来进行训练。该方法能够学习到对于任意分布在共享度量空间上的全局地面度量。

**结果:** 通过使用涵盖多种疾病的单细胞RNA测序数据，展示了所提方法的有效性和可解释性，在嵌入、聚类和分类任务中的性能得到了提升。

**结论:** 学习到的全局地面度量使得最优传输距离更加准确，从而改善了嵌入、聚类和分类任务的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Global+Ground+Metric+Learning+with+Applications+to+scRNA+data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15383&send_immediately=true&force_search=false)

**原文摘要:** Optimal transport provides a robust framework for comparing probability
distributions. Its effectiveness is significantly influenced by the choice of
the underlying ground metric. Traditionally, the ground metric has either been
(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a
supervised way, by utilizing labeled data to learn a suitable ground metric for
enhanced task-specific performance. Yet, predefined metrics typically cannot
account for the inherent structure and varying importance of different features
in the data, and existing supervised approaches to ground metric learning often
do not generalize across multiple classes or are restricted to distributions
with shared supports. To address these limitations, we propose a novel approach
for learning metrics for arbitrary distributions over a shared metric space.
Our method provides a distance between individual points like a global metric,
but requires only class labels on a distribution-level for training. The
learned global ground metric enables more accurate optimal transport distances,
leading to improved performance in embedding, clustering and classification
tasks. We demonstrate the effectiveness and interpretability of our approach
using patient-level scRNA-seq data spanning multiple diseases.

</details>


### [79] [Provable Maximum Entropy Manifold Exploration via Diffusion Models](https://arxiv.org/abs/2506.15385)
*Riccardo De Santi, Marin Vlastelica, Ya-Ping Hsieh, Zebang Shen, Niao He, Andreas Krause*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的探索框架，通过最大化预训练扩散模型定义的数据流形上的熵来实现探索，并基于密度估计提出了新的探索原则。为了克服密度估计的困难并使方法具有可扩展性，利用了扩散模型诱导的密度熵与其得分函数之间的基本联系。基于此开发了一个算法，该算法将探索问题作为预训练扩散模型的连续微调来解决，并证明了在现实假设下该算法收敛于最优探索扩散模型。实验评估表明，在合成和高维文本到图像扩散上都取得了有希望的结果。


<details>
  <summary>更多</summary>
  
**动机:** 研究旨在解决真实世界决策问题中的探索挑战，如科学发现领域，目标是生成真正新颖的设计而非模仿现有数据分布。研究试图利用生成模型的表现力进行探索，而不依赖于显式的不确定性量化。

**方法:** 文章引入了一个新框架，将探索视为由预训练扩散模型隐式定义的近似数据流形上的熵最大化。接着，基于密度估计提出了一种新的探索原则。为了解决密度估计的难题并提高方法的可扩展性，研究人员利用了扩散模型诱导的密度熵与分数函数之间的基本关系。基于这一理论，他们开发了一种基于镜像下降的算法，该算法通过连续微调预训练扩散模型来解决探索问题。

**结果:** 研究人员证明了所提算法在实际假设条件下能够收敛至最优探索扩散模型。此外，通过对合成及高维文本到图像扩散任务的实证评价，展示了该方法的有效性和前景。

**结论:** 这项工作提供了一种新的方式来利用生成模型进行有效的探索，而无需依赖明确的不确定性度量。提出的基于熵最大化的探索框架和相应的算法为处理复杂数据分布下的创新设计提供了有力工具。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Provable+Maximum+Entropy+Manifold+Exploration+via+Diffusion+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15385，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15385&send_immediately=true&force_search=false)

**原文摘要:** Exploration is critical for solving real-world decision-making problems such
as scientific discovery, where the objective is to generate truly novel designs
rather than mimic existing data distributions. In this work, we address the
challenge of leveraging the representational power of generative models for
exploration without relying on explicit uncertainty quantification. We
introduce a novel framework that casts exploration as entropy maximization over
the approximate data manifold implicitly defined by a pre-trained diffusion
model. Then, we present a novel principle for exploration based on density
estimation, a problem well-known to be challenging in practice. To overcome
this issue and render this method truly scalable, we leverage a fundamental
connection between the entropy of the density induced by a diffusion model and
its score function. Building on this, we develop an algorithm based on mirror
descent that solves the exploration problem as sequential fine-tuning of a
pre-trained diffusion model. We prove its convergence to the optimal
exploratory diffusion model under realistic assumptions by leveraging recent
understanding of mirror flows. Finally, we empirically evaluate our approach on
both synthetic and high-dimensional text-to-image diffusion, demonstrating
promising results.

</details>


### [80] [Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control](https://arxiv.org/abs/2506.15397)
*Sepehr Elahi, Paula Mürmann, Patrick Thiran*

**主要类别:** cs.LG

**AI概要:** 本文针对未知传播图的SIS模型传染病爆发，提出了一种新的包含-排除学习算法来推断图结构，并为SRM问题设计了多项式时间复杂度的最优算法及一种高效的贪婪启发式算法，实验验证了这些方法在合成和真实数据上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 研究者们面对的问题是如何在疾病高度传染的情况下，通过最佳接种策略最小化疾病的灭绝时间。然而以往的工作都假设传播图是已知的，这在实际中并不总是成立。本研究旨在解决当传播图未知时，仅通过观察节点的感染状态来最小化SIS模型下传染病爆发的灭绝时间。

**方法:** 将问题分为两部分：学习图结构和确定有效的疫苗接种策略。提出了基于包含-排除原理的新颖学习算法，并且首次建立了其样本复杂度用于图恢复。接着，详细描述了一个针对SRM问题的最优算法，并证明了对于有界树宽的图，该算法运行时间是关于顶点数的多项式。此外，还提供了一个适用于任意图的高效且有效的多项式时间贪婪启发式算法。

**结果:** 研究表明新提出的包含-排除学习算法能够有效地恢复图结构，并且所设计的算法在SRM问题上具有良好的性能。同时，提供的贪婪启发式算法在任何图上都表现出色。通过合成数据和真实世界数据的实验，数值结果验证了学习算法和疫苗接种算法的有效性。

**结论:** 这项工作为未知传播网络下的SIS模型提供了有效的方法来估计网络结构并制定最优疫苗接种策略，从而有助于最小化传染病的灭绝时间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learn+to+Vaccinate%3A+Combining+Structure+Learning+and+Effective+Vaccination+for+Epidemic+and+Outbreak+Control，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15397，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15397&send_immediately=true&force_search=false)

**原文摘要:** The Susceptible-Infected-Susceptible (SIS) model is a widely used model for
the spread of information and infectious diseases, particularly non-immunizing
ones, on a graph. Given a highly contagious disease, a natural question is how
to best vaccinate individuals to minimize the disease's extinction time. While
previous works showed that the problem of optimal vaccination is closely linked
to the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that
the graph is known, which is often not the case in practice. In this work, we
consider the problem of minimizing the extinction time of an outbreak modeled
by an SIS model where the graph on which the disease spreads is unknown and
only the infection states of the vertices are observed. To this end, we split
the problem into two: learning the graph and determining effective vaccination
strategies. We propose a novel inclusion-exclusion-based learning algorithm
and, unlike previous approaches, establish its sample complexity for graph
recovery. We then detail an optimal algorithm for the SRM problem and prove
that its running time is polynomial in the number of vertices for graphs with
bounded treewidth. This is complemented by an efficient and effective
polynomial-time greedy heuristic for any graph. Finally, we present experiments
on synthetic and real-world data that numerically validate our learning and
vaccination algorithms.

</details>


### [81] [Semi-supervised Graph Anomaly Detection via Robust Homophily Learning](https://arxiv.org/abs/2506.15448)
*Guoguo Ai, Hezhe Qiao, Hui Yan, Guansong Pang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的半监督图异常检测方法RHO，通过自适应频率响应滤波器和图正常性对齐来学习具有多样同质性的正常节点模式，并在八个真实世界数据集上展示了优于现有技术的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的半监督图异常检测方法假设所有正常节点都具有相似程度的同质性，并且标记的正常节点能够很好地代表整个正常类别的同质性模式。然而，在实际应用中，正常节点可能表现出多种不同的同质性，这使得现有方法的假设不总是成立。

**方法:** 提出的方法名为鲁棒同质性学习（RHO），它包含两个新颖模块：自适应频率响应滤波器（AdaFreq）和图正常性对齐（GNA）。AdaFreq 学习一组自适应频谱滤波器，这些滤波器能够在节点属性的通道内和跨通道视图中捕捉不同频率成分。GNA 用于加强通道内和跨通道同质性表示之间的一致性，以强化两种视图中学到的正常性。

**结果:** 实验结果表明，RHO 能够有效地从一小部分正常节点中学习到多样的、通常是未被充分代表的同质性，并且在八个现实世界的 GAD 数据集上显著优于最新的竞争方法。

**结论:** RHO 提供了一种有效的方法来处理半监督图异常检测中的多样化同质性问题，通过其创新设计的组件实现了对正常节点更加准确的建模，从而提高了异常检测的准确性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semi-supervised+Graph+Anomaly+Detection+via+Robust+Homophily+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15448，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15448&send_immediately=true&force_search=false)

**原文摘要:** Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled
normal nodes to identify abnormal nodes from a large set of unlabeled nodes in
a graph. Current methods in this line posit that 1) normal nodes share a
similar level of homophily and 2) the labeled normal nodes can well represent
the homophily patterns in the normal class. However, this assumption often does
not hold well since normal nodes in a graph can exhibit diverse homophily in
real-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily
Learning, to adaptively learn such homophily patterns. RHO consists of two
novel modules, adaptive frequency response filters (AdaFreq) and graph
normality alignment (GNA). AdaFreq learns a set of adaptive spectral filters
that capture different frequency components of the labeled normal nodes with
varying homophily in the channel-wise and cross-channel views of node
attributes. GNA is introduced to enforce consistency between the channel-wise
and cross-channel homophily representations to robustify the normality learned
by the filters in the two views. Experiments on eight real-world GAD datasets
show that RHO can effectively learn varying, often under-represented, homophily
in the small normal node set and substantially outperforms state-of-the-art
competing methods. Code is available at https://github.com/mala-lab/RHO.

</details>


### [82] [Creating User-steerable Projections with Interactive Semantic Mapping](https://arxiv.org/abs/2506.15479)
*Artur André Oliveira, Mateus Espadoto, Roberto Hirata Jr., Roberto M. Cesar Jr., Alex C. Telea*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的用户引导的投影框架，用于图像和文本数据，通过多模态大型语言模型进行零样本分类，使用户能够通过自然语言提示动态指导投影，以探索数据中未明确呈现的高层次语义关系。


<details>
  <summary>更多</summary>
  
**动机:** 现有的降维技术无法探索没有直接以变量或类别标签形式提供的语义结构。

**方法:** 引入一种新颖的用户导向投影框架，适用于图像和文字数据，利用多模态大语言模型（MLLMs）进行零样本分类，从而实现可定制、可解释的数据可视化。该方法允许用户通过自然语言引导提示来动态调整投影，指定用户感兴趣的但不在数据维度中明确体现的高级语义关系。

**结果:** 通过对多个数据集的评估显示，该方法不仅提高了聚类分离度，还把降维过程转变成一个交互式、用户驱动的过程。

**结论:** 该方法在完全自动化的降维技术和以人为中心的数据探索之间架起了一座桥梁，为特定分析需求提供了灵活且适应性强的方式来定制投影。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Creating+User-steerable+Projections+with+Interactive+Semantic+Mapping，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15479，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15479&send_immediately=true&force_search=false)

**原文摘要:** Dimensionality reduction (DR) techniques map high-dimensional data into
lower-dimensional spaces. Yet, current DR techniques are not designed to
explore semantic structure that is not directly available in the form of
variables or class labels. We introduce a novel user-guided projection
framework for image and text data that enables customizable, interpretable,
data visualizations via zero-shot classification with Multimodal Large Language
Models (MLLMs). We enable users to steer projections dynamically via
natural-language guiding prompts, to specify high-level semantic relationships
of interest to the users which are not explicitly present in the data
dimensions. We evaluate our method across several datasets and show that it not
only enhances cluster separation, but also transforms DR into an interactive,
user-driven process. Our approach bridges the gap between fully automated DR
techniques and human-centered data exploration, offering a flexible and
adaptive way to tailor projections to specific analytical needs.

</details>


### [83] [Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review](https://arxiv.org/abs/2506.15506)
*Salijona Dyrmishi, Mohamed Djilani, Thibault Simonetto, Salah Ghamizi, Maxime Cordy*

**主要类别:** cs.LG

**AI概要:** 本文是对针对表格机器学习模型的对抗性攻击的首次系统文献综述，总结了关键趋势，分类了攻击策略，并讨论了其实用性和现实世界应用的考量。


<details>
  <summary>更多</summary>
  
**动机:** 尽管在计算机视觉和自然语言处理等领域对对抗性攻击进行了广泛的研究，但关于表格数据的研究仍然分散。本文旨在填补这一空白，提供一个清晰且结构化的概述，以指导未来对于表格机器学习中对抗脆弱性的理解和解决工作。

**方法:** 本文采用了系统文献回顾的方法，整理并分析了有关针对表格数据的机器学习模型的对抗性攻击的相关研究。

**结果:** 文章总结出了对抗性攻击的关键趋势，将攻击策略进行了分类，并探讨了这些策略如何应对实际应用中的考虑因素。此外还指出了当前面临的挑战和开放的研究问题。

**结论:** 通过提供一个清晰且有条理的概览，本篇综述论文意在为理解与解决表格机器学习中的对抗性漏洞指引方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Insights+on+Adversarial+Attacks+for+Tabular+Machine+Learning+via+a+Systematic+Literature+Review，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15506，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15506&send_immediately=true&force_search=false)

**原文摘要:** Adversarial attacks in machine learning have been extensively reviewed in
areas like computer vision and NLP, but research on tabular data remains
scattered. This paper provides the first systematic literature review focused
on adversarial attacks targeting tabular machine learning models. We highlight
key trends, categorize attack strategies and analyze how they address practical
considerations for real-world applicability. Additionally, we outline current
challenges and open research questions. By offering a clear and structured
overview, this review aims to guide future efforts in understanding and
addressing adversarial vulnerabilities in tabular machine learning.

</details>


### [84] [Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning](https://arxiv.org/abs/2506.15544)
*Roger Creus Castanyer, Johan Obando-Ceron, Lu Li, Pierre-Luc Bacon, Glen Berseth, Aaron Courville, Pablo Samuel Castro*

**主要类别:** cs.LG

**AI概要:** 本文探讨了深度强化学习网络在扩展时遇到的性能下降问题，通过实证分析指出非平稳性和由于次优架构选择导致的梯度病理是主要原因，并提出了一系列直接干预措施来稳定梯度流动，从而提高网络在不同深度和宽度下的表现。


<details>
  <summary>更多</summary>
  
**动机:** 深度强化学习网络在扩展时经常出现性能下降的问题，而这个问题的根本原因尚不清楚。尽管已有研究提出了改进机制，但它们往往比较复杂且未能明确指出困难背后的成因。

**方法:** 进行了一系列实证分析，表明非平稳性与次优架构选择导致的梯度问题是扩展难题的原因。提出直接干预措施来稳定梯度流。

**结果:** 所提出的干预措施简单易行，与现有算法兼容，能够在大规模下有效提升性能。这些发现在多种智能体和环境套件上得到了验证。

**结论:** 该研究表明，通过采取适当的干预措施解决梯度流动问题，可以显著改善深度强化学习网络在扩大规模时的表现。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+Gradients+for+Stable+Learning+at+Scale+in+Deep+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15544&send_immediately=true&force_search=false)

**原文摘要:** Scaling deep reinforcement learning networks is challenging and often results
in degraded performance, yet the root causes of this failure mode remain poorly
understood. Several recent works have proposed mechanisms to address this, but
they are often complex and fail to highlight the causes underlying this
difficulty. In this work, we conduct a series of empirical analyses which
suggest that the combination of non-stationarity with gradient pathologies, due
to suboptimal architectural choices, underlie the challenges of scale. We
propose a series of direct interventions that stabilize gradient flow, enabling
robust performance across a range of network depths and widths. Our
interventions are simple to implement and compatible with well-established
algorithms, and result in an effective mechanism that enables strong
performance even at large scales. We validate our findings on a variety of
agents and suites of environments.

</details>


### [85] [Task-Agnostic Experts Composition for Continual Learning](https://arxiv.org/abs/2506.15566)
*Luigi Quarantiello, Andrea Cossu, Vincenzo Lomonaco*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种通过组合零样本专家模型来提高神经网络组成性的方法，这种方法在具有挑战性的基准测试中取得了比基线算法更高的准确率，并且需要较少的计算资源。


<details>
  <summary>更多</summary>
  
**动机:** 人类推理过程中的一个基本能力是组成性，它允许将复杂问题分解为更简单的元素。对于神经网络来说，这种属性也非常重要，特别是当目标是建立更加高效和可持续的人工智能框架时。

**方法:** 研究者们提出了一种组成性的方法，即通过集成一组零样本专家模型（Expert Composition method），并使用一个旨在测试组成性能力的具有挑战性的基准来评估所提出的方法学。

**结果:** 实验表明，提出的专家组合方法能够比基线算法实现更高的准确性，同时需要较少的计算资源，因此更为高效。

**结论:** 专家组合方法在保持高准确性的同时提高了效率，为构建更加高效和可持续的人工智能框架提供了一个新的方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Task-Agnostic+Experts+Composition+for+Continual+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15566，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15566&send_immediately=true&force_search=false)

**原文摘要:** Compositionality is one of the fundamental abilities of the human reasoning
process, that allows to decompose a complex problem into simpler elements. Such
property is crucial also for neural networks, especially when aiming for a more
efficient and sustainable AI framework. We propose a compositional approach by
ensembling zero-shot a set of expert models, assessing our methodology using a
challenging benchmark, designed to test compositionality capabilities. We show
that our Expert Composition method is able to achieve a much higher accuracy
than baseline algorithms while requiring less computational resources, hence
being more efficient.

</details>


### [86] [MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing](https://arxiv.org/abs/2506.15571)
*Le Vu Anh, Nguyen Viet Anh, Mehmet Dik, Tu Nguyen Thi Ngoc*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为MicroRicci的新型自调优局部Ricci流求解器，它结合了编码理论的思想，并且只需要很少的参数。该方法在保证几何和感知质量的同时，显著减少了迭代次数并加快了运行速度，适用于实时和资源受限的应用场景。


<details>
  <summary>更多</summary>
  
**动机:** 实时大规模网格平滑仍然是一个重大挑战：经典的Ricci流求解器需要昂贵的全局更新，而贪婪启发式方法则存在收敛慢或调整困难的问题。

**方法:** MicroRicci利用来自编码理论的概念设计了一个只需1K+200个参数的小型化解决方案。其核心是一个贪婪的综合解码步骤，能在O(E)时间内定位并修正最大的曲率误差，并通过两个小型神经模块动态选择顶点和步长。

**结果:** 在SJTU-TMQA数据集上，MicroRicci将迭代次数从950±140减少到400±80（2.4倍加速），并将曲率范围从0.19收紧至0.185。此外，实现了UV失真与MOS相关性r=-0.93的好成绩。每轮迭代仅增加0.25毫秒时间（总耗时从0.80到1.05毫秒），最终端到端运行时间相比现有最先进方法提速1.8倍。

**结论:** MicroRicci通过线性时间更新、自动超参数适应以及高质量的几何和感知结果，证明了自己非常适合于图形、模拟及相关领域中的实时及资源有限应用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MicroRicci%3A+A+Greedy+and+Local+Ricci+Flow+Solver+for+Self-Tuning+Mesh+Smoothing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15571，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15571&send_immediately=true&force_search=false)

**原文摘要:** Real-time mesh smoothing at scale remains a formidable challenge: classical
Ricci-flow solvers demand costly global updates, while greedy heuristics suffer
from slow convergence or brittle tuning. We present MicroRicci, the first truly
self-tuning, local Ricci-flow solver that borrows ideas from coding theory and
packs them into just 1K + 200 parameters. Its primary core is a greedy
syndrome-decoding step that pinpoints and corrects the largest curvature error
in O(E) time, augmented by two tiny neural modules that adaptively choose
vertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,
MicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),
tightens curvature spread from 0.19 to 0.185, and achieves a remarkable
UV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per
iteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration
over state-of-the-art methods. MicroRicci's combination of linear-time updates,
automatic hyperparameter adaptation, and high-quality geometric and perceptual
results makes it well suited for real-time, resource-limited applications in
graphics, simulation, and related fields.

</details>


### [87] [Memory-Efficient Differentially Private Training with Gradient Random Projection](https://arxiv.org/abs/2506.15588)
*Alex Mulrooney, Devansh Gupta, James Flemings, Huanyu Zhang, Murali Annavaram, Meisam Razaviyayn, Xinwei Zhang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的差分隐私训练方法DP-GRAPE，通过梯度随机投影减少了内存使用，同时保持了与现有方法相当的实用性，并且在预训练视觉变换器和微调RoBERTa-Large时分别降低了超过63%和70%的内存占用。


<details>
  <summary>更多</summary>
  
**动机:** 标准的差分隐私方法（如DP-Adam）由于每样本梯度裁剪导致高内存开销，限制了可扩展性。

**方法:** DP-GRAPE引入了三个关键修改：1) 梯度在投影后进行私有化；2) 用随机高斯矩阵替代基于SVD的子空间；3) 在反向传播过程中应用投影。

**结果:** 理论分析显示DP-GRAPE达到了与DP-SGD相当的隐私-效用权衡。实验表明，DP-GRAPE可以减少差分隐私训练的内存占用，而不会牺牲准确性或训练时间。

**结论:** DP-GRAPE能够显著降低差分隐私训练过程中的内存使用量，同时保持良好的性能，适用于大规模模型的微调。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Memory-Efficient+Differentially+Private+Training+with+Gradient+Random+Projection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15588，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15588&send_immediately=true&force_search=false)

**原文摘要:** Differential privacy (DP) protects sensitive data during neural network
training, but standard methods like DP-Adam suffer from high memory overhead
due to per-sample gradient clipping, limiting scalability. We introduce
DP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly
reduces memory usage while maintaining utility on par with first-order DP
approaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces
three key modifications: (1) gradients are privatized after projection, (2)
random Gaussian matrices replace SVD-based subspaces, and (3) projection is
applied during backpropagation. These contributions eliminate the need for
costly SVD computations, enable substantial memory savings, and lead to
improved utility. Despite operating in lower-dimensional subspaces, our
theoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off
comparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE
can reduce the memory footprint of DP training without sacrificing accuracy or
training time. In particular, DP-GRAPE reduces memory usage by over 63% when
pre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as
compared to DP-Adam, while achieving similar performance. We further
demonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with
up to 6.7 billion parameters.

</details>


### [88] [CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization](https://arxiv.org/abs/2506.15654)
*Ranting Hu*

**主要类别:** cs.LG

**AI概要:** 本文研究了Advantage-Weighted Regression (AWR) 方法在处理次优离线数据时可能学到过度保守策略的问题，并提出了Corruption-Averse AWR (CAWR) 方法来缓解这一问题。通过采用鲁棒的损失函数和基于优势优先的经验回放方法，CAWR 能够从次优离线数据中学习到更好的策略，从而提高策略优化性能。


<details>
  <summary>更多</summary>
  
**动机:** 论文动机在于解决Advantage-Weighted Regression (AWR) 家族算法的一个局限性，即由于数据损坏导致学习到过度保守的策略，特别是在次优离线数据中的探索不足问题。

**方法:** 文章提出了一种新的方法称为抗腐蚀优势加权回归(CAWR)，该方法结合了一系列鲁棒的损失函数用于策略优化以及一种基于优势的优先经验回放方法，以筛选出不良的探索。

**结果:** D4RL基准测试上的数值实验表明，所提出的方法可以从次优的离线数据中学习更优的策略，显著提高了策略优化的表现。

**结论:** 结论是通过使用CAWR方法，可以有效地减少由于离线数据集中的不良探索对策略优化的影响，从而使得从次优离线数据中学习到的策略更加优越。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CAWR%3A+Corruption-Averse+Advantage-Weighted+Regression+for+Robust+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15654，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15654&send_immediately=true&force_search=false)

**原文摘要:** Offline reinforcement learning (offline RL) algorithms often require
additional constraints or penalty terms to address distribution shift issues,
such as adding implicit or explicit policy constraints during policy
optimization to reduce the estimation bias of functions. This paper focuses on
a limitation of the Advantage-Weighted Regression family (AWRs), i.e., the
potential for learning over-conservative policies due to data corruption,
specifically the poor explorations in suboptimal offline data. We study it from
two perspectives: (1) how poor explorations impact the theoretically optimal
policy based on KL divergence, and (2) how such poor explorations affect the
approximation of the theoretically optimal policy. We prove that such
over-conservatism is mainly caused by the sensitivity of the loss function for
policy optimization to poor explorations, and the proportion of poor
explorations in offline datasets. To address this concern, we propose
Corruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a
set of robust loss functions during policy optimization and an advantage-based
prioritized experience replay method to filter out poor explorations. Numerical
experiments on the D4RL benchmark show that our method can learn superior
policies from suboptimal offline data, significantly enhancing the performance
of policy optimization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [CALM: Contextual Analog Logic with Multimodality](https://arxiv.org/abs/2506.14936)
*Maxwell J. Jacobson, Corey J. Maley, Yexiang Xue*

**主要类别:** cs.AI

**AI概要:** 该论文提出了一种结合符号推理与神经生成的多模态上下文模拟逻辑（CALM），它能够基于多模态数据做出上下文敏感的决策。CALM通过神经网络和符号推理模块来迭代地细化每个谓词的模拟真值，从而在空白填充物体放置任务中达到92.2%的准确率，优于传统逻辑和其他基线模型。


<details>
  <summary>更多</summary>
  
**动机:** 经典二值逻辑系统不能捕捉人类决策中的细微差别，并且需要人为地在多模态环境中进行设置，这往往是临时、僵化且脆弱的。虽然神经网络擅长从多模态数据中提取丰富的上下文信息，但缺乏可解释的推理结构。因此，CALM旨在弥合逻辑与神经感知之间的差距，创建一种可以对多模态输入进行推理的模拟逻辑。

**方法:** CALM使用领域树表示每个谓词，当其实体的上下文基础被确定时，领域树会迭代地细化其模拟真值。这种迭代细化由能够捕捉多模态信息的神经网络预测，并通过一个符号推理模块过滤，以确保满足约束条件。

**结果:** 在填空物体放置任务中，CALM达到了92.2%的准确率，超过了经典逻辑(86.3%)和LLM(59.4%)基线。此外，它还展示了与逻辑约束及细致的人类偏好相一致的空间热图生成能力，这一点得到了人类研究的支持。

**结论:** CALM展示了在多模态环境中进行逻辑结构推理并符合偏好的潜力。它为下一代AI系统奠定了基础，这些系统既要求逻辑的精确性和可解释性，又要求神经网络处理多模态信息的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CALM%3A+Contextual+Analog+Logic+with+Multimodality，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14936，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14936&send_immediately=true&force_search=false)

**原文摘要:** In this work, we introduce Contextual Analog Logic with Multimodality (CALM).
CALM unites symbolic reasoning with neural generation, enabling systems to make
context-sensitive decisions grounded in real-world multi-modal data.
  Background: Classic bivalent logic systems cannot capture the nuance of human
decision-making. They also require human grounding in multi-modal environments,
which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting
rich contextual information from multi-modal data, but lack interpretable
structures for reasoning.
  Objectives: CALM aims to bridge the gap between logic and neural perception,
creating an analog logic that can reason over multi-modal inputs. Without this
integration, AI systems remain either brittle or unstructured, unable to
generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate
to analog truth values computed by neural networks and constrained search.
  Methods: CALM represents each predicate using a domain tree, which
iteratively refines its analog truth value when the contextual groundings of
its entities are determined. The iterative refinement is predicted by neural
networks capable of capturing multi-modal information and is filtered through a
symbolic reasoning module to ensure constraint satisfaction.
  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%
accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It
also demonstrated spatial heatmap generation aligned with logical constraints
and delicate human preferences, as shown by a human study.
  Conclusions: CALM demonstrates the potential to reason with logic structure
while aligning with preferences in multi-modal environments. It lays the
foundation for next-gen AI systems that require the precision and
interpretation of logic and the multimodal information processing of neural
networks.

</details>


### [90] [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.14990)
*Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Mykola Pechenizkiy*

**主要类别:** cs.AI

**AI概要:** 介绍了MEAL，首个专为持续多智能体强化学习设计的基准，它利用JAX进行GPU加速，能够在标准台式PC上在几个小时内完成100个任务序列的持续学习。


<details>
  <summary>更多</summary>
  
**动机:** 现有的持续学习（CL）基准在CPU上运行环境，导致计算瓶颈，并限制了任务序列的长度。此外，在合作多智能体设置中的持续学习是一个相对未被充分探索的领域。

**方法:** 提出了MEAL，一个专为持续多智能体强化学习(CMARL)定制的基准，通过使用JAX实现GPU加速，从而克服了现有基准的计算限制。

**结果:** 研究表明，单纯地结合流行的持续学习和多智能体强化学习方法可以在简单环境中表现良好，但在需要持续协调和适应的更复杂场景中则表现不佳。

**结论:** 通过消融研究确定了对于MEAL上的CMARL至关重要的架构和算法特性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MEAL%3A+A+Benchmark+for+Continual+Multi-Agent+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14990，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14990&send_immediately=true&force_search=false)

**原文摘要:** Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms, with environment availability strongly
impacting research. One particularly underexplored intersection is continual
learning (CL) in cooperative multi-agent settings. To remedy this, we introduce
MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark
tailored for continual multi-agent reinforcement learning (CMARL). Existing CL
benchmarks run environments on the CPU, leading to computational bottlenecks
and limiting the length of task sequences. MEAL leverages JAX for GPU
acceleration, enabling continual learning across sequences of 100 tasks on a
standard desktop PC in a few hours. We show that naively combining popular CL
and MARL methods yields strong performance on simple environments, but fails to
scale to more complex settings requiring sustained coordination and adaptation.
Our ablation study identifies architectural and algorithmic features critical
for CMARL on MEAL.

</details>


### [91] [Truncated Proximal Policy Optimization](https://arxiv.org/abs/2506.15050)
*Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, Yonghui Wu*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为Truncated Proximal Policy Optimization (T-PPO)的新方法，它通过简化策略更新和限制响应长度来提高大型语言模型(LLMs)的训练效率。T-PPO解决了传统PPO方法在长生成过程中硬件利用率低的问题，并通过引入Extended Generalized Advantage Estimation(EGAE)和一种计算优化机制来加速训练过程。实验结果显示T-PPO能够将推理LLMs的训练效率提升至多2.5倍，并且优于现有竞争者。


<details>
  <summary>更多</summary>
  
**动机:** 传统的近端策略优化（PPO）及其变体虽然允许模型通过试错学习，但由于其固有的在线策略性质，特别是在响应长度增加时，会变得非常耗时。此外，完全同步的长生成过程导致了硬件利用率低下，因为资源在等待完整回放完成期间经常处于闲置状态。为了解决这些问题，研究者提出了T-PPO以改进训练效率。

**方法:** 研究人员开发了Truncated Proximal Policy Optimization (T-PPO)，这是对PPO的一种新扩展，旨在通过简化政策更新流程以及生成长度受限的回应来提高培训效率。T-PPO包括两个主要贡献：首先，提出了Extended Generalized Advantage Estimation (EGAE)用于从不完整的回应中估计优势同时保持政策学习的一致性；其次，设计了一种计算上经过优化的机制，允许独立优化策略和价值模型。这种机制通过有选择地过滤提示和截断标记减少了冗余计算，从而加快了训练过程而不牺牲收敛性能。

**结果:** 实验结果表明，在AIME 2024上的32B基础模型测试中，T-PPO相较于现有的方法，能将推理型大型语言模型的训练效率提高最多2.5倍。这不仅验证了T-PPO的有效性，也显示了它在同类技术中的优越表现。

**结论:** 这项工作展示了T-PPO作为一种有效的方法来提高大型语言模型的训练效率，特别是在执行需要长时间思考的任务时。通过减少不必要的计算并允许更灵活的模型优化，T-PPO代表了朝着更高效、更快速训练复杂推理能力的语言模型迈出的重要一步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Truncated+Proximal+Policy+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15050，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15050&send_immediately=true&force_search=false)

**原文摘要:** Recently, test-time scaling Large Language Models (LLMs) have demonstrated
exceptional reasoning capabilities across scientific and professional tasks by
generating long chains-of-thought (CoT). As a crucial component for developing
these reasoning models, reinforcement learning (RL), exemplified by Proximal
Policy Optimization (PPO) and its variants, allows models to learn through
trial and error. However, PPO can be time-consuming due to its inherent
on-policy nature, which is further exacerbated by increasing response lengths.
In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a
novel extension to PPO that improves training efficiency by streamlining policy
update and length-restricted response generation. T-PPO mitigates the issue of
low hardware utilization, an inherent drawback of fully synchronized
long-generation procedures, where resources often sit idle during the waiting
periods for complete rollouts. Our contributions are two-folds. First, we
propose Extended Generalized Advantage Estimation (EGAE) for advantage
estimation derived from incomplete responses while maintaining the integrity of
policy learning. Second, we devise a computationally optimized mechanism that
allows for the independent optimization of the policy and value models. By
selectively filtering prompt and truncated tokens, this mechanism reduces
redundant computations and accelerates the training process without sacrificing
convergence performance. We demonstrate the effectiveness and efficacy of T-PPO
on AIME 2024 with a 32B base model. The experimental results show that T-PPO
improves the training efficiency of reasoning LLMs by up to 2.5x and
outperforms its existing competitors.

</details>


### [92] [HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges](https://arxiv.org/abs/2506.15196)
*Xianliang Yang, Ling Zhang, Haolong Qian, Lei Song, Jiang Bian*

**主要类别:** cs.AI

**AI概要:** 介绍了一种名为HeurAgenix的双阶段超启发式框架，该框架利用大型语言模型（LLM）来演化和选择合适的启发式算法解决组合优化问题，并在基准测试中表现出色。


<details>
  <summary>更多</summary>
  
**动机:** 传统的启发式算法设计依赖于人工专业知识并且难以泛化到不同的实例中。

**方法:** HeurAgenix使用大型语言模型（LLMs），首先通过对比种子启发式解与更高质量解来提取可重用的进化策略；然后在解决问题时动态选择最适合当前问题状态的启发式算法。为提高灵活性，选择器可以是先进的LLM或微调过的轻量级模型。此外，为了减少由于组合优化复杂性导致的可靠监督稀缺问题，研究者采用双重奖励机制对轻量级启发式选择器进行微调。

**结果:** 广泛的实验表明，HeurAgenix不仅优于现有的基于LLM的超启发式方法，而且能够匹敌甚至超越专门求解器的表现。

**结论:** HeurAgenix提供了一种新颖且有效的手段来应对组合优化问题，通过自动化的方式生成并选择启发式算法，从而减轻了对于人工设计的依赖。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HeurAgenix%3A+Leveraging+LLMs+for+Solving+Complex+Combinatorial+Optimization+Challenges，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15196，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15196&send_immediately=true&force_search=false)

**原文摘要:** Heuristic algorithms play a vital role in solving combinatorial optimization
(CO) problems, yet traditional designs depend heavily on manual expertise and
struggle to generalize across diverse instances. We introduce
\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large
language models (LLMs) that first evolves heuristics and then selects among
them automatically. In the heuristic evolution phase, HeurAgenix leverages an
LLM to compare seed heuristic solutions with higher-quality solutions and
extract reusable evolution strategies. During problem solving, it dynamically
picks the most promising heuristic for each problem state, guided by the LLM's
perception ability. For flexibility, this selector can be either a
state-of-the-art LLM or a fine-tuned lightweight model with lower inference
cost. To mitigate the scarcity of reliable supervision caused by CO complexity,
we fine-tune the lightweight heuristic selector with a dual-reward mechanism
that jointly exploits singals from selection preferences and state perception,
enabling robust selection under noisy annotations. Extensive experiments on
canonical benchmarks show that HeurAgenix not only outperforms existing
LLM-based hyper-heuristics but also matches or exceeds specialized solvers.
Code is available at https://github.com/microsoft/HeurAgenix.

</details>


### [93] [Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study](https://arxiv.org/abs/2506.15207)
*Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk*

**主要类别:** cs.AI

**AI概要:** 本文探讨了基于强化学习（RL）和多智能体强化学习（MARL）的自主地球观测任务规划，通过模拟环境评估了最新MARL算法的表现，结果表明MARL能够有效地平衡成像与资源管理，并为非中心化的地球观测任务提供实用指南。


<details>
  <summary>更多</summary>
  
**动机:** 随着近地轨道卫星数量的快速增长，虽然地球观测任务在气候监测、灾害管理等方面取得了显著进展，但多卫星系统的自主协调仍是一个基本挑战。传统的优化方法难以应对动态地球观测任务中的实时决策需求。

**方法:** 研究者们首先对单个卫星的操作进行了建模，然后利用多智能体强化学习框架扩展到了多卫星星座。他们还解决了能量和数据存储限制、卫星观测不确定性以及部分可观测条件下的分散协调复杂性等问题。

**结果:** 研究表明，多智能体强化学习可以有效解决多卫星协调中的非稳态性和奖励互依性问题，同时实现成像与资源管理之间的平衡。

**结论:** 该研究为自主卫星操作奠定了基础，并为改善去中心化地球观测任务中的策略学习提供了实用指导。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Multi-Agent+Reinforcement+Learning+for+Autonomous+Multi-Satellite+Earth+Observation%3A+A+Realistic+Case+Study，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15207&send_immediately=true&force_search=false)

**原文摘要:** The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised
Earth Observation (EO) missions, addressing challenges in climate monitoring,
disaster management, and more. However, autonomous coordination in
multi-satellite systems remains a fundamental challenge. Traditional
optimisation approaches struggle to handle the real-time decision-making
demands of dynamic EO missions, necessitating the use of Reinforcement Learning
(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we
investigate RL-based autonomous EO mission planning by modelling
single-satellite operations and extending to multi-satellite constellations
using MARL frameworks. We address key challenges, including energy and data
storage limitations, uncertainties in satellite observations, and the
complexities of decentralised coordination under partial observability. By
leveraging a near-realistic satellite simulation environment, we evaluate the
training stability and performance of state-of-the-art MARL algorithms,
including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can
effectively balance imaging and resource management while addressing
non-stationarity and reward interdependency in multi-satellite coordination.
The insights gained from this study provide a foundation for autonomous
satellite operations, offering practical guidelines for improving policy
learning in decentralised EO missions.

</details>


### [94] [Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels](https://arxiv.org/abs/2506.15225)
*Jiahao You, Ziye Jia, Chao Dong, Qihui Wu, Zhu Han*

**主要类别:** cs.AI

**AI概要:** 论文提出了一种基于无人机和船舶合作的多接入边缘计算框架，以解决海上物联网中不确定任务带来的计算卸载和资源分配问题。通过将长期约束转换为短期约束，并利用异构智能体软演员-评论家算法解决马尔可夫博弈问题，最终优化了总执行时间。


<details>
  <summary>更多</summary>
  
**动机:** 近年来，海上物联网（MIoT）的计算需求迅速增长，而基于无人机（UAVs）和船只的多接入边缘计算（MEC）可以满足这些需求。然而，不确定性的海上任务给计算卸载和资源分配带来了显著挑战。

**方法:** 提出了一个用于计算卸载和资源分配的合作MEC框架，该框架包括MIoT设备、无人机和船只。为了应对不可预测的任务到达和变化的计算资源可用性，使用Lyapunov优化方法来处理不确定的MIoT任务。通过将长期约束转化为一系列短期的小规模优化问题，然后考虑到无人机和船只行动与资源的异质性，进一步将小规模优化问题重新表述为马尔可夫博弈（MG）。此外，提出了一种异构代理软演员-评论家算法，用以顺序更新各种神经网络并有效解决MG问题。

**结果:** 仿真结果验证了所提方法在处理计算卸载和资源分配问题上的有效性。

**结论:** 本文提出的合作MEC框架结合了无人机和船只的优势，能够有效地解决由于不确定性任务导致的海上计算卸载和资源分配效率低下的问题。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Joint+Computation+Offloading+and+Resource+Allocation+for+Uncertain+Maritime+MEC+via+Cooperation+of+UAVs+and+Vessels，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15225，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15225&send_immediately=true&force_search=false)

**原文摘要:** The computation demands from the maritime Internet of Things (MIoT) increase
rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels
based multi-access edge computing (MEC) can fulfill these MIoT requirements.
However, the uncertain maritime tasks present significant challenges of
inefficient computation offloading and resource allocation. In this paper, we
focus on the maritime computation offloading and resource allocation through
the cooperation of UAVs and vessels, with consideration of uncertain tasks.
Specifically, we propose a cooperative MEC framework for computation offloading
and resource allocation, including MIoT devices, UAVs and vessels. Then, we
formulate the optimization problem to minimize the total execution time. As for
the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the
unpredictable task arrivals and varying computational resource availability. By
converting the long-term constraints into short-term constraints, we obtain a
set of small-scale optimization problems. Further, considering the
heterogeneity of actions and resources of UAVs and vessels, we reformulate the
small-scale optimization problem into a Markov game (MG). Moreover, a
heterogeneous-agent soft actor-critic is proposed to sequentially update
various neural networks and effectively solve the MG problem. Finally,
simulations are conducted to verify the effectiveness in addressing
computational offloading and resource allocation.

</details>


### [95] [Efficient and Generalizable Environmental Understanding for Visual Navigation](https://arxiv.org/abs/2506.15377)
*Ruoyu Wang, Xinshu Li, Chen Wang, Lina Yao*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种新的导航方法Causality-Aware Navigation (CAN)，它通过引入因果理解模块来提高代理对环境的理解能力，并在各种任务和模拟环境中表现出优于现有方法的性能。


<details>
  <summary>更多</summary>
  
**动机:** 现有的视觉导航方法虽然表现良好，但它们通常同时处理所有历史观察数据，忽视了数据内部的关联结构，这可能限制了任务性能的进一步提升。

**方法:** 通过因果关系的角度审视导航任务的独特特征，提出了一个因果框架以揭示传统序列方法的局限性。基于此洞察，开发了Causality-Aware Navigation (CAN) 方法，该方法包含一个因果理解模块用以增强代理对环境的理解能力。

**结果:** 实验评估表明，提出的CAN方法在不同任务及仿真环境下均持续优于基线模型。此外，广泛的消融研究表明这些改进归功于因果理解模块，在强化学习和监督学习设置下都能有效泛化且不增加计算开销。

**结论:** Causality-Aware Navigation (CAN) 通过引入因果理解机制显著提高了导航任务的表现，为未来的研究提供了新方向。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+and+Generalizable+Environmental+Understanding+for+Visual+Navigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15377，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15377&send_immediately=true&force_search=false)

**原文摘要:** Visual Navigation is a core task in Embodied AI, enabling agents to navigate
complex environments toward given objectives. Across diverse settings within
Navigation tasks, many necessitate the modelling of sequential data accumulated
from preceding time steps. While existing methods perform well, they typically
process all historical observations simultaneously, overlooking the internal
association structure within the data, which may limit the potential for
further improvements in task performance. We address this by examining the
unique characteristics of Navigation tasks through the lens of causality,
introducing a causal framework to highlight the limitations of conventional
sequential methods. Leveraging this insight, we propose Causality-Aware
Navigation (CAN), which incorporates a Causal Understanding Module to enhance
the agent's environmental understanding capability. Empirical evaluations show
that our approach consistently outperforms baselines across various tasks and
simulation environments. Extensive ablations studies attribute these gains to
the Causal Understanding Module, which generalizes effectively in both
Reinforcement and Supervised Learning settings without computational overhead.

</details>


### [96] [Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents](https://arxiv.org/abs/2506.15567)
*Aline Dobrovsky, Konstantin Schekotihin, Christian Burmer*

**主要类别:** cs.AI

**AI概要:** 本文探讨了基于大型语言模型的规划代理（LPA）的设计与实现，该代理能够协助失效分析工程师处理分析案例。LPA结合了大型语言模型、高级规划能力和外部工具使用，从而能够自主处理复杂查询、从外部系统检索相关数据并生成可读响应。评估结果表明，该代理在支持失效分析任务方面具有操作有效性和可靠性。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能组件越来越多地集成到失效分析实验室的计算基础设施中，如何将这些组件组织成连贯且高效的工作流程成为了一个挑战。为了解决这一问题，并提高失效分析过程中的自动化程度和效率，提出了基于大型语言模型的规划代理（LPA）。

**方法:** 研究者设计并实现了一个基于大型语言模型（LLM）的规划代理（LPA），它整合了大型语言模型、高级规划功能以及对外部工具的应用。LPA能够自动处理复杂的查询请求，从外部系统获取相关信息，并产生易于理解的人类可读回应。

**结果:** 通过评估证明了LPA在支持失效分析任务时的操作有效性和可靠性。

**结论:** 基于大型语言模型的规划代理（LPA）展示了其在协助失效分析工程师处理案例方面的潜力，能够显著提升自动化水平及工作效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Managing+Complex+Failure+Analysis+Workflows+with+LLM-based+Reasoning+and+Acting+Agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15567，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15567&send_immediately=true&force_search=false)

**原文摘要:** Failure Analysis (FA) is a highly intricate and knowledge-intensive process.
The integration of AI components within the computational infrastructure of FA
labs has the potential to automate a variety of tasks, including the detection
of non-conformities in images, the retrieval of analogous cases from diverse
data sources, and the generation of reports from annotated images. However, as
the number of deployed AI models increases, the challenge lies in orchestrating
these components into cohesive and efficient workflows that seamlessly
integrate with the FA process.
  This paper investigates the design and implementation of a Large Language
Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their
analysis cases. The LPA integrates LLMs with advanced planning capabilities and
external tool utilization, enabling autonomous processing of complex queries,
retrieval of relevant data from external systems, and generation of
human-readable responses. Evaluation results demonstrate the agent's
operational effectiveness and reliability in supporting FA tasks.

</details>


### [97] [The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games](https://arxiv.org/abs/2506.15624)
*Lyle Goodyear, Rachel Guo, Ramesh Johari*

**主要类别:** cs.AI

**AI概要:** 本文提出了一个统一的框架，用于构建自然语言状态表示，以在重复多智能体游戏中提示大型语言模型（LLMs）代理。该框架沿三个轴定义了状态表示方法：动作信息性、奖励信息性和提示风格。通过应用到动态自私路由游戏中，研究发现特定类型的自然语言状态表示能够使LLM代理的行为更接近博弈论均衡预测，并且游戏玩法更加稳定。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型作为动态环境下的决策者展现出潜力，但由于其无状态特性，需要创建历史的自然语言表示。之前关于使用LLM代理的游戏的研究采用了临时的方法来编码游戏历史，这不仅模糊了状态表示对代理行为的影响，也限制了不同研究之间的可比性。

**方法:** 提出了一种系统化构建自然语言“状态”表示的统一框架，旨在为重复多智能体游戏中的LLM代理提供提示。该框架沿着三个维度对状态表示方法进行了表征：行动信息量、奖励信息量以及提示样式（或自然语言压缩）。

**结果:** 尽管选择的游戏相对简单，但研究人员发现LLM代理的行为关键依赖于自然语言状态表示。特别是，当给代理提供的是总结而非完整的历史自然语言表示、有关遗憾的信息而不是原始收益、以及他人行动的有限信息时，代理的行为更接近于博弈论平衡预测，并且游戏玩法更加稳定。

**结论:** 本研究表明，通过调整自然语言状态表示的方式，可以显著影响LLM代理在重复多智能体游戏中的行为模式及其稳定性。适当的自然语言状态表示能够促使LLM代理产生更符合理论预期的行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Effect+of+State+Representation+on+LLM+Agent+Behavior+in+Dynamic+Routing+Games，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15624，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15624&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have shown promise as decision-makers in dynamic
settings, but their stateless nature necessitates creating a natural language
representation of history. We present a unifying framework for systematically
constructing natural language "state" representations for prompting LLM agents
in repeated multi-agent games. Previous work on games with LLM agents has taken
an ad hoc approach to encoding game history, which not only obscures the impact
of state representation on agents' behavior, but also limits comparability
between studies. Our framework addresses these gaps by characterizing methods
of state representation along three axes: action informativeness (i.e., the
extent to which the state representation captures actions played); reward
informativeness (i.e., the extent to which the state representation describes
rewards obtained); and prompting style (or natural language compression, i.e.,
the extent to which the full text history is summarized).
  We apply this framework to a dynamic selfish routing game, chosen because it
admits a simple equilibrium both in theory and in human subject experiments
\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find
that there are key dependencies of LLM agent behavior on the natural language
state representation. In particular, we observe that representations which
provide agents with (1) summarized, rather than complete, natural language
representations of past history; (2) information about regrets, rather than raw
payoffs; and (3) limited information about others' actions lead to behavior
that more closely matches game theoretic equilibrium predictions, and with more
stable game play by the agents. By contrast, other representations can exhibit
either large deviations from equilibrium, higher variation in dynamic game play
over time, or both.

</details>


### [98] [The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy](https://arxiv.org/abs/2506.15639)
*James Weichert, Daniel Dunlap, Mohammed Farghally, Hoda Eldardiry*

**主要类别:** cs.AI

**AI概要:** 本文介绍了为计算机科学课程开发的人工智能政策模块，旨在帮助学生更好地理解AI伦理和政策，并通过前后调查评估了学生对AI伦理和政策的态度变化。


<details>
  <summary>更多</summary>
  
**动机:** 随着人工智能在个人和专业领域中的应用日益广泛，未来的人工智能从业者不仅需要关注AI伦理，还需要了解如何将抽象的伦理原则和规范性政策偏好融入到AI系统的设计与开发中。然而，当前的高等教育计算课程并没有充分准备好应对这些需求。

**方法:** 作者开发了一个AI政策模块并将其引入计算机科学课程，该模块包括一个关于'AI监管'的技术作业。通过对试点项目2.0版本进行更新和扩展，并使用前测后测问卷来评价学生对于AI伦理与政策的态度变化。

**结果:** 经过模块学习后，学生们对AI技术的伦理影响表示了更多的担忧，同时他们也表达了对自己参与AI监管讨论能力的信心增强。

**结论:** AI监管作业被证明是一个有效的工具，它有助于探索AI一致性（即AI行为与人类价值观的一致程度）的界限，并强调了政策在解决伦理挑战方面的作用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+AI+Policy+Module%3A+Developing+Computer+Science+Student+Competency+in+AI+Ethics+and+Policy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15639，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15639&send_immediately=true&force_search=false)

**原文摘要:** As artificial intelligence (AI) further embeds itself into many settings
across personal and professional contexts, increasing attention must be paid
not only to AI ethics, but also to the governance and regulation of AI
technologies through AI policy. However, the prevailing post-secondary
computing curriculum is currently ill-equipped to prepare future AI
practitioners to confront increasing demands to implement abstract ethical
principles and normative policy preferences into the design and development of
AI systems. We believe that familiarity with the 'AI policy landscape' and the
ability to translate ethical principles to practices will in the future
constitute an important responsibility for even the most technically-focused AI
engineers.
  Toward preparing current computer science (CS) students for these new
expectations, we developed an AI Policy Module to introduce discussions of AI
policy into the CS curriculum. Building on a successful pilot in fall 2024, in
this innovative practice full paper we present an updated and expanded version
of the module, including a technical assignment on "AI regulation". We present
the findings from our pilot of the AI Policy Module 2.0, evaluating student
attitudes towards AI ethics and policy through pre- and post-module surveys.
Following the module, students reported increased concern about the ethical
impacts of AI technologies while also expressing greater confidence in their
abilities to engage in discussions about AI regulation. Finally, we highlight
the AI Regulation Assignment as an effective and engaging tool for exploring
the limits of AI alignment and emphasizing the role of 'policy' in addressing
ethical challenges.

</details>


### [99] [Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement](https://arxiv.org/abs/2506.15647)
*Weixiang Zhao, Jiahe Guo, Yang Deng, Xingyu Sui, Yulin Hu, Yanyan Zhao, Wanxiang Che, Bing Qin, Tat-Seng Chua, Ting Liu*

**主要类别:** cs.AI

**AI概要:** 该研究探讨了大型推理模型（LRMs）中存在的过度思考问题，并提出两种轻量级方法——效率导向和自我奖励效率强化学习，以提高这些模型的推理效率。实验表明，这些方法在保持或提高任务性能的同时显著减少了推理长度。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型推理模型（LRMs）通过模仿人类的深思熟虑思维极大地增强了语言模型解决复杂问题的能力，但它们往往表现出过度思考的现象，即生成不必要的冗长且重复的内容，这降低了效率并增加了推理成本。

**方法:** 研究者们首先分析了这种低效性的表征和行为根源，发现LRMs天生具备更简洁推理的能力。基于此，他们提出了两种轻量级方法来增强LRM的效率：一是无需训练的激活导向技术“效率导向”，它通过模型表示空间中的单一方向调节推理行为；二是“自我奖励效率强化学习”框架，该框架通过奖励简洁正确的解决方案动态平衡任务准确性和简洁性。

**结果:** 在七个LRM基础架构上进行的广泛实验表明，所提出的方法在多个数学推理基准测试中能够显著减少推理长度，同时保持或提高任务表现。

**结论:** 研究表明，通过利用和指导现有模型的内在能力，可以自引导地改善推理效率。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+and+Exploiting+the+Inherent+Efficiency+within+Large+Reasoning+Models+for+Self-Guided+Efficiency+Enhancement，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15647，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15647&send_immediately=true&force_search=false)

**原文摘要:** Recent advancements in large reasoning models (LRMs) have significantly
enhanced language models' capabilities in complex problem-solving by emulating
human-like deliberative thinking. However, these models often exhibit
overthinking (i.e., the generation of unnecessarily verbose and redundant
content), which hinders efficiency and inflates inference cost. In this work,
we explore the representational and behavioral origins of this inefficiency,
revealing that LRMs inherently possess the capacity for more concise reasoning.
Empirical analyses show that correct reasoning paths vary significantly in
length, and the shortest correct responses often suffice, indicating untapped
efficiency potential. Exploiting these findings, we propose two lightweight
methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a
training-free activation steering technique that modulates reasoning behavior
via a single direction in the model's representation space. Second, we develop
Self-Rewarded Efficiency RL, a reinforcement learning framework that
dynamically balances task accuracy and brevity by rewarding concise correct
solutions. Extensive experiments on seven LRM backbones across multiple
mathematical reasoning benchmarks demonstrate that our methods significantly
reduce reasoning length while preserving or improving task performance. Our
results highlight that reasoning efficiency can be improved by leveraging and
guiding the intrinsic capabilities of existing models in a self-guided manner.

</details>


### [100] [SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence](https://arxiv.org/abs/2506.15672)
*Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, Volker Tresp*

**主要类别:** cs.AI

**AI概要:** 提出了一种名为SwarmAgentic的框架，它能够从零开始自动生成并优化代理系统及其协作能力。通过语言驱动探索和受粒子群优化启发的反馈引导更新，该方法在六个真实世界任务中优于所有基准，并在TravelPlanner基准上相对于ADAS实现了261.8%的相对改进。


<details>
  <summary>更多</summary>
  
**动机:** 现有的代理系统生成框架缺乏完全自主性，无法从头开始生成代理、自我优化代理功能以及促进协作，这限制了系统的适应性和可扩展性。

**方法:** SwarmAgentic框架通过语言驱动的探索来构建代理系统，并将代理的功能和协作作为相互依赖的部分进行联合优化。为了有效搜索系统级结构，该框架维护了一个候选系统群体，并通过受粒子群优化（PSO）启发的反馈引导更新机制来进化这些系统。

**结果:** 在涉及高级规划、系统级协调和创造性推理的六项开放且探索性的实际任务中进行了评估。仅给定任务描述和目标函数的情况下，SwarmAgentic的表现优于所有基线，在TravelPlanner基准测试中相比ADAS取得了261.8%的相对提升。

**结论:** SwarmAgentic框架代表了向可扩展和自主代理系统设计迈出的重要一步，结合了群智能与全自动多代理生成。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SwarmAgentic%3A+Towards+Fully+Automated+Agentic+System+Generation+via+Swarm+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15672，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15672&send_immediately=true&force_search=false)

**原文摘要:** The rapid progress of Large Language Models has advanced agentic systems in
decision-making, coordination, and task execution. Yet, existing agentic system
generation frameworks lack full autonomy, missing from-scratch agent
generation, self-optimizing agent functionality, and collaboration, limiting
adaptability and scalability. We propose SwarmAgentic, a framework for fully
automated agentic system generation that constructs agentic systems from
scratch and jointly optimizes agent functionality and collaboration as
interdependent components through language-driven exploration. To enable
efficient search over system-level structures, SwarmAgentic maintains a
population of candidate systems and evolves them via feedback-guided updates,
drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our
method on six real-world, open-ended, and exploratory tasks involving
high-level planning, system-level coordination, and creative reasoning. Given
only a task description and an objective function, SwarmAgentic outperforms all
baselines, achieving a +261.8% relative improvement over ADAS on the
TravelPlanner benchmark, highlighting the effectiveness of full automation in
structurally unconstrained tasks. This framework marks a significant step
toward scalable and autonomous agentic system design, bridging swarm
intelligence with fully automated system multi-agent generation. Our code is
publicly released at https://yaoz720.github.io/SwarmAgentic/.

</details>


### [101] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang*

**主要类别:** cs.AI

**AI概要:** 本文介绍了一种名为Embodied Web Agents的新范式，它能够将物理交互与大规模网络推理无缝结合。通过构建统一的模拟平台和基准测试，揭示了现有AI系统与人类能力之间的显著差距，并提出了在具身认知和网络规模知识访问交叉点上的挑战与机遇。


<details>
  <summary>更多</summary>
  
**动机:** 当前的人工智能体大多局限于单一领域，要么处理大量在线获取的数字信息和知识，要么通过感知、规划和行动与物理世界互动，但很少能同时做到两者。这种分离限制了解决需要综合物理和数字智能的任务的能力，如根据网上食谱烹饪、利用动态地图数据导航或使用网络知识解释现实世界的地标。

**方法:** 研究者首先开发了Embodied Web Agents任务环境，这是一个统一的模拟平台，紧密集成了逼真的3D室内和室外环境与功能性的网页界面。基于此平台，他们构建并发布了Embodied Web Agents Benchmark，涵盖了一系列多样的任务，包括烹饪、导航、购物、旅游以及地理定位等，所有这些任务都需要跨越物理和数字领域的协调推理。

**结果:** 实验结果揭示了最先进的AI系统与人类能力之间存在显著的表现差距，指出了在具身认知与大规模网络知识访问交汇处所面临的挑战与机会。

**结论:** Embodied Web Agents为AI代理提供了一个新的范例，该范例能够流畅地桥接具身化与网络级推理。通过建立一个统一的仿真平台及一系列多样化任务组成的评估基准，强调了跨域智能的重要性。此外，公开的数据集、代码和网站为进一步的研究提供了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Embodied+Web+Agents%3A+Bridging+Physical-Digital+Realms+for+Integrated+Agent+Intelligence，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15677，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15677&send_immediately=true&force_search=false)

**原文摘要:** AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [102] [Optimal Convergence Rates of Deep Neural Network Classifiers](https://arxiv.org/abs/2506.14899)
*Zihan Zhang, Lei Shi, Ding-Xuan Zhou*

**主要类别:** stat.ML

**AI概要:** 本文研究了在Tsybakov噪声条件下和合成假设下的二分类问题，证明了与输入维度无关的最优收敛率，并展示了ReLU深度神经网络（DNN）训练可以达到这个最优收敛率（忽略对数因子）。


<details>
  <summary>更多</summary>
  
**动机:** 研究目的是为了理解在高维设置下，使用ReLU DNN进行分类任务时表现出色的理论依据。

**方法:** 通过扩展先前工作中的oracle不等式技术来建立结果，并考虑了数据分布的条件类别概率函数是由$q+1$个向量值多变量函数组成的假设。

**结果:** 得出了一个与输入维度$d$无关的最优收敛率公式，并表明用铰链损失训练的ReLU DNN可以达到这个收敛率（至多差一个对数因子）。

**结论:** 该研究为ReLU DNN在实际分类任务尤其是高维环境下的出色表现提供了理论支持。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimal+Convergence+Rates+of+Deep+Neural+Network+Classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14899，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14899&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we study the binary classification problem on $[0,1]^d$ under
the Tsybakov noise condition (with exponent $s \in [0,\infty]$) and the
compositional assumption. This assumption requires the conditional class
probability function of the data distribution to be the composition of $q+1$
vector-valued multivariate functions, where each component function is either a
maximum value function or a H\"{o}lder-$\beta$ smooth function that depends
only on $d_*$ of its input variables. Notably, $d_*$ can be significantly
smaller than the input dimension $d$. We prove that, under these conditions,
the optimal convergence rate for the excess 0-1 risk of classifiers is $$
\left( \frac{1}{n}
\right)^{\frac{\beta\cdot(1\wedge\beta)^q}{{\frac{d_*}{s+1}+(1+\frac{1}{s+1})\cdot\beta\cdot(1\wedge\beta)^q}}}\;\;\;,
$$ which is independent of the input dimension $d$. Additionally, we
demonstrate that ReLU deep neural networks (DNNs) trained with hinge loss can
achieve this optimal convergence rate up to a logarithmic factor. This result
provides theoretical justification for the excellent performance of ReLU DNNs
in practical classification tasks, particularly in high-dimensional settings.
The technique used to establish these results extends the oracle inequality
presented in our previous work. The generalized approach is of independent
interest.

</details>


### [103] [Double Machine Learning for Conditional Moment Restrictions: IV regression, Proximal Causal Learning and Beyond](https://arxiv.org/abs/2506.14950)
*Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska*

**主要类别:** stat.ML

**AI概要:** 提出了一种名为DML-CMR的两阶段条件矩限制(CMR)估计器，它通过减少偏差并遵循双/去偏机器学习(DML)框架来提供无偏估计，并且具有快速收敛速度的保证。


<details>
  <summary>更多</summary>
  
**动机:** 解决条件矩限制(CMRs)是统计学、因果推断和计量经济学中考虑的一个关键问题，旨在求解满足某些条件矩等式的感兴趣函数。许多因果推断技术都是CMR问题。大多数CMR估计量使用两阶段方法，其中第一阶段估计直接插入第二阶段以估计感兴趣的函数。然而，直接插入第一阶段估计量会导致第二阶段出现严重的偏差。特别是对于最近提出的在两个阶段都使用深度神经网络(DNN)估计量的CMR估计量，存在正则化和过拟合偏差的问题。

**方法:** 提出了DML-CMR，这是一种两阶段CMR估计器，能够提供无偏估计，并具有快速收敛速度的保障。根据双/去偏机器学习(DML)框架，导出了一个新的学习目标以减少偏差，并开发了DML-CMR算法。

**结果:** 研究表明DML-CMR估计器可以在参数化和温和的正则性条件下达到最小最大最优收敛速率O(N^{-1/2})，其中N是样本量。将DML-CMR应用于一系列使用DNN估计器的问题，包括工具变量回归和近端因果学习，在真实数据集上展示了最先进的性能。

**结论:** DML-CMR是一种有效的两阶段CMR估计器，能够为各种问题提供无偏估计，并且具有比现有估计器更快的收敛速度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Double+Machine+Learning+for+Conditional+Moment+Restrictions%3A+IV+regression%2C+Proximal+Causal+Learning+and+Beyond，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14950，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14950&send_immediately=true&force_search=false)

**原文摘要:** Solving conditional moment restrictions (CMRs) is a key problem considered in
statistics, causal inference, and econometrics, where the aim is to solve for a
function of interest that satisfies some conditional moment equalities.
Specifically, many techniques for causal inference, such as instrumental
variable (IV) regression and proximal causal learning (PCL), are CMR problems.
Most CMR estimators use a two-stage approach, where the first-stage estimation
is directly plugged into the second stage to estimate the function of interest.
However, naively plugging in the first-stage estimator can cause heavy bias in
the second stage. This is particularly the case for recently proposed CMR
estimators that use deep neural network (DNN) estimators for both stages, where
regularisation and overfitting bias is present. We propose DML-CMR, a two-stage
CMR estimator that provides an unbiased estimate with fast convergence rate
guarantees. We derive a novel learning objective to reduce bias and develop the
DML-CMR algorithm following the double/debiased machine learning (DML)
framework. We show that our DML-CMR estimator can achieve the minimax optimal
convergence rate of $O(N^{-1/2})$ under parameterisation and mild regularity
conditions, where $N$ is the sample size. We apply DML-CMR to a range of
problems using DNN estimators, including IV regression and proximal causal
learning on real-world datasets, demonstrating state-of-the-art performance
against existing CMR estimators and algorithms tailored to those problems.

</details>


### [104] [An Observation on Lloyd's k-Means Algorithm in High Dimensions](https://arxiv.org/abs/2506.14952)
*David Silva-Sánchez, Roy R. Lederman*

**主要类别:** stat.ML

**AI概要:** 本文研究了k-means算法在高维度、高噪声和小样本量情况下的失效问题，通过简单的高斯混合模型（GMM）进行理论解释，并确定了数据的几乎每个划分都可能成为k-means算法固定点的情况。


<details>
  <summary>更多</summary>
  
**动机:** 本研究旨在解决k-means算法在高维环境、高噪声水平和有限样本大小的情况下性能不佳的问题，特别是在复杂的场景中，比如掩膜GMMs以及来自冷冻电子显微镜应用中的挑战。

**方法:** 采用理论分析的方式，利用简单的高斯混合模型（GMM）来说明k-means算法为何会在某些情况下失效。

**结果:** 研究表明，在特定条件下，数据的几乎所有分组都会变成k-means算法的一个固定点，这揭示了该算法在高维且有噪声的数据集上的局限性。

**结论:** 结论是k-means算法在高维、高噪声和样本量有限的情况下存在固有的局限性，对于复杂情况如掩膜GMM或冷冻电镜的应用，需要考虑其他更合适的聚类方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Observation+on+Lloyd%27s+k-Means+Algorithm+in+High+Dimensions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.14952，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.14952&send_immediately=true&force_search=false)

**原文摘要:** Clustering and estimating cluster means are core problems in statistics and
machine learning, with k-means and Expectation Maximization (EM) being two
widely used algorithms. In this work, we provide a theoretical explanation for
the failure of k-means in high-dimensional settings with high noise and limited
sample sizes, using a simple Gaussian Mixture Model (GMM). We identify regimes
where, with high probability, almost every partition of the data becomes a
fixed point of the k-means algorithm. This study is motivated by challenges in
the analysis of more complex cases, such as masked GMMs, and those arising from
applications in Cryo-Electron Microscopy.

</details>


### [105] [Performative Validity of Recourse Explanations](https://arxiv.org/abs/2506.15366)
*Gunnar König, Hidde Fokkema, Timo Freiesleben, Celestine Mendler-Dünner, Ulrike on Luxburg*

**主要类别:** stat.ML

**AI概要:** 当许多申请者根据算法决策系统的建议采取行动时，可能会改变数据中的统计规律性，并且在模型重新拟合后也改变了决策边界。这可能导致补救建议变得无效。研究正式描述了在表现力下补救解释保持有效的条件，并发现如果补救行为受到非因果变量的影响或干预，它们可能变得无效。


<details>
  <summary>更多</summary>
  
**动机:** 论文探讨了当大量申请者按照算法给出的补救建议采取行动后，这种集体行为如何影响数据的统计特征，并且一旦模型更新，又如何改变决策边界。这种现象称为表现力，它可能导致原本有效的补救建议失效。

**方法:** 通过形式化分析，研究者确定了在考虑表现力的情况下，补救解释能够保持有效的条件。

**结果:** 研究的关键发现是，如果补救行动受到了非因果变量的影响或者干预了这些变量，那么这些建议可能会变得无效。

**结论:** 基于上述分析，作者不建议使用标准的反事实解释和因果补救方法，而是提倡仅对因果变量提出行动建议的补救方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Performative+Validity+of+Recourse+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15366，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15366&send_immediately=true&force_search=false)

**原文摘要:** When applicants get rejected by an algorithmic decision system, recourse
explanations provide actionable suggestions for how to change their input
features to get a positive evaluation. A crucial yet overlooked phenomenon is
that recourse explanations are performative: When many applicants act according
to their recommendations, their collective behavior may change statistical
regularities in the data and, once the model is refitted, also the decision
boundary. Consequently, the recourse algorithm may render its own
recommendations invalid, such that applicants who make the effort of
implementing their recommendations may be rejected again when they reapply. In
this work, we formally characterize the conditions under which recourse
explanations remain valid under performativity. A key finding is that recourse
actions may become invalid if they are influenced by or if they intervene on
non-causal variables. Based on our analysis, we caution against the use of
standard counterfactual explanations and causal recourse methods, and instead
advocate for recourse methods that recommend actions exclusively on causal
variables.

</details>


### [106] [Time-dependent density estimation using binary classifiers](https://arxiv.org/abs/2506.15505)
*Agnimitra Dasgupta, Javier Murgoitio-Esandi, Ali Fardisi, Assad A Oberai*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种数据驱动的方法，通过从样本路径学习多变量随机过程的时间依赖概率密度。该方法使用基于对比估计的新型时间依赖二分类器来区分随机过程在两个相邻时间点的实现。此外，该方法能够显式地建模时间依赖的概率分布，并且可用于合成新的随机向量样本以及在无监督异常检测中的应用。实验表明，该方法能够准确重构复杂的时间依赖、多模式和近似退化的密度，并能有效地扩展到中等高维问题，可靠地检测现实世界数据中的罕见事件。


<details>
  <summary>更多</summary>
  
**动机:** 研究动机在于开发一种能够从样本路径中学习多变量随机过程的时间依赖概率密度的方法，同时这种方法需要能够显式地表示出这个概率分布，并且能够在感兴趣的时域内获取概率密度值。

**方法:** 提出的数据驱动方法包括训练一个基于对比估计的新颖时间依赖二分类器，以区分随机过程在两个邻近时间点的实现情况。对于随机激励驱动系统的应用，使用随机插值生成必要的训练样本路径，然后利用基于梯度的马尔可夫链蒙特卡洛方法产生新的样本。

**结果:** 通过数值实验验证了所提方法可以精确重建复杂的时间依赖性、多模态和近乎简并的密度函数，并且能够有效应用于中等维度较高的问题上，还展示了在无监督异常检测应用中的实用性。

**结论:** 所提出的方法被证明是有效的，它能够准确地重建复杂的时间依赖性和多模态密度，并且能够很好地扩展到较高维度的问题，同时还可以可靠地识别实际数据中的罕见事件。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Time-dependent+density+estimation+using+binary+classifiers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15505，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15505&send_immediately=true&force_search=false)

**原文摘要:** We propose a data-driven method to learn the time-dependent probability
density of a multivariate stochastic process from sample paths, assuming that
the initial probability density is known and can be evaluated. Our method uses
a novel time-dependent binary classifier trained using a contrastive
estimation-based objective that trains the classifier to discriminate between
realizations of the stochastic process at two nearby time instants.
Significantly, the proposed method explicitly models the time-dependent
probability distribution, which means that it is possible to obtain the value
of the probability density within the time horizon of interest. Additionally,
the input before the final activation in the time-dependent classifier is a
second-order approximation to the partial derivative, with respect to time, of
the logarithm of the density. We apply the proposed approach to approximate the
time-dependent probability density functions for systems driven by stochastic
excitations. We also use the proposed approach to synthesize new samples of a
random vector from a given set of its realizations. In such applications, we
generate sample paths necessary for training using stochastic interpolants.
Subsequently, new samples are generated using gradient-based Markov chain Monte
Carlo methods because automatic differentiation can efficiently provide the
necessary gradient. Further, we demonstrate the utility of an explicit
approximation to the time-dependent probability density function through
applications in unsupervised outlier detection. Through several numerical
experiments, we show that the proposed method accurately reconstructs complex
time-dependent, multi-modal, and near-degenerate densities, scales effectively
to moderately high-dimensional problems, and reliably detects rare events among
real-world data.

</details>


### [107] [Revisiting Randomization in Greedy Model Search](https://arxiv.org/abs/2506.15643)
*Xin Chen, Jason M. Klusowski, Yan Shuo Tan, Chang Yu*

**主要类别:** stat.ML

**AI概要:** 本文提出了一种基于特征子采样的贪婪前向选择估计器的集成方法，该方法在稀疏线性回归中使用。通过动态规划的新颖实现，提高了计算效率，并且在多种设置下优于lasso和弹性网络等流行方法。此外，文章通过实验证明了随机集成不仅类似于收缩，而且可以同时减少训练误差和自由度，从而改变了基础估计器的偏差-方差权衡曲线。在正交特征的情况下，集成估计器使用逻辑权重重新调整普通最小二乘系数，扩大了模型搜索空间。


<details>
  <summary>更多</summary>
  
**动机:** 集成随机估计器（如随机森林）已经成为现代数据科学中的基本技术，但可能计算成本高昂，其改善预测性能的机制也未被充分理解。

**方法:** 提出了一个贪婪前向选择估计器的集成，这些估计器通过特征子采样进行随机化——在每次迭代中，从随机子集内选择最佳特征。并设计了一种基于动态规划的新实现，以显著提高计算效率。

**结果:** 通过细致的数值实验表明，所提出的方法可以在广泛的设定下超越诸如lasso和弹性网络之类的流行方法。并且，与普遍认为的随机集成相当于收缩的观点相反，该方法能够同时降低训练误差和自由度，从而改变基础估计器的整体偏差-方差权衡曲线。

**结论:** 研究结果增强了我们对随机森林的理解，并提示一般的隐式正则化可能比显式正则化具有更复杂的影响。在正交特征的情况下，集成估计器利用双参数逻辑权重族重新缩放普通最小二乘系数，从而扩展了模型搜索空间。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Revisiting+Randomization+in+Greedy+Model+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.15643，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.15643&send_immediately=true&force_search=false)

**原文摘要:** Combining randomized estimators in an ensemble, such as via random forests,
has become a fundamental technique in modern data science, but can be
computationally expensive. Furthermore, the mechanism by which this improves
predictive performance is poorly understood. We address these issues in the
context of sparse linear regression by proposing and analyzing an ensemble of
greedy forward selection estimators that are randomized by feature subsampling
-- at each iteration, the best feature is selected from within a random subset.
We design a novel implementation based on dynamic programming that greatly
improves its computational efficiency. Furthermore, we show via careful
numerical experiments that our method can outperform popular methods such as
lasso and elastic net across a wide range of settings. Next, contrary to
prevailing belief that randomized ensembling is analogous to shrinkage, we show
via numerical experiments that it can simultaneously reduce training error and
degrees of freedom, thereby shifting the entire bias-variance trade-off curve
of the base estimator. We prove this fact rigorously in the setting of
orthogonal features, in which case, the ensemble estimator rescales the
ordinary least squares coefficients with a two-parameter family of logistic
weights, thereby enlarging the model search space. These results enhance our
understanding of random forests and suggest that implicit regularization in
general may have more complicated effects than explicit regularization.

</details>
