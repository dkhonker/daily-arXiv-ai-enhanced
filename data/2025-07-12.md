<div id=toc></div>

# 目录

- [cs.LG](#cs.LG) [总数: 87]
- [cs.AI](#cs.AI) [总数: 26]
- [cs.CR](#cs.CR) [总数: 18]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate](https://arxiv.org/abs/2507.07129)
*A. Bochkov*

**主要类别:** cs.LG

**AI概要:** This paper proposes an alternative, flexible approach to scale large language models through non-trainable input embeddings, allowing for modular composition and layer-wise growth without architectural modification.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind the research is to find a more flexible and less resource-intensive process for scaling large language models (LLMs) than the prevailing monolithic, end-to-end training.

**方法:** The paper explores an alternative approach to model development using non-trainable, deterministic input embeddings. It demonstrates two scaling paradigms: seamless modular composition and progressive layer-wise growth.

**结果:** Specialist models can be merged post-training into a single Mixture-of-Experts model with improved performance. A layer-wise constructive training methodology shows stable convergence and correlation between model depth and complex reasoning abilities.

**结论:** The study suggests a shift from monolithic optimization to a more biological or constructive model of AI development, opening new avenues for resource-efficient scaling and continual learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Growing+Transformers%3A+Modular+Composition+and+Layer-wise+Expansion+on+a+Frozen+Substrate，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07129，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07129&send_immediately=true&force_search=false)

**原文摘要:** The prevailing paradigm for scaling large language models (LLMs) involves
monolithic, end-to-end training, a resource-intensive process that lacks
flexibility. This paper explores an alternative, constructive approach to model
development, built upon the foundation of non-trainable, deterministic input
embeddings. In prior [1], we established that high-level semantic reasoning can
emerge in Transformers using frozen embeddings derived from the visual
structure of Unicode glyphs. Here, we demonstrate that this fixed
representational substrate acts as a universal "docking port," enabling two
powerful and efficient scaling paradigms: seamless modular composition and
progressive layer-wise growth.
  First, we show that specialist models trained on disparate datasets (e.g.,
Russian and Chinese text) can be merged into a single, more capable
Mixture-of-Experts (MoE) model, post-training, with zero architectural
modification. This is achieved by simply averaging their output logits. The
resulting MoE model exhibits immediate performance improvements on reasoning
benchmarks like MMLU, surpassing its constituent experts without catastrophic
forgetting. Second, we introduce a layer-wise constructive training
methodology, where a deep Transformer is "grown" by progressively stacking and
training one layer at a time. This method demonstrates stable convergence and a
clear correlation between model depth and the emergence of complex reasoning
abilities, such as those required for SQuAD.
  Our findings suggest a paradigm shift from monolithic optimization towards a
more biological or constructive model of AI development, where complexity is
built incrementally and modules can be composed freely. This opens new avenues
for resource-efficient scaling, continual learning, and a more democratized
ecosystem for building powerful AI systems. We release all code and models to
facilitate further research.

</details>


### [2] [FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval](https://arxiv.org/abs/2507.07135)
*François Gardères, Shizhe Chen, Camille-Sovanneary Gauthier, Jean Ponce*

**主要类别:** cs.LG

**AI概要:** This paper introduces FACap, a large-scale fashion-domain CIR dataset, and proposes a new CIR model FashionBLIP-2, which fine-tunes the general-domain BLIP-2 model on FACap to better account for fine-grained fashion-specific information.


<details>
  <summary>更多</summary>
  
**动机:** Recent methods for CIR leverage large pretrained vision-language models (VLMs) and achieve good performance on general-domain concepts like color and texture. However, they still struggle with application domains like fashion, because the rich and diverse vocabulary used in fashion requires specific fine-grained vision and language understanding.

**方法:** FACap, a large-scale, automatically constructed fashion-domain CIR dataset. It leverages web-sourced fashion images and a two-stage annotation pipeline powered by a VLM and a large language model (LLM) to generate accurate and detailed modification texts.

**结果:** Experimental results show that the combination of FashionBLIP-2 and pretraining with FACap significantly improves the model's performance in fashion CIR especially for retrieval with fine-grained modification texts.

**结论:** The combination of FashionBLIP-2 and pretraining with FACap significantly improves the model's performance in fashion CIR especially for retrieval with fine-grained modification texts.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FACap%3A+A+Large-scale+Fashion+Dataset+for+Fine-grained+Composed+Image+Retrieval，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07135，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07135&send_immediately=true&force_search=false)

**原文摘要:** The composed image retrieval (CIR) task is to retrieve target images given a
reference image and a modification text. Recent methods for CIR leverage large
pretrained vision-language models (VLMs) and achieve good performance on
general-domain concepts like color and texture. However, they still struggle
with application domains like fashion, because the rich and diverse vocabulary
used in fashion requires specific fine-grained vision and language
understanding. An additional difficulty is the lack of large-scale fashion
datasets with detailed and relevant annotations, due to the expensive cost of
manual annotation by specialists. To address these challenges, we introduce
FACap, a large-scale, automatically constructed fashion-domain CIR dataset. It
leverages web-sourced fashion images and a two-stage annotation pipeline
powered by a VLM and a large language model (LLM) to generate accurate and
detailed modification texts. Then, we propose a new CIR model FashionBLIP-2,
which fine-tunes the general-domain BLIP-2 model on FACap with lightweight
adapters and multi-head query-candidate matching to better account for
fine-grained fashion-specific information. FashionBLIP-2 is evaluated with and
without additional fine-tuning on the Fashion IQ benchmark and the enhanced
evaluation dataset enhFashionIQ, leveraging our pipeline to obtain
higher-quality annotations. Experimental results show that the combination of
FashionBLIP-2 and pretraining with FACap significantly improves the model's
performance in fashion CIR especially for retrieval with fine-grained
modification texts, demonstrating the value of our dataset and approach in a
highly demanding environment such as e-commerce websites. Code is available at
https://fgxaos.github.io/facap-paper-website/.

</details>


### [3] [Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge](https://arxiv.org/abs/2507.07137)
*Eric Yeats, Darryl Hannan, Henry Kvinge, Timothy Doster, Scott Mahan*

**主要类别:** cs.LG

**AI概要:** This paper introduces autoeval-dmun, an automated tool used to evaluate popular diffusion model unlearning methods.


<details>
  <summary>更多</summary>
  
**动机:** Machine unlearning (MU) is a cost-effective method to cleanse undesired information from foundational diffusion models. However, it can be challenging and labor-intensive to prove that the information has been fully removed and MU can damage diffusion model performance on surrounding concepts.

**方法:** The paper introduces autoeval-dmun, an automated tool which leverages (vision-) language models to thoroughly assess unlearning in diffusion models.

**结果:** Autoeval-dmun extracts structured, relevant world knowledge from the language model to identify nearby concepts which are likely damaged by unlearning and to circumvent unlearning with adversarial prompts.

**结论:** Language models can impose semantic orderings of nearby concepts and effectively circumvent unlearning with synthetic adversarial prompts.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automating+Evaluation+of+Diffusion+Model+Unlearning+with+%28Vision-%29+Language+Model+World+Knowledge，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07137&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning (MU) is a promising cost-effective method to cleanse
undesired information (generated concepts, biases, or patterns) from
foundational diffusion models. While MU is orders of magnitude less costly than
retraining a diffusion model without the undesired information, it can be
challenging and labor-intensive to prove that the information has been fully
removed from the model. Moreover, MU can damage diffusion model performance on
surrounding concepts that one would like to retain, making it unclear if the
diffusion model is still fit for deployment. We introduce autoeval-dmun, an
automated tool which leverages (vision-) language models to thoroughly assess
unlearning in diffusion models. Given a target concept, autoeval-dmun extracts
structured, relevant world knowledge from the language model to identify nearby
concepts which are likely damaged by unlearning and to circumvent unlearning
with adversarial prompts. We use our automated tool to evaluate popular
diffusion model unlearning methods, revealing that language models (1) impose
semantic orderings of nearby concepts which correlate well with unlearning
damage and (2) effectively circumvent unlearning with synthetic adversarial
prompts.

</details>


### [4] [GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction](https://arxiv.org/abs/2507.07138)
*Francesco Ferrini, Veronica Lachi, Antonio Longa, Bruno Lepri, Andrea Passerini*

**主要类别:** cs.LG

**AI概要:** SP4LP, a novel framework combining GNN-based node encodings with sequence modeling over shortest paths, captures multi-hop relational patterns efficiently.


<details>
  <summary>更多</summary>
  
**动机:** GNNs struggle to capture link-specific structural patterns crucial for accurate link prediction.

**方法:** Combines GNN-based node encodings with sequence modeling over shortest paths.

**结果:** SP4LP achieves state-of-the-art performance across link prediction benchmarks.

**结论:** SP4LP is a more expressive and efficient approach for link prediction in graphs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GNNs+Meet+Sequence+Models+Along+the+Shortest-Path%3A+an+Expressive+Method+for+Link+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07138，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07138&send_immediately=true&force_search=false)

**原文摘要:** Graph Neural Networks (GNNs) often struggle to capture the link-specific
structural patterns crucial for accurate link prediction, as their node-centric
message-passing schemes overlook the subgraph structures connecting a pair of
nodes. Existing methods to inject such structural context either incur high
computational cost or rely on simplistic heuristics (e.g., common neighbor
counts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest
Path for Link Prediction), a novel framework that combines GNN-based node
encodings with sequence modeling over shortest paths. Specifically, SP4LP first
applies a GNN to compute representations for all nodes, then extracts the
shortest path between each candidate node pair and processes the resulting
sequence of node embeddings using a sequence model. This design enables SP4LP
to capture expressive multi-hop relational patterns with computational
efficiency. Empirically, SP4LP achieves state-of-the-art performance across
link prediction benchmarks. Theoretically, we prove that SP4LP is strictly more
expressive than standard message-passing GNNs and several state-of-the-art
structural features methods, establishing it as a general and principled
approach for link prediction in graphs.

</details>


### [5] [Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts](https://arxiv.org/abs/2507.07140)
*Samin Yeasar Arnob, Zhan Su, Minseon Kim, Oleksiy Ostapenko, Riyasat Ohib, Esra'a Saleh, Doina Precup, Lucas Caccia, Alessandro Sordoni*

**主要类别:** cs.LG

**AI概要:** This paper explores the properties of sparse adapters as building blocks of modular architectures and proposes a simple method for training them, demonstrating their superiority over LoRA and full fine-tuning.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to study the properties of sparse adapters as potential building blocks of modular architectures that can be rapidly adapted on the fly for specific downstream tasks without additional fine-tuning.

**方法:** The authors propose a simple method for training sparse adapters and investigate their merging properties by merging adapters for up to 20 natural language processing tasks.

**结果:** Sparse adapters outperform both LoRA and full fine-tuning in the authors' setting and yield superior in-distribution performance post-merging compared to LoRA or full model merging.

**结论:** Sparse adapters are effective building blocks for modular architectures and outperform LoRA and full fine-tuning in certain settings, but achieving strong held-out performance is still a challenge.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring+Sparse+Adapters+for+Scalable+Merging+of+Parameter+Efficient+Experts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07140，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07140&send_immediately=true&force_search=false)

**原文摘要:** Merging parameter-efficient task experts has recently gained growing
attention as a way to build modular architectures that can be rapidly adapted
on the fly for specific downstream tasks, without requiring additional
fine-tuning. Typically, LoRA serves as the foundational building block of such
parameter-efficient modular architectures, leveraging low-rank weight
structures to reduce the number of trainable parameters. In this paper, we
study the properties of sparse adapters, which train only a subset of weights
in the base neural network, as potential building blocks of modular
architectures. First, we propose a simple method for training highly effective
sparse adapters, which is conceptually simpler than existing methods in the
literature and surprisingly outperforms both LoRA and full fine-tuning in our
setting. Next, we investigate the merging properties of these sparse adapters
by merging adapters for up to 20 natural language processing tasks, thus
scaling beyond what is usually studied in the literature. Our findings
demonstrate that sparse adapters yield superior in-distribution performance
post-merging compared to LoRA or full model merging. Achieving strong held-out
performance remains a challenge for all methods considered.

</details>


### [6] [Str-GCL: Structural Commonsense Driven Graph Contrastive Learning](https://arxiv.org/abs/2507.07141)
*Dongxiao He, Yongqi Huang, Jitao Zhao, Xiaobao Wang, Zhen Wang*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel framework called Structural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL), which leverages first-order logic rules to represent structural commonsense and explicitly integrates them into the GCL framework.


<details>
  <summary>更多</summary>
  
**动机:** Current GCL methods primarily focus on capturing implicit semantic relationships, often overlooking the structural commonsense embedded within the graph's structure and attributes, which contains underlying knowledge crucial for effective representation learning.

**方法:** Structural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL) leverages first-order logic rules to represent structural commonsense and explicitly integrates them into the GCL framework. It introduces topological and attribute-based rules without altering the original graph and employs a representation alignment mechanism.

**结果:** Extensive experiments demonstrate that Str-GCL outperforms existing GCL methods.

**结论:** Str-GCL outperforms existing GCL methods, providing a new perspective on leveraging structural commonsense in graph representation learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Str-GCL%3A+Structural+Commonsense+Driven+Graph+Contrastive+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07141&send_immediately=true&force_search=false)

**原文摘要:** Graph Contrastive Learning (GCL) is a widely adopted approach in
self-supervised graph representation learning, applying contrastive objectives
to produce effective representations. However, current GCL methods primarily
focus on capturing implicit semantic relationships, often overlooking the
structural commonsense embedded within the graph's structure and attributes,
which contains underlying knowledge crucial for effective representation
learning. Due to the lack of explicit information and clear guidance in general
graph, identifying and integrating such structural commonsense in GCL poses a
significant challenge. To address this gap, we propose a novel framework called
Structural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL).
Str-GCL leverages first-order logic rules to represent structural commonsense
and explicitly integrates them into the GCL framework. It introduces
topological and attribute-based rules without altering the original graph and
employs a representation alignment mechanism to guide the encoder in
effectively capturing this commonsense. To the best of our knowledge, this is
the first attempt to directly incorporate structural commonsense into GCL.
Extensive experiments demonstrate that Str-GCL outperforms existing GCL
methods, providing a new perspective on leveraging structural commonsense in
graph representation learning.

</details>


### [7] [Understanding Malware Propagation Dynamics through Scientific Machine Learning](https://arxiv.org/abs/2507.07143)
*Karthik Pappu, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat*

**主要类别:** cs.LG

**AI概要:** This paper applies scientific machine learning to improve malware propagation models, demonstrating that hybrid physics-informed models outperform both analytical and neural approaches.


<details>
  <summary>更多</summary>
  
**动机:** Accurately modeling malware propagation is essential for designing effective cybersecurity defenses against adaptive threats.

**方法:** The study evaluates three approaches: classical Ordinary Differential Equations (ODEs), Universal Differential Equations (UDEs), and Neural ODEs using data from the Code Red worm outbreak. A symbolic recovery method is introduced to transform neural feedback into mathematical expressions.

**结果:** The UDE approach reduces prediction error by 44% compared to traditional and neural baselines and preserves interpretability. Suppression mechanisms such as network saturation, security response, and malware variant evolution are revealed.

**结论:** Hybrid physics-informed models offer improved predictive accuracy and deeper insight into malware spread dynamics, supporting the development of early warning systems and targeted cyber defense interventions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Understanding+Malware+Propagation+Dynamics+through+Scientific+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07143&send_immediately=true&force_search=false)

**原文摘要:** Accurately modeling malware propagation is essential for designing effective
cybersecurity defenses, particularly against adaptive threats that evolve in
real time. While traditional epidemiological models and recent neural
approaches offer useful foundations, they often fail to fully capture the
nonlinear feedback mechanisms present in real-world networks. In this work, we
apply scientific machine learning to malware modeling by evaluating three
approaches: classical Ordinary Differential Equations (ODEs), Universal
Differential Equations (UDEs), and Neural ODEs. Using data from the Code Red
worm outbreak, we show that the UDE approach substantially reduces prediction
error compared to both traditional and neural baselines by 44%, while
preserving interpretability. We introduce a symbolic recovery method that
transforms the learned neural feedback into explicit mathematical expressions,
revealing suppression mechanisms such as network saturation, security response,
and malware variant evolution. Our results demonstrate that hybrid
physics-informed models can outperform both purely analytical and purely neural
approaches, offering improved predictive accuracy and deeper insight into the
dynamics of malware spread. These findings support the development of early
warning systems, efficient outbreak response strategies, and targeted cyber
defense interventions.

</details>


### [8] [CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs](https://arxiv.org/abs/2507.07145)
*Zhaojing Zhou, Xunchao Li, Minghao Li, Handi Zhang, Haoshuang Wang, Wenbin Chang, Yiqun Liu, Qingqing Dang, Dianhai Yu, Yanjun Ma, Haifeng Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出一种新的面向大型语言模型的推理优化量化方法——卷积码量化（CCQ），能够有效减少模型大小，解决了在极低位条件下模型性能下降的问题，并在多个基准测试中展示了其出色的性能。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）的快速扩展提升了推理成本并增加了部署障碍。虽然8或4位的量化可以缓解这一问题，但低于3位的方法面临严重的准确性、可伸缩性和效率下降问题。

**方法:** 研究提出了一种名为卷积码量化（CCQ）的方法，这是一种面向推理优化的量化方法，可以将LLMs压缩到2.0-2.75位，而且准确度损失最小。CCQ集成了一个硬件感知的位移编码和解码方案与卷积码、混合编码和码簇，共同克服了准确性-速度瓶颈。

**结果:** 实验表明，CCQ在各个基准测试中的LLMs上都表现出色。研究成功地将DeepSeek-V3(总参数671B)压缩至184GB，ERNIE-4.5-300B-A47B压缩至89GB，使得ERNIE 4.5能够在单个GPU上部署，并消除了卡间通信。

**结论:** CCQ实现了在各种基准测试中对LLMs的出色性能表现，能有效压缩大型模型，使之可以在单个GPU上部署，并且2位的ERNIE-4.5-300B-A47B模型和推理引擎已经开源。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CCQ%3A+Convolutional+Code+for+Extreme+Low-bit+Quantization+in+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07145，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07145&send_immediately=true&force_search=false)

**原文摘要:** The rapid scaling of Large Language Models (LLMs) elevates inference costs
and compounds substantial deployment barriers. While quantization to 8 or 4
bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and
efficiency degradation. We propose Convolutional Code Quantization (CCQ), an
inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits
with minimal accuracy loss. Departing from error-prone scalar quantization or
slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding
and decoding solution with Convolutional Code, Hybrid Encoding, and Code
Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a
lookup-free encoding space, enabling a linear mapping between the codebook and
weight vectors, thereby optimizing inference performance. Meanwhile, by drawing
on the concept of data mapping from vector quantization, we minimize the
performance degradation of the model under extremely low-bit conditions.
Experiments demonstrate that CCQ achieves outstanding performance on LLMs
across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to
184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE
4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B
model and inference engine have been open-sourced.

</details>


### [9] [An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs](https://arxiv.org/abs/2507.07146)
*Zixuan Huang, Kecheng Huang, Lihao Yin, Bowei He, Huiling Zhen, Mingxuan Yuan, Zili Shao*

**主要类别:** cs.LG

**AI概要:** 尽管大型语言模型已经过严格训练和微调以确保安全，但它们仍然容易受到越狱攻击的影响。本研究提出了一种创新的基于注意力感知的GNN输入分类器——G-Guard，用于防御对LLM的多轮越狱攻击。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型（LLMs）虽然经过严格的训练和微调以确保安全，但仍然容易受到越狱攻击的影响。最近出现的多轮攻击加剧了这一问题。与单轮攻击不同，多轮攻击逐渐升级对话，使其更难以检测和缓解，即便在被识别后仍是如此。

**方法:** 提出了一种创新的基于注意力感知的GNN输入分类器，用于防御对LLM的多轮越狱攻击。G-Guard为多轮查询构建了一个实体图，明确捕获了有害关键词与查询之间的关系，即使这些关键词仅出现在之前的查询中。此外，引入了一种注意力感知增强机制，根据多轮对话检索最相似的单轮查询。

**结果:** 评估结果表明，G-Guard在所有数据集和评估指标上都优于所有基线方法。

**结论:** G-Guard在所有数据集和评估指标上都优于所有基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+attention-aware+GNN-based+input+defender+against+multi-turn+jailbreak+on+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07146&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) have gained widespread popularity and are
increasingly integrated into various applications. However, their capabilities
can be exploited for both benign and harmful purposes. Despite rigorous
training and fine-tuning for safety, LLMs remain vulnerable to jailbreak
attacks. Recently, multi-turn attacks have emerged, exacerbating the issue.
Unlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,
making them more difficult to detect and mitigate, even after they are
identified.
  In this study, we propose G-Guard, an innovative attention-aware GNN-based
input classifier designed to defend against multi-turn jailbreak attacks on
LLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly
capturing relationships between harmful keywords and queries even when those
keywords appear only in previous queries. Additionally, we introduce an
attention-aware augmentation mechanism that retrieves the most similar
single-turn query based on the multi-turn conversation. This retrieved query is
treated as a labeled node in the graph, enhancing the ability of GNN to
classify whether the current query is harmful. Evaluation results demonstrate
that G-Guard outperforms all baselines across all datasets and evaluation
metrics.

</details>


### [10] [Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation](https://arxiv.org/abs/2507.07147)
*Sua Lee, Kyubum Shin, Jung Ho Park*

**主要类别:** cs.LG

**AI概要:** This paper proposes Description-free Multi-prompt Learning (DeMul), which eliminates the process of extracting descriptions and instead directly distills knowledge from LLM into prompts, achieving superior performance across 11 recognition datasets.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods that extract text-based responses from LLM to incorporate into prompts suffer from high variability and low reliability.

**方法:** Description-free Multi-prompt Learning (DeMul) eliminates the process of extracting descriptions and instead directly distills knowledge from LLM into prompts. Prompt weighting is used to reflect the importance of different prompts during training.

**结果:** Experimental results show that DeMul achieves superior performance across 11 recognition datasets.

**结论:** The proposed Description-free Multi-prompt Learning (DeMul) approach achieves superior performance across 11 recognition datasets.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Weighted+Multi-Prompt+Learning+with+Description-free+Large+Language+Model+Distillation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07147，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07147&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in pre-trained Vision Language Models (VLM) have shown
promising potential for effectively adapting to downstream tasks through prompt
learning, without the need for additional annotated paired datasets. To
supplement the text information in VLM trained on correlations with vision
data, new approaches leveraging Large Language Models (LLM) in prompts have
been proposed, enhancing robustness to unseen and diverse data. Existing
methods typically extract text-based responses (i.e., descriptions) from LLM to
incorporate into prompts; however, this approach suffers from high variability
and low reliability. In this work, we propose Description-free Multi-prompt
Learning(DeMul), a novel method that eliminates the process of extracting
descriptions and instead directly distills knowledge from LLM into prompts. By
adopting a description-free approach, prompts can encapsulate richer semantics
while still being represented as continuous vectors for optimization, thereby
eliminating the need for discrete pre-defined templates. Additionally, in a
multi-prompt setting, we empirically demonstrate the potential of prompt
weighting in reflecting the importance of different prompts during training.
Experimental results show that our approach achieves superior performance
across 11 recognition datasets.

</details>


### [11] [Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching](https://arxiv.org/abs/2507.07192)
*Huibo Xu, Runlong Yu, Likang Wu, Xianquan Wang, Qi Liu*

**主要类别:** cs.LG

**AI概要:** To address challenges in diffusion models for time series forecasting, this paper proposes CGFM, which learns from the errors of an auxiliary model and integrates historical data for improved predictions.


<details>
  <summary>更多</summary>
  
**动机:** Diffusion models face limitations like rigid source distributions and limited sampling paths in time series forecasting, and flow matching offers faster generation, higher-quality outputs, and greater flexibility while possessing the ability to utilize valuable information from the prediction errors of prior models.

**方法:** The paper proposes Conditional Guided Flow Matching (CGFM) which extends flow matching by incorporating the outputs of an auxiliary model, integrating historical data as conditions and guidance, constructing two-sided conditional probability paths, and using a general affine path to expand the space of probability paths.

**结果:** Extensive experiments show that CGFM consistently enhances and outperforms state-of-the-art models.

**结论:** CGFM enhances and outperforms state-of-the-art models in time series forecasting.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+the+Last+Mile+of+Prediction%3A+Enhancing+Time+Series+Forecasting+with+Conditional+Guided+Flow+Matching，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07192&send_immediately=true&force_search=false)

**原文摘要:** Diffusion models, a type of generative model, have shown promise in time
series forecasting. But they face limitations like rigid source distributions
and limited sampling paths, which hinder their performance. Flow matching
offers faster generation, higher-quality outputs, and greater flexibility,
while also possessing the ability to utilize valuable information from the
prediction errors of prior models, which were previously inaccessible yet
critically important. To address these challenges and fully unlock the untapped
potential of flow matching, we propose Conditional Guided Flow Matching (CGFM).
CGFM extends flow matching by incorporating the outputs of an auxiliary model,
enabling a previously unattainable capability in the field: learning from the
errors of the auxiliary model. For time series forecasting tasks, it integrates
historical data as conditions and guidance, constructs two-sided conditional
probability paths, and uses a general affine path to expand the space of
probability paths, ultimately leading to improved predictions. Extensive
experiments show that CGFM consistently enhances and outperforms
state-of-the-art models, highlighting its effectiveness in advancing
forecasting methods.

</details>


### [12] [Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning](https://arxiv.org/abs/2507.07197)
*Elia Piccoli, Malio Li, Giacomo Carfì, Vincenzo Lomonaco, Davide Bacciu*

**主要类别:** cs.LG

**AI概要:** This paper proposes Weight Sharing Attention (WSA), a new architecture for combining embeddings of multiple pre-trained models to shape an enriched state representation in Reinforcement Learning.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this work is to effectively combine and leverage the hidden information of different pre-trained models simultaneously in Reinforcement Learning.

**方法:** The paper proposes Weight Sharing Attention (WSA), a new architecture that combines embeddings of multiple pre-trained models to shape an enriched state representation.

**结果:** WSA obtains comparable performance on multiple Atari games compared to end-to-end models. The study also shows how scaling the number of models influences agents' performance during and after training.

**结论:** Weight Sharing Attention (WSA) is a promising approach to combine embeddings of multiple pre-trained models for enriched state representation in Reinforcement Learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Combining+Pre-Trained+Models+for+Enhanced+Feature+Representation+in+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07197&send_immediately=true&force_search=false)

**原文摘要:** The recent focus and release of pre-trained models have been a key components
to several advancements in many fields (e.g. Natural Language Processing and
Computer Vision), as a matter of fact, pre-trained models learn disparate
latent embeddings sharing insightful representations. On the other hand,
Reinforcement Learning (RL) focuses on maximizing the cumulative reward
obtained via agent's interaction with the environment. RL agents do not have
any prior knowledge about the world, and they either learn from scratch an
end-to-end mapping between the observation and action spaces or, in more recent
works, are paired with monolithic and computationally expensive Foundational
Models. How to effectively combine and leverage the hidden information of
different pre-trained models simultaneously in RL is still an open and
understudied question. In this work, we propose Weight Sharing Attention (WSA),
a new architecture to combine embeddings of multiple pre-trained models to
shape an enriched state representation, balancing the tradeoff between
efficiency and performance. We run an extensive comparison between several
combination modes showing that WSA obtains comparable performance on multiple
Atari games compared to end-to-end models. Furthermore, we study the
generalization capabilities of this approach and analyze how scaling the number
of models influences agents' performance during and after training.

</details>


### [13] [Scale leads to compositional generalization](https://arxiv.org/abs/2507.07207)
*Florian Redhardt, Yassir Akram, Simon Schug*

**主要类别:** cs.LG

**AI概要:** This paper explores whether standard neural networks can generalize over tasks with shared compositional structure, finding that scaling data and model size leads to compositional generalization, and demonstrating that multilayer perceptrons can approximate compositional task families with a linear number of neurons.


<details>
  <summary>更多</summary>
  
**动机:** To understand what it takes for a standard neural network to generalize over tasks that share compositional structure despite their continuous, distributed nature.

**方法:** The study investigates the ability of standard neural networks to generalize over tasks that share compositional structure by scaling data and model size, examining different task encodings, and proving that multilayer perceptrons can approximate compositional task families.

**结果:** Simply scaling data and model size leads to compositional generalization; this holds across different task encodings as long as the training distribution sufficiently covers the task space; multilayer perceptrons can approximate compositional task families with a linear number of neurons; if networks generalize compositionally, the constituents of a task can be linearly decoded from their hidden activations.

**结论:** Scaling data and model size leads to compositional generalization in standard neural networks, and multilayer perceptrons can approximate a general class of compositional task families with a linear number of neurons. If networks successfully generalize compositionally, the constituents of a task can be linearly decoded from their hidden activations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Scale+leads+to+compositional+generalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07207&send_immediately=true&force_search=false)

**原文摘要:** Can neural networks systematically capture discrete, compositional task
structure despite their continuous, distributed nature? The impressive
capabilities of large-scale neural networks suggest that the answer to this
question is yes. However, even for the most capable models, there are still
frequent failure cases that raise doubts about their compositionality. Here, we
seek to understand what it takes for a standard neural network to generalize
over tasks that share compositional structure. We find that simply scaling data
and model size leads to compositional generalization. We show that this holds
across different task encodings as long as the training distribution
sufficiently covers the task space. In line with this finding, we prove that
standard multilayer perceptrons can approximate a general class of
compositional task families to arbitrary precision using only a linear number
of neurons with respect to the number of task modules. Finally, we uncover that
if networks successfully compositionally generalize, the constituents of a task
can be linearly decoded from their hidden activations. We show that this metric
correlates with failures of text-to-image generation models to compose known
concepts.

</details>


### [14] [Bias-Aware Mislabeling Detection via Decoupled Confident Learning](https://arxiv.org/abs/2507.07216)
*Yunyi Li, Maria De-Arteaga, Maytal Saar-Tsechansky*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的机器学习框架DeCoLe，用于处理数据标签偏差问题，从而提高数据质量。


<details>
  <summary>更多</summary>
  
**动机:** 标签偏差是一个广泛认可的紧迫问题，但目前解决这一问题的有效方法仍然稀缺。

**方法:** 提出了一种基于机器学习的框架DeCoLe，用于检测受标签偏差影响的数据集中的错误标记实例。

**结果:** 实证结果表明，DeCoLe在偏差意识的错误标记检测方面表现出色，始终优于其他标签错误检测方法。

**结论:** DeCoLe有效地解决了标签偏差的问题，并可作为提升数据可靠性的有力工具集成到组织的数据管理实践中。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bias-Aware+Mislabeling+Detection+via+Decoupled+Confident+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07216，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07216&send_immediately=true&force_search=false)

**原文摘要:** Reliable data is a cornerstone of modern organizational systems. A notable
data integrity challenge stems from label bias, which refers to systematic
errors in a label, a covariate that is central to a quantitative analysis, such
that its quality differs across social groups. This type of bias has been
conceptually and empirically explored and is widely recognized as a pressing
issue across critical domains. However, effective methodologies for addressing
it remain scarce. In this work, we propose Decoupled Confident Learning
(DeCoLe), a principled machine learning based framework specifically designed
to detect mislabeled instances in datasets affected by label bias, enabling
bias aware mislabelling detection and facilitating data quality improvement. We
theoretically justify the effectiveness of DeCoLe and evaluate its performance
in the impactful context of hate speech detection, a domain where label bias is
a well documented challenge. Empirical results demonstrate that DeCoLe excels
at bias aware mislabeling detection, consistently outperforming alternative
approaches for label error detection. Our work identifies and addresses the
challenge of bias aware mislabeling detection and offers guidance on how DeCoLe
can be integrated into organizational data management practices as a powerful
tool to enhance data reliability.

</details>


### [15] [Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems](https://arxiv.org/abs/2507.07222)
*Minchan Jeong, J. Jon Ryu, Se-Young Yun, Gregory W. Wornell*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种学习随机动力系统中Koopman算子的前k个奇异函数的新方法，避免了数值不稳定的线性代数运算，同时保持了与现代深度学习管道的兼容性。


<details>
  <summary>更多</summary>
  
**动机:** 近期动态模式分解（DMD）的进展表明，可以通过数据驱动的方式使用轨迹数据来识别系统的主导模式。但是目前的方法在目标计算过程中需要通过潜在数值不稳定的运算，这可能会导致梯度估计偏差并阻碍对大系统的可扩展性。

**方法:** 基于低秩逼近的思想，提出了一种可扩展且概念上简单的方法来学习随机动力系统中Koopman算子的前k个奇异函数。

**结果:** 实证结果显示，所学得的奇异子空间在诸如特征分析和多步预测等下游任务中既可靠又有效。

**结论:** 提出的方法消除了不稳定的线性代数运算的需要，并且在下游任务中表现出可靠和有效的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Parametric+SVD+of+Koopman+Operator+for+Stochastic+Dynamical+Systems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07222，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07222&send_immediately=true&force_search=false)

**原文摘要:** The Koopman operator provides a principled framework for analyzing nonlinear
dynamical systems through linear operator theory. Recent advances in dynamic
mode decomposition (DMD) have shown that trajectory data can be used to
identify dominant modes of a system in a data-driven manner. Building on this
idea, deep learning methods such as VAMPnet and DPNet have been proposed to
learn the leading singular subspaces of the Koopman operator. However, these
methods require backpropagation through potentially numerically unstable
operations on empirical second moment matrices, such as singular value
decomposition and matrix inversion, during objective computation, which can
introduce biased gradient estimates and hinder scalability to large systems. In
this work, we propose a scalable and conceptually simple method for learning
the top-k singular functions of the Koopman operator for stochastic dynamical
systems based on the idea of low-rank approximation. Our approach eliminates
the need for unstable linear algebraic operations and integrates easily into
modern deep learning pipelines. Empirical results demonstrate that the learned
singular subspaces are both reliable and effective for downstream tasks such as
eigen-analysis and multi-step prediction.

</details>


### [16] [An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation](https://arxiv.org/abs/2507.07236)
*Maya Kruse, Majid Afshar, Saksham Khatwani, Anoop Mayampurath, Guanhua Chen, Yanjun Gao*

**主要类别:** cs.LG

**AI概要:** 本文探讨了在大型语言模型中利用多样性来改进校准性和预测性能的方法。


<details>
  <summary>更多</summary>
  
**动机:** 大型语言模型在不同输入中的行为往往不一致，这表明了在高风险环境中量化不确定性的必要性。

**方法:** 研究提出了一种简单的方法，即通过子集集成利用多个大型语言模型的不确定性（MUSE）。

**结果:** 实验结果表明，在二元预测任务上，与单个模型和朴素集成基线相比，该方法提高了校准性和预测性能。

**结论:** 使用MUSE方法可以提高大型语言模型的校准性和预测性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Information-Theoretic+Perspective+on+Multi-LLM+Uncertainty+Estimation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07236，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07236&send_immediately=true&force_search=false)

**原文摘要:** Large language models (LLMs) often behave inconsistently across inputs,
indicating uncertainty and motivating the need for its quantification in
high-stakes settings. Prior work on calibration and uncertainty quantification
often focuses on individual models, overlooking the potential of model
diversity. We hypothesize that LLMs make complementary predictions due to
differences in training and the Zipfian nature of language, and that
aggregating their outputs leads to more reliable uncertainty estimates. To
leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a
simple information-theoretic method that uses Jensen-Shannon Divergence to
identify and aggregate well-calibrated subsets of LLMs. Experiments on binary
prediction tasks demonstrate improved calibration and predictive performance
compared to single-model and naive ensemble baselines.

</details>


### [17] [Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture](https://arxiv.org/abs/2507.07237)
*Erfan Hamdi, Emma Lejeune*

**主要类别:** cs.LG

**AI概要:** This paper introduces a challenging dataset based on phase field modeling simulations designed to benchmark and advance machine learning methods for fracture modeling.


<details>
  <summary>更多</summary>
  
**动机:** Machine learning techniques have shown promise in approximating PFM simulations but most studies rely on overly simple benchmarks that do not reflect the true complexity of the fracture processes where PFM excels as a method. Therefore, a challenging dataset is introduced to address this gap.

**方法:** The study introduces a challenging dataset based on PFM simulations designed to benchmark and advance ML methods for fracture modeling. The dataset includes three energy decomposition methods, two boundary conditions, and 1,000 random initial crack configurations for a total of 6,000 simulations. Each sample contains 100 time steps capturing the temporal evolution of the crack field. Physics Informed Neural Networks (PINN), Fourier Neural Operators (FNO) and UNet models are implemented and evaluated as baselines.

**结果:** The combination of the dataset and baseline models drawn from the literature provides a standardized and challenging benchmark for evaluating machine learning approaches to solid mechanics. The results highlight both the promise and limitations of popular current models.

**结论:** The dataset and baseline models provide a standardized and challenging benchmark for evaluating machine learning approaches to solid mechanics. The results highlight both the promise and limitations of popular current models and demonstrate the utility of the dataset as a testbed for advancing machine learning in fracture mechanics research.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Robust+Surrogate+Models%3A+Benchmarking+Machine+Learning+Approaches+to+Expediting+Phase+Field+Simulations+of+Brittle+Fracture，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07237，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07237&send_immediately=true&force_search=false)

**原文摘要:** Data driven approaches have the potential to make modeling complex, nonlinear
physical phenomena significantly more computationally tractable. For example,
computational modeling of fracture is a core challenge where machine learning
techniques have the potential to provide a much needed speedup that would
enable progress in areas such as mutli-scale modeling and uncertainty
quantification. Currently, phase field modeling (PFM) of fracture is one such
approach that offers a convenient variational formulation to model crack
nucleation, branching and propagation. To date, machine learning techniques
have shown promise in approximating PFM simulations. However, most studies rely
on overly simple benchmarks that do not reflect the true complexity of the
fracture processes where PFM excels as a method. To address this gap, we
introduce a challenging dataset based on PFM simulations designed to benchmark
and advance ML methods for fracture modeling. This dataset includes three
energy decomposition methods, two boundary conditions, and 1,000 random initial
crack configurations for a total of 6,000 simulations. Each sample contains 100
time steps capturing the temporal evolution of the crack field. Alongside this
dataset, we also implement and evaluate Physics Informed Neural Networks
(PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and
explore the impact of ensembling strategies on prediction accuracy. With this
combination of our dataset and baseline models drawn from the literature we aim
to provide a standardized and challenging benchmark for evaluating machine
learning approaches to solid mechanics. Our results highlight both the promise
and limitations of popular current models, and demonstrate the utility of this
dataset as a testbed for advancing machine learning in fracture mechanics
research.

</details>


### [18] [Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention](https://arxiv.org/abs/2507.07247)
*Zhengyu Tian, Anantha Padmanaban Krishna Kumar, Hemant Krishnakumar, Reza Rawassizadeh*

**主要类别:** cs.LG

**AI概要:** This paper benchmarks eight attention mechanisms in training GPT-2 architecture and finds that optimized kernel implementations achieve the best energy efficiency.


<details>
  <summary>更多</summary>
  
**动机:** As large language models (LLMs) and visual language models (VLMs) grow in scale and application, attention mechanisms have become a central computational bottleneck due to their high memory and time complexity.

**方法:** Benchmark eight attention mechanisms in training GPT-2 architecture, measuring key metrics including training time, GPU memory usage, FLOPS, CPU usage, and power consumption.

**结果:** Attention mechanisms with optimized kernel implementations, including Flash Attention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent Attention (MLA), achieve the best energy efficiency.

**结论:** Attention mechanisms with optimized kernel implementations achieve the best energy efficiency and lower GPU power alone does not guarantee reduced energy use.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Attentions+Under+the+Microscope%3A+A+Comparative+Study+of+Resource+Utilization+for+Variants+of+Self-Attention，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07247，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07247&send_immediately=true&force_search=false)

**原文摘要:** As large language models (LLMs) and visual language models (VLMs) grow in
scale and application, attention mechanisms have become a central computational
bottleneck due to their high memory and time complexity. While many efficient
attention variants have been proposed, there remains a lack of rigorous
evaluation on their actual energy usage and hardware resource demands during
training. In this work, we benchmark eight attention mechanisms in training
GPT-2 architecture, measuring key metrics including training time, GPU memory
usage, FLOPS, CPU usage, and power consumption. Our results reveal that
attention mechanisms with optimized kernel implementations, including Flash
Attention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent
Attention (MLA), achieve the best energy efficiency. We further show that lower
GPU power alone does not guarantee reduced energy use, as training time plays
an equally important role. Our study highlights the importance of energy-aware
benchmarking in attention design and provides a practical insight for selecting
resource-efficient mechanisms. All our codes are available at GitHub.

</details>


### [19] [Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning](https://arxiv.org/abs/2507.07259)
*Giulio Rossolini, Fabio Brau, Alessandro Biondi, Battista Biggio, Giorgio Buttazzo*

**主要类别:** cs.LG

**AI概要:** This paper explores a security vulnerability in partitioned deep learning models in IoT environments, where intercepted intermediate features can be used to craft highly transferable adversarial examples. The authors propose an exploitation strategy to address this issue.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to explore a previously overlooked vulnerability in partitioned deep learning models deployed across the edge of IoT environments. Even when both the edge and cloud components of the model are inaccessible (i.e., black-box), intercepted intermediate features can still pose a serious threat.

**方法:** This work proposes an exploitation strategy specifically designed for distributed settings, involving reconstructing the original tensor shape from vectorized transmitted features using simple statistical analysis, and adapting surrogate architectures accordingly to enable effective feature distillation.

**结果:** Surrogate models trained with the proposed strategy, leveraging intermediate features, tremendously improve the transferability of adversarial attacks.

**结论:** The findings underscore the urgent need to account for intermediate feature leakage in the design of secure distributed deep learning systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploiting+Edge+Features+for+Transferable+Adversarial+Attacks+in+Distributed+Machine+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07259，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07259&send_immediately=true&force_search=false)

**原文摘要:** As machine learning models become increasingly deployed across the edge of
internet of things environments, a partitioned deep learning paradigm in which
models are split across multiple computational nodes introduces a new dimension
of security risk. Unlike traditional inference setups, these distributed
pipelines span the model computation across heterogeneous nodes and
communication layers, thereby exposing a broader attack surface to potential
adversaries. Building on these motivations, this work explores a previously
overlooked vulnerability: even when both the edge and cloud components of the
model are inaccessible (i.e., black-box), an adversary who intercepts the
intermediate features transmitted between them can still pose a serious threat.
We demonstrate that, under these mild and realistic assumptions, an attacker
can craft highly transferable proxy models, making the entire deep learning
system significantly more vulnerable to evasion attacks. In particular, the
intercepted features can be effectively analyzed and leveraged to distill
surrogate models capable of crafting highly transferable adversarial examples
against the target model. To this end, we propose an exploitation strategy
specifically designed for distributed settings, which involves reconstructing
the original tensor shape from vectorized transmitted features using simple
statistical analysis, and adapting surrogate architectures accordingly to
enable effective feature distillation. A comprehensive and systematic
experimental evaluation has been conducted to demonstrate that surrogate models
trained with the proposed strategy, i.e., leveraging intermediate features,
tremendously improve the transferability of adversarial attacks. These findings
underscore the urgent need to account for intermediate feature leakage in the
design of secure distributed deep learning systems.

</details>


### [20] [Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors](https://arxiv.org/abs/2507.07261)
*Chunzhuo Wang, Hans Hallez, Bart Vanrumste*

**主要类别:** cs.LG

**AI概要:** This study explores the use of multimodal learning to combine wearable and contactless sensing modalities for food intake gesture detection.


<details>
  <summary>更多</summary>
  
**动机:** To explore whether combining wearable and contactless sensing modalities through multimodal learning can further improve food intake gesture detection performance.

**方法:** A robust multimodal temporal convolutional network with cross-modal attention (MM-TCN-CMA) is proposed to integrate IMU and radar data.

**结果:** Experimental results show that the proposed framework improves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU models, respectively.

**结论:** The proposed framework effectively fuses IMU and radar data for food intake gesture detection and demonstrates robustness under missing modality conditions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Robust+Multimodal+Learning+Framework+For+Intake+Gesture+Detection+Using+Contactless+Radar+and+Wearable+IMU+Sensors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07261，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07261&send_immediately=true&force_search=false)

**原文摘要:** Automated food intake gesture detection plays a vital role in dietary
monitoring, enabling objective and continuous tracking of eating behaviors to
support better health outcomes. Wrist-worn inertial measurement units (IMUs)
have been widely used for this task with promising results. More recently,
contactless radar sensors have also shown potential. This study explores
whether combining wearable and contactless sensing modalities through
multimodal learning can further improve detection performance. We also address
a major challenge in multimodal learning: reduced robustness when one modality
is missing. To this end, we propose a robust multimodal temporal convolutional
network with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and
radar data, enhance gesture detection, and maintain performance under missing
modality conditions. A new dataset comprising 52 meal sessions (3,050 eating
gestures and 797 drinking gestures) from 52 participants is developed and made
publicly available. Experimental results show that the proposed framework
improves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU
models, respectively. Under missing modality scenarios, the framework still
achieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This
is the first study to demonstrate a robust multimodal learning framework that
effectively fuses IMU and radar data for food intake gesture detection.

</details>


### [21] [Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time](https://arxiv.org/abs/2507.07271)
*Julianna Piskorz, Krzysztof Kacprzyk, Mihaela van der Schaar*

**主要类别:** cs.LG

**AI概要:** This paper presents a novel framework for modeling treatment effect trajectories over dose and time to improve on the limitations of traditional ATE metrics.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this research is to address the limitation of ATE in capturing nuanced dynamics of treatment effects with both dose and time, especially in healthcare settings.

**方法:** The study proposes a framework for modeling treatment effect trajectories as smooth surfaces over dose and time. It adapts SemanticODE to the causal setting where treatment effects are never directly observed. The approach decouples the estimation of trajectory shape from the specification of clinically relevant properties.

**结果:** The method yields accurate, interpretable, and editable models of treatment dynamics which could provide clinically actionable insights such as onset time, peak effect, and duration of benefit.

**结论:** The proposed method provides accurate, interpretable, and editable models of treatment dynamics that can facilitate rigorous causal analysis and practical decision-making in healthcare.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond+the+ATE%3A+Interpretable+Modelling+of+Treatment+Effects+over+Dose+and+Time，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07271，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07271&send_immediately=true&force_search=false)

**原文摘要:** The Average Treatment Effect (ATE) is a foundational metric in causal
inference, widely used to assess intervention efficacy in randomized controlled
trials (RCTs). However, in many applications -- particularly in healthcare --
this static summary fails to capture the nuanced dynamics of treatment effects
that vary with both dose and time. We propose a framework for modelling
treatment effect trajectories as smooth surfaces over dose and time, enabling
the extraction of clinically actionable insights such as onset time, peak
effect, and duration of benefit. To ensure interpretability, robustness, and
verifiability -- key requirements in high-stakes domains -- we adapt
SemanticODE, a recent framework for interpretable trajectory modelling, to the
causal setting where treatment effects are never directly observed. Our
approach decouples the estimation of trajectory shape from the specification of
clinically relevant properties (e.g., maxima, inflection points), supporting
domain-informed priors, post-hoc editing, and transparent analysis. We show
that our method yields accurate, interpretable, and editable models of
treatment dynamics, facilitating both rigorous causal analysis and practical
decision-making.

</details>


### [22] [TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores](https://arxiv.org/abs/2507.07276)
*Aaron Foote, Danny Krizanc*

**主要类别:** cs.LG

**AI概要:** This paper presents TRIP, a test that detects unreliable permutation feature importance scores in machine learning models.


<details>
  <summary>更多</summary>
  
**动机:** Permutation feature importance has been shown to be misleading in the presence of dependent features, therefore a method is needed to detect unreliable permutation feature importance scores.

**方法:** Development of TRIP (Test for Reliable Interpretation via Permutation) and demonstration of its use in high dimensional settings.

**结果:** TRIP is able to detect unreliable permutation feature importance scores that are the result of model extrapolation.

**结论:** The TRIP test can be used to reliably detect when permutation feature importance scores are unreliable.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRIP%3A+A+Nonparametric+Test+to+Diagnose+Biased+Feature+Importance+Scores，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07276，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07276&send_immediately=true&force_search=false)

**原文摘要:** Along with accurate prediction, understanding the contribution of each
feature to the making of the prediction, i.e., the importance of the feature,
is a desirable and arguably necessary component of a machine learning model.
For a complex model such as a random forest, such importances are not innate --
as they are, e.g., with linear regression. Efficient methods have been created
to provide such capabilities, with one of the most popular among them being
permutation feature importance due to its efficiency, model-agnostic nature,
and perceived intuitiveness. However, permutation feature importance has been
shown to be misleading in the presence of dependent features as a result of the
creation of unrealistic observations when permuting the dependent features. In
this work, we develop TRIP (Test for Reliable Interpretation via Permutation),
a test requiring minimal assumptions that is able to detect unreliable
permutation feature importance scores that are the result of model
extrapolation. To build on this, we demonstrate how the test can be
complemented in order to allow its use in high dimensional settings. Through
testing on simulated data and applications, our results show that the test can
be used to reliably detect when permutation feature importance scores are
unreliable.

</details>


### [23] [Natural Evolutionary Search meets Probabilistic Numerics](https://arxiv.org/abs/2507.07288)
*Pierre Osselin, Masaki Adachi, Xiaowen Dong, Michael A. Osborne*

**主要类别:** cs.LG

**AI概要:** This paper introduces Probabilistic Natural Evolutionary Strategy Algorithms (ProbNES), enhancing the NES framework with Bayesian quadrature, addressing the limited sample efficiency in NES algorithms.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the limited sample efficiency in Natural Evolution Strategies (NES) algorithms due to their reliance on random sampling and Monte Carlo estimates.

**方法:** The paper introduces Probabilistic Natural Evolutionary Strategy Algorithms (ProbNES), which enhance the NES framework with Bayesian quadrature.

**结果:** ProbNES algorithms consistently outperform their non-probabilistic counterparts as well as global sample efficient methods such as Bayesian Optimisation (BO) or $\pi$BO across a wide range of tasks.

**结论:** Probabilistic Natural Evolutionary Strategy Algorithms (ProbNES) enhance the NES framework with Bayesian quadrature and consistently outperform their non-probabilistic counterparts as well as global sample efficient methods across a wide range of tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Natural+Evolutionary+Search+meets+Probabilistic+Numerics，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07288，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07288&send_immediately=true&force_search=false)

**原文摘要:** Zeroth-order local optimisation algorithms are essential for solving
real-valued black-box optimisation problems. Among these, Natural Evolution
Strategies (NES) represent a prominent class, particularly well-suited for
scenarios where prior distributions are available. By optimising the objective
function in the space of search distributions, NES algorithms naturally
integrate prior knowledge during initialisation, making them effective in
settings such as semi-supervised learning and user-prior belief frameworks.
However, due to their reliance on random sampling and Monte Carlo estimates,
NES algorithms can suffer from limited sample efficiency. In this paper, we
introduce a novel class of algorithms, termed Probabilistic Natural
Evolutionary Strategy Algorithms (ProbNES), which enhance the NES framework
with Bayesian quadrature. We show that ProbNES algorithms consistently
outperforms their non-probabilistic counterparts as well as global sample
efficient methods such as Bayesian Optimisation (BO) or $\pi$BO across a wide
range of tasks, including benchmark test functions, data-driven optimisation
tasks, user-informed hyperparameter tuning tasks and locomotion tasks.

</details>


### [24] [GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing](https://arxiv.org/abs/2507.07735)
*Peiyan Zhang, Haibo Jin, Liying Kang, Haohan Wang*

**主要类别:** cs.LG

**AI概要:** This paper reviews existing practices for evaluating vulnerabilities in LLMs and introduces GuardVal, a new protocol that assesses LLMs' capacity to handle safety-critical situations by dynamically generating jailbreak prompts.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the challenges in evaluating vulnerabilities of Large Language Models (LLMs) due to their evolving nature and the sophistication required to probe these vulnerabilities effectively.

**方法:** The paper introduces GuardVal, a new evaluation protocol that dynamically generates and refines jailbreak prompts based on the defender LLM's state. It also proposes an optimization method to prevent stagnation during prompt refinement.

**结果:** The results show distinct behavioral patterns among models from Mistral-7b to GPT-4 across 10 safety domains, offering insights into their robustness.

**结论:** The paper concludes that the GuardVal evaluation protocol provides a comprehensive view of LLMs' robustness and deepens the understanding of their behavior, which can inform future research and development of more secure models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GuardVal%3A+Dynamic+Large+Language+Model+Jailbreak+Evaluation+for+Comprehensive+Safety+Testing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07735，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07735&send_immediately=true&force_search=false)

**原文摘要:** Jailbreak attacks reveal critical vulnerabilities in Large Language Models
(LLMs) by causing them to generate harmful or unethical content. Evaluating
these threats is particularly challenging due to the evolving nature of LLMs
and the sophistication required in effectively probing their vulnerabilities.
Current benchmarks and evaluation methods struggle to fully address these
challenges, leaving gaps in the assessment of LLM vulnerabilities. In this
paper, we review existing jailbreak evaluation practices and identify three
assumed desiderata for an effective jailbreak evaluation protocol. To address
these challenges, we introduce GuardVal, a new evaluation protocol that
dynamically generates and refines jailbreak prompts based on the defender LLM's
state, providing a more accurate assessment of defender LLMs' capacity to
handle safety-critical situations. Moreover, we propose a new optimization
method that prevents stagnation during prompt refinement, ensuring the
generation of increasingly effective jailbreak prompts that expose deeper
weaknesses in the defender LLMs. We apply this protocol to a diverse set of
models, from Mistral-7b to GPT-4, across 10 safety domains. Our findings
highlight distinct behavioral patterns among the models, offering a
comprehensive view of their robustness. Furthermore, our evaluation process
deepens the understanding of LLM behavior, leading to insights that can inform
future research and drive the development of more secure models.

</details>


### [25] [Estimating Dataset Dimension via Singular Metrics under the Manifold Hypothesis: Application to Inverse Problems](https://arxiv.org/abs/2507.07291)
*Paola Causin, Alessio Marta*

**主要类别:** cs.LG

**AI概要:** Proposes a framework using a Mixture of Variational Autoencoders and tools from Riemannian geometry to address challenges related to estimating the intrinsic dimension of datasets, constructing appropriate local coordinates, and learning mappings between ambient and manifold spaces.


<details>
  <summary>更多</summary>
  
**动机:** High-dimensional datasets often exhibit low-dimensional geometric structures, as suggested by the manifold hypothesis. Fully leveraging this insight requires to deal with three key tasks: estimating the intrinsic dimension (ID) of the manifold, constructing appropriate local coordinates, and learning mappings between ambient and manifold spaces.

**方法:** A framework that addresses all these challenges using a Mixture of Variational Autoencoders (VAEs) and tools from Riemannian geometry. The estimated ID guides the construction of an atlas of local charts using a mixture of invertible VAEs, enabling accurate manifold parameterization and efficient inference.

**结果:** This approach enhances solutions to ill-posed inverse problems, particularly in biomedical imaging, by enforcing that reconstructions lie on the learned manifold. The impact of network pruning on manifold geometry and reconstruction quality is explored, showing that the intrinsic dimension serves as an effective proxy for monitoring model capacity.

**结论:** The intrinsic dimension serves as an effective proxy for monitoring model capacity, and the proposed framework using a Mixture of Variational Autoencoders and tools from Riemannian geometry enhances solutions to ill-posed inverse problems in biomedical imaging.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Estimating+Dataset+Dimension+via+Singular+Metrics+under+the+Manifold+Hypothesis%3A+Application+to+Inverse+Problems，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07291，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07291&send_immediately=true&force_search=false)

**原文摘要:** High-dimensional datasets often exhibit low-dimensional geometric structures,
as suggested by the manifold hypothesis, which implies that data lie on a
smooth manifold embedded in a higher-dimensional ambient space. While this
insight underpins many advances in machine learning and inverse problems, fully
leveraging it requires to deal with three key tasks: estimating the intrinsic
dimension (ID) of the manifold, constructing appropriate local coordinates, and
learning mappings between ambient and manifold spaces. In this work, we propose
a framework that addresses all these challenges using a Mixture of Variational
Autoencoders (VAEs) and tools from Riemannian geometry. We specifically focus
on estimating the ID of datasets by analyzing the numerical rank of the VAE
decoder pullback metric. The estimated ID guides the construction of an atlas
of local charts using a mixture of invertible VAEs, enabling accurate manifold
parameterization and efficient inference. We how this approach enhances
solutions to ill-posed inverse problems, particularly in biomedical imaging, by
enforcing that reconstructions lie on the learned manifold. Lastly, we explore
the impact of network pruning on manifold geometry and reconstruction quality,
showing that the intrinsic dimension serves as an effective proxy for
monitoring model capacity.

</details>


### [26] [Discretization-independent multifidelity operator learning for partial differential equations](https://arxiv.org/abs/2507.07292)
*Jacob Hauck, Yanzhi Zhang*

**主要类别:** cs.LG

**AI概要:** This paper presents a new encode-approximate-reconstruct model for operator learning that uses neural representations and achieves discretization independence, showing improved accuracy and efficiency in multifidelity learning.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to create a discretization-independent operator learning model that can effectively handle multifidelity learning and bridge the gap between theoretical formulations and practical realizations.

**方法:** The paper develops an encode-approximate-reconstruct operator learning model with learned neural representations of bases for input and output function distributions. Theoretical approximation guarantees are established.

**结果:** Numerical experiments show that multifidelity training improves accuracy and computational efficiency, enhancing empirical discretization independence.

**结论:** The proposed model offers a new approach to operator learning that is robust, efficient, and accurate, especially in multifidelity learning scenarios.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Discretization-independent+multifidelity+operator+learning+for+partial+differential+equations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07292，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07292&send_immediately=true&force_search=false)

**原文摘要:** We develop a new and general encode-approximate-reconstruct operator learning
model that leverages learned neural representations of bases for input and
output function distributions. We introduce the concepts of \textit{numerical
operator learning} and \textit{discretization independence}, which clarify the
relationship between theoretical formulations and practical realizations of
operator learning models. Our model is discretization-independent, making it
particularly effective for multifidelity learning. We establish theoretical
approximation guarantees, demonstrating uniform universal approximation under
strong assumptions on the input functions and statistical approximation under
weaker conditions. To our knowledge, this is the first comprehensive study that
investigates how discretization independence enables robust and efficient
multifidelity operator learning. We validate our method through extensive
numerical experiments involving both local and nonlocal PDEs, including
time-independent and time-dependent problems. The results show that
multifidelity training significantly improves accuracy and computational
efficiency. Moreover, multifidelity training further enhances empirical
discretization independence.

</details>


### [27] [AdeptHEQ-FL: Adaptive Homomorphic Encryption for Federated Learning of Hybrid Classical-Quantum Models with Dynamic Layer Sparing](https://arxiv.org/abs/2507.07316)
*Md Abrar Jahin, Taufikur Rahman Fuad, M. F. Mridha, Nafiz Fahad, Md. Jakir Hossen*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为AdeptHEQ-FL的统一混合经典-量子FL框架，该框架在非独立同分布环境中平衡了模型性能、隐私保护和通信效率，并在CIFAR-10数据集上取得了显著的准确性提升并减少了通信开销。


<details>
  <summary>更多</summary>
  
**动机:** 联合学习（FL）在平衡模型性能、隐私保护和通信效率方面面临固有的挑战，尤其是在非独立同分布的分散环境中。最近的方法要么牺牲正式的隐私保证，要么产生高昂的开销，要么忽视量子增强的表达能力。

**方法:** AdeptHEQ-FL是一个统一的混合经典-量子FL框架，集成了以下四个关键组件：(i)用于表达性分散学习的混合CNN-PQC架构，(ii)利用差分私有验证准确性的自适应准确性加权聚合方案，(iii)用于敏感模型层的安全聚合的选择性同态加密（HE），以及(iv)动态逐层自适应冻结以最小化通信开销同时保持量子适应性。

**结果:** AdeptHEQ-FL在CIFAR-10数据集上相对于Standard-FedQNN和FHE-FedQNN分别实现了约25.43%和约14.17%的准确性提升。此外，通过冻结较不重要的层，它减少了通信开销，证明了我们对FL的隐私保护、资源感知设计的效率和实用性。

**结论:** AdeptHEQ-FL框架通过集成混合CNN-PQC架构、自适应准确性加权聚合方案、选择性同态加密和动态逐层自适应冻结，实现了在非独立同分布环境中平衡模型性能、隐私保护和通信效率的目标。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AdeptHEQ-FL%3A+Adaptive+Homomorphic+Encryption+for+Federated+Learning+of+Hybrid+Classical-Quantum+Models+with+Dynamic+Layer+Sparing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07316，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07316&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning (FL) faces inherent challenges in balancing model
performance, privacy preservation, and communication efficiency, especially in
non-IID decentralized environments. Recent approaches either sacrifice formal
privacy guarantees, incur high overheads, or overlook quantum-enhanced
expressivity. We introduce AdeptHEQ-FL, a unified hybrid classical-quantum FL
framework that integrates (i) a hybrid CNN-PQC architecture for expressive
decentralized learning, (ii) an adaptive accuracy-weighted aggregation scheme
leveraging differentially private validation accuracies, (iii) selective
homomorphic encryption (HE) for secure aggregation of sensitive model layers,
and (iv) dynamic layer-wise adaptive freezing to minimize communication
overhead while preserving quantum adaptability. We establish formal privacy
guarantees, provide convergence analysis, and conduct extensive experiments on
the CIFAR-10, SVHN, and Fashion-MNIST datasets. AdeptHEQ-FL achieves a $\approx
25.43\%$ and $\approx 14.17\%$ accuracy improvement over Standard-FedQNN and
FHE-FedQNN, respectively, on the CIFAR-10 dataset. Additionally, it reduces
communication overhead by freezing less important layers, demonstrating the
efficiency and practicality of our privacy-preserving, resource-aware design
for FL.

</details>


### [28] [Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy](https://arxiv.org/abs/2507.07320)
*Dongyu Wei, Xiaoren Xu, Shiwen Mao, Mingzhe Chen*

**主要类别:** cs.LG

**AI概要:** This paper proposes a secure and communication-efficient clustered federated learning design with differential privacy techniques and a novel optimization algorithm.


<details>
  <summary>更多</summary>
  
**动机:** To optimize the performance of clustered federated learning (CFL) by jointly optimizing resource block allocation and user scheduling while incorporating differential privacy (DP) techniques.

**方法:** A novel dynamic penalty function assisted value decomposed multi-agent reinforcement learning (DPVD-MARL) algorithm is proposed.

**结果:** Simulation results show that the DPVD-MARL can improve the convergence rate by up to 20% and the ultimate accumulated rewards by 15% compared to independent Q-learning.

**结论:** The DPVD-MARL algorithm improves the convergence rate and ultimate accumulated rewards compared to independent Q-learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Communication+and+Device+Clustering+for+Clustered+Federated+Learning+with+Differential+Privacy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07320&send_immediately=true&force_search=false)

**原文摘要:** In this paper, a secure and communication-efficient clustered federated
learning (CFL) design is proposed. In our model, several base stations (BSs)
with heterogeneous task-handling capabilities and multiple users with
non-independent and identically distributed (non-IID) data jointly perform CFL
training incorporating differential privacy (DP) techniques. Since each BS can
process only a subset of the learning tasks and has limited wireless resource
blocks (RBs) to allocate to users for federated learning (FL) model parameter
transmission, it is necessary to jointly optimize RB allocation and user
scheduling for CFL performance optimization. Meanwhile, our considered CFL
method requires devices to use their limited data and FL model information to
determine their task identities, which may introduce additional communication
overhead. We formulate an optimization problem whose goal is to minimize the
training loss of all learning tasks while considering device clustering, RB
allocation, DP noise, and FL model transmission delay. To solve the problem, we
propose a novel dynamic penalty function assisted value decomposed multi-agent
reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to
independently determine their connected users, RBs, and DP noise of the
connected users but jointly minimize the training loss of all learning tasks
across all BSs. Different from the existing MARL methods that assign a large
penalty for invalid actions, we propose a novel penalty assignment scheme that
assigns penalty depending on the number of devices that cannot meet
communication constraints (e.g., delay), which can guide the MARL scheme to
quickly find valid actions, thus improving the convergence speed. Simulation
results show that the DPVD-MARL can improve the convergence rate by up to 20%
and the ultimate accumulated rewards by 15% compared to independent Q-learning.

</details>


### [29] [Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning](https://arxiv.org/abs/2507.07323)
*Dongyu Wei, Xiaoren Xu, Yuchen Liu, H. Vincent Poor, Mingzhe Chen*

**主要类别:** cs.LG

**AI概要:** This paper investigates deceptive signal-assisted private split learning, proposing a soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA) to prevent eavesdroppers from collecting model and data information.


<details>
  <summary>更多</summary>
  
**动机:** To prevent eavesdroppers from collecting model and data information in collaborative training.

**方法:** Soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA).

**结果:** Simulation results show that the proposed method improves the convergence rate by up to 3x and reduces the information leaked to eavesdroppers by up to 13% compared to the traditional SAC algorithm.

**结论:** The proposed ICM-CA method improves the convergence rate and reduces the information leaked to eavesdroppers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimizing+Model+Splitting+and+Device+Task+Assignment+for+Deceptive+Signal+Assisted+Private+Multi-hop+Split+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07323&send_immediately=true&force_search=false)

**原文摘要:** In this paper, deceptive signal-assisted private split learning is
investigated. In our model, several edge devices jointly perform collaborative
training, and some eavesdroppers aim to collect the model and data information
from devices. To prevent the eavesdroppers from collecting model and data
information, a subset of devices can transmit deceptive signals. Therefore, it
is necessary to determine the subset of devices used for deceptive signal
transmission, the subset of model training devices, and the models assigned to
each model training device. This problem is formulated as an optimization
problem whose goal is to minimize the information leaked to eavesdroppers while
meeting the model training energy consumption and delay constraints. To solve
this problem, we propose a soft actor-critic deep reinforcement learning
framework with intrinsic curiosity module and cross-attention (ICM-CA) that
enables a centralized agent to determine the model training devices, the
deceptive signal transmission devices, the transmit power, and sub-models
assigned to each model training device without knowing the position and
monitoring probability of eavesdroppers. The proposed method uses an ICM module
to encourage the server to explore novel actions and states and a CA module to
determine the importance of each historical state-action pair thus improving
training efficiency. Simulation results demonstrate that the proposed method
improves the convergence rate by up to 3x and reduces the information leaked to
eavesdroppers by up to 13% compared to the traditional SAC algorithm.

</details>


### [30] [Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery](https://arxiv.org/abs/2507.07328)
*Malikussaid, Hilal Hudan Nuha*

**主要类别:** cs.LG

**AI概要:** This paper addresses the issue of LLMs generating scientifically plausible but factually incorrect information in chemistry by fine-tuning a specialized model with a dual-domain dataset, resulting in improved chemical validity but also highlighting ongoing challenges.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to address the 'plausibility-validity gap' in Large Language Models (LLMs) when generating information in specialized domains like chemistry.

**方法:** The paper describes a systematic methodology which involves utilizing the Magistral Small model with integrated reasoning capabilities and fine-tuning it using Low-Rank Adaptation (LoRA). A key component is the creation of a 'dual-domain dataset' for molecular properties and chemical reactions, standardized to ensure quality.

**结果:** The fine-tuned model shows significant improvements over the baseline in format adherence, chemical validity, and synthesis route feasibility. It learns syntactic correctness more readily than chemical possibility and synthesis feasibility. Competitive performance with human experts was observed in some areas, but limitations such as errors in stereochemistry persist.

**结论:** This work establishes a viable framework for adapting generalist LLMs into reliable, specialized tools for chemical research, while also delineating critical areas for future improvement.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bridging+the+Plausibility-Validity+Gap+by+Fine-Tuning+a+Reasoning-Enhanced+LLM+for+Chemical+Synthesis+and+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07328，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07328&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models (LLMs) often generate scientifically plausible but
factually invalid information, a challenge we term the "plausibility-validity
gap," particularly in specialized domains like chemistry. This paper presents a
systematic methodology to bridge this gap by developing a specialized
scientific assistant. We utilized the Magistral Small model, noted for its
integrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation
(LoRA). A key component of our approach was the creation of a "dual-domain
dataset," a comprehensive corpus curated from various sources encompassing both
molecular properties and chemical reactions, which was standardized to ensure
quality. Our evaluation demonstrates that the fine-tuned model achieves
significant improvements over the baseline model in format adherence, chemical
validity of generated molecules, and the feasibility of proposed synthesis
routes. The results indicate a hierarchical learning pattern, where syntactic
correctness is learned more readily than chemical possibility and synthesis
feasibility. While a comparative analysis with human experts revealed
competitive performance in areas like chemical creativity and reasoning, it
also highlighted key limitations, including persistent errors in
stereochemistry, a static knowledge cutoff, and occasional reference
hallucination. This work establishes a viable framework for adapting generalist
LLMs into reliable, specialized tools for chemical research, while also
delineating critical areas for future improvement.

</details>


### [31] [Leveraging Manifold Embeddings for Enhanced Graph Transformer Representations and Learning](https://arxiv.org/abs/2507.07335)
*Ankit Jyothish, Ali Jannesari*

**主要类别:** cs.LG

**AI概要:** Prepending a Riemannian mixture-of-experts layer to a graph transformer enhances accuracy and interpretability of node classification.


<details>
  <summary>更多</summary>
  
**动机:** Graph transformers typically embed every node in a single Euclidean space, which blurs heterogeneous topologies. The aim is to provide intrinsic geometric explanations to the latent space and capture both euclidean and non-euclidean features.

**方法:** A lightweight Riemannian mixture-of-experts layer is prepended to a state-of-the-art ensemble graph transformer, which routes each node to various kinds of manifolds that best match its local structure.

**结果:** The proposed projector lifts accuracy by up to 3% on four node-classification benchmarks.

**结论:** The use of a geometry-aware projection in graph transformers improves both the predictive power and interpretability of graph representations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Leveraging+Manifold+Embeddings+for+Enhanced+Graph+Transformer+Representations+and+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07335，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07335&send_immediately=true&force_search=false)

**原文摘要:** Graph transformers typically embed every node in a single Euclidean space,
blurring heterogeneous topologies. We prepend a lightweight Riemannian
mixture-of-experts layer that routes each node to various kinds of manifold,
mixture of spherical, flat, hyperbolic - best matching its local structure.
These projections provide intrinsic geometric explanations to the latent space.
Inserted into a state-of-the-art ensemble graph transformer, this projector
lifts accuracy by up to 3% on four node-classification benchmarks. The ensemble
makes sure that both euclidean and non-euclidean features are captured.
Explicit, geometry-aware projection thus sharpens predictive power while making
graph representations more interpretable.

</details>


### [32] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda, Weisi Guo*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种基于条件生成对抗网络（cGAN）的框架，用于制作规避IDS机制的隐秘对抗攻击。


<details>
  <summary>更多</summary>
  
**动机:** 随着无人机系统（UAVs）在民用空域的日益融合，凸显了对于有弹性和智能的入侵检测系统（IDS）的需求，因为传统的异常检测方法通常无法识别新的威胁。

**方法:** 研究首先设计了一个健壮的多类IDS分类器，训练数据包括良性的无人机遥测和已知的网络攻击，然后使用cGAN扰动已知攻击，生成与OOD分布保持统计相似性的对抗样本。为了检测这样的扰动，研究实现了一个条件变分自动编码器(CVAE)，利用负对数似然将对抗输入从真实的OOD样本中分离出来。

**结果:** 比较评估显示，基于CVAE的后悔分显著优于传统的基于马氏距离的检测器在识别隐秘对抗威胁方面的能力。

**结论:** 该研究强调了先进的概率建模在增强IDS对自适应的、基于生成模型的网络入侵能力方面的重要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generative+Adversarial+Evasion+and+Out-of-Distribution+Detection+for+UAV+Cyber-Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2506.21142，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.21142&send_immediately=true&force_search=false)

**原文摘要:** The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [33] [Zero-Shot Context Generalization in Reinforcement Learning from Few Training Contexts](https://arxiv.org/abs/2507.07348)
*James Chapman, Kedar Karhadkar, Guido Montufar*

**主要类别:** cs.LG

**AI概要:** This paper addresses the challenge of DRL's limited generalization to new contexts by introducing the CEBE and CSE methods.


<details>
  <summary>更多</summary>
  
**动机:** Despite the success of DRL across multiple domains, policies trained via DRL often struggle to generalize to evaluation environments with different parameters. Obtaining sufficient training data across diverse contexts is impractical in real-world applications.

**方法:** The study introduces the context-enhanced Bellman equation (CEBE) for improving generalization when training on a single context and derives context sample enhancement (CSE) as an efficient data augmentation method for approximating the CEBE.

**结果:** Numerical validations in simulation environments show that CSE has the potential to improve generalization in DRL.

**结论:** The introduction of the context-enhanced Bellman equation (CEBE) and context sample enhancement (CSE) provides a promising approach to improve generalization in DRL, particularly in deterministic control environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Zero-Shot+Context+Generalization+in+Reinforcement+Learning+from+Few+Training+Contexts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07348，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07348&send_immediately=true&force_search=false)

**原文摘要:** Deep reinforcement learning (DRL) has achieved remarkable success across
multiple domains, including competitive games, natural language processing, and
robotics. Despite these advancements, policies trained via DRL often struggle
to generalize to evaluation environments with different parameters. This
challenge is typically addressed by training with multiple contexts and/or by
leveraging additional structure in the problem. However, obtaining sufficient
training data across diverse contexts can be impractical in real-world
applications. In this work, we consider contextual Markov decision processes
(CMDPs) with transition and reward functions that exhibit regularity in context
parameters. We introduce the context-enhanced Bellman equation (CEBE) to
improve generalization when training on a single context. We prove both
analytically and empirically that the CEBE yields a first-order approximation
to the Q-function trained across multiple contexts. We then derive context
sample enhancement (CSE) as an efficient data augmentation method for
approximating the CEBE in deterministic control environments. We numerically
validate the performance of CSE in simulation environments, showcasing its
potential to improve generalization in DRL.

</details>


### [34] [Learning from positive and unlabeled examples -Finite size sample bounds](https://arxiv.org/abs/2507.07354)
*Farnam Mansouri, Shai Ben-David*

**主要类别:** cs.LG

**AI概要:** 本文提供了在更广泛设定下PU学习的统计复杂性理论分析，未假设学习者事先知道类别先验，并证明了所需样本大小（包括正标签和未标签样本）的上下限。


<details>
  <summary>更多</summary>
  
**动机:** PU学习在许多现实世界的应用中出现。大多数现有工作依赖于简化的假设，即正标签实例的数据是由数据生成分布限制为正标签实例中抽取的和/或正标签点的比例（即类别先验）是学习者事先知道的。

**方法:** 提供PU学习在更广泛设定下的统计复杂性的理论分析，不假设学习者事先知道类别先验，证明所需样本大小（包括正标签和未标签样本）的上下限。

**结果:** 证明了所需样本大小（包括正标签和未标签样本）的上下限。

**结论:** 不同于大多数先前的工作，我们的研究并不假设学习者事先知道类别先验。我们证明了所需样本大小（包括正标签和未标签样本）的上下限。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+from+positive+and+unlabeled+examples+-Finite+size+sample+bounds，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07354&send_immediately=true&force_search=false)

**原文摘要:** PU (Positive Unlabeled) learning is a variant of supervised classification
learning in which the only labels revealed to the learner are of positively
labeled instances. PU learning arises in many real-world applications. Most
existing work relies on the simplifying assumptions that the positively labeled
training data is drawn from the restriction of the data generating distribution
to positively labeled instances and/or that the proportion of positively
labeled points (a.k.a. the class prior) is known apriori to the learner. This
paper provides a theoretical analysis of the statistical complexity of PU
learning under a wider range of setups. Unlike most prior work, our study does
not assume that the class prior is known to the learner. We prove upper and
lower bounds on the required sample sizes (of both the positively labeled and
the unlabeled samples).

</details>


### [35] [Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning](https://arxiv.org/abs/2507.07359)
*Zheyu Zhang, Jiayuan Dong, Jie Liu, Xun Huan*

**主要类别:** cs.LG

**AI概要:** 本文提出了GO-CBED，一个目标导向的贝叶斯框架，用于序列因果实验设计，能更有效地针对用户感兴趣的因果量。


<details>
  <summary>更多</summary>
  
**动机:** 与选择旨在推断完整因果模型的干预措施的传统方法不同，GO-CBED采取一种更集中、更高效的方法进行实验。

**方法:** 该论文提出了一种目标导向的贝叶斯框架——GO-CBED，用于序列因果实验设计。这个框架直接最大化用户指定的因果量信息增益（EIG），而不是推断整个因果模型。为了解决精确EIG计算的难题，引入了一个变分下界估计器，通过基于transformer的策略网络和基于标准化流的变分后验进行联合优化。

**结果:** 演示了GO-CBED在各种因果推理和发现任务中持续优于现有基准，包括合成结构因果模型和半合成基因调控网络，特别是在实验预算有限和复杂因果机制的情况下。

**结论:** GO-CBED框架在具有有限实验预算和复杂因果机制的环境中表现优于现有的基准，证明了将实验设计目标与特定研究目标对齐以及前瞻性序列规划的好处。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Goal-Oriented+Sequential+Bayesian+Experimental+Design+for+Causal+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07359，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07359&send_immediately=true&force_search=false)

**原文摘要:** We present GO-CBED, a goal-oriented Bayesian framework for sequential causal
experimental design. Unlike conventional approaches that select interventions
aimed at inferring the full causal model, GO-CBED directly maximizes the
expected information gain (EIG) on user-specified causal quantities of
interest, enabling more targeted and efficient experimentation. The framework
is both non-myopic, optimizing over entire intervention sequences, and
goal-oriented, targeting only model aspects relevant to the causal query. To
address the intractability of exact EIG computation, we introduce a variational
lower bound estimator, optimized jointly through a transformer-based policy
network and normalizing flow-based variational posteriors. The resulting policy
enables real-time decision-making via an amortized network. We demonstrate that
GO-CBED consistently outperforms existing baselines across various causal
reasoning and discovery tasks-including synthetic structural causal models and
semi-synthetic gene regulatory networks-particularly in settings with limited
experimental budgets and complex causal mechanisms. Our results highlight the
benefits of aligning experimental design objectives with specific research
goals and of forward-looking sequential planning.

</details>


### [36] [Atherosclerosis through Hierarchical Explainable Neural Network Analysis](https://arxiv.org/abs/2507.07373)
*Irsyad Adam, Steven Swee, Erika Yilin, Ethan Ji, William Speier, Dean Wang, Alex Bui, Wei Wang, Karol Watson, Peipei Ping*

**主要类别:** cs.LG

**AI概要:** This paper introduces ATHENA, a hierarchical graph neural network framework for personalized classification of subclinical atherosclerosis, which leverages clinical features and molecular data of patients.


<details>
  <summary>更多</summary>
  
**动机:** Current graph-based methods for disease classification lack consistency and comprehension regarding cohort-wide features, and understanding patient subtypes often considers clinical feature similarity in isolation.

**方法:** ATHENA constructs a novel hierarchical network representation through integrated modality learning and optimizes learned patient-specific molecular fingerprints.

**结果:** ATHENA has significantly boosted subclinical atherosclerosis classification performance across various baselines by up to 13% in AUC and 20% in F1 score.

**结论:** ATHENA enables mechanistically-informed patient subtype discovery and improves the prediction of atherosclerotic disease progression.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Atherosclerosis+through+Hierarchical+Explainable+Neural+Network+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07373，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07373&send_immediately=true&force_search=false)

**原文摘要:** In this work, we study the problem pertaining to personalized classification
of subclinical atherosclerosis by developing a hierarchical graph neural
network framework to leverage two characteristic modalities of a patient:
clinical features within the context of the cohort, and molecular data unique
to individual patients. Current graph-based methods for disease classification
detect patient-specific molecular fingerprints, but lack consistency and
comprehension regarding cohort-wide features, which are an essential
requirement for understanding pathogenic phenotypes across diverse
atherosclerotic trajectories. Furthermore, understanding patient subtypes often
considers clinical feature similarity in isolation, without integration of
shared pathogenic interdependencies among patients. To address these
challenges, we introduce ATHENA: Atherosclerosis Through Hierarchical
Explainable Neural Network Analysis, which constructs a novel hierarchical
network representation through integrated modality learning; subsequently, it
optimizes learned patient-specific molecular fingerprints that reflect
individual omics data, enforcing consistency with cohort-wide patterns. With a
primary clinical dataset of 391 patients, we demonstrate that this
heterogeneous alignment of clinical features with molecular interaction
patterns has significantly boosted subclinical atherosclerosis classification
performance across various baselines by up to 13% in area under the receiver
operating curve (AUC) and 20% in F1 score. Taken together, ATHENA enables
mechanistically-informed patient subtype discovery through explainable AI
(XAI)-driven subnetwork clustering; this novel integration framework
strengthens personalized intervention strategies, thereby improving the
prediction of atherosclerotic disease progression and management of their
clinical actionable outcomes.

</details>


### [37] [Bradley-Terry and Multi-Objective Reward Modeling Are Complementary](https://arxiv.org/abs/2507.07375)
*Zhiwei Zhang, Hui Liu, Xiaomin Li, Zhenwei Dai, Jingying Zeng, Fali Wang, Minhua Lin, Ramraj Chandradevan, Zhen Li, Chen Luo, Xianfeng Tang, Qi He, Suhang Wang*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的统一奖励建模框架，以应对强化学习中奖励黑客攻击的问题，并提升了模型性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管RLHF在处理人类反馈时非常有效，但它容易受到奖励黑客攻击，尤其是在更具挑战性的分布外（OOD）环境中，当前的方法对此效果不佳。

**方法:** 通过共享嵌入空间联合训练Bradley-Terry单目标和多目标回归奖励函数，建立了BT损失和回归目标之间的理论联系。

**结果:** 实验结果表明，所提出框架显著提高了奖励模型的鲁棒性和评分性能，使7B模型的表现超过了70B基线模型。

**结论:** 提出的统一奖励建模框架显著提高了奖励模型的鲁棒性和评分性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Bradley-Terry+and+Multi-Objective+Reward+Modeling+Are+Complementary，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07375，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07375&send_immediately=true&force_search=false)

**原文摘要:** Reward models trained on human preference data have demonstrated strong
effectiveness in aligning Large Language Models (LLMs) with human intent under
the framework of Reinforcement Learning from Human Feedback (RLHF). However,
RLHF remains vulnerable to reward hacking, where the policy exploits
imperfections in the reward function rather than genuinely learning the
intended behavior. Although significant efforts have been made to mitigate
reward hacking, they predominantly focus on and evaluate in-distribution
scenarios, where the training and testing data for the reward model share the
same distribution. In this paper, we empirically show that state-of-the-art
methods struggle in more challenging out-of-distribution (OOD) settings. We
further demonstrate that incorporating fine-grained multi-attribute scores
helps address this challenge. However, the limited availability of high-quality
data often leads to weak performance of multi-objective reward functions, which
can negatively impact overall performance and become the bottleneck. To address
this issue, we propose a unified reward modeling framework that jointly trains
Bradley--Terry (BT) single-objective and multi-objective regression-based
reward functions using a shared embedding space. We theoretically establish a
connection between the BT loss and the regression objective and highlight their
complementary benefits. Specifically, the regression task enhances the
single-objective reward function's ability to mitigate reward hacking in
challenging OOD settings, while BT-based training improves the scoring
capability of the multi-objective reward function, enabling a 7B model to
outperform a 70B baseline. Extensive experimental results demonstrate that our
framework significantly improves both the robustness and the scoring
performance of reward models.

</details>


### [38] [GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction](https://arxiv.org/abs/2507.07388)
*Zesheng Liu, Maryam Rahnemoonfar*

**主要类别:** cs.LG

**AI概要:** This paper introduces GRIT, a graph transformer for ice layer thickness that combines an attention mechanism with a graph learning framework, demonstrating lower prediction errors and effective capture of temporal changes across ice layers.


<details>
  <summary>更多</summary>
  
**动机:** To gain a deeper understanding of the thickness and variability of internal ice layers in Radar imagery for monitoring snow accumulation, evaluating ice dynamics processes, and minimizing uncertainties in climate models.

**方法:** The paper introduces GRIT, a graph transformer for ice layer thickness that integrates an inductive geometric graph learning framework with an attention mechanism.

**结果:** GRIT demonstrates consistently lower prediction errors compared to baseline graph neural networks and effectively captures temporal changes across ice layers.

**结论:** GRIT offers a promising approach to modeling ice layer thickness, providing robust predictions and highlighting the effectiveness of attention mechanisms in capturing temporal changes across ice layers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是GRIT%3A+Graph+Transformer+For+Internal+Ice+Layer+Thickness+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07388，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07388&send_immediately=true&force_search=false)

**原文摘要:** Gaining a deeper understanding of the thickness and variability of internal
ice layers in Radar imagery is essential in monitoring the snow accumulation,
better evaluating ice dynamics processes, and minimizing uncertainties in
climate models. Radar sensors, capable of penetrating ice, capture detailed
radargram images of internal ice layers. In this work, we introduce GRIT, graph
transformer for ice layer thickness. GRIT integrates an inductive geometric
graph learning framework with an attention mechanism, designed to map the
relationships between shallow and deeper ice layers. Compared to baseline graph
neural networks, GRIT demonstrates consistently lower prediction errors. These
results highlight the attention mechanism's effectiveness in capturing temporal
changes across ice layers, while the graph transformer combines the strengths
of transformers for learning long-range dependencies with graph neural networks
for capturing spatial patterns, enabling robust modeling of complex
spatiotemporal dynamics.

</details>


### [39] [ST-GRIT: Spatio-Temporal Graph Transformer For Internal Ice Layer Thickness Prediction](https://arxiv.org/abs/2507.07389)
*Zesheng Liu, Maryam Rahnemoonfar*

**主要类别:** cs.LG

**AI概要:** This paper presents ST-GRIT, a spatio-temporal graph transformer that effectively processes radargrams to capture the spatiotemporal relationships between ice layers. Experimental evaluation on radargram data from the Greenland ice sheet demonstrates its superior performance compared to current state-of-the-art methods.


<details>
  <summary>更多</summary>
  
**动机:** Understanding the thickness and variability of internal ice layers in radar imagery is crucial for monitoring snow accumulation, assessing ice dynamics, and reducing uncertainties in climate models.

**方法:** ST-GRIT, a spatio-temporal graph transformer designed to process radargrams and capture the spatiotemporal relationships between shallow and deep ice layers.

**结果:** ST-GRIT consistently outperforms current state-of-the-art methods and other baseline graph neural networks by achieving lower root mean-squared error.

**结论:** ST-GRIT provides a more comprehensive and effective approach for processing radargrams by leveraging separate spatial and temporal attention blocks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ST-GRIT%3A+Spatio-Temporal+Graph+Transformer+For+Internal+Ice+Layer+Thickness+Prediction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07389，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07389&send_immediately=true&force_search=false)

**原文摘要:** Understanding the thickness and variability of internal ice layers in radar
imagery is crucial for monitoring snow accumulation, assessing ice dynamics,
and reducing uncertainties in climate models. Radar sensors, capable of
penetrating ice, provide detailed radargram images of these internal layers. In
this work, we present ST-GRIT, a spatio-temporal graph transformer for ice
layer thickness, designed to process these radargrams and capture the
spatiotemporal relationships between shallow and deep ice layers. ST-GRIT
leverages an inductive geometric graph learning framework to extract local
spatial features as feature embeddings and employs a series of temporal and
spatial attention blocks separately to model long-range dependencies
effectively in both dimensions. Experimental evaluation on radargram data from
the Greenland ice sheet demonstrates that ST-GRIT consistently outperforms
current state-of-the-art methods and other baseline graph neural networks by
achieving lower root mean-squared error. These results highlight the advantages
of self-attention mechanisms on graphs over pure graph neural networks,
including the ability to handle noise, avoid oversmoothing, and capture
long-range dependencies. Moreover, the use of separate spatial and temporal
attention blocks allows for distinct and robust learning of spatial
relationships and temporal patterns, providing a more comprehensive and
effective approach.

</details>


### [40] [Learning Collective Variables from Time-lagged Generation](https://arxiv.org/abs/2507.07390)
*Seonghyun Park, Kiyoung Seong, Soojung Yang, Rafael Gómez-Bombarelli, Sungsoo Ahn*

**主要类别:** cs.LG

**AI概要:** 本研究介绍了一个新的框架TLC，可以从生成模型的时间滞后条件中自动学习集体变量（CVs），以捕捉缓慢的动力学行为，并在丙氨酸二肽系统上验证了其优异的性能。


<details>
  <summary>更多</summary>
  
**动机:** 由于长时间尺度的问题，直接通过分子动力学模拟观察如状态转换这样的罕见事件是困难的。增强采样技术通过在精心选择的低维特征（称为集体变量，CVs）上引入偏差来克服这个问题。现有机器学习方法通常专注于区分元稳定状态，而未完全编码对于精确采样至关重要的详细动力学。

**方法:** 研究提出了一种名为TLC的框架，该框架直接从生成模型的时间滞后条件中学习集体变量（CVs）。不同于建模静态玻尔兹曼分布，TLC建模一个时间滞后条件分布，得到能够捕捉缓慢动态行为的CVs。

**结果:** 通过对丙氨酸二肽系统的验证，包括导向分子动力学（SMD）和实时概率增强采样（OPES）在内的两种基于CV的增强采样任务，显示出TLC框架在转换路径采样和状态区分方面的性能等于或优于现有的MLCV方法。

**结论:** TLC框架在丙氨酸二肽系统上的表现证明了其在转换路径采样和状态区分方面的能力，与现有的MLCV方法相比具有相同或更优的性能。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning+Collective+Variables+from+Time-lagged+Generation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07390，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07390&send_immediately=true&force_search=false)

**原文摘要:** Rare events such as state transitions are difficult to observe directly with
molecular dynamics simulations due to long timescales. Enhanced sampling
techniques overcome this by introducing biases along carefully chosen
low-dimensional features, known as collective variables (CVs), which capture
the slow degrees of freedom. Machine learning approaches (MLCVs) have automated
CV discovery, but existing methods typically focus on discriminating
meta-stable states without fully encoding the detailed dynamics essential for
accurate sampling. We propose TLC, a framework that learns CVs directly from
time-lagged conditions of a generative model. Instead of modeling the static
Boltzmann distribution, TLC models a time-lagged conditional distribution
yielding CVs to capture the slow dynamic behavior. We validate TLC on the
Alanine Dipeptide system using two CV-based enhanced sampling tasks: (i)
steered molecular dynamics (SMD) and (ii) on-the-fly probability enhanced
sampling (OPES), demonstrating equal or superior performance compared to
existing MLCV methods in both transition path sampling and state
discrimination.

</details>


### [41] [Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for Statement Autoformalization](https://arxiv.org/abs/2507.07399)
*Yuntian Liu, Tao Zhu, Xiaoyang Liu, Yu Chen, Zhaoxuan Liu, Qingfeng Guo, Jiashuo Zhang, Kangjie Bao, Tao Luo*

**主要类别:** cs.LG

**AI概要:** This paper proposes GTED, a novel evaluation framework for statement autoformalization that overcomes limitations of existing methods.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the issues of existing evaluation methods which lack semantic understanding, face high computational costs, and are constrained by the progress of automated theorem proving.

**方法:** The proposed method standardizes formal statements and converts them into operator trees to determine semantic similarity using the GTED metric.

**结果:** GTED outperforms all baseline metrics by achieving the highest accuracy and Kappa scores on the miniF2F and ProofNet benchmarks.

**结论:** GTED provides a more faithful metric for automated evaluation of statement autoformalization.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Generalized+Tree+Edit+Distance+%28GTED%29%3A+A+Faithful+Evaluation+Metric+for+Statement+Autoformalization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07399，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07399&send_immediately=true&force_search=false)

**原文摘要:** Statement autoformalization, the automated translation of statement from
natural language into formal languages, has become a subject of extensive
research, yet the development of robust automated evaluation metrics remains
limited. Existing evaluation methods often lack semantic understanding, face
challenges with high computational costs, and are constrained by the current
progress of automated theorem proving. To address these issues, we propose GTED
(Generalized Tree Edit Distance), a novel evaluation framework that first
standardizes formal statements and converts them into operator trees, then
determines the semantic similarity using the eponymous GTED metric. On the
miniF2F and ProofNet benchmarks, GTED outperforms all baseline metrics by
achieving the highest accuracy and Kappa scores, thus providing the community
with a more faithful metric for automated evaluation. The code and experimental
results are available at https://github.com/XiaoyangLiu-sjtu/GTED.

</details>


### [42] [HGMP:Heterogeneous Graph Multi-Task Prompt Learning](https://arxiv.org/abs/2507.07405)
*Pengfei Jiao, Jialong Ni, Di Jin, Xuan Guo, Huan Liu, Hongjiang Chen, Yanxian Bi*

**主要类别:** cs.LG

**AI概要:** This paper addresses the mismatch issue in pre-training and fine-tuning methods for heterogeneous graph neural networks. It proposes a novel multi-task prompt framework (HGMP) with a graph-level contrastive pre-training strategy and heterogeneous feature prompts, showing superior performance across various tasks.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the issue of a mismatch between pre-trained models and downstream tasks in the field of heterogeneous graph neural networks, which leads to suboptimal performance in certain application scenarios.

**方法:** The paper proposes a multi-task prompt framework for the heterogeneous graph domain, HGMP. It reformulates all downstream tasks into a unified graph-level task format and addresses the limitations of existing graph prompt learning methods by designing a graph-level contrastive pre-training strategy. Heterogeneous feature prompts are also introduced.

**结果:** Experimental results on public datasets show that the proposed method adapts well to various tasks and significantly outperforms baseline methods.

**结论:** The proposed HGMP framework effectively adapts to various tasks and outperforms baseline methods, demonstrating the potential of prompt learning in heterogeneous graph tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HGMP%3AHeterogeneous+Graph+Multi-Task+Prompt+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07405，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07405&send_immediately=true&force_search=false)

**原文摘要:** The pre-training and fine-tuning methods have gained widespread attention in
the field of heterogeneous graph neural networks due to their ability to
leverage large amounts of unlabeled data during the pre-training phase,
allowing the model to learn rich structural features. However, these methods
face the issue of a mismatch between the pre-trained model and downstream
tasks, leading to suboptimal performance in certain application scenarios.
Prompt learning methods have emerged as a new direction in heterogeneous graph
tasks, as they allow flexible adaptation of task representations to address
target inconsistency. Building on this idea, this paper proposes a novel
multi-task prompt framework for the heterogeneous graph domain, named HGMP.
First, to bridge the gap between the pre-trained model and downstream tasks, we
reformulate all downstream tasks into a unified graph-level task format. Next,
we address the limitations of existing graph prompt learning methods, which
struggle to integrate contrastive pre-training strategies in the heterogeneous
graph domain. We design a graph-level contrastive pre-training strategy to
better leverage heterogeneous information and enhance performance in multi-task
scenarios. Finally, we introduce heterogeneous feature prompts, which enhance
model performance by refining the representation of input graph features.
Experimental results on public datasets show that our proposed method adapts
well to various tasks and significantly outperforms baseline methods.

</details>


### [43] [Neural networks leverage nominally quantum and post-quantum representations](https://arxiv.org/abs/2507.07432)
*Paul M. Riechers, Thomas J. Elliott, Adam S. Shai*

**主要类别:** cs.LG

**AI概要:** This paper shows how deep neural networks intrinsically discover and represent beliefs over low-dimensional generative models of their training data.


<details>
  <summary>更多</summary>
  
**动机:** The researchers aimed to understand how neural networks represent beliefs about the world during inference.

**方法:** The study used pretrained deep neural networks, including transformers and RNNs, on next-token prediction tasks.

**结果:** Neural networks easily discover representations that no finite classical circuit can achieve. The geometry of neural activations is largely independent of the network architecture.

**结论:** Deep neural networks have the intrinsic ability to represent beliefs over low-dimensional generative models of their training data.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+networks+leverage+nominally+quantum+and+post-quantum+representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07432，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07432&send_immediately=true&force_search=false)

**原文摘要:** We show that deep neural networks, including transformers and RNNs,
pretrained as usual on next-token prediction, intrinsically discover and
represent beliefs over 'quantum' and 'post-quantum' low-dimensional generative
models of their training data -- as if performing iterative Bayesian updates
over the latent state of this world model during inference as they observe more
context. Notably, neural nets easily find these representation whereas there is
no finite classical circuit that would do the job. The corresponding geometric
relationships among neural activations induced by different input sequences are
found to be largely independent of neural-network architecture. Each point in
this geometry corresponds to a history-induced probability density over all
possible futures, and the relative displacement of these points reflects the
difference in mechanism and magnitude for how these distinct pasts affect the
future.

</details>


### [44] [General purpose models for the chemical sciences](https://arxiv.org/abs/2507.07456)
*Nawaf Alampara, Anagha Aneesh, Martiño Ríos-García, Adrian Mirza, Mara Schilling-Wilhelmi, Ali Asghar Aghajani, Meiling Sun, Gordan Prastalo, Kevin Maik Jablonka*

**主要类别:** cs.LG

**AI概要:** 本篇文献综述讨论了通用模型（GPMs）的基本构建原则及它们在化学科学中的应用，指出尽管许多应用仍处原型阶段，但预计在未来几年将逐渐成熟。


<details>
  <summary>更多</summary>
  
**动机:** 数据驱动技术有巨大的潜力来推动和加速化学科学的发展。然而，化学科学也带来了非常多样、小规模、模糊的数据集的独特挑战，这些数据集在传统机器学习方法中难以完全利用。

**方法:** 讨论了GPMs的基本构建原则，并回顾了这些模型在化学科学整个科研过程中的近期应用。

**结果:** 通用模型（如大型语言模型）具有解决未直接训练任务的能力，并能在不同格式下灵活操作少量的数据。

**结论:** 虽然许多这些应用仍处于原型阶段，但我们预计，随着对GPMs兴趣的增加，许多应用将在未来几年内成熟。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是General+purpose+models+for+the+chemical+sciences，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07456，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07456&send_immediately=true&force_search=false)

**原文摘要:** Data-driven techniques have a large potential to transform and accelerate the
chemical sciences. However, chemical sciences also pose the unique challenge of
very diverse, small, fuzzy datasets that are difficult to leverage in
conventional machine learning approaches completely. A new class of models,
general-purpose models (GPMs) such as large language models, have shown the
ability to solve tasks they have not been directly trained on, and to flexibly
operate with low amounts of data in different formats. In this review, we
discuss fundamental building principles of GPMs and review recent applications
of those models in the chemical sciences across the entire scientific process.
While many of these applications are still in the prototype phase, we expect
that the increasing interest in GPMs will make many of them mature in the
coming years.

</details>


### [45] [Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning](https://arxiv.org/abs/2507.07485)
*Wooseong Jeong, Kuk-Jin Yoon*

**主要类别:** cs.LG

**AI概要:** This paper proposes DTME-MTL to address negative transfer in MTL by operating in token space, improving performance with minimal overhead.


<details>
  <summary>更多</summary>
  
**动机:** To address the problem of negative transfer in Multi-Task Learning (MTL) within a shared network and improve adaptability.

**方法:** Dynamic Token Modulation and Expansion (DTME-MTL) is proposed, which enhances adaptability and reduces overfitting by identifying gradient conflicts in token space and applying adaptive solutions based on conflict type.

**结果:** Extensive experiments demonstrate that DTME-MTL consistently improves multi-task performance with minimal computational overhead.

**结论:** DTME-MTL offers a scalable and effective solution for enhancing transformer-based MTL models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Resolving+Token-Space+Gradient+Conflicts%3A+Token+Space+Manipulation+for+Transformer-Based+Multi-Task+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07485，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07485&send_immediately=true&force_search=false)

**原文摘要:** Multi-Task Learning (MTL) enables multiple tasks to be learned within a
shared network, but differences in objectives across tasks can cause negative
transfer, where the learning of one task degrades another task's performance.
While pre-trained transformers significantly improve MTL performance, their
fixed network capacity and rigid structure limit adaptability. Previous dynamic
network architectures attempt to address this but are inefficient as they
directly convert shared parameters into task-specific ones. We propose Dynamic
Token Modulation and Expansion (DTME-MTL), a framework applicable to any
transformer-based MTL architecture. DTME-MTL enhances adaptability and reduces
overfitting by identifying gradient conflicts in token space and applying
adaptive solutions based on conflict type. Unlike prior methods that mitigate
negative transfer by duplicating network parameters, DTME-MTL operates entirely
in token space, enabling efficient adaptation without excessive parameter
growth. Extensive experiments demonstrate that DTME-MTL consistently improves
multi-task performance with minimal computational overhead, offering a scalable
and effective solution for enhancing transformer-based MTL models.

</details>


### [46] [Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs. Deep Learning](https://arxiv.org/abs/2507.07511)
*Joris Suurmeijer, Ivo Pascal de Jong, Matias Valdenegro-Toro, Andreea Ioana Sburlea*

**主要类别:** cs.LG

**AI概要:** This paper compares different methods of uncertainty quantification in BCI classifiers.


<details>
  <summary>更多</summary>
  
**动机:** To address the problem of overconfidence typically seen in Deep Learning in uncertainty quantification for BCIs.

**方法:** Comparison of uncertainty quantification ability of established BCI classifiers using Common Spatial Patterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in Deep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as standard Convolutional Neural Networks (CNNs).

**结果:** MDRM is underconfident, which was solved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best uncertainty estimates, but Deep Ensembles and standard CNNs give the best classifications.

**结论:** CSP-LDA and MDRM-T provide the best uncertainty estimates, while Deep Ensembles and standard CNNs offer the best classifications. The ability of models to differentiate between easy and difficult estimates can enhance the accuracy of a Motor Imagery BCI by rejecting ambiguous samples.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Uncertainty+Quantification+for+Motor+Imagery+BCI+--+Machine+Learning+vs.+Deep+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07511，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07511&send_immediately=true&force_search=false)

**原文摘要:** Brain-computer interfaces (BCIs) turn brain signals into functionally useful
output, but they are not always accurate. A good Machine Learning classifier
should be able to indicate how confident it is about a given classification, by
giving a probability for its classification. Standard classifiers for Motor
Imagery BCIs do give such probabilities, but research on uncertainty
quantification has been limited to Deep Learning. We compare the uncertainty
quantification ability of established BCI classifiers using Common Spatial
Patterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in
Deep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as
standard Convolutional Neural Networks (CNNs).
  We found that the overconfidence typically seen in Deep Learning is not a
problem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we
solved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best
uncertainty estimates, but Deep Ensembles and standard CNNs give the best
classifications. We show that all models are able to separate between easy and
difficult estimates, so that we can increase the accuracy of a Motor Imagery
BCI by rejecting samples that are ambiguous.

</details>


### [47] [Neural Concept Verifier: Scaling Prover-Verifier Games via Concept Encodings](https://arxiv.org/abs/2507.07532)
*Berkant Turan, Suhrab Asadulla, David Steinmann, Wolfgang Stammer, Sebastian Pokutta*

**主要类别:** cs.LG

**AI概要:** This paper introduces the Neural Concept Verifier (NCV), a unified framework combining Prover-Verifier Games with concept encodings for interpretable, nonlinear classification in high-dimensional settings.


<details>
  <summary>更多</summary>
  
**动机:** Prover-Verifier Games have not been applied to complex inputs such as high-dimensional images and Concept Bottleneck Models are limited by their reliance on low-capacity linear predictors.

**方法:** NCV utilizes minimally supervised concept discovery models to extract structured concept encodings from raw inputs. A prover selects a subset of these encodings, which a verifier uses exclusively for decision-making.

**结果:** NCV outperforms CBM and pixel-based PVG classifier baselines on high-dimensional, logically complex datasets and helps mitigate shortcut behavior.

**结论:** NCV is a promising step toward performative, verifiable AI.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neural+Concept+Verifier%3A+Scaling+Prover-Verifier+Games+via+Concept+Encodings，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07532，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07532&send_immediately=true&force_search=false)

**原文摘要:** While Prover-Verifier Games (PVGs) offer a promising path toward
verifiability in nonlinear classification models, they have not yet been
applied to complex inputs such as high-dimensional images. Conversely, Concept
Bottleneck Models (CBMs) effectively translate such data into interpretable
concepts but are limited by their reliance on low-capacity linear predictors.
In this work, we introduce the Neural Concept Verifier (NCV), a unified
framework combining PVGs with concept encodings for interpretable, nonlinear
classification in high-dimensional settings. NCV achieves this by utilizing
recent minimally supervised concept discovery models to extract structured
concept encodings from raw inputs. A prover then selects a subset of these
encodings, which a verifier -- implemented as a nonlinear predictor -- uses
exclusively for decision-making. Our evaluations show that NCV outperforms CBM
and pixel-based PVG classifier baselines on high-dimensional, logically complex
datasets and also helps mitigate shortcut behavior. Overall, we demonstrate NCV
as a promising step toward performative, verifiable AI.

</details>


### [48] [Real-Time Decorrelation-Based Anomaly Detection for Multivariate Time Series](https://arxiv.org/abs/2507.07559)
*Amirhossein Sadough, Mahyar Shahsavari, Mark Wijtvliet, Marcel van Gerven*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的异常检测方法DAD，适用于实时处理大规模多变量数据流，表现出了比现有技术更优的性能和一致性。


<details>
  <summary>更多</summary>
  
**动机:** 异常检测(AD)在识别偏离预期模式的数据实例方面扮演着至关重要的角色，这些数据可能预示着关键事件，如系统故障、欺诈活动或罕见的医疗状况。随着物联网的兴起，对实时AD的需求激增，因为必须即时处理大量多变量传感器数据。

**方法:** 我们提出了DAD，一种基于在线去相关学习方法的新型实时多变量时间序列异常检测方法。

**结果:** 广泛实验表明，与最先进的方法相比，DAD在不同类型异常中均表现出最一致且优越的性能。其对维度增加的鲁棒性使得它特别适合于实时、高维数据流。

**结论:** DAD在检测效能和计算效率之间达到了最佳平衡，并为实时、内存受限的异常检测设定了新的标准。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Real-Time+Decorrelation-Based+Anomaly+Detection+for+Multivariate+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07559，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07559&send_immediately=true&force_search=false)

**原文摘要:** Anomaly detection (AD) plays a vital role across a wide range of real-world
domains by identifying data instances that deviate from expected patterns,
potentially signaling critical events such as system failures, fraudulent
activities, or rare medical conditions. The demand for real-time AD has surged
with the rise of the (Industrial) Internet of Things, where massive volumes of
multivariate sensor data must be processed instantaneously. Real-time AD
requires methods that not only handle high-dimensional streaming data but also
operate in a single-pass manner, without the burden of storing historical
instances, thereby ensuring minimal memory usage and fast decision-making. We
propose DAD, a novel real-time decorrelation-based anomaly detection method for
multivariate time series, based on an online decorrelation learning approach.
Unlike traditional proximity-based or reconstruction-based detectors that
process entire data or windowed instances, DAD dynamically learns and monitors
the correlation structure of data sample by sample in a single pass, enabling
efficient and effective detection. To support more realistic benchmarking
practices, we also introduce a practical hyperparameter tuning strategy
tailored for real-time anomaly detection scenarios. Extensive experiments on
widely used benchmark datasets demonstrate that DAD achieves the most
consistent and superior performance across diverse anomaly types compared to
state-of-the-art methods. Crucially, its robustness to increasing
dimensionality makes it particularly well-suited for real-time,
high-dimensional data streams. Ultimately, DAD not only strikes an optimal
balance between detection efficacy and computational efficiency but also sets a
new standard for real-time, memory-constrained anomaly detection.

</details>


### [49] [COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation](https://arxiv.org/abs/2507.07580)
*Uliana Parkina, Maxim Rakhuba*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel inversion-free regularized framework for context-aware low-rank approximation in neural networks, addressing numerical instabilities and handling challenging scenarios.


<details>
  <summary>更多</summary>
  
**动机:** Existing methods for context-aware low-rank approximation in neural networks suffer from numerical instabilities due to reliance on explicit Gram matrix computation and inversion.

**方法:** A novel inversion-free regularized framework based on stable decompositions is proposed to address the limitations of existing methods.

**结果:** The proposed method can handle challenging scenarios such as calibration matrices exceeding GPU memory capacity, nearly singular input activation matrices, and insufficient data. It converges to a desired approximation with explicit error bounds.

**结论:** The proposed inversion-free regularized framework overcomes numerical instabilities and pitfalls of prior art in context-aware low-rank approximation for compression and fine-tuning of large-scale neural networks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是COALA%3A+Numerically+Stable+and+Efficient+Framework+for+Context-Aware+Low-Rank+Approximation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07580，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07580&send_immediately=true&force_search=false)

**原文摘要:** Recent studies suggest that context-aware low-rank approximation is a useful
tool for compression and fine-tuning of modern large-scale neural networks. In
this type of approximation, a norm is weighted by a matrix of input
activations, significantly improving metrics over the unweighted case.
Nevertheless, existing methods for neural networks suffer from numerical
instabilities due to their reliance on classical formulas involving explicit
Gram matrix computation and their subsequent inversion. We demonstrate that
this can degrade the approximation quality or cause numerically singular
matrices.
  To address these limitations, we propose a novel inversion-free regularized
framework that is based entirely on stable decompositions and overcomes the
numerical pitfalls of prior art. Our method can handle possible challenging
scenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when
input activation matrices are nearly singular, and even (3) when insufficient
data prevents unique approximation. For the latter, we prove that our solution
converges to a desired approximation and derive explicit error bounds.

</details>


### [50] [CHOMET: Conditional Handovers via Meta-Learning](https://arxiv.org/abs/2507.07581)
*Michail Kalntis, Fernando A. Kuipers, George Iosifidis*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel framework for optimizing conditional handovers in cellular networks using meta-learning, providing robust dynamic regret guarantees and superior performance.


<details>
  <summary>更多</summary>
  
**动机:** Traditional handovers face significant challenges such as prolonged delays and increased failures. Conditional handovers (CHOs) introduce new challenges including efficient resource allocation and managing signaling/communication overhead.

**方法:** This paper presents a novel framework aligned with the O-RAN paradigm that leverages meta-learning for CHO optimization.

**结果:** The proposed framework demonstrates at least 180% superior performance than other 3GPP benchmarks in volatile signal conditions.

**结论:** The proposed framework provides robust dynamic regret guarantees and superior performance in volatile signal conditions.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是CHOMET%3A+Conditional+Handovers+via+Meta-Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07581，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07581&send_immediately=true&force_search=false)

**原文摘要:** Handovers (HOs) are the cornerstone of modern cellular networks for enabling
seamless connectivity to a vast and diverse number of mobile users. However, as
mobile networks become more complex with more diverse users and smaller cells,
traditional HOs face significant challenges, such as prolonged delays and
increased failures. To mitigate these issues, 3GPP introduced conditional
handovers (CHOs), a new type of HO that enables the preparation (i.e., resource
allocation) of multiple cells for a single user to increase the chance of HO
success and decrease the delays in the procedure. Despite its advantages, CHO
introduces new challenges that must be addressed, including efficient resource
allocation and managing signaling/communication overhead from frequent cell
preparations and releases. This paper presents a novel framework aligned with
the O-RAN paradigm that leverages meta-learning for CHO optimization, providing
robust dynamic regret guarantees and demonstrating at least 180% superior
performance than other 3GPP benchmarks in volatile signal conditions.

</details>


### [51] [Improving Clustering on Occupational Text Data through Dimensionality Reduction](https://arxiv.org/abs/2507.07582)
*Iago Xabier Vázquez García, Damla Partanaz, Emrullah Fatih Yetkin*

**主要类别:** cs.LG

**AI概要:** This study proposes an optimal clustering mechanism for occupations in the O*NET database, using BERT-based techniques and clustering approaches to obtain a map between occupation definitions.


<details>
  <summary>更多</summary>
  
**动机:** Occupations in the O*NET database have varying definitions across firms and countries. Mapping these definitions is necessary for expanding collected data for differently defined occupations.

**方法:** A pipeline using several BERT-based techniques with various clustering approaches and dimensionality reduction methods was used. The effect of dimensionality reduction on clustering algorithm performance metrics was examined. A specialized silhouette approach was used to improve results.

**结果:** The study obtained a map between occupation definitions using BERT-based techniques and clustering approaches. Dimensionality reduction improved clustering algorithm performance metrics. Results were further improved by using a specialized silhouette approach.

**结论:** The proposed clustering-based mapping approach with dimensionality reduction can help distinguish occupations automatically, creating new paths for people wanting to change their careers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Improving+Clustering+on+Occupational+Text+Data+through+Dimensionality+Reduction，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07582，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07582&send_immediately=true&force_search=false)

**原文摘要:** In this study, we focused on proposing an optimal clustering mechanism for
the occupations defined in the well-known US-based occupational database,
O*NET. Even though all occupations are defined according to well-conducted
surveys in the US, their definitions can vary for different firms and
countries. Hence, if one wants to expand the data that is already collected in
O*NET for the occupations defined with different tasks, a map between the
definitions will be a vital requirement. We proposed a pipeline using several
BERT-based techniques with various clustering approaches to obtain such a map.
We also examined the effect of dimensionality reduction approaches on several
metrics used in measuring performance of clustering algorithms. Finally, we
improved our results by using a specialized silhouette approach. This new
clustering-based mapping approach with dimensionality reduction may help
distinguish the occupations automatically, creating new paths for people
wanting to change their careers.

</details>


### [52] [Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data](https://arxiv.org/abs/2507.07589)
*Arpana Sinhal, Anay Sinhal, Amit Sinhal*

**主要类别:** cs.LG

**AI概要:** This study addresses gaps in stress-detection methodologies by introducing a multimodal dataset and using advanced machine learning models, advancing the development of deployable stress-monitoring systems.


<details>
  <summary>更多</summary>
  
**动机:** Healthcare professionals, particularly nurses, face elevated occupational stress, a concern amplified during the COVID-19 pandemic. While wearable sensors offer promising avenues for real-time stress monitoring, existing studies often lack comprehensive datasets and robust analytical frameworks.

**方法:** The study introduces a multimodal dataset comprising physiological signals, electrodermal activity, heart rate and skin temperature. The dataset underwent preprocessing with the Synthetic Minority Over sampling Technique (SMOTE). Advanced machine learning models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were evaluated and combined into a Stacking Classifier.

**结果:** By using a publicly accessible dataset and a reproducible analytical pipeline, this work advances the development of deployable stress-monitoring systems.

**结论:** This work advances the development of deployable stress-monitoring systems, offering practical implications for safeguarding healthcare workers' mental health.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stress+Monitoring+in+Healthcare%3A+An+Ensemble+Machine+Learning+Framework+Using+Wearable+Sensor+Data，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07589，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07589&send_immediately=true&force_search=false)

**原文摘要:** Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.

</details>


### [53] [Synthetic MC via Biological Transmitters: Therapeutic Modulation of the Gut-Brain Axis](https://arxiv.org/abs/2507.07604)
*Sebastian Lotter, Elisabeth Mohr, Andrina Rutsch, Lukas Brand, Francesca Ronchi, Laura Díaz-Marugán*

**主要类别:** cs.LG

**AI概要:** This paper proposes a machine learning model to identify modulators and modulatory pathways of the gut-brain axis for therapeutic purposes.


<details>
  <summary>更多</summary>
  
**动机:** Generating signals inside the human body for synthetic molecular communication poses technological, legal, safety, and ethical challenges. This paper considers an SMC system where signals are generated indirectly via the modulation of a natural in-body MC system, namely the gut-brain axis (GBA).

**方法:** The paper proposes a machine learning model to verify theoretical requirements for therapeutic GBA modulation when only limited data exists. The model is evaluated on several datasets.

**结果:** The proposed machine learning model confirms excellent accuracy in identifying different modulators of the GBA.

**结论:** The proposed machine learning model is effective in identifying different modulators of the GBA and specific modulatory pathways that are important for therapeutic GBA modulation.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synthetic+MC+via+Biological+Transmitters%3A+Therapeutic+Modulation+of+the+Gut-Brain+Axis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07604，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07604&send_immediately=true&force_search=false)

**原文摘要:** Synthetic molecular communication (SMC) is a key enabler for future
healthcare systems in which Internet of Bio-Nano-Things (IoBNT) devices
facilitate the continuous monitoring of a patient's biochemical signals. To
close the loop between sensing and actuation, both the detection and the
generation of in-body molecular communication (MC) signals is key. However,
generating signals inside the human body, e.g., via synthetic nanodevices,
poses a challenge in SMC, due to technological obstacles as well as legal,
safety, and ethical issues. Hence, this paper considers an SMC system in which
signals are generated indirectly via the modulation of a natural in-body MC
system, namely the gut-brain axis (GBA). Therapeutic GBA modulation is already
established as treatment for neurological diseases, e.g., drug refractory
epilepsy (DRE), and performed via the administration of nutritional supplements
or specific diets. However, the molecular signaling pathways that mediate the
effect of such treatments are mostly unknown. Consequently, existing treatments
are standardized or designed heuristically and able to help only some patients
while failing to help others. In this paper, we propose to leverage personal
health data, e.g., gathered by in-body IoBNT devices, to design more versatile
and robust GBA modulation-based treatments as compared to the existing ones. To
show the feasibility of our approach, we define a catalog of theoretical
requirements for therapeutic GBA modulation. Then, we propose a machine
learning model to verify these requirements for practical scenarios when only
limited data on the GBA modulation exists. By evaluating the proposed model on
several datasets, we confirm its excellent accuracy in identifying different
modulators of the GBA. Finally, we utilize the proposed model to identify
specific modulatory pathways that play an important role for therapeutic GBA
modulation.

</details>


### [54] [TransformEEG: Towards Improving Model Generalizability in Deep Learning-based EEG Parkinson's Disease Detection](https://arxiv.org/abs/2507.07622)
*Federico Del Pup, Riccardo Brun, Filippo Iotti, Edoardo Paccagnella, Mattia Pezzato, Sabrina Bertozzo, Andrea Zanola, Louis Fabrice Tshimanga, Henning Müller, Manfredo Atzori*

**主要类别:** cs.LG

**AI概要:** This paper introduces TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's disease detection using EEG data.


<details>
  <summary>更多</summary>
  
**动机:** Current state-of-the-art DL models suffer from poor generalizability caused by high inter-subject variability. This high variability underscores the need for enhancing model generalizability by developing new architectures better tailored to EEG data.

**方法:** This paper introduces TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's disease detection using EEG data.

**结果:** TransformEEG achieved the highest balanced accuracy's median (78.45%) as well as the lowest interquartile range (6.37%) across all the N-LNSO partitions. When combined with data augmentation and threshold correction, median accuracy increased to 80.10%, with an interquartile range of 5.74%.

**结论:** TransformEEG produces more consistent and less skewed results. It demonstrates a substantial reduction in variability and more reliable PD detection using EEG data compared to the other investigated models.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TransformEEG%3A+Towards+Improving+Model+Generalizability+in+Deep+Learning-based+EEG+Parkinson%27s+Disease+Detection，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07622，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07622&send_immediately=true&force_search=false)

**原文摘要:** Electroencephalography (EEG) is establishing itself as an important,
low-cost, noninvasive diagnostic tool for the early detection of Parkinson's
Disease (PD). In this context, EEG-based Deep Learning (DL) models have shown
promising results due to their ability to discover highly nonlinear patterns
within the signal. However, current state-of-the-art DL models suffer from poor
generalizability caused by high inter-subject variability. This high
variability underscores the need for enhancing model generalizability by
developing new architectures better tailored to EEG data. This paper introduces
TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's
disease detection using EEG data. Unlike transformer models based on the EEGNet
structure, TransformEEG incorporates a depthwise convolutional tokenizer. This
tokenizer is specialized in generating tokens composed by channel-specific
features, which enables more effective feature mixing within the self-attention
layers of the transformer encoder. To evaluate the proposed model, four public
datasets comprising 290 subjects (140 PD patients, 150 healthy controls) were
harmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out
(N-LNSO) cross-validation was performed to provide an unbiased comparison
against seven other consolidated EEG deep learning models. TransformEEG
achieved the highest balanced accuracy's median (78.45%) as well as the lowest
interquartile range (6.37%) across all the N-LNSO partitions. When combined
with data augmentation and threshold correction, median accuracy increased to
80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG
produces more consistent and less skewed results. It demonstrates a substantial
reduction in variability and more reliable PD detection using EEG data compared
to the other investigated models.

</details>


### [55] [Sparse Self-Federated Learning for Energy Efficient Cooperative Intelligence in Society 5.0](https://arxiv.org/abs/2507.07613)
*Davide Domini, Laura Erhan, Gianluca Aguzzi, Lucia Cavallaro, Amirhossein Douzandeh Zenoozi, Antonio Liotta, Mirko Viroli*

**主要类别:** cs.LG

**AI概要:** This paper introduces SParSeFuL, a resource-aware approach to Federated Learning that addresses the sustainability demands of emerging IoT ecosystems.


<details>
  <summary>更多</summary>
  
**动机:** Federated Learning struggles to meet the sustainability demands of emerging IoT ecosystems necessary for Society 5.0, as traditional FL approaches require excessive communication bandwidth and computational resources, making them environmentally unsustainable at scale.

**方法:** Sparse Proximity-based Self-Federated Learning (SParSeFuL) combines aggregate computing for self-organization with neural network sparsification to reduce energy and bandwidth consumption.

**结果:** The proposed approach reduces energy and bandwidth consumption, allowing resource-constrained devices to participate in a human-centered technological future balancing social advancement with environmental responsibility.

**结论:** Sparse Proximity-based Self-Federated Learning (SParSeFuL) bridges the gap between privacy-preserving collaborative intelligence and environmental sustainability in emerging IoT ecosystems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparse+Self-Federated+Learning+for+Energy+Efficient+Cooperative+Intelligence+in+Society+5.0，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07613，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07613&send_immediately=true&force_search=false)

**原文摘要:** Federated Learning offers privacy-preserving collaborative intelligence but
struggles to meet the sustainability demands of emerging IoT ecosystems
necessary for Society 5.0-a human-centered technological future balancing
social advancement with environmental responsibility. The excessive
communication bandwidth and computational resources required by traditional FL
approaches make them environmentally unsustainable at scale, creating a
fundamental conflict with green AI principles as billions of
resource-constrained devices attempt to participate. To this end, we introduce
Sparse Proximity-based Self-Federated Learning (SParSeFuL), a resource-aware
approach that bridges this gap by combining aggregate computing for
self-organization with neural network sparsification to reduce energy and
bandwidth consumption.

</details>


### [56] [OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting](https://arxiv.org/abs/2507.07754)
*Jaeheun Jung, Bosung Jung, Suhyun Bae, Donghun Lee*

**主要类别:** cs.LG

**AI概要:** 本文探讨了机器遗忘领域中现有方法的不足，并提出了一种新的遗忘算法OPC，该算法能够实现深度遗忘并提高对攻击的抵抗力。


<details>
  <summary>更多</summary>
  
**动机:** 现有的遗忘方法往往浅层遗忘，即模型仅调整了响应而内部表示仍保留足够信息以恢复被遗忘的数据或行为，这在隐私、法律或伦理要求下是不够的。

**方法:** 研究定义了一个“深度遗忘”的理论标准，并提出了一个有效的近似算法，用于构建新的通用遗忘算法：One-Point-Contraction (OPC)。

**结果:** 图像分类遗忘基准上的实证评估显示，OPC不仅实现了有效的遗忘性能，而且对性能恢复攻击和梯度反转攻击都具有优越的抵御能力。

**结论:** OPC算法通过其理论基础强制执行深度特征遗忘，展示了机器遗忘方法需要改进的必要性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是OPC%3A+One-Point-Contraction+Unlearning+Toward+Deep+Feature+Forgetting，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07754，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07754&send_immediately=true&force_search=false)

**原文摘要:** Machine unlearning seeks to remove the influence of particular data or class
from trained models to meet privacy, legal, or ethical requirements. Existing
unlearning methods tend to forget shallowly: phenomenon of an unlearned model
pretend to forget by adjusting only the model response, while its internal
representations retain information sufficiently to restore the forgotten data
or behavior. We empirically confirm the widespread shallowness by reverting the
forgetting effect of various unlearning methods via training-free performance
recovery attack and gradient-inversion-based data reconstruction attack. To
address this vulnerability fundamentally, we define a theoretical criterion of
``deep forgetting'' based on one-point-contraction of feature representations
of data to forget. We also propose an efficient approximation algorithm, and
use it to construct a novel general-purpose unlearning algorithm:
One-Point-Contraction (OPC). Empirical evaluations on image classification
unlearning benchmarks show that OPC achieves not only effective unlearning
performance but also superior resilience against both performance recovery
attack and gradient-inversion attack. The distinctive unlearning performance of
OPC arises from the deep feature forgetting enforced by its theoretical
foundation, and recaps the need for improved robustness of machine unlearning
methods.

</details>


### [57] [Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation](https://arxiv.org/abs/2507.07621)
*Junyu Luo, Yuhao Tang, Yiwei Fu, Xiao Luo, Zhizhuo Kou, Zhiping Xiao, Wei Ju, Wentao Zhang, Ming Zhang*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为SLOGAN的新方法，通过稀疏因果发现与生成干预解决现有无监督图领域适应方法中的因果-虚假特征纠缠问题和全局对齐策略失败的问题，从而实现更稳定有效的图表示迁移。


<details>
  <summary>更多</summary>
  
**动机:** 无监督图领域适应（UGDA）利用有标签的源域图来在未标记的目标域中实现有效性能，尽管存在分布偏移。然而，现有方法常常由于因果-虚假特征纠缠和全局对齐策略失败导致次优结果。

**方法:** SLOGAN通过构建稀疏因果图结构、利用互信息瓶颈约束解开稀疏且稳定的因果特征，并通过变分推断压缩依赖于领域的虚假相关性；设计了一种生成干预机制，通过跨域特征重组打破局部虚假耦合，同时通过协方差约束保持因果特征的语义一致性；引入类别自适应动态校准策略，确保稳定辨别学习，减轻目标域伪标签中的误差累积。

**结果:** SLOGAN通过稀疏因果建模和动态干预机制实现了稳定的图表示迁移，具体包括解开稀疏、稳定的因果特征，处理残余虚假相关性，以及减轻目标域伪标签中的误差累积。

**结论:** SLOGAN在多个真实世界数据集上的广泛实验表明，其性能显著优于现有的基线方法。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Sparse+Causal+Discovery+with+Generative+Intervention+for+Unsupervised+Graph+Domain+Adaptation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07621，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07621&send_immediately=true&force_search=false)

**原文摘要:** Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain
graphs to achieve effective performance in unlabeled target domains despite
distribution shifts. However, existing methods often yield suboptimal results
due to the entanglement of causal-spurious features and the failure of global
alignment strategies. We propose SLOGAN (Sparse Causal Discovery with
Generative Intervention), a novel approach that achieves stable graph
representation transfer through sparse causal modeling and dynamic intervention
mechanisms. Specifically, SLOGAN first constructs a sparse causal graph
structure, leveraging mutual information bottleneck constraints to disentangle
sparse, stable causal features while compressing domain-dependent spurious
correlations through variational inference. To address residual spurious
correlations, we innovatively design a generative intervention mechanism that
breaks local spurious couplings through cross-domain feature recombination
while maintaining causal feature semantic consistency via covariance
constraints. Furthermore, to mitigate error accumulation in target domain
pseudo-labels, we introduce a category-adaptive dynamic calibration strategy,
ensuring stable discriminative learning. Extensive experiments on multiple
real-world datasets demonstrate that SLOGAN significantly outperforms existing
baselines.

</details>


### [58] [Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training](https://arxiv.org/abs/2507.07778)
*Wooseong Jeong, Jegyeong Cho, Youngho Yoon, Kuk-Jin Yoon*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种新的测试时训练方法，称为S4T，可以有效地处理多个任务并同步它们的行为，从而提高性能。


<details>
  <summary>更多</summary>
  
**动机:** 在现实世界部署中，将神经网络推广到未见过的目标领域是一项重大挑战。传统的测试时训练方法在面对多任务时表现不佳，因为适应一个任务所需的步骤可能与其他任务的要求不同步。

**方法:** 研究提出了一种新的测试时训练方法，称为S4T，通过预测跨域任务关系来同步任务。

**结果:** 实证结果表明，S4T在各种基准测试中优于最先进的TTT方法。

**结论:** S4T是一个有效的测试时训练方法，可以处理多个任务并同步它们的行为。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Synchronizing+Task+Behavior%3A+Aligning+Multiple+Tasks+during+Test-Time+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07778，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07778&send_immediately=true&force_search=false)

**原文摘要:** Generalizing neural networks to unseen target domains is a significant
challenge in real-world deployments. Test-time training (TTT) addresses this by
using an auxiliary self-supervised task to reduce the domain gap caused by
distribution shifts between the source and target. However, we find that when
models are required to perform multiple tasks under domain shifts, conventional
TTT methods suffer from unsynchronized task behavior, where the adaptation
steps needed for optimal performance in one task may not align with the
requirements of other tasks. To address this, we propose a novel TTT approach
called Synchronizing Tasks for Test-time Training (S4T), which enables the
concurrent handling of multiple tasks. The core idea behind S4T is that
predicting task relations across domain shifts is key to synchronizing tasks
during test time. To validate our approach, we apply S4T to conventional
multi-task benchmarks, integrating it with traditional TTT protocols. Our
empirical results show that S4T outperforms state-of-the-art TTT methods across
various benchmarks.

</details>


### [59] [Optimization Guarantees for Square-Root Natural-Gradient Variational Inference](https://arxiv.org/abs/2507.07853)
*Navish Kumar, Thomas Möllenhoff, Mohammad Emtiyaz Khan, Aurelien Lucchi*

**主要类别:** cs.LG

**AI概要:** This paper establishes convergence guarantees for natural-gradient variational-Gaussian inference using square-root parameterization.


<details>
  <summary>更多</summary>
  
**动机:** To address the challenge of establishing theoretical convergence guarantees for variational inference with natural-gradient descent, even for simple cases involving concave log-likelihoods and a Gaussian approximation.

**方法:** The study uses a square-root parameterization for the Gaussian covariance in order to establish novel convergence guarantees for natural-gradient variational-Gaussian inference and its continuous-time gradient flow.

**结果:** The experiments demonstrate the effectiveness of natural gradient methods and their advantages over algorithms using Euclidean or Wasserstein geometries.

**结论:** The use of square-root parameterization for the Gaussian covariance allows for establishing convergence guarantees for natural-gradient variational-Gaussian inference, showing advantages over algorithms that use Euclidean or Wasserstein geometries.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Optimization+Guarantees+for+Square-Root+Natural-Gradient+Variational+Inference，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07853，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07853&send_immediately=true&force_search=false)

**原文摘要:** Variational inference with natural-gradient descent often shows fast
convergence in practice, but its theoretical convergence guarantees have been
challenging to establish. This is true even for the simplest cases that involve
concave log-likelihoods and use a Gaussian approximation. We show that the
challenge can be circumvented for such cases using a square-root
parameterization for the Gaussian covariance. This approach establishes novel
convergence guarantees for natural-gradient variational-Gaussian inference and
its continuous-time gradient flow. Our experiments demonstrate the
effectiveness of natural gradient methods and highlight their advantages over
algorithms that use Euclidean or Wasserstein geometries.

</details>


### [60] [HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on Hyperledger Fabric](https://arxiv.org/abs/2507.07637)
*Carlos Beis Penedo, Rebeca P. Díaz Redondo, Ana Fernández Vilas, Manuel Fernández Veiga, Francisco Troncoso Pastoriza*

**主要类别:** cs.LG

**AI概要:** This paper introduces HLF-FSL, a decentralized architecture combining Federated Split Learning with Hyperledger Fabric, achieving privacy preservation and enterprise-level performance.


<details>
  <summary>更多</summary>
  
**动机:** To address the need for scalable and privacy-preserving solutions in collaborative machine learning for sensitive domains, overcoming limitations of conventional Federated Learning and Split Learning.

**方法:** The study proposes a decentralized architecture that integrates Federated Split Learning (FSL) with Hyperledger Fabric (HLF), utilizing chaincode for coordination and leveraging HLF's features for privacy.

**结果:** HLF-FSL matches centralized FSL accuracy and reduces training time per epoch compared to Ethereum-based works, with tests showing minimal blockchain overhead and maintained accuracy.

**结论:** The HLF-FSL architecture presents a viable solution for privacy-preserving machine learning, suitable for enterprise use.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是HLF-FSL.+A+Decentralized+Federated+Split+Learning+Solution+for+IoT+on+Hyperledger+Fabric，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07637，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07637&send_immediately=true&force_search=false)

**原文摘要:** Collaborative machine learning in sensitive domains demands scalable, privacy
preserving solutions for enterprise deployment. Conventional Federated Learning
(FL) relies on a central server, introducing single points of failure and
privacy risks, while Split Learning (SL) partitions models for privacy but
scales poorly due to sequential training. We present a decentralized
architecture that combines Federated Split Learning (FSL) with the permissioned
blockchain Hyperledger Fabric (HLF). Our chaincode orchestrates FSL's split
model execution and peer-to-peer aggregation without any central coordinator,
leveraging HLF's transient fields and Private Data Collections (PDCs) to keep
raw data and model activations private. On CIFAR-10 and MNIST benchmarks,
HLF-FSL matches centralized FSL accuracy while reducing per epoch training time
compared to Ethereum-based works. Performance and scalability tests show
minimal blockchain overhead and preserved accuracy, demonstrating enterprise
grade viability.

</details>


### [61] [Some Theoretical Results on Layerwise Effective Dimension Oscillations in Finite Width ReLU Networks](https://arxiv.org/abs/2507.07675)
*Darshan Makwana*

**主要类别:** cs.LG

**AI概要:** This paper analyzes the effective dimension of layers in ReLU networks, revealing insights into rank behavior and its implications on network expressivity.


<details>
  <summary>更多</summary>
  
**动机:** To understand how the layerwise effective dimension in finite width fully-connected ReLU networks behaves, particularly the rank of feature matrices.

**方法:** Analysis of the layerwise effective dimension in fully-connected ReLU networks with random Gaussian weights. Derivation of closed-form expressions for expected rank of hidden activation matrices.

**结果:** A main result showing the expected rank deficit decays geometrically. Proof of a sub-Gaussian concentration bound. Identification of 'revival' depths where expected rank attains local maxima.

**结论:** The oscillatory rank behavior is a finite-width phenomenon and under certain conditions the rank remains nearly full, providing a more nuanced understanding of deep network expressivity.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Some+Theoretical+Results+on+Layerwise+Effective+Dimension+Oscillations+in+Finite+Width+ReLU+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07675，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07675&send_immediately=true&force_search=false)

**原文摘要:** We analyze the layerwise effective dimension (rank of the feature matrix) in
fully-connected ReLU networks of finite width. Specifically, for a fixed batch
of $m$ inputs and random Gaussian weights, we derive closed-form expressions
for the expected rank of the \$m\times n\$ hidden activation matrices. Our main
result shows that $\mathbb{E}[EDim(\ell)]=m[1-(1-2/\pi)^\ell]+O(e^{-c m})$ so
that the rank deficit decays geometrically with ratio $1-2 / \pi \approx
0.3634$. We also prove a sub-Gaussian concentration bound, and identify the
"revival" depths at which the expected rank attains local maxima. In
particular, these peaks occur at depths
$\ell_k^*\approx(k+1/2)\pi/\log(1/\rho)$ with height $\approx (1-e^{-\pi/2}) m
\approx 0.79m$. We further show that this oscillatory rank behavior is a
finite-width phenomenon: under orthogonal weight initialization or strong
negative-slope leaky-ReLU, the rank remains (nearly) full. These results
provide a precise characterization of how random ReLU layers alternately
collapse and partially revive the subspace of input variations, adding nuance
to prior work on expressivity of deep networks.

</details>


### [62] [UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient Neural Inference on MCUs](https://arxiv.org/abs/2507.07885)
*Ashe Neth, Sawinder kaur, Mohammad Nur Hossain Khan, Subrata Biswas, Asif Salekin, Bashima Islam*

**主要类别:** cs.LG

**AI概要:** The paper introduces UnIT, an unstructured inference-time pruning method for efficient deployment of deep neural networks on microcontrollers.


<details>
  <summary>更多</summary>
  
**动机:** Existing pruning methods underutilize the opportunity for fine-grained efficiency on devices without SIMD support or parallel compute.

**方法:** UnIT (Unstructured Inference-Time pruning), a lightweight method that dynamically identifies and skips unnecessary MAC operations during inference guided by input-specific activation patterns.

**结果:** UnIT achieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and 27.33% to 84.38% lower energy consumption compared to training-time pruned models while maintaining accuracy with 0.48-7%. Under domain shift, UnIT matches or exceeds the accuracy of retrained models while requiring significantly fewer MACs.

**结论:** Unstructured inference-time pruning is a viable and practical solution for efficient, retraining-free deployment of deep neural networks on MCUs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是UnIT%3A+Scalable+Unstructured+Inference-Time+Pruning+for+MAC-efficient+Neural+Inference+on+MCUs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07885，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07885&send_immediately=true&force_search=false)

**原文摘要:** Existing pruning methods are typically applied during training or compile
time and often rely on structured sparsity. While compatible with low-power
microcontrollers (MCUs), structured pruning underutilizes the opportunity for
fine-grained efficiency on devices without SIMD support or parallel compute. To
address these limitations, we introduce UnIT (Unstructured Inference-Time
pruning), a lightweight method that dynamically identifies and skips
unnecessary multiply-accumulate (MAC) operations during inference, guided by
input-specific activation patterns. Unlike structured pruning, UnIT embraces
irregular sparsity and does not require retraining or hardware specialization.
It transforms pruning decisions into lightweight comparisons, replacing
multiplications with threshold checks and approximated divisions. UnIT further
optimizes compute by reusing threshold computations across multiple connections
and applying layer- and group-specific pruning sensitivity. We present three
fast, hardware-friendly division approximations tailored to the capabilities of
common embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT
achieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and
27.33% to 84.38% lower energy consumption compared to training-time pruned
models, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT
matches or exceeds the accuracy of retrained models while requiring
significantly fewer MACs. These results establish unstructured inference-time
pruning as a viable and practical solution for efficient, retraining-free
deployment of deep neural networks on MCUs.

</details>


### [63] [Balancing the Past and Present: A Coordinated Replay Framework for Federated Class-Incremental Learning](https://arxiv.org/abs/2507.07712)
*Zhuang Qi, Lei Meng, Han Yu*

**主要类别:** cs.LG

**AI概要:** 提出了一种名为FedCBDR的新方法，用于解决Federated Class Incremental Learning中的类别不平衡问题，通过全局协调机制和重新加权学习目标来实现平衡的类别采样，并能够提高模型的泛化能力。


<details>
  <summary>更多</summary>
  
**动机:** 为了解决FCIL中的类别不平衡问题，包括在重放缓冲区中由于有限的全局意识导致的不平衡以及在重放类别和新到达类别之间的不平衡。

**方法:** 提出了一种名为FedCBDR的类别平衡数据回放方法，该方法采用全局协调机制进行类级内存构建，并重新加权学习目标以减轻上述不平衡。

**结果:** 实验结果证实，FedCBDR在异构数据分布下实现了平衡的类别采样，在早期和近期任务之间的任务不平衡下提高了泛化能力，并且在六种最先进的方法之上实现了2％-15％的Top-1精度提升。

**结论:** FedCBDR是一种有效的解决FCIL中类别不平衡问题的方法，可以提高模型的泛化能力并改善早期和近期任务之间的任务不平衡。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Balancing+the+Past+and+Present%3A+A+Coordinated+Replay+Framework+for+Federated+Class-Incremental+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07712，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07712&send_immediately=true&force_search=false)

**原文摘要:** Federated Class Incremental Learning (FCIL) aims to collaboratively process
continuously increasing incoming tasks across multiple clients. Among various
approaches, data replay has become a promising solution, which can alleviate
forgetting by reintroducing representative samples from previous tasks.
However, their performance is typically limited by class imbalance, both within
the replay buffer due to limited global awareness and between replayed and
newly arrived classes. To address this issue, we propose a class wise balancing
data replay method for FCIL (FedCBDR), which employs a global coordination
mechanism for class-level memory construction and reweights the learning
objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has
two key components: 1) the global-perspective data replay module reconstructs
global representations of prior task in a privacy-preserving manner, which then
guides a class-aware and importance-sensitive sampling strategy to achieve
balanced replay; 2) Subsequently, to handle class imbalance across tasks, the
task aware temperature scaling module adaptively adjusts the temperature of
logits at both class and instance levels based on task dynamics, which reduces
the model's overconfidence in majority classes while enhancing its sensitivity
to minority classes. Experimental results verified that FedCBDR achieves
balanced class-wise sampling under heterogeneous data distributions and
improves generalization under task imbalance between earlier and recent tasks,
yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.

</details>


### [64] [Agentic Retrieval of Topics and Insights from Earnings Calls](https://arxiv.org/abs/2507.07906)
*Anant Gupta, Rajarshi Bhowmik, Geoffrey Gunow*

**主要类别:** cs.LG

**AI概要:** This paper proposes an LLM-agent driven approach to discover and retrieve emerging topics from quarterly earnings calls, aiming to overcome the limitations of traditional topic modeling techniques.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the limitation of traditional topic modeling techniques in dynamically capturing emerging topics and their relationships as industries evolve.

**方法:** An LLM-agent driven approach is used to extract topics, structure them into a hierarchical ontology, and establish relationships between new and existing topics through a topic ontology.

**结果:** The approach demonstrates ontology coherence, topic evolution accuracy, and the ability to surface emerging financial trends.

**结论:** The proposed LLM-agent driven approach is effective in discovering and retrieving emerging topics from quarterly earnings calls, providing valuable company-level insights and tracking of emerging trends over time.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic+Retrieval+of+Topics+and+Insights+from+Earnings+Calls，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07906，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07906&send_immediately=true&force_search=false)

**原文摘要:** Tracking the strategic focus of companies through topics in their earnings
calls is a key task in financial analysis. However, as industries evolve,
traditional topic modeling techniques struggle to dynamically capture emerging
topics and their relationships. In this work, we propose an LLM-agent driven
approach to discover and retrieve emerging topics from quarterly earnings
calls. We propose an LLM-agent to extract topics from documents, structure them
into a hierarchical ontology, and establish relationships between new and
existing topics through a topic ontology. We demonstrate the use of extracted
topics to infer company-level insights and emerging trends over time. We
evaluate our approach by measuring ontology coherence, topic evolution
accuracy, and its ability to surface emerging financial trends.

</details>


### [65] [Low Resource Reconstruction Attacks Through Benign Prompts](https://arxiv.org/abs/2507.07947)
*Sol Yarkoni, Roi Livni*

**主要类别:** cs.LG

**AI概要:** This paper highlights the risks associated with generative models like diffusion models, focusing on unintentional privacy breaches and copyright infringements through the use of simple, seemingly harmless prompts.


<details>
  <summary>更多</summary>
  
**动机:** To better understand and control the risks related to privacy, copyright infringements and data stewardship in generative models such as diffusion models.

**方法:** Our method builds on an intuition from previous works which leverages domain knowledge and identifies a fundamental vulnerability that stems from the use of scraped data from e-commerce platforms.

**结果:** We devise a new attack that requires low resources, assumes little to no access to the actual training set, and identifies seemingly benign prompts that lead to potentially-risky image reconstruction.

**结论:** The risks associated with generative models, particularly in relation to privacy and copyright, are significant and can occur unintentionally through the use of seemingly benign prompts.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Low+Resource+Reconstruction+Attacks+Through+Benign+Prompts，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07947，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07947&send_immediately=true&force_search=false)

**原文摘要:** The recent advances in generative models such as diffusion models have raised
several risks and concerns related to privacy, copyright infringements and data
stewardship. To better understand and control the risks, various researchers
have created techniques, experiments and attacks that reconstruct images, or
part of images, from the training set. While these techniques already establish
that data from the training set can be reconstructed, they often rely on
high-resources, excess to the training set as well as well-engineered and
designed prompts.
  In this work, we devise a new attack that requires low resources, assumes
little to no access to the actual training set, and identifies, seemingly,
benign prompts that lead to potentially-risky image reconstruction. This
highlights the risk that images might even be reconstructed by an uninformed
user and unintentionally. For example, we identified that, with regard to one
existing model, the prompt ``blue Unisex T-Shirt'' can generate the face of a
real-life human model. Our method builds on an intuition from previous works
which leverages domain knowledge and identifies a fundamental vulnerability
that stems from the use of scraped data from e-commerce platforms, where
templated layouts and images are tied to pattern-like prompts.

</details>


### [66] [Efficient and Scalable Estimation of Distributional Treatment Effects with Multi-Task Neural Networks](https://arxiv.org/abs/2507.07738)
*Tomu Hirata, Undral Byambadalai, Tatsushi Oka, Shota Yasui, Shingo Uto*

**主要类别:** cs.LG

**AI概要:** A novel multi-task neural network approach is proposed for estimating distributional treatment effects (DTE) in randomized experiments, providing more granular insights into experiment outcomes.


<details>
  <summary>更多</summary>
  
**动机:** Estimating distributional treatment effects (DTE) with regression adjustment methods presents significant challenges, particularly in large-scale datasets commonly encountered in industry. Precision in the distribution tails suffers due to data imbalance, and computational inefficiencies arise from the need to solve numerous regression problems.

**方法:** The method leverages multi-task neural networks to estimate conditional outcome distributions while incorporating monotonic shape constraints and multi-threshold label learning to enhance accuracy.

**结果:** The experimental results consistently demonstrate superior performance across various datasets, including both simulated and real-world datasets.

**结论:** The proposed multi-task neural network approach is a robust and practical solution for modern causal inference applications that require a detailed understanding of treatment effect heterogeneity.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+and+Scalable+Estimation+of+Distributional+Treatment+Effects+with+Multi-Task+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07738，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07738&send_immediately=true&force_search=false)

**原文摘要:** We propose a novel multi-task neural network approach for estimating
distributional treatment effects (DTE) in randomized experiments. While DTE
provides more granular insights into the experiment outcomes over conventional
methods focusing on the Average Treatment Effect (ATE), estimating it with
regression adjustment methods presents significant challenges. Specifically,
precision in the distribution tails suffers due to data imbalance, and
computational inefficiencies arise from the need to solve numerous regression
problems, particularly in large-scale datasets commonly encountered in
industry. To address these limitations, our method leverages multi-task neural
networks to estimate conditional outcome distributions while incorporating
monotonic shape constraints and multi-threshold label learning to enhance
accuracy. To demonstrate the practical effectiveness of our proposed method, we
apply our method to both simulated and real-world datasets, including a
randomized field experiment aimed at reducing water consumption in the US and a
large-scale A/B test from a leading streaming platform in Japan. The
experimental results consistently demonstrate superior performance across
various datasets, establishing our method as a robust and practical solution
for modern causal inference applications requiring a detailed understanding of
treatment effect heterogeneity.

</details>


### [67] [Reinforcement Learning with Action Chunking](https://arxiv.org/abs/2507.07969)
*Qiyang Li, Zhiyuan Zhou, Sergey Levine*

**主要类别:** cs.LG

**AI概要:** This paper presents Q-chunking, a recipe for improving reinforcement learning algorithms for long-horizon, sparse-reward tasks in the offline-to-online RL setting.


<details>
  <summary>更多</summary>
  
**动机:** Effective exploration and sample-efficient learning remain central challenges in the offline-to-online RL setting.

**方法:** Q-chunking adopts action chunking by directly running RL in a 'chunked' action space.

**结果:** Q-chunking exhibits strong offline performance and online sample efficiency.

**结论:** Q-chunking outperforms prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Reinforcement+Learning+with+Action+Chunking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07969，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07969&send_immediately=true&force_search=false)

**原文摘要:** We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.

</details>


### [68] [EXPO: Stable Reinforcement Learning with Expressive Policies](https://arxiv.org/abs/2507.07986)
*Perry Dong, Qiyang Li, Dorsa Sadigh, Chelsea Finn*

**主要类别:** cs.LG

**AI概要:** This paper addresses the challenge of stable value maximization when training expressive policy classes with online RL by proposing Expressive Policy Optimization (EXPO), a sample-efficient online RL algorithm. The EXPO algorithm yields significant improvements in sample efficiency over prior methods.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind the study is to address the challenge of stable value maximization when training expressive policy classes with online RL.

**方法:** The study proposes Expressive Policy Optimization (EXPO), a sample-efficient online reinforcement learning (RL) algorithm that utilizes an on-the-fly policy to maximize value with two parameterized policies.

**结果:** The EXPO algorithm yields up to 2-3x improvement in sample efficiency on average over prior methods in both settings - fine-tuning a pretrained policy given offline data and leveraging offline data to train online.

**结论:** The proposed Expressive Policy Optimization (EXPO) algorithm yields significant improvements in sample efficiency over prior methods in both fine-tuning a pretrained policy given offline data and leveraging offline data to train online.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EXPO%3A+Stable+Reinforcement+Learning+with+Expressive+Policies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07986，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07986&send_immediately=true&force_search=false)

**原文摘要:** We study the problem of training and fine-tuning expressive policies with
online reinforcement learning (RL) given an offline dataset. Training
expressive policy classes with online RL present a unique challenge of stable
value maximization. Unlike simpler Gaussian policies commonly used in online
RL, expressive policies like diffusion and flow-matching policies are
parameterized by a long denoising chain, which hinders stable gradient
propagation from actions to policy parameters when optimizing against some
value function. Our key insight is that we can address stable value
maximization by avoiding direct optimization over value with the expressive
policy and instead construct an on-the-fly RL policy to maximize Q-value. We
propose Expressive Policy Optimization (EXPO), a sample-efficient online RL
algorithm that utilizes an on-the-fly policy to maximize value with two
parameterized policies -- a larger expressive base policy trained with a stable
imitation learning objective and a light-weight Gaussian edit policy that edits
the actions sampled from the base policy toward a higher value distribution.
The on-the-fly policy optimizes the actions from the base policy with the
learned edit policy and chooses the value maximizing action from the base and
edited actions for both sampling and temporal-difference (TD) backup. Our
approach yields up to 2-3x improvement in sample efficiency on average over
prior methods both in the setting of fine-tuning a pretrained policy given
offline data and in leveraging offline data to train online.

</details>


### [69] [TRIX- Trading Adversarial Fairness via Mixed Adversarial Training](https://arxiv.org/abs/2507.07768)
*Tejaswini Medi, Steffen Jung, Margret Keuper*

**主要类别:** cs.LG

**AI概要:** This paper introduces TRIX, a feature-aware adversarial training framework that addresses adversarial unfairness by adaptively assigning adversaries based on class strength.


<details>
  <summary>更多</summary>
  
**动机:** Existing approaches overlook disparities in class-wise vulnerability, resulting in adversarial unfairness. Strong classes do not require strong adversaries during training, while weak classes benefit from stronger adversaries.

**方法:** TRIX, a feature-aware adversarial training framework that adaptively assigns weaker targeted adversaries to strong classes and stronger untargeted adversaries to weak classes.

**结果:** Comprehensive experiments demonstrate that TRIX significantly improves worst-case class accuracy on both clean and adversarial data, reducing inter-class robustness disparities.

**结论:** TRIX improves worst-case class accuracy on both clean and adversarial data, reducing inter-class robustness disparities while preserving overall accuracy.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是TRIX-+Trading+Adversarial+Fairness+via+Mixed+Adversarial+Training，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07768，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07768&send_immediately=true&force_search=false)

**原文摘要:** Adversarial Training (AT) is a widely adopted defense against adversarial
examples. However, existing approaches typically apply a uniform training
objective across all classes, overlooking disparities in class-wise
vulnerability. This results in adversarial unfairness: classes with well
distinguishable features (strong classes) tend to become more robust, while
classes with overlapping or shared features(weak classes) remain
disproportionately susceptible to adversarial attacks. We observe that strong
classes do not require strong adversaries during training, as their non-robust
features are quickly suppressed. In contrast, weak classes benefit from
stronger adversaries to effectively reduce their vulnerabilities. Motivated by
this, we introduce TRIX, a feature-aware adversarial training framework that
adaptively assigns weaker targeted adversaries to strong classes, promoting
feature diversity via uniformly sampled targets, and stronger untargeted
adversaries to weak classes, enhancing their focused robustness. TRIX further
incorporates per-class loss weighting and perturbation strength adjustments,
building on prior work, to emphasize weak classes during the optimization.
Comprehensive experiments on standard image classification benchmarks,
including evaluations under strong attacks such as PGD and AutoAttack,
demonstrate that TRIX significantly improves worst-case class accuracy on both
clean and adversarial data, reducing inter-class robustness disparities, and
preserves overall accuracy. Our results highlight TRIX as a practical step
toward fair and effective adversarial defense.

</details>


### [70] [BEAVER: Building Environments with Assessable Variation for Evaluating Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.07769)
*Ruohong Liu, Jack Umenberger, Yize Chen*

**主要类别:** cs.LG

**AI概要:** This paper addresses the scalability issue of reinforcement learning (RL) in building energy management, focusing on generalization across different environments and objectives. It proposes a formal characterization of the generalization space, a formulation of the multi-objective contextual RL problem, and provides a benchmark for evaluating RL algorithms. Results indicate that existing methods perform well but degrade in varying environments, emphasizing the importance of including contextual information.


<details>
  <summary>更多</summary>
  
**动机:** While there has been success in designing RL-based agents for building energy management, the scalability of these approaches in terms of efficiency and generalization remains an open question. The work aims to address this gap by formally characterizing the generalization space and formulating the multi-objective contextual RL problem.

**方法:** The authors formally characterize the generalization space for cross-environment, multi-objective building energy management tasks and formulate the multi-objective contextual RL problem. They parameterize contextual information in realistic building RL environments and construct a novel benchmark for evaluating generalizable RL algorithms.

**结果:** Existing multi-objective RL methods can achieve reasonable trade-offs between conflicting objectives but struggle under certain environment variations, highlighting the need for incorporating dynamics-dependent contextual information.

**结论:** Incorporating dynamics-dependent contextual information into the policy learning process is crucial for RL algorithms to generalize well across varied building environments and objectives.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BEAVER%3A+Building+Environments+with+Assessable+Variation+for+Evaluating+Multi-Objective+Reinforcement+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07769，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07769&send_immediately=true&force_search=false)

**原文摘要:** Recent years have seen significant advancements in designing reinforcement
learning (RL)-based agents for building energy management. While individual
success is observed in simulated or controlled environments, the scalability of
RL approaches in terms of efficiency and generalization across building
dynamics and operational scenarios remains an open question. In this work, we
formally characterize the generalization space for the cross-environment,
multi-objective building energy management task, and formulate the
multi-objective contextual RL problem. Such a formulation helps understand the
challenges of transferring learned policies across varied operational contexts
such as climate and heat convection dynamics under multiple control objectives
such as comfort level and energy consumption. We provide a principled way to
parameterize such contextual information in realistic building RL environments,
and construct a novel benchmark to facilitate the evaluation of generalizable
RL algorithms in practical building control tasks. Our results show that
existing multi-objective RL methods are capable of achieving reasonable
trade-offs between conflicting objectives. However, their performance degrades
under certain environment variations, underscoring the importance of
incorporating dynamics-dependent contextual information into the policy
learning process.

</details>


### [71] [Space-Filling Regularization for Robust and Interpretable Nonlinear State Space Models](https://arxiv.org/abs/2507.07792)
*Hermann Klein, Max Heinz Herkersdorf, Oliver Nelles*

**主要类别:** cs.LG

**AI概要:** A new space-filling regularization technique is proposed to ensure a favorable data distribution in state space, improving the interpretability and robustness of local model network architectures.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to address the issue of significant state trajectory deformations during training, which can cause poor data coverage of the state space and deteriorate interpretability and robustness properties.

**方法:** This paper proposes a new type of space-filling regularization that introduces a data-distribution-based penalty to ensure a favorable data distribution in state space. Two regularization techniques for the data point distributions of the state trajectories for local affine state space models are presented.

**结果:** The proposed approach is demonstrated in local model network architectures and the results are shown on a widely known system identification benchmark.

**结论:** The proposed space-filling regularization technique ensures a favorable data distribution in state space, improving the interpretability and robustness of local model network architectures.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Space-Filling+Regularization+for+Robust+and+Interpretable+Nonlinear+State+Space+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07792，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07792&send_immediately=true&force_search=false)

**原文摘要:** The state space dynamics representation is the most general approach for
nonlinear systems and often chosen for system identification. During training,
the state trajectory can deform significantly leading to poor data coverage of
the state space. This can cause significant issues for space-oriented training
algorithms which e.g. rely on grid structures, tree partitioning, or similar.
Besides hindering training, significant state trajectory deformations also
deteriorate interpretability and robustness properties. This paper proposes a
new type of space-filling regularization that ensures a favorable data
distribution in state space via introducing a data-distribution-based penalty.
This method is demonstrated in local model network architectures where good
interpretability is a major concern. The proposed approach integrates ideas
from modeling and design of experiments for state space structures. This is why
we present two regularization techniques for the data point distributions of
the state trajectories for local affine state space models. Beyond that, we
demonstrate the results on a widely known system identification benchmark.

</details>


### [72] [Deep Survival Analysis in Multimodal Medical Data: A Parametric and Probabilistic Approach with Competing Risks](https://arxiv.org/abs/2507.07804)
*Alba Garrido, Alejandro Almodóvar, Patricia A. Apellániz, Juan Parras, Santiago Zazo*

**主要类别:** cs.LG

**AI概要:** 本文介绍了一种名为SAMVAE的多模态深度学习框架，用于生存分析，能够建模单个和竞争风险情景，并评估了多模态数据整合对生存预测的影响。


<details>
  <summary>更多</summary>
  
**动机:** 准确的生存预测在肿瘤学中对于预后和治疗计划至关重要。传统方法往往依赖于单一的数据模式，限制了其捕捉肿瘤生物学复杂性的能力。

**方法:** 研究提出了一种新的深度学习架构SAMVAE，该架构整合了六种数据模式：临床变量、四种分子谱和组织病理图像。

**结果:** 在两个癌症队列上评估了SAMVAE，结果表明，对于不同数据集的标准生存分析和竞争风险情景，多模态数据的成功整合是可行的。

**结论:** SAMVAE是一个有效的多模态深度学习框架，用于肿瘤学中的生存分析，并为可解释、数据驱动的生存分析奠定了基础。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Deep+Survival+Analysis+in+Multimodal+Medical+Data%3A+A+Parametric+and+Probabilistic+Approach+with+Competing+Risks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07804，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07804&send_immediately=true&force_search=false)

**原文摘要:** Accurate survival prediction is critical in oncology for prognosis and
treatment planning. Traditional approaches often rely on a single data
modality, limiting their ability to capture the complexity of tumor biology. To
address this challenge, we introduce a multimodal deep learning framework for
survival analysis capable of modeling both single and competing risks
scenarios, evaluating the impact of integrating multiple medical data sources
on survival predictions. We propose SAMVAE (Survival Analysis Multimodal
Variational Autoencoder), a novel deep learning architecture designed for
survival prediction that integrates six data modalities: clinical variables,
four molecular profiles, and histopathological images. SAMVAE leverages
modality specific encoders to project inputs into a shared latent space,
enabling robust survival prediction while preserving modality specific
information. Its parametric formulation enables the derivation of clinically
meaningful statistics from the output distributions, providing patient-specific
insights through interactive multimedia that contribute to more informed
clinical decision-making and establish a foundation for interpretable,
data-driven survival analysis in oncology. We evaluate SAMVAE on two cancer
cohorts breast cancer and lower grade glioma applying tailored preprocessing,
dimensionality reduction, and hyperparameter optimization. The results
demonstrate the successful integration of multimodal data for both standard
survival analysis and competing risks scenarios across different datasets. Our
model achieves competitive performance compared to state-of-the-art multimodal
survival models. Notably, this is the first parametric multimodal deep learning
architecture to incorporate competing risks while modeling continuous time to a
specific event, using both tabular and image data.

</details>


### [73] [Pay Attention to Attention Distribution: A New Local Lipschitz Bound for Transformers](https://arxiv.org/abs/2507.07814)
*Nikolay Yudin, Alexander Gaponov, Sergei Kudriashov, Maxim Rakhuba*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel local Lipschitz bound for self-attention blocks of transformers, which is more accurate and unveils the dependence of the Lipschitz constant on attention score maps. The paper also introduces a new lightweight regularization term called JaSMin, which boosts the transformer's robustness.


<details>
  <summary>更多</summary>
  
**动机:** To improve the robustness of transformers by proposing a more accurate local Lipschitz bound and understanding the way distributions inside the attention map affect the robustness.

**方法:** The paper proposes a novel local Lipschitz bound for self-attention blocks of transformers, based on a refined closed-form expression for the spectral norm of the softmax function. The paper also introduces a new lightweight regularization term called JaSMin.

**结果:** The proposed bound is more accurate than in the prior art, and unveils the dependence of the Lipschitz constant on attention score maps. The new JaSMin regularization boosts the transformer's robustness and decreases local Lipschitz constants of the whole network.

**结论:** The new local Lipschitz bound and JaSMin regularization can improve the robustness of transformers.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pay+Attention+to+Attention+Distribution%3A+A+New+Local+Lipschitz+Bound+for+Transformers，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07814，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07814&send_immediately=true&force_search=false)

**原文摘要:** We present a novel local Lipschitz bound for self-attention blocks of
transformers. This bound is based on a refined closed-form expression for the
spectral norm of the softmax function. The resulting bound is not only more
accurate than in the prior art, but also unveils the dependence of the
Lipschitz constant on attention score maps. Based on the new findings, we
suggest an explanation of the way distributions inside the attention map affect
the robustness from the Lipschitz constant perspective. We also introduce a new
lightweight regularization term called JaSMin (Jacobian Softmax norm
Minimization), which boosts the transformer's robustness and decreases local
Lipschitz constants of the whole network.

</details>


### [74] [An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces and Applications](https://arxiv.org/abs/2507.07826)
*Erfan Mirzaei, Andreas Maurer, Vladimir R. Kostic, Massimiliano Pontil*

**主要类别:** cs.LG

**AI概要:** This study addresses the challenge of learning from non-iid data by developing new Bernstein inequalities for vector-valued processes.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is the challenge of learning from non-independent and non-identically distributed data in statistical learning.

**方法:** The study develops new data-dependent Bernstein inequalities for both stationary and non-stationary processes. These inequalities are applied to covariance operator estimation and operator learning in dynamical systems.

**结果:** Results include novel risk bounds for covariance operator estimation and operator learning in dynamical systems. Numerical experiments support the practical utility of these bounds.

**结论:** The newly developed data-dependent Bernstein inequalities are effective for vector-valued processes in Hilbert space, as shown by theoretical and numerical results.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Empirical+Bernstein+Inequality+for+Dependent+Data+in+Hilbert+Spaces+and+Applications，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07826，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07826&send_immediately=true&force_search=false)

**原文摘要:** Learning from non-independent and non-identically distributed data poses a
persistent challenge in statistical learning. In this study, we introduce
data-dependent Bernstein inequalities tailored for vector-valued processes in
Hilbert space. Our inequalities apply to both stationary and non-stationary
processes and exploit the potential rapid decay of correlations between
temporally separated variables to improve estimation. We demonstrate the
utility of these bounds by applying them to covariance operator estimation in
the Hilbert-Schmidt norm and to operator learning in dynamical systems,
achieving novel risk bounds. Finally, we perform numerical experiments to
illustrate the practical implications of these bounds in both contexts.

</details>


### [75] [Towards Benchmarking Foundation Models for Tabular Data With Text](https://arxiv.org/abs/2507.07829)
*Martin Mráz, Breenda Das, Anshul Gupta, Lennart Purucker, Frank Hutter*

**主要类别:** cs.LG

**AI概要:** This paper proposes strategies for incorporating text into conventional tabular pipelines and benchmarks state-of-the-art tabular foundation models on handling textual data.


<details>
  <summary>更多</summary>
  
**动机:** Foundation models for tabular data are rapidly evolving, with increasing interest in extending them to support additional modalities such as free-text features. However, existing benchmarks for tabular data rarely include textual columns, and identifying real-world tabular datasets with semantically rich text features is non-trivial.

**方法:** A series of simple yet effective ablation-style strategies for incorporating text into conventional tabular pipelines is proposed.

**结果:** State-of-the-art tabular foundation models can handle textual data by manually curating a collection of real-world tabular datasets with meaningful textual features.

**结论:** The study is an important step towards improving benchmarking of foundation models for tabular data with text.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+Benchmarking+Foundation+Models+for+Tabular+Data+With+Text，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07829，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07829&send_immediately=true&force_search=false)

**原文摘要:** Foundation models for tabular data are rapidly evolving, with increasing
interest in extending them to support additional modalities such as free-text
features. However, existing benchmarks for tabular data rarely include textual
columns, and identifying real-world tabular datasets with semantically rich
text features is non-trivial. We propose a series of simple yet effective
ablation-style strategies for incorporating text into conventional tabular
pipelines. Moreover, we benchmark how state-of-the-art tabular foundation
models can handle textual data by manually curating a collection of real-world
tabular datasets with meaningful textual features. Our study is an important
step towards improving benchmarking of foundation models for tabular data with
text.

</details>


### [76] ["So, Tell Me About Your Policy...": Distillation of interpretable policies from Deep Reinforcement Learning agents](https://arxiv.org/abs/2507.07848)
*Giovanni Dispoto, Paolo Bonetti, Marcello Restelli*

**主要类别:** cs.LG

**AI概要:** This paper proposes a novel algorithm with theoretical guarantees that extracts an interpretable policy without disregarding expert behavior peculiarities.


<details>
  <summary>更多</summary>
  
**动机:** The lack of interpretability in Deep Reinforcement Learning (DRL) techniques, especially in mission-critical and real-world settings.

**方法:** A novel algorithm supported by theoretical guarantees that extracts an interpretable policy using the advantage function and previously collected experience.

**结果:** The proposed algorithm demonstrates the ability to extract meaningful information from complex expert policies.

**结论:** The proposed algorithm can extract meaningful information from complex expert policies and is empirically evaluated on classic control environments and a financial trading scenario.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是%22So%2C+Tell+Me+About+Your+Policy...%22%3A+Distillation+of+interpretable+policies+from+Deep+Reinforcement+Learning+agents，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07848&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in Reinforcement Learning (RL) largely benefit from the
inclusion of Deep Neural Networks, boosting the number of novel approaches
proposed in the field of Deep Reinforcement Learning (DRL). These techniques
demonstrate the ability to tackle complex games such as Atari, Go, and other
real-world applications, including financial trading. Nevertheless, a
significant challenge emerges from the lack of interpretability, particularly
when attempting to comprehend the underlying patterns learned, the relative
importance of the state features, and how they are integrated to generate the
policy's output. For this reason, in mission-critical and real-world settings,
it is often preferred to deploy a simpler and more interpretable algorithm,
although at the cost of performance. In this paper, we propose a novel
algorithm, supported by theoretical guarantees, that can extract an
interpretable policy (e.g., a linear policy) without disregarding the
peculiarities of expert behavior. This result is obtained by considering the
advantage function, which includes information about why an action is superior
to the others. In contrast to previous works, our approach enables the training
of an interpretable policy using previously collected experience. The proposed
algorithm is empirically evaluated on classic control environments and on a
financial trading scenario, demonstrating its ability to extract meaningful
information from complex expert policies.

</details>


### [77] [Pre-Trained AI Model Assisted Online Decision-Making under Missing Covariates: A Theoretical Perspective](https://arxiv.org/abs/2507.07852)
*Haichen Hu, David Simchi-Levi*

**主要类别:** cs.LG

**AI概要:** This paper analyzes how the presence of a pre-trained AI model influences the regret of a sequential contextual decision-making process. A novel notion called 'model elasticity' is introduced to quantify the sensitivity of the reward function to the discrepancy between true and imputed covariates. Under the missing at random (MAR) setting, sequential calibration of the pre-trained model can significantly improve the quality of the imputed covariates.


<details>
  <summary>更多</summary>
  
**动机:** To study how the presence of a pre-trained AI model influences the regret of the decision-making process and to introduce a novel notion called 'model elasticity'.

**方法:** Theoretical analysis of how the presence of a pre-trained AI model influences the regret of the decision-making process. Introduction of 'model elasticity' concept and sequential calibration of the pre-trained model using tools from orthogonal statistical learning and doubly robust regression under MAR setting.

**结果:** Under the MAR setting, sequential calibration of the pre-trained model significantly improves the quality of the imputed covariates, leading to much better regret guarantees.

**结论:** Having an accurate pre-trained model is valuable in sequential decision-making tasks and model elasticity may serve as a fundamental metric for understanding and improving the integration of pre-trained models in data-driven decision-making problems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Pre-Trained+AI+Model+Assisted+Online+Decision-Making+under+Missing+Covariates%3A+A+Theoretical+Perspective，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07852，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07852&send_immediately=true&force_search=false)

**原文摘要:** We study a sequential contextual decision-making problem in which certain
covariates are missing but can be imputed using a pre-trained AI model. From a
theoretical perspective, we analyze how the presence of such a model influences
the regret of the decision-making process. We introduce a novel notion called
"model elasticity", which quantifies the sensitivity of the reward function to
the discrepancy between the true covariate and its imputed counterpart. This
concept provides a unified way to characterize the regret incurred due to model
imputation, regardless of the underlying missingness mechanism. More
surprisingly, we show that under the missing at random (MAR) setting, it is
possible to sequentially calibrate the pre-trained model using tools from
orthogonal statistical learning and doubly robust regression. This calibration
significantly improves the quality of the imputed covariates, leading to much
better regret guarantees. Our analysis highlights the practical value of having
an accurate pre-trained model in sequential decision-making tasks and suggests
that model elasticity may serve as a fundamental metric for understanding and
improving the integration of pre-trained models in a wide range of data-driven
decision-making problems.

</details>


### [78] [Credit Risk Analysis for SMEs Using Graph Neural Networks in Supply Chain](https://arxiv.org/abs/2507.07854)
*Zizhou Zhang, Qinyan Shen, Zhuohuan Hu, Qianying Liu, Huijie Shen*

**主要类别:** cs.LG

**AI概要:** This paper presents a GNN-based framework for predicting SME credit risk using transaction and social data, surpassing traditional methods and providing valuable insights for regulators and stress testers.


<details>
  <summary>更多</summary>
  
**动机:** SMEs are vital to the modern economy, yet their credit risk analysis often struggles with scarce data, especially for online lenders lacking direct credit records.

**方法:** This paper introduces a Graph Neural Network (GNN)-based framework, leveraging SME interactions from transaction and social data to map spatial dependencies and predict loan default risks.

**结果:** Tests on real-world datasets show the GNN surpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for supply chain mining and default prediction, respectively.

**结论:** The GNN-based framework provides a scalable and effective tool for assessing SME credit risk, surpassing traditional and other GNN baselines.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Credit+Risk+Analysis+for+SMEs+Using+Graph+Neural+Networks+in+Supply+Chain，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07854&send_immediately=true&force_search=false)

**原文摘要:** Small and Medium-sized Enterprises (SMEs) are vital to the modern economy,
yet their credit risk analysis often struggles with scarce data, especially for
online lenders lacking direct credit records. This paper introduces a Graph
Neural Network (GNN)-based framework, leveraging SME interactions from
transaction and social data to map spatial dependencies and predict loan
default risks. Tests on real-world datasets from Discover and Ant Credit (23.4M
nodes for supply chain analysis, 8.6M for default prediction) show the GNN
surpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for
supply chain mining and default prediction, respectively. It also helps
regulators model supply chain disruption impacts on banks, accurately
forecasting loan defaults from material shortages, and offers Federal Reserve
stress testers key data for CCAR risk buffers. This approach provides a
scalable, effective tool for assessing SME credit risk.

</details>


### [79] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou, Shujian Zhang, Brice Magdalou, John Lambert, Ehsan Amid, Richard Nock, Andrew Hard*

**主要类别:** cs.LG

**AI概要:** This paper shows DPO as a connection between loss functions and stochastic choice, highlighting its application and theoretical extensions.


<details>
  <summary>更多</summary>
  
**动机:** To show that DPO is a specific form of connection between two major theories in ML context of learning from preferences.

**方法:** The paper establishes a connection between Savage's losses and stochastic choice theory in the context of DPO.

**结果:** The connection includes support for abstention, non-convex objectives and allows for notable extensions of DPO setting.

**结论:** Understanding DPO from a general principled perspective is crucial for its application and development.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Principled+Foundations+for+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07855，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07855&send_immediately=true&force_search=false)

**原文摘要:** In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [80] [Predicting and generating antibiotics against future pathogens with ApexOracle](https://arxiv.org/abs/2507.07862)
*Tianang Leng, Fangping Wan, Marcelo Der Torossian Torres, Cesar de la Fuente-Nunez*

**主要类别:** cs.LG

**AI概要:** This paper introduces ApexOracle, an AI model that predicts the antibacterial potency of existing compounds and designs de novo molecules active against strains it has never encountered.


<details>
  <summary>更多</summary>
  
**动机:** Discovering antibiotics effective against emerging pathogens is becoming increasingly critical due to antimicrobial resistance (AMR) escalating and outpacing current antibiotic development.

**方法:** ApexOracle incorporates pathogen-specific context through the integration of molecular features captured via a foundational discrete diffusion language model and a dual-embedding framework that combines genomic- and literature-derived strain representations.

**结果:** Across diverse bacterial species and chemical modalities, ApexOracle consistently outperformed state-of-the-art approaches in activity prediction and demonstrated reliable transferability to novel pathogens with little or no antimicrobial data.

**结论:** ApexOracle offers a scalable strategy for countering AMR and preparing for future infectious-disease outbreaks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Predicting+and+generating+antibiotics+against+future+pathogens+with+ApexOracle，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07862，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07862&send_immediately=true&force_search=false)

**原文摘要:** Antimicrobial resistance (AMR) is escalating and outpacing current antibiotic
development. Thus, discovering antibiotics effective against emerging pathogens
is becoming increasingly critical. However, existing approaches cannot rapidly
identify effective molecules against novel pathogens or emerging drug-resistant
strains. Here, we introduce ApexOracle, an artificial intelligence (AI) model
that both predicts the antibacterial potency of existing compounds and designs
de novo molecules active against strains it has never encountered. Departing
from models that rely solely on molecular features, ApexOracle incorporates
pathogen-specific context through the integration of molecular features
captured via a foundational discrete diffusion language model and a
dual-embedding framework that combines genomic- and literature-derived strain
representations. Across diverse bacterial species and chemical modalities,
ApexOracle consistently outperformed state-of-the-art approaches in activity
prediction and demonstrated reliable transferability to novel pathogens with
little or no antimicrobial data. Its unified representation-generation
architecture further enables the in silico creation of "new-to-nature"
molecules with high predicted efficacy against priority threats. By pairing
rapid activity prediction with targeted molecular generation, ApexOracle offers
a scalable strategy for countering AMR and preparing for future
infectious-disease outbreaks.

</details>


### [81] [Can AI-predicted complexes teach machine learning to compute drug binding affinity?](https://arxiv.org/abs/2507.07882)
*Wei-Tse Hsu, Savva Grevtsev, Thomas Douglas, Aniket Magarkar, Philip C. Biggin*

**主要类别:** cs.LG

**AI概要:** This paper evaluates the use of co-folding models for synthetic data augmentation in training machine learning-based scoring functions for binding affinity prediction.


<details>
  <summary>更多</summary>
  
**动机:** To investigate the feasibility of using co-folding models for synthetic data augmentation in training MLSFs.

**方法:** Evaluation of co-folding models for synthetic data augmentation in training MLSFs for binding affinity prediction.

**结果:** Performance gains depend critically on the structural quality of augmented data. Simple heuristics were established for identifying high-quality co-folding predictions without reference structures.

**结论:** Co-folding models can be used for synthetic data augmentation in training MLSFs, but the structural quality of augmented data is critical.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+AI-predicted+complexes+teach+machine+learning+to+compute+drug+binding+affinity%3F，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07882，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07882&send_immediately=true&force_search=false)

**原文摘要:** We evaluate the feasibility of using co-folding models for synthetic data
augmentation in training machine learning-based scoring functions (MLSFs) for
binding affinity prediction. Our results show that performance gains depend
critically on the structural quality of augmented data. In light of this, we
established simple heuristics for identifying high-quality co-folding
predictions without reference structures, enabling them to substitute for
experimental structures in MLSF training. Our study informs future data
augmentation strategies based on co-folding models.

</details>


### [82] [SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization with Joint Global-Local Perturbation](https://arxiv.org/abs/2507.07883)
*Hao Ban, Gokul Ram Subramani, Kaiyi Ji*

**主要类别:** cs.LG

**AI概要:** This paper proposes SAMO, a lightweight sharpness-aware multi-task optimization approach that effectively mitigates task conflicts in multi-task learning.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to mitigate task conflicts in multi-task learning (MTL) optimization by integrating sharpness-aware minimization (SAM) into MTL.

**方法:** The proposed method, SAMO, leverages a joint global-local perturbation to address the challenges of combining average loss gradient and individual task gradients. Local perturbations are approximated using only forward passes and are layerwise normalized.

**结果:** Extensive experiments on multi-task benchmarks demonstrate both the effectiveness and efficiency of the proposed method.

**结论:** SAMO is an effective and efficient approach for multi-task learning optimization.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是SAMO%3A+A+Lightweight+Sharpness-Aware+Approach+for+Multi-Task+Optimization+with+Joint+Global-Local+Perturbation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07883，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07883&send_immediately=true&force_search=false)

**原文摘要:** Multi-task learning (MTL) enables a joint model to capture commonalities
across multiple tasks, reducing computation costs and improving data
efficiency. However, a major challenge in MTL optimization is task conflicts,
where the task gradients differ in direction or magnitude, limiting model
performance compared to single-task counterparts. Sharpness-aware minimization
(SAM) minimizes task loss while simultaneously reducing the sharpness of the
loss landscape. Our empirical observations show that SAM effectively mitigates
task conflicts in MTL. Motivated by these findings, we explore integrating SAM
into MTL but face two key challenges. While both the average loss gradient and
individual task gradients-referred to as global and local
information-contribute to SAM, how to combine them remains unclear. Moreover,
directly computing each task gradient introduces significant computational and
memory overheads. To address these challenges, we propose SAMO, a lightweight
\textbf{S}harpness-\textbf{A}ware \textbf{M}ulti-task \textbf{O}ptimization
approach, that leverages a joint global-local perturbation. The local
perturbations are approximated using only forward passes and are layerwise
normalized to improve efficiency. Extensive experiments on a suite of
multi-task benchmarks demonstrate both the effectiveness and efficiency of our
method. Code is available at https://github.com/OptMN-Lab/SAMO.

</details>


### [83] [Efficient Causal Discovery for Autoregressive Time Series](https://arxiv.org/abs/2507.07898)
*Mohammad Fesanghary, Achintya Gopal*

**主要类别:** cs.LG

**AI概要:** This study presents a novel constraint-based algorithm for causal structure learning specifically designed for nonlinear autoregressive time series.


<details>
  <summary>更多</summary>
  
**动机:** To reduce computational complexity compared to existing methods, making it more efficient and scalable to larger problems.

**方法:** A novel constraint-based algorithm for causal structure learning specifically designed for nonlinear autoregressive time series.

**结果:** Our algorithm not only outperforms current techniques, but also excels in scenarios with limited data availability.

**结论:** Our algorithm has the potential for practical applications in fields requiring efficient and accurate causal inference from nonlinear time series data.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient+Causal+Discovery+for+Autoregressive+Time+Series，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07898，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07898&send_immediately=true&force_search=false)

**原文摘要:** In this study, we present a novel constraint-based algorithm for causal
structure learning specifically designed for nonlinear autoregressive time
series. Our algorithm significantly reduces computational complexity compared
to existing methods, making it more efficient and scalable to larger problems.
We rigorously evaluate its performance on synthetic datasets, demonstrating
that our algorithm not only outperforms current techniques, but also excels in
scenarios with limited data availability. These results highlight its potential
for practical applications in fields requiring efficient and accurate causal
inference from nonlinear time series data.

</details>


### [84] [Plausible Counterfactual Explanations of Recommendations](https://arxiv.org/abs/2507.07919)
*Jakub Černý, Jiří Němeček, Ivan Dovica, Jakub Mareček*

**主要类别:** cs.LG

**AI概要:** 本文提出了一种在推荐系统中生成高度可信的反事实解释的方法，并通过数值评估和用户研究进行了评估。


<details>
  <summary>更多</summary>
  
**动机:** 解释在各种推荐系统中扮演着多种角色，从法律规定的后见之明，到用户体验的组成部分，再到说服力的关键。反事实解释是一种自然且有用的解释形式。

**方法:** 提出了一种在推荐系统中生成高度可信的反事实解释的方法。

**结果:** 该方法通过了数值评估和用户研究的验证。

**结论:** 推荐系统中的反事实解释可以提高用户的信任度和满意度。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Plausible+Counterfactual+Explanations+of+Recommendations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07919，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07919&send_immediately=true&force_search=false)

**原文摘要:** Explanations play a variety of roles in various recommender systems, from a
legally mandated afterthought, through an integral element of user experience,
to a key to persuasiveness. A natural and useful form of an explanation is the
Counterfactual Explanation (CE). We present a method for generating highly
plausible CEs in recommender systems and evaluate it both numerically and with
a user study.

</details>


### [85] [Dynamic Chunking for End-to-End Hierarchical Sequence Modeling](https://arxiv.org/abs/2507.07955)
*Sukjun Hwang, Brandon Wang, Albert Gu*

**主要类别:** cs.LG

**AI概要:** This paper addresses the issue of tokenization being a barrier for end-to-end language models by introducing H-Nets, which use a dynamic chunking mechanism to learn segmentation strategies jointly with the model, resulting in superior performance across different languages and data types.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to overcome the barrier that pre-processing steps such as tokenization present to true end-to-end foundation models in language modeling.

**方法:** The paper introduces new techniques for dynamic chunking and incorporates them into an explicit hierarchical network (H-Net) that operates at multiple levels of abstraction, replacing the traditional tokenization-LM-detokenization pipeline.

**结果:** H-Nets outperform strong Transformer language models when compute- and data-matched, show increased robustness and learn meaningful data-dependent chunking strategies. They also demonstrate better scaling with data and improved performance in languages and modalities with weaker tokenization heuristics.

**结论:** H-Nets offer a promising direction for developing more robust and scalable models that can learn directly from raw data, with the potential to improve performance in a variety of languages and modalities.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic+Chunking+for+End-to-End+Hierarchical+Sequence+Modeling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07955，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07955&send_immediately=true&force_search=false)

**原文摘要:** Despite incredible progress in language models (LMs) in recent years, largely
resulting from moving away from specialized models designed for specific tasks
to general models based on powerful architectures (e.g. the Transformer) that
learn everything from raw data, pre-processing steps such as tokenization
remain a barrier to true end-to-end foundation models. We introduce a
collection of new techniques that enable a dynamic chunking mechanism which
automatically learns content -- and context -- dependent segmentation
strategies learned jointly with the rest of the model. Incorporating this into
an explicit hierarchical network (H-Net) allows replacing the (implicitly
hierarchical) tokenization-LM-detokenization pipeline with a single model
learned fully end-to-end. When compute- and data- matched, an H-Net with one
stage of hierarchy operating at the byte level outperforms a strong Transformer
language model operating over BPE tokens. Iterating the hierarchy to multiple
stages further increases its performance by modeling multiple levels of
abstraction, demonstrating significantly better scaling with data and matching
a token-based Transformer of twice its size. H-Nets pretrained on English show
significantly increased character-level robustness, and qualitatively learn
meaningful data-dependent chunking strategies without any heuristics or
explicit supervision. Finally, the H-Net's improvement over tokenized pipelines
is further increased in languages and modalities with weaker tokenization
heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement
in data efficiency over baselines), showing the potential of true end-to-end
models that learn and scale better from unprocessed data.

</details>


### [86] [Prospective Learning in Retrospect](https://arxiv.org/abs/2507.07965)
*Yuxin Bai, Cecelia Shuai, Ashwin De Silva, Siyu Yu, Pratik Chaudhari, Joshua T. Vogelstein*

**主要类别:** cs.LG

**AI概要:** This paper extends the prospective learning framework to sequential decision-making scenarios, specifically foraging, to overcome the limitations of the PAC learning framework.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to overcome the limitations of the Probably Approximately Correct (PAC) learning framework which fails to account for dynamic data distributions and evolving objectives in most real-world applications of artificial intelligence.

**方法:** The paper builds on the recently introduced mathematical framework of prospective learning to improve the algorithm and numerical results.

**结果:** The paper presents preliminary results that improve the algorithm and numerical results of the prospective learning framework.

**结论:** The paper concludes that the prospective learning framework can be extended to sequential decision-making scenarios, specifically foraging.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Prospective+Learning+in+Retrospect，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07965，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07965&send_immediately=true&force_search=false)

**原文摘要:** In most real-world applications of artificial intelligence, the distributions
of the data and the goals of the learners tend to change over time. The
Probably Approximately Correct (PAC) learning framework, which underpins most
machine learning algorithms, fails to account for dynamic data distributions
and evolving objectives, often resulting in suboptimal performance. Prospective
learning is a recently introduced mathematical framework that overcomes some of
these limitations. We build on this framework to present preliminary results
that improve the algorithm and numerical results, and extend prospective
learning to sequential decision-making scenarios, specifically foraging. Code
is available at: https://github.com/neurodata/prolearn2.

</details>


### [87] [Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs](https://arxiv.org/abs/2507.07996)
*Ziyue Li, Yang Li, Tianyi Zhou*

**主要类别:** cs.LG

**AI概要:** This paper explores if a pretrained neural network can adjust its architecture for different inputs without finetuning. It finds that manipulating layers of a pretrained large language model can create a customized model for each test sample.


<details>
  <summary>更多</summary>
  
**动机:** Investigate whether a pretrained neural network can adapt its architecture to different inputs without finetuning and whether all layers are necessary for simple tasks or adequate for challenging tasks.

**方法:** Manipulating layers of pretrained large language models as separate modules to form a customized chain-of-layers (CoLa) per sample, using Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample.

**结果:** For >75% of samples with correct predictions by the original LLM, shorter CoLa can be found, and for >60% of samples with originally incorrect predictions, CoLa achieving correct predictions can be identified.

**结论:** The fixed architecture of pre-trained LLMs might not be optimal for different samples and test-time depth adaptation could enhance the model's generalization power.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Skip+a+Layer+or+Loop+it%3F+Test-Time+Depth+Adaptation+of+Pretrained+LLMs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07996，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07996&send_immediately=true&force_search=false)

**原文摘要:** Can a pretrained neural network adapt its architecture to different inputs
without any finetuning? Do we need all layers for simple tasks, and are they
adequate for challenging tasks? We found that the layers of a pretrained large
language model (LLM) can be manipulated as separate modules to build a better
and even shallower model customized for each test sample. In particular, each
layer from the pretrained model can be skipped/pruned or repeated multiple
times as recurrent neural networks (RNN), and stacked with others in arbitrary
orders, yielding a chain-of-layers (CoLa) per sample. This compositional space
greatly expands the scope of existing works on looped/recurrent pretrained
modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree
Search (MCTS) protocol to explore and identify the optimal CoLa for each sample
from math and commonsense reasoning benchmarks. Compared to a static model of a
fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same
layer(s) (slow thinking), and combining both, offering more flexible, dynamic
architectures for different inputs. We conduct an extensive analysis of the
MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples
with correct predictions by the original LLM, we can find shorter CoLa,
suggesting a large space for improving inference efficiency; (2) For >60% of
samples with originally incorrect predictions, we can identify CoLa achieving
correct predictions, suggesting a large space of performance enhancement. Our
results highlight the shortcomings of using a fixed architecture of pre-trained
LLMs for inference on different samples and pave the way to unlock the
generalization power of test-time depth adaptation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [88] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas, Mehmet Mercangoz*

**主要类别:** cs.AI

**AI概要:** 本论文提出了一种统一的代理框架，利用大型语言模型进行离散故障恢复规划和连续过程控制，展示了在不同案例研究中的优越表现，并分析了关键的失败模式。


<details>
  <summary>更多</summary>
  
**动机:** 现代化学工艺日益复杂，加之劳动力短缺和复杂的故障场景，需要一种新的自动化范式，将符号推理与自适应控制相结合。

**方法:** 研究采用有限状态机作为可解释的操作范围：一个由LLM驱动的规划代理通过FSM提出恢复序列，一个模拟代理执行并检查每个转换，一个验证-重提示循环迭代优化无效计划。

**结果:** 在案例研究1中，GPT-4o和GPT-4o-mini在180个随机生成的大小不一的FSM中，在五次重提示内达到100%的有效路径成功率，表现优于开源的LLM。在案例研究2中，同样的框架调节了双加热器输入，在持续不对称干扰下保持目标平均温度。与经典的PID控制相比，基于LLM的控制器达到了类似的性能。

**结论:** 研究表明，通过结构化反馈和模块化代理，大型语言模型可以统一高级符号规划和低级连续控制，为化工领域的弹性、语言驱动自动化铺平道路。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Autonomous+Control+Leveraging+LLMs%3A+An+Agentic+Framework+for+Next-Generation+Industrial+Automation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07115，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07115&send_immediately=true&force_search=false)

**原文摘要:** The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [89] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran, Shuang Chen, Jingjing Deng, Hubert P. H. Shum*

**主要类别:** cs.AI

**AI概要:** 本文解决了艺术作品分类中的AI偏见问题，重点在于因某些艺术风格主导导致数据集不平衡而引发的偏见问题。


<details>
  <summary>更多</summary>
  
**动机:** 尽管先前的研究已经在提高分类性能方面取得进展，但它却大体上忽视了解决这些潜在偏差的关键需求，即在处理离散分布数据时的偏差问题。我们的见解强调了在对有偏训练数据进行艺术分类的AI模型中减轻偏差的更稳健方法的必要性。

**方法:** 我们提出了一种新的OOD信息模型偏差自适应采样方法，称为BOOST（Bias-Oriented OOD Sampling and Tuning）。该方法通过动态调整温度缩放和采样概率来解决这些挑战，从而促进所有类别的更公平表示。

**结果:** 我们评估了我们提出的针对KaoKore和PACS数据集的方法，重点关注模型减少类别偏差的能力。我们进一步提出了一个新的指标，同数据集OOD检测得分（SODC），旨在评估类别分离和每个类别的偏差减少情况。我们的方法展示了平衡高性能与公平性的能力。

**结论:** 我们的方法在保持高性能的同时实现了公平性，为艺术领域的AI模型去偏提供了一种可靠的解决方案。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是BOOST%3A+Out-of-Distribution-Informed+Adaptive+Sampling+for+Bias+Mitigation+in+Stylistic+Convolutional+Neural+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07134，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07134&send_immediately=true&force_search=false)

**原文摘要:** The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [90] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim, Junsik Kim, Hwidong Bae, Woongcheol Yang, Sangdon Park, Sohee Bae*

**主要类别:** cs.AI

**AI概要:** This paper introduces SIBP to enable reliable trading in games, achieving high accuracy and laying groundwork for trustworthy NPC interactions.


<details>
  <summary>更多</summary>
  
**动机:** Large Language Models struggle with rule-governed trading systems, often committing rule violations that erode player trust, motivating the need for reliable trading mechanisms.

**方法:** The paper proposes State-Inference-Based Prompting (SIBP), decomposing trading into six states within a unified prompt framework and implementing context-aware item referencing and placeholder-based price calculations.

**结果:** Evaluation across 100 trading dialogues shows >97% state compliance, >95% referencing accuracy, and 99.7% calculation precision.

**结论:** SIBP provides a practical foundation for trustworthy NPC interactions in commercial games, maintaining computational efficiency while outperforming baselines.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是State-Inference-Based+Prompting+for+Natural+Language+Trading+with+Game+NPCs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07203，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07203&send_immediately=true&force_search=false)

**原文摘要:** Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [91] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang, Frank Montabon, Kristin Yvonne Rozier*

**主要类别:** cs.AI

**AI概要:** This paper explores the use of neurosymbolic methods and a question tree approach for identifying illicit activities in supply chains using news articles.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is the challenge of analyzing complex supply chain networks, especially when there are illicit activities involved such as counterfeit parts, forced labor, or human trafficking. Traditional machine learning techniques require large training data sets which are not available in the case of illicit supply chains.

**方法:** The study explores neurosymbolic methods for identifying instances of illicit activity in supply chains and compares the effectiveness of manual and automated feature extraction from news articles. A question tree approach for querying a large language model is proposed.

**结果:** The result of the study is the proposal of a question tree approach for querying a large language model to identify and quantify the relevance of articles related to forced labor in supply chains.

**结论:** The study concludes that a question tree approach for querying a large language model can systematically evaluate the differences between human and machine classification of news articles related to forced labor in supply chains.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Neurosymbolic+Feature+Extraction+for+Identifying+Forced+Labor+in+Supply+Chains，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07217&send_immediately=true&force_search=false)

**原文摘要:** Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [92] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu, Milind Sarkar, Anto I. Lonappan, Íñigo Zubeldia, Pablo Villanueva-Domingo, Santiago Casas, Christian Fidler, Chetana Amancharla, Ujjwal Tiwari, Adrian Bayer, Chadi Ait Ekiou, Miles Cranmer, Adrian Dimitrov, James Fergusson, Kahaan Gandhi, Sven Krippendorf, Andrew Laverick, Julien Lesgourgues, Antony Lewis, Thomas Meier, Blake Sherwin, Kristen Surrao, Francisco Villaescusa-Navarro, Chi Wang, Xueqing Xu, Boris Bolliet*

**主要类别:** cs.AI

**AI概要:** A multi-agent system, cmbagent, automates scientific research tasks with superior performance over state-of-the-art LLMs.


<details>
  <summary>更多</summary>
  
**动机:** Automation of scientific research tasks using a multi-agent system.

**方法:** The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point.

**结果:** Cmbagent successfully carried out a PhD level cosmology task and performed better than state-of-the-art LLMs on two benchmark sets.

**结论:** The cmbagent system shows superior performance over state-of-the-art LLMs in executing a PhD level cosmology task and two benchmark sets.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Open+Source+Planning+%26+Control+System+with+Language+Agents+for+Autonomous+Scientific+Discovery，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07257，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07257&send_immediately=true&force_search=false)

**原文摘要:** We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [93] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

**主要类别:** cs.AI

**AI概要:** This paper explores the use of large-language models for efficient exploration in multi-agent reinforcement learning.


<details>
  <summary>更多</summary>
  
**动机:** Efficient exploration is a known problem in deep reinforcement learning and this issue is magnified in multi-agent reinforcement learning. The idea of expert exploration could offer a solution to this problem.

**方法:** The study investigates the application of large-language models as expert planners for efficient exploration in planning based tasks for multiple agents.

**结果:** The application of large-language models as expert planners can enhance efficient exploration in multi-agent reinforcement learning environments.

**结论:** Large-language models can be applied as expert planners for efficient exploration in multi-agent reinforcement learning.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Application+of+LLMs+to+Multi-Robot+Path+Planning+and+Task+Allocation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07302，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07302&send_immediately=true&force_search=false)

**原文摘要:** Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [94] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu, Wei Dai, Jiaen Liu, Ching Wing Kwok, Zongheng Wu, Xudong Xiao, Ao Sun, Sheng Fu, Jianyuan Zhan, Yian Wang, Takatomo Saito, Sicheng Lai*

**主要类别:** cs.AI

**AI概要:** This paper introduces ViDove, a translation agent system designed for multimodal input that achieves significantly higher translation quality.


<details>
  <summary>更多</summary>
  
**动机:** LLM-based translation agents are typically limited to text-only inputs. ViDove is designed for multimodal input.

**方法:** ViDove leverages visual and contextual background information to enhance the translation process. Additionally, it integrates a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge.

**结果:** ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines.

**结论:** ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是ViDove%3A+A+Translation+Agent+System+with+Multimodal+Context+and+Memory-Augmented+Reasoning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07306，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07306&send_immediately=true&force_search=false)

**原文摘要:** LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [95] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball, Greg Gluch, Shafi Goldwasser, Frauke Kreuter, Omer Reingold, Guy N. Rothblum*

**主要类别:** cs.AI

**AI概要:** This paper investigates the challenges of aligning large language models (LLMs) with safety measures, showing computational difficulties in filtering harmful input prompts and generated outputs.


<details>
  <summary>更多</summary>
  
**动机:** With the increased deployment of large language models (LLMs), there is a concern about their potential misuse for generating harmful content. The study addresses the alignment challenge, aiming to ensure safety in the use of LLMs.

**方法:** The study focuses on filters to prevent the generation of unsafe information, specifically the filtering of the input prompt before it reaches the model and filtering the output after generation. The research demonstrates computational challenges in filtering both prompts and outputs under cryptographic hardness assumptions.

**结果:** There exist LLMs for which there are no efficient prompt filters as adversarial prompts that elicit harmful behavior can be easily constructed. Output filtering is computationally intractable in a natural setting. The study also identifies further computational barriers in formalized and relaxed mitigation approaches.

**结论:** Safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); black-box access to the LLM will not suffice. An aligned AI system's intelligence cannot be separated from its judgment.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+the+Impossibility+of+Separating+Intelligence+from+Judgment%3A+The+Computational+Intractability+of+Filtering+for+AI+Alignment，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07341，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07341&send_immediately=true&force_search=false)

**原文摘要:** With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [96] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai, Haoyu Wang, Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haifeng Chen, Yanjie Fu*

**主要类别:** cs.AI

**AI概要:** Proposed Sim-to-Dec framework, combining generative simulation and history-future dual-aware decision model, significantly improves timely delivery rates and profit in supply chain transportation.


<details>
  <summary>更多</summary>
  
**动机:** High responsiveness and economic efficiency are critical objectives in supply chain transportation, both of which are influenced by strategic decisions on shipping mode. An integrated framework combining an efficient simulator with an intelligent decision-making algorithm can provide an observable, low-risk environment for transportation strategy design.

**方法:** Sim-to-Dec consists of a generative simulation module, which leverages autoregressive modeling to simulate continuous state changes, reducing dependence on handcrafted domain-specific rules and enhancing robustness against data fluctuations; and a history-future dual-aware decision model, refined iteratively through end-to-end optimization with simulator interactions.

**结果:** Extensive experiments conducted on three real-world datasets demonstrate that Sim-to-Dec significantly improves timely delivery rates and profit.

**结论:** The Sim-to-Dec framework significantly improves timely delivery rates and profit.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Supply+Chain+Optimization+via+Generative+Simulation+and+Iterative+Decision+Policies，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07355&send_immediately=true&force_search=false)

**原文摘要:** High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [97] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang, Yuwei Wan, Yinqiao Li, Yudai Matsuda, Tong Xie, Linqi Song*

**主要类别:** cs.AI

**AI概要:** 本文提出了一种名为DrugMCTS的新框架，解决了当前大型语言模型在药物发现领域的局限性，并通过实验验证了其优越性能。


<details>
  <summary>更多</summary>
  
**动机:** 尽管大型语言模型在科学领域如药物发现方面展现了巨大的潜力，但在超出预训练期间获取的知识进行推理时，其效果仍然受限。传统方法如微调或检索增强生成，在施加高计算开销或未能充分利用结构化科学数据方面存在局限性。

**方法:** 提出了一种新的框架DrugMCTS，该框架综合了RAG、多代理协作和蒙特卡洛树搜索来重新定位药物。框架采用五个专门的代理负责检索和分析分子及蛋白质信息，实现了结构化和迭代推理。

**结果:** 大量实验表明，DrugMCTS在DrugBank和KIBA数据集上的召回率和鲁棒性显著高于通用的大规模语言模型和深度学习基线。

**结论:** DrugMCTS框架通过结构化推理、基于代理的协作和反馈驱动的搜索机制，为药物发现领域的LLM应用提供了重要进步。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是DrugMCTS%3A+a+drug+repurposing+framework+combining+multi-agent%2C+RAG+and+Monte+Carlo+Tree+Search，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07426&send_immediately=true&force_search=false)

**原文摘要:** Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [98] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan, Changjiu Jiang, Yu Duan, Mingcong Lei, Jiageng Li, Yitian Hong, Xinrun Wang, Bo An*

**主要类别:** cs.AI

**AI概要:** Introducing StarDojo, a new benchmark for evaluating AI agents in complex production-living environments, highlighting significant limitations in current models.


<details>
  <summary>更多</summary>
  
**动机:** Autonomous agents navigating human society must master both production activities and social interactions, yet existing benchmarks rarely evaluate these skills simultaneously.

**方法:** Introducing StarDojo, a novel benchmark based on Stardew Valley, designed to assess AI agents in open-ended production-living simulations.

**结果:** Extensive evaluations of state-of-the-art MLLMs agents demonstrate substantial limitations, with the best-performing model, GPT-4.1, achieving only a 12.7% success rate.

**结论:** StarDojo aims to facilitate further research towards robust, open-ended agents in complex production-living environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是StarDojo%3A+Benchmarking+Open-Ended+Behaviors+of+Agentic+Multimodal+LLMs+in+Production-Living+Simulations+with+Stardew+Valley，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07445，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07445&send_immediately=true&force_search=false)

**原文摘要:** Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [99] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle, Thomas McGee, Hamza Giaffar, Taylor Webb, Ida Momennejad*

**主要类别:** cs.AI

**AI概要:** This position paper proposes AlgEval, a framework for systematic research into the algorithms that LLMs learn and use, aiming to uncover algorithmic primitives and their algorithmic composition to solve task-specific problems.


<details>
  <summary>更多</summary>
  
**动机:** Studies addressing the question of what algorithms LLMs actually learn and use to solve problems are sparse, as research priorities are focused on improving performance through scale, leaving a theoretical and empirical gap in understanding emergent algorithms.

**方法:** The paper proposes AlgEval, a framework for systematic research into the algorithms that LLMs learn and use.

**结果:** AlgEval aims to uncover algorithmic primitives, reflected in latent representations, attention, and inference-time compute, and their algorithmic composition to solve task-specific problems.

**结论:** The rigorous, systematic evaluation of how LLMs actually solve tasks provides an alternative to resource-intensive scaling, reorienting the field toward a principled understanding of underlying computations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Position%3A+We+Need+An+Algorithmic+Understanding+of+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07544，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07544&send_immediately=true&force_search=false)

**原文摘要:** What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [100] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala, Jordi Planes, Joao Marques-Silva*

**主要类别:** cs.AI

**AI概要:** This paper analyzes the relationship between explanations and undesired facets of rule-based ML models, and concludes that popular tools for learning these models may lead to sets exhibiting negative facets.


<details>
  <summary>更多</summary>
  
**动机:** Incorrect explanations can mislead human decision makers, therefore rigor of explanations is paramount in high-risk uses of ML and data mining.

**方法:** The paper develops algorithms for the analysis of these undesired facets of rule-based systems.

**结果:** The paper relates explanations with well-known undesired facets of rule-based ML models, which include negative overlap and several forms of redundancy.

**结论:** Well-known and widely used tools for learning rule-based ML models will induce rule sets that exhibit one or more negative facets.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是On+Trustworthy+Rule-Based+Models+and+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07576，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07576&send_immediately=true&force_search=false)

**原文摘要:** A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [101] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su, Di Wang, Chunyan Miao*

**主要类别:** cs.AI

**AI概要:** This paper introduces Context Pooling, a novel method to enhance GNN-based models for link prediction in KGs. It applies graph pooling and enables the generation of query-specific graphs for inductive settings. The method achieves SOTA performance in most settings.


<details>
  <summary>更多</summary>
  
**动机:** Recent investigations show that vanilla aggregation does not significantly impact the performance of GNN-based models for link prediction in KGs. Thus, there is a need to introduce a methodology to enhance the efficacy of these models.

**方法:** The paper introduces a novel method named Context Pooling which applies graph pooling in KGs and enables the generation of query-specific graphs for inductive settings. Two metrics, neighborhood precision and neighborhood recall, are devised to assess the neighbors' logical relevance regarding given queries.

**结果:** The method is generic and assessed on two SOTA models and three public transductive and inductive datasets, achieving SOTA performance in 42 out of 48 settings.

**结论:** Context Pooling enhances the performance of GNN-based models for link prediction in KGs and achieves SOTA performance in most settings.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Context+Pooling%3A+Query-specific+Graph+Pooling+for+Generic+Inductive+Link+Prediction+in+Knowledge+Graphs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07595，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07595&send_immediately=true&force_search=false)

**原文摘要:** Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [102] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi, Jim Black, Christopher Palmer, Muhammad Javed, Hazel Clothier, Jim Buttery, Gerardo Luis Dimaguila*

**主要类别:** cs.AI

**AI概要:** This study evaluates fine-tuned Llama 3.2 models for extracting vaccine info, showing potential for automated data extraction and efficient vaccine safety surveillance.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to support near real-time vaccine safety surveillance by extracting vaccine-related information from emergency department triage notes.

**方法:** The study evaluates fine-tuned Llama 3.2 models using prompt engineering to create a labeled dataset confirmed by human annotators, comparing performance with prompt-engineered models, fine-tuned models, and a rule-based approach.

**结果:** The fine-tuned Llama 3 billion parameter model outperformed other models in accuracy of extracting vaccine names and model quantization enabled deployment in resource-constrained environments.

**结论:** The study concludes that fine-tuned Llama models show great potential in automating the extraction of vaccine-related information, supporting efficient vaccine safety surveillance and early detection of adverse events.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Enhancing+Vaccine+Safety+Surveillance%3A+Extracting+Vaccine+Mentions+from+Emergency+Department+Triage+Notes+Using+Fine-Tuned+Large+Language+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07599，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07599&send_immediately=true&force_search=false)

**原文摘要:** This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [103] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli, Thomas Krak, Cassio de Campos*

**主要类别:** cs.AI

**AI概要:** This paper presents a new method for belief inference in a type of credal network using Dempster-Shafer theory, highlighting its efficiency and robustness while also discussing its pros and cons.


<details>
  <summary>更多</summary>
  
**动机:** The motivation for this paper is to explore belief inference in credal networks, specifically addressing the propagation of uncertainty through a subclass known as chains.

**方法:** The paper proposes a novel framework for propagating uncertainty through chains, a subclass of credal networks, using belief and plausibility functions based on Dempster-Shafer theory.

**结果:** Numerical results demonstrate the practical utility of belief inference within this framework, offering insights into its advantages and limitations for chains and potentially for general credal networks.

**结论:** The paper concludes that the proposed belief-based inference method for chains in credal networks offers a computationally efficient and robust approach to uncertainty representation, with identified advantages and limitations.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards+conservative+inference+in+credal+networks+using+belief+functions%3A+the+case+of+credal+chains，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07619，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07619&send_immediately=true&force_search=false)

**原文摘要:** This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [104] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov, Abdelrahman Eldesokey, Michael Birsak, John Femiani, Bernard Ghanem, Peter Wonka*

**主要类别:** cs.AI

**AI概要:** This paper introduces PlanQA, a benchmark for assessing geometric and spatial reasoning in large language models using structured representations of indoor scenes.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to address the lack of geometric and spatial reasoning capabilities in large-language models when dealing with real-world layouts.

**方法:** The study uses PlanQA, a diagnostic benchmark, to evaluate the geometric and spatial reasoning of various open-source and commercial large-language models based on structured indoor scene representations encoded symbolically.

**结果:** Results indicate that while models may handle shallow queries well, they often fail in simulating physical constraints, maintaining spatial coherence, or generalizing when layouts are perturbed.

**结论:** PlanQA exposes a significant blind spot in current large-language models regarding reasoning about real-world layouts and encourages new research to improve this aspect.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是PlanQA%3A+A+Benchmark+for+Spatial+Reasoning+in+LLMs+using+Structured+Representations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07644，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07644&send_immediately=true&force_search=false)

**原文摘要:** We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [105] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian, Kai Yang, Ye Ouyang, Xiaozhou Ye*

**主要类别:** cs.AI

**AI概要:** This paper presents a comprehensive analysis of DPO's dynamics revealing its limitations. To overcome these, authors propose a bilevel optimization framework integrating supervised fine-tuning with an enhanced DPO objective, achieving better performance and alignment with intended preferences.


<details>
  <summary>更多</summary>
  
**动机:** DPO is highly sensitive to initialization and tends to misallocate probability mass, which can inadvertently shift probability toward irrelevant or undesired responses. This misallocation may unintentionally reinforce model bias, thereby compromising both the stability of model alignment and the consistency with intended preferences.

**方法:** A theoretically grounded bilevel optimization framework that tightly integrate supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference optimization.

**结果:** Experiments on challenging reasoning and summarization benchmarks elucidate that our method consistently improves reasoning accuracy and better aligns output distributions with intended preferences, outperforming standard DPO.

**结论:** Stable preference optimization provides new insights into the design of preference-based alignment objectives and opens up new avenues towards more reliable and interpretable language model alignment.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable+Preference+Optimization+for+LLMs%3A+A+Bilevel+Approach+Beyond+Direct+Preference+Optimization，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07723，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07723&send_immediately=true&force_search=false)

**原文摘要:** Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [106] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin, Anne-Emmanuelle Ceulemans, François Glineur*

**主要类别:** cs.AI

**AI概要:** 本研究提出了一种基于轮廓线对小提琴进行分类的方法，以区分是否经过尺寸缩减。


<details>
  <summary>更多</summary>
  
**动机:** 小提琴的尺寸缩减影响其多个特性，尤其是轮廓线。虽然专家们观察到了这样的差异，但尚未对其进行定量研究。

**方法:** 研究基于轮廓线对小提琴进行分类的方法。通过对25个乐器的3D几何网格进行分析，提取每个乐器的10-20条轮廓线，并用类似抛物线的曲线拟合每条线。计算额外的特征参数并处理异常值和不同数量的级别，最终得到每个乐器的数值轮廓。然后应用分类方法来评估仅通过几何学是否可以预测尺寸缩减。

**结果:** 使用几何学特征区分改造过和未改造的小提琴在某种程度上是可行的。整个小提琴存在一个变化的连续谱，对于它们来说量化缩减程度较为困难。发现开口参数beta是最具预测性的特征。

**结论:** 几何学特征，特别是开口参数beta，可以在一定程度上预测小提琴是否被改造过。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Identification+of+Violin+Reduction+via+Contour+Lines+Classification，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07743，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07743&send_immediately=true&force_search=false)

**原文摘要:** The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [107] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard, Akshaya Jagadeesh, Alex Cook, Steele Billings, Nicholas Skytland, Alicia Llewellyn, Jackson Paull, Nathan Paull, Nolan Kurylo, Keatra Nesbitt, Robert Gruenewald, Anthony Jantzi, Omar Chavez*

**主要类别:** cs.AI

**AI概要:** This paper introduces the Flourishing AI Benchmark, which measures how well AI models contribute to human flourishing across seven dimensions.


<details>
  <summary>更多</summary>
  
**动机:** To introduce the Flourishing AI Benchmark (FAI Benchmark), an evaluation framework assessing AI alignment with human flourishing across seven specific dimensions.

**方法:** The benchmark evaluates LLM AI systems alignment with holistic human well-being through a comprehensive methodology incorporating 1,229 objective and subjective questions. Specialized judge LLMs and cross-dimensional evaluation are used, employing geometric mean scoring.

**结果:** Initial testing of 28 leading language models shows that some models approach holistic alignment (highest-scoring models achieve 72/100), but none are acceptably aligned across all dimensions, particularly lacking in Faith and Spirituality, Character and Virtue, and Meaning and Purpose.

**结论:** The research establishes a framework for developing AI systems that actively support human flourishing across multiple dimensions, rather than merely avoiding harm.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Measuring+AI+Alignment+with+Human+Flourishing，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07787，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07787&send_immediately=true&force_search=false)

**原文摘要:** This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [108] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu, Jiaqian Yu, Xiongfeng Peng, Yiwei Chen, Weiming Li, Jaewook Yoo, Sunghyun Chunag, Dongwook Lee, Daehyun Ji, Chao Zhang*

**主要类别:** cs.AI

**AI概要:** This paper proposes MoSE, a skill-oriented Mixture-of-Experts model for autonomous driving systems.


<details>
  <summary>更多</summary>
  
**动机:** The motivation is to develop an end-to-end autonomous driving system that has better generalization and interpretation by using large language models (LLMs) and vision language models (VLMs) trained with web-scale data.

**方法:** The authors propose a skill-oriented MoE model called MoSE, which mimics the human drivers' learning and reasoning process. They also propose a skill-oriented routing mechanism and build a hierarchical skill dataset to pretrain the router.

**结果:** MoSE integrates valuable auxiliary tasks in one single forward process without introducing extra computational cost. It outperforms several larger models and achieves state-of-the-art performance with significantly reduced activated model size.

**结论:** The MoSE model, with less than 3B sparsely activated parameters, outperforms several models with 8B+ parameters on the CODA AD corner case reasoning task and achieves state-of-the-art performance with a significantly reduced activated model size.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是MoSE%3A+Skill-by-Skill+Mixture-of-Expert+Learning+for+Autonomous+Driving，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07818，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07818&send_immediately=true&force_search=false)

**原文摘要:** Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [109] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek, Keondo Park, Jeonggil Ko, Min-hwan Oh, Taesik Gong, Hyung-Sin Kim*

**主要类别:** cs.AI

**AI概要:** This paper advocates for adaptive sensing as a necessary shift in AI systems design, demonstrating its potential to improve efficiency and performance while addressing sustainability and equitable access concerns.


<details>
  <summary>更多</summary>
  
**动机:** The current trend of scaling neural models and expanding training datasets incurs significant environmental, economic, and ethical costs. Adaptive sensing, inspired by biological sensory systems, offers a more efficient alternative.

**方法:** The paper outlines a roadmap for integrating adaptive sensing into real-world applications, assesses technical and ethical challenges, and proposes research directions such as standardized benchmarks, real-time algorithms, multimodal integration, and privacy-preserving methods.

**结果:** Empirical evidence shows that adaptive sensing enables small models to surpass substantially larger models trained with significantly more data and compute.

**结论:** Adaptive sensing is a promising new approach that can significantly improve the efficiency and performance of AI systems, paving the way for sustainable, robust, and equitable artificial intelligence systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AI+Should+Sense+Better%2C+Not+Just+Scale+Bigger%3A+Adaptive+Sensing+as+a+Paradigm+Shift，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07820，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07820&send_immediately=true&force_search=false)

**原文摘要:** Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [110] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd, Ada Diaconescu, Jean-Louis Dessalles*

**主要类别:** cs.AI

**AI概要:** 本文探讨了因果关系在机器学习模型中的应用，并提出了一种算法来识别非布尔、黑盒和随机系统的实际原因。


<details>
  <summary>更多</summary>
  
**动机:** 最近关于可解释人工智能（XAI）的文献受到了批评。传统的XAI和因果关系文献侧重于理解哪些因素导致哪些结果，这对于研究人员和工程师来说是有价值的，但这不是非专家用户所期望的解释。非专家用户通常期待的是目标结果的实际原因，即实际原因。形式化这个概念仍然是一个开放的问题。此外，确定实际原因是NP完全问题，并且几乎没有实用的解决方案来近似形式定义。

**方法:** 作者提出了一组算法来识别具有多项式复杂度的实际原因，并且可以根据需要调整精度和详尽程度。

**结果:** 实验表明，这些算法可以识别不同类别的系统的实际原因，包括现有方法无法处理的非布尔、黑盒和随机系统，并且可以通过更多的计算时间来提高精度和详尽性。

**结论:** 作者提出了一组算法，可以在多项式复杂度内识别实际原因，并可调整精度和详尽程度。实验表明这些算法可以处理现有方法无法处理的不同类别的系统，并且可以通过更多的计算时间来提高精度和详尽性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Searching+for+actual+causes%3A+Approximate+algorithms+with+adjustable+precision，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07857&send_immediately=true&force_search=false)

**原文摘要:** Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [111] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang, Na Zhao, Jianglong Qing, Qing xu, Kaiwen Pan, Ting luo*

**主要类别:** cs.AI

**AI概要:** This paper addresses the challenges faced by large language models in legal dispute analysis and proposes an enhanced framework that integrates prompt engineering with multidimensional knowledge graphs.


<details>
  <summary>更多</summary>
  
**动机:** Large language models face significant limitations in legal dispute analysis, including insufficient legal knowledge representation, limited concept understanding, and reasoning deficiencies.

**方法:** This research proposes an enhanced framework integrating prompt engineering with multidimensional knowledge graphs.

**结果:** Experimental results demonstrate significant performance improvements in legal dispute analysis, enabling accurate legal application analysis for complex cases while exhibiting nuanced understanding of judicial decision-making logic.

**结论:** The proposed framework provides a novel technical approach for implementing intelligent legal assistance systems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是An+Integrated+Framework+of+Prompt+Engineering+and+Multidimensional+Knowledge+Graphs+for+Legal+Dispute+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07893，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07893&send_immediately=true&force_search=false)

**原文摘要:** The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [112] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach, Jayson Lynch, Neil Thompson*

**主要类别:** cs.AI

**AI概要:** This paper argues that the diminishing returns to compute scaling will lead to a convergence of AI model capabilities, which requires reexamination of AI strategy and policy.


<details>
  <summary>更多</summary>
  
**动机:** Contrary to prevailing intuition, the diminishing returns to compute scaling will lead to a convergence of AI model capabilities.

**方法:** Developing a model illustrating that under a fixed-distribution next-token objective, the marginal capability returns to raw compute shrink substantially and analyzing empirical data on the capability difference of AI models over time.

**结果:** Proxies like training loss differences capture important capability measures, and the increasing ability of meek models requires reexamination of AI strategy and policy.

**结论:** The diminishing returns to compute scaling will lead to a convergence of AI model capabilities, and AI strategy and policy require reexamination.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Meek+Models+Shall+Inherit+the+Earth，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07931，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07931&send_immediately=true&force_search=false)

**原文摘要:** The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [113] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson, Sonia Jaffe, Will Wang, Scott Counts, Siddharth Suri*

**主要类别:** cs.AI

**AI概要:** This paper analyzes how people use AI in their work and calculates an AI applicability score for each occupation, finding that AI has the highest applicability in knowledge work and occupations that involve providing and communicating information.


<details>
  <summary>更多</summary>
  
**动机:** To understand the effects of AI on the economy by analyzing how people use AI in their work activities and how successful and broadly those activities are done.

**方法:** The study analyzed a dataset of 200k anonymized and privacy-scrubbed conversations between users and Microsoft Bing Copilot, classifying the activities people use AI for, and calculating an AI applicability score for each occupation.

**结果:** The most common work activities people seek AI assistance for involve gathering information and writing. The most common activities that AI itself is performing are providing information and assistance, writing, teaching, and advising.

**结论:** AI has the highest applicability in knowledge work and occupations that involve providing and communicating information. The success of AI application is also related to wage and education levels.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Working+with+AI%3A+Measuring+the+Occupational+Implications+of+Generative+AI，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07935，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07935&send_immediately=true&force_search=false)

**原文摘要:** Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [114] [WatchWitch: Interoperability, Privacy, and Autonomy for the Apple Watch](https://arxiv.org/abs/2507.07210)
*Nils Rollshausen, Alexander Heinrich, Matthias Hollick, Jiska Classen*

**主要类别:** cs.CR

**AI概要:** 逆向工程苹果手表的无线协议，发现多个安全问题，并提供增强隐私控制和数据自主性的安卓重新实现——WatchWitch。


<details>
  <summary>更多</summary>
  
**动机:** 智能手表收集大量个人健康和健身数据，但用户对这些数据的处理方式没有选择权。

**方法:** 通过逆向工程苹果手表的无线协议，发现了苹果专有实现中的多个安全问题；开发了自定义的安卓重新实现——WatchWitch。

**结果:** 实现了与苹果产品的实际互操作性，提供了增强的隐私控制和数据自主性。

**结论:** 为智能手表生态系统中提供更多消费者选择，并让用户更好地掌控自己的设备。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是WatchWitch%3A+Interoperability%2C+Privacy%2C+and+Autonomy+for+the+Apple+Watch，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07210，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07210&send_immediately=true&force_search=false)

**原文摘要:** Smartwatches such as the Apple Watch collect vast amounts of intimate health
and fitness data as we wear them. Users have little choice regarding how this
data is processed: The Apple Watch can only be used with Apple's iPhones, using
their software and their cloud services. We are the first to publicly
reverse-engineer the watch's wireless protocols, which led to discovering
multiple security issues in Apple's proprietary implementation. With
WatchWitch, our custom Android reimplementation, we break out of Apple's walled
garden -- demonstrating practical interoperability with enhanced privacy
controls and data autonomy. We thus pave the way for more consumer choice in
the smartwatch ecosystem, offering users more control over their devices.

</details>


### [115] [Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis](https://arxiv.org/abs/2507.07244)
*Faissal Ahmadou, Sepehr Ghaffarzadegan, Boubakr Nour, Makan Pourzandi, Mourad Debbabi, Chadi Assi*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新方案，利用语言模型和自然语言处理技术，从威胁报告中自动提取攻击测试流，以节省时间、减少错误，并确保网络安全测试的全面覆盖和健壮性。


<details>
  <summary>更多</summary>
  
**动机:** 在不断演变的网络安全形势下，快速识别和缓解高级持续威胁（APTs）至关重要。安全从业者依赖详细的威胁报告来了解攻击者使用的战术、技术和程序（TTPs）。然而，从这些报告中手动提取攻击测试流需要难以捉摸的知识，并且耗时且容易出错。

**方法:** 该论文提出了一种名为FLOWGUARDIAN的新解决方案，利用语言模型（即BERT）和自然语言处理（NLP）技术，从非结构化的威胁报告中自动提取攻击测试流。FLOWGUARDIAN系统地分析和上下文化安全事件，重建攻击序列，然后生成全面的测试流。

**结果:** 这种自动化方法不仅节省时间并减少人为错误，还确保了网络安全测试中的全面覆盖和健壮性。

**结论:** FLOWGUARDIAN通过实证验证展示了其在准确性和效率方面的优势，大大增强了安全团队在主动威胁搜寻和事件响应方面的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Automated+Attack+Testflow+Extraction+from+Cyber+Threat+Report+using+BERT+for+Contextual+Analysis，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07244，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07244&send_immediately=true&force_search=false)

**原文摘要:** In the ever-evolving landscape of cybersecurity, the rapid identification and
mitigation of Advanced Persistent Threats (APTs) is crucial. Security
practitioners rely on detailed threat reports to understand the tactics,
techniques, and procedures (TTPs) employed by attackers. However, manually
extracting attack testflows from these reports requires elusive knowledge and
is time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a
novel solution leveraging language models (i.e., BERT) and Natural Language
Processing (NLP) techniques to automate the extraction of attack testflows from
unstructured threat reports. FLOWGUARDIAN systematically analyzes and
contextualizes security events, reconstructs attack sequences, and then
generates comprehensive testflows. This automated approach not only saves time
and reduces human error but also ensures comprehensive coverage and robustness
in cybersecurity testing. Empirical validation using public threat reports
demonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing
the capabilities of security teams in proactive threat hunting and incident
response.

</details>


### [116] [Disa: Accurate Learning-based Static Disassembly with Attentions](https://arxiv.org/abs/2507.07246)
*Peicheng Wang, Monika Santra, Mingyu Liu, Cong Sun, Dongrui Zeng, Gang Tan*

**主要类别:** cs.CR

**AI概要:** This paper proposes Disa, a new learning-based disassembly approach that improves the accuracy and efficiency of disassembly.


<details>
  <summary>更多</summary>
  
**动机:** For reverse engineering related security domains, such as vulnerability detection, malware analysis, and binary hardening, disassembly is crucial yet challenging.

**方法:** Disa, a new learning-based disassembly approach that uses the information of superset instructions over the multi-head self-attention to learn the instructions' correlations, thus being able to infer function entry-points and instruction boundaries.

**结果:** Disa outperforms prior deep-learning disassembly approaches in function entry-point identification, especially achieving 9.1% and 13.2% F1-score improvement on binaries respectively obfuscated by the disassembly desynchronization technique and popular source-level obfuscator.

**结论:** Disa generates more accurate CFGs with a 4.4% reduction in Average Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based approach.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Disa%3A+Accurate+Learning-based+Static+Disassembly+with+Attentions，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07246，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07246&send_immediately=true&force_search=false)

**原文摘要:** For reverse engineering related security domains, such as vulnerability
detection, malware analysis, and binary hardening, disassembly is crucial yet
challenging. The fundamental challenge of disassembly is to identify
instruction and function boundaries. Classic approaches rely on file-format
assumptions and architecture-specific heuristics to guess the boundaries,
resulting in incomplete and incorrect disassembly, especially when the binary
is obfuscated. Recent advancements of disassembly have demonstrated that deep
learning can improve both the accuracy and efficiency of disassembly. In this
paper, we propose Disa, a new learning-based disassembly approach that uses the
information of superset instructions over the multi-head self-attention to
learn the instructions' correlations, thus being able to infer function
entry-points and instruction boundaries. Disa can further identify instructions
relevant to memory block boundaries to facilitate an advanced block-memory
model based value-set analysis for an accurate control flow graph (CFG)
generation. Our experiments show that Disa outperforms prior deep-learning
disassembly approaches in function entry-point identification, especially
achieving 9.1% and 13.2% F1-score improvement on binaries respectively
obfuscated by the disassembly desynchronization technique and popular
source-level obfuscator. By achieving an 18.5% improvement in the memory block
precision, Disa generates more accurate CFGs with a 4.4% reduction in Average
Indirect Call Targets (AICT) compared with the state-of-the-art heuristic-based
approach.

</details>


### [117] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz, David Megías*

**主要类别:** cs.CR

**AI概要:** A semi-fragile watermarking scheme for multiple band images using a tree-structured vector quantization approach.


<details>
  <summary>更多</summary>
  
**动机:** To present a semi-fragile watermarking scheme for multiple band images that can detect any significant modification of the original image.

**方法:** A tree-structured vector quantization approach is applied to the pixel signatures of remote sensing images. The image is segmented into three-dimensional blocks, and a tree-structured vector quantizer is built for each block.

**结果:** The method is able to preserve the mark under lossy compression (above a given threshold) and detects possibly forged blocks and their position in the whole image.

**结论:** The method can preserve the mark under lossy compression and detect possibly forged blocks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Semi-fragile+watermarking+of+remote+sensing+images+using+DWT%2C+vector+quantization+and+automatic+tiling，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07250，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07250&send_immediately=true&force_search=false)

**原文摘要:** A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [118] [FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning](https://arxiv.org/abs/2507.07258)
*Rami Darwish, Mahmoud Abdelsalam, Sajad Khorsandroo, Kaushik Roy*

**主要类别:** cs.CR

**AI概要:** 本文提出了一种新的联邦学习框架FedP3E，通过交换扰动后的类别原型而非原始数据或梯度，解决了现有FL算法在处理类别不平衡和非独立同分布数据时的不足，尤其在恶意软件检测领域表现突出。


<details>
  <summary>更多</summary>
  
**动机:** 随着物联网生态系统在关键领域的不断扩展，它们已成为越来越复杂和大规模恶意软件攻击的显着目标。为了应对敏感的物联网生成数据和数据异质性，需要既能保护隐私又能适应数据多样性的检测框架。

**方法:** 本文提出了一种新的FL框架FedP3E（Privacy-Preserving Prototype Exchange），它支持间接跨客户端表示共享，同时保持数据隐私。每个客户端使用高斯混合模型（GMMs）构建类别的原型，用高斯噪声扰动，并仅将这些紧凑的摘要传输到服务器。聚合后的原型再分发回客户端，并整合到本地训练中，通过基于SMOTE的增强来加强少数恶意软件类别的表示。

**结果:** 实验结果表明，与传统的FL算法（如FedAvg和FedProx）相比，FedP3E在非独立同分布（non-IID）和类别不平衡的真实部署环境下表现更佳。特别是对于稀有或不相交的恶意软件类别，FedP3E能够显著提高检测性能。

**结论:** FedP3E在N-BaIoT数据集上的评估显示，在不同程度的数据不平衡情况下，该框架能够有效减轻统计异质性的负面影响，并以最小的通信开销增强局部模型的能力。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是FedP3E%3A+Privacy-Preserving+Prototype+Exchange+for+Non-IID+IoT+Malware+Detection+in+Cross-Silo+Federated+Learning，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07258，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07258&send_immediately=true&force_search=false)

**原文摘要:** As IoT ecosystems continue to expand across critical sectors, they have
become prominent targets for increasingly sophisticated and large-scale malware
attacks. The evolving threat landscape, combined with the sensitive nature of
IoT-generated data, demands detection frameworks that are both
privacy-preserving and resilient to data heterogeneity. Federated Learning (FL)
offers a promising solution by enabling decentralized model training without
exposing raw data. However, standard FL algorithms such as FedAvg and FedProx
often fall short in real-world deployments characterized by class imbalance and
non-IID data distributions -- particularly in the presence of rare or disjoint
malware classes. To address these challenges, we propose FedP3E
(Privacy-Preserving Prototype Exchange), a novel FL framework that supports
indirect cross-client representation sharing while maintaining data privacy.
Each client constructs class-wise prototypes using Gaussian Mixture Models
(GMMs), perturbs them with Gaussian noise, and transmits only these compact
summaries to the server. The aggregated prototypes are then distributed back to
clients and integrated into local training, supported by SMOTE-based
augmentation to enhance representation of minority malware classes. Rather than
relying solely on parameter averaging, our prototype-driven mechanism enables
clients to enrich their local models with complementary structural patterns
observed across the federation -- without exchanging raw data or gradients.
This targeted strategy reduces the adverse impact of statistical heterogeneity
with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset
under realistic cross-silo scenarios with varying degrees of data imbalance.

</details>


### [119] [Shuffling for Semantic Secrecy](https://arxiv.org/abs/2507.07401)
*Fupei Chen, Liyao Xiang, Haoxiang Sun, Hei Victor Cheng, Kaiming Shen*

**主要类别:** cs.CR

**AI概要:** This paper examines the security aspect of deep learning in semantic communications from a shuffling perspective. The aim is to improve secure coding schemes for a better tradeoff between transmission and leakage rates.


<details>
  <summary>更多</summary>
  
**动机:** To improve upon the conventional secure coding scheme and strike a desirable tradeoff between transmission rate and leakage rate from a novel shuffling perspective.

**方法:** A novel semantic security communication system is devised wherein the random shuffling pattern plays the role of the shared secret key.

**结果:** Simulations demonstrate the significant advantage of the proposed method over the benchmark in boosting secure transmission, especially when channels are prone to strong noise and unpredictable fading.

**结论:** The proposed random shuffling method provides a significant advantage in boosting secure transmission, particularly when channels are prone to strong noise and unpredictable fading.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Shuffling+for+Semantic+Secrecy，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07401，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07401&send_immediately=true&force_search=false)

**原文摘要:** Deep learning draws heavily on the latest progress in semantic
communications. The present paper aims to examine the security aspect of this
cutting-edge technique from a novel shuffling perspective. Our goal is to
improve upon the conventional secure coding scheme to strike a desirable
tradeoff between transmission rate and leakage rate. To be more specific, for a
wiretap channel, we seek to maximize the transmission rate while minimizing the
semantic error probability under the given leakage rate constraint. Toward this
end, we devise a novel semantic security communication system wherein the
random shuffling pattern plays the role of the shared secret key. Intuitively,
the permutation of feature sequences via shuffling would distort the semantic
essence of the target data to a sufficient extent so that eavesdroppers cannot
access it anymore. The proposed random shuffling method also exhibits its
flexibility in working for the existing semantic communication system as a
plugin. Simulations demonstrate the significant advantage of the proposed
method over the benchmark in boosting secure transmission, especially when
channels are prone to strong noise and unpredictable fading.

</details>


### [120] [Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models](https://arxiv.org/abs/2507.07406)
*Jikesh Thapa, Gurrehmat Chahal, Serban Voinea Gabreanu, Yazan Otoum*

**主要类别:** cs.CR

**AI概要:** This paper evaluates ML, DL, and LLM-based methods for phishing detection, showing that while LLMs underperform in raw accuracy, they have potential for identifying subtle phishing cues and can be cost-efficient with good adversarial robustness.


<details>
  <summary>更多</summary>
  
**动机:** The need for detection systems that strike a balance between high accuracy and computational efficiency due to increasingly sophisticated phishing attacks.

**方法:** Comparative evaluation of traditional Machine Learning (ML), Deep Learning (DL), and quantized small-parameter Large Language Models (LLMs) for phishing detection.

**结果:** Models like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above 80%, using only 17GB of VRAM. Lightweight LLMs can provide concise, interpretable explanations to support real-time decision-making.

**结论:** Optimized LLMs are promising components in phishing defence systems and offer a path forward for integrating explainable, efficient AI into modern cybersecurity frameworks.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Phishing+Detection+in+the+Gen-AI+Era%3A+Quantized+LLMs+vs+Classical+Models，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07406&send_immediately=true&force_search=false)

**原文摘要:** Phishing attacks are becoming increasingly sophisticated, underscoring the
need for detection systems that strike a balance between high accuracy and
computational efficiency. This paper presents a comparative evaluation of
traditional Machine Learning (ML), Deep Learning (DL), and quantized
small-parameter Large Language Models (LLMs) for phishing detection. Through
experiments on a curated dataset, we show that while LLMs currently
underperform compared to ML and DL methods in terms of raw accuracy, they
exhibit strong potential for identifying subtle, context-based phishing cues.
We also investigate the impact of zero-shot and few-shot prompting strategies,
revealing that LLM-rephrased emails can significantly degrade the performance
of both ML and LLM-based detectors. Our benchmarking highlights that models
like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above
80%, using only 17GB of VRAM, supporting their viability for cost-efficient
deployment. We further assess the models' adversarial robustness and
cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide
concise, interpretable explanations to support real-time decision-making. These
findings position optimized LLMs as promising components in phishing defence
systems and offer a path forward for integrating explainable, efficient AI into
modern cybersecurity frameworks.

</details>


### [121] [Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks](https://arxiv.org/abs/2507.07413)
*Mohammad F. Al-Hammouri, Yazan Otoum, Rasha Atwa, Amiya Nayak*

**主要类别:** cs.CR

**AI概要:** This paper proposes a novel hybrid intrusion detection system that combines traditional signature-based methods with GPT-2's semantic analysis to better detect cyber threats, showing promising improvements in accuracy and responsiveness.


<details>
  <summary>更多</summary>
  
**动机:** The increasing sophistication of cyber threats in distributed and resource-constrained environments necessitates dynamic and adaptive intrusion detection systems.

**方法:** A hybrid IDS framework combining signature-based techniques with GPT-2-driven semantic analysis was developed and tested on a representative intrusion dataset.

**结果:** The proposed model improved detection accuracy by 6.3%, reduced false positives by 9.0%, and maintained near real-time responsiveness.

**结论:** The integration of language models like GPT-2 with traditional signature-based methods can significantly enhance intrusion detection systems, making them more adaptive and accurate for modern environments.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid+LLM-Enhanced+Intrusion+Detection+for+Zero-Day+Threats+in+IoT+Networks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07413，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07413&send_immediately=true&force_search=false)

**原文摘要:** This paper presents a novel approach to intrusion detection by integrating
traditional signature-based methods with the contextual understanding
capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become
increasingly sophisticated, particularly in distributed, heterogeneous, and
resource-constrained environments such as those enabled by the Internet of
Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems
(IDSs) becomes increasingly urgent. While traditional methods remain effective
for detecting known threats, they often fail to recognize new and evolving
attack patterns. In contrast, GPT-2 excels at processing unstructured data and
identifying complex semantic relationships, making it well-suited to uncovering
subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges
the robustness of signature-based techniques with the adaptability of
GPT-2-driven semantic analysis. Experimental evaluations on a representative
intrusion dataset demonstrate that our model enhances detection accuracy by
6.3%, reduces false positives by 9.0%, and maintains near real-time
responsiveness. These results affirm the potential of language model
integration to build intelligent, scalable, and resilient cybersecurity
defences suited for modern connected environments.

</details>


### [122] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj, Brindha Raghuraman, Nagarani Gopalakrishnan, Yazan Otoum*

**主要类别:** cs.CR

**AI概要:** This paper explores the growing cyber threats to critical infrastructure systems and proposes an AI-driven cybersecurity framework for better protection.


<details>
  <summary>更多</summary>
  
**动机:** With the rising interconnectivity of critical infrastructure systems, they become more exposed to various cyber threats, calling for an effective solution to mitigate these risks.

**方法:** The paper proposes a hybrid AI-driven cybersecurity framework for real-time vulnerability detection, threat modelling, and automated remediation.

**结果:** The research highlights the threat landscape, attack vectors, and presents a novel AI-driven framework that addresses adversarial AI, regulatory compliance, and integration challenges.

**结论:** The study provides a comprehensive approach to tackle cybersecurity vulnerabilities in critical infrastructure systems, emphasizing the role of AI and offering actionable insights.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Autonomous+AI-based+Cybersecurity+Framework+for+Critical+Infrastructure%3A+Real-Time+Threat+Mitigation，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07416，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07416&send_immediately=true&force_search=false)

**原文摘要:** Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [123] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya, Andrey Labunets, Sicun Gao, Earlence Fernandes*

**主要类别:** cs.CR

**AI概要:** 本文评估了针对大型语言模型提示注入攻击的一类流行防御措施的鲁棒性，并发现这些措施并不如其所声称的那样安全。


<details>
  <summary>更多</summary>
  
**动机:** 评估基于微调模型分离指令和数据的防御措施在白盒环境下的鲁棒性。

**方法:** 通过构建基于优化的强大攻击方法来评估此类防御措施的鲁棒性，并提出一种新的基于注意力机制的攻击算法。

**结果:** 对两个最近的白盒防御系统SecAlign和StruQ进行攻击实验，成功率高达70%，攻击者预算（以token计）适度增加。

**结论:** 这类防御措施并不具备所声称的安全特性，需要进一步研究以提高其鲁棒性。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是May+I+have+your+Attention%3F+Breaking+Fine-Tuning+based+Prompt+Injection+Defenses+using+Architecture-Aware+Attacks，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07417，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07417&send_immediately=true&force_search=false)

**原文摘要:** A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


### [124] [RADAR: a Radio-based Analytics for Dynamic Association and Recognition of pseudonyms in VANETs](https://arxiv.org/abs/2507.07732)
*Giovanni Gambigliani Zoccoli, Filip Valgimigli, Dario Stabili, Mirco Marchetti*

**主要类别:** cs.CR

**AI概要:** This paper presents RADAR, a tracking algorithm that exploits multiple radio signals emitted by a modern vehicle to break privacy-preserving pseudonym schemes deployed in VANETs.


<details>
  <summary>更多</summary>
  
**动机:** The motivation behind this paper is to improve tracking over standard de-anonymization approaches in realistic scenarios where the attacker does not have full coverage of the entire vehicle path.

**方法:** The study uses an experimental evaluation comparing three different metrics for pseudonym and Wi-Fi probe identifier association: Count, Statistical RSSI, and Pearson RSSI.

**结果:** The result shows that the Pearson RSSI metric is better at tracking vehicles under pseudonym-changing schemes in all scenarios and against previous works.

**结论:** The paper concludes that the RADAR algorithm is effective in tracking vehicles in C-ITS by exploiting multiple radio signals and breaks privacy-preserving pseudonym schemes in VANETs.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是RADAR%3A+a+Radio-based+Analytics+for+Dynamic+Association+and+Recognition+of+pseudonyms+in+VANETs，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07732，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07732&send_immediately=true&force_search=false)

**原文摘要:** This paper presents RADAR, a tracking algorithm for vehicles participating in
Cooperative Intelligent Transportation Systems (C-ITS) that exploits multiple
radio signals emitted by a modern vehicle to break privacy-preserving pseudonym
schemes deployed in VANETs. This study shows that by combining Dedicated Short
Range Communication (DSRC) and Wi-Fi probe request messages broadcast by the
vehicle, it is possible to improve tracking over standard de-anonymization
approaches that only leverage DSRC, especially in realistic scenarios where the
attacker does not have full coverage of the entire vehicle path. The
experimental evaluation compares three different metrics for pseudonym and
Wi-Fi probe identifier association (Count, Statistical RSSI, and Pearson RSSI),
demonstrating that the Pearson RSSI metric is better at tracking vehicles under
pseudonym-changing schemes in all scenarios and against previous works. As an
additional contribution to the state-of-the-art, we publicly release all
implementations and simulation scenarios used in this work.

</details>


### [125] [Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors](https://arxiv.org/abs/2507.07773)
*Youqian Zhang, Xinyu Ji, Zhihao Wang, Qinhong Jiang*

**主要类别:** cs.CR

**AI概要:** 我们研究了一种新的针对图像传感器的电磁信号注入攻击，这可以导致捕获的图像中出现类似彩虹的颜色伪影，并影响物体检测模型的准确性。


<details>
  <summary>更多</summary>
  
**动机:** 这些系统依赖于视觉数据的完整性来进行决策。在这项工作中，我们调查了一类新的电磁信号注入攻击，这类攻击针对图像传感器的模拟域，允许对手操纵原始视觉输入而不触发传统的数字完整性检查。

**方法:** 我们揭示了一种之前未记录的针对CMOS图像传感器的攻击现象：通过精心调谐的电磁干扰在图像传感器捕获的图像中诱导出类似彩虹的颜色伪影。我们进一步评估了这些攻击对最先进的物体检测模型的影响，显示注入的伪影通过图像信号处理管道传播并导致显著的误判。

**结果:** 攻击的影响是显著的，注入的伪影通过图像信号处理管道传播并导致显著的误判。

**结论:** 我们的研究结果突显了视觉感知堆栈中的一个关键且未充分探索的漏洞，强调了在这些系统中需要更强大的物理层攻击防御措施。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Rainbow+Artifacts+from+Electromagnetic+Signal+Injection+Attacks+on+Image+Sensors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07773，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07773&send_immediately=true&force_search=false)

**原文摘要:** Image sensors are integral to a wide range of safety- and security-critical
systems, including surveillance infrastructure, autonomous vehicles, and
industrial automation. These systems rely on the integrity of visual data to
make decisions. In this work, we investigate a novel class of electromagnetic
signal injection attacks that target the analog domain of image sensors,
allowing adversaries to manipulate raw visual inputs without triggering
conventional digital integrity checks. We uncover a previously undocumented
attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced
in images captured by image sensors through carefully tuned electromagnetic
interference. We further evaluate the impact of these attacks on
state-of-the-art object detection models, showing that the injected artifacts
propagate through the image signal processing pipeline and lead to significant
mispredictions. Our findings highlight a critical and underexplored
vulnerability in the visual perception stack, highlighting the need for more
robust defenses against physical-layer attacks in such systems.

</details>


### [126] [Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking](https://arxiv.org/abs/2507.07871)
*Toluwani Aremu, Noor Hussein, Munachiso Nwadike, Samuele Poppi, Jie Zhang, Karthik Nandakumar, Neil Gong, Nils Lukas*

**主要类别:** cs.CR

**AI概要:** 该论文研究了如何通过多密钥扩展来减轻对GenAI提供者生成内容的水印盗窃攻击，同时提供了理论保证并实证展示了其在多个数据集上的有效性。


<details>
  <summary>更多</summary>
  
**动机:** 水印为GenAI提供者提供了一种建立其生成内容来源的有希望的解决方案。然而，水印盗窃攻击是GenAI提供者面临的威胁。

**方法:** 研究提出了一种多密钥扩展方法，以减轻盗窃攻击，并将此方法应用于任何模态的任何水印方法后验。

**结果:** 研究提供了理论保证，并通过实验证明了该方法在多个数据集上使伪造效果显著降低。

**结论:** 该研究正式定义了水印伪造的威胁，即生成有害的、带水印的内容，并通过安全游戏对该威胁进行了建模。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Mitigating+Watermark+Stealing+Attacks+in+Generative+Models+via+Multi-Key+Watermarking，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07871，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07871&send_immediately=true&force_search=false)

**原文摘要:** Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.

</details>


### [127] [The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web](https://arxiv.org/abs/2507.07901)
*Sree Bhargavi Balija, Rekha Singal, Abhishek Singh, Ramesh Raskar, Erfan Darzi, Raghu Bala, Thomas Hardjono, Ken Huang*

**主要类别:** cs.CR

**AI概要:** This paper presents the Nanda Unified Architecture, a decentralized framework designed to address the urgent demands for interoperability, trust, and economic coordination in fragmented AI agent ecosystems.


<details>
  <summary>更多</summary>
  
**动机:** The fragmentation of AI agent ecosystems has created urgent demands for interoperability, trust, and economic coordination that current protocols cannot address at scale.

**方法:** The Nanda Unified Architecture is built around three core innovations: fast DID-based agent discovery through distributed registries, semantic agent cards with verifiable credentials and composability profiles, and a dynamic trust layer that integrates behavioral attestations with policy compliance. The system introduces X42/H42 micropayments for economic coordination and MAESTRO, a security framework incorporating Synergetics' patented AgentTalk protocol and secure containerization.

**结果:** Real-world deployments demonstrate 99.9 percent compliance in healthcare applications and substantial monthly transaction volumes with strong privacy guarantees.

**结论:** The Nanda Unified Architecture enables a globally interoperable Internet of Agents where trust becomes the native currency of collaboration across both enterprise and Web3 ecosystems.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是The+Trust+Fabric%3A+Decentralized+Interoperability+and+Economic+Coordination+for+the+Agentic+Web，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07901，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07901&send_immediately=true&force_search=false)

**原文摘要:** The fragmentation of AI agent ecosystems has created urgent demands for
interoperability, trust, and economic coordination that current protocols --
including MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,
2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present
the Nanda Unified Architecture, a decentralized framework built around three
core innovations: fast DID-based agent discovery through distributed
registries, semantic agent cards with verifiable credentials and composability
profiles, and a dynamic trust layer that integrates behavioral attestations
with policy compliance. The system introduces X42/H42 micropayments for
economic coordination and MAESTRO, a security framework incorporating
Synergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure
containerization. Real-world deployments demonstrate 99.9 percent compliance in
healthcare applications and substantial monthly transaction volumes with strong
privacy guarantees. By unifying MIT's trust research with production
deployments from Cisco and Synergetics, we show how cryptographic proofs and
policy-as-code transform agents into trust-anchored participants in a
decentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a
globally interoperable Internet of Agents where trust becomes the native
currency of collaboration across both enterprise and Web3 ecosystems.

</details>


### [128] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau, Giuseppe Desolda, Francesco Greco, Lucio Davide Spano, Luca Viganò*

**主要类别:** cs.CR

**AI概要:** This paper explores the use of Large Language Models (LLMs) to generate explanations for phishing warnings, finding that LLM-generated explanations can be as effective as manually crafted ones.


<details>
  <summary>更多</summary>
  
**动机:** Phishing has become a prominent risk in modern cybersecurity, and warning dialogues have limitations that affect their effectiveness. This research aims to assess the capacity of Large Language Models (LLMs) to generate clear, concise, and scalable explanations for phishing warnings.

**方法:** A large-scale between-subjects user study (N = 750) was carried out to compare the influence of warning dialogues supplemented with manually generated explanations against those generated by two LLMs, Claude 3.5 Sonnet and Llama 3.3 70B.

**结果:** Well-constructed LLM-generated explanations can equal or surpass manually crafted explanations in reducing susceptibility to phishing; Claude-generated warnings exhibited particularly robust performance. Feature-based explanations were more effective for genuine phishing attempts, whereas counterfactual explanations diminished false-positive rates.

**结论:** LLMs can be used to automatically build explanations for warning users against phishing, and such solutions are scalable, adaptive, and consistent with human-centred values.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Can+Large+Language+Models+Improve+Phishing+Defense%3F+A+Large-Scale+Controlled+Experiment+on+Warning+Dialogue+Explanations，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07916，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07916&send_immediately=true&force_search=false)

**原文摘要:** Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


### [129] [KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps](https://arxiv.org/abs/2507.07927)
*Jenny Blessing, Ross J. Anderson, Alastair R. Beresford*

**主要类别:** cs.CR

**AI概要:** 本研究调查了Android设备中硬件支持的密钥存储使用情况，发现尽管有行业倡议鼓励采用，但大部分应用并未使用受信任硬件来保护用户数据。同时，性能测试表明，最先进的安全硬件在某些加密操作上表现不佳。


<details>
  <summary>更多</summary>
  
**动机:** 研究Android设备中硬件支持的密钥存储的使用情况，以及其在保护用户敏感数据方面的效果。

**方法:** 分析了490,119个Android应用程序，收集了开发者如何使用受信任硬件的数据，并与每个应用程序收集的敏感用户数据进行了对比。还对移动设备中受信任硬件的性能进行了实证分析。

**结果:** 发现56.3%自称为处理敏感用户数据的应用程序根本不使用Android的受信任硬件功能，仅有5.03%的应用程序使用最强大的安全元件。此外，安全元件在性能上对非对称加密和大负载对称加密不切实际。

**结论:** 尽管硬件支持的密钥存储具有安全优势，但其性能限制和采用率低仍然是一个问题。需要进一步改进硬件性能和推广使用。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是KeyDroid%3A+A+Large-Scale+Analysis+of+Secure+Key+Storage+in+Android+Apps，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07927，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07927&send_immediately=true&force_search=false)

**原文摘要:** Most contemporary mobile devices offer hardware-backed storage for
cryptographic keys, user data, and other sensitive credentials. Such hardware
protects credentials from extraction by an adversary who has compromised the
main operating system, such as a malicious third-party app. Since 2011, Android
app developers can access trusted hardware via the Android Keystore API. In
this work, we conduct the first comprehensive survey of hardware-backed key
storage in Android devices. We analyze 490 119 Android apps, collecting data on
how trusted hardware is used by app developers (if used at all) and
cross-referencing our findings with sensitive user data collected by each app,
as self-reported by developers via the Play Store's data safety labels.
  We find that despite industry-wide initiatives to encourage adoption, 56.3%
of apps self-reporting as processing sensitive user data do not use Android's
trusted hardware capabilities at all, while just 5.03% of apps collecting some
form of sensitive data use the strongest form of trusted hardware, a secure
element distinct from the main processor. To better understand the potential
downsides of using secure hardware, we conduct the first empirical analysis of
trusted hardware performance in mobile devices, measuring the runtime of common
cryptographic operations across both software- and hardware-backed keystores.
We find that while hardware-backed key storage using a coprocessor is viable
for most common cryptographic operations, secure elements capable of preventing
more advanced attacks make performance infeasible for symmetric encryption with
non-negligible payloads and any kind of asymmetric encryption.

</details>


### [130] [EinHops: Einsum Notation for Expressive Homomorphic Operations on RNS-CKKS Tensors](https://arxiv.org/abs/2507.07972)
*Karthik Garimella, Austin Ebel, Brandon Reagen*

**主要类别:** cs.CR

**AI概要:** 本论文旨在解决全同态加密（FHE）中执行多维张量运算的问题。通过采用爱因斯坦求和约定（einsum）符号，提出了一种最小化的系统EinHops，该系统可以将einsum表达式分解为固定序列的FHE操作，从而实现加密张量操作，同时保持对底层打包策略的完全可见性。


<details>
  <summary>更多</summary>
  
**动机:** 全同态加密（FHE）方案允许直接在加密数据上进行计算，有效地闭合了安全和外包计算的循环，但其指令集有限，使得执行多维张量运算具有挑战性。实践者必须将这些张量打包成一维向量，并将张量操作映射到这种一维布局上，而非传统的嵌套结构。先前的系统虽然在自动化此过程方面取得了显著进展，但通常隐藏了关键的打包决策，导致难以进行调试、优化和构建于这些系统之上。

**方法:** 研究中采用爱因斯坦求和约定（einsum）符号处理FHE中的多维张量运算。einsum符号在其语法中明确编码了维度结构和操作，自然揭示了应该如何打包和转换张量。将einsum表达式分解为一组固定的FHE友好操作，并实现设计呈现EinHops系统。

**结果:** 提出了一个最小化的系统EinHops，该系统将einsum表达式分解为固定序列的FHE操作，从而实现了加密张量操作，同时保持对底层打包策略的完全可见性。EinHops在一系列张量操作上进行了评估，从简单的转置到复杂的多维收缩，结果表明，einsum符号的显式性质使我们能够构建一个简单、通用且可解释的FHE张量系统。

**结论:** EinHops是一个简单、通用且可解释的FHE张量系统，它通过保持底层打包策略的完全可见性，使开发者能够使用FHE执行加密张量操作。

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EinHops%3A+Einsum+Notation+for+Expressive+Homomorphic+Operations+on+RNS-CKKS+Tensors，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07972&send_immediately=true&force_search=false)

**原文摘要:** Fully Homomorphic Encryption (FHE) is an encryption scheme that allows for
computation to be performed directly on encrypted data, effectively closing the
loop on secure and outsourced computing. Data is encrypted not only during rest
and transit, but also during processing. However, FHE provides a limited
instruction set: SIMD addition, SIMD multiplication, and cyclic rotation of 1-D
vectors. This restriction makes performing multi-dimensional tensor operations
challenging. Practitioners must pack these tensors into 1-D vectors and map
tensor operations onto this one-dimensional layout rather than their
traditional nested structure. And while prior systems have made significant
strides in automating this process, they often hide critical packing decisions
behind layers of abstraction, making debugging, optimizing, and building on top
of these systems difficult.
  In this work, we approach multi-dimensional tensor operations in FHE through
Einstein summation (einsum) notation. Einsum notation explicitly encodes
dimensional structure and operations in its syntax, naturally exposing how
tensors should be packed and transformed. We decompose einsum expressions into
a fixed set of FHE-friendly operations. We implement our design and present
EinHops, a minimalist system that factors einsum expressions into a fixed
sequence of FHE operations. EinHops enables developers to perform encrypted
tensor operations using FHE while maintaining full visibility into the
underlying packing strategy. We evaluate EinHops on a range of tensor
operations from a simple transpose to complex multi-dimensional contractions.
We show that the explicit nature of einsum notation allows us to build an FHE
tensor system that is simple, general, and interpretable. We open-source
EinHops at the following repository: https://github.com/baahl-nyu/einhops.

</details>


### [131] [Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974)
*Sizhe Chen, Yizhu Wang, Nicholas Carlini, Chawin Sitawarin, David Wagner*

**主要类别:** cs.CR

**AI概要:** The paper proposes DefensiveToken, a test-time defense method providing prompt injection robustness in large language model systems.


<details>
  <summary>更多</summary>
  
**动机:** Prompt injection attacks pose a significant threat to LLM systems interacting with external data. Test-time defenses like defensive prompting have been proposed but they are less effective than training-time defenses that change the model parameters.

**方法:** DefensiveToken, a test-time defense with prompt injection robustness comparable to training-time alternatives. DefensiveTokens are newly inserted as special tokens, whose embeddings are optimized for security.

**结果:** In security-sensitive cases, system developers can append a few DefensiveTokens before the LLM input to achieve security with a minimal utility drop. In scenarios where security is less of a concern, developers can simply skip DefensiveTokens.

**结论:** DefensiveTokens provide a flexible switch between SOTA utility and almost-SOTA security at test time.

**与AI讨论:** [Discuss with Kimi](https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Defending+Against+Prompt+Injection+With+a+Few+DefensiveTokens，链接是https%3A%2F%2Farxiv.org%2Fabs%2F2507.07974，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2507.07974&send_immediately=true&force_search=false)

**原文摘要:** When large language model (LLM) systems interact with external data to
perform complex tasks, a new attack, namely prompt injection, becomes a
significant threat. By injecting instructions into the data accessed by the
system, the attacker is able to override the initial user task with an
arbitrary task directed by the attacker. To secure the system, test-time
defenses, e.g., defensive prompting, have been proposed for system developers
to attain security only when needed in a flexible manner. However, they are
much less effective than training-time defenses that change the model
parameters. Motivated by this, we propose DefensiveToken, a test-time defense
with prompt injection robustness comparable to training-time alternatives.
DefensiveTokens are newly inserted as special tokens, whose embeddings are
optimized for security. In security-sensitive cases, system developers can
append a few DefensiveTokens before the LLM input to achieve security with a
minimal utility drop. In scenarios where security is less of a concern,
developers can simply skip DefensiveTokens; the LLM system remains the same as
there is no defense, generating high-quality responses. Thus, DefensiveTokens,
if released alongside the model, allow a flexible switch between the
state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code
is available at https://github.com/Sizhe-Chen/DefensiveToken.

</details>
