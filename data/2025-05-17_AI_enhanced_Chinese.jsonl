{"id": "2505.09649", "pdf": "https://arxiv.org/pdf/2505.09649", "abs": "https://arxiv.org/abs/2505.09649", "authors": ["Abisha Thapa Magar", "Anup Shakya"], "title": "Next Word Suggestion using Graph Neural Network", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language Modeling is a prevalent task in Natural Language Processing. The\ncurrently existing most recent and most successful language models often tend\nto build a massive model with billions of parameters, feed in a tremendous\namount of text data, and train with enormous computation resources which\nrequire millions of dollars. In this project, we aim to address an important\nsub-task in language modeling, i.e., context embedding. We propose an approach\nto exploit the Graph Convolution operation in GNNs to encode the context and\nuse it in coalition with LSTMs to predict the next word given a local context\nof preceding words. We test this on the custom Wikipedia text corpus using a\nvery limited amount of resources and show that this approach works fairly well\nto predict the next word.", "AI": {"tldr": "This project focuses on an important sub-task in language modeling, context embedding. It proposes an approach using Graph Convolution operation in GNNs with LSTMs to predict the next word given a local context of preceding words. Experiments on a custom Wikipedia text corpus demonstrate its effectiveness with limited resources.", "motivation": "The motivation is to address an important sub-task in language modeling, context embedding, by proposing a more cost-effective method without building massive models or requiring immense computation resources.", "method": "The method involves using Graph Convolution operation in GNNs to encode the context and combining it with LSTMs to predict the next word based on a local context of preceding words.", "result": "The approach works fairly well in predicting the next word when tested on a custom Wikipedia text corpus.", "conclusion": "This project demonstrates the potential of using Graph Convolution operations in GNNs combined with LSTMs for effective context embedding in language modeling, even with limited resources."}}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655", "abs": "https://arxiv.org/abs/2505.09655", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.", "AI": {"tldr": "This paper proposes a Diversity-aware Reward Adjustment (DRA) method that enhances the exploration capability of reinforcement learning models for language model post-training, especially in low-resource settings. DRA improves the performance on mathematical reasoning tasks by adjusting rewards based on semantic diversity.", "motivation": "To overcome the limitation of existing methods like GRPO which fail to capture semantic diversity among sampled completions, leading to a diversity-quality inconsistency.", "method": "Introducing Diversity-aware Reward Adjustment (DRA) that uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones, integrating seamlessly with GRPO and its variant DR.", "result": "DRA improves the performance on five mathematical reasoning benchmarks, achieving state-of-the-art results with an average accuracy of 58.2% using limited resources.", "conclusion": "The proposed DRA method effectively addresses the diversity-quality inconsistency issue and can be applied to enhance reinforcement learning models in low-resource scenarios."}}
{"id": "2505.09662", "pdf": "https://arxiv.org/pdf/2505.09662", "abs": "https://arxiv.org/abs/2505.09662", "authors": ["Philipp Schoenegger", "Francesco Salvi", "Jiacheng Liu", "Xiaoli Nan", "Ramit Debnath", "Barbara Fasolo", "Evelina Leivada", "Gabriel Recchia", "Fritz G\u00fcnther", "Ali Zarifhonarvar", "Joe Kwon", "Zahoor Ul Islam", "Marco Dehnert", "Daryl Y. H. Lee", "Madeline G. Reinecke", "David G. Kamper", "Mert Koba\u015f", "Adam Sandford", "Jonas Kgomo", "Luke Hewitt", "Shreya Kapoor", "Kerem Oktar", "Eyup Engin Kucuk", "Bo Feng", "Cameron R. Jones", "Izzy Gainsburg", "Sebastian Olschewski", "Nora Heinzelmann", "Francisco Cruz", "Ben M. Tappin", "Tao Ma", "Peter S. Park", "Rayan Onyonka", "Arthur Hjorth", "Peter Slattery", "Qingcheng Zeng", "Lennart Finke", "Igor Grossmann", "Alessandro Salatiello", "Ezra Karger"], "title": "Large Language Models Are More Persuasive Than Incentivized Human Persuaders", "categories": ["cs.CL", "I.2.7; H.1.2; K.4.1; H.5.2"], "comment": null, "summary": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.", "AI": {"tldr": "This study compares the persuasive abilities of a large language model (Claude Sonnet 3.5) with incentivized human persuaders in an interactive, real-time conversational quiz setting. Results show that the LLM outperforms humans in both truthful and deceptive persuasion contexts.", "motivation": "To investigate whether AI's persuasion capabilities surpass those of humans in real-money bonus scenarios.", "method": "A preregistered, large-scale incentivized experiment involving quiz takers and persuaders (humans or LLMs) attempting to influence quiz takers' answers.", "result": "LLM persuaders achieved higher compliance rates than humans in both truthful and deceptive persuasion contexts. They increased quiz takers' accuracy and earnings when guiding towards correct answers but decreased accuracy and earnings when steering towards incorrect answers.", "conclusion": "AI's persuasive capabilities already exceed those of humans with real-money incentives. This highlights the need for new alignment and governance frameworks for increasingly capable AI persuaders."}}
{"id": "2505.09666", "pdf": "https://arxiv.org/pdf/2505.09666", "abs": "https://arxiv.org/abs/2505.09666", "authors": ["Yumin Choi", "Jinheon Baek", "Sung Ju Hwang"], "title": "System Prompt Optimization with Meta-Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.", "AI": {"tldr": "This paper introduces a novel problem called bilevel system prompt optimization, which aims to design robust and transferable system prompts for large language models. A meta-learning framework is proposed to optimize system prompts across multiple datasets while updating user prompts iteratively.", "motivation": "Existing work on prompt optimization has focused on user prompts, but this paper emphasizes the importance of system prompts that can be applied across different tasks and domains.", "method": "A meta-learning framework is proposed to optimize system prompts by considering various user prompts across multiple datasets and updating user prompts iteratively.", "result": "Experiments on 14 unseen datasets from 5 different domains show that the approach produces system prompts that generalize well to diverse user prompts and enables rapid adaptation to unseen tasks.", "conclusion": "The proposed method demonstrates the effectiveness of bilevel system prompt optimization in improving the performance of large language models."}}
{"id": "2505.09746", "pdf": "https://arxiv.org/pdf/2505.09746", "abs": "https://arxiv.org/abs/2505.09746", "authors": ["Xabier Morales", "Ayah Elsayed", "Debbie Zhao", "Filip Loncaric", "Ainhoa Aguado", "Mireia Masias", "Gina Quill", "Marc Ramos", "Ada Doltra", "Ana Garcia", "Marta Sitges", "David Marlevi", "Alistair Young", "Martyn Nash", "Bart Bijnens", "Oscar Camara"], "title": "A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium", "categories": ["cs.CV"], "comment": null, "summary": "The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.", "AI": {"tldr": "This study introduces an open-source computational framework for analyzing 4D Flow MRI of the left atrium, which can provide accurate automated segmentations and assess energy, vorticity, and pressure parameters as potential prognostic biomarkers.", "motivation": "To enhance the understanding of left atrial hemodynamics and overcome the limitations of conventional ultrasound analysis by using 4D Flow MRI.", "method": "Developing an open-source computational framework tailored for the analysis of 4D Flow MRI in the left atrium, which enables comprehensive qualitative and quantitative analysis of advanced hemodynamic parameters.", "result": "The framework proves robust to data from different centers of varying quality, producing high-accuracy automated segmentations. It also conducts the first comprehensive assessment of energy, vorticity, and pressure parameters in the left atrium across a spectrum of disorders.", "conclusion": "The introduced framework could be used to analyze 4D Flow MRI of the left atrium and assess potential prognostic biomarkers."}}
{"id": "2505.09639", "pdf": "https://arxiv.org/pdf/2505.09639", "abs": "https://arxiv.org/abs/2505.09639", "authors": ["Quentin Cohen-Solal"], "title": "Study and improvement of search algorithms in two-players perfect information games", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Games, in their mathematical sense, are everywhere (game industries,\neconomics, defense, education, chemistry, biology, ...).Search algorithms in\ngames are artificial intelligence methods for playing such games.\nUnfortunately, there is no study on these algorithms that evaluates the\ngenerality of their performance. We propose to address this gap in the case of\ntwo-player zero-sum games with perfect information. Furthermore, we propose a\nnew search algorithm and we show that, for a short search time, it outperforms\nall studied algorithms on all games in this large experiment and that, for a\nmedium search time, it outperforms all studied algorithms on 17 of the 22\nstudied games.", "AI": {"tldr": "A new search algorithm is proposed and shown to outperform existing ones in two-player zero-sum games with perfect information.", "motivation": "To evaluate the generality of search algorithms' performance in two-player zero-sum games with perfect information.", "method": "The proposed new search algorithm", "result": "The new search algorithm shows superior performance compared to other studied algorithms.", "conclusion": "For a short search time, the new search algorithm outperforms all studied algorithms on all games in the large experiment. For a medium search time, it outperforms all studied algorithms on 17 of the 22 studied games."}}
{"id": "2505.09659", "pdf": "https://arxiv.org/pdf/2505.09659", "abs": "https://arxiv.org/abs/2505.09659", "authors": ["Long Chen", "Xiaotian Song", "Yanan Sun"], "title": "LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Spiking Large Language Models (LLMs) have emerged as an energy-efficient\nalternative to conventional LLMs through their event-driven computation. To\neffectively obtain spiking LLMs, researchers develop different ANN-to-SNN\nconversion methods by leveraging pre-trained ANN parameters while inheriting\nthe energy efficiency of SNN. However, existing conversion methods struggle\nwith extreme activation outliers and incompatible nonlinear operations of\nANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for\nfully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel\nneurons to convert the activation outlier and nonlinear operation of ANN-based\nLLMs. Moreover, LAS tailors the spike-equivalent Transformer components for\nspiking LLMs, which can ensure full spiking conversion without any loss of\nperformance. Experimental results on six language models and two\nvision-language models demonstrate that LAS achieves loss-less conversion.\nNotably, on OPT-66B, LAS even improves the accuracy of 2\\% on the WSC task. In\naddition, the parameter and ablation studies further verify the effectiveness\nof LAS. The source code is available at https://github.com/lc783/LAS", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ANN-SNN\u8f6c\u6362\u65b9\u6cd5LAS\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u57fa\u4e8e\u5c16\u5cf0\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u635f\u8f6c\u6362\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684ANN-SNN\u8f6c\u6362\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u57fa\u4e8eANN\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6781\u7aef\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u4e0d\u517c\u5bb9\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u795e\u7ecf\u5143\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u5e76\u5b9a\u5236\u4e86\u5c16\u5cf0\u7b49\u6548\u7684Transformer\u7ec4\u4ef6\u4ee5\u786e\u4fdd\u5b8c\u5168\u5c16\u5cf0\u8f6c\u6362\u4e14\u4e0d\u5931\u6027\u80fd\u3002", "result": "\u5728\u516d\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u65e0\u635f\u8f6c\u6362\uff0c\u5728OPT-66B\u4e0a\u751a\u81f3\u63d0\u9ad8\u4e86WSC\u4efb\u52a1\u7684\u51c6\u786e\u73872%\u3002", "conclusion": "\u63d0\u51fa\u7684LAS\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u635f\u5c16\u5cf0\u8f6c\u6362\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.09701", "pdf": "https://arxiv.org/pdf/2505.09701", "abs": "https://arxiv.org/abs/2505.09701", "authors": ["Xin Liu", "Lechen Zhang", "Sheza Munir", "Yiyang Gu", "Lu Wang"], "title": "VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at generating long-form responses, but\nevaluating their factuality remains challenging due to complex inter-sentence\ndependencies within the generated facts. Prior solutions predominantly follow a\ndecompose-decontextualize-verify pipeline but often fail to capture essential\ncontext and miss key relational facts. In this paper, we introduce VeriFact, a\nfactuality evaluation framework designed to enhance fact extraction by\nidentifying and resolving incomplete and missing facts to support more accurate\nverification results. Moreover, we introduce FactRBench , a benchmark that\nevaluates both precision and recall in long-form model responses, whereas prior\nwork primarily focuses on precision. FactRBench provides reference fact sets\nfrom advanced LLMs and human-written answers, enabling recall assessment.\nEmpirical evaluations show that VeriFact significantly enhances fact\ncompleteness and preserves complex facts with critical relational information,\nresulting in more accurate factuality evaluation. Benchmarking various open-\nand close-weight LLMs on FactRBench indicate that larger models within same\nmodel family improve precision and recall, but high precision does not always\ncorrelate with high recall, underscoring the importance of comprehensive\nfactuality assessment.", "AI": {"tldr": "Introduce VeriFact for better factuality evaluation of large language models and create FactRBench for assessing both precision and recall.", "motivation": "Evaluating the factuality of large language models is challenging due to complex inter-sentence dependencies in generated facts. Existing methods often fail to capture necessary context and miss key relational facts.", "method": "Introducing VeriFact, a factuality evaluation framework focusing on identifying and resolving incomplete and missing facts.", "result": "VeriFact enhances fact extraction and supports more accurate verification results. FactRBench evaluates both precision and recall in long-form model responses, providing reference fact sets from advanced LLMs and human-written answers.", "conclusion": "VeriFact significantly improves fact completeness and preserves critical relational information, offering more precise factuality evaluation."}}
{"id": "2505.09827", "pdf": "https://arxiv.org/pdf/2505.09827", "abs": "https://arxiv.org/abs/2505.09827", "authors": ["Julian Tanke", "Takashi Shibuya", "Kengo Uchida", "Koichi Saito", "Yuki Mitsufuji"], "title": "Dyadic Mamba: Long-term Dyadic Human Motion Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 HuMoGen Workshop", "summary": "Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.", "AI": {"tldr": "This paper introduces Dyadic Mamba, a novel State-Space Model (SSM)-based approach for generating high-quality dyadic human motion from text descriptions of arbitrary length.", "motivation": "Existing transformer-based methods struggle with longer sequences due to limitations in positional encoding schemes.", "method": "Dyadic Mamba uses a simple architecture facilitating information flow through concatenation without complex cross-attention mechanisms.", "result": "It performs competitively on short-term benchmarks and outperforms transformers on longer sequences.", "conclusion": "SSM-based architectures show promise for long-term dyadic human motion synthesis from text."}}
{"id": "2505.09640", "pdf": "https://arxiv.org/pdf/2505.09640", "abs": "https://arxiv.org/abs/2505.09640", "authors": ["Tom\u00e1s Capdevielle", "Santiago Cifuentes"], "title": "Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms", "categories": ["cs.AI", "68T01", "I.2.0"], "comment": "22 pages, 7 figures", "summary": "Given a classification model and a prediction for some input, there are\nheuristic strategies for ranking features according to their importance in\nregard to the prediction. One common approach to this task is rooted in\npropositional logic and the notion of \\textit{sufficient reason}. Through this\nconcept, the categories of relevant and necessary features were proposed in\norder to identify the crucial aspects of the input. This paper improves the\nexisting techniques and algorithms for deciding which are the relevant and/or\nnecessary features, showing in particular that necessity can be detected\nefficiently in complex models such as neural networks. We also generalize the\nnotion of relevancy and study associated problems. Moreover, we present a new\nglobal notion (i.e. that intends to explain whether a feature is important for\nthe behavior of the model in general, not depending on a particular input) of\n\\textit{usefulness} and prove that it is related to relevancy and necessity.\nFurthermore, we develop efficient algorithms for detecting it in decision trees\nand other more complex models, and experiment on three datasets to analyze its\npractical utility.", "AI": {"tldr": "This paper improves existing techniques for identifying crucial features in classification models using concepts from propositional logic.", "motivation": "To improve the identification of relevant and necessary features in complex models like neural networks.", "method": "Improving existing techniques and algorithms for deciding on relevant and/or necessary features, generalizing the notion of relevancy, and introducing a new global notion of usefulness.", "result": "Efficient detection of necessity in complex models and development of algorithms for detecting usefulness in decision trees and other models.", "conclusion": "The paper shows that necessity can be efficiently detected in complex models and proves the relationship between usefulness and relevancy/necessity."}}
{"id": "2505.09663", "pdf": "https://arxiv.org/pdf/2505.09663", "abs": "https://arxiv.org/abs/2505.09663", "authors": ["Julian B\u00fcchel", "Iason Chalas", "Giovanni Acampa", "An Chen", "Omobayode Fagbohungbe", "Sidney Tsai", "Kaoutar El Maghraoui", "Manuel Le Gallo", "Abbas Rahimi", "Abu Sebastian"], "title": "Analog Foundation Models", "categories": ["cs.LG"], "comment": "43 pages, 8 figures, under review", "summary": "Analog in-memory computing (AIMC) is a promising compute paradigm to improve\nspeed and power efficiency of neural network inference beyond the limits of\nconventional von Neumann-based architectures. However, AIMC introduces\nfundamental challenges such as noisy computations and strict constraints on\ninput and output quantization. Because of these constraints and imprecisions,\noff-the-shelf LLMs are not able to achieve 4-bit-level performance when\ndeployed on AIMC-based hardware. While researchers previously investigated\nrecovering this accuracy gap on small, mostly vision-based models, a generic\nmethod applicable to LLMs pre-trained on trillions of tokens does not yet\nexist. In this work, we introduce a general and scalable method to robustly\nadapt LLMs for execution on noisy, low-precision analog hardware. Our approach\nenables state-of-the-art models $\\unicode{x2013}$ including\nPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\\unicode{x2013}$ to retain\nperformance comparable to 4-bit weight, 8-bit activation baselines, despite the\npresence of analog noise and quantization constraints. Additionally, we show\nthat as a byproduct of our training methodology, analog foundation models can\nbe quantized for inference on low-precision digital hardware. Finally, we show\nthat our models also benefit from test-time compute scaling, showing better\nscaling behavior than models trained with 4-bit weight and 8-bit static input\nquantization. Our work bridges the gap between high-capacity LLMs and efficient\nanalog hardware, offering a path toward energy-efficient foundation models.\nCode is available at https://github.com/IBM/analog-foundation-models .", "AI": {"tldr": "This paper presents a general and scalable method to adapt large language models for execution on noisy, low-precision analog hardware, achieving performance comparable to 4-bit weight and 8-bit activation baselines.", "motivation": "To address the challenge of deploying large language models on analog in-memory computing hardware which introduces fundamental challenges like noisy computations and strict input/output quantization constraints.", "method": "Introducing a new training methodology that allows large language models to perform well on noisy, low-precision analog hardware.", "result": "State-of-the-art models can maintain performance similar to 4-bit weight and 8-bit activation baselines on analog hardware, and can also be quantized for inference on low-precision digital hardware.", "conclusion": "This work bridges the gap between high-capacity large language models and efficient analog hardware, providing a pathway towards energy-efficient foundation models."}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724", "abs": "https://arxiv.org/abs/2505.09724", "authors": ["Gino Carmona-D\u00edaz", "William Jim\u00e9nez-Leal", "Mar\u00eda Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Berm\u00fadez"], "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u8fdb\u884c\u6587\u672c\u5206\u6790\u7684\u5206\u6b65\u6559\u7a0b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u8fed\u4ee3\u548c\u534f\u4f5c\u8fc7\u7a0b\u5f00\u53d1\u548c\u5e94\u7528\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6f5c\u529b\u4e0e\u9650\u5236\u3002", "motivation": "\u5206\u6790\u6587\u672c\uff08\u5982\u5f00\u653e\u5f0f\u56de\u5e94\u3001\u6807\u9898\u6216\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff09\u662f\u4e00\u4e2a\u8017\u65f6\u4e14\u5bb9\u6613\u4ea7\u751f\u504f\u89c1\u7684\u8fc7\u7a0b\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5f88\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u7528\u9884\u5b9a\u4e49\u7684\uff08\u81ea\u4e0a\u800c\u4e0b\u7684\uff09\u6216\u6570\u636e\u9a71\u52a8\u7684\uff08\u81ea\u4e0b\u800c\u4e0a\u7684\uff09\u5206\u7c7b\u6cd5\u8fdb\u884c\u6587\u672c\u5206\u6790\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u8d28\u91cf\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9010\u6b65\u6559\u7a0b\uff0c\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u7814\u7a76\u4eba\u5458\u548cLLMs\u4e4b\u95f4\u7684\u8fed\u4ee3\u548c\u534f\u4f5c\u8fc7\u7a0b\uff0c\u6709\u6548\u5730\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u5e94\u7528\u7528\u4e8e\u5206\u6790\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u5206\u7c7b\u6cd5\u3002\u4f7f\u7528\u53c2\u4e0e\u8005\u63d0\u4f9b\u7684\u4e2a\u4eba\u76ee\u6807\u4f5c\u4e3a\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u7f16\u5199\u63d0\u793a\u6765\u5ba1\u67e5\u6570\u636e\u96c6\u5e76\u751f\u6210\u751f\u6d3b\u9886\u57df\u7684\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u548c\u76f4\u63a5\u4fee\u6539\u8bc4\u4f30\u548c\u4f18\u5316\u5206\u7c7b\u6cd5\uff0c\u6d4b\u8bd5\u5206\u7c7b\u6cd5\u5e76\u8bc4\u4f30\u7f16\u7801\u8005\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u5e94\u7528\u5206\u7c7b\u6cd5\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\uff0c\u5177\u6709\u9ad8\u7f16\u7801\u8005\u95f4\u53ef\u9760\u6027\u3002", "result": "\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528LLMs\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5728\u4e2a\u4eba\u76ee\u6807\u7684\u4f8b\u5b50\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7f16\u7801\u8005\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u6587\u672c\u5206\u6790\u7684\u53ef\u80fd\u6027\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2505.09829", "pdf": "https://arxiv.org/pdf/2505.09829", "abs": "https://arxiv.org/abs/2505.09829", "authors": ["Tushar Kataria", "Shireen Y. Elhabian"], "title": "BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes", "categories": ["cs.CV"], "comment": null, "summary": "Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.", "AI": {"tldr": "Propose BoundarySeg, a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation.", "motivation": "Reduce annotation costs while maintaining segmentation accuracy in low data regimes.", "method": "BoundarySeg multi-task framework with organ boundary prediction as an auxiliary task", "result": "Performance comparable to or exceeding state-of-the-art semi-supervised approaches without using unannotated data or increasing computational demands", "conclusion": "BoundarySeg is simple, effective, and computationally efficient for medical image segmentation."}}
{"id": "2505.09737", "pdf": "https://arxiv.org/pdf/2505.09737", "abs": "https://arxiv.org/abs/2505.09737", "authors": ["Osher Elhadad", "Reuth Mirsky"], "title": "General Dynamic Goal Recognition", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted for publication at Generalization in Planning (GenPlan) as\n  part of AAAI 2025 workshops", "summary": "Understanding an agent's intent through its behavior is essential in\nhuman-robot interaction, interactive AI systems, and multi-agent\ncollaborations. This task, known as Goal Recognition (GR), poses significant\nchallenges in dynamic environments where goals are numerous and constantly\nevolving. Traditional GR methods, designed for a predefined set of goals, often\nstruggle to adapt to these dynamic scenarios. To address this limitation, we\nintroduce the General Dynamic GR problem - a broader definition of GR - aimed\nat enabling real-time GR systems and fostering further research in this area.\nExpanding on this foundation, this paper employs a model-free goal-conditioned\nRL approach to enable fast adaptation for GR across various changing tasks.", "AI": {"tldr": "This paper introduces General Dynamic Goal Recognition (GR) problem and uses a model-free goal-conditioned RL approach to achieve fast adaptation for GR across different changing tasks.", "motivation": "Goal recognition is crucial in human-robot interaction and multi-agent collaborations. However, traditional GR methods struggle in dynamic environments with numerous and evolving goals.", "method": "A model-free goal-conditioned RL approach is used to enable fast adaptation for GR.", "result": "The proposed method can adapt quickly for GR across various changing tasks.", "conclusion": "This work broadens the scope of GR by introducing the General Dynamic GR problem and provides a potential solution using a model-free goal-conditioned RL approach."}}
{"id": "2505.09702", "pdf": "https://arxiv.org/pdf/2505.09702", "abs": "https://arxiv.org/abs/2505.09702", "authors": ["Yezi Liu", "Prathyush Poduval", "Wenjun Huang", "Yang Ni", "Hanning Chen", "Mohsen Imani"], "title": "Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing", "categories": ["cs.LG"], "comment": null, "summary": "Graph unlearning is a crucial approach for protecting user privacy by erasing\nthe influence of user data on trained graph models. Recent developments in\ngraph unlearning methods have primarily focused on maintaining model prediction\nperformance while removing user information. However, we have observed that\nwhen user information is deleted from the model, the prediction distribution\nacross different sensitive groups often changes. Furthermore, graph models are\nshown to be prone to amplifying biases, making the study of fairness in graph\nunlearning particularly important. This raises the question: Does graph\nunlearning actually introduce bias? Our findings indicate that the predictions\nof post-unlearning models become highly correlated with sensitive attributes,\nconfirming the introduction of bias in the graph unlearning process. To address\nthis issue, we propose a fair graph unlearning method, FGU. To guarantee\nprivacy, FGU trains shard models on partitioned subgraphs, unlearns the\nrequested data from the corresponding subgraphs, and retrains the shard models\non the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing\nprocess: it first enables shard-level fairness by incorporating a fairness\nregularizer in the shard model retraining, and then achieves global-level\nfairness by aligning all shard models to minimize global disparity. Our\nexperiments demonstrate that FGU achieves superior fairness while maintaining\nprivacy and accuracy. Additionally, FGU is robust to diverse unlearning\nrequests, ensuring fairness and utility performance across various data\ndistributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u516c\u5e73\u56fe\u9057\u5fd8\u65b9\u6cd5(FGU)\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u9690\u79c1\u548c\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u56fe\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u504f\u89c1\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u9057\u5fd8\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u4fdd\u6301\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5220\u9664\u7528\u6237\u4fe1\u606f\uff0c\u4f46\u5f53\u7528\u6237\u4fe1\u606f\u4ece\u6a21\u578b\u4e2d\u5220\u9664\u65f6\uff0c\u4e0d\u540c\u654f\u611f\u7ec4\u4e4b\u95f4\u7684\u9884\u6d4b\u5206\u5e03\u5f80\u5f80\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u5e76\u4e14\u56fe\u6a21\u578b\u5bb9\u6613\u653e\u5927\u504f\u89c1\uff0c\u4f7f\u5f97\u7814\u7a76\u56fe\u9057\u5fd8\u4e2d\u7684\u516c\u5e73\u6027\u5c24\u4e3a\u91cd\u8981\u3002", "method": "FGU\u65b9\u6cd5\u901a\u8fc7\u5728\u5212\u5206\u7684\u5c0f\u56fe\u4e0a\u8bad\u7ec3\u5206\u7247\u6a21\u578b\uff0c\u4ece\u76f8\u5e94\u7684\u5b50\u56fe\u4e2d\u9057\u5fd8\u8bf7\u6c42\u7684\u6570\u636e\uff0c\u5e76\u5728\u4fee\u6539\u540e\u7684\u5b50\u56fe\u4e0a\u91cd\u65b0\u8bad\u7ec3\u5206\u7247\u6a21\u578b\u6765\u4fdd\u8bc1\u9690\u79c1\u3002\u4e3a\u4e86\u786e\u4fdd\u516c\u5e73\u6027\uff0cFGU\u91c7\u7528\u53cc\u5c42\u53bb\u504f\u8fc7\u7a0b\uff1a\u9996\u5148\u5728\u5206\u7247\u6a21\u578b\u518d\u8bad\u7ec3\u4e2d\u52a0\u5165\u516c\u5e73\u6b63\u5219\u5316\u5668\u6765\u5b9e\u73b0\u5206\u7247\u7ea7\u516c\u5e73\u6027\uff0c\u7136\u540e\u901a\u8fc7\u4f7f\u6240\u6709\u5206\u7247\u6a21\u578b\u5bf9\u9f50\u6765\u6700\u5c0f\u5316\u5168\u5c40\u5dee\u5f02\u4ee5\u5b9e\u73b0\u5168\u5c40\u7ea7\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFGU\u5728\u4fdd\u8bc1\u9690\u79c1\u548c\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u8fd8\u80fd\u5b9e\u73b0\u4f18\u79c0\u7684\u516c\u5e73\u6027\uff0c\u5e76\u4e14\u5bf9\u4e0d\u540c\u7684\u9057\u5fd8\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u5404\u79cd\u6570\u636e\u5206\u5e03\u4e0b\u90fd\u80fd\u4fdd\u6301\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cFGU\u5728\u4fdd\u8bc1\u9690\u79c1\u548c\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u8fd8\u80fd\u5b9e\u73b0\u4f18\u79c0\u7684\u516c\u5e73\u6027\uff0c\u5e76\u4e14\u5bf9\u4e0d\u540c\u7684\u9057\u5fd8\u8bf7\u6c42\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u5404\u79cd\u6570\u636e\u5206\u5e03\u4e0b\u90fd\u80fd\u4fdd\u6301\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.09738", "pdf": "https://arxiv.org/pdf/2505.09738", "abs": "https://arxiv.org/abs/2505.09738", "authors": ["Shaurya Sharthak", "Vinayak Pahalwan", "Adithya Kamath", "Adarsh Shirawalmath"], "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.", "AI": {"tldr": "This paper presents Tokenadapt, a model-agnostic tokenizer transplantation method that improves efficiency and performance of pretrained language models.", "motivation": "Traditional tokenization schemes limit the efficiency and performance of pretrained language models, especially in multilingual or specialized applications.", "method": "Tokenadapt introduces a hybrid heuristic for initializing new token embeddings and proposes pre-tokenization learning for multi-word Supertokens.", "result": "Empirical studies show Tokenadapt outperforms other methods like Transtokenizer and ReTok, achieving better compression and lower perplexity.", "conclusion": "Tokenadapt effectively addresses the limitations of traditional tokenization approaches with minimal retraining needs."}}
{"id": "2505.09858", "pdf": "https://arxiv.org/pdf/2505.09858", "abs": "https://arxiv.org/abs/2505.09858", "authors": ["Danush Kumar Venkatesh", "Isabel Funke", "Micha Pfeiffer", "Fiona Kolbinger", "Hanna Maria Schmeiser", "Juergen Weitz", "Marius Distler", "Stefanie Speidel"], "title": "Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models", "categories": ["cs.CV"], "comment": "Early accept at MICCAI 2025", "summary": "Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.", "AI": {"tldr": "This paper introduces a novel text-conditioned diffusion-based method to synthesize high-fidelity surgical videos for under-represented classes, addressing data imbalance issues. The method uses a two-stage process with 2D latent diffusion for spatial content and temporal attention layers for temporal consistency. It also includes a rejection sampling strategy to select suitable synthetic samples.", "motivation": "To overcome the data imbalance issue often found in surgical video datasets, which hinders the development of high-performing models.", "method": "A unique two-stage, text-conditioned diffusion-based method is proposed. It uses a 2D latent diffusion model for spatial content and integrates temporal attention layers for temporal consistency. A rejection sampling strategy is introduced to select synthetic samples.", "result": "The method improves model performance on downstream tasks like surgical action recognition and intra-operative event prediction when incorporating synthetic videos.", "conclusion": "This work demonstrates the effectiveness of using synthesized surgical videos to address data imbalance and enhance model performance."}}
{"id": "2505.09755", "pdf": "https://arxiv.org/pdf/2505.09755", "abs": "https://arxiv.org/abs/2505.09755", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "title": "Explainability Through Human-Centric Design for XAI in Lung Cancer Detection", "categories": ["cs.AI"], "comment": null, "summary": "Deep learning models have shown promise in lung pathology detection from\nchest X-rays, but widespread clinical adoption remains limited due to opaque\nmodel decision-making. In prior work, we introduced ClinicXAI, a human-centric,\nexpert-guided concept bottleneck model (CBM) designed for interpretable lung\ncancer diagnosis. We now extend that approach and present XpertXAI, a\ngeneralizable expert-driven model that preserves human-interpretable clinical\nconcepts while scaling to detect multiple lung pathologies. Using a\nhigh-performing InceptionV3-based classifier and a public dataset of chest\nX-rays with radiology reports, we compare XpertXAI against leading post-hoc\nexplainability methods and an unsupervised CBM, XCBs. We assess explanations\nthrough comparison with expert radiologist annotations and medical ground\ntruth. Although XpertXAI is trained for multiple pathologies, our expert\nvalidation focuses on lung cancer. We find that existing techniques frequently\nfail to produce clinically meaningful explanations, omitting key diagnostic\nfeatures and disagreeing with radiologist judgments. XpertXAI not only\noutperforms these baselines in predictive accuracy but also delivers\nconcept-level explanations that better align with expert reasoning. While our\nfocus remains on explainability in lung cancer detection, this work illustrates\nhow human-centric model design can be effectively extended to broader\ndiagnostic contexts - offering a scalable path toward clinically meaningful\nexplainable AI in medical diagnostics.", "AI": {"tldr": "XpertXAI\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u4e13\u5bb6\u9a71\u52a8\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u80f8\u90e8X\u5149\u7247\u4e2d\u68c0\u6d4b\u591a\u79cd\u80ba\u90e8\u75be\u75c5\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u53ef\u89e3\u91ca\u7684\u4eba\u7c7b\u4e34\u5e8a\u6982\u5ff5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cXpertXAI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6982\u5ff5\u7ea7\u89e3\u91ca\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u7684\u5224\u65ad\u76f8\u4e00\u81f4\u3002", "motivation": "\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u80ba\u75c5\u68c0\u6d4b\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u4fbf\u4e8e\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aXpertXAI\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eInceptionV3\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528\u516c\u5171\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u4e0e\u653e\u5c04\u5b66\u62a5\u544a\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "XpertXAI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6982\u5ff5\u7ea7\u89e3\u91ca\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u540e\u5904\u7406\u89e3\u91ca\u65b9\u6cd5\u548c\u65e0\u76d1\u7763CBM\uff08XCBs\uff09\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u6765\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u8bca\u65ad\u73af\u5883\u4e2d\uff0c\u4ece\u800c\u4e3a\u533b\u5b66\u8bca\u65ad\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2505.09704", "pdf": "https://arxiv.org/pdf/2505.09704", "abs": "https://arxiv.org/abs/2505.09704", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Charalampos Kalalas", "Paolo Dini"], "title": "Energy-Efficient Federated Learning for AIoT using Clustering Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While substantial research has been devoted to optimizing model performance,\nconvergence rates, and communication efficiency, the energy implications of\nfederated learning (FL) within Artificial Intelligence of Things (AIoT)\nscenarios are often overlooked in the existing literature. This study examines\nthe energy consumed during the FL process, focusing on three main\nenergy-intensive processes: pre-processing, communication, and local learning,\nall contributing to the overall energy footprint. We rely on the observation\nthat device/client selection is crucial for speeding up the convergence of\nmodel training in a distributed AIoT setting and propose two\nclustering-informed methods. These clustering solutions are designed to group\nAIoT devices with similar label distributions, resulting in clusters composed\nof nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity\noften encountered in real-world distributed learning applications. Throughout\nextensive numerical experimentation, we demonstrate that our clustering\nstrategies typically achieve high convergence rates while maintaining low\nenergy consumption when compared to other recent approaches available in the\nliterature.", "AI": {"tldr": "This study examines the energy consumed during the federated learning process in AIoT scenarios, focusing on three energy-intensive processes: pre-processing, communication, and local learning. Two clustering-informed methods are proposed to group AIoT devices with similar label distributions, achieving high convergence rates while maintaining low energy consumption.", "motivation": "The energy implications of federated learning within AIoT scenarios are often overlooked in the existing literature.", "method": "Two clustering-informed methods are proposed to group AIoT devices with similar label distributions.", "result": "These methods alleviate the heterogeneity often encountered in real-world distributed learning applications and maintain low energy consumption.", "conclusion": "Our clustering strategies typically achieve high convergence rates while maintaining low energy consumption compared to other recent approaches."}}
{"id": "2505.09794", "pdf": "https://arxiv.org/pdf/2505.09794", "abs": "https://arxiv.org/abs/2505.09794", "authors": ["J. Moreno-Casanova", "J. M. Au\u00f1\u00f3n", "A. M\u00e1rtinez-P\u00e9rez", "M. E. P\u00e9rez-Mart\u00ednez", "M. E. Gas-L\u00f3pez"], "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.", "AI": {"tldr": "This study explores the use of NLP techniques, specifically Named Entity Recognition (NER), to automatically identify and extract key clinical information from electronic health records (EHRs) related to lung and breast cancer.", "motivation": "Manual extraction of information from clinical reports is time-consuming and error-prone, limiting the efficiency of data-driven approaches in healthcare. The study focuses on lung and breast cancer due to their high incidence and significant impact on public health.", "method": "The study utilized GMV's NLP tool uQuery and fine-tuned the bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained in Spanish, to perform NER on a dataset from Health Research Institute Hospital La Fe.", "result": "The results show strong overall performance in identifying entities like MET and PAT, although challenges remain with less frequent entities like EVOL.", "conclusion": "NLP techniques can enhance the accuracy and efficiency of data extraction from EHRs for lung and breast cancer."}}
{"id": "2505.09859", "pdf": "https://arxiv.org/pdf/2505.09859", "abs": "https://arxiv.org/abs/2505.09859", "authors": ["Andrew Jun Lee", "Taylor Webb", "Trevor Bihl", "Keith Holyoak", "Hongjing Lu"], "title": "Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction", "categories": ["cs.CV"], "comment": "Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society", "summary": "The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.", "AI": {"tldr": "A new model called Probabilistic Schema Induction (PSI) is introduced which employs deep learning to perform analogical mapping over structured representations of examples, forming a compositional concept called a schema.", "motivation": "To model rapid human-like learning of compositional visual concepts", "method": "PSI employs a novel conception of similarity that weighs object-level similarity and relational similarity, as well as a mechanism for amplifying relations relevant to classification.", "result": "PSI produces human-like learning performance and outperforms two controls.", "conclusion": "These findings suggest that structured representations and analogical mapping are critical to modeling rapid human-like learning of compositional visual concepts."}}
{"id": "2505.09787", "pdf": "https://arxiv.org/pdf/2505.09787", "abs": "https://arxiv.org/abs/2505.09787", "authors": ["Ziruo Yi", "Ting Xiao", "Mark V. Albert"], "title": "A Multimodal Multi-Agent Framework for Radiology Report Generation", "categories": ["cs.AI"], "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce diagnostic\nreports from medical images, with the potential to enhance clinical workflows\nand reduce radiologists' workload. While recent approaches leveraging\nmultimodal large language models (MLLMs) and retrieval-augmented generation\n(RAG) have achieved strong results, they continue to face challenges such as\nfactual inconsistency, hallucination, and cross-modal misalignment. We propose\na multimodal multi-agent framework for RRG that aligns with the stepwise\nclinical reasoning workflow, where task-specific agents handle retrieval, draft\ngeneration, visual analysis, refinement, and synthesis. Experimental results\ndemonstrate that our approach outperforms a strong baseline in both automatic\nmetrics and LLM-based evaluations, producing more accurate, structured, and\ninterpretable reports. This work highlights the potential of clinically aligned\nmulti-agent frameworks to support explainable and trustworthy clinical AI\napplications.", "AI": {"tldr": "Proposed a multi-agent framework for radiology report generation that improves accuracy and interpretability.", "motivation": "To address challenges such as factual inconsistency, hallucination, and cross-modal misalignment faced by previous approaches leveraging multimodal large language models (MLLMs) and retrieval-augmented generation (RAG).", "method": "A multimodal multi-agent framework for RRG that aligns with the stepwise clinical reasoning workflow, where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis.", "result": "Our approach improves the accuracy, structure, and interpretability of radiology report generation.", "conclusion": "Our approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations, producing more accurate, structured, and interpretable reports."}}
{"id": "2505.09710", "pdf": "https://arxiv.org/pdf/2505.09710", "abs": "https://arxiv.org/abs/2505.09710", "authors": ["Konstantinos Fotopoulos", "Petros Maragos"], "title": "Training Deep Morphological Neural Networks as Universal Approximators", "categories": ["cs.LG"], "comment": null, "summary": "We investigate deep morphological neural networks (DMNNs). We demonstrate\nthat despite their inherent non-linearity, activations between layers are\nessential for DMNNs. We then propose several new architectures for DMNNs, each\nwith a different constraint on their parameters. For the first (resp. second)\narchitecture, we work under the constraint that the majority of parameters\n(resp. learnable parameters) should be part of morphological operations. We\nempirically show that our proposed networks can be successfully trained, and\nare more prunable than linear networks. To the best of our knowledge, we are\nthe first to successfully train DMNNs under such constraints, although the\ngeneralization capabilities of our networks remain limited. Finally, we propose\na hybrid network architecture combining linear and morphological layers,\nshowing empirically that the inclusion of morphological layers significantly\naccelerates the convergence of gradient descent with large batches.", "AI": {"tldr": "\u7814\u7a76\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u67b6\u6784\uff0c\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u6210\u529f\u8bad\u7ec3DMNNs\uff0c\u53d1\u73b0\u5f62\u6001\u5c42\u80fd\u52a0\u901f\u68af\u5ea6\u4e0b\u964d\u6536\u655b\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\uff0c\u5e76\u63a2\u8ba8\u5c42\u95f4\u6fc0\u6d3b\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u5982\u4f55\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u6210\u529f\u8bad\u7ec3\u5b83\u4eec\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u5bf9\u53c2\u6570\u6709\u4e0d\u540c\u7ea6\u675f\u7684\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\u3002", "result": "\u63d0\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u6210\u529f\u8bad\u7ec3\uff0c\u5e76\u4e14\u6bd4\u7ebf\u6027\u7f51\u7edc\u66f4\u5bb9\u6613\u4fee\u526a\uff1b\u5f62\u6001\u5c42\u663e\u8457\u52a0\u901f\u4e86\u5927\u6279\u6b21\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u3002", "conclusion": "\u5c3d\u7ba1\u6df1\u5ea6\u5f62\u6001\u795e\u7ecf\u7f51\u7edc\uff08DMNNs\uff09\u5177\u6709\u56fa\u6709\u7684\u975e\u7ebf\u6027\uff0c\u4f46\u5c42\u95f4\u7684\u6fc0\u6d3b\u5bf9\u4e8eDMNNs\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u6bcf\u4e2a\u67b6\u6784\u5bf9\u53c2\u6570\u6709\u4e0d\u540c\u7684\u7ea6\u675f\u3002\u6211\u4eec\u7684\u63d0\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u6210\u529f\u8bad\u7ec3\uff0c\u5e76\u4e14\u6bd4\u7ebf\u6027\u7f51\u7edc\u66f4\u5bb9\u6613\u4fee\u526a\u3002\u6211\u4eec\u9996\u6b21\u5728\u8fd9\u6837\u7684\u7ea6\u675f\u4e0b\u6210\u529f\u8bad\u7ec3\u4e86DMNNs\uff0c\u5c3d\u7ba1\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u548c\u5f62\u6001\u5c42\u7684\u6df7\u5408\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u8bc1\u8868\u660e\u5f62\u6001\u5c42\u663e\u8457\u52a0\u901f\u4e86\u5927\u6279\u6b21\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u3002"}}
{"id": "2505.09807", "pdf": "https://arxiv.org/pdf/2505.09807", "abs": "https://arxiv.org/abs/2505.09807", "authors": ["Timour Ichmoukhamedov", "David Martens"], "title": "Exploring the generalization of LLM truth directions on conversational formats", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.", "AI": {"tldr": "This work explores the generalization of truth direction in LLMs across conversational formats and proposes a method using a fixed key phrase to enhance lie detection reliability.", "motivation": "To investigate how the truth direction in LLMs generalizes between different conversational formats and to improve the generalization ability of lie detectors.", "method": "Exploration of the generalization of the truth direction between various conversational formats and proposing a solution involving adding a fixed key phrase at the end of each conversation.", "result": "Good generalization between short conversations ending on a lie, but poor generalization to longer formats where the lie appears earlier. A proposed solution improves this type of generalization.", "conclusion": "The study concludes that while there is a universal truth direction in LLMs allowing linear separability of true and false statements, reliable lie detectors that generalize across different conversational formats still face significant challenges."}}
{"id": "2505.09915", "pdf": "https://arxiv.org/pdf/2505.09915", "abs": "https://arxiv.org/abs/2505.09915", "authors": ["Zhe Xin", "Chenyang Wu", "Penghui Huang", "Yanyong Zhang", "Yinian Mao", "Guoquan Huang"], "title": "Large-Scale Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.", "AI": {"tldr": "This paper presents LSG-SLAM, a large-scale 3D Gaussian Splatting-based visual SLAM system using stereo cameras. It introduces a multi-modality strategy for pose estimation under large view changes and uses continuous Gaussian Splatting submaps for scalability in large-scale scenarios. Loop detection and pose optimization are performed between Gaussian Splatting submaps, followed by a structure refinement module to enhance reconstruction quality.", "motivation": "To address the lack of robustness in neural radiance fields and 3D Gaussian splatting-based visual SLAM systems for large-scale outdoor scenarios, especially those requiring RGBD sensors.", "method": "LSG-SLAM employs a multi-modality strategy for pose estimation under large view changes, feature-alignment warping constraints, continuous Gaussian Splatting submaps for scalability, loop detection through place recognition, and a structure refinement module.", "result": "LSG-SLAM achieves superior performance compared to existing neural, 3DGS-based, and traditional approaches when evaluated on the EuRoc and KITTI datasets.", "conclusion": "The proposed LSG-SLAM demonstrates strong performance in large-scale outdoor visual SLAM tasks, particularly with stereo cameras."}}
{"id": "2505.09920", "pdf": "https://arxiv.org/pdf/2505.09920", "abs": "https://arxiv.org/abs/2505.09920", "authors": ["Shan Yang", "Yongli Zhu"], "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025", "summary": "This paper presents a study on using different offline reinforcement learning\nalgorithms for microgrid voltage regulation with solar power penetration. When\nenvironment interaction is unviable due to technical or safety reasons, the\nproposed approach can still obtain an applicable model through offline-style\ntraining on a previously collected dataset, lowering the negative impact of\nlacking online environment interactions. Experiment results on the IEEE 33-bus\nsystem demonstrate the feasibility and effectiveness of the proposed approach\non different offline datasets, including the one with merely low-quality\nexperience.", "AI": {"tldr": "This paper studies offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration, demonstrating their feasibility and effectiveness on various offline datasets, even those with low-quality experience.", "motivation": "To address the issue of lacking online environment interactions in microgrid voltage regulation, which may be due to technical or safety reasons.", "method": "Using different offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration.", "result": "The proposed approach can obtain an applicable model through offline-style training on previously collected datasets.", "conclusion": "The study shows that the proposed approach is feasible and effective for microgrid voltage regulation with solar power penetration on different offline datasets, including those with low-quality experience."}}
{"id": "2505.09716", "pdf": "https://arxiv.org/pdf/2505.09716", "abs": "https://arxiv.org/abs/2505.09716", "authors": ["George Dimitriadis. Spyridon Samothrakis"], "title": "Out-of-distribution generalisation is hard: evidence from ARC-like tasks", "categories": ["cs.LG", "cs.AI"], "comment": "Submission to NeurIPS 2025", "summary": "Out-of-distribution (OOD) generalisation is considered a hallmark of human\nand animal intelligence. To achieve OOD through composition, a system must\ndiscover the environment-invariant properties of experienced input-output\nmappings and transfer them to novel inputs. This can be realised if an\nintelligent system can identify appropriate, task-invariant, and composable\ninput features, as well as the composition methods, thus allowing it to act\nbased not on the interpolation between learnt data points but on the\ntask-invariant composition of those features. We propose that in order to\nconfirm that an algorithm does indeed learn compositional structures from data,\nit is not enough to just test on an OOD setup, but one also needs to confirm\nthat the features identified are indeed compositional. We showcase this by\nexploring two tasks with clearly defined OOD metrics that are not OOD solvable\nby three commonly used neural networks: a Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and a Transformer. In addition, we develop\ntwo novel network architectures imbued with biases that allow them to be\nsuccessful in OOD scenarios. We show that even with correct biases and almost\nperfect OOD performance, an algorithm can still fail to learn the correct\nfeatures for compositional generalisation.", "AI": {"tldr": "This paper discusses out-of-distribution (OOD) generalization and how intelligent systems can learn compositional structures from data. The authors argue that simply testing on an OOD setup is insufficient to confirm that an algorithm learns compositional structures, and they demonstrate this by exploring two tasks where common neural networks fail. They also introduce two new network architectures that perform well in OOD scenarios.", "motivation": "To investigate the ability of intelligent systems to learn compositional structures from data and generalize out-of-distribution.", "method": "Testing the performance of common neural networks (MLP, CNN, Transformer) and developing two novel network architectures with specific biases.", "result": "Common neural networks fail in OOD scenarios, while the newly developed architectures perform well, but even with correct biases, an algorithm can still fail to learn the correct features for compositional generalization.", "conclusion": "Learning compositional structures requires more than just good performance in OOD scenarios; identifying truly compositional features is crucial."}}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825", "abs": "https://arxiv.org/abs/2505.09825", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u89e3\u8bfb\u63a8\u7406\u7684\u7ec6\u8bfb\u57fa\u51c6KRISTEVA\uff0c\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u8bfb\u80fd\u529b\u4e0a\u867d\u6709\u4e00\u5b9a\u6c34\u5e73\uff0c\u4f46\u4ecd\u900a\u8272\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "motivation": "\u867d\u7136\u7ec6\u8bfb\u88ab\u8ba4\u4e3a\u662f\u6279\u5224\u6027\u601d\u7ef4\u7684\u57fa\u7840\uff0c\u5e76\u5e7f\u6cdb\u4f5c\u4e3a\u5927\u5b66\u8bfe\u7a0b\u7684\u5fc5\u8981\u7ec4\u6210\u90e8\u5206\u88ab\u91c7\u7528\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u672a\u88ab\u8bc4\u4f30\u8fc7\u5176\u7ec6\u8bfb\u80fd\u529b\uff0c\u4e14\u591a\u5b66\u79d1\u57fa\u51c6\u5982MMLU\u4e5f\u4e0d\u5305\u62ec\u6587\u5b66\u4f5c\u4e3a\u4e3b\u9898\u3002", "method": "\u63d0\u51fa\u4e86KRISTEVA\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u7ec6\u8bfb\u57fa\u51c6\uff0c\u5305\u542b1331\u4e2a\u591a\u9009\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u9010\u6b65\u589e\u52a0\u96be\u5ea6\u7684\u4efb\u52a1\u96c6\u6765\u6a21\u62df\u4e0d\u540c\u7684\u7ec6\u8bfb\u8fc7\u7a0b\u5143\u7d20\u3002", "result": "\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4e0e\u63a8\u7406\u6587\u5b66\u4f5c\u54c1\u65b9\u9762\u5c55\u73b0\u51fa\u4e86\u4e0d\u540c\u7a0b\u5ea6\u7684\u80fd\u529b\uff0c\u4f46\u572810\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "conclusion": "\u5c3d\u7ba1\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u67d0\u4e9b\u5927\u5b66\u7ea7\u522b\u7684\u7ec6\u8bfb\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u51c6\u786e\u6027\uff0849.7%-69.7%\uff09\uff0c\u4f46\u5728\u6211\u4eec11\u4e2a\u4efb\u52a1\u4e2d\u768410\u4e2a\u4efb\u52a1\u4e0a\uff0c\u5b83\u4eec\u7684\u8868\u73b0\u4ecd\u843d\u540e\u4e8e\u6709\u7ecf\u9a8c\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002"}}
{"id": "2505.09926", "pdf": "https://arxiv.org/pdf/2505.09926", "abs": "https://arxiv.org/abs/2505.09926", "authors": ["Bin-Bin Gao", "Yue Zhu", "Jiangtao Yan", "Yuezhi Cai", "Weixi Zhang", "Meng Wang", "Jun Liu", "Yong Liu", "Lei Wang", "Chengjie Wang"], "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 15 figures, 22 tables", "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.", "AI": {"tldr": "A new method called AdaptCLIP is proposed for universal visual anomaly detection, which improves performance across different domains.", "motivation": "To address the limitations of current methods in designing prompts, handling complex token interactions, and requiring additional fine-tuning for visual anomaly detection.", "method": "AdaptCLIP uses adaptive visual and textual representations learned alternately and incorporates both contextual and aligned residual features for comparative learning. It adds three simple adapters to CLIP models.", "result": "AdaptCLIP shows superior performance on 12 anomaly detection benchmarks from industrial and medical domains compared to existing methods.", "conclusion": "AdaptCLIP demonstrates the effectiveness of its approach in universal visual anomaly detection without additional fine-tuning."}}
{"id": "2505.09923", "pdf": "https://arxiv.org/pdf/2505.09923", "abs": "https://arxiv.org/abs/2505.09923", "authors": ["Minjung Shin", "Donghyun Kim", "Jeh-Kwang Ryu"], "title": "\"There Is No Such Thing as a Dumb Question,\" But There Are Good Ones", "categories": ["cs.AI"], "comment": "8 pages, 4 figures and 4 tables. This work has been accepted for\n  presentation as a poster with full paper publication at CogSci 2025. This is\n  the final submission", "summary": "Questioning has become increasingly crucial for both humans and artificial\nintelligence, yet there remains limited research comprehensively assessing\nquestion quality. In response, this study defines good questions and presents a\nsystematic evaluation framework. We propose two key evaluation dimensions:\nappropriateness (sociolinguistic competence in context) and effectiveness\n(strategic competence in goal achievement). Based on these foundational\ndimensions, a rubric-based scoring system was developed. By incorporating\ndynamic contextual variables, our evaluation framework achieves structure and\nflexibility through semi-adaptive criteria. The methodology was validated using\nthe CAUS and SQUARE datasets, demonstrating the ability of the framework to\naccess both well-formed and problematic questions while adapting to varied\ncontexts. As we establish a flexible and comprehensive framework for question\nevaluation, this study takes a significant step toward integrating questioning\nbehavior with structured analytical methods grounded in the intrinsic nature of\nquestioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u95ee\u9898\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u7ef4\u5ea6\uff08\u5408\u9002\u6027\u548c\u6709\u6548\u6027\uff09\u8bc4\u4f30\u95ee\u9898\u8d28\u91cf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u95ee\u9898\u8d28\u91cf\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u5b9a\u4e49\u548c\u8bc4\u4f30\u597d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\uff1a\u5408\u9002\u6027\uff08\u8bed\u5883\u4e2d\u7684\u793e\u4f1a\u8bed\u8a00\u80fd\u529b\uff09\u548c\u6709\u6548\u6027\uff08\u76ee\u6807\u5b9e\u73b0\u7684\u6218\u7565\u80fd\u529b\uff09\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u57fa\u7840\u7ef4\u5ea6\u5f00\u53d1\u4e86\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u5206\u7cfb\u7edf\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u534a\u81ea\u9002\u5e94\u6807\u51c6\u5b9e\u73b0\u4e86\u7ed3\u6784\u548c\u7075\u6d3b\u6027\u3002", "result": "\u901a\u8fc7CAUS\u548cSQUARE\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u826f\u597d\u548c\u6709\u95ee\u9898\u7684\u95ee\u9898\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u5168\u9762\u7684\u95ee\u9898\u8bc4\u4f30\u6846\u67b6\uff0c\u671d\u7740\u5c06\u63d0\u95ee\u884c\u4e3a\u4e0e\u57fa\u4e8e\u63d0\u95ee\u672c\u8d28\u7684\u7ed3\u6784\u5316\u5206\u6790\u65b9\u6cd5\u76f8\u7ed3\u5408\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.09733", "pdf": "https://arxiv.org/pdf/2505.09733", "abs": "https://arxiv.org/abs/2505.09733", "authors": ["Alpaslan Gokcen", "Ali Boyaci"], "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios.", "AI": {"tldr": "This study proposes a federated learning method to address data quality issues and demonstrates significant performance improvement on benchmark datasets.", "motivation": "Data quality issues like noisy labels, missing classes and imbalanced distributions significantly challenge the effectiveness of federated learning. Therefore, it is necessary to propose a method to address these problems.", "method": "This study proposes a federated learning methodology that systematically addresses data quality issues including noise, class imbalance and missing labels. The approach enhances data integrity through adaptive noise cleaning, collaborative conditional GAN-based synthetic data generation and robust federated model training.", "result": "Experimental evaluations on benchmark datasets show significant improvements in federated model performance under varying noise and class imbalance conditions.", "conclusion": "The proposed method effectively improves federated model performance especially macro-F1 score under different noise and class imbalance conditions. It provides a robust, scalable and privacy compliant solution for real-world federated learning."}}
{"id": "2505.09852", "pdf": "https://arxiv.org/pdf/2505.09852", "abs": "https://arxiv.org/abs/2505.09852", "authors": ["Apollinaire Poli Nemkova", "Sarath Chandra Lingareddy", "Sagnik Ray Choudhury", "Mark V. Albert"], "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.", "AI": {"tldr": "Large Language Models (LLMs) were investigated for their ability to predict violent conflict escalation and fatalities without external data. The parametric knowledge of LLMs was compared with non-parametric capabilities which use structured and unstructured context from conflict datasets and recent news reports. A two-part evaluation framework was used to evaluate the models' performance.", "motivation": "The study aimed to explore whether LLMs can be useful for early warning systems, humanitarian planning, and policy-making by predicting conflict escalation and fatalities.", "method": "The parametric knowledge of LLMs was compared with non-parametric capabilities using Retrieval-Augmented Generation (RAG). The models were evaluated in two parts: parametric and non-parametric settings.", "result": "The study found that LLMs have strengths and limitations for conflict forecasting and that augmenting them with structured external knowledge enhances their performance.", "conclusion": "LLMs show potential for conflict forecasting but require augmentation with structured external knowledge for better performance."}}
{"id": "2505.09927", "pdf": "https://arxiv.org/pdf/2505.09927", "abs": "https://arxiv.org/abs/2505.09927", "authors": ["Siqi Yin", "Shaolei Liu", "Manning Wang"], "title": "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.", "AI": {"tldr": "This paper proposes a novel source-free domain adaptation (SFDA) framework that introduces preadaptation to generate a preadapted model, a data-dependent frequency prompt for style translation, and a style-related layer fine-tuning strategy to improve the efficiency and quality of model adaptation in medical datasets.", "motivation": "Existing SFDA methods face challenges such as limited access to labeled source domain data in medical datasets, reliance on domain-specific image style translation and self-supervision techniques with suboptimal results, and inefficiency in training the entire model under limited supervision.", "method": "The proposed framework includes preadaptation for generating a preadapted model, a data-dependent frequency prompt for effective style translation, and a style-related layer fine-tuning strategy for efficient adaptation using target domain images and pseudo-labels.", "result": "Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks show that the proposed method outperforms existing state-of-the-art methods.", "conclusion": "The proposed SFDA framework improves the quality of pseudo-labels and the efficiency of adaptation, demonstrating superior performance compared to current methods."}}
{"id": "2505.09932", "pdf": "https://arxiv.org/pdf/2505.09932", "abs": "https://arxiv.org/abs/2505.09932", "authors": ["Kevin J McNamara", "Rhea Pritham Marpu"], "title": "Demystifying AI Agents: The Final Generation of Intelligence", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.MA"], "comment": null, "summary": "The trajectory of artificial intelligence (AI) has been one of relentless\nacceleration, evolving from rudimentary rule-based systems to sophisticated,\nautonomous agents capable of complex reasoning and interaction. This whitepaper\nchronicles this remarkable journey, charting the key technological\nmilestones--advancements in prompting, training methodologies, hardware\ncapabilities, and architectural innovations--that have converged to create the\nAI agents of today. We argue that these agents, exemplified by systems like\nOpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in\nAI development, potentially constituting the \"final generation\" of intelligence\nas we currently conceive it. We explore the capabilities and underlying\ntechnologies of these agents, grounded in practical examples, while also\nexamining the profound societal implications and the unprecedented pace of\nprogress that suggests intelligence is now doubling approximately every six\nmonths. The paper concludes by underscoring the critical need for wisdom and\nforesight in navigating the opportunities and challenges presented by this\npowerful new era of intelligence.", "AI": {"tldr": "This whitepaper discusses the development of AI from simple rule-based systems to advanced autonomous agents, highlighting key milestones and their societal impacts. It emphasizes the rapid advancement of AI, suggesting intelligence doubles every six months, and stresses the importance of wisdom in managing this new era.", "motivation": "To document the evolution of AI and highlight its significant advancements and societal implications.", "method": "Chronological documentation of AI milestones and analysis of their impact.", "result": "AI agents like ChatGPT and Grok represent the 'final generation' of current intelligence, showcasing complex reasoning and interaction capabilities.", "conclusion": "Wisdom and foresight are crucial for navigating the opportunities and challenges of the powerful new era of AI."}}
{"id": "2505.09742", "pdf": "https://arxiv.org/pdf/2505.09742", "abs": "https://arxiv.org/abs/2505.09742", "authors": ["Yuan-Hang Zhang", "Massimiliano Di Ventra"], "title": "A Generative Neural Annealer for Black-Box Combinatorial Optimization", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.NE"], "comment": "15 pages, 3 figures", "summary": "We propose a generative, end-to-end solver for black-box combinatorial\noptimization that emphasizes both sample efficiency and solution quality on NP\nproblems. Drawing inspiration from annealing-based algorithms, we treat the\nblack-box objective as an energy function and train a neural network to model\nthe associated Boltzmann distribution. By conditioning on temperature, the\nnetwork captures a continuum of distributions--from near-uniform at high\ntemperatures to sharply peaked around global optima at low\ntemperatures--thereby learning the structure of the energy landscape and\nfacilitating global optimization. When queries are expensive, the\ntemperature-dependent distributions naturally enable data augmentation and\nimprove sample efficiency. When queries are cheap but the problem remains hard,\nthe model learns implicit variable interactions, effectively \"opening\" the\nblack box. We validate our approach on challenging combinatorial tasks under\nboth limited and unlimited query budgets, showing competitive performance\nagainst state-of-the-art black-box optimizers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u7684\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u5728\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u4e3a\u4e86\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fd9\u79cd\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u4ece\u57fa\u4e8e\u9000\u706b\u7684\u7b97\u6cd5\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5c06\u9ed1\u76d2\u76ee\u6807\u89c6\u4e3a\u80fd\u91cf\u51fd\u6570\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u5efa\u6a21\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u3002\u901a\u8fc7\u8c03\u8282\u6e29\u5ea6\uff0c\u7f51\u7edc\u80fd\u591f\u6355\u6349\u4ece\u9ad8\u6e29\u5ea6\u4e0b\u7684\u63a5\u8fd1\u5747\u5300\u5206\u5e03\u5230\u4f4e\u6e29\u5ea6\u4e0b\u56f4\u7ed5\u5168\u5c40\u6700\u4f18\u89e3\u7684\u5c16\u5cf0\u5206\u5e03\uff0c\u4ece\u800c\u5b66\u4e60\u80fd\u91cf\u666f\u89c2\u7684\u7ed3\u6784\u5e76\u4fc3\u8fdb\u5168\u5c40\u4f18\u5316\u3002", "result": "\u6211\u4eec\u5728\u5177\u6709\u6709\u9650\u548c\u65e0\u9650\u67e5\u8be2\u9884\u7b97\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u7684\u7aef\u5230\u7aef\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u9ed1\u76d2\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728NP\u95ee\u9898\u4e0a\u5f3a\u8c03\u4e86\u6837\u672c\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002\u901a\u8fc7\u5c06\u9ed1\u76d2\u76ee\u6807\u89c6\u4e3a\u80fd\u91cf\u51fd\u6570\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u5efa\u6a21\u76f8\u5173\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u8be5\u7f51\u7edc\u4ece\u9ad8\u6e29\u5ea6\u4e0b\u7684\u63a5\u8fd1\u5747\u5300\u5206\u5e03\u5230\u4f4e\u6e29\u5ea6\u4e0b\u56f4\u7ed5\u5168\u5c40\u6700\u4f18\u89e3\u7684\u5c16\u5cf0\u5206\u5e03\uff0c\u4ece\u800c\u5b66\u4e60\u80fd\u91cf\u666f\u89c2\u7684\u7ed3\u6784\u5e76\u4fc3\u8fdb\u5168\u5c40\u4f18\u5316\u3002\u6211\u4eec\u5728\u5177\u6709\u6709\u9650\u548c\u65e0\u9650\u67e5\u8be2\u9884\u7b97\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u4f18\u5316\u5668\u7ade\u4e89\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09902", "pdf": "https://arxiv.org/pdf/2505.09902", "abs": "https://arxiv.org/abs/2505.09902", "authors": ["Martin Capdevila", "Esteban Villa Turek", "Ellen Karina Chumbe Fernandez", "Luis Felipe Polo Galvez", "Luis Cadavid", "Andrea Marroquin", "Rebeca Vargas Quesada", "Johanna Crew", "Nicole Vallejo Galarraga", "Christopher Rodriguez", "Diego Gutierrez", "Radhi Datla"], "title": "Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are, by definition, based on language. In an effort to\nunderscore the critical need for regional localized models, this paper examines\nprimary differences between variants of written Spanish across Latin America\nand Spain, with an in-depth sociocultural and linguistic contextualization\ntherein. We argue that these differences effectively constitute significant\ngaps in the quotidian use of Spanish among dialectal groups by creating\nsociolinguistic dissonances, to the extent that locale-sensitive AI models\nwould play a pivotal role in bridging these divides. In doing so, this approach\ninforms better and more efficient localization strategies that also serve to\nmore adequately meet inclusivity goals, while securing sustainable active daily\nuser growth in a major low-risk investment geographic area. Therefore,\nimplementing at least the proposed five sub variants of Spanish addresses two\nlines of action: to foment user trust and reliance on AI language models while\nalso demonstrating a level of cultural, historical, and sociolinguistic\nawareness that reflects positively on any internationalization strategy.", "AI": {"tldr": "This paper explores regional variations of written Spanish across Latin America and Spain, emphasizing their sociocultural and linguistic differences. It argues for the necessity of locale-sensitive AI models to bridge these gaps, promote inclusivity, and support sustainable user growth.", "motivation": "To highlight the importance of regional localized models due to significant differences in the quotidian use of Spanish among dialectal groups.", "method": "An in-depth analysis of sociocultural and linguistic contexts of Spanish variants.", "result": "Locale-sensitive AI models can play a pivotal role in bridging sociolinguistic dissonances and enhancing localization strategies.", "conclusion": "Implementing five sub-variants of Spanish can foster user trust, cultural awareness, and support internationalization strategies."}}
{"id": "2505.09935", "pdf": "https://arxiv.org/pdf/2505.09935", "abs": "https://arxiv.org/abs/2505.09935", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Quoc Dai Tran"], "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6VRU-CIPI\uff0c\u5b83\u5229\u7528GRU\u548cTransformer\u673a\u5236\u6765\u9884\u6d4b\u57ce\u5e02\u4ea4\u53c9\u8def\u53e3\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u7a7f\u8d8a\u610f\u56fe\uff0c\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u4e14\u901a\u8fc7\u4e0eI2V\u901a\u4fe1\u96c6\u6210\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4ea4\u53c9\u8def\u53e3\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u7406\u89e3\u5e76\u5728\u91ce\u5916\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u57ce\u5e02\u4ea4\u53c9\u8def\u53e3\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u9053\u8def\u4f7f\u7528\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u5176\u4e2d\u6700\u5173\u952e\u7684\u884c\u4e3a\u4e4b\u4e00\u662f\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u7a7f\u8d8a\u610f\u56fe\uff0c\u5bf9\u8fd9\u4e9b\u610f\u56fe\u7684\u8bef\u5224\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0e\u8fce\u9762\u800c\u6765\u7684\u8f66\u8f86\u53d1\u751f\u5371\u9669\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVRU-CIPI\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u5e8f\u5217\u6ce8\u610f\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4bVRU\u5728\u4ea4\u53c9\u8def\u53e3\u7684\u7a7f\u8d8a\u610f\u56fe\u3002\u8be5\u6a21\u578b\u4f7f\u7528Gated Recurrent Unit (GRU)\u6765\u6355\u83b7VRU\u79fb\u52a8\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u7f16\u7801\u4e0a\u4e0b\u6587\u548c\u7a7a\u95f4\u4f9d\u8d56\u6027\u4ee5\u9884\u6d4b\u7a7f\u8d8a\u65b9\u5411\u3002", "result": "\u5728UCF-VRU\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684VRU-CIPI\u6846\u67b6\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u4e3a96.45\uff05\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u6bcf\u79d233\u5e27\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVRU-CIPI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u57ce\u5e02\u4ea4\u53c9\u8def\u53e3\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u7a7f\u8d8a\u610f\u56fe\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u6355\u6349VRU\u8fd0\u52a8\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u7f16\u7801\u5173\u952e\u7684\u4e0a\u4e0b\u6587\u548c\u7a7a\u95f4\u4f9d\u8d56\u6027\u4ee5\u9884\u6d4b\u7a7f\u8d8a\u65b9\u5411\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728UCF-VRU\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u8fbe\u523096.45%\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u6bcf\u79d233\u5e27\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e0e\u57fa\u7840\u8bbe\u65bd\u5230\u8f66\u8f86\uff08I2V\uff09\u901a\u4fe1\u96c6\u6210\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u53ca\u65f6\u6fc0\u6d3b\u7a7f\u8d8a\u4fe1\u53f7\u5e76\u4e3a\u8fde\u63a5\u7684\u8f66\u8f86\u63d0\u4f9b\u65e9\u671f\u8b66\u544a\u6765\u4e3b\u52a8\u63d0\u9ad8\u4ea4\u53c9\u53e3\u7684\u5b89\u5168\u6027\uff0c\u4ece\u800c\u786e\u4fdd\u6240\u6709\u9053\u8def\u7528\u6237\u66f4\u987a\u7545\u548c\u5b89\u5168\u7684\u4ea4\u4e92\u3002"}}
{"id": "2505.09970", "pdf": "https://arxiv.org/pdf/2505.09970", "abs": "https://arxiv.org/abs/2505.09970", "authors": ["Mrinal Rawat", "Ambuje Gupta", "Rushil Goomer", "Alessandro Di Bari", "Neha Gupta", "Roberto Pieraccini"], "title": "Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "The ReAct (Reasoning + Action) capability in large language models (LLMs) has\nbecome the foundation of modern agentic systems. Recent LLMs, such as\nDeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through\nthe generation of ample intermediate tokens, which help build a strong premise\nbefore producing the final output tokens. In this paper, we introduce Pre-Act,\na novel approach that enhances the agent's performance by creating a multi-step\nexecution plan along with the detailed reasoning for the given user input. This\nplan incrementally incorporates previous steps and tool outputs, refining\nitself after each step execution until the final response is obtained. Our\napproach is applicable to both conversational and non-conversational agents. To\nmeasure the performance of task-oriented agents comprehensively, we propose a\ntwo-level evaluation framework: (1) turn level and (2) end-to-end. Our\nturn-level evaluation, averaged across five models, shows that our approach,\nPre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While\nthis approach is effective for larger models, smaller models crucial for\npractical applications, where latency and cost are key constraints, often\nstruggle with complex reasoning tasks required for agentic systems. To address\nthis limitation, we fine-tune relatively small models such as Llama 3.1 (8B &\n70B) using the proposed Pre-Act approach. Our experiments show that the\nfine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action\naccuracy (turn-level) and a 28% improvement in goal completion rate\n(end-to-end) on the Almita (out-of-domain) dataset.", "AI": {"tldr": "A new method called Pre-Act is introduced to improve the performance of agent systems by creating a multi-step execution plan with detailed reasoning. The method shows better performance than ReAct, especially when fine-tuning smaller models.", "motivation": "To enhance the performance of agent systems with better reasoning and action capabilities.", "method": "Introducing Pre-Act, which creates a multi-step execution plan with detailed reasoning for user inputs.", "result": "Pre-Act outperforms ReAct by 70% in Action Recall on the Almita dataset at the turn level. Fine-tuned smaller models also show significant improvements in action accuracy and goal completion rate.", "conclusion": "Pre-Act is an effective approach for improving agent system performance, particularly beneficial for smaller models in practical applications."}}
{"id": "2505.09756", "pdf": "https://arxiv.org/pdf/2505.09756", "abs": "https://arxiv.org/abs/2505.09756", "authors": ["Zhaoyang Shi"], "title": "Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration", "categories": ["cs.LG", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "We propose a new framework for multi-agent reinforcement learning (MARL),\nwhere the agents cooperate in a time-evolving network with latent community\nstructures and mixed memberships. Unlike traditional neighbor-based or fixed\ninteraction graphs, our community-based framework captures flexible and\nabstract coordination patterns by allowing each agent to belong to multiple\noverlapping communities. Each community maintains shared policy and value\nfunctions, which are aggregated by individual agents according to personalized\nmembership weights. We also design actor-critic algorithms that exploit this\nstructure: agents inherit community-level estimates for policy updates and\nvalue learning, enabling structured information sharing without requiring\naccess to other agents' policies. Importantly, our approach supports both\ntransfer learning by adapting to new agents or tasks via membership estimation,\nand active learning by prioritizing uncertain communities during exploration.\nTheoretically, we establish convergence guarantees under linear function\napproximation for both actor and critic updates. To our knowledge, this is the\nfirst MARL framework that integrates community structure, transferability, and\nactive learning with provable guarantees.", "AI": {"tldr": "A novel MARL framework integrating community structure, transferability, and active learning with theoretical guarantees.", "motivation": "To capture flexible and abstract coordination patterns in time-evolving networks with latent community structures.", "method": "Each agent belongs to multiple overlapping communities maintaining shared policy and value functions. Actor-critic algorithms exploit this structure.", "result": "Supports transfer learning and active learning while providing convergence guarantees under linear function approximation.", "conclusion": "This is the first MARL framework integrating community structure, transferability, and active learning with provable guarantees."}}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924", "abs": "https://arxiv.org/abs/2505.09924", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\n\\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6c34\u5370\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8elogits\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u6c34\u5370\u7b56\u7565\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0cAI\u751f\u6210\u6587\u672c\u7684\u6ee5\u7528\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u6c34\u5370\u6280\u672f\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6c34\u5370\u65b9\u6848\u5728\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6c34\u5370\u6846\u67b6\uff0c\u5305\u62ec\u4e32\u884c\u3001\u5e76\u884c\u548c\u6df7\u5408\u4e09\u79cd\u7b56\u7565\uff0c\u5e76\u901a\u8fc7token\u71b5\u548c\u8bed\u4e49\u71b5\u81ea\u9002\u5e94\u5730\u5d4c\u5165\u6c34\u5370\uff0c\u4ee5\u4f18\u5316\u68c0\u6d4b\u80fd\u529b\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4e0d\u540c\u7684\u6c34\u5370\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u4e14\u4ee3\u7801\u5df2\u7ecf\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2505.09939", "pdf": "https://arxiv.org/pdf/2505.09939", "abs": "https://arxiv.org/abs/2505.09939", "authors": ["Zhe Shan", "Lei Zhou", "Liu Mao", "Shaofan Chen", "Chuanqiu Ren", "Xia Xie"], "title": "Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IGARSS 2025", "summary": "In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.", "AI": {"tldr": "This study introduces a new remote sensing change detection task called non-registration change detection to handle emergencies like natural disasters. It proposes eight real-world scenarios for non-registration problems and transforms a registration change detection dataset into a non-registration version. It also shows that non-registration change detection can severely impact advanced methods.", "motivation": "To address the increasing number of emergencies such as natural disasters, anthropogenic accidents, and military strikes.", "method": "Systematically proposing eight scenarios for non-registration problems and developing image transformation schemes to convert a registration change detection dataset into a non-registration version.", "result": "Non-registration change detection can cause significant damage to state-of-the-art methods.", "conclusion": "Introduced a novel remote sensing change detection task and demonstrated its challenges and impacts on advanced methods."}}
{"id": "2505.10034", "pdf": "https://arxiv.org/pdf/2505.10034", "abs": "https://arxiv.org/abs/2505.10034", "authors": ["Changzeng Fu", "Zelin Fu", "Xinhe Kuang", "Jiacheng Dong", "Qi Zhang", "Kaifeng Su", "Yikai Su", "Wenbo Shi", "Junfeng Yao", "Yuliang Zhao", "Shiqi Zhao", "Jiadong Wang", "Siyang Song", "Chaoran Liu", "Yuichiro Yoshikawa", "Bj\u00f6rn Schuller", "Hiroshi Ishiguro"], "title": "The First MPDD Challenge: Multimodal Personality-aware Depression Detection", "categories": ["cs.AI", "68T07", "I.2.0; H.5.1"], "comment": "This paper has been accepted as part of the MPDD Challenge in the\n  ACMMM 2025 Grand Challenge", "summary": "Depression is a widespread mental health issue affecting diverse age groups,\nwith notable prevalence among college students and the elderly. However,\nexisting datasets and detection methods primarily focus on young adults,\nneglecting the broader age spectrum and individual differences that influence\ndepression manifestation. Current approaches often establish a direct mapping\nbetween multimodal data and depression indicators, failing to capture the\ncomplexity and diversity of depression across individuals. This challenge\nincludes two tracks based on age-specific subsets: Track 1 uses the\nMPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses\nthe MPDD-Young dataset for detecting depression in younger participants. The\nMultimodal Personality-aware Depression Detection (MPDD) Challenge aims to\naddress this gap by incorporating multimodal data alongside individual\ndifference factors. We provide a baseline model that fuses audio and video\nmodalities with individual difference information to detect depression\nmanifestations in diverse populations. This challenge aims to promote the\ndevelopment of more personalized and accurate de pression detection methods,\nadvancing mental health research and fostering inclusive detection systems.\nMore details are available on the official challenge website:\nhttps://hacilab.github.io/MPDDChallenge.github.io.", "AI": {"tldr": "The MPDD Challenge aims to improve depression detection by considering individual differences and using multimodal data, providing age-specific datasets and a baseline model.", "motivation": "Existing datasets and detection methods primarily focus on young adults, neglecting the broader age spectrum and individual differences that influence depression manifestation. Current approaches often fail to capture the complexity and diversity of depression across individuals.", "method": "A baseline model that fuses audio and video modalities with individual difference information is provided to detect depression manifestations in diverse populations.", "result": "The challenge includes two tracks based on age-specific subsets: Track 1 uses the MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses the MPDD-Young dataset for detecting depression in younger participants.", "conclusion": "The Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to address the gap in current depression detection methods by incorporating multimodal data alongside individual difference factors."}}
{"id": "2505.09768", "pdf": "https://arxiv.org/pdf/2505.09768", "abs": "https://arxiv.org/abs/2505.09768", "authors": ["Xiukun Wei", "Xueru Zhang"], "title": "Self-Consuming Generative Models with Adversarially Curated Data", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in generative models have made it increasingly difficult to\ndistinguish real data from model-generated synthetic data. Using synthetic data\nfor successive training of future model generations creates \"self-consuming\nloops\", which may lead to model collapse or training instability. Furthermore,\nsynthetic data is often subject to human feedback and curated by users based on\ntheir preferences. Ferbach et al. (2024) recently showed that when data is\ncurated according to user preferences, the self-consuming retraining loop\ndrives the model to converge toward a distribution that optimizes those\npreferences. However, in practice, data curation is often noisy or\nadversarially manipulated. For example, competing platforms may recruit\nmalicious users to adversarially curate data and disrupt rival models. In this\npaper, we study how generative models evolve under self-consuming retraining\nloops with noisy and adversarially curated data. We theoretically analyze the\nimpact of such noisy data curation on generative models and identify conditions\nfor the robustness of the retraining process. Building on this analysis, we\ndesign attack algorithms for competitive adversarial scenarios, where a\nplatform with a limited budget employs malicious users to misalign a rival's\nmodel from actual user preferences. Experiments on both synthetic and\nreal-world datasets demonstrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "This paper studies the impact of noisy and adversarially curated data on generative models under self-consuming retraining loops. It provides theoretical analysis and designs attack algorithms for competitive scenarios.", "motivation": "To understand how generative models evolve under self-consuming retraining loops with noisy and adversarially curated data, especially in competitive environments.", "method": "Theoretical analysis and designing attack algorithms.", "result": "Identified conditions for the robustness of the retraining process and demonstrated the effectiveness of the proposed algorithms on synthetic and real-world datasets.", "conclusion": "Generative models can be significantly affected by noisy and adversarially curated data in self-consuming retraining loops, but certain conditions can ensure robustness."}}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930", "abs": "https://arxiv.org/abs/2505.09930", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Prompt optimization (PO) offers a practical alternative to fine-tuning large\nlanguage models (LLMs), enabling performance improvements without altering\nmodel weights. Existing methods typically rely on advanced, large-scale LLMs\nlike GPT-4 to generate optimized prompts. However, due to limited downward\ncompatibility, verbose, instruction-heavy prompts from advanced LLMs can\noverwhelm lightweight inference models and degrade response quality. In this\nwork, we rethink prompt optimization through the lens of interpretable design.\nWe first identify a set of model-agnostic prompt quality merits and empirically\nvalidate their effectiveness in enhancing prompt and response quality. We then\nintroduce MePO, a merit-guided, lightweight, and locally deployable prompt\noptimizer trained on our preference dataset built from merit-aligned prompts\ngenerated by a lightweight LLM. Unlike prior work, MePO avoids online\noptimization reliance, reduces cost and privacy concerns, and, by learning\nclear, interpretable merits, generalizes effectively to both large-scale and\nlightweight inference models. Experiments demonstrate that MePO achieves better\nresults across diverse tasks and model types, offering a scalable and robust\nsolution for real-world deployment. Our model and dataset are available at:\nhttps://github.com/MidiyaZhu/MePO", "AI": {"tldr": "Prompt optimization (PO) is a method for improving large language models' performance without changing model weights. This paper introduces MePO, a new PO approach that uses interpretable design and lightweight training to optimize prompts effectively for both large and small models.", "motivation": "The motivation is to address the limitations of existing PO methods that rely on advanced LLMs, which can overwhelm lightweight models and degrade response quality.", "method": "MePO is a merit-guided, lightweight, and locally deployable prompt optimizer trained on a preference dataset built from merit-aligned prompts generated by a lightweight LLM.", "result": "MePO outperforms previous PO methods across diverse tasks and model types, providing a scalable and robust solution for real-world deployment.", "conclusion": "MePO offers a practical, efficient, and privacy-conscious way to optimize prompts for various language models."}}
{"id": "2505.09943", "pdf": "https://arxiv.org/pdf/2505.09943", "abs": "https://arxiv.org/abs/2505.09943", "authors": ["Jiakun Deng", "Kexuan Li", "Xingye Cui", "Jiaxuan Li", "Chang Long", "Tian Pu", "Zhenming Peng"], "title": "CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.", "AI": {"tldr": "This paper presents CSPENet, a new method for infrared small target detection that improves target localization and contour representation under dense clutter.", "motivation": "Existing methods struggle with accurately localizing dim targets and perceiving contour information in dense clutter environments.", "method": "CSPENet includes a surround-convergent prior extraction module, a dual-branch priors embedding architecture, and an attention-guided feature enhancement module.", "result": "Experiments on three public datasets show that CSPENet outperforms other state-of-the-art methods.", "conclusion": "The proposed CSPENet method significantly enhances infrared small target detection performance."}}
{"id": "2505.10074", "pdf": "https://arxiv.org/pdf/2505.10074", "abs": "https://arxiv.org/abs/2505.10074", "authors": ["Mohamed Abdelmagied", "Mohamed Amine Chatti", "Shoeb Joarder", "Qurat Ul Ain", "Rawaa Alatrash"], "title": "Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs", "categories": ["cs.AI", "cs.CY"], "comment": "Accepted at EMOOCs 2025", "summary": "Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.", "AI": {"tldr": "This paper proposes a Graph RAG pipeline that uses Educational Knowledge Graphs and Personal Knowledge Graphs to guide learners in understanding knowledge concepts in MOOC platforms.", "motivation": "The lack of direct interaction in MOOCs and the unreliability of LLMs due to hallucinations motivated the research.", "method": "The proposed pipeline includes a PKG-based Question Generation method and an EduKG-based Question Answering method.", "result": "The evaluation with 3 expert instructors on 3 different MOOCs showed the potential of Graph RAG in providing a personalized learning experience.", "conclusion": "Graph RAG can empower learners to understand new knowledge concepts in MOOCs through personalized guidance."}}
{"id": "2505.09792", "pdf": "https://arxiv.org/pdf/2505.09792", "abs": "https://arxiv.org/abs/2505.09792", "authors": ["Michael Kamfonas"], "title": "Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This case study applies a phased hyperparameter optimization process to\ncompare multitask natural language model variants that utilize multiphase\nlearning rate scheduling and optimizer parameter grouping. We employ short,\nBayesian optimization sessions that leverage multi-fidelity, hyperparameter\nspace pruning, progressive halving, and a degree of human guidance. We utilize\nthe Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn\nGaussian process minimization. Initially, we use efficient low-fidelity sprints\nto prune the hyperparameter space. Subsequent sprints progressively increase\ntheir model fidelity and employ hyperband pruning for efficiency. A second\naspect of our approach is using a meta-learner to tune threshold values to\nresolve classification probabilities during inference. We demonstrate our\nmethod on a collection of variants of the 2021 Joint Entity and Relation\nExtraction model proposed by Eberts and Ulges.", "AI": {"tldr": "This case study compares different multitask natural language model variants using a phased hyperparameter optimization process.", "motivation": "To improve the performance of multitask natural language models by optimizing their hyperparameters.", "method": "A phased hyperparameter optimization process with short Bayesian optimization sessions leveraging multi-fidelity, hyperparameter space pruning, progressive halving, and human guidance.", "result": "The method was demonstrated on variants of the 2021 Joint Entity and Relation Extraction model proposed by Eberts and Ulges.", "conclusion": "The phased hyperparameter optimization process can effectively optimize multitask natural language models."}}
{"id": "2505.09945", "pdf": "https://arxiv.org/pdf/2505.09945", "abs": "https://arxiv.org/abs/2505.09945", "authors": ["Deeksha Prahlad", "Chanhee Lee", "Dongha Kim", "Hokeun Kim"], "title": "Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)", "summary": "The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.", "AI": {"tldr": "This paper proposes an approach using retrieval augmented generation with knowledge graphs to improve the accuracy of large language model's personalized response generation.", "motivation": "LLMs often undergo high levels of over-fitting, resulting in the generation of extra and incorrect data, thus causing hallucinations in output generation. One of the root causes of such problems is the lack of timely, factual, and personalized information fed to the LLM.", "method": "introducing retrieval augmented generation (RAG) using knowledge graphs (KGs) to assist the LLM in personalized response generation tailored to the users.", "result": "Our approach works significantly better in understanding personal information and generating accurate responses compared to the baseline LLMs using personal data as text inputs, with a moderate reduction in response time.", "conclusion": "Our experimental results show that our approach works significantly better in understanding personal information and generating accurate responses compared to the baseline LLMs using personal data as text inputs, with a moderate reduction in response time."}}
{"id": "2505.09965", "pdf": "https://arxiv.org/pdf/2505.09965", "abs": "https://arxiv.org/abs/2505.09965", "authors": ["Hao Yang", "Tao Tan", "Shuai Tan", "Weiqin Yang", "Kunyan Cai", "Calvin Chen", "Yue Sun"], "title": "MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.", "AI": {"tldr": "MambaControl uses a novel framework combining selective state-space modeling with diffusion processes to predict Alzheimer's disease progression with high accuracy.", "motivation": "Existing methods struggle with longitudinal dependencies and structural consistency in progressive disorders.", "method": "Integrates selective state-space modelling with diffusion processes, uses Mamba-based long-range modelling combined with graph-guided anatomical control, introduces Fourier-enhanced spectral graph representations.", "result": "State-of-the-art performance in Alzheimer's disease prediction with improved progression prediction quality and anatomical fidelity.", "conclusion": "MambaControl has potential for personalized prognosis and clinical decision support."}}
{"id": "2505.10093", "pdf": "https://arxiv.org/pdf/2505.10093", "abs": "https://arxiv.org/abs/2505.10093", "authors": ["Hsuan-Lei Shao"], "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI", "categories": ["cs.AI", "cs.CL", "I.2.4; H.3.3; J.5"], "comment": "4 pages, 4 figures", "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.", "AI": {"tldr": "This study uses AI techniques to extract information from 1,367 Taiwan studies papers and creates a knowledge graph and vector database to help explore the field's intellectual trajectories and research gaps.", "motivation": "There is a need to systematically revisit and reorganize decades of Taiwan-based China Studies scholarship.", "method": "Generative AI techniques and large language models are used to extract entity relation triples from academic texts, which are then visualized through a D3.js based system to form a knowledge graph and vector database.", "result": "The infrastructure created allows users to explore conceptual nodes and semantic relationships across the corpus, revealing intellectual trajectories, thematic clusters, and research gaps.", "conclusion": "This work shows how generative AI can enhance scholarly access to literature and support a reimagined scholarly infrastructure for regional knowledge systems."}}
{"id": "2505.09810", "pdf": "https://arxiv.org/pdf/2505.09810", "abs": "https://arxiv.org/abs/2505.09810", "authors": ["Daniel Waddington", "Cornel Constantinescu"], "title": "Lossless Compression for LLM Tensor Incremental Snapshots", "categories": ["cs.LG"], "comment": null, "summary": "During the training of Large Language Models (LLMs), tensor data is\nperiodically \"checkpointed\" to persistent storage to allow recovery of work\ndone in the event of failure. The volume of data that must be copied during\neach checkpoint, even when using reduced-precision representations such as\nbfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be\nmoved across a network and written to a storage system before the next epoch\noccurs. With a view to ultimately building an optimized checkpointing solution,\nthis paper presents experimental analysis of checkpoint data used to derive a\ndesign that maximizes the use of lossless compression to reduce the volume of\ndata. We examine how tensor data and its compressibility evolve during model\ntraining and evaluate the efficacy of existing common off-the-shelf general\npurpose compression engines combined with known data optimization techniques\nsuch as byte-grouping and incremental delta compression.\n  Leveraging our analysis we have built an effective compression solution,\nknown as Language Model Compressor (LMC), which is based on byte-grouping and\nHuffman encoding. LMC offers more compression performance than the best\nalternative (BZ2) but with an order-of-magnitude reduction in the time needed\nto perform the compression. We show that a 16-core parallel implementation of\nLMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76\nGiB/s respectively. This increase in performance ultimately reduces the CPU\nresources needed and provides more time to copy the data to the storage system\nbefore the next epoch thus allowing for higher-frequency checkpoints.", "AI": {"tldr": "This paper analyzes the checkpointing process in Large Language Models (LLMs) training and proposes an effective compression solution called Language Model Compressor (LMC) which improves compression performance while reducing processing time.", "motivation": "To optimize the checkpointing process in LLMs training by reducing the volume of checkpoint data.", "method": "Experimental analysis of checkpoint data to maximize lossless compression, leveraging byte-grouping and Huffman encoding.", "result": "LMC outperforms BZ2 in compression performance with significantly less processing time. A 16-core parallel implementation achieves 2.78 GiB/s compression and 3.76 GiB/s decompression throughput.", "conclusion": "The proposed LMC solution reduces CPU resource needs and allows for higher-frequency checkpoints."}}
{"id": "2505.10013", "pdf": "https://arxiv.org/pdf/2505.10013", "abs": "https://arxiv.org/abs/2505.10013", "authors": ["Lake Yin", "Fan Huang"], "title": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 1 figure", "summary": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.", "AI": {"tldr": "This paper introduces DIF (Demographic Implicit Fairness), a new method to benchmark implicit bias in large language models.", "motivation": "Concerns over potential biases in large language models inherited from training data.", "method": "Developed a method for calculating DIF by evaluating preexisting LLM logic and math problem datasets with sociodemographic personas.", "result": "DIF can statistically validate the presence of implicit bias in LLM behavior.", "conclusion": "There is an inverse trend between question answering accuracy and implicit bias."}}
{"id": "2505.09967", "pdf": "https://arxiv.org/pdf/2505.09967", "abs": "https://arxiv.org/abs/2505.09967", "authors": ["Liqian Deng"], "title": "TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.", "AI": {"tldr": "This paper presents a novel framework for facial expression recognition in the wild, focusing on Texture Key Driver Factors (TKDF) to capture subtle and localized texture cues. The proposed method uses a Texture-Aware Feature Extractor (TAFE) and Dual Contextual Information Filtering (DCIF) to achieve state-of-the-art performance on RAF-DB and KDEF datasets.", "motivation": "To address the challenges of recognizing facial expressions due to subtle and localized features and complex variations in facial appearance.", "method": "Introduce a framework focusing on Texture Key Driver Factors (TKDF), using a Texture-Aware Feature Extractor (TAFE) with a ResNet-based backbone enhanced with multi-branch attention and Dual Contextual Information Filtering (DCIF) for refining features.", "result": "The method achieves state-of-the-art performance on RAF-DB and KDEF datasets.", "conclusion": "Incorporating Texture Key Driver Factors (TKDF) into Facial Expression Recognition (FER) pipelines is effective and robust."}}
{"id": "2505.10188", "pdf": "https://arxiv.org/pdf/2505.10188", "abs": "https://arxiv.org/abs/2505.10188", "authors": ["Felix Liedeker", "Olivia Sanchez-Graillet", "Moana Seidler", "Christian Brandt", "J\u00f6rg Wellmer", "Philipp Cimiano"], "title": "A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support", "categories": ["cs.AI"], "comment": "Presented at 'The First Workshop on Natural Language Argument-Based\n  Explanations', co-located with ECAI 2024", "summary": "As the field of healthcare increasingly adopts artificial intelligence, it\nbecomes important to understand which types of explanations increase\ntransparency and empower users to develop confidence and trust in the\npredictions made by machine learning (ML) systems. In shared decision-making\nscenarios where doctors cooperate with ML systems to reach an appropriate\ndecision, establishing mutual trust is crucial. In this paper, we explore\ndifferent approaches to generating explanations in eXplainable AI (XAI) and\nmake their underlying arguments explicit so that they can be evaluated by\nmedical experts. In particular, we present the findings of a user study\nconducted with physicians to investigate their perceptions of various types of\nAI-generated explanations in the context of diagnostic decision support. The\nstudy aims to identify the most effective and useful explanations that enhance\nthe diagnostic process. In the study, medical doctors filled out a survey to\nassess different types of explanations. Further, an interview was carried out\npost-survey to gain qualitative insights on the requirements of explanations\nincorporated in diagnostic decision support. Overall, the insights gained from\nthis study contribute to understanding the types of explanations that are most\neffective.", "AI": {"tldr": "This paper explores different approaches for generating explanations in XAI and evaluates their effectiveness through a user study with physicians.", "motivation": "To understand which types of explanations increase transparency and build trust between doctors and ML systems in shared decision-making scenarios.", "method": "A user study involving physicians who completed surveys and participated in interviews to assess different types of AI-generated explanations.", "result": "The study identified the most effective and useful explanations that improve the diagnostic process.", "conclusion": "The findings contribute to understanding the types of explanations that are most effective in enhancing the diagnostic process."}}
{"id": "2505.09812", "pdf": "https://arxiv.org/pdf/2505.09812", "abs": "https://arxiv.org/abs/2505.09812", "authors": ["Anastasija Tashkova", "Stefan Eftimov", "Bojan Ristov", "Slobodan Kalajdziski"], "title": "Comparative Analysis of Stroke Prediction Models Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Stroke remains one of the most critical global health challenges, ranking as\nthe second leading cause of death and the third leading cause of disability\nworldwide. This study explores the effectiveness of machine learning algorithms\nin predicting stroke risk using demographic, clinical, and lifestyle data from\nthe Stroke Prediction Dataset. By addressing key methodological challenges such\nas class imbalance and missing data, we evaluated the performance of multiple\nmodels, including Logistic Regression, Random Forest, and XGBoost. Our results\ndemonstrate that while these models achieve high accuracy, sensitivity remains\na limiting factor for real-world clinical applications. In addition, we\nidentify the most influential predictive features and propose strategies to\nimprove machine learning-based stroke prediction. These findings contribute to\nthe development of more reliable and interpretable models for the early\nassessment of stroke risk.", "AI": {"tldr": "This study examines the effectiveness of various machine learning algorithms in predicting stroke risk with the aim to address challenges like class imbalance and missing data. The models, though accurate, show low sensitivity which limits their application in clinical settings.", "motivation": "To develop more reliable and interpretable models for the early assessment of stroke risk due to its significant impact on global health.", "method": "Using demographic, clinical, and lifestyle data from the Stroke Prediction Dataset, evaluating Logistic Regression, Random Forest, and XGBoost models while handling class imbalance and missing data.", "result": "High accuracy was achieved by the models but sensitivity remains low, limiting practical clinical use.", "conclusion": "Identifies influential predictive features and suggests strategies to improve machine learning-based stroke prediction."}}
{"id": "2505.10063", "pdf": "https://arxiv.org/pdf/2505.10063", "abs": "https://arxiv.org/abs/2505.10063", "authors": ["Han Peng", "Jinhao Jiang", "Zican Dong", "Wayne Xin Zhao", "Lei Fang"], "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability", "categories": ["cs.CL"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.", "AI": {"tldr": "A novel two-stage method named CAFE is introduced to improve multi-document question-answering by progressively reducing the impact of irrelevant documents.", "motivation": "Existing methods face challenges in balancing retrieval precision and recall for long-context inputs in large language models.", "method": "Two-stage coarse-to-fine approach: Coarse-grained filtering and fine-grained steering.", "result": "Experiments show that CAFE surpasses baselines, improving SubEM by 22.1% and 13.7% over SFT and RAG methods respectively.", "conclusion": "CAFE effectively enhances multi-document question-answering capabilities in large language models."}}
{"id": "2505.09971", "pdf": "https://arxiv.org/pdf/2505.09971", "abs": "https://arxiv.org/abs/2505.09971", "authors": ["Yuan Gao", "Shaobo Xia", "Sheng Nie", "Cheng Wang", "Xiaohuan Xi", "Bisheng Yang"], "title": "APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "18 pages,12 figures", "summary": "Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.", "AI": {"tldr": "Proposed APCoTTA, a Continuous Test-Time Adaptation method for ALS point cloud semantic segmentation, improving performance with novel mechanisms and creating new benchmarks.", "motivation": "To solve the problem of domain shifts in ALS point clouds and improve model performance in evolving, unlabeled target domains.", "method": "Dynamic trainable layer selection module, entropy-based consistency loss, and random parameter interpolation mechanism.", "result": "APCoTTA outperforms direct inference on two benchmarks.", "conclusion": "APCoTTA improves performance on two benchmarks with mIoU improvements of about 9% and 14%."}}
{"id": "2505.10278", "pdf": "https://arxiv.org/pdf/2505.10278", "abs": "https://arxiv.org/abs/2505.10278", "authors": ["Taian Guo", "Haiyang Shen", "Jinsheng Huang", "Zhengyang Mao", "Junyu Luo", "Zhuoru Chen", "Xuhui Liu", "Bingyu Xia", "Luchen Liu", "Yun Ma", "Ming Zhang"], "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction", "categories": ["cs.AI"], "comment": null, "summary": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.", "AI": {"tldr": "This paper presents MASS, a multi-agent system based on large language models for portfolio construction. It achieves continuous excess returns through large-scale simulations and end-to-end optimization, outperforming six state-of-the-art baselines across three challenging A-share stock pools.", "motivation": "Existing LLM-based multi-agent systems are limited to pure simulations or predefined workflows, reducing their applicability and effectiveness.", "method": "MASS progressively increases the number of agents for large-scale simulations and optimizes agent distribution end-to-end via reverse optimization.", "result": "Performance experiments, ablation studies, backtesting experiments, experiments on updated data and stock pools, scaling experiments, parameter sensitivity experiments, and visualization experiments show MASS's superiority.", "conclusion": "The paradigm established by MASS can be expanded to other similar tasks, and its implementation is open-sourced."}}
{"id": "2505.09820", "pdf": "https://arxiv.org/pdf/2505.09820", "abs": "https://arxiv.org/abs/2505.09820", "authors": ["Sajib Biswas", "Mao Nishino", "Samuel Jacob Chacko", "Xiuwen Liu"], "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8d8a\u72f1\u653b\u51fb\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7cfb\u7edf\u5730\u7406\u89e3\u5b83\u4eec\u5bf9\u4e8e\u63d0\u9ad8\u5176\u5b89\u5168\u6027\u548c\u53d1\u6325\u5176\u5168\u90e8\u6f5c\u529b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u6709\u6548\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6280\u672f\u6765\u5b9e\u73b0\u5bf9LLMs\u7684\u8d8a\u72f1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4f7f\u7528\u6307\u6570\u68af\u5ea6\u4e0b\u964d\u548cBregman\u6295\u5f71\u65b9\u6cd5\u7684\u5185\u5728\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u4f18\u5316\u7684\u4e00\u70ed\u7f16\u7801\u59cb\u7ec8\u4fdd\u6301\u5728\u6982\u7387\u5355\u5f62\u5185\u3002", "result": "\u8be5\u6280\u672f\u5728\u4e94\u4e2a\u5f00\u6e90LLMs\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u5728\u56db\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6280\u672f\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u5176\u4ed6\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u8d8a\u72f1\u6280\u672f\u3002", "conclusion": "\u8be5\u6280\u672f\u5728\u51e0\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u4e0a\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u8d8a\u72f1\uff0c\u4e0e\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u8d8a\u72f1\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u6280\u672f\u7684\u6210\u529f\u7387\u66f4\u9ad8\u4e14\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2505.10066", "pdf": "https://arxiv.org/pdf/2505.10066", "abs": "https://arxiv.org/abs/2505.10066", "authors": ["Michael Fire", "Yitzhak Elbazis", "Adi Wasenstein", "Lior Rokach"], "title": "Dark LLMs: The Growing Threat of Unaligned AI Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "68T50, 68T05, 68P25", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.", "AI": {"tldr": "Large Language Models (LLMs) are susceptible to jailbreaking due to unfiltered training data. This study reveals a universal jailbreak attack that compromises multiple state-of-the-art models, enabling harmful outputs. Despite responsible disclosure, major LLM providers have shown inadequate responses.", "motivation": "To identify the growing threat of dark LLMs and unethical modifications through jailbreak techniques.", "method": "Uncovered a universal jailbreak attack compromising multiple state-of-the-art LLMs.", "result": "The attack effectively enables LLMs to answer almost any question and produce harmful outputs upon request.", "conclusion": "Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated."}}
{"id": "2505.09986", "pdf": "https://arxiv.org/pdf/2505.09986", "abs": "https://arxiv.org/abs/2505.09986", "authors": ["Yimin Zhou", "Yichong Xia", "Sicheng Pan", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.", "AI": {"tldr": "An underwater image compression algorithm called HQUIC is introduced, which uses an ALTC module to predict attenuation coefficients and global light information, a codebook to extract common objects, and dynamic weighting of multi-scale frequency components to improve compression efficiency.", "motivation": "Contemporary underwater image compression algorithms do not fully utilize the unique characteristics of underwater scenes, leading to suboptimal performance.", "method": "HQUIC employs an ALTC module to predict attenuation coefficients and global light information, a codebook to extract common objects, and dynamically weights multi-scale frequency components.", "result": "HQUIC outperforms state-of-the-art compression methods on diverse underwater datasets.", "conclusion": "HQUIC is a novel underwater image compression algorithm that improves compression efficiency by exploiting underwater-image-specific features."}}
{"id": "2505.10309", "pdf": "https://arxiv.org/pdf/2505.10309", "abs": "https://arxiv.org/abs/2505.10309", "authors": ["Tuan Dung Nguyen", "Duncan J. Watts", "Mark E. Whiting"], "title": "Empirically evaluating commonsense intelligence in large language models with large-scale human judgments", "categories": ["cs.AI", "cs.HC", "cs.SI"], "comment": null, "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.", "AI": {"tldr": "This paper proposes a new method to evaluate common sense in AI, particularly in large language models (LLMs), by considering the diversity among human judgments. It finds that most LLMs perform below the human median in individual common sense tasks and correlate modestly with real humans in simulating a hypothetical population.", "motivation": "The paper challenges the assumption that human common sense is homogeneous and emphasizes the importance of incorporating observed human diversity into AI evaluations.", "method": "The proposed method measures the correspondence between a model's judgment and that of a human population, treating LLMs as independent survey respondents or simulators of a hypothetical population.", "result": "Smaller, open-weight models outperform larger, proprietary models in both individual and population-based evaluations.", "conclusion": "The study suggests that AI models should be adapted to different human collectivities with diverse knowledge bases."}}
{"id": "2505.09822", "pdf": "https://arxiv.org/pdf/2505.09822", "abs": "https://arxiv.org/abs/2505.09822", "authors": ["Changhao Shi", "Gal Mishne"], "title": "Learning Kronecker-Structured Graphs from Smooth Signals", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Graph learning, or network inference, is a prominent problem in graph signal\nprocessing (GSP). GSP generalizes the Fourier transform to non-Euclidean\ndomains, and graph learning is pivotal to applying GSP when these domains are\nunknown. With the recent prevalence of multi-way data, there has been growing\ninterest in product graphs that naturally factorize dependencies across\ndifferent ways. However, the types of graph products that can be learned are\nstill limited for modeling diverse dependency structures. In this paper, we\nstudy the problem of learning a Kronecker-structured product graph from smooth\nsignals. Unlike the more commonly used Cartesian product, the Kronecker product\nmodels dependencies in a more intricate, non-separable way, but posits harder\nconstraints on the graph learning problem. To tackle this non-convex problem,\nwe propose an alternating scheme to optimize each factor graph and provide\ntheoretical guarantees for its asymptotic convergence. The proposed algorithm\nis also modified to learn factor graphs of the strong product. We conduct\nexperiments on synthetic and real-world graphs and demonstrate our approach's\nefficacy and superior performance compared to existing methods.", "AI": {"tldr": "This paper studies learning a Kronecker-structured product graph from smooth signals, proposing an alternating optimization scheme with theoretical convergence guarantees, and demonstrates superior performance over existing methods.", "motivation": "To model diverse dependency structures in multi-way data using the more intricate Kronecker product instead of the commonly used Cartesian product.", "method": "Proposing an alternating optimization scheme to tackle the non-convex problem of learning Kronecker-structured product graphs, with modifications for strong product graphs.", "result": "The proposed method demonstrates efficacy and superior performance compared to existing methods through experiments on synthetic and real-world graphs.", "conclusion": "Learning Kronecker-structured product graphs is pivotal for applying GSP to unknown domains with multi-way data, and the proposed alternating optimization scheme provides theoretical guarantees and practical advantages."}}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081", "abs": "https://arxiv.org/abs/2505.10081", "authors": ["Wisdom Aduah", "Francois Meyer"], "title": "Designing and Contextualising Probes for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.", "AI": {"tldr": "This paper investigates linguistic knowledge in pretrained language models (PLMs) for African languages, finding that PLMs adapted for these languages encode more linguistic information than massively multilingual PLMs.", "motivation": "To understand why PLMs for African languages are improving and what linguistic knowledge they encode.", "method": "Training layer-wise probes for six typologically diverse African languages and designing control tasks for the MasakhaPOS dataset.", "result": "PLMs adapted for African languages encode more linguistic information about target languages than massively multilingual PLMs. Token-level syntactic information is concentrated in middle-to-last layers, while sentence-level semantic information is distributed across all layers. Performance reflects internal knowledge of PLMs rather than probe memorisation.", "conclusion": "This study highlights the internal mechanisms underlying the success of strategies like active learning and multilingual adaptation by applying established interpretability techniques to African-language PLMs."}}
{"id": "2505.09990", "pdf": "https://arxiv.org/pdf/2505.09990", "abs": "https://arxiv.org/abs/2505.09990", "authors": ["Long Cheng", "Jiafei Duan", "Yi Ru Wang", "Haoquan Fang", "Boyang Li", "Yushan Huang", "Elvis Wang", "Ainaz Eftekhar", "Jason Lee", "Wentao Yuan", "Rose Hendrix", "Noah A. Smith", "Fei Xia", "Dieter Fox", "Ranjay Krishna"], "title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing", "categories": ["cs.CV"], "comment": "10 Pages, Dataset and code:https://pointarena.github.io/", "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/", "AI": {"tldr": "PointArena\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6307\u5411\u80fd\u529b\u7684\u7efc\u5408\u5e73\u53f0\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aPoint-Bench\u6570\u636e\u96c6\u3001Point-Battle\u4ea4\u4e92\u5f0f\u7ade\u6280\u573a\u548cPoint-Act\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u3002\u7814\u7a76\u53d1\u73b0Molmo-72B\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u7279\u5b9a\u4efb\u52a1\u7684\u6709\u76d1\u7763\u8bad\u7ec3\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u53ea\u5173\u6ce8\u6307\u4ee3\u7269\u4f53\u5b9a\u4f4d\u4efb\u52a1\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u63a8\u7406\u573a\u666f\u4e2d\u7684\u6307\u5411\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86PointArena\u5e73\u53f0\uff0c\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1aPoint-Bench\u6570\u636e\u96c6\u3001Point-Battle\u7ade\u6280\u573a\u548cPoint-Act\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u3002\u5bf9\u591a\u79cd\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002", "result": "Molmo-72B\u5728\u6240\u6709\u8bc4\u4f30\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u6709\u76d1\u7763\u7684\u6307\u5411\u4efb\u52a1\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u5728\u591a\u9636\u6bb5\u8bc4\u4f30\u7ba1\u9053\u4e2d\u89c2\u5bdf\u5230\u5f3a\u76f8\u5173\u6027\uff0c\u8868\u660e\u7cbe\u786e\u7684\u6307\u5411\u80fd\u529b\u5bf9\u4e8e\u8fde\u63a5\u62bd\u8c61\u63a8\u7406\u4e0e\u5b9e\u9645\u4e16\u754c\u884c\u52a8\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u7cbe\u786e\u6307\u5411\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7PointArena\u5e73\u53f0\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u6307\u5411\u80fd\u529b\u3002"}}
{"id": "2505.10328", "pdf": "https://arxiv.org/pdf/2505.10328", "abs": "https://arxiv.org/abs/2505.10328", "authors": ["Alvin Combrink", "Stephie Do", "Kristofer Bengtsson", "Sabino Francesco Roselli", "Martin Fabian"], "title": "A Comparative Study of SMT and MILP for the Nurse Rostering Problem", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 3 figures", "summary": "The effects of personnel scheduling on the quality of care and working\nconditions for healthcare personnel have been thoroughly documented. However,\nthe ever-present demand and large variation of constraints make healthcare\nscheduling particularly challenging. This problem has been studied for decades,\nwith limited research aimed at applying Satisfiability Modulo Theories (SMT).\nSMT has gained momentum within the formal verification community in the last\ndecades, leading to the advancement of SMT solvers that have been shown to\noutperform standard mathematical programming techniques.\n  In this work, we propose generic constraint formulations that can model a\nwide range of real-world scheduling constraints. Then, the generic constraints\nare formulated as SMT and MILP problems and used to compare the respective\nstate-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired\nrostering problems. Experimental results show how each solver excels for\ncertain types of problems; the MILP solver generally performs better when the\nproblem is highly constrained or infeasible, while the SMT solver performs\nbetter otherwise. On real-world inspired problems containing a more varied set\nof shifts and personnel, the SMT solver excels. Additionally, it was noted\nduring experimentation that the SMT solver was more sensitive to the way the\ngeneric constraints were formulated, requiring careful consideration and\nexperimentation to achieve better performance. We conclude that SMT-based\nmethods present a promising avenue for future research within the domain of\npersonnel scheduling.", "AI": {"tldr": "This work proposes generic constraint formulations for healthcare scheduling and compares SMT and MILP solvers, showing each excels at certain types of problems.", "motivation": "To address the challenges in healthcare scheduling with Satisfiability Modulo Theories (SMT).", "method": "Proposing generic constraint formulations and comparing Z3 (SMT solver) and Gurobi (MILP solver) on academic and real-world inspired rostering problems.", "result": "Each solver excels for certain types of problems; MILP performs better with highly constrained or infeasible problems, while SMT performs better with varied shifts and personnel.", "conclusion": "SMT-based methods are promising for future research in personnel scheduling."}}
{"id": "2505.09847", "pdf": "https://arxiv.org/pdf/2505.09847", "abs": "https://arxiv.org/abs/2505.09847", "authors": ["Liyang Zhao", "Olurotimi Seton", "Himadeep Reddy Reddivari", "Suvendu Jena", "Shadow Zhao", "Rachit Kumar", "Changshuai Wei"], "title": "Causal Predictive Optimization and Generation for Business AI", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "comment": null, "summary": "The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.", "AI": {"tldr": "This paper presents a new approach called Causal Predictive Optimization and Generation for optimizing the sales process in B2B businesses. It includes three layers: prediction layer with causal ML, optimization layer with constraint optimization and contextual bandit, and serving layer with Generative AI and feedback-loop. The authors detail its implementation and deployment in LinkedIn and share insights that can be applied to similar systems.", "motivation": "Optimizing the sales process is crucial for the success of B2B businesses. The paper aims to introduce a principled approach to sales optimization and business AI.", "method": "The approach includes three layers: 1) prediction layer with causal ML, 2) optimization layer with constraint optimization and contextual bandit, and 3) serving layer with Generative AI and feedback-loop.", "result": "The system was implemented and deployed in LinkedIn, showing significant improvements over legacy systems.", "conclusion": "The paper concludes by sharing learnings and insights from the implementation and deployment of the Causal Predictive Optimization and Generation system, which can be broadly applied to the field."}}
{"id": "2505.10089", "pdf": "https://arxiv.org/pdf/2505.10089", "abs": "https://arxiv.org/abs/2505.10089", "authors": ["Wei Liu", "Sony Trenous", "Leonardo F. R. Ribeiro", "Bill Byrne", "Felix Hieber"], "title": "XRAG: Cross-lingual Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.", "AI": {"tldr": "A new benchmark called XRAG is proposed to assess the cross-lingual Retrieval-Augmented Generation (RAG) capabilities of large language models (LLMs). XRAG consists of questions derived from recent news articles requiring external knowledge and covers both monolingual and multilingual retrieval scenarios. The dataset's complex nature is indicated by the performance gap between humans and LLMs.", "motivation": "To evaluate the cross-lingual generation abilities of LLMs in RAG settings where user language differs from retrieval results.", "method": "Constructing a novel benchmark named XRAG using a dataset construction pipeline that generates questions needing complex reasoning.", "result": "XRAG highlights two challenges in cross-lingual RAG: maintaining response language correctness in monolingual retrieval and reasoning across languages in multilingual retrieval.", "conclusion": "XRAG is a useful benchmark for studying LLM reasoning abilities, especially in cross-lingual contexts."}}
{"id": "2505.09997", "pdf": "https://arxiv.org/pdf/2505.09997", "abs": "https://arxiv.org/abs/2505.09997", "authors": ["Jinhyun Jang", "Jiyeong Lee", "Kwanghoon Sohn"], "title": "Descriptive Image-Text Matching with Graded Contextual Similarity", "categories": ["cs.CV"], "comment": null, "summary": "Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.", "AI": {"tldr": "DITM\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u6587\u672c\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u8bed\u8a00\u7684\u63cf\u8ff0\u7075\u6d3b\u6027\u6765\u5b66\u4e60\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5206\u7ea7\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u8d85\u8d8a\u4e86\u4e25\u683c\u7684\u4e8c\u5143\u76d1\u7763\uff0c\u589e\u5f3a\u4e86\u53d1\u73b0\u6700\u4f73\u5339\u914d\u548c\u6f5c\u5728\u6b63\u6837\u672c\u5bf9\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8868\u793a\u590d\u6742\u7684\u56fe\u50cf\u6587\u672c\u5173\u7cfb\u65b9\u9762\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6587\u672c\u5339\u914d\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u7a00\u758f\u4e8c\u5143\u76d1\u7763\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u4e4b\u95f4\u56fa\u6709\u7684\u591a\u5bf9\u591a\u5bf9\u5e94\u5173\u7cfb\u4ee5\u53ca\u4ece\u4e00\u822c\u5230\u5177\u4f53\u63cf\u8ff0\u7684\u9690\u5f0f\u8fde\u63a5\u3002DITM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u63a2\u7d22\u8bed\u8a00\u7684\u63cf\u8ff0\u7075\u6d3b\u6027\u6765\u5b66\u4e60\u5206\u7ea7\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u3002", "method": "DITM\u901a\u8fc7\u7d2f\u79ef\u8bcd\u9891-\u9006\u6587\u6863\u9891\u7387\uff08TF-IDF\uff09\u8ba1\u7b97\u6bcf\u4e2a\u53e5\u5b50\u7684\u63cf\u8ff0\u5206\u6570\uff0c\u4ee5\u6b64\u5e73\u8861\u6210\u5bf9\u76f8\u4f3c\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u5229\u7528\u53e5\u5b50\u63cf\u8ff0\u6027\u8fdb\u884c\u9c81\u68d2\u56fe\u50cf\u6587\u672c\u5339\u914d\uff1a\uff081\uff09\u52a8\u6001\u653e\u677e\u6b63\u8d1f\u5bf9\u4e4b\u95f4\u7684\u8fde\u901a\u6027\u4ee5\u7ec6\u5316\u9519\u8bef\u8d1f\u6807\u7b7e\uff0c\uff082\uff09\u6309\u7167\u901a\u7528\u5230\u5177\u4f53\u7684\u987a\u5e8f\u5bf9\u76f8\u5173\u53e5\u5b50\u96c6\u8fdb\u884c\u5bf9\u9f50\u4ee5\u6784\u5efa\u66f4\u7cbe\u786e\u7684\u5339\u914d\u3002", "result": "DITM\u5728MS-COCO\u3001Flickr30K\u548cCxC\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5176\u65b9\u6cd5\u5728\u8868\u793a\u590d\u6742\u56fe\u50cf\u6587\u672c\u5173\u7cfb\u65b9\u9762\u66f4\u6709\u6548\u3002\u6b64\u5916\uff0c\u5728HierarCaps\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5206\u6790\u8868\u660e\uff0cDITM\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5206\u5c42\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "DITM\u901a\u8fc7\u63a2\u7d22\u8bed\u8a00\u7684\u63cf\u8ff0\u7075\u6d3b\u6027\uff0c\u8d85\u8d8a\u4e86\u4e25\u683c\u7684\u4e8c\u5143\u76d1\u7763\uff0c\u589e\u5f3a\u4e86\u53d1\u73b0\u6700\u4f73\u5339\u914d\u548c\u6f5c\u5728\u6b63\u6837\u672c\u5bf9\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDITM\u5728\u5904\u7406\u590d\u6742\u7684\u56fe\u50cf\u6587\u672c\u5173\u7cfb\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5206\u5c42\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.10361", "pdf": "https://arxiv.org/pdf/2505.10361", "abs": "https://arxiv.org/abs/2505.10361", "authors": ["David Abel", "Michael Bowling", "Andr\u00e9 Barreto", "Will Dabney", "Shi Dong", "Steven Hansen", "Anna Harutyunyan", "Khimya Khetarpal", "Clare Lyle", "Razvan Pascanu", "Georgios Piliouras", "Doina Precup", "Jonathan Richens", "Mark Rowland", "Tom Schaul", "Satinder Singh"], "title": "Plasticity as the Mirror of Empowerment", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Our first finding is that\nplasticity is the mirror of empowerment: The agent's plasticity is identical to\nthe empowerment of the environment, and vice versa. Our second finding\nestablishes a tension between the plasticity and empowerment of an agent,\nsuggesting that agent design needs to be mindful of both characteristics. We\nexplore the implications of these findings, and suggest that plasticity,\nempowerment, and their relationship are essential to understanding agency.", "AI": {"tldr": "This paper introduces a new concept called plasticity, which measures how much an agent can be influenced by its past observations. It also explores the connection between plasticity and empowerment, suggesting that both need to be considered in agent design.", "motivation": "To explore the foundational capacity of agents being influenced by past observations and how this relates to empowerment.", "method": "Defining plasticity using a new information-theoretic quantity called the generalized directed information, which generalizes Massey's directed information.", "result": "Plasticity is found to be the mirror of empowerment, and there is a tension between them in agent design.", "conclusion": "Understanding plasticity, empowerment, and their relationship is crucial for comprehending agency."}}
{"id": "2505.09848", "pdf": "https://arxiv.org/pdf/2505.09848", "abs": "https://arxiv.org/abs/2505.09848", "authors": ["Aditya Raj", "Golrokh Mirzaei"], "title": "Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection", "categories": ["cs.LG", "eess.IV"], "comment": "11 pages", "summary": "Imaging and genomic data offer distinct and rich features, and their\nintegration can unveil new insights into the complex landscape of diseases. In\nthis study, we present a novel approach utilizing radiogenomic data including\nstructural MRI images and gene expression data, for Alzheimer's disease\ndetection. Our framework introduces a novel heterogeneous bipartite graph\nrepresentation learning featuring two distinct node types: genes and images.\nThe network can effectively classify Alzheimer's disease (AD) into three\ndistinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)\nclasses, utilizing a small dataset. Additionally, it identified which genes\nplay a significant role in each of these classification groups. We evaluate the\nperformance of our approach using metrics including classification accuracy,\nrecall, precision, and F1 score. The proposed technique holds potential for\nextending to radiogenomic-based classification to other diseases.", "AI": {"tldr": "Integrating imaging and genomic data via a novel method that uses a heterogeneous bipartite graph representation learning, this study successfully classifies Alzheimer's disease into three stages and identifies key genes associated with each stage.", "motivation": "To integrate imaging and genomic data for new insights into disease complexities, specifically focusing on Alzheimer's disease detection.", "method": "Developing a novel heterogeneous bipartite graph representation learning featuring genes and images as distinct node types.", "result": "Effectively classifying Alzheimer's disease into three stages and identifying significant genes for each classification group using a small dataset.", "conclusion": "The proposed method has potential applications in radiogenomic-based disease classification beyond Alzheimer's."}}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113", "abs": "https://arxiv.org/abs/2505.10113", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.", "AI": {"tldr": "This paper introduces S-MedQA, a new English medical QA dataset, and finds that improvements in medical QA tasks are mainly due to domain shifting rather than knowledge injection.", "motivation": "To benchmark large language models in fine-grained clinical specialties and check the applicability of a popular hypothesis related to knowledge injection.", "method": "Using S-MedQA to test the applicability of a popular hypothesis related to knowledge injection in medical QA.", "result": "Training on data from a specialty does not necessarily lead to best performance on that specialty and token probabilities of clinically relevant terms for all specialties increase consistently.", "conclusion": "We conclude that improvement gains in medical QA tasks come mostly from domain shifting rather than knowledge injection, suggesting the need to reconsider the role of fine-tuning data in the medical domain."}}
{"id": "2505.09998", "pdf": "https://arxiv.org/pdf/2505.09998", "abs": "https://arxiv.org/abs/2505.09998", "authors": ["Ying Zang", "Yuanqi Hu", "Xinyu Chen", "Yuxia Xu", "Suhui Wang", "Chunan Yu", "Lanyun Zhu", "Deyi Ji", "Xin Xu", "Tianrun Chen"], "title": "From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.", "AI": {"tldr": "A new 3D sketch-driven garment generation framework is introduced, enabling ordinary users to create high-quality digital clothing through simple 3D sketches.", "motivation": "Existing 3D garment design tools are inaccessible to everyday users due to technical barriers and limited data.", "method": "Combining conditional diffusion model, sketch encoder, and adaptive curriculum learning strategy to interpret imprecise input and produce realistic garments.", "result": "The method outperforms existing baselines in fidelity and usability.", "conclusion": "This work demonstrates the potential of democratized fashion design on next-generation consumer platforms."}}
{"id": "2505.10399", "pdf": "https://arxiv.org/pdf/2505.10399", "abs": "https://arxiv.org/abs/2505.10399", "authors": ["Kaivalya Rawal", "Zihao Fu", "Eoin Delaney", "Chris Russell"], "title": "Evaluating Model Explanations without Ground Truth", "categories": ["cs.AI", "cs.LG", "I.2.6"], "comment": "https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth", "summary": "There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6AXE\uff0c\u8be5\u6846\u67b6\u4e0d\u9700\u8981\u7406\u60f3\u7684\u771f\u5b9e\u89e3\u91ca\u8fdb\u884c\u6bd4\u8f83\uff0c\u4e5f\u4e0d\u4f9d\u8d56\u4e8e\u6a21\u578b\u654f\u611f\u6027\uff0c\u63d0\u4f9b\u4e86\u89e3\u91ca\u8d28\u91cf\u7684\u72ec\u7acb\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u57fa\u7ebf\u6765\u9a8c\u8bc1AXE\uff0c\u5c55\u793a\u4e86\u5b83\u5982\u4f55\u7528\u4e8e\u68c0\u6d4b\u89e3\u91ca\u516c\u5e73\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u89e3\u91ca\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5c40\u90e8\u7279\u5f81\u91cd\u8981\u6027\u89e3\u91ca\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86AXE\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u57fa\u7ebf\u6765\u9a8c\u8bc1AXE\uff0c\u5c55\u793a\u4e86\u5b83\u5982\u4f55\u7528\u4e8e\u68c0\u6d4b\u89e3\u91ca\u516c\u5e73\u6027\u3002", "result": "AXE\u6846\u67b6\u80fd\u591f\u72ec\u7acb\u5730\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u68c0\u6d4b\u89e3\u91ca\u516c\u5e73\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6AXE\uff0c\u8be5\u6846\u67b6\u4e0d\u9700\u8981\u7406\u60f3\u7684\u771f\u5b9e\u89e3\u91ca\u8fdb\u884c\u6bd4\u8f83\uff0c\u4e5f\u4e0d\u4f9d\u8d56\u4e8e\u6a21\u578b\u654f\u611f\u6027\uff0c\u63d0\u4f9b\u4e86\u89e3\u91ca\u8d28\u91cf\u7684\u72ec\u7acb\u5ea6\u91cf\u3002"}}
{"id": "2505.09851", "pdf": "https://arxiv.org/pdf/2505.09851", "abs": "https://arxiv.org/abs/2505.09851", "authors": ["Shun Wang", "Shun-Li Shang", "Zi-Kui Liu", "Wenrui Hao"], "title": "ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "9 pages, 4 figures", "summary": "Traditional entropy-based methods - such as cross-entropy loss in\nclassification problems - have long been essential tools for quantifying\nuncertainty and disorder in data and developing artificial intelligence\nalgorithms. However, the rapid growth of data across various domains has\nintroduced new challenges, particularly the integration of heterogeneous\ndatasets with intrinsic disparities. In this paper, we extend zentropy theory\ninto the data science domain by introducing intrinsic entropy, enabling more\neffective learning from heterogeneous data sources. We propose a\nzentropy-enhanced neural network (ZENN) that simultaneously learns both energy\nand intrinsic entropy components, capturing the underlying structure of\nmulti-source data. To support this, we redesign the neural network architecture\nto better reflect the intrinsic properties and variability inherent in diverse\ndatasets. We demonstrate the effectiveness of ZENN on classification tasks and\nenergy landscape reconstructions, showing its superior generalization\ncapabilities and robustness-particularly in predicting high-order derivatives.\nAs a practical application, we employ ZENN to reconstruct the Helmholtz energy\nlandscape of Fe3Pt using data generated from DFT and capture key material\nbehaviors, including negative thermal expansion and the critical point in the\ntemperature-pressure space. Overall, our study introduces a novel approach for\ndata-driven machine learning grounded in zentropy theory, highlighting ZENN as\na versatile and robust deep learning framework for scientific problems\ninvolving complex, heterogeneous datasets.", "AI": {"tldr": "This paper extends zentropy theory into data science by proposing ZENN, which effectively learns from heterogeneous data and demonstrates superior performance in classification tasks and energy landscape reconstructions.", "motivation": "The need to effectively learn from heterogeneous data sources with intrinsic disparities due to the rapid growth of data across various domains.", "method": "Introducing intrinsic entropy to extend zentropy theory into the data science domain and proposing a zentropy-enhanced neural network (ZENN) that learns both energy and intrinsic entropy components.", "result": "ZENN shows superior generalization capabilities and robustness, especially in predicting high-order derivatives, and is successfully applied to reconstruct the Helmholtz energy landscape of Fe3Pt.", "conclusion": "The study introduces a novel approach for data-driven machine learning based on zentropy theory, demonstrating ZENN's versatility and robustness in handling complex, heterogeneous datasets."}}
{"id": "2505.10143", "pdf": "https://arxiv.org/pdf/2505.10143", "abs": "https://arxiv.org/abs/2505.10143", "authors": ["Longchao Da", "Parth Mitesh Shah", "Kuan-Ru Liou", "Jiaxing Zhang", "Hua Wei"], "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4; H.3.3"], "comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track", "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.", "AI": {"tldr": "This paper presents GE-Chat, a knowledge graph enhanced framework for evidence-based response generation in large language models.", "motivation": "To address the issue of unreliable outputs from large language models, particularly hallucinated responses, and improve trustworthiness.", "method": "Proposes GE-Chat, which uses a knowledge graph to enhance retrieval-augmented generation, along with chain-of-thought logic generation, n-hop sub-graph searching, and entailment-based sentence generation.", "result": "Demonstrates improved performance in identifying exact evidence in free-form contexts, providing a reliable way to examine LLM conclusion resources and judge trustworthiness.", "conclusion": "GE-Chat offers a solution to enhance the reliability and trustworthiness of large language model outputs by leveraging knowledge graphs and advanced reasoning techniques."}}
{"id": "2505.10016", "pdf": "https://arxiv.org/pdf/2505.10016", "abs": "https://arxiv.org/abs/2505.10016", "authors": ["Shijie Lyu"], "title": "Application of YOLOv8 in monocular downward multiple Car Target detection", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "comment": "Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering", "summary": "Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u6539\u8fdb\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u6280\u672f\u3001\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u7f51\u7edc\u6a21\u578b\u548c\u65b0\u7684\u68c0\u6d4b\u7ba1\u9053\uff0c\u5728\u591a\u5c3a\u5ea6\u3001\u5c0f\u7269\u4f53\u548c\u8fdc\u8ddd\u79bb\u7269\u4f53\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6a21\u578b\u5bf9\u5927\u5c0f\u7269\u4f53\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523065%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u6b64\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u7279\u522b\u9002\u5408\u81ea\u4e3b\u9a7e\u9a76\u7ade\u8d5b\u5982FSAC\u4e2d\u5355\u76ee\u6807\u548c\u5c0f\u7269\u4f53\u68c0\u6d4b\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u73af\u5883\u611f\u77e5\u7684\u96f7\u8fbe\u3001\u9053\u8def\u611f\u77e5\u7684\u6444\u50cf\u5934\u4ee5\u53ca\u8f66\u8f86\u4f20\u611f\u5668\u7f51\u7edc\u5b58\u5728\u9ad8\u6210\u672c\u3001\u6613\u53d7\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u5f71\u54cd\u53ca\u5206\u8fa8\u7387\u6709\u9650\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u3002", "method": "\u5728YOLOv8\u6846\u67b6\u5185\u96c6\u6210\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u6280\u672f\u3001\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u7f51\u7edc\u6a21\u578b\u548c\u65b0\u7684\u68c0\u6d4b\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3a\u540e\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5927\u3001\u5c0f\u7269\u4f53\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u8fbe\u523065%\uff0c\u8f83\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u8fdb\u6b65\u3002", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdb\u6a21\u578b\u5728\u591a\u5c3a\u5ea6\u3001\u5c0f\u7269\u4f53\u548c\u8fdc\u7a0b\u7269\u4f53\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u81ea\u4e3b\u9a7e\u9a76\u7ade\u8d5b\u4e2d\u5355\u76ee\u6807\u548c\u5c0f\u7269\u4f53\u68c0\u6d4b\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.10468", "pdf": "https://arxiv.org/pdf/2505.10468", "abs": "https://arxiv.org/abs/2505.10468", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "categories": ["cs.AI"], "comment": "32 pages, 14 figures, 11 tables", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "AI": {"tldr": "This study clarifies the difference between AI Agents and Agentic AI by offering a detailed taxonomy, application mapping, and challenge analysis. It compares their design philosophies and capabilities, analyzing their architectures, operational mechanisms, interaction styles, and autonomy levels.", "motivation": "Clarify the distinction between AI Agents and Agentic AI, providing a roadmap for developing robust, scalable, and explainable AI systems.", "method": "Provide a structured conceptual taxonomy, application mapping, and challenge analysis. Compare AI Agents and Agentic AI through architectural evolution, operational mechanisms, interaction styles, and autonomy levels.", "result": "Identify unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions.", "conclusion": "This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems."}}
{"id": "2505.09854", "pdf": "https://arxiv.org/pdf/2505.09854", "abs": "https://arxiv.org/abs/2505.09854", "authors": ["Harikrishna Kuttivelil", "Katia Obraczka"], "title": "Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence", "categories": ["cs.LG", "cs.ET", "cs.MA", "cs.SI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "As demand for intelligent services rises and edge devices become more\ncapable, distributed learning at the network edge has emerged as a key enabling\ntechnology. While existing paradigms like federated learning (FL) and\ndecentralized FL (DFL) enable privacy-preserving distributed learning in many\nscenarios, they face potential challenges in connectivity and synchronization\nimposed by resource-constrained and infrastructure-less environments. While\nmore robust, gossip learning (GL) algorithms have generally been designed for\nhomogeneous data distributions and may not suit all contexts. This paper\nintroduces Chisme, a novel suite of protocols designed to address the\nchallenges of implementing robust intelligence in the network edge,\ncharacterized by heterogeneous data distributions, episodic connectivity, and\nlack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and\nasynchronous GL (Chisme-GL) variants that enable collaborative yet\ndecentralized model training that considers underlying data heterogeneity. We\nintroduce a data similarity heuristic that allows agents to opportunistically\ninfer affinity with each other using the existing communication of model\nupdates in decentralized FL and GL. We leverage the heuristic to extend DFL's\nmodel aggregation and GL's model merge mechanisms for better personalized\ntraining while maintaining collaboration. While Chisme-DFL is a synchronous\ndecentralized approach whose resource utilization scales linearly with network\nsize, Chisme-GL is fully asynchronous and has a lower, constant resource\nrequirement independent of network size. We demonstrate that Chisme methods\noutperform their standard counterparts in model training over distributed and\nheterogeneous data in network scenarios ranging from less connected and\nreliable networks to fully connected and lossless networks.", "AI": {"tldr": "This paper introduces Chisme, a suite of protocols designed to address the challenges of implementing robust intelligence in the network edge with heterogeneous data distributions, episodic connectivity, and lack of infrastructure.", "motivation": "The motivation is to overcome the limitations of existing paradigms like federated learning and decentralized FL in connectivity and synchronization imposed by resource-constrained and infrastructure-less environments, as well as the limitations of gossip learning algorithms which are generally designed for homogeneous data distributions.", "method": "Chisme includes both synchronous DFL (Chisme-DFL) and asynchronous GL (Chisme-GL) variants that enable collaborative yet decentralized model training that considers underlying data heterogeneity. A data similarity heuristic is introduced to allow agents to infer affinity with each other using existing communication of model updates in decentralized FL and GL, which is leveraged to extend DFL's model aggregation and GL's model merge mechanisms for better personalized training while maintaining collaboration.", "result": "Chisme methods outperform their standard counterparts in model training over distributed and heterogeneous data in network scenarios ranging from less connected and reliable networks to fully connected and lossless networks.", "conclusion": "Chisme addresses the challenges of implementing robust intelligence in the network edge with heterogeneous data distributions, episodic connectivity, and lack of infrastructure."}}
{"id": "2505.10182", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.", "AI": {"tldr": "This study evaluates Reasoning CPT, a form of continual pretraining that uses synthetic data to reconstruct hidden thought processes, revealing its ability to enhance performance across various domains, especially on difficult problems.", "motivation": "To explore how to effectively synthesize training data for reasoning and how such data affect a wide range of domains.", "method": "Applying Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and comparing it to standard CPT on the MMLU benchmark.", "result": "Reasoning CPT consistently improves performance across all evaluated domains. Reasoning skills acquired in one domain transfer effectively to others. The performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty.", "conclusion": "Reasoning CPT can improve performance across all evaluated domains, and the performance gap with conventional methods widens as problem difficulty increases."}}
{"id": "2505.10027", "pdf": "https://arxiv.org/pdf/2505.10027", "abs": "https://arxiv.org/abs/2505.10027", "authors": ["Shijie Lyu"], "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)", "summary": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.", "AI": {"tldr": "A reinforcement learning-based method for fine-tuning latent diffusion models to enhance super-resolution quality and adaptability in complex remote sensing scenes.", "motivation": "Existing deep learning methods have limitations in handling complex scenes and preserving image details.", "method": "Constructing a reinforcement learning environment with states, actions, and rewards and optimizing decision objectives through proximal policy optimization during the reverse denoising process of the LDM model.", "result": "Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes.", "conclusion": "The proposed reinforcement learning-based latent diffusion model fine-tuning method improves super-resolution quality and adaptability across scenes."}}
{"id": "2505.10543", "pdf": "https://arxiv.org/pdf/2505.10543", "abs": "https://arxiv.org/abs/2505.10543", "authors": ["Annie Wong", "Thomas B\u00e4ck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "title": "Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.", "AI": {"tldr": "This study examines the effectiveness of different prompting techniques in enhancing the adaptability of large language models in dynamic environments.", "motivation": "To evaluate the potential of large language models as self-learning and reasoning agents in dynamic environments.", "method": "Systematic evaluation of self-reflection, heuristic mutation, and planning as prompting techniques through experiments with various open-source language models.", "result": "Larger models generally outperform smaller ones, but strategic prompting can close the performance gap. Advanced prompting techniques benefit smaller models on complex games, but less so for high-performing large models. Advanced reasoning methods yield variable outcomes, with some leading to significant improvements but others causing performance drops. Human-level emergent reasoning is not observed, and crucial areas like planning, reasoning, and spatial coordination remain limited.", "conclusion": "Current-generation large language models have fundamental shortcomings that cannot be fully overcome by self-reflective prompting alone. The complexity of reasoning requires moving beyond static benchmarks."}}
{"id": "2505.09855", "pdf": "https://arxiv.org/pdf/2505.09855", "abs": "https://arxiv.org/abs/2505.09855", "authors": ["Alexander Y. Ku", "Thomas L. Griffiths", "Stephanie C. Y. Chan"], "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86Transformer\u6a21\u578b\u4e2d\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\uff08IWL\u548cICL\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u73af\u5883\u7a33\u5b9a\u6027\u5f71\u54cdIWL\u548cICL\u7684\u4f7f\u7528\u504f\u597d\uff0c\u63d0\u793a\u53ef\u9760\u6027\u589e\u5f3aICL\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3Transformer\u6a21\u578b\u4e2d\u7684\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\uff08IWL\u548cICL\uff09\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u7814\u7a76\u4eba\u5458\u53d7\u5230\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u7c7b\u4f3c\u9002\u5e94\u7b56\u7565\u7684\u542f\u53d1\uff0c\u5373\u57fa\u56e0\u7f16\u7801\u548c\u8868\u578b\u53ef\u5851\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u64cd\u4f5c\u5316\u73af\u5883\u7684\u53ef\u9884\u6d4b\u6027\u7ef4\u5ea6\uff0c\u5e76\u7cfb\u7edf\u5730\u8c03\u67e5\u5176\u5bf9Transformer\u6a21\u578b\u4e2dICL/IWL\u5e73\u8861\u7684\u5f71\u54cd\u3002\u4f7f\u7528\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u6765\u5c55\u793a\u7ed3\u679c\u3002", "result": "\u5728\u9ad8\u73af\u5883\u7a33\u5b9a\u6027\u4e0b\uff0cIWL\u88ab\u4f18\u5148\u4f7f\u7528\uff1b\u5728\u9ad8\u63d0\u793a\u53ef\u9760\u6027\u4e0b\uff0cICL\u66f4\u6709\u6548\u3002\u4efb\u52a1\u4f9d\u8d56\u6027\u7684\u5b66\u4e60\u52a8\u6001\u53d8\u5316\u63ed\u793a\u4e86\u521d\u59cbIWL\u9636\u6bb5\u540e\u6765\u8f6c\u53d8\u4e3aICL\u4e3b\u5bfc\u7684\u73b0\u8c61\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u7684\u73af\u5883\u7a33\u5b9a\u6027\u4e0b\uff0cTransformer\u6a21\u578b\u4f1a\u504f\u5411\u4f7f\u7528\u4e0d\u540c\u7684\u5b66\u4e60\u6a21\u5f0f\u3002\u5728\u9ad8\u73af\u5883\u7a33\u5b9a\u6027\u4e0b\uff0cIWL\u88ab\u4f18\u5148\u4f7f\u7528\uff1b\u800c\u5728\u9ad8\u63d0\u793a\u53ef\u9760\u6027\u4e0b\uff0cICL\u66f4\u6709\u6548\u3002\u6b64\u5916\uff0c\u4efb\u52a1\u4f9d\u8d56\u6027\u7684\u5b66\u4e60\u52a8\u6001\u53d8\u5316\u4e5f\u63ed\u793a\u4e86\u521d\u59cbIWL\u9636\u6bb5\u540e\u6765\u8f6c\u53d8\u4e3aICL\u4e3b\u5bfc\u7684\u73b0\u8c61\u3002\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u4e86\u76f8\u5bf9\u6210\u672c\u5047\u8bbe\uff0c\u5e76\u5f3a\u8c03\u4e86\u53ef\u9884\u6d4b\u6027\u4f5c\u4e3a\u6307\u5bfcTransformer\u81ea\u9002\u5e94\u7b56\u7565\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2505.10185", "pdf": "https://arxiv.org/pdf/2505.10185", "abs": "https://arxiv.org/abs/2505.10185", "authors": ["Seongyun Lee", "Seungone Kim", "Minju Seo", "Yongrae Jo", "Dongyoung Go", "Hyeonbin Hwang", "Jinho Park", "Xiang Yue", "Sean Welleck", "Graham Neubig", "Moontae Lee", "Minjoon Seo"], "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.", "AI": {"tldr": "This paper introduces the CoT Encyclopedia, a new framework for analyzing and steering reasoning in large language models.", "motivation": "To understand the reasoning strategies underlying the capabilities of large language models.", "method": "Automatically extract diverse reasoning criteria from model-generated CoTs, embed them into a semantic space, cluster them into representative categories, and derive contrastive rubrics to interpret reasoning behavior.", "result": "The framework produces more interpretable and comprehensive analyses than existing methods and enables performance gains by predicting which strategy a model is likely to use and guiding it toward more effective alternatives.", "conclusion": "Training data format has a greater impact on reasoning behavior than data domain, highlighting the importance of format-aware model design."}}
{"id": "2505.10030", "pdf": "https://arxiv.org/pdf/2505.10030", "abs": "https://arxiv.org/abs/2505.10030", "authors": ["Miit Daga", "Dhriti Parikh", "Swarna Priya Ramu"], "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication", "summary": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.", "AI": {"tldr": "DeepSeqCoco is a deep learning model for automatic coconut tree disease identification, which shows higher accuracy (up to 99.5%) and faster processing times compared to existing methods.", "motivation": "Current methods for identifying coconut tree diseases are manual, labor-intensive, and not scalable, especially in developing countries where early diagnosis is crucial for agricultural yield protection.", "method": "DeepSeqCoco uses deep learning techniques and was tested with various optimizers like SGD, Adam, and hybrid configurations to optimize performance metrics.", "result": "The model achieved up to 99.5% accuracy, up to 5% higher than existing models, with the hybrid SGD-Adam configuration showing the lowest validation loss of 2.81%. Training and prediction times were reduced by up to 18% and 85%, respectively.", "conclusion": "DeepSeqCoco demonstrates potential for improving precision agriculture through an AI-based, scalable, and efficient disease monitoring system."}}
{"id": "2306.07615", "pdf": "https://arxiv.org/pdf/2306.07615", "abs": "https://arxiv.org/abs/2306.07615", "authors": ["Heqin Zhu", "Quan Quan", "Qingsong Yao", "Zaiyi Liu", "S. Kevin Zhou"], "title": "UOD: Universal One-shot Detection of Anatomical Landmarks", "categories": ["cs.CV", "cs.AI"], "comment": "Eealy accepted by MICCAI 2023. 11pages, 4 figures, 2 tables. arXiv\n  admin note: text overlap with arXiv:2203.06433", "summary": "One-shot medical landmark detection gains much attention and achieves great\nsuccess for its label-efficient training process. However, existing one-shot\nlearning methods are highly specialized in a single domain and suffer domain\npreference heavily in the situation of multi-domain unlabeled data. Moreover,\none-shot learning is not robust that it faces performance drop when annotating\na sub-optimal image. To tackle these issues, we resort to developing a\ndomain-adaptive one-shot landmark detection framework for handling multi-domain\nmedical images, named Universal One-shot Detection (UOD). UOD consists of two\nstages and two corresponding universal models which are designed as\ncombinations of domain-specific modules and domain-shared modules. In the first\nstage, a domain-adaptive convolution model is self-supervised learned to\ngenerate pseudo landmark labels. In the second stage, we design a\ndomain-adaptive transformer to eliminate domain preference and build the global\ncontext for multi-domain data. Even though only one annotated sample from each\ndomain is available for training, the domain-shared modules help UOD aggregate\nall one-shot samples to detect more robust and accurate landmarks. We\ninvestigated both qualitatively and quantitatively the proposed UOD on three\nwidely-used public X-ray datasets in different anatomical domains (i.e., head,\nhand, chest) and obtained state-of-the-art performances in each domain. The\ncode is available at\nhttps://github.com/heqin-zhu/UOD_universal_oneshot_detection.", "AI": {"tldr": "A new method called UOD is developed for one-shot medical landmark detection across multiple domains.", "motivation": "Existing methods struggle with domain preference and robustness issues in multi-domain settings.", "method": "UOD uses two stages with domain-specific and domain-shared modules, including a self-supervised convolution model and a domain-adaptive transformer.", "result": "UOD shows superior performance compared to previous methods on three X-ray datasets.", "conclusion": "UOD demonstrates effectiveness in handling multi-domain medical images with limited labeled data."}}
{"id": "2505.09861", "pdf": "https://arxiv.org/pdf/2505.09861", "abs": "https://arxiv.org/abs/2505.09861", "authors": ["John Bencina", "Erkut Aykutlug", "Yue Chen", "Zerui Zhang", "Stephanie Sorenson", "Shao Tang", "Changshuai Wei"], "title": "LiDDA: Data Driven Attribution at LinkedIn", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ME"], "comment": null, "summary": "Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.", "AI": {"tldr": "This paper introduces a new attribution approach using transformers to handle different types of data for marketing intelligence.", "motivation": "To improve modern marketing intelligence by developing a more effective data-driven attribution method.", "method": "A unified transformer-based attribution approach that can handle various types of data including member-level, aggregate-level, and external macro factors.", "result": "The approach was implemented on a large scale at LinkedIn and showed significant impact.", "conclusion": "The proposed method has broad applicability in the marketing and ad tech fields."}}
{"id": "2505.10202", "pdf": "https://arxiv.org/pdf/2505.10202", "abs": "https://arxiv.org/abs/2505.10202", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "YiMing Cheng", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.", "AI": {"tldr": "VQ-Logits\u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u663e\u8457\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5c42\u7684\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u8d1f\u8f7d\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u8f83\u9ad8\u6548\u7387\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u53c2\u6570\u5e76\u52a0\u5feb\u4e86\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u5176\u5e9e\u5927\u7684\u8f93\u51fa\u8bcd\u6c47\u8868\u800c\u9762\u4e34\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6700\u540e\u7684\u7ebf\u6027\u6295\u5f71\u5c42\u5360\u7528\u4e86\u5927\u91cf\u8d44\u6e90\u3002", "method": "VQ-Logits\u5229\u7528\u5411\u91cf\u91cf\u5316\u6765\u66ff\u6362\u5927\u578b\u8f93\u51fa\u5d4c\u5165\u77e9\u9635\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u5c0f\u7684\u5171\u4eab\u4ee3\u7801\u672c\u6620\u5c04\u5230\u5b8c\u6574\u7684\u8bcd\u6c47\u7a7a\u95f4\u3002", "result": "\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVQ-Logits\u5b9e\u73b0\u4e86\u9ad8\u8fbe99%\u7684\u53c2\u6570\u51cf\u5c11\u548c6\u500d\u7684logit\u8ba1\u7b97\u52a0\u901f\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u4e864%\u7684\u56f0\u60d1\u5ea6\u3002", "conclusion": "VQ-Logits\u5c55\u793a\u4e86\u5176\u5728\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5c42\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u6027\u80fd\u4e0a\u51e0\u4e4e\u6ca1\u6709\u635f\u5931\u3002"}}
{"id": "2505.10046", "pdf": "https://arxiv.org/pdf/2505.10046", "abs": "https://arxiv.org/abs/2505.10046", "authors": ["Bingda Tang", "Boyang Zheng", "Xichen Pan", "Sayak Paul", "Saining Xie"], "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.", "AI": {"tldr": "This paper explores the design space of combining large language models and diffusion transformers for text-to-image synthesis, providing a detailed comparison with alternatives and offering a clear training recipe.", "motivation": "To address the lack of detailed comparisons and undisclosed design details in previous studies on text-to-image synthesis.", "method": "An empirical study conducting controlled comparisons with established baselines and analyzing important design choices.", "result": "Provides a clear, reproducible recipe for training at scale.", "conclusion": "Hopes to offer meaningful data points and practical guidelines for future research in multi-modal generation."}}
{"id": "2410.13778", "pdf": "https://arxiv.org/pdf/2410.13778", "abs": "https://arxiv.org/abs/2410.13778", "authors": ["Michelangelo Olmo Nogara Notarianni", "Filippo Leveni", "Diego Stucchi", "Luca Frittoli", "Giacomo Boracchi"], "title": "Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "AALTD workshop at ECML 2024 (https://ecml-aaltd.github.io/aaltd2024/)", "summary": "We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.", "AI": {"tldr": "KQT-EWMA is a non-parametric change-detection algorithm that combines Kernel-QuantTree histogram and EWMA statistic for monitoring multivariate data streams online.", "motivation": "To present a non-parametric change-detection algorithm that can control false alarms and detect changes in multivariate data streams.", "method": "Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA)", "result": "KQT-EWMA can control ARL_0 and achieve detection delays comparable to or lower than state-of-the-art methods.", "conclusion": "KQT-EWMA can control ARL_0 while achieving detection delays comparable to or lower than state-of-the-art methods."}}
{"id": "2505.09864", "pdf": "https://arxiv.org/pdf/2505.09864", "abs": "https://arxiv.org/abs/2505.09864", "authors": ["Aditya Panangat"], "title": "BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks", "categories": ["cs.LG"], "comment": "6 pages, 0 figures, 2 tables", "summary": "Over the past decade, the use of machine learning has increased\nexponentially. Models are far more complex than ever before, growing to\ngargantuan sizes and housing millions of weights. Unfortunately, the fact that\nlarge models have become the state of the art means that it often costs\nmillions of dollars to train and operate them. These expenses not only hurt\ncompanies but also bar non-wealthy individuals from contributing to new\ndevelopments and force consumers to pay greater prices for AI. Current methods\nused to prune models, such as iterative magnitude pruning, have shown great\naccuracy but require an iterative training sequence that is incredibly\ncomputationally and environmentally taxing. To solve this problem, BINGO is\nintroduced. BINGO, during the training pass, studies specific subsets of a\nneural network one at a time to gauge how significant of a role each weight\nplays in contributing to a network's accuracy. By the time training is done,\nBINGO generates a significance score for each weight, allowing for\ninsignificant weights to be pruned in one shot. BINGO provides an\naccuracy-preserving pruning technique that is less computationally intensive\nthan current methods, allowing for a world where AI growth does not have to\nmean model growth, as well.", "AI": {"tldr": "Large models are expensive to train and operate. BINGO introduces a method to identify and prune insignificant weights in one step, preserving accuracy while reducing computational cost.", "motivation": "Reduce the high cost of training and operating large machine learning models, making AI development more accessible.", "method": "BINGO studies subsets of a neural network during training to assign a significance score to each weight, enabling one-shot pruning of insignificant weights.", "result": "BINGO achieves accuracy-preserving pruning with lower computational requirements compared to existing methods.", "conclusion": "BINGO offers a solution to make AI development more affordable by efficiently pruning models without sacrificing accuracy."}}
{"id": "2505.10218", "pdf": "https://arxiv.org/pdf/2505.10218", "abs": "https://arxiv.org/abs/2505.10218", "authors": ["Zongsheng Wang", "Kaili Sun", "Bowen Wu", "Qun Yu", "Ying Li", "Baoxun Wang"], "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward", "categories": ["cs.CL"], "comment": null, "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.", "AI": {"tldr": "We introduce RAIDEN-R1, a novel reinforcement learning framework that improves role consistency in conversational agents through verifiable role-awareness reward and advanced reasoning strategies.", "motivation": "Role-playing conversational agents (RPCAs) face persistent challenges in maintaining role consistency.", "method": "We propose RAIDEN-R1, a novel reinforcement learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The method introduces both singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys.", "result": "Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, respectively, outperforming baseline models while maintaining robustness.", "conclusion": "This work bridges the non-quantifiability gap in RPCA training and provides insights into role-aware reasoning patterns, advancing the development of RPCAs."}}
{"id": "2505.10049", "pdf": "https://arxiv.org/pdf/2505.10049", "abs": "https://arxiv.org/abs/2505.10049", "authors": ["Jinlong Fan", "Xuepu Zeng", "Jing Zhang", "Mingming Gong", "Yuxiang Yang", "Dacheng Tao"], "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u52a8\u6001\u573a\u666f\u8868\u793a\u4e0e\u91cd\u5efa\u9886\u57df\u7684200\u591a\u7bc7\u8bba\u6587\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6982\u89c8\uff0c\u6db5\u76d6\u4ece\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5230\u663e\u5f0f\u9ad8\u65af\u57fa\u5143\u7684\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd0\u52a8\u8868\u793a\u8303\u5f0f\u3001\u91cd\u5efa\u6280\u672f\u3001\u8f85\u52a9\u4fe1\u606f\u6574\u5408\u7b56\u7565\u53ca\u6b63\u5219\u5316\u65b9\u6cd5\u7b49\u5173\u952e\u65b9\u9762\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u8868\u793a\u548c\u91cd\u5efa\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u8fd9\u4e9b\u6280\u672f\u6700\u521d\u7528\u4e8e\u9759\u6001\u73af\u5883\uff0c\u4f46\u968f\u7740\u5bf9\u590d\u67424D\u52a8\u6001\u573a\u666f\u7684\u7814\u7a76\u6df1\u5165\uff0c\u5176\u5e94\u7528\u8303\u56f4\u4e0d\u65ad\u6269\u5927\u3002", "method": "\u5bf9\u8d85\u8fc7200\u7bc7\u5173\u4e8e\u4f7f\u7528\u8f90\u5c04\u573a\u8fdb\u884c\u52a8\u6001\u573a\u666f\u8868\u793a\u7684\u8bba\u6587\u8fdb\u884c\u4e86\u5206\u7c7b\u8bc4\u4f30\uff0c\u5305\u62ec\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5230\u663e\u5f0f\u9ad8\u65af\u57fa\u5143\u7684\u5404\u79cd\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u5173\u952e\u89c6\u89d2\u8fdb\u884c\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\u6765\u7ec4\u7ec7\u5404\u79cd\u65b9\u6cd5\u8bba\uff0c\u5e76\u901a\u8fc7\u6279\u5224\u6027\u5ba1\u67e5\u6307\u51fa\u4e86\u8be5\u9886\u57df\u7684\u6301\u7eed\u6311\u6218\u548c\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u603b\u7ed3\u4e86\u52a8\u6001\u573a\u666f\u8868\u793a\u548c\u91cd\u5efa\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6743\u5a01\u53c2\u8003\uff0c\u540c\u65f6\u4e3a\u6709\u7ecf\u9a8c\u7684\u4ece\u4e1a\u8005\u63d0\u4f9b\u7cfb\u7edf\u6027\u7684\u6982\u5ff5\u539f\u7406\u548c\u5b9e\u8df5\u524d\u6cbf\u7406\u89e3\u3002"}}
{"id": "2505.03084", "pdf": "https://arxiv.org/pdf/2505.03084", "abs": "https://arxiv.org/abs/2505.03084", "authors": ["Shashank Kapoor", "Sanjay Surendranath Girija", "Lakshit Arora", "Dipen Pradhan", "Ankit Shetgaonkar", "Aman Raj"], "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in IEEE COMPSAC 2025", "summary": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world.", "AI": {"tldr": "This paper surveys adversarial attacks targeting four modalities: text, image, video, and audio. It provides a view of the adversarial attack landscape in the multimodal world.", "motivation": "To address the lack of a practitioner-focused view on adversarial attacks in the multimodal world.", "method": "Survey of adversarial attacks targeting all four modalities.", "result": "A comprehensive summarization of the threat landscape in the multimodal world.", "conclusion": "This survey is the first comprehensive summarization of the threat landscape in the multimodal world."}}
{"id": "2505.09901", "pdf": "https://arxiv.org/pdf/2505.09901", "abs": "https://arxiv.org/abs/2505.09901", "authors": ["Ziyuan Zhang", "Darcy Wang", "Ningyuan Chen", "Rodrigo Mansur", "Vahid Sarhangian"], "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.", "AI": {"tldr": "This paper investigates the exploration-exploitation trade-off in large language models (LLMs), comparing their decision-making behavior with humans and MAB algorithms using multi-armed bandit tasks.", "motivation": "To examine whether LLMs exhibit similar decision-making behavior to humans and can achieve comparable or superior performance.", "method": "Using interpretable choice models to capture the E&E strategies of agents and investigating how explicit reasoning affects LLM decision-making through different prompting strategies and reasoning-enhanced models.", "result": "Reasoning shifts LLMs towards more human-like behavior, showing similar levels of random and directed exploration as humans in simple tasks but struggling with adaptability in complex, non-stationary environments.", "conclusion": "LLMs show promise as simulators of human behavior and automated decision-making tools but have limitations, especially in complex, changing environments."}}
{"id": "2505.10260", "pdf": "https://arxiv.org/pdf/2505.10260", "abs": "https://arxiv.org/abs/2505.10260", "authors": ["Poli Apollinaire Nemkova", "Solomon Ubani", "Mark V. Albert"], "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.", "AI": {"tldr": "This study examines five top-tier large language models' capability for zero-shot and few-shot annotation of a complex social media post dataset in Russian and Ukrainian, focusing on binary classification of human rights violation references. Model annotations are compared against human-labeled data in both English and Russian, revealing error patterns and cross-linguistic adaptability.", "motivation": "To assess the reliability and applicability of large language models in handling sensitive, domain-specific tasks in multilingual contexts, especially concerning subjective and context-dependent judgments.", "method": "Binary classification task using zero-shot and few-shot annotations from GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2 on a dataset of Russian and Ukrainian social media posts, compared against human double-annotated labels.", "result": "The models' performance varies under different prompting conditions and languages, with unique error patterns and disagreements identified, offering insights into their strengths, limitations, and cross-linguistic capabilities.", "conclusion": "This research highlights the potential and challenges of using large language models for sensitive multilingual tasks, emphasizing the need for careful evaluation and adaptation when deploying such models in real-world scenarios."}}
{"id": "2505.10055", "pdf": "https://arxiv.org/pdf/2505.10055", "abs": "https://arxiv.org/abs/2505.10055", "authors": ["Ijazul Haq", "Yingjie Zhang", "Irfan Ali Khan"], "title": "PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.", "AI": {"tldr": "Evaluate the performance of Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language.", "motivation": "NLP in Pashto faces challenges due to the cursive nature of its script and a scarcity of structured datasets.", "method": "Developed a synthetic Pashto OCR dataset, PsOCR, covering variations across 1,000 unique font families, colors, image sizes, and layouts.", "result": "Gemini achieved the best performance among all models; Qwen-7B performed best among open-source models.", "conclusion": "This work provides insights into the capabilities and limitations of current LMMs for OCR tasks in Pashto and establishes a foundation for further research."}}
{"id": "2505.09593", "pdf": "https://arxiv.org/pdf/2505.09593", "abs": "https://arxiv.org/abs/2505.09593", "authors": ["Filippo Leveni", "Guilherme Weigert Cassales", "Bernhard Pfahringer", "Albert Bifet", "Giacomo Boracchi"], "title": "Online Isolation Forest", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at International Conference on Machine Learning (ICML 2024)", "summary": "The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.", "AI": {"tldr": "Online-iForest\u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4e13\u4e3a\u6d41\u6761\u4ef6\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u65e0\u7f1d\u8ddf\u8e2a\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u8bc1\u660e\u5b83\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u6240\u6709\u7ade\u4e89\u5bf9\u624b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5feb\u901f\u8bc6\u522b\u5f02\u5e38\u7684\u5e94\u7528\u573a\u666f\u5982\u7f51\u7edc\u5b89\u5168\u3001\u6b3a\u8bc8\u548c\u6545\u969c\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u6d41\u4e0a\u4e0b\u6587\u4e2d\u9700\u8981\u91cd\u590d\u8bbf\u95ee\u5185\u5b58\u4e2d\u7684\u6570\u636e\uff0c\u5e76\u4e14\u65bd\u52a0\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff1b\u73b0\u6709\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5f80\u5f80\u901a\u8fc7\u5468\u671f\u6027\u91cd\u65b0\u8bad\u7ec3\u6765\u9002\u5e94\u5728\u7ebf\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOnline-iForest\u7684\u65b0\u65b9\u6cd5\uff0c\u4e13\u4e3a\u6d41\u6761\u4ef6\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u65e0\u7f1d\u8ddf\u8e2a\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\uff0cOnline-iForest\u7684\u8868\u73b0\u4e0e\u5728\u7ebf\u66ff\u4ee3\u65b9\u6848\u76f8\u5f53\uff0c\u5e76\u4e14\u63a5\u8fd1\u7ecf\u8fc7\u5468\u671f\u6027\u91cd\u65b0\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u3002", "conclusion": "Online-iForest\u5728\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u7ade\u4e89\u5bf9\u624b\uff0c\u4f7f\u5176\u6210\u4e3a\u5feb\u901f\u8bc6\u522b\u5f02\u5e38\u7684\u91cd\u8981\u5e94\u7528\u4e2d\u7684\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5982\u7f51\u7edc\u5b89\u5168\u3001\u6b3a\u8bc8\u548c\u6545\u969c\u68c0\u6d4b\u3002"}}
{"id": "2505.09907", "pdf": "https://arxiv.org/pdf/2505.09907", "abs": "https://arxiv.org/abs/2505.09907", "authors": ["Linwei Zhang", "LuFeng", "Ruijia Liang"], "title": "Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "With the growing demand for healthy foods, agricultural product price\nforecasting has become increasingly important. Hass avocados, as a high-value\ncrop, exhibit complex price fluctuations influenced by factors such as\nseasonality, region, and weather. Traditional prediction models often struggle\nwith highly nonlinear and dynamic data. To address this, we propose a hybrid\ndeep learning model, TCN-MLP-Attention Architecture, combining Temporal\nConvolutional Networks (TCN) for sequential feature extraction, Multi-Layer\nPerceptrons (MLP) for nonlinear interactions, and an Attention mechanism for\ndynamic feature weighting. The dataset used covers over 50,000 records of Hass\navocado sales across the U.S. from 2015 to 2018, including variables such as\nsales volume, average price, time, region, weather, and variety type, collected\nfrom point-of-sale systems and the Hass Avocado Board. After systematic\npreprocessing, including missing value imputation and feature normalization,\nthe proposed model was trained and evaluated. Experimental results demonstrate\nthat the TCN-MLP-Attention model achieves excellent predictive performance,\nwith an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.\nThis research provides a scalable and effective approach for time series\nforecasting in agricultural markets and offers valuable insights for\nintelligent supply chain management and price strategy optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408TCN\u3001MLP\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u54c8\u65af\u725b\u6cb9\u679c\u4ef7\u683c\u9884\u6d4b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4eba\u4eec\u5bf9\u5065\u5eb7\u98df\u54c1\u7684\u9700\u6c42\u589e\u52a0\uff0c\u519c\u4ea7\u54c1\u4ef7\u683c\u9884\u6d4b\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u54c8\u65af\u725b\u6cb9\u679c\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u4ef7\u503c\u4f5c\u7269\uff0c\u5176\u4ef7\u683c\u53d7\u5b63\u8282\u6027\u3001\u5730\u533a\u548c\u5929\u6c14\u7b49\u56e0\u7d20\u5f71\u54cd\u590d\u6742\u6ce2\u52a8\u3002\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u5e94\u5bf9\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u52a8\u6001\u7684\u6570\u636e\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u5305\u542b50,000\u591a\u6761\u8bb0\u5f55\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e862015\u5e74\u81f32018\u5e74\u7f8e\u56fd\u5404\u5730\u7684\u54c8\u65af\u725b\u6cb9\u679c\u9500\u552e\u60c5\u51b5\uff0c\u53d8\u91cf\u5305\u62ec\u9500\u552e\u91cf\u3001\u5e73\u5747\u4ef7\u683c\u3001\u65f6\u95f4\u3001\u5730\u533a\u3001\u5929\u6c14\u548c\u54c1\u79cd\u7c7b\u578b\u7b49\u3002\u6570\u636e\u6765\u81eaPOS\u7cfb\u7edf\u548c\u54c8\u65af\u725b\u6cb9\u679c\u59d4\u5458\u4f1a\u3002\u7814\u7a76\u91c7\u7528\u4e86\u7f3a\u5931\u503c\u586b\u8865\u548c\u7279\u5f81\u5f52\u4e00\u5316\u7b49\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u7136\u540e\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684TCN-MLP-\u6ce8\u610f\u529b\u67b6\u6784\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684TCN-MLP-\u6ce8\u610f\u529b\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cRMSE\u4e3a1.23\uff0cMSE\u4e3a1.51\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TCN\u3001MLP\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u54c8\u65af\u725b\u6cb9\u679c\u7684\u4ef7\u683c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u519c\u4e1a\u5e02\u573a\u7684\u65f6\u5e8f\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u4f9b\u4e86\u667a\u80fd\u4f9b\u5e94\u94fe\u7ba1\u7406\u548c\u4ef7\u683c\u7b56\u7565\u4f18\u5316\u7684\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.10261", "pdf": "https://arxiv.org/pdf/2505.10261", "abs": "https://arxiv.org/abs/2505.10261", "authors": ["Rui Yang", "Huitao Li", "Matthew Yu Heng Wong", "Yuhe Ke", "Xin Li", "Kunyu Yu", "Jingchi Liao", "Jonathan Chong Kai Liew", "Sabarinath Vinod Nair", "Jasmine Chiat Ling Ong", "Irene Li", "Douglas Teodoro", "Chuan Hong", "Daniel Shu Wei Ting", "Nan Liu"], "title": "The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.", "AI": {"tldr": "\u6bd4\u8f83\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u533b\u5b66\u4efb\u52a1\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u5f3a\u8c03\u5176\u9053\u5fb7\u4f7f\u7528\u7684\u5fc5\u8981\u6027", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u4e0d\u540c\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u5dee\u5f02", "method": "\u5206\u6790\u4e8619,123\u7bc7\u7814\u7a76", "result": "\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u6027\u4efb\u52a1\u4e0a\u6709\u4f18\u52bf\uff0c\u800c\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u4fe1\u606f\u63d0\u53d6\u548c\u5206\u6790\u4efb\u52a1\u4e0a\u5360\u4e3b\u5bfc\u5730\u4f4d", "conclusion": "\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0c\u786e\u4fdd\u8fd9\u4e9b\u6280\u672f\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u9053\u5fb7\u4f7f\u7528\u81f3\u5173\u91cd\u8981"}}
{"id": "2505.10072", "pdf": "https://arxiv.org/pdf/2505.10072", "abs": "https://arxiv.org/abs/2505.10072", "authors": ["Rui-Yang Ju", "Sheng-Yen Huang", "Yi-Ping Hung"], "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.", "AI": {"tldr": "ToonifyGB\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7Gaussian blendshapes\u5408\u6210\u591a\u6837\u5316\u7684\u98ce\u683c\u53163D\u5934\u50cf\u3002\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u98ce\u683c\u5316\u89c6\u9891\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5408\u6210Gaussian blendshapes\u3002\u5728Arcane\u548cPixar\u98ce\u683c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6269\u5c55Toonify\u6846\u67b6\u4ee5\u5408\u6210\u591a\u6837\u5316\u7684\u98ce\u683c\u53163D\u5934\u50cf\u3002", "method": "\u63d0\u51faToonifyGB\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u98ce\u683c\u5316\u89c6\u9891\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5408\u6210Gaussian blendshapes\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Arcane\u548cPixar\u98ce\u683c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ToonifyGB\u80fd\u591f\u9ad8\u6548\u5730\u6e32\u67d3\u5177\u6709\u4efb\u610f\u8868\u60c5\u7684\u98ce\u683c\u5316\u5934\u50cf\u3002"}}
{"id": "2505.09922", "pdf": "https://arxiv.org/pdf/2505.09922", "abs": "https://arxiv.org/abs/2505.09922", "authors": ["Zichen Liu", "Wei Zhang", "Tiejun Li"], "title": "Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity", "categories": ["cs.LG"], "comment": "22 pages", "summary": "Euclidean diffusion models have achieved remarkable success in generative\nmodeling across diverse domains, and they have been extended to manifold case\nin recent advances. Instead of explicitly utilizing the structure of special\nmanifolds as studied in previous works, we investigate direct sampling of the\nEuclidean diffusion models for general manifold-constrained data in this paper.\nWe reveal the multiscale singularity of the score function in the embedded\nspace of manifold, which hinders the accuracy of diffusion-generated samples.\nWe then present an elaborate theoretical analysis of the singularity structure\nof the score function by separating it along the tangential and normal\ndirections of the manifold. To mitigate the singularity and improve the\nsampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces\nnon-isotropic noise along the normal direction to reduce scale discrepancies,\nand (2) Tango-DM, which trains only the tangential component of the score\nfunction using a tangential-only loss function. Numerical experiments\ndemonstrate that our methods achieve superior performance on distributions over\nvarious manifolds with complex geometries.", "AI": {"tldr": "This paper investigates direct sampling of Euclidean diffusion models for manifold-constrained data and proposes two novel methods, Niso-DM and Tango-DM, to improve sampling accuracy.", "motivation": "Investigate direct sampling of the Euclidean diffusion models for general manifold-constrained data instead of explicitly utilizing the structure of special manifolds.", "method": "Niso-DM, which introduces non-isotropic noise along the normal direction to reduce scale discrepancies, and Tango-DM, which trains only the tangential component of the score function using a tangential-only loss function.", "result": "Superior performance on distributions over various manifolds with complex geometries.", "conclusion": "Our methods achieve superior performance on distributions over various manifolds with complex geometries."}}
{"id": "2505.10282", "pdf": "https://arxiv.org/pdf/2505.10282", "abs": "https://arxiv.org/abs/2505.10282", "authors": ["Dubai Li", "Nan Jiang", "Kangping Huang", "Ruiqi Tu", "Shuyu Ouyang", "Huayu Yu", "Lin Qiao", "Chen Yu", "Tianshu Zhou", "Danyang Tong", "Qian Wang", "Mengtao Li", "Xiaofeng Zeng", "Yu Tian", "Xinping Tian", "Jingsong Li"], "title": "From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making", "categories": ["cs.CL"], "comment": null, "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.", "AI": {"tldr": "Quicker, an AI-powered clinical decision support system, automates evidence synthesis and generates clinical recommendations, showing strong performance in helping physicians make faster and more reliable decisions.", "motivation": "Integrating clinical evidence into real-time practice is challenging due to the enormous workload, complex professional processes, and time constraints.", "method": "This study introduces Quicker, an evidence-based clinical decision support system powered by large language models (LLMs), designed to automate evidence synthesis and generate clinical recommendations modeled after standard clinical guideline development processes.", "result": "Quicker-implemented chain covers all phases, from questions to clinical recommendations, and further enables customized decision-making through integrated tools and interactive user interfaces. The Q2CRBench-3 benchmark dataset was developed based on clinical guideline development records for three different diseases.", "conclusion": "In general, our findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions."}}
{"id": "2505.10088", "pdf": "https://arxiv.org/pdf/2505.10088", "abs": "https://arxiv.org/abs/2505.10088", "authors": ["Yuncheng Guo", "Xiaodong Gu"], "title": "MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file", "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.", "AI": {"tldr": "Proposes Multi-Modal Representation Learning (MMRL) to improve generalization of vision-language models and introduces an improved version, MMRL++, which reduces parameters and enhances interactions.", "motivation": "Adapting large-scale pre-trained Vision-Language Models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks.", "method": "Introduces Multi-Modal Representation Learning (MMRL), which includes a shared, learnable, modality-agnostic representation space. Also proposes MMRL++, a parameter-efficient and interaction-aware extension.", "result": "Extensive experiments on 15 datasets show consistent performance improvement.", "conclusion": "MMRL and MMRL++ outperform state-of-the-art methods and achieve a strong balance between task-specific adaptation and generalization."}}
{"id": "2505.09925", "pdf": "https://arxiv.org/pdf/2505.09925", "abs": "https://arxiv.org/abs/2505.09925", "authors": ["Yutao Yang", "Jie Zhou", "Junsong Li", "Qianjun Pan", "Bihao Zhan", "Qin Chen", "Xipeng Qiu", "Liang He"], "title": "Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an interactive continual learning paradigm where AI\nmodels dynamically learn new skills from real-time human feedback while\nretaining prior knowledge. This paradigm distinctively addresses two major\nlimitations of traditional continual learning: (1) dynamic model updates using\nstreaming, real-time human-annotated data, rather than static datasets with\nfixed labels, and (2) the assumption of clean labels, by explicitly handling\nthe noisy feedback common in real-world interactions. To tackle these problems,\nwe propose RiCL, a Reinforced interactive Continual Learning framework\nleveraging Large Language Models (LLMs) to learn new skills effectively from\ndynamic feedback. RiCL incorporates three key components: a temporal\nconsistency-aware purifier to automatically discern clean from noisy samples in\ndata streams; an interaction-aware direct preference optimization strategy to\nalign model behavior with human intent by reconciling AI-generated and\nhuman-provided feedback; and a noise-resistant contrastive learning module that\ncaptures robust representations by exploiting inherent data relationships, thus\navoiding reliance on potentially unreliable labels. Extensive experiments on\ntwo benchmark datasets (FewRel and TACRED), contaminated with realistic noise\npatterns, demonstrate that our RiCL approach substantially outperforms existing\ncombinations of state-of-the-art online continual learning and noisy-label\nlearning methods.", "AI": {"tldr": "This paper proposes a novel interactive continual learning paradigm named RiCL that leverages LLMs to learn new skills from real-time human feedback while retaining prior knowledge. The method addresses limitations of traditional continual learning approaches.", "motivation": "Traditional continual learning approaches suffer from limitations like reliance on static datasets with fixed labels and clean labels assumption, which do not hold in real-world scenarios where data is noisy and dynamic.", "method": "RiCL incorporates three key components: a temporal consistency-aware purifier, an interaction-aware direct preference optimization strategy, and a noise-resistant contrastive learning module.", "result": "Experiments show that RiCL outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods on two benchmark datasets with realistic noise patterns.", "conclusion": "The proposed RiCL framework demonstrates effectiveness in learning from dynamic and noisy feedback, providing a promising direction for future research in interactive continual learning."}}
{"id": "2505.10320", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "AI": {"tldr": "J1\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3AI\u5224\u65ad\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u7684\u63d0\u793a\u8f6c\u6362\u4e3a\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5224\u65ad\u4efb\u52a1\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u5224\u65ad\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u5224\u65ad\u504f\u5dee\u3002\u5b9e\u9a8c\u8868\u660e\uff0cJ1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u6539\u8fdbAI\u8bc4\u4f30\u7684\u8d28\u91cf\uff0c\u589e\u5f3a\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u80fd\u529b\uff0c\u5bfb\u627e\u6700\u4f73\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5c06\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u7684\u63d0\u793a\u8f6c\u6362\u4e3a\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5224\u65ad\u4efb\u52a1\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "J1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5305\u62ec\u4eceDeepSeek-R1\u84b8\u998f\u51fa\u6765\u7684\u6a21\u578b\u3002", "conclusion": "J1\u901a\u8fc7\u5b66\u4e60\u5236\u5b9a\u8bc4\u4f30\u6807\u51c6\u3001\u5bf9\u6bd4\u81ea\u52a8\u751f\u6210\u7684\u53c2\u8003\u7b54\u6848\u4ee5\u53ca\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u7684\u6b63\u786e\u6027\uff0c\u63d0\u9ad8\u4e86\u5224\u65ad\u8d28\u91cf\u3002"}}
{"id": "2505.10118", "pdf": "https://arxiv.org/pdf/2505.10118", "abs": "https://arxiv.org/abs/2505.10118", "authors": ["Yangfu Li", "Hongjian Zhan", "Tianyi Chen", "Qi Liu", "Yue Lu"], "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering", "categories": ["cs.CV", "cs.CL"], "comment": "31 pages,9 figures,conference", "summary": "Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.", "AI": {"tldr": "MoB improves visual token pruning by addressing the trade-off between prompt alignment and visual preservation, achieving efficient performance with fewer tokens.", "motivation": "Existing visual token pruning methods overlook the varying relative importance of prompt alignment and visual preservation across tasks, leading to inconsistent performance.", "method": "MoB reformulates visual token pruning as a bi-objective covering problem and handles the trade-off between prompt alignment and visual preservation through budget allocation via greedy radius trading.", "result": "MoB achieves high performance preservation with a significantly reduced number of visual tokens, accelerating models without significant performance loss.", "conclusion": "MoB provides a provable performance bound and shows linear scalability with respect to the number of input visual tokens, allowing it to adapt to challenging pruning scenarios."}}
{"id": "2505.09949", "pdf": "https://arxiv.org/pdf/2505.09949", "abs": "https://arxiv.org/abs/2505.09949", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Samgyu Yang", "Abdulrahman Faden"], "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors", "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.", "AI": {"tldr": "This research uses a fine-tuned large language model to analyze freeway crash data and identify crash causation factors like alcohol-impaired driving and speeding.", "motivation": "To understand the factors contributing to traffic crashes and develop strategies to reduce their severity.", "method": "Fine-tuning the Llama3 8B model using QLoRA on a dataset created from 226 traffic safety studies, then using zero-shot classification to identify crash causation.", "result": "The model successfully identified primary crash causes and provided comprehensive explanations. It also showed practical applicability with high agreement from traffic safety researchers.", "conclusion": "LLMs can comprehensively analyze crash causation and contribute to more effective traffic safety practices."}}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354", "abs": "https://arxiv.org/abs/2505.10354", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u7ef4\u5bc6\u96c6\u4e14\u53ef\u89e3\u91ca\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578bLDIR\uff0c\u5176\u6570\u503c\u7ef4\u5ea6\u8868\u793a\u901a\u8fc7\u6700\u8fdc\u70b9\u91c7\u6837\u4e0e\u4e0d\u540c\u951a\u6587\u672c\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u5728\u591a\u4e2a\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u5ea6\u3001\u68c0\u7d22\u548c\u805a\u7c7b\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5d4c\u5165\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u4f18\u79c0\u4f46\u96be\u4ee5\u8ffd\u8e2a\u548c\u89e3\u91ca\uff1b\u7a00\u758f\u53ef\u89e3\u91ca\u7684\u8bcd\u888b\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff1b\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6587\u672c\u5d4c\u5165\u901a\u5e38\u7ef4\u5ea6\u8f83\u9ad8\u3002", "method": "\u63d0\u51fa\u4f4e\u7ef4\uff08\u4f4e\u4e8e500\uff09\u5bc6\u96c6\u4e14\u53ef\u89e3\u91ca\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578bLDIR\uff0c\u5176\u7ef4\u5ea6\u6570\u503c\u901a\u8fc7\u6700\u8fdc\u70b9\u91c7\u6837\u8868\u793a\u4e0e\u4e0d\u540c\u951a\u6587\u672c\u7684\u8bed\u4e49\u76f8\u5173\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86LDIR\u7684\u6027\u80fd\uff0c\u63a5\u8fd1\u9ed1\u76d2\u57fa\u51c6\u6a21\u578b\u4e14\u4f18\u4e8e\u5176\u4ed6\u53ef\u89e3\u91ca\u5d4c\u5165\u57fa\u51c6\u6a21\u578b\uff0c\u4e14\u7ef4\u5ea6\u66f4\u5c11\u3002", "conclusion": "\u63d0\u51fa\u7684LDIR\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7ef4\u5ea6\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.10124", "pdf": "https://arxiv.org/pdf/2505.10124", "abs": "https://arxiv.org/abs/2505.10124", "authors": ["Ziad Kheil", "Lucas Robinet", "Laurent Risser", "Soleakhena Ken"], "title": "IMITATE: Image Registration with Context for unknown time frame recovery", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE ISBI 2025", "summary": "In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .", "AI": {"tldr": "This paper introduces a novel method called IMITATE to estimate unknown condition-related images without needing fixed images, applying it to manage image moving tumors in radiotherapy treatments using 4D-CT scans.", "motivation": "To solve the problem of reconstructing artifact-free volumes from irregular breathing patterns during radiotherapy treatments.", "method": "Using a conditional U-Net architecture to model the new image registration formalism.", "result": "Artifact-free volumes were successfully reconstructed from 4D-CT clinical data with real-time latencies.", "conclusion": "The proposed method effectively removes reconstruction artifacts caused by irregular breathing, hysteresis, and poor correlation between breathing signals and internal motion."}}
{"id": "2505.09952", "pdf": "https://arxiv.org/pdf/2505.09952", "abs": "https://arxiv.org/abs/2505.09952", "authors": ["Tianyu Huai", "Jie Zhou", "Yuxuan Cai", "Qin Chen", "Wen Wu", "Xingjiao Wu", "Xipeng Qiu", "Liang He"], "title": "Task-Core Memory Management and Consolidation for Long-term Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to Neurips2025", "summary": "In this paper, we focus on a long-term continual learning (CL) task, where a\nmodel learns sequentially from a stream of vast tasks over time, acquiring new\nknowledge while retaining previously learned information in a manner akin to\nhuman learning. Unlike traditional CL settings, long-term CL involves handling\na significantly larger number of tasks, which exacerbates the issue of\ncatastrophic forgetting. Our work seeks to address two critical questions: 1)\nHow do existing CL methods perform in the context of long-term CL? and 2) How\ncan we mitigate the catastrophic forgetting that arises from prolonged\nsequential updates? To tackle these challenges, we propose a novel framework\ninspired by human memory mechanisms for long-term continual learning (Long-CL).\nSpecifically, we introduce a task-core memory management strategy to\nefficiently index crucial memories and adaptively update them as learning\nprogresses. Additionally, we develop a long-term memory consolidation mechanism\nthat selectively retains hard and discriminative samples, ensuring robust\nknowledge retention. To facilitate research in this area, we construct and\nrelease two multi-modal and textual benchmarks, MMLongCL-Bench and\nTextLongCL-Bench, providing a valuable resource for evaluating long-term CL\napproaches. Experimental results show that Long-CL outperforms the previous\nstate-of-the-art by 7.4\\% and 6.5\\% AP on the two benchmarks, respectively,\ndemonstrating the effectiveness of our approach.", "AI": {"tldr": "This paper introduces Long-CL, a novel framework for long-term continual learning inspired by human memory mechanisms, which addresses catastrophic forgetting and outperforms previous methods on two newly released benchmarks.", "motivation": "To address the challenges of catastrophic forgetting in long-term continual learning with a large number of tasks.", "method": "Proposes a task-core memory management strategy and a long-term memory consolidation mechanism.", "result": "Outperforms previous state-of-the-art methods by 7.4% and 6.5% AP on the two constructed benchmarks.", "conclusion": "The proposed Long-CL framework effectively mitigates catastrophic forgetting and improves performance in long-term continual learning."}}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356", "abs": "https://arxiv.org/abs/2505.10356", "authors": ["Chunyu Ye", "Shaonan Wang"], "title": "Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli", "categories": ["cs.CL"], "comment": null, "summary": "Decoding thoughts from brain activity offers valuable insights into human\ncognition and enables promising applications in brain-computer interaction.\nWhile prior studies have explored language reconstruction from fMRI data, they\nare typically limited to single-modality inputs such as images or audio. In\ncontrast, human thought is inherently multimodal. To bridge this gap, we\npropose a unified and flexible framework for reconstructing coherent language\nfrom brain recordings elicited by diverse input modalities-visual, auditory,\nand textual. Our approach leverages visual-language models (VLMs), using\nmodality-specific experts to jointly interpret information across modalities.\nExperiments demonstrate that our method achieves performance comparable to\nstate-of-the-art systems while remaining adaptable and extensible. This work\nadvances toward more ecologically valid and generalizable mind decoding.", "AI": {"tldr": "This paper proposes a unified framework for reconstructing language from brain activity elicited by different input modalities (visual, auditory, and textual). The approach uses modality-specific experts from visual-language models to decode thoughts and shows comparable performance with adaptability.", "motivation": "Prior studies on decoding thoughts from brain activity focused on single-modal inputs like images or audio, whereas human thought is multimodal. This paper aims to bridge the gap by proposing a multimodal framework for brain activity decoding.", "method": "The proposed method uses visual-language models with modality-specific experts to interpret information across visual, auditory, and textual modalities for reconstructing coherent language from brain recordings.", "result": "Experiments show that the proposed method achieves performance comparable to state-of-the-art systems while maintaining adaptability and extensibility.", "conclusion": "This work contributes to more ecologically valid and generalizable mind decoding by introducing a unified framework for multimodal brain activity decoding."}}
{"id": "2505.10152", "pdf": "https://arxiv.org/pdf/2505.10152", "abs": "https://arxiv.org/abs/2505.10152", "authors": ["Yikang Wei"], "title": "Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization", "categories": ["cs.CV"], "comment": "IJCAI 2025", "summary": "Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.", "AI": {"tldr": "Propose a new method called MCSAD for federated domain generalization, which improves performance by generating data in broader style space and conducting domain-invariant learning.", "motivation": "To address the limited style space issue caused by existing style augmentation methods either exploring data styles within isolated source domain or interpolating style information across existing source domains under the data decentralization scenario.", "method": "Multi-source Collaborative Style Augmentation and Domain-invariant learning (MCSAD)", "result": "Extensive experiments on multiple domain generalization datasets show that our method significantly outperforms the state-of-the-art federated domain generalization methods.", "conclusion": "Our method significantly outperforms the state-of-the-art federated domain generalization methods."}}
{"id": "2505.09955", "pdf": "https://arxiv.org/pdf/2505.09955", "abs": "https://arxiv.org/abs/2505.09955", "authors": ["Jaeho Kim", "Seulki Lee"], "title": "TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 Accept", "summary": "Unsupervised domain adaptation (UDA) for time series data remains a critical\nchallenge in deep learning, with traditional pseudo-labeling strategies failing\nto capture temporal patterns and channel-wise shifts between domains, producing\nsub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that\naddresses these limitations by modeling the joint distribution $P(\\mathbf{X},\ny)$ of the source domain through code transition matrices, where the codes are\nderived from vector quantization (VQ) of time series patches. Our method\nconstructs class- and channel-wise code transition matrices from the source\ndomain and employs Bayes' rule for target domain adaptation, generating\npseudo-labels based on channel-wise weighted class-conditional likelihoods.\nTransPL offers three key advantages: explicit modeling of temporal transitions\nand channel-wise shifts between different domains, versatility towards\ndifferent UDA scenarios (e.g., weakly-supervised UDA), and explainable\npseudo-label generation. We validate TransPL's effectiveness through extensive\nanalysis on four time series UDA benchmarks and confirm that it consistently\noutperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%\naccuracy improvement, 4.9% F1 improvement), while providing interpretable\ninsights into the domain adaptation process through its learned code transition\nmatrices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTransPL\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u95ee\u9898\uff0c\u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\u663e\u5f0f\u5efa\u6a21\u4e0d\u540c\u57df\u4e4b\u95f4\u7684\u65f6\u5e8f\u8f6c\u6362\u548c\u901a\u9053\u504f\u79fb\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u3002\u5728\u56db\u4e2a\u65f6\u95f4\u5e8f\u5217UDA\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5206\u6790\u9a8c\u8bc1\u4e86TransPL\u7684\u6709\u6548\u6027\uff0c\u5176\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u7684\u4f2a\u6807\u7b7e\u65b9\u6cd5\u63d0\u9ad8\u4e866.1%\u548c4.9%\u3002", "motivation": "\u4f20\u7edf\u7684\u65f6\u95f4\u5e8f\u5217UDA\u4f2a\u6807\u7b7e\u7b56\u7565\u65e0\u6cd5\u6355\u6349\u65f6\u5e8f\u6a21\u5f0f\u548c\u57df\u95f4\u7684\u901a\u9053\u504f\u79fb\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684\u4f2a\u6807\u7b7e\u3002", "method": "\u5f15\u5165TransPL\u65b9\u6cd5\uff0c\u5229\u7528\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u4ece\u65f6\u95f4\u5e8f\u5217\u7247\u6bb5\u4e2d\u83b7\u53d6\u4ee3\u7801\uff0c\u5e76\u6784\u5efa\u7c7b\u548c\u901a\u9053\u7ea7\u522b\u7684\u4ee3\u7801\u8f6c\u6362\u77e9\u9635\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u751f\u6210\u76ee\u6807\u57df\u7684\u4f2a\u6807\u7b7e\u3002", "result": "TransPL\u5728\u56db\u4e2a\u65f6\u95f4\u5e8f\u5217UDA\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u663e\u793a\u51fa\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4f2a\u6807\u7b7e\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u9886\u57df\u9002\u5e94\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6d1e\u5bdf\u3002", "conclusion": "TransPL\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217UDA\u4e2d\u7684\u65f6\u5e8f\u548c\u901a\u9053\u504f\u79fb\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684UDA\u573a\u666f\u3002"}}
{"id": "2505.10389", "pdf": "https://arxiv.org/pdf/2505.10389", "abs": "https://arxiv.org/abs/2505.10389", "authors": ["Benjamin White", "Anastasia Shimorina"], "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.", "AI": {"tldr": "This paper investigates the design of an aspect-based sentiment analysis system using LLMs, focusing on quadruple opinion extraction. It shows that a combined multi-domain model performs as well as specialized single-domain models but with less complexity.", "motivation": "To develop an effective and efficient aspect-based sentiment analysis system using LLMs for real-world applications.", "method": "Fine-tuning a single model to handle multiple domain-specific taxonomies simultaneously.", "result": "The combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity.", "conclusion": "A unified approach using LLMs can be effective for aspect-based sentiment analysis across different domains and languages."}}
{"id": "2505.10169", "pdf": "https://arxiv.org/pdf/2505.10169", "abs": "https://arxiv.org/abs/2505.10169", "authors": ["Matthias K\u00fcmmerer", "Harneet Khanuja", "Matthias Bethge"], "title": "Modeling Saliency Dataset Bias", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.", "AI": {"tldr": "This paper explores the challenges of predicting fixations across different saliency datasets, finding a significant performance drop due to dataset bias. It proposes a novel architecture that effectively addresses this issue by introducing fewer than 20 dataset-specific parameters, achieving state-of-the-art results.", "motivation": "To overcome the challenge of dataset bias affecting the performance of saliency prediction models when applied across different datasets.", "method": "Introducing a novel architecture that extends a dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters focusing on multi-scale structure, center bias, and fixation spread.", "result": "The proposed model significantly reduces the generalization gap, achieving state-of-the-art results on three datasets of the MIT/Tuebingen Saliency Benchmark, especially when adapting to respective training datasets.", "conclusion": "The model not only improves performance across datasets but also offers insights into spatial saliency properties."}}
{"id": "2505.09959", "pdf": "https://arxiv.org/pdf/2505.09959", "abs": "https://arxiv.org/abs/2505.09959", "authors": ["Zengxia Guo", "Bohui An", "Zhongqi Lu"], "title": "Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated reinforcement learning (FRL) methods usually share the encrypted\nlocal state or policy information and help each client to learn from others\nwhile preserving everyone's privacy. In this work, we propose that sharing the\napproximated behavior metric-based state projection function is a promising way\nto enhance the performance of FRL and concurrently provides an effective\nprotection of sensitive information. We introduce FedRAG, a FRL framework to\nlearn a computationally practical projection function of states for each client\nand aggregating the parameters of projection functions at a central server. The\nFedRAG approach shares no sensitive task-specific information, yet provides\ninformation gain for each client. We conduct extensive experiments on the\nDeepMind Control Suite to demonstrate insightful results.", "AI": {"tldr": "This paper introduces FedRAG, a federated reinforcement learning framework that enhances performance by sharing approximated behavior metric-based state projection functions without exposing sensitive task-specific information.", "motivation": "To improve the performance of Federated Reinforcement Learning while protecting sensitive information.", "method": "Proposing a method where clients learn a projection function of states and the central server aggregates these functions' parameters. This approach does not share sensitive task-specific information but still provides information gain for each client.", "result": "Extensive experiments were conducted on the DeepMind Control Suite to demonstrate insightful results.", "conclusion": "Sharing approximated behavior metric-based state projection functions is a promising way to enhance Federated Reinforcement Learning performance while ensuring privacy."}}
{"id": "2505.10402", "pdf": "https://arxiv.org/pdf/2505.10402", "abs": "https://arxiv.org/abs/2505.10402", "authors": ["Yihong Dong", "Yuchen Liu", "Xue Jiang", "Zhi Jin", "Ge Li"], "title": "Rethinking Repetition Problems of LLMs in Code Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025 (main)", "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.", "AI": {"tldr": "Propose an efficient decoding approach called RPG to alleviate the repetition problems in code generation for LLMs.", "motivation": "The problem of repetitions during the generation process in code generation, especially structural repetition, continues to linger. Previous work mainly focused on content repetition.", "method": "An efficient decoding approach called RPG (Repetition Penalization based on Grammar) is proposed to alleviate the repetition problems in code generation for LLMs.", "result": "RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.", "conclusion": "RPG efficiently reduces structural repetition in code generation for LLMs by leveraging grammar rules."}}
{"id": "2505.10205", "pdf": "https://arxiv.org/pdf/2505.10205", "abs": "https://arxiv.org/abs/2505.10205", "authors": ["Umair Haroon", "Ahmad AlMughrabi", "Thanasis Zoumpekas", "Ricardo Marques", "Petia Radeva"], "title": "VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.", "AI": {"tldr": "This paper presents VolE, a novel framework that uses mobile device-driven 3D reconstruction to estimate food volume without reference or depth information.", "motivation": "Accurate food volume estimation is important for medical nutrition management and health monitoring applications, but current methods have limitations.", "method": "VolE captures images and camera locations in free motion to generate precise 3D models using AR-capable mobile devices and leverages food video segmentation for food mask generation.", "result": "Experiments show that VolE outperforms existing volume estimation techniques across multiple datasets with a MAPE of 2.22%.", "conclusion": "VolE is a superior method for food volume estimation."}}
{"id": "2505.09969", "pdf": "https://arxiv.org/pdf/2505.09969", "abs": "https://arxiv.org/abs/2505.09969", "authors": ["Ali Azimi Lamir", "Shiva Razzagzadeh", "Zeynab Rezaei"], "title": "A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study presents a machine learning-based framework for heart disease\nprediction using the heart-disease dataset, comprising 303 samples with 14\nfeatures. The methodology involves data preprocessing, model training, and\nevaluation using three classifiers: Logistic Regression, K-Nearest Neighbors\n(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and\nRandomizedSearchCV was employed to enhance model performance. The Random Forest\nclassifier outperformed other models, achieving an accuracy of 91% and an\nF1-score of 0.89. Evaluation metrics, including precision, recall, and\nconfusion matrix, revealed balanced performance across classes. The proposed\nmodel demonstrates strong potential for aiding clinical decision-making by\neffectively predicting heart disease. Limitations such as dataset size and\ngeneralizability underscore the need for future studies using larger and more\ndiverse datasets. This work highlights the utility of machine learning in\nhealthcare, offering insights for further advancements in predictive\ndiagnostics.", "AI": {"tldr": "A machine learning framework using Logistic Regression, KNN, and Random Forest is developed for heart disease prediction, showing that Random Forest outperforms others with 91% accuracy and 0.89 F1-score.", "motivation": "To develop an effective machine learning-based system for heart disease prediction using the heart-disease dataset.", "method": "Data preprocessing, model training using Logistic Regression, KNN, and Random Forest, hyperparameter tuning with GridSearchCV and RandomizedSearchCV.", "result": "Random Forest classifier achieved the best performance with 91% accuracy and 0.89 F1-score, showing balanced performance across classes.", "conclusion": "The proposed machine learning model has strong potential for aiding clinical decision-making but requires larger and more diverse datasets to improve generalizability."}}
{"id": "2505.10409", "pdf": "https://arxiv.org/pdf/2505.10409", "abs": "https://arxiv.org/abs/2505.10409", "authors": ["Yue Guo", "Jae Ho Sohn", "Gondy Leroy", "Trevor Cohen"], "title": "Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.", "AI": {"tldr": "Large language models (LLMs) show promise in generating plain language summaries (PLSs), but they may not enhance comprehension compared to human-written PLSs. Evaluation methods should go beyond surface quality.", "motivation": "To assess whether LLM-generated PLSs are understandable and comparable to human-written PLSs.", "method": "Crowdsourced evaluation with 150 participants using subjective Likert-scale ratings and objective comprehension measures.", "result": "LLMs produce PLSs that seem similar to human-written ones in subjective evaluations but lead to poorer comprehension. Automated metrics poorly align with human judgments.", "conclusion": "Evaluation frameworks should prioritize comprehension over surface-level quality, and generation methods should focus on optimizing for layperson understanding."}}
{"id": "2505.10223", "pdf": "https://arxiv.org/pdf/2505.10223", "abs": "https://arxiv.org/abs/2505.10223", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MIDL 2025", "summary": "Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.", "AI": {"tldr": "Evaluate alternative augmentation strategies (MixUp and Auxiliary Fourier Augmentation) to improve out-of-distribution generalization and robustness in medical image segmentation.", "motivation": "Address performance degradation of medical image segmentation models due to mismatches between training and test distributions in real-world clinical settings.", "method": "Systematically evaluate MixUp and Auxiliary Fourier Augmentation methods.", "result": "These augmentation methods improve out-of-distribution generalization and robustness to imaging variations in cardiac cine MRI and prostate MRI segmentation tasks.", "conclusion": "Integration of these augmentation techniques into nnU-Net training pipelines enhances the reliability of medical segmentation models in real-world applications."}}
{"id": "2505.09983", "pdf": "https://arxiv.org/pdf/2505.09983", "abs": "https://arxiv.org/abs/2505.09983", "authors": ["Changxun Zhu", "Qilong Wu", "Lingjuan Lyu", "Shibei Xue"], "title": "Sybil-based Virtual Data Poisoning Attacks in Federated Learning", "categories": ["cs.CR", "cs.LG"], "comment": "7 pages, 6 figures, accepted by IEEE Codit 2025", "summary": "Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data.", "AI": {"tldr": "A sybil-based virtual data poisoning attack is proposed to reduce costs in federated learning.", "motivation": "To address the vulnerability of federated learning to poisoning attacks by malicious adversaries and reduce the costs of achieving effective attacks.", "method": "Generating sybil nodes to amplify the poisoning model's impact and developing a virtual data generation method based on gradient matching to reduce neural network computational complexity.", "result": "The method outperforms other attack algorithms in simulation.", "conclusion": "This paper proposes a novel sybil-based virtual data poisoning attack that effectively reduces the cost of poisoning federated learning."}}
{"id": "2505.10413", "pdf": "https://arxiv.org/pdf/2505.10413", "abs": "https://arxiv.org/abs/2505.10413", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Zhicheng Dou"], "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.", "AI": {"tldr": "Proposes LongRefiner, an efficient plug-and-play refiner for long-document processing in RAG applications.", "motivation": "To reduce inference costs and improve performance in long-context input scenarios.", "method": "Dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model.", "result": "Achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline.", "conclusion": "LongRefiner provides practical insights for real-world long-text RAG applications."}}
{"id": "2505.10231", "pdf": "https://arxiv.org/pdf/2505.10231", "abs": "https://arxiv.org/abs/2505.10231", "authors": ["Haozhe Luo", "Ziyu Zhou", "Zixin Shu", "Aur\u00e9lie Pahud de Mortanges", "Robert Berke", "Mauricio Reyes"], "title": "On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.", "AI": {"tldr": "Deep neural networks in medical imaging have biases leading to fairness gaps. Incorporating human insights reduces these gaps and improves out-of-domain generalization.", "motivation": "To explore Human-AI alignment and fairness in medical imaging.", "method": "Systematic exploration.", "result": "Incorporating human insights reduces fairness gaps and enhances out-of-domain generalization, but excessive alignment may introduce performance trade-offs.", "conclusion": "Human-AI alignment is a promising approach for fair, robust, and generalizable medical AI systems."}}
{"id": "2505.10003", "pdf": "https://arxiv.org/pdf/2505.10003", "abs": "https://arxiv.org/abs/2505.10003", "authors": ["Tianyu Jiao", "Zhuoran Xiao", "Yihang Huang", "Chenhui Ye", "Yijia Feng", "Liyu Cai", "Jiang Chang", "Fangkun Liu", "Yin Xu", "Dazhi He", "Yunfeng Guan", "Wenjun Zhang"], "title": "AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets.", "AI": {"tldr": "This paper proposes AI2MMUM, a scalable task-aware AI-air interface multi-modal universal model for 6G systems, demonstrating superior performance in various physical layer tasks.", "motivation": "To design a universal model capable of processing multi-modal data and executing diverse air interface tasks for future wireless systems.", "method": "Proposes AI2MMUM with LLM backbone, fine-tuning, fixed task keywords, learnable prefix prompts, frozen radio modality encoders, adapter layers, and lightweight task-specific heads.", "result": "Achieves SOTA performance across five representative physical environment/wireless channel-based downstream tasks using WAIR-D and DeepMIMO datasets.", "conclusion": "AI2MMUM demonstrates flexibility and effectiveness in performing various physical layer tasks according to subtle task instructions."}}
{"id": "2505.10446", "pdf": "https://arxiv.org/pdf/2505.10446", "abs": "https://arxiv.org/abs/2505.10446", "authors": ["Zemin Huang", "Zhiyang Chen", "Zijun Wang", "Tiancheng Li", "Guo-Jun Qi"], "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.", "AI": {"tldr": "DCoLT is a novel reasoning framework for diffusion language models, which optimizes the entire reasoning trajectory with reinforcement learning to improve the correctness of the final answer.", "motivation": "To develop a more flexible and effective reasoning method for diffusion language models compared to traditional chain-of-thought methods.", "method": "DCoLT treats each intermediate step as a latent 'thinking' action and uses outcome-based reinforcement learning to optimize the reasoning trajectory.", "result": "Experiments on math and code generation tasks show that DCoLT-reinforced DLMs outperform other DLMs trained by supervised fine-tuning or reinforcement learning.", "conclusion": "DCoLT improves the reasoning accuracy of diffusion language models significantly across different datasets and tasks."}}
{"id": "2505.10238", "pdf": "https://arxiv.org/pdf/2505.10238", "abs": "https://arxiv.org/abs/2505.10238", "authors": ["Yanbo Ding"], "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.", "AI": {"tldr": "This paper introduces MTVCrafter, a novel framework for human image animation that directly models 4D motion sequences instead of relying on 2D pose images. It uses 4DMoT to tokenize 3D motion and MV-DiT for animation, achieving superior performance and generalization.", "motivation": "Existing methods rely on 2D-rendered pose images for motion guidance, limiting generalization and losing essential 3D information.", "method": "Proposes MTVCrafter, including 4DMoT for tokenizing 3D motion into 4D motion tokens and MV-DiT for leveraging these tokens in animation.", "result": "Achieves state-of-the-art results with an FID-VID of 6.98, surpassing previous methods, and shows good generalization to various characters and scenarios.", "conclusion": "Introduces a new direction for pose-guided human video generation by effectively utilizing 4D motion information."}}
{"id": "2505.10007", "pdf": "https://arxiv.org/pdf/2505.10007", "abs": "https://arxiv.org/abs/2505.10007", "authors": ["Zijun Chen", "Shengbo Wang", "Nian Si"], "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments.", "AI": {"tldr": "This paper studies distributionally robust average-reward reinforcement learning, proposing two algorithms with near-optimal sample complexity.", "motivation": "Stable long-term performance in critical applications like robotics, operations research, and healthcare.", "method": "Proposes two algorithms: one reduces the problem to a DR discounted MDP, the other introduces an anchoring state to stabilize the transition kernels.", "result": "Both algorithms attain a sample complexity of O(|S||A|t_mix^2\u03b5^-2) under certain conditions.", "conclusion": "The first finite-sample convergence guarantee for DR average-reward reinforcement learning is achieved."}}
{"id": "2505.10493", "pdf": "https://arxiv.org/pdf/2505.10493", "abs": "https://arxiv.org/abs/2505.10493", "authors": ["Shaohan Wang", "Licheng Zhang", "Zheren Fu", "Zhendong Mao"], "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.", "AI": {"tldr": "This paper introduces CL-RAG, a multi-stage Curriculum Learning-based framework for training Retrieval-Augmented Generation (RAG) systems. By constructing training data with varying difficulty levels and training models progressively from easy to difficult tasks, CL-RAG enhances the generalization ability of both retrievers and generators within the RAG system. The approach improves performance by 2%-4% across four open-domain QA datasets compared to several advanced methods.", "motivation": "To address the issue of varying document effectiveness in RAG systems and improve the adaptability of retrievers and generators during training, inspired by human cognitive learning principles.", "method": "A multi-stage Curriculum Learning-based framework named CL-RAG that constructs training data with different difficulty levels and trains models progressively.", "result": "CL-RAG shows consistent effectiveness across four open-domain QA datasets, improving performance by 2%-4% over multiple advanced methods.", "conclusion": "The proposed CL-RAG framework enhances the generalization ability of RAG systems, achieving better performance across different datasets."}}
{"id": "2505.10250", "pdf": "https://arxiv.org/pdf/2505.10250", "abs": "https://arxiv.org/abs/2505.10250", "authors": ["Wenhao Shen", "Wanqi Yin", "Xiaofeng Yang", "Cheng Chen", "Chaoyue Song", "Zhongang Cai", "Lei Yang", "Hao Wang", "Guosheng Lin"], "title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR", "summary": "Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.", "AI": {"tldr": "Proposes ADHMR, a framework that aligns a diffusion-based HMR model through preference optimization. Uses an HMR-Scorer model to evaluate predictions and improve existing HMR models.", "motivation": "To address depth ambiguity and occlusions in human mesh recovery from a single image, especially for in-the-wild images.", "method": "Trains an HMR-Scorer model, creates a preference dataset, and uses it to fine-tune the base model via direct preference optimization.", "result": "ADHMR outperforms current state-of-the-art methods.", "conclusion": "The proposed ADHMR framework improves the alignment of 3D human mesh predictions with 2D image observations and enhances robustness to in-the-wild images."}}
{"id": "2505.10010", "pdf": "https://arxiv.org/pdf/2505.10010", "abs": "https://arxiv.org/abs/2505.10010", "authors": ["Jing-Cheng Pang", "Kaiyuan Li", "Yidi Wang", "Si-Hang Yang", "Shengyi Jiang", "Yang Yu"], "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts", "categories": ["cs.LG"], "comment": null, "summary": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.", "AI": {"tldr": "This paper introduces ImagineBench, a new benchmark for evaluating offline RL algorithms that use both real and synthetic data from large language models.", "motivation": "Reinforcement learning depends heavily on extensive real-world interaction data. Recent work shows that large language models can generate synthetic experience (imaginary rollouts) to master novel tasks, but progress in this field is hindered due to the lack of a standard benchmark.", "method": "Introduce ImagineBench, the first comprehensive benchmark for evaluating offline RL algorithms that leverage both real rollouts and LLM-imaginary rollouts.", "result": "Existing offline RL algorithms achieve only 35.44% success rate in hard tasks when using LLM-imaginary rollouts, compared to 64.37% when trained on real rollouts.", "conclusion": "Simply applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, highlighting the need for algorithm advancements to better leverage LLM-imaginary rollouts."}}
{"id": "2505.10494", "pdf": "https://arxiv.org/pdf/2505.10494", "abs": "https://arxiv.org/abs/2505.10494", "authors": ["Yutao Mou", "Xiao Deng", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.", "AI": {"tldr": "CoV-Eval is a multi-task benchmark for evaluating LLM code security, including tasks like code completion, vulnerability repair, detection and classification. VC-Judge is an improved model for reviewing LLM-generated programs for vulnerabilities.", "motivation": "Current code security benchmarks lack comprehensive assessment across multiple dimensions of code security.", "method": "Proposed CoV-Eval benchmark and developed VC-Judge model.", "result": "Most LLMs identify vulnerable codes well but struggle with generating secure codes, recognizing specific vulnerability types and performing repairs.", "conclusion": "Comprehensive experiments and analyses offer insights for future research in LLM code security."}}
{"id": "2505.10257", "pdf": "https://arxiv.org/pdf/2505.10257", "abs": "https://arxiv.org/abs/2505.10257", "authors": ["Hao Lu", "Jiaqi Tang", "Jiyao Wang", "Yunfan LU", "Xu Cao", "Qingyong Hu", "Yin Wang", "Yuting Zhang", "Tianxin Xie", "Yunpeng Zhang", "Yong Chen", "Jiayu. Gao", "Bin Huang", "Dengbo He", "Shuiguang Deng", "Hao Chen", "Ying-Cong Chen"], "title": "Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot", "categories": ["cs.CV"], "comment": null, "summary": "The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.", "AI": {"tldr": "This paper presents SAGE DeeR, a driving agent that adapts to individual preferences, understands complex inputs, and improves through implicit learning.", "motivation": "To meet the comfort, interaction, and safety needs of different users in intelligent driving cockpits.", "method": "Developing a Super-Aligned and GEneralist DRiving agent that aligns with individual preferences, processes multi-modal inputs, and uses self-eliciting for improved capabilities.", "result": "Achieved super alignment, generalist understanding, and self-eliciting features; created a large-scale benchmark for measuring perceptual decision-making and alignment accuracy.", "conclusion": "SAGE DeeR demonstrates advanced adaptability and understanding capabilities in intelligent driving environments."}}
{"id": "2505.10037", "pdf": "https://arxiv.org/pdf/2505.10037", "abs": "https://arxiv.org/abs/2505.10037", "authors": ["Takafumi Ito", "Lysenko Artem", "Tatsuhiko Tsunoda"], "title": "Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "comment": "10 pages, 3 figures", "summary": "Quantum-classical Hybrid Machine Learning (QHML) models are recognized for\ntheir robust performance and high generalization ability even for relatively\nsmall datasets. These qualities offer unique advantages for anti-cancer drug\nresponse prediction, where the number of available samples is typically small.\nHowever, such hybrid models appear to be very sensitive to the data encoding\nused at the interface of a neural network and a quantum circuit, with\nsuboptimal choices leading to stability issues. To address this problem, we\npropose a novel strategy that uses a normalization function based on a\nmoderated gradient version of the $\\tanh$. This method transforms the outputs\nof the neural networks without concentrating them at the extreme value ranges.\nOur idea was evaluated on a dataset of gene expression and drug response\nmeasurements for various cancer cell lines, where we compared the prediction\nperformance of a classical deep learning model and several QHML models. These\nresults confirmed that QHML performed better than the classical models when\ndata was optimally normalized. This study opens up new possibilities for\nbiomedical data analysis using quantum computers.", "AI": {"tldr": "This paper introduces a new normalization strategy for improving the stability of Quantum-classical Hybrid Machine Learning models used in predicting anti-cancer drug responses.", "motivation": "To improve the stability of QHML models by addressing their sensitivity to data encoding, especially when dealing with small datasets in biomedical applications.", "method": "Proposes a normalization function based on a moderated gradient version of tanh to transform neural network outputs, preventing concentration at extreme value ranges.", "result": "The proposed method was tested on a dataset of gene expression and drug response measurements, showing improved prediction performance of QHML models over classical models when data was optimally normalized.", "conclusion": "This research suggests potential advancements in biomedical data analysis using quantum computers."}}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507", "abs": "https://arxiv.org/abs/2505.10507", "authors": ["Benedikt Ebing", "Goran Glava\u0161"], "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.", "AI": {"tldr": "This work investigates low-level design decisions in using word aligners for label projection in translation-based cross-lingual transfer for token classification tasks.", "motivation": "To improve the performance of translation-based XLT for token classification tasks by optimizing low-level design decisions in label projection using word aligners.", "method": "Systematically investigate the effects of different algorithms for projecting labels between multi-token spans, filtering strategies to reduce noisy mappings, and pre-tokenization of translated sentences.", "result": "Optimized word aligner-based label projection performs comparably or better than marker-based methods, and an ensemble method further improves performance while reducing sensitivity to design choices.", "conclusion": "Optimizing low-level design decisions in word aligner-based label projection can significantly enhance translation-based XLT for token classification tasks."}}
{"id": "2505.10258", "pdf": "https://arxiv.org/pdf/2505.10258", "abs": "https://arxiv.org/abs/2505.10258", "authors": ["Michael Hubbertz", "Pascal Colling", "Qi Han", "Tobias Meisen"], "title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper was accepted at the CVPR WAD 2025 Workshop", "summary": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.", "AI": {"tldr": "This paper proposes an innovative offline mapping approach that incorporates driver trails into the map creation process using transformer-based deep learning models, which outperforms state-of-the-art online mapping approaches.", "motivation": "To address the challenges faced by online mapping such as temporal consistency, sensor occlusion, runtime, and generalization.", "method": "Integrating trail data from the ego vehicle and other traffic participants to construct a comprehensive global map using transformer-based deep learning models.", "result": "The proposed method demonstrates superior performance compared to state-of-the-art online mapping approaches, with improved generalization to unseen environments and sensor configurations.", "conclusion": "This offline mapping approach enables continuous updates while remaining sensor-agnostic, facilitating efficient data transfer."}}
{"id": "2505.10039", "pdf": "https://arxiv.org/pdf/2505.10039", "abs": "https://arxiv.org/abs/2505.10039", "authors": ["Hang Chen", "Jiaying Zhu", "Xinyu Yang", "Wenya Wang"], "title": "Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Circuit discovery has gradually become one of the prominent methods for\nmechanistic interpretability, and research on circuit completeness has also\ngarnered increasing attention. Methods of circuit discovery that do not\nguarantee completeness not only result in circuits that are not fixed across\ndifferent runs but also cause key mechanisms to be omitted. The nature of\nincompleteness arises from the presence of OR gates within the circuit, which\nare often only partially detected in standard circuit discovery methods. To\nthis end, we systematically introduce three types of logic gates: AND, OR, and\nADDER gates, and decompose the circuit into combinations of these logical\ngates. Through the concept of these gates, we derive the minimum requirements\nnecessary to achieve faithfulness and completeness. Furthermore, we propose a\nframework that combines noising-based and denoising-based interventions, which\ncan be easily integrated into existing circuit discovery methods without\nsignificantly increasing computational complexity. This framework is capable of\nfully identifying the logic gates and distinguishing them within the circuit.\nIn addition to the extensive experimental validation of the framework's ability\nto restore the faithfulness, completeness, and sparsity of circuits, using this\nframework, we uncover fundamental properties of the three logic gates, such as\ntheir proportions and contributions to the output, and explore how they behave\namong the functionalities of language models.", "AI": {"tldr": "This paper introduces a framework that improves circuit discovery methods by fully identifying and distinguishing logic gates, ensuring faithfulness, completeness, and sparsity of circuits.", "motivation": "To address the incompleteness issue in standard circuit discovery methods caused by partially detecting OR gates.", "method": "Introduces three types of logic gates (AND, OR, ADDER), decomposes circuits into combinations of these gates, and proposes a framework combining noising-based and denoising-based interventions.", "result": "The framework successfully restores the faithfulness, completeness, and sparsity of circuits through extensive experiments.", "conclusion": "Uncovered fundamental properties of logic gates and their contributions to language model functionalities."}}
{"id": "2505.10518", "pdf": "https://arxiv.org/pdf/2505.10518", "abs": "https://arxiv.org/abs/2505.10518", "authors": ["Anastasios Gerontopoulos", "Spyros Gidaris", "Nikos Komodakis"], "title": "Multi-Token Prediction Needs Registers", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.", "AI": {"tldr": "Proposes MuToR, an effective method for multi-token prediction that can be easily integrated into existing language models for better performance in fine-tuning and pretraining.", "motivation": "Existing multi-token prediction methods have not consistently improved performance across different settings like fine-tuning, so there is a need for a more versatile method.", "method": "Introducing learnable register tokens into the input sequence and having each token predict future targets.", "result": "MuToR shows effectiveness and versatility in various use cases including supervised fine-tuning, parameter-efficient fine-tuning, and pretraining for both language and vision domains.", "conclusion": "MuToR is a simple and effective approach for multi-token prediction that can be easily integrated into existing pretrained language models without requiring any architectural changes."}}
{"id": "2505.10267", "pdf": "https://arxiv.org/pdf/2505.10267", "abs": "https://arxiv.org/abs/2505.10267", "authors": ["Pavel Korotaev", "Petr Surovtsev", "Alexander Kapitanov", "Karina Kvanchiani", "Aleksandr Nagaev"], "title": "HandReader: Advanced Techniques for Efficient Fingerspelling Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/ai-forever/handreader", "summary": "Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.", "AI": {"tldr": "This paper presents HandReader, a set of three architectures that improve fingerspelling recognition accuracy using RGB video and keypoint data.", "motivation": "Previous methods on fingerspelling recognition have mainly focused on processing the temporal dimension of videos but have left room for improvement in accuracy.", "method": "The paper proposes HandReader which includes HandReader_RGB, HandReader_KP, and HandReader_RGB+KP. HandReader_RGB uses the Temporal Shift-Adaptive Module (TSAM) to process RGB video features. HandReader_KP is based on the Temporal Pose Encoder (TPE) using keypoints as tensors, and HandReader_RGB+KP has a joint encoder to combine both modalities.", "result": "The HandReader models outperform existing approaches on the ChicagoFSWild and ChicagoFSWild+ datasets. They also show high performance on the new Znaki dataset for Russian fingerspelling.", "conclusion": "The paper introduces HandReader, a set of three architectures that achieve state-of-the-art results on the ChicagoFSWild and ChicagoFSWild+ datasets for fingerspelling recognition. Additionally, the models perform well on the newly introduced Znaki dataset for Russian fingerspelling."}}
{"id": "2505.10040", "pdf": "https://arxiv.org/pdf/2505.10040", "abs": "https://arxiv.org/abs/2505.10040", "authors": ["Lei Song", "Jiaxing Li", "Shihan Guan", "Youyong Kong"], "title": "Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their\ncapacity to preserve previously acquired knowledge amid the assimilation of\nnovel information. Rehearsal-based techniques revisit historical examples,\nadopted as a principal strategy to alleviate this phenomenon. However, memory\nexplosion and privacy infringements impose significant constraints on their\nutility. Non-Exemplar methods circumvent the prior issues through Prototype\nReplay (PR), yet feature drift presents new challenges. In this paper, our\nempirical findings reveal that Prototype Contrastive Learning (PCL) exhibits\nless pronounced drift than conventional PR. Drawing upon PCL, we propose\nInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar\nContinual Graph Learning (NECGL). Exploiting graph structural information, we\nformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature\ndistributions towards high-impact nodes to augment the model's capacity for\nassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)\nsafeguards task memory by regularizing discontinuities in class relationships.\nMoreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,\nfostering greater inter-class discriminability. Evaluations on four node\nclassification benchmark datasets demonstrate that our method outperforms\nexisting state-of-the-art methods, achieving a better trade-off between\nplasticity and stability.", "AI": {"tldr": "This paper introduces Instance-Prototype Affinity Learning (IPAL), a novel approach for Non-Exemplar Continual Graph Learning (NECGL) using Prototype Contrastive Learning (PCL) to address the issue of catastrophic forgetting in Graph Neural Networks (GNN) without revisiting historical data.", "motivation": "To solve the problem of catastrophic forgetting in GNNs when learning new information, while avoiding memory explosion and privacy concerns associated with rehearsal-based techniques.", "method": "Introduce IPAL which uses Topology-Integrated Gaussian Prototypes (TIGP) and Instance-Prototype Affinity Distillation (IPAD) to guide feature distributions and maintain task memory respectively. Also, include a Decision Boundary Perception (DBP) mechanism within PCL.", "result": "Performance evaluations on four node classification benchmarks show that IPAL outperforms current state-of-the-art methods in balancing plasticity and stability.", "conclusion": "The proposed NECGL framework, leveraging PCL, demonstrates improved performance in continual learning tasks for GNNs without the need for historical data."}}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527", "abs": "https://arxiv.org/abs/2505.10527", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "title": "WorldPM: Scaling Human Preference Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "AI": {"tldr": "World Preference Modeling (WorldPM) is proposed to explore the scaling potential in preference modeling. Extensive experiments demonstrate its effectiveness for preference fine-tuning and generalization performance improvement.", "motivation": "Scaling laws in language modeling inspire the study of similar laws in preference modeling.", "method": "Collecting preference data from public forums and conducting extensive training with models of different sizes.", "result": "Adversarial metrics scale up, objective metrics show emergent behavior, subjective metrics don't demonstrate scaling trends. WorldPM improves generalization performance across various human preference datasets and enhances in-house and public evaluation sets.", "conclusion": "WorldPM demonstrates scalability potential and effectiveness as a foundation for preference fine-tuning."}}
{"id": "2505.10281", "pdf": "https://arxiv.org/pdf/2505.10281", "abs": "https://arxiv.org/abs/2505.10281", "authors": ["Mengqiu Xu", "Kaixin Chen", "Heng Guo", "Yixiang Huang", "Ming Wu", "Zhenwei Shi", "Chuang Zhang", "Jun Guo"], "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.", "AI": {"tldr": "Introduce MFogHub, a multi-regional and multi-satellite dataset for marine fog detection and forecasting, comprising over 68,000 high-resolution samples from 15 regions and 6 satellites.", "motivation": "Limited availability of open-source datasets for marine fog detection and forecasting, especially those that can evaluate model performance across diverse conditions.", "method": "Create a new dataset called MFogHub which integrates annotated marine fog observations from multiple regions and satellites.", "result": "MFogHub contains over 68,000 high-resolution samples from 15 coastal fog-prone regions and six geostationary satellites.", "conclusion": "MFogHub can help reveal generalization fluctuations and serve as a valuable resource for developing scalable fog prediction techniques."}}
{"id": "2505.10050", "pdf": "https://arxiv.org/pdf/2505.10050", "abs": "https://arxiv.org/abs/2505.10050", "authors": ["Fahad Almalki", "Mehedi Masud"], "title": "Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional machine learning models often prioritize predictive accuracy,\noften at the expense of model transparency and interpretability. The lack of\ntransparency makes it difficult for organizations to comply with regulatory\nrequirements and gain stakeholders trust. In this research, we propose a fraud\ndetection framework that combines a stacking ensemble of well-known gradient\nboosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable\nartificial intelligence (XAI) techniques are used to enhance the transparency\nand interpretability of the model's decisions. We used SHAP (SHapley Additive\nExplanations) for feature selection to identify the most important features.\nFurther efforts were made to explain the model's predictions using Local\nInterpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots\n(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection\ndataset, which includes more than 590,000 real transaction records, was used to\nevaluate the proposed model. The model achieved a high performance with an\naccuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent\nrelated approaches. These results indicate that combining high prediction\naccuracy with transparent interpretability is possible and could lead to a more\nethical and trustworthy solution in financial fraud detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u4e0eXAI\u6280\u672f\u7684\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u826f\u597d\u53ef\u89e3\u91ca\u6027\u7684\u7edf\u4e00\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u867d\u7136\u6709\u8f83\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42\u5e76\u83b7\u5f97\u5229\u76ca\u76f8\u5173\u8005\u7684\u4fe1\u4efb\u3002", "method": "\u4f7f\u7528XGBoost\u3001LightGBM\u548cCatBoost\u7684\u53e0\u52a0\u96c6\u6210\uff0c\u5e76\u7ed3\u5408SHAP\u3001LIME\u3001PDP\u548cPFI\u7b49XAI\u6280\u672f\u63d0\u9ad8\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6a21\u578b\u5728IEEE-CIS\u6b3a\u8bc8\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8699%\u7684\u51c6\u786e\u7387\u548c0.99\u7684AUC-ROC\u5206\u6570\uff0c\u4f18\u4e8e\u591a\u4e2a\u8fd1\u671f\u76f8\u5173\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u7684\u53e0\u52a0\u96c6\u6210\u6846\u67b6\u4e0eXAI\u6280\u672f\u5728\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\u4e0e\u900f\u660e\u89e3\u91ca\u6027\u7684\u7edf\u4e00\u3002"}}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554", "abs": "https://arxiv.org/abs/2505.10554", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment", "AI": {"tldr": "Meta-ability alignment improves reasoning capabilities of large models effectively.", "motivation": "To address the limitations of unpredictability and uncontrollability of emergent reasoning behaviors in large reasoning models.", "method": "Using automatically generated, self-verifiable tasks and a three stage-pipeline including individual alignment, parameter-space merging, and domain-specific reinforcement learning.", "result": "Performance boost by over 10% relative to instruction-tuned baselines and an additional 2% average gain in performance ceiling across math, coding, and science benchmarks.", "conclusion": "Explicit meta-ability alignment offers a scalable and dependable foundation for reasoning."}}
{"id": "2505.10289", "pdf": "https://arxiv.org/pdf/2505.10289", "abs": "https://arxiv.org/abs/2505.10289", "authors": ["Yue Wang", "Shuai Xu", "Xuelin Zhu", "Yicong Li"], "title": "MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u578b\uff0c\u6539\u8fdb\u4e86\u7ec6\u7c92\u5ea6\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u7684\u6355\u6349\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4e8eCLIP\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u4f46\u5ffd\u7565\u4e86\u5176\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\uff08MSCI\uff09\u6a21\u578b\uff0c\u5229\u7528CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e2d\u95f4\u5c42\u4fe1\u606f\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u81ea\u9002\u5e94\u805a\u5408\u5668\u5206\u522b\u63d0\u53d6\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u4e2d\u7684\u5c40\u90e8\u4fe1\u606f\u548c\u9ad8\u5c42\u89c6\u89c9\u7279\u5f81\u4e2d\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u9010\u6b65\u4e92\u52a8\u673a\u5236\u5c06\u5176\u878d\u5165\u6587\u672c\u8868\u793a\u4e2d\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684MSCI\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\uff08MSCI\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e24\u4e2a\u81ea\u9002\u5e94\u805a\u5408\u5668\u6765\u63d0\u53d6\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u4e2d\u7684\u5c40\u90e8\u4fe1\u606f\u548c\u9ad8\u5c42\u89c6\u89c9\u7279\u5f81\u4e2d\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u9010\u6b65\u4e92\u52a8\u673a\u5236\u5c06\u5176\u878d\u5165\u6587\u672c\u8868\u793a\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u7ec6\u7c92\u5ea6\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\u3002\u6b64\u5916\uff0cMSCI \u6a21\u578b\u8fd8\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u7684\u7ec4\u5408\u4ee5\u53ca\u540c\u4e00\u7ec4\u5408\u5185\u7684\u4e0d\u540c\u5143\u7d20\u52a8\u6001\u8c03\u6574\u5168\u5c40\u548c\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u4f7f\u5176\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5404\u79cd\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4e09\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.10057", "pdf": "https://arxiv.org/pdf/2505.10057", "abs": "https://arxiv.org/abs/2505.10057", "authors": ["Tiancong Cheng", "Ying Zhang", "Yuxuan Liang", "Roger Zimmermann", "Zhiwen Yu", "Bin Guo"], "title": "JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation", "categories": ["cs.LG"], "comment": null, "summary": "Depth estimation and scene segmentation are two important tasks in\nintelligent transportation systems. A joint modeling of these two tasks will\nreduce the requirement for both the storage and training efforts. This work\nexplores how the multi-task distillation could be used to improve such unified\nmodeling. While existing solutions transfer multiple teachers' knowledge in a\nstatic way, we propose a self-adaptive distillation method that can dynamically\nadjust the knowledge amount from each teacher according to the student's\ncurrent learning ability. Furthermore, as multiple teachers exist, the\nstudent's gradient update direction in the distillation is more prone to be\nerroneous where knowledge forgetting may occur. To avoid this, we propose a\nknowledge trajectory to record the most essential information that a model has\nlearnt in the past, based on which a trajectory-based distillation loss is\ndesigned to guide the student to follow the learning curve similarly in a\ncost-effective way. We evaluate our method on multiple benchmarking datasets\nincluding Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,\nour method achieves a clearly improvement. The code is provided in the\nsupplementary materials.", "AI": {"tldr": "This work explores how the multi-task distillation could be used to improve the unified modeling of depth estimation and scene segmentation tasks.", "motivation": "A joint modeling of depth estimation and scene segmentation tasks in intelligent transportation systems will reduce the requirement for both the storage and training efforts.", "method": "self-adaptive distillation method and a trajectory-based distillation loss", "result": "Our method achieves a clearly improvement.", "conclusion": "Compared to the state-of-the-art solutions, our method achieves a clearly improvement."}}
{"id": "2505.10292", "pdf": "https://arxiv.org/pdf/2505.10292", "abs": "https://arxiv.org/abs/2505.10292", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation", "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "31 pages, 14 figures", "summary": "Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.", "AI": {"tldr": "A new dataset named StoryReasoning with 4,178 stories from movie images helps improve visual storytelling by maintaining consistent characters and objects.", "motivation": "To solve the problems of referential hallucinations in visual storytelling systems.", "method": "Introduce cross-frame object re-identification, chain-of-thought reasoning, and a grounding scheme linking text to visual entities.", "result": "Fine-tuned Qwen2.5-VL 7B improves storytelling performance reducing hallucinations by 12.3%.", "conclusion": "The proposed approach effectively maintains character and object consistency in visual storytelling."}}
{"id": "2505.10083", "pdf": "https://arxiv.org/pdf/2505.10083", "abs": "https://arxiv.org/abs/2505.10083", "authors": ["Chengsen Wang", "Qi Qi", "Zhongwen Rao", "Lujia Pan", "Jingyu Wang", "Jianxin Liao"], "title": "ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.", "AI": {"tldr": "This paper proposes ChronoSteer, a multimodal time series forecasting model that integrates large language models and time series foundation models via text-to-instruction transformation. It introduces a decoupled framework and a two-stage training strategy to handle data scarcity, achieving superior performance.", "motivation": "To leverage both temporal and textual information for better forecasting by combining large language models and time series foundation models.", "method": "A decoupled framework where textual events are transformed into revision instructions to guide the output of time series foundation models. A two-stage training strategy using synthetic data is also devised.", "result": "ChronoSteer outperforms unimodal models by 25.7% and surpasses the previous best multimodal method by 22.5% in prediction accuracy.", "conclusion": "The integration of large language models and time series foundation models through text-to-instruction transformation shows great promise in multimodal time series forecasting."}}
{"id": "2505.10294", "pdf": "https://arxiv.org/pdf/2505.10294", "abs": "https://arxiv.org/abs/2505.10294", "authors": ["Guillaume Balezo", "Roger Trullo", "Albert Pla Planas", "Etienne Decenciere", "Thomas Walter"], "title": "MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models", "categories": ["cs.CV", "q-bio.TO", "68T07 (Primary), 92C55 (Secondary)", "I.4.9; I.2.10; I.5.4; J.3"], "comment": null, "summary": "Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.", "AI": {"tldr": "MIPHEI uses H&E images to predict mIF signals for comprehensive marker panels, achieving accurate cell-type classification and showing potential for large-scale H&E dataset analysis.", "motivation": "To enable more precise cell type identification via proteomic markers without the cost and logistical constraints of mIF, bridging the gap between H&E and mIF analysis.", "method": "A U-Net-inspired architecture integrating ViT foundation models as encoders to predict mIF signals from H&E images.", "result": "Accurate cell-type classification from H&E alone with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, outperforming a state-of-the-art baseline and a random classifier for most markers.", "conclusion": "MIPHEI effectively captures complex relationships between nuclear morphologies and molecular markers, offering a promising approach for cell-type-aware analysis of large-scale H&E datasets."}}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117", "abs": "https://arxiv.org/abs/2505.10117", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfei Wang", "Jun Luo"], "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.", "AI": {"tldr": "This paper presents MiCo, a hierarchical language agent framework using large language models to solve the Online Dynamic Multidimensional Bin Packing problem in cloud VM scheduling, demonstrating strong performance in large-scale and fluctuating demand scenarios.", "motivation": "To address limitations of traditional methods in handling large-scale complexity and fluctuating demands in cloud VM scheduling.", "method": "Proposes MiCo framework with two stages: Option Miner discovers non-context-aware strategies via LLMs, and Option Composer integrates them with contextual strategies.", "result": "Achieves a 96.9% competitive ratio in large-scale scenarios with over 10,000 VMs, maintaining high performance under nonstationary requests and diverse configurations.", "conclusion": "MiCo demonstrates effectiveness in complex and large-scale cloud environments through extensive real-world experiments."}}
{"id": "2505.10351", "pdf": "https://arxiv.org/pdf/2505.10351", "abs": "https://arxiv.org/abs/2505.10351", "authors": ["Jie Zhu", "Jirong Zha", "Ding Li", "Leye Wang"], "title": "A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability", "categories": ["cs.CV"], "comment": "An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders", "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.", "AI": {"tldr": "This paper investigates membership inference on visual self-supervised models under a more realistic black-box attack setting. A unified method called PartCrop is proposed which crops parts of objects in an image to query responses within the image in representation space. Extensive attacks and defenses are conducted showing the effectiveness and generalization of PartCrop as well as proposing a scalable PartCrop-v2.", "motivation": "Investigate membership inference on visual self-supervised models under a more realistic black-box attack setting.", "method": "Propose a unified method called PartCrop which crops parts of objects in an image to query responses within the image in representation space.", "result": "Extensive attacks and defenses are conducted showing the effectiveness and generalization of PartCrop.", "conclusion": "Propose a scalable PartCrop-v2 by introducing two structural improvements to PartCrop."}}
{"id": "2505.10120", "pdf": "https://arxiv.org/pdf/2505.10120", "abs": "https://arxiv.org/abs/2505.10120", "authors": ["Guillaume Godin"], "title": "All You Need Is Synthetic Task Augmentation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 3 Figures, 6 tables", "summary": "Injecting rule-based models like Random Forests into differentiable neural\nnetwork frameworks remains an open challenge in machine learning. Recent\nadvancements have demonstrated that pretrained models can generate efficient\nmolecular embeddings. However, these approaches often require extensive\npretraining and additional techniques, such as incorporating posterior\nprobabilities, to boost performance. In our study, we propose a novel strategy\nthat jointly trains a single Graph Transformer neural network on both sparse\nmultitask molecular property experimental targets and synthetic targets derived\nfrom XGBoost models trained on Osmordred molecular descriptors. These synthetic\ntasks serve as independent auxiliary tasks. Our results show consistent and\nsignificant performance improvement across all 19 molecular property prediction\ntasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms\nthe XGBoost single-task learner. This demonstrates that synthetic task\naugmentation is an effective method for enhancing neural model performance in\nmultitask molecular property prediction without the need for feature injection\nor pretraining.", "AI": {"tldr": "A novel multitask Graph Transformer approach enhances molecular property prediction by using synthetic tasks generated from XGBoost models.", "motivation": "To address the challenge of integrating rule-based models into differentiable neural networks for molecular property prediction.", "method": "Jointly training a Graph Transformer neural network on both experimental targets and synthetic targets derived from XGBoost models trained on molecular descriptors.", "result": "Performance improvements were observed across all 19 molecular property prediction tasks, with the multitask Graph Transformer outperforming the XGBoost single-task learner for 16 out of 19 targets.", "conclusion": "The proposed multitask Graph Transformer model, which uses synthetic tasks derived from XGBoost models, consistently improves performance across 19 molecular property prediction tasks."}}
{"id": "2505.10352", "pdf": "https://arxiv.org/pdf/2505.10352", "abs": "https://arxiv.org/abs/2505.10352", "authors": ["Shihao Zou", "Qingfeng Li", "Wei Ji", "Jingjing Li", "Yongkui Yang", "Guoqi Li", "Chao Dong"], "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\\mathcal{O}(T)$ Complexity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer", "AI": {"tldr": "An efficient spike-driven video Transformer called SpikeVideoFormer is introduced with linear temporal complexity.", "motivation": "Existing SNN-based Transformers mainly focus on single-image tasks without effectively leveraging SNNs' efficiency in video-based vision tasks.", "method": "A spike-driven Hamming attention (SDHA) is designed to provide a theoretically guided adaptation from traditional real-valued attention to spike-driven attention. Various spike-driven space-time attention designs are analyzed and an optimal scheme is identified.", "result": "The model demonstrates generalization ability and efficiency across diverse downstream video tasks, achieving state-of-the-art performance with over 15% improvement on the latter two tasks compared to existing SNN approaches. It also matches the performance of recent ANN-based methods while offering significant efficiency gains.", "conclusion": "SpikeVideoFormer achieves state-of-the-art performance in video tasks while maintaining superior energy efficiency."}}
{"id": "2505.10125", "pdf": "https://arxiv.org/pdf/2505.10125", "abs": "https://arxiv.org/abs/2505.10125", "authors": ["Wujun Zhou", "Shu Ding", "ZeLin Li", "Wei Wang"], "title": "Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning enables the clients to collaboratively train a global\nmodel, which is aggregated from local models. Due to the heterogeneous data\ndistributions over clients and data privacy in federated learning, it is\ndifficult to train local models to achieve a well-performed global model. In\nthis paper, we introduce the adaptability of local models, i.e., the average\nperformance of local models on data distributions over clients, and enhance the\nperformance of the global model by improving the adaptability of local models.\nSince each client does not know the data distributions over other clients, the\nadaptability of the local model cannot be directly optimized. First, we provide\nthe property of an appropriate local model which has good adaptability on the\ndata distributions over clients. Then, we formalize the property into the local\ntraining objective with a constraint and propose a feasible solution to train\nthe local model. Extensive experiments on federated learning benchmarks\ndemonstrate that our method significantly improves the adaptability of local\nmodels and achieves a well-performed global model that consistently outperforms\nthe baseline methods.", "AI": {"tldr": "This paper introduces the adaptability of local models and enhances the performance of the global model by improving the adaptability of local models.", "motivation": "In federated learning, due to the heterogeneous data distributions over clients and data privacy, it is difficult to train local models to achieve a well-performed global model.", "method": "Introduce the adaptability of local models and enhance the performance of the global model by improving the adaptability of local models. Formalize the property into the local training objective with a constraint and propose a feasible solution to train the local model.", "result": "Extensive experiments on federated learning benchmarks demonstrate that our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods.", "conclusion": "Our method significantly improves the adaptability of local models and achieves a well-performed global model that consistently outperforms the baseline methods."}}
{"id": "2505.10420", "pdf": "https://arxiv.org/pdf/2505.10420", "abs": "https://arxiv.org/abs/2505.10420", "authors": ["Andrei Arhire", "Radu Timofte"], "title": "Learned Lightweight Smartphone ISP with Unpaired Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW 2025", "summary": "The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .", "AI": {"tldr": "This paper proposes an unpaired training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content.", "motivation": "Developing a learned ISP is a difficult and costly step due to the requirement of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images.", "method": "An unpaired training method with a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks.", "result": "Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics.", "conclusion": "Our unpaired approach can eliminate the need for direct correspondences between raw images and ground-truth data with matching content."}}
{"id": "2505.10128", "pdf": "https://arxiv.org/pdf/2505.10128", "abs": "https://arxiv.org/abs/2505.10128", "authors": ["Huy Q. Le", "Latif U. Khan", "Choong Seon Hong"], "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": "IWCMC 2025", "summary": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.", "AI": {"tldr": "This study presents FedAPC, a novel prototype-based federated learning framework that improves the generalization ability of the global model under domain heterogeneity by using prototype augmentation. The method enhances feature diversity and model robustness through aligning local features with global prototypes.", "motivation": "To address the challenge of domain heterogeneity in Federated Learning which hinders the global model's convergence.", "method": "FedAPC uses prototypes derived from augmented data mean features to capture richer representations and aligns local features with global prototypes.", "result": "The framework outperforms state-of-the-art baselines on the Office-10 and Digits datasets, showing superior performance.", "conclusion": "FedAPC effectively improves the generalization ability of the global model under domain heterogeneity in Federated Learning."}}
{"id": "2505.10453", "pdf": "https://arxiv.org/pdf/2505.10453", "abs": "https://arxiv.org/abs/2505.10453", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "title": "Vision language models have difficulty recognizing virtual objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "AI": {"tldr": "This study examines the capability of vision language models (VLMs) in understanding visuospatial properties of scenes depicted in images by testing their ability to process virtual objects mentioned in prompts.", "motivation": "To explore how well VLMs comprehend the visuospatial properties of scenes in images.", "method": "Using descriptions of virtual objects paired with images, evaluating state-of-the-art VLMs' reasoning about spatial relations between real and virtual objects.", "result": "The results show that current VLMs have inadequate ability to process virtual objects.", "conclusion": "The study suggests that VLMs need improvement in comprehending visuospatial properties of scenes involving virtual objects."}}
{"id": "2505.10147", "pdf": "https://arxiv.org/pdf/2505.10147", "abs": "https://arxiv.org/abs/2505.10147", "authors": ["Yash", "Nikhil Karamchandani", "Avishek Ghosh"], "title": "Near Optimal Best Arm Identification for Clustered Bandits", "categories": ["cs.LG", "cs.MA"], "comment": "To be published in ICML 2025", "summary": "This work investigates the problem of best arm identification for multi-agent\nmulti-armed bandits. We consider $N$ agents grouped into $M$ clusters, where\neach cluster solves a stochastic bandit problem. The mapping between agents and\nbandits is a priori unknown. Each bandit is associated with $K$ arms, and the\ngoal is to identify the best arm for each agent under a $\\delta$-probably\ncorrect ($\\delta$-PC) framework, while minimizing sample complexity and\ncommunication overhead.\n  We propose two novel algorithms: Clustering then Best Arm Identification\n(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a\ntwo-phase approach that first clusters agents based on the bandit problems they\nare learning, followed by identifying the best arm for each cluster. BAI-Cl\nreverses the sequence by identifying the best arms first and then clustering\nagents accordingly. Both algorithms leverage the successive elimination\nframework to ensure computational efficiency and high accuracy.\n  We establish $\\delta$-PC guarantees for both methods, derive bounds on their\nsample complexity, and provide a lower bound for this problem class. Moreover,\nwhen $M$ is small (a constant), we show that the sample complexity of a variant\nof BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic\nand real-world datasets (MovieLens, Yelp) demonstrate the superior performance\nof the proposed algorithms in terms of sample and communication efficiency,\nparticularly in settings where $M \\ll N$.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222", "abs": "https://arxiv.org/abs/2505.10222", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.", "AI": {"tldr": "ComplexFormer introduces a novel Complex Multi-Head Attention (CMHA) that allows each attention head to independently model semantic and positional information in the complex plane. It also includes per-head Euler transformation and adaptive differential rotation mechanisms to enhance flexibility and performance.", "motivation": "The motivation is to address the limitations of existing transformer models in effectively integrating positional information while maintaining the flexibility of multi-head attention.", "method": "ComplexFormer uses Complex Multi-Head Attention (CMHA) which models semantic and positional differences within the complex plane. It applies per-head Euler transformation and adaptive differential rotation mechanisms.", "result": "Experiments on various tasks showed that ComplexFormer outperforms strong baselines like RoPE-Transformers in terms of performance, generation perplexity, and long-context coherence.", "conclusion": "ComplexFormer provides a more expressive and adaptable attention mechanism, demonstrating strong parameter efficiency and superior performance across multiple tasks."}}
{"id": "2505.10473", "pdf": "https://arxiv.org/pdf/2505.10473", "abs": "https://arxiv.org/abs/2505.10473", "authors": ["Fengdi Zhang", "Hongkun Cao", "Ruqi Huang"], "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.", "AI": {"tldr": "ControlGS is a 3D Gaussian splatting optimization method that provides semantically meaningful and cross-scene consistent quantity-quality control while maintaining strong performance.", "motivation": "Existing 3D Gaussian splatting methods lack the ability for users to intuitively adjust the trade-off between Gaussian quantity and rendering quality to suit practical needs.", "method": "ControlGS achieves semantically meaningful and cross-scene consistent quantity-quality control through a single training run using a user-specified hyperparameter.", "result": "ControlGS outperforms baselines by achieving higher rendering quality with fewer Gaussians and supports a broad adjustment range with stepless control over the trade-off.", "conclusion": "ControlGS provides a solution for adjusting the trade-off between Gaussian quantity and rendering quality in 3D Gaussian splatting."}}
{"id": "2505.10167", "pdf": "https://arxiv.org/pdf/2505.10167", "abs": "https://arxiv.org/abs/2505.10167", "authors": ["Saikat Barua", "Mostafizur Rahman", "Shehenaz Khaled", "Md Jafor Sadek", "Rafiul Islam", "Shahnewaz Siddique"], "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "16 pages, 6 figures, 7 equations", "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.", "AI": {"tldr": "This paper introduces QuXAI, a framework based on Q-MEDLEY, for explaining feature importance in hybrid quantum-classical machine learning (HQML) models with quantized feature encoding and classical learning. It shows that Q-MEDLEY effectively explains influential aspects and noise in HQML models and performs well in classical validation.", "motivation": "To address the lack of robust global and local explainability approaches for HQML models employing quantized feature encoding followed by classical learning.", "method": "Creating HQML models with quantum feature maps, using Q-MEDLEY which combines feature-based inferences while preserving the quantum transformation stage and visualizing attributions.", "result": "Q-MEDLEY successfully identifies influential classical aspects and noise in HQML models and performs competitively against established XAI techniques in classical validation settings.", "conclusion": "QuXAI improves the interpretability and reliability of HQML models, fostering greater confidence and safe/responsible use of quantum-enhanced AI technology."}}
{"id": "2505.10481", "pdf": "https://arxiv.org/pdf/2505.10481", "abs": "https://arxiv.org/abs/2505.10481", "authors": ["Ilya Ovodov", "Petr Surovtsev", "Karina Kvanchiani", "Alexander Kapitanov", "Alexander Nagaev"], "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.", "AI": {"tldr": "This study introduces Logos, a new Russian Sign Language dataset, which improves cross-language ISLR model training and annotation policies. It shows that pre-training on Logos can serve as a universal encoder for other language SLR tasks, and explicitly annotating visually similar signs enhances model quality.", "motivation": "The limited amount of data for individual sign languages and ambiguity in dataset labeling pose challenges for ISLR model training and annotation policies.", "method": "Introduce Logos, a novel Russian Sign Language dataset, and explore cross-language transfer learning approaches with multiple classification heads.", "result": "A model pre-trained on Logos can be used as a universal encoder for other language SLR tasks, and explicitly labeling visually similar signs improves trained model quality. The study outperforms current state-of-the-art results for the WLASL dataset and gets competitive results for the AUTSL dataset with a single stream model processing solely RGB video.", "conclusion": "The Logos dataset is the most extensive ISLR dataset by the number of signers and one of the largest available datasets while being the largest RSL dataset in size and vocabulary."}}
{"id": "2505.10172", "pdf": "https://arxiv.org/pdf/2505.10172", "abs": "https://arxiv.org/abs/2505.10172", "authors": ["Zeyan Li", "Libing Chen", "Yin Tang"], "title": "Does Scaling Law Apply in Time Series Forecasting?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Rapid expansion of model size has emerged as a key challenge in time series\nforecasting. From early Transformer with tens of megabytes to recent\narchitectures like TimesNet with thousands of megabytes, performance gains have\noften come at the cost of exponentially increasing parameter counts. But is\nthis scaling truly necessary? To question the applicability of the scaling law\nin time series forecasting, we propose Alinear, an ultra-lightweight\nforecasting model that achieves competitive performance using only k-level\nparameters. We introduce a horizon-aware adaptive decomposition mechanism that\ndynamically rebalances component emphasis across different forecast lengths,\nalongside a progressive frequency attenuation strategy that achieves stable\nprediction in various forecasting horizons without incurring the computational\noverhead of attention mechanisms. Extensive experiments on seven benchmark\ndatasets demonstrate that Alinear consistently outperforms large-scale models\nwhile using less than 1% of their parameters, maintaining strong accuracy\nacross both short and ultra-long forecasting horizons. Moreover, to more fairly\nevaluate model efficiency, we propose a new parameter-aware evaluation metric\nthat highlights the superiority of ALinear under constrained model budgets. Our\nanalysis reveals that the relative importance of trend and seasonal components\nvaries depending on data characteristics rather than following a fixed pattern,\nvalidating the necessity of our adaptive design. This work challenges the\nprevailing belief that larger models are inherently better and suggests a\nparadigm shift toward more efficient time series modeling.", "AI": {"tldr": "\u63d0\u51faAlinear\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u53c2\u6570\u91cf\u6781\u5c0f\uff0c\u6311\u6218\u4e86\u5927\u6a21\u578b\u66f4\u4f18\u7684\u4f20\u7edf\u89c2\u5ff5\u3002", "motivation": "\u8d28\u7591\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u4e2d\u5927\u6a21\u578b\u53c2\u6570\u91cf\u5448\u6307\u6570\u589e\u957f\u7684\u5fc5\u8981\u6027\uff0c\u63a2\u7d22\u8f7b\u91cf\u5316\u6a21\u578b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5f15\u5165\u9002\u5e94\u6027\u5206\u89e3\u673a\u5236\u548c\u6e10\u8fdb\u9891\u7387\u8870\u51cf\u7b56\u7565\uff0c\u4e0d\u4f9d\u8d56\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7a33\u5b9a\u9884\u6d4b\u3002", "result": "Alinear\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u4e0d\u5230\u51761%\u7684\u53c2\u6570\u91cf\u3002", "conclusion": "\u6b64\u7814\u7a76\u63ed\u793a\u4e86\u8d8b\u52bf\u548c\u5b63\u8282\u6210\u5206\u7684\u91cd\u8981\u6027\u968f\u6570\u636e\u7279\u5f81\u53d8\u5316\u800c\u975e\u56fa\u5b9a\u6a21\u5f0f\u7684\u4e8b\u5b9e\uff0c\u5efa\u8bae\u5411\u66f4\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2505.10483", "pdf": "https://arxiv.org/pdf/2505.10483", "abs": "https://arxiv.org/abs/2505.10483", "authors": ["Yi Li", "Haonan Wang", "Qixiang Zhang", "Boyu Xiao", "Chenchang Hu", "Hualiang Wang", "Xiaomeng Li"], "title": "UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric", "summary": "The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.", "AI": {"tldr": "Introduce UniEval, the first unified evaluation framework for multimodal models without extra models, images, or annotations.", "motivation": "Lack of unified evaluation framework for multimodal models", "method": "Develop UniEval framework including UniBench and UniScore metric", "result": "UniBench more challenging than existing benchmarks, UniScore aligns closely with human evaluations", "conclusion": "UniEval provides simplified and unified evaluation process for multimodal models"}}
{"id": "2505.10192", "pdf": "https://arxiv.org/pdf/2505.10192", "abs": "https://arxiv.org/abs/2505.10192", "authors": ["Prashant P. Shinde", "Priyadarshini P. Pai", "Shashishekar P. Adiga", "K. Subramanya Mayya", "Yongbeom Seo", "Myungsoo Hwang", "Heeyoung Go", "Changmin Park"], "title": "Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data", "categories": ["cs.LG"], "comment": null, "summary": "In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models.", "AI": {"tldr": "This study generates synthetic SEM images with known defect distributions to train and test object detection models for EUV patterning defect detection, finding YOLOv8 to be the most effective at detecting smaller defects.", "motivation": "To address the lack of defect-annotated quality data with good representation of smaller defects in EUV pattering, which hinders the deployment of deep learning-based defect detection models in semiconductor fabrication lines.", "method": "Artificially generating SEM images with known defect distributions and autonomously annotating them, then applying state-of-the-art object detection models to evaluate their performance.", "result": "YOLOv8 achieved the highest mean average precision of 96%, surpassing EfficientNet (83%) and SSD (77%). It also demonstrated reliable detection of smaller defects and showed promising results in real SEM data testing.", "conclusion": "The YOLOv8 model shows the best performance in detecting smaller defects using both synthetic and real SEM data."}}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465", "abs": "https://arxiv.org/abs/2505.10465", "authors": ["Yizhou liu", "Ziming Liu", "Jeff Gore"], "title": "Superposition Yields Robust Neural Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8868\u793a\u91cd\u53e0\u5bf9\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8868\u793a\u91cd\u53e0\u662fLLMs\u4e2d\u89c2\u5bdf\u5230\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u91cd\u8981\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u8d77\u6e90\uff0c\u5373\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u635f\u5931\u4f1a\u6309\u7167\u5e42\u5f8b\u51cf\u5c11\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\u6765\u7814\u7a76\u6a21\u578b\u5927\u5c0f\u4e0e\u635f\u5931\u4e4b\u95f4\u7684\u7f29\u653e\u5173\u7cfb\uff0c\u5e76\u5206\u6790\u4e86\u56db\u79cd\u5f00\u6e90LLMs\u3002", "result": "\u5728\u5f31\u53e0\u52a0\u60c5\u51b5\u4e0b\uff0c\u5f53\u53ea\u6709\u6700\u9891\u7e41\u51fa\u73b0\u7684\u7279\u5f81\u88ab\u8868\u793a\u4e14\u6ca1\u6709\u5e72\u6270\u65f6\uff0c\u635f\u5931\u968f\u6a21\u578b\u5927\u5c0f\u7684\u7f29\u653e\u53d6\u51b3\u4e8e\u6f5c\u5728\u7279\u5f81\u9891\u7387\uff1b\u5982\u679c\u7279\u5f81\u9891\u7387\u9075\u5faa\u5e42\u5f8b\uff0c\u5219\u635f\u5931\u4e5f\u4f1a\u5982\u6b64\u3002\u800c\u5728\u5f3a\u53e0\u52a0\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u7279\u5f81\u90fd\u88ab\u8868\u793a\u4f46\u76f8\u4e92\u91cd\u53e0\uff0c\u635f\u5931\u5219\u4e0e\u6a21\u578b\u7ef4\u5ea6\u6210\u53cd\u6bd4\u3002", "conclusion": "\u8868\u793a\u91cd\u53e0\u662f\u89c2\u5bdf\u5230\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u91cd\u8981\u673a\u5236\u3002\u8fd9\u4e9b\u89c1\u89e3\u6709\u671b\u6fc0\u53d1\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u91cf\u548c\u53c2\u6570\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10496", "pdf": "https://arxiv.org/pdf/2505.10496", "abs": "https://arxiv.org/abs/2505.10496", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "AI": {"tldr": "A comprehensive evaluation framework named CheXGenBench is introduced to assess synthetic chest radiograph generation across different models. It evaluates fidelity, privacy risks, and clinical utility using standardized metrics. The framework reveals inefficiencies in current evaluation methods and provides a new standard for the medical AI community.", "motivation": "To overcome methodological inconsistencies and provide a unified evaluation protocol for assessing synthetic chest radiograph generation.", "method": "Introduces CheXGenBench with over 20 quantitative metrics evaluating generation quality, privacy risks, and clinical applicability across 11 leading text-to-image architectures.", "result": "Identifies inefficiencies in existing evaluation protocols, particularly in assessing generative fidelity. Establishes a standardized benchmark for the medical AI community and releases a high-quality synthetic dataset, SynthCheX-75K.", "conclusion": "CheXGenBench sets a new standard for evaluating synthetic chest radiograph generation, enabling more objective and reproducible comparisons."}}
{"id": "2505.10198", "pdf": "https://arxiv.org/pdf/2505.10198", "abs": "https://arxiv.org/abs/2505.10198", "authors": ["Mariano Ferrero", "Jos\u00e9 Omar Chelotti", "Luciano Sebasti\u00e1n Martinez-Rau", "Leandro Vignolo", "Mart\u00edn Pires", "Julio Ricardo Galli", "Leonardo Luis Giovanini", "Hugo Leonardo Rufiner"], "title": "A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals", "categories": ["cs.LG"], "comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence", "summary": "Monitoring feeding behaviour is a relevant task for efficient herd management\nand the effective use of available resources in grazing cattle. The ability to\nautomatically recognise animals' feeding activities through the identification\nof specific jaw movements allows for the improvement of diet formulation, as\nwell as early detection of metabolic problems and symptoms of animal\ndiscomfort, among other benefits. The use of sensors to obtain signals for such\nmonitoring has become popular in the last two decades. The most frequently\nemployed sensors include accelerometers, microphones, and cameras, each with\nits own set of advantages and drawbacks. An unexplored aspect is the\nsimultaneous use of multiple sensors with the aim of combining signals in order\nto enhance the precision of the estimations. In this direction, this work\nintroduces a deep neural network based on the fusion of acoustic and inertial\nsignals, composed of convolutional, recurrent, and dense layers. The main\nadvantage of this model is the combination of signals through the automatic\nextraction of features independently from each of them. The model has emerged\nfrom an exploration and comparison of different neural network architectures\nproposed in this work, which carry out information fusion at different levels.\nFeature-level fusion has outperformed data and decision-level fusion by at\nleast a 0.14 based on the F1-score metric. Moreover, a comparison with\nstate-of-the-art machine learning methods is presented, including traditional\nand deep learning approaches. The proposed model yielded an F1-score value of\n0.802, representing a 14% increase compared to previous methods. Finally,\nresults from an ablation study and post-training quantization evaluation are\nalso reported.", "AI": {"tldr": "This work introduces a deep neural network that combines acoustic and inertial signals to monitor feeding behavior in grazing cattle more accurately.", "motivation": "Efficient herd management and resource utilization in grazing cattle require monitoring feeding behavior, which can also help detect metabolic problems and animal discomfort.", "method": "A deep neural network based on the fusion of acoustic and inertial signals, including convolutional, recurrent, and dense layers, was developed to automatically extract features from each signal.", "result": "The model achieved an F1-score value of 0.802, surpassing state-of-the-art machine learning methods by 14%. Feature-level fusion proved superior to data and decision-level fusion by at least 0.14 based on the F1-score metric.", "conclusion": "This study demonstrates the effectiveness of using multiple sensor signals and deep neural networks for improving the accuracy of feeding behavior recognition in cattle."}}
{"id": "2505.10475", "pdf": "https://arxiv.org/pdf/2505.10475", "abs": "https://arxiv.org/abs/2505.10475", "authors": ["Mouxiang Chen", "Binyuan Hui", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Jianling Sun", "Junyang Lin", "Zhongxin Liu"], "title": "Parallel Scaling Law for Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.", "AI": {"tldr": "We introduce a new method called Parallel Scaling (ParScale) that increases model's parallel computation during both training and inference time. This method is more inference-efficient than parameter scaling or inference-time scaling.", "motivation": "To find a more efficient way to scale language models.", "method": "Applying diverse and learnable transformations to the input, executing forward passes of the model in parallel, and dynamically aggregating the outputs.", "result": "A model with P parallel streams is similar to scaling the parameters by O(logP) while showing superior inference efficiency. ParScale can use up to 22x less memory increase and 6x less latency increase compared to parameter scaling that achieves the same performance improvement.", "conclusion": "The new scaling law potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning."}}
{"id": "2505.10497", "pdf": "https://arxiv.org/pdf/2505.10497", "abs": "https://arxiv.org/abs/2505.10497", "authors": ["Iurii Medvedev", "Nuno Goncalves"], "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.", "AI": {"tldr": "A novel dual-branch classification strategy is proposed to enhance deep network robustness against face morphing attacks.", "motivation": "Modern face recognition systems need to be robust against presentation attacks like face morphing.", "method": "Introduce a dual-branch classification strategy that modifies the labeling ambiguity of face morphs.", "result": "The method improves the model's ability to distinguish morph images from genuine ones and shows effectiveness on public benchmarks.", "conclusion": "The approach is universally applicable and can be integrated into existing face recognition training pipelines."}}
{"id": "2505.10213", "pdf": "https://arxiv.org/pdf/2505.10213", "abs": "https://arxiv.org/abs/2505.10213", "authors": ["Mohammadmahdi Ghasemloo", "Alireza Moradi"], "title": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u57df\u77e5\u8bc6\u8f6c\u79fb\u6846\u67b6\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6ca1\u6709\u8f85\u52a9\u4fe1\u606f\u7684\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e4b\u5916\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u9700\u8981\u5efa\u7acb\u6700\u4f73\u5b9e\u8df5\u6765\u5229\u7528\u5176\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5730\u5411\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ce8\u5165\u7ed3\u6784\u5316\u65f6\u95f4\u4fe1\u606f\u7684\u65b0\u6846\u67b6\u4ee5\u63d0\u9ad8\u5176\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u6ca1\u6709\u8f85\u52a9\u4fe1\u606f\u7684\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u77e5\u8bc6\u8f6c\u79fb\u7b56\u7565\u6709\u6f5c\u529b\u5f25\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7279\u5b9a\u9886\u57df\u7684\u9884\u6d4b\u4efb\u52a1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.10495", "pdf": "https://arxiv.org/pdf/2505.10495", "abs": "https://arxiv.org/abs/2505.10495", "authors": ["Vibha Belavadi", "Tushar Vatsa", "Dewang Sultania", "Suhas Suresha", "Ishita Verma", "Cheng Chen", "Tracy Holloway King", "Michael Friedrich"], "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing", "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.", "AI": {"tldr": "This paper introduces a new router-based architecture to generate high-quality synthetic data for fine-tuning large language models in function calling tasks without real user interaction data.", "motivation": "The lack of real-world task-specific data and privacy concerns make it difficult to train models directly on real user interaction data.", "method": "A router-based architecture that uses domain resources, text-to-text and vision-to-text language models to generate synthetic training data.", "result": "Evaluation on real user queries shows improved function classification accuracy and API parameter selection.", "conclusion": "The proposed method sets new benchmarks for function calling tasks by outperforming traditional approaches."}}
{"id": "2505.10533", "pdf": "https://arxiv.org/pdf/2505.10533", "abs": "https://arxiv.org/abs/2505.10533", "authors": ["Aaryan Sharma", "Shivansh Gupta", "Samar Agarwal", "Vishak Prasad C.", "Ganesh Ramakrishnan"], "title": "Enhancing Multi-Image Question Answering via Submodular Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.", "AI": {"tldr": "This paper proposes an enhancement to the retriever framework in the MIRAGE model by using submodular subset selection techniques to pre-select semantically relevant images, improving retrieval performance in Multiple Image Question Answering tasks.", "motivation": "To address the challenges faced by large multimodal models when dealing with multiple images, especially in terms of scalability and retrieval performance.", "method": "Enhancing the retriever framework in MIRAGE by incorporating query-aware submodular functions like GraphCut to pre-select relevant images before the main retrieval component.", "result": "The proposed method demonstrates improved effectiveness in the submodular-retriever pipeline, particularly beneficial in scenarios with large haystack sizes.", "conclusion": "The use of anchor-based queries and data augmentation enhances the retrieval performance in multiple image question answering tasks."}}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526", "abs": "https://arxiv.org/abs/2505.10526", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.", "AI": {"tldr": "Introduce MASSV, a two-phase approach that adapts small language models into multimodal drafters for speculative decoding in vision-language models, improving accepted length and inference speed.", "motivation": "Speculative decoding accelerates language model inference but faces challenges when applied to vision-language models due to small drafters lacking architectural components for processing visual inputs and mismatched token predictions with target models.", "method": "MASSV connects the vision encoder of the target VLM to the draft model via a trainable projector and uses self-distilled visual instruction tuning to align token predictions.", "result": "Experiments show MASSV increases accepted length by up to 30% and achieves end-to-end inference speedups of up to 1.46x on visually-grounded tasks.", "conclusion": "MASSV offers a scalable and architecture-compatible method to accelerate current and future vision-language models."}}
{"id": "2505.10541", "pdf": "https://arxiv.org/pdf/2505.10541", "abs": "https://arxiv.org/abs/2505.10541", "authors": ["Pengfei Wang", "Guohai Xu", "Weinong Wang", "Junjie Yang", "Jie Lou", "Yunhua Xue"], "title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.", "AI": {"tldr": "This paper introduces a novel approach to assess the visual comprehension of Multimodal Large Language Models by defining implicit visual misunderstanding and proposing a new metric called attention accuracy.", "motivation": "Existing benchmarks for MLLMs mainly focus on answer correctness but ignore whether the models truly understand visual inputs.", "method": "Decoupling visual and textual modalities in the causal attention module and analyzing the attention distribution across network layers.", "result": "The authors reveal that attention increasingly focuses on the correct image and propose a scale-agnostic metric called attention accuracy to better evaluate visual understanding.", "conclusion": "The proposed method provides a more reliable assessment of visual understanding in MLLMs and demonstrates its effectiveness across different granularities."}}
{"id": "2505.10259", "pdf": "https://arxiv.org/pdf/2505.10259", "abs": "https://arxiv.org/abs/2505.10259", "authors": ["Xiangwen Zhuge", "Xu Shen", "Zeyu Wang", "Fan Dang", "Xuan Ding", "Danyang Li", "Yahui Han", "Tianxiang Hao", "Zheng Yang"], "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices", "categories": ["cs.LG"], "comment": null, "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .", "AI": {"tldr": "SpecOffload is a high-throughput inference engine that improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x by embedding speculative decoding into offloading.", "motivation": "Existing systems offload model weights to CPU memory due to limited GPU memory, which incurs substantial I/O overhead and leads to underutilized GPU cores and low impact on performance from GPU memory.", "method": "SpecOffload embeds speculative decoding into offloading and unlocks latent GPU resources for storing and executing a draft model used for speculative decoding.", "result": "SpecOffload improves GPU core utilization by 4.49x and boosts inference throughput by 2.54x compared to the best baseline.", "conclusion": "SpecOffload is an efficient inference engine that significantly improves GPU core utilization and inference throughput for LLM inference on resource-constrained devices."}}
{"id": "2505.10551", "pdf": "https://arxiv.org/pdf/2505.10551", "abs": "https://arxiv.org/abs/2505.10551", "authors": ["Yiwen Liu", "Jessica Bader", "Jae Myung Kim"], "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025", "summary": "With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.", "AI": {"tldr": "This paper investigates the necessity of enforcing feasibility when generating synthetic training data for CLIP-based classifiers and finds that it has minimal effect on performance.", "motivation": "Investigating whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture.", "method": "Introducing VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model.", "result": "Feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. The attribute matters on whether the feasible/infeasible images adversarially influence the classification performance. Mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets.", "conclusion": "Enforcing feasibility is not necessary when generating synthetic training data for CLIP-based classifiers."}}
{"id": "2505.10262", "pdf": "https://arxiv.org/pdf/2505.10262", "abs": "https://arxiv.org/abs/2505.10262", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Lajos Hanzo"], "title": "Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The charging scheduling problem of Electric Buses (EBs) is investigated based\non Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is\nconceived, where the time horizon includes multiple charging and operating\nperiods in a day, while each period is further divided into multiple time\nsteps. To overcome the challenge of long-range multi-phase planning with sparse\nreward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP\ninto a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical\nDouble Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is\nproposed for simultaneously solving the decision problems arising at different\ntemporal resolutions. As a result, the high-level agent learns an effective\npolicy for prescribing the charging targets for every charging period, while\nthe low-level agent learns an optimal policy for setting the charging power of\nevery time step within a single charging period, with the aim of minimizing the\ncharging costs while meeting the charging target. It is proved that the flat\npolicy constructed by superimposing the optimal high-level policy and the\noptimal low-level policy performs as well as the optimal policy of the original\nMDP. Since jointly learning both levels of policies is challenging due to the\nnon-stationarity of the high-level agent and the sampling inefficiency of the\nlow-level agent, we divide the joint learning process into two phases and\nexploit our new HER algorithm to manipulate the experience replay buffers for\nboth levels of agents. Numerical experiments are performed with the aid of\nreal-world data to evaluate the performance of the proposed algorithm.", "AI": {"tldr": "This paper investigates the charging scheduling problem of Electric Buses using Deep Reinforcement Learning. It proposes a Hierarchical DRL approach to decouple the original MDP into a high-level SMDP and multiple low-level MDPs. An HDDQN-HER algorithm is proposed to solve decision problems at different temporal resolutions. Numerical experiments using real-world data evaluate the algorithm's performance.", "motivation": "To address the charging scheduling problem of Electric Buses using Deep Reinforcement Learning with the aim of minimizing charging costs.", "method": "Proposes a Hierarchical DRL approach including a high-level SMDP and multiple low-level MDPs. Uses the HDDQN-HER algorithm to handle experience replay buffers for both levels of agents.", "result": "Proves that the flat policy constructed by combining high-level and low-level policies performs as well as the optimal policy of the original MDP. Numerical experiments are conducted with real-world data.", "conclusion": "The proposed Hierarchical DRL approach effectively solves the charging scheduling problem of Electric Buses with minimized charging costs."}}
{"id": "2505.10557", "pdf": "https://arxiv.org/pdf/2505.10557", "abs": "https://arxiv.org/abs/2505.10557", "authors": ["Ke Wang", "Junting Pan", "Linda Wei", "Aojun Zhou", "Weikang Shi", "Zimu Lu", "Han Xiao", "Yunqiao Yang", "Houxing Ren", "Mingjie Zhan", "Hongsheng Li"], "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4ee3\u7801\u4f5c\u4e3a\u76d1\u7763\u6765\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u56fe\u50cf\u5230\u4ee3\u7801\u6a21\u578b\uff08FigCodifier\uff09\u548c\u4e00\u4e2a\u540d\u4e3aImgCode-8.6M\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u6b64\u6570\u636e\u96c6\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u5b66\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff08MM-MathInstruct-3M\uff09\u3002\u6700\u7ec8\u63d0\u51fa\u7684MathCoder-VL\u6a21\u578b\u5728\u6240\u6709\u516d\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u5f00\u6e90SOTA\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u6570\u5b66\u56fe\u89e3\u4e2d\u7684\u590d\u6742\u7ec6\u8282\uff0c\u8fd9\u4e9b\u7ec6\u8282\u5bf9\u4e8e\u89e3\u51b3\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u963b\u788d\u4e86\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u5229\u7528\u4ee3\u7801\u4f5c\u4e3a\u76d1\u7763\u6765\u5efa\u7acb\u56fe\u50cf\u548c\u4ee3\u7801\u4e4b\u95f4\u7684\u7cbe\u786e\u8fde\u63a5\uff0c\u91c7\u7528\u6a21\u578b\u5faa\u73af\u7684\u65b9\u6cd5\u5171\u540c\u5f00\u53d1\u56fe\u50cf\u5230\u4ee3\u7801\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u7684MathCoder-VL\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u5f00\u6e90SOTA\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u8d85\u8fc7\u4e86GPT-4o\u548cClaude 3.5 Sonnet\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5f00\u653e\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u4f9b\u516c\u4f17\u4f7f\u7528\u3002"}}
{"id": "2505.10264", "pdf": "https://arxiv.org/pdf/2505.10264", "abs": "https://arxiv.org/abs/2505.10264", "authors": ["Francesco Diana", "Andr\u00e9 Nusser", "Chuan Xu", "Giovanni Neglia"], "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u6062\u590d\u5927\u89c4\u6a21\u6570\u636e\u6279\u6b21\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u91cd\u5efa\u653b\u51fb\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4f9d\u8d56\u4e8e\u5bf9\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u7684\u5047\u8bbe\u6216\u5f53\u6279\u91cf\u5927\u5c0f\u8d85\u8fc7\u51e0\u5341\u4e2a\u6837\u672c\u65f6\u6548\u7387\u663e\u8457\u4e0b\u964d\u3002", "method": "\u5229\u7528\u5168\u8fde\u63a5\u5c42\u7684\u65b0\u51e0\u4f55\u89c6\u89d2\u6765\u5236\u4f5c\u6076\u610f\u6a21\u578b\u53c2\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u5ba2\u6237\u7aef\u8bad\u7ec3\u6570\u636e\u7684\u91cd\u5efa\u3002", "result": "\u5728\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u6210\u529f\u5730\u91cd\u5efa\u6bd4\u4ee5\u524d\u6280\u672f\u5927\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u6570\u636e\u6279\u6b21\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b8c\u7f8e\u6062\u590d\u5206\u7c7b\u4efb\u52a1\u4e2d\u4efb\u610f\u5927\u5c0f\u7684\u6570\u636e\u6279\u6b21\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u5904\u7406\u6bd4\u4e4b\u524d\u6280\u672f\u5927\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u6570\u636e\u6279\u6b21\u3002"}}
{"id": "2505.10562", "pdf": "https://arxiv.org/pdf/2505.10562", "abs": "https://arxiv.org/abs/2505.10562", "authors": ["Wenxuan Wang", "Fan Zhang", "Yufeng Cui", "Haiwen Diao", "Zhuoyan Luo", "Huchuan Lu", "Jing Liu", "Xinlong Wang"], "title": "End-to-End Vision Tokenizer Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.", "AI": {"tldr": "Proposes ETT, an end-to-end vision tokenizer tuning approach that jointly optimizes vision tokenization and target autoregressive tasks, achieving 2-6% performance gains in multimodal understanding and visual generation tasks compared to frozen tokenizer baselines.", "motivation": "Existing vision tokenization assumes visual tokens can generalize well across various tasks, but this leads to a misalignment where the loss of vision tokenization becomes a bottleneck for target tasks.", "method": "ETT leverages visual embeddings of the tokenizer codebook and optimizes vision tokenizers end-to-end with both reconstruction and caption objectives, integrating seamlessly into existing training pipelines with minimal architecture modifications.", "result": "ETT achieves significant performance gains in multimodal understanding and visual generation tasks without sacrificing the original reconstruction capability.", "conclusion": "ETT is a simple yet effective method that can empower multimodal foundation models beyond image generation and understanding."}}
{"id": "2505.10271", "pdf": "https://arxiv.org/pdf/2505.10271", "abs": "https://arxiv.org/abs/2505.10271", "authors": ["Rafael Pablos Sarabia", "Joachim Nyborg", "Morten Birk", "Jeppe Liborius Sj\u00f8rup", "Anders Lillevang Vesterholt", "Ira Assent"], "title": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.", "AI": {"tldr": "A deep learning model integrating multiple data sources outperforms existing systems in European high-resolution precipitation forecasting.", "motivation": "Overcoming the limitations of radar-only deep learning models with short forecast lead times.", "method": "The model efficiently integrates multiple data sources including radar, satellite, and physics-based numerical weather prediction (NWP), featuring a compact architecture.", "result": "Accurate forecasts with robust uncertainty quantification through consistent probabilistic maps.", "conclusion": "The presented deep learning model surpasses current operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models, setting a new standard for high-resolution precipitation forecasting in Europe."}}
{"id": "2505.10565", "pdf": "https://arxiv.org/pdf/2505.10565", "abs": "https://arxiv.org/abs/2505.10565", "authors": ["Zehan Wang", "Siyu Chen", "Lihe Yang", "Jialei Wang", "Ziang Zhang", "Hengshuang Zhao", "Zhou Zhao"], "title": "Depth Anything with Any Prior", "categories": ["cs.CV"], "comment": "Home page: https://prior-depth-anything.github.io/", "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdPrior Depth Anything\u6846\u67b6\uff0c\u7ed3\u5408\u5ea6\u91cf\u4fe1\u606f\u548c\u51e0\u4f55\u7ed3\u6784\u751f\u6210\u51c6\u786e\u7684\u6df1\u5ea6\u56fe\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u7cbe\u786e\u7684\u5ea6\u91cf\u4fe1\u606f\uff0c\u8981\u4e48\u7f3a\u4e4f\u5bf9\u4efb\u610f\u573a\u666f\u7684\u901a\u7528\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u7cbe\u786e\u4f46\u4e0d\u5b8c\u6574\u7684\u5ea6\u91cf\u4fe1\u606f\u4e0e\u5b8c\u6574\u4f46\u76f8\u5bf9\u7684\u51e0\u4f55\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ece\u7c97\u5230\u7ec6\u7684\u7ba1\u9053\uff0c\u9010\u6b65\u6574\u5408\u4e24\u79cd\u4e92\u8865\u7684\u6df1\u5ea6\u6e90\u3002\u9996\u5148\u5f15\u5165\u4e86\u50cf\u7d20\u7ea7\u5ea6\u91cf\u5bf9\u9f50\u548c\u8ddd\u79bb\u611f\u77e5\u52a0\u6743\u6765\u9884\u586b\u5145\u591a\u6837\u7684\u5ea6\u91cf\u5148\u9a8c\u3002\u5176\u6b21\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u6a21\u578b\u6765\u7ec6\u5316\u6df1\u5ea6\u5148\u9a8c\u7684\u56fa\u6709\u566a\u58f0\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u4e86\u4ee5\u524d\u7684\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrior Depth Anything\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u51c6\u786e\u3001\u5bc6\u96c6\u4e14\u8be6\u7ec6\u7684\u5ea6\u91cf\u6df1\u5ea6\u56fe\u3002\u6a21\u578b\u5c55\u793a\u4e86\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5353\u8d8a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5207\u6362\u9884\u6d4b\u6a21\u578b\u6765\u6539\u8fdb\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002"}}
{"id": "2505.10272", "pdf": "https://arxiv.org/pdf/2505.10272", "abs": "https://arxiv.org/abs/2505.10272", "authors": ["Niklas Dexheimer", "Sascha Gaudlitz", "Johannes Schmidt-Hieber"], "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Hebbian learning is a key principle underlying learning in biological neural\nnetworks. It postulates that synaptic changes occur locally, depending on the\nactivities of pre- and postsynaptic neurons. While Hebbian learning based on\nneuronal firing rates is well explored, much less is known about learning rules\nthat account for precise spike-timing. We relate a Hebbian\nspike-timing-dependent plasticity rule to noisy gradient descent with respect\nto a natural loss function on the probability simplex. This connection allows\nus to prove that the learning rule eventually identifies the presynaptic neuron\nwith the highest activity. We also discover an intrinsic connection to noisy\nmirror descent.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c16\u5cf0\u65f6\u95f4\u7684Hebbian\u5b66\u4e60\u89c4\u5219\uff0c\u63ed\u793a\u4e86\u5b83\u4e0e\u68af\u5ea6\u4e0b\u964d\u548c\u955c\u50cf\u4e0b\u964d\u7684\u5173\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6700\u7ec8\u80fd\u8bc6\u522b\u51fa\u7a81\u89e6\u524d\u795e\u7ecf\u5143\u4e2d\u6d3b\u52a8\u6700\u9ad8\u7684\u90a3\u4e2a\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u7cbe\u786e\u5c16\u5cf0\u65f6\u95f4\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u56e0\u4e3a\u5173\u4e8e\u8fd9\u65b9\u9762\u7684\u77e5\u8bc6\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "\u901a\u8fc7\u5c06Hebbian\u57fa\u4e8e\u5c16\u5cf0\u7684\u65f6\u95f4\u4f9d\u8d56\u53ef\u5851\u6027\u89c4\u5219\u4e0e\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u7684\u81ea\u7136\u635f\u5931\u51fd\u6570\u7684\u5608\u6742\u68af\u5ea6\u4e0b\u964d\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u53d1\u73b0\u4e86\u4e0e\u5608\u6742\u955c\u50cf\u4e0b\u964d\u7684\u5185\u5728\u8054\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u5b66\u4e60\u89c4\u5219\u7684\u884c\u4e3a\u3002", "conclusion": "\u8be5\u5b66\u4e60\u89c4\u5219\u6700\u7ec8\u8bc6\u522b\u51fa\u7a81\u89e6\u524d\u795e\u7ecf\u5143\u4e2d\u6d3b\u52a8\u6700\u9ad8\u7684\u4e00\u4e2a\u3002"}}
{"id": "2505.10566", "pdf": "https://arxiv.org/pdf/2505.10566", "abs": "https://arxiv.org/abs/2505.10566", "authors": ["Yen-Chi Cheng", "Krishna Kumar Singh", "Jae Shin Yoon", "Alex Schwing", "Liangyan Gui", "Matheus Gadelha", "Paul Guerrero", "Nanxuan Zhao"], "title": "3D-Fixup: Advancing Photo Editing with 3D Priors", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. Project page: https://3dfixup.github.io/", "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/", "AI": {"tldr": "We introduce 3D-Fixup, a new framework for 2D image editing guided by learned 3D priors, supporting complex edits like object translation and rotation.", "motivation": "The challenge of 3D-aware image editing due to objects being specified by a single image.", "method": "A training-based approach using diffusion models and 3D guidance from an Image-to-3D model, with a designed data generation pipeline for high-quality training.", "result": "Effective support for complex, identity-coherent 3D-aware edits with high-quality results.", "conclusion": "Integrating 3D priors advances diffusion models' application in realistic image manipulation."}}
{"id": "2505.10296", "pdf": "https://arxiv.org/pdf/2505.10296", "abs": "https://arxiv.org/abs/2505.10296", "authors": ["Jiaju Qi", "Lei Lei", "Thorsteinn Jonsson", "Dusit Niyato"], "title": "Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The growing adoption of Electric Buses (EBs) represents a significant step\ntoward sustainable development. By utilizing Internet of Things (IoT) systems,\ncharging stations can autonomously determine charging schedules based on\nreal-time data. However, optimizing EB charging schedules remains a critical\nchallenge due to uncertainties in travel time, energy consumption, and\nfluctuating electricity prices. Moreover, to address real-world complexities,\ncharging policies must make decisions efficiently across multiple time scales\nand remain scalable for large EB fleets. In this paper, we propose a\nHierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the\noriginal Markov Decision Process (MDP) into two augmented MDPs. To solve these\nMDPs and enable multi-timescale decision-making, we introduce a novel HDRL\nalgorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization\nEnhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic\n(DAC) algorithm for large-scale EB fleets are addressed through enhancements at\nboth decision levels. At the high level, we redesign the decentralized actor\nnetwork and integrate an attention mechanism to extract relevant global state\ninformation for each EB, decreasing the size of neural networks. At the low\nlevel, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is\nincorporated into the DAC framework, enabling decentralized and coordinated\ncharging power decisions, reducing computational complexity and enhancing\nconvergence speed. Extensive experiments with real-world data demonstrate the\nsuperior performance and scalability of DAC-MAPPO-E in optimizing EB fleet\ncharging schedules.", "AI": {"tldr": "This paper proposes a Hierarchical Deep Reinforcement Learning (HDRL) approach called Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E) to optimize electric bus charging schedules. The approach addresses uncertainties and real-world complexities, and experiments show its superior performance and scalability.", "motivation": "Optimizing electric bus charging schedules remains a critical challenge due to uncertainties in travel time, energy consumption, and fluctuating electricity prices, as well as the need for efficient multi-timescale decision-making and scalability for large fleets.", "method": "A Hierarchical Deep Reinforcement Learning approach is proposed, reformulating the original Markov Decision Process into two augmented MDPs. A novel HDRL algorithm, DAC-MAPPO-E, is introduced to solve these MDPs. Enhancements are made at both high and low decision levels to address scalability challenges.", "result": "Extensive experiments with real-world data demonstrate the superior performance and scalability of DAC-MAPPO-E in optimizing electric bus fleet charging schedules.", "conclusion": "The proposed HDRL approach effectively optimizes electric bus charging schedules under various uncertainties and real-world complexities, showing promising results in terms of performance and scalability."}}
{"id": "2505.10297", "pdf": "https://arxiv.org/pdf/2505.10297", "abs": "https://arxiv.org/abs/2505.10297", "authors": ["Chibueze Peace Obioma", "Youcheng Sun", "Mustafa A. Mustafa"], "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Submitted to ESORICS 2025", "summary": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.", "AI": {"tldr": "This paper presents FeRA, a novel defense mechanism for detecting backdoor attacks in federated learning systems with heterogeneous, non-IID data.", "motivation": "To address the challenge of detecting backdoor attacks in federated learning due to the diverse, non-IID data produced by resource-constrained edge devices.", "method": "FeRA uses cross-client attention over internal feature representations to compute an anomaly score based on representation reconstruction errors, distinguishing between benign and malicious clients.", "result": "FeRA shows robust performance across various FL scenarios, reducing backdoor attack success rates while maintaining high accuracy on the main task.", "conclusion": "FeRA is a model-agnostic, attack-agnostic defense mechanism that does not require labeled reference data, making it suitable for heterogeneous and resource-limited edge deployments."}}
{"id": "2505.10441", "pdf": "https://arxiv.org/pdf/2505.10441", "abs": "https://arxiv.org/abs/2505.10441", "authors": ["Filippo Leveni", "Luca Magri", "Giacomo Boracchi", "Cesare Alippi"], "title": "PIF: Anomaly detection via preference embedding", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Pattern Recognition (ICPR\n  2020)", "summary": "We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.", "AI": {"tldr": "A novel anomaly detection method named PIF is proposed, which combines adaptive isolation methods with preference embedding.", "motivation": "To detect anomalies with respect to structured patterns.", "method": "Embed data in a high dimensional space and use PI-Forest to compute an anomaly score.", "result": "PIF outperforms other state-of-the-art methods on both synthetic and real datasets.", "conclusion": "PI-Forest is superior in measuring arbitrary distances and isolating points in the preference space."}}
{"id": "2505.10307", "pdf": "https://arxiv.org/pdf/2505.10307", "abs": "https://arxiv.org/abs/2505.10307", "authors": ["Yiyang Zhao", "Chengpei Wu", "Lilin Zhang", "Ning Yang"], "title": "Negative Metric Learning for Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Graph contrastive learning (GCL) often suffers from false negatives, which\ndegrades the performance on downstream tasks. The existing methods addressing\nthe false negative issue usually rely on human prior knowledge, still leading\nGCL to suboptimal results. In this paper, we propose a novel Negative Metric\nLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative\nMetric Network (NMN) to build a negative metric space, in which false negatives\ncan be distinguished better from true negatives based on their distance to\nanchor node. To overcome the lack of explicit supervision signals for NML, we\npropose a joint training scheme with bi-level optimization objective, which\nimplicitly utilizes the self-supervision signals to iteratively optimize the\nencoder and the negative metric network. The solid theoretical analysis and the\nextensive experiments conducted on widely used benchmarks verify the\nsuperiority of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5NML-GCL\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8d1f\u5ea6\u91cf\u7f51\u7edc\u548c\u53cc\u5c42\u4f18\u5316\u76ee\u6807\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5047\u9634\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u56fe\u5bf9\u6bd4\u5b66\u4e60(GCL)\u7ecf\u5e38\u53d7\u5230\u5047\u9634\u6027\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u964d\u4f4e\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002\u73b0\u6709\u7684\u89e3\u51b3\u5047\u9634\u6027\u95ee\u9898\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4eba\u5de5\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ecd\u7136\u5bfc\u81f4GCL\u7684\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNML-GCL\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u53ef\u5b66\u4e60\u7684\u8d1f\u5ea6\u91cf\u7f51\u7edc(NMN)\u6784\u5efa\u8d1f\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53cc\u5c42\u4f18\u5316\u76ee\u6807\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u7684NML-GCL\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aNML-GCL\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u53ef\u5b66\u4e60\u7684\u8d1f\u5ea6\u91cf\u7f51\u7edc(NMN)\u6784\u5efa\u8d1f\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5728\u8fd9\u4e2a\u7a7a\u95f4\u4e2d\uff0c\u57fa\u4e8e\u951a\u8282\u70b9\u7684\u8ddd\u79bb\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5c06\u9519\u8bef\u5426\u5b9a\u4e0e\u771f\u5b9e\u5426\u5b9a\u533a\u5206\u5f00\u6765\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u514b\u670dNML\u7f3a\u4e4f\u663e\u5f0f\u76d1\u7763\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53cc\u5c42\u4f18\u5316\u76ee\u6807\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u9690\u5f0f\u5730\u5229\u7528\u81ea\u76d1\u7763\u4fe1\u53f7\u6765\u8fed\u4ee3\u4f18\u5316\u7f16\u7801\u5668\u548c\u8d1f\u5ea6\u91cf\u7f51\u7edc\u3002\u7406\u8bba\u5206\u6790\u548c\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.10457", "pdf": "https://arxiv.org/pdf/2505.10457", "abs": "https://arxiv.org/abs/2505.10457", "authors": ["Matteo Gambella", "Vicente Javier Castro Solar", "Manuel Roveri"], "title": "SEAL: Searching Expandable Architectures for Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "8 pages, 5 figures", "summary": "Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.", "AI": {"tldr": "Introduces SEAL, a NAS-based method for data-incremental learning that reduces forgetting and improves accuracy while keeping model size low.", "motivation": "Balancing plasticity and stability in incremental learning settings, especially in resource-constrained environments.", "method": "A NAS-based framework tailored for data-incremental learning, which adapts the model structure dynamically by expanding it only when necessary, based on a capacity estimation metric.", "result": "SEAL outperforms previous methods in reducing forgetting and enhancing accuracy with a smaller model size.", "conclusion": "SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods."}}
{"id": "2505.10322", "pdf": "https://arxiv.org/pdf/2505.10322", "abs": "https://arxiv.org/abs/2505.10322", "authors": ["Yijie Zhou", "Shi Pu"], "title": "Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks.", "AI": {"tldr": "This paper introduces a refined model of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under practical assumptions, analyzes its convergence, and shows it outperforms existing methods in wall-clock convergence time across various scenarios.", "motivation": "The need for leveraging distributed data without central control, enhancing scalability and privacy in decentralized optimization.", "method": "Introduces ADSGD, analyzes Asynchronous Stochastic Block Coordinate Descent (ASBCD) as a tool, and establishes convergence result without assuming bounded data heterogeneity.", "result": "ADSGD converges under computation-delay-independent step sizes and performs better than existing methods in wall-clock convergence time across various scenarios.", "conclusion": "ADSGD is simple, efficient in memory and communication, and resilient to communication and computation delays, making it suitable for real-world decentralized learning tasks."}}
{"id": "2505.10325", "pdf": "https://arxiv.org/pdf/2505.10325", "abs": "https://arxiv.org/abs/2505.10325", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "AI": {"tldr": "This paper proposes ALERT, a method that detects feature distribution changes and triggers model re-training to improve AI model performance in wireless networks.", "motivation": "In real deployment, feature distribution changes may degrade the performance of AI models and lead to undesired behaviors.", "method": "ALERT includes three components: representation learning (using MLP), statistical testing (using Kolmogorov-Smirnov and Population Stability Index tests), and utility assessment (using a new function).", "result": "The proposed method outperforms ten standard drift detection methods on two wireless network use cases.", "conclusion": "The proposed ALERT method can effectively detect feature distribution changes and trigger model re-training in wireless network use cases."}}
{"id": "2505.10330", "pdf": "https://arxiv.org/pdf/2505.10330", "abs": "https://arxiv.org/abs/2505.10330", "authors": ["Jonathan Clifford Balloch"], "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change", "categories": ["cs.LG", "cs.AI"], "comment": "PhD Dissertation, 131 pages", "summary": "Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.", "AI": {"tldr": "This work focuses on enabling real-world autonomous systems to adapt to environmental changes using improved deep RL methods.", "motivation": "Conventional RL methods struggle to adapt when conditions change as they assume a stationary environment and do not account for changes after training.", "method": "Deep reinforcement learning (RL) is used with proposed strategies for efficient online adaptation.", "result": "The proposed strategies allow RL agents to efficiently adapt their behavior during deployment without catastrophically forgetting useful prior knowledge.", "conclusion": "This dissertation demonstrates that efficient online adaptation requires two key capabilities: prioritized exploration and sampling strategies and selective preservation of prior knowledge."}}
{"id": "2505.10331", "pdf": "https://arxiv.org/pdf/2505.10331", "abs": "https://arxiv.org/abs/2505.10331", "authors": ["Luca Muscarnera", "Luigi Loreti", "Giovanni Todeschini", "Alessio Fumagalli", "Francesco Regazzoni"], "title": "Emergence of Structure in Ensembles of Random Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6a21\u578b\u6765\u7814\u7a76\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u6700\u4f18\u6e29\u5ea6\u4f7f\u5f97\u5206\u7c7b\u6548\u679c\u6700\u4f73\uff0c\u4e14\u6b64\u6027\u8d28\u4e0d\u4f9d\u8d56\u4e8e\u5177\u4f53\u6559\u5e08\u5206\u7c7b\u5668\u548c\u5206\u7c7b\u5668\u6570\u91cf\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u7cfb\u7edf\u5982\u4f55\u4ece\u5fae\u89c2\u65e0\u5e8f\u8fc7\u6e21\u5230\u5b8f\u89c2\u6709\u5e8f\u7684\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5206\u7c7b\u635f\u5931\u7684\u80fd\u91cfGibbs\u6d4b\u5ea6\u5b9a\u4e49\u52a0\u6743\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\uff0c\u5e76\u5206\u6790\u5176\u96c6\u4f53\u884c\u4e3a\u7684\u7406\u8bba\u6a21\u578b\u3002", "result": "\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u4e2a\u7279\u5b9a\u7684\u6e29\u5ea6\u53c2\u6570\u4f7f\u5f97\u5206\u7c7b\u8fbe\u5230\u6700\u4f18\uff0c\u4e14\u8fd9\u79cd\u6700\u4f18\u6027\u5177\u6709\u666e\u9002\u6027\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u73b0\u8c61\u5728MNIST\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u968f\u673a\u5206\u7c7b\u5668\u96c6\u5408\u5728\u7279\u5b9a\u6e29\u5ea6\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u6700\u4f18\u5206\u7c7b\uff0c\u4e14\u8fd9\u79cd\u6700\u4f18\u6027\u4e0e\u6559\u5e08\u5206\u7c7b\u5668\u548c\u968f\u673a\u5206\u7c7b\u5668\u7684\u6570\u91cf\u65e0\u5173\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u73b0\u8c61\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u7c7b\u6bd4\u63ed\u793a\u4e86\u5176\u81ea\u7ec4\u7ec7\u7279\u6027\u3002"}}
{"id": "2505.10344", "pdf": "https://arxiv.org/pdf/2505.10344", "abs": "https://arxiv.org/abs/2505.10344", "authors": ["Alan Jeffares", "Liyuan Liu"], "title": "An Introduction to Discrete Variational Autoencoders", "categories": ["cs.LG"], "comment": "Tutorial paper", "summary": "Variational Autoencoders (VAEs) are well-established as a principled approach\nto probabilistic unsupervised learning with neural networks. Typically, an\nencoder network defines the parameters of a Gaussian distributed latent space\nfrom which we can sample and pass realizations to a decoder network. This model\nis trained to reconstruct its inputs and is optimized through the evidence\nlower bound. In recent years, discrete latent spaces have grown in popularity,\nsuggesting that they may be a natural choice for many data modalities (e.g.\ntext). In this tutorial, we provide a rigorous, yet practical, introduction to\ndiscrete variational autoencoders -- specifically, VAEs in which the latent\nspace is made up of latent variables that follow a categorical distribution. We\nassume only a basic mathematical background with which we carefully derive each\nstep from first principles. From there, we develop a concrete training recipe\nand provide an example implementation, hosted at\nhttps://github.com/alanjeffares/discreteVAE.", "AI": {"tldr": "This tutorial introduces discrete variational autoencoders (VAEs) with categorical distributed latent variables, providing a rigorous derivation and training recipe.", "motivation": "To introduce discrete latent spaces which are becoming popular for many data modalities like text.", "method": "Using encoder and decoder networks, focusing on a categorical distribution for the latent space.", "result": "A concrete training method and an example implementation of discrete VAEs.", "conclusion": "Discrete VAEs offer a promising approach for probabilistic unsupervised learning with discrete latent variables."}}
{"id": "2505.10347", "pdf": "https://arxiv.org/pdf/2505.10347", "abs": "https://arxiv.org/abs/2505.10347", "authors": ["Gabriel S. Gama", "Valdir Grassi Jr"], "title": "Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The code will be made publicly available.", "AI": {"tldr": "Evaluate the effectiveness of Specialized Multi-Task Optimizers (SMTOs) and whether equally weighted tasks can achieve competitive results.", "motivation": "Addressing critiques suggesting SMTO results were influenced by poor hyperparameter optimization and lack of regularization.", "method": "Extensive empirical evaluation of SMTOs on complex multi-task problems.", "result": "SMTOs perform well compared to uniform loss; fixed weights can achieve competitive performance compared to SMTOs.", "conclusion": "Uniform loss can perform similarly to SMTOs in some instances."}}
{"id": "2505.10360", "pdf": "https://arxiv.org/pdf/2505.10360", "abs": "https://arxiv.org/abs/2505.10360", "authors": ["Victor Petr\u00e9n Bach Hansen", "Lasse Krogsb\u00f8ll", "Jonas Lyngs\u00f8", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maal\u00f8e"], "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": null, "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "AI": {"tldr": "Introduces a real-time AI-scribing method called Facts that extracts clinical info and generates more accurate and concise notes.", "motivation": "AI-scribing solutions for healthcare currently rely on one-shot or few-shot prompts which can lead to long notes with more hallucinations and misrepresentation of clinicians' intent, posing risks to patient safety.", "method": "Introduce a method called Facts which extracts salient clinical information in real-time and uses it recursively to generate the final note.", "result": "The FactsR method results in more accurate and concise notes and opens up new use cases within real-time decision support.", "conclusion": "This approach places the clinician-in-the-loop of note generation and improves patient safety."}}
{"id": "2505.10392", "pdf": "https://arxiv.org/pdf/2505.10392", "abs": "https://arxiv.org/abs/2505.10392", "authors": ["Aryan Mishra", "Lizhen Lin"], "title": "Schreier-Coset Graph Propagation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure , preprint", "summary": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.", "AI": {"tldr": "This paper presents Schrier-Coset Graph Propagation (SCGP), an efficient method for improving long-range message passing in graph neural networks.", "motivation": "Existing methods to solve the over-squashing problem in GNNs, such as graph rewiring and bottleneck-resistant architectures like Cayley and expander graphs, have introduced scalability issues.", "method": "SCGP uses Schreier-coset embeddings to enrich node features, which helps to avoid over-squashing and improves the network's expressive capacity.", "result": "SCGP has been shown to achieve performance on par with or exceeding expander graph and rewired GNN baselines, particularly excelling in processing hierarchical and modular graph structures.", "conclusion": "Schrier-Coset Graph Propagation (SCGP) introduces a new way to improve long-range message passing in graph neural networks without increasing memory usage."}}
{"id": "2505.10407", "pdf": "https://arxiv.org/pdf/2505.10407", "abs": "https://arxiv.org/abs/2505.10407", "authors": ["Wenhao Ding", "Choon Hwai Yap", "Kangjun Ji", "Sim\u00e3o Castro"], "title": "Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning", "categories": ["cs.LG", "68T07"], "comment": "10 pages, 2 figures", "summary": "A generative model for the mesh geometry of intracranial aneurysms (IA) is\ncrucial for training networks to predict blood flow forces in real time, which\nis a key factor affecting disease progression. This need is necessitated by the\nabsence of a large IA image datasets. Existing shape generation methods\nstruggle to capture realistic IA features and ignore the relationship between\nIA pouches and parent vessels, limiting physiological realism and their\ngeneration cannot be controlled to have specific morphological measurements. We\npropose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh\ngenerator. In the first stage, AneuG generates low-dimensional Graph Harmonic\nDeformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,\nconstrained to morphing energy statistics truths. GHD enables more accurate\nshape encoding than alternatives. In the second stage, AneuG generates parent\nvessels conditioned on GHD tokens, by generating vascular centreline and\npropagating the cross-section. AneuG's IA shape generation can further be\nconditioned to have specific clinically relevant morphological measurements.\nThis is useful for studies to understand shape variations represented by\nclinical measurements, and for flow simulation studies to understand effects of\nspecific clinical shape parameters on fluid dynamics. Source code and\nimplementation details are available at\nhttps://github.com/anonymousaneug/AneuG.", "AI": {"tldr": "AneuG is a novel two-stage VAE-based IA mesh generator that can generate realistic intracranial aneurysm pouch shapes and parent vessels with controlled morphological measurements.", "motivation": "The lack of a large IA image dataset and the limitations of existing shape generation methods motivated the development of AneuG.", "method": "AneuG uses a two-stage approach where it first generates low-dimensional GHD tokens to encode and reconstruct aneurysm pouch shapes, then generates parent vessels conditioned on GHD tokens.", "result": "AneuG demonstrated the ability to generate realistic IA shapes with controlled morphological measurements, which is useful for understanding shape variations and flow simulations.", "conclusion": "AneuG provides a new method for generating realistic IA shapes with controlled morphological measurements, which can benefit disease progression studies and flow simulation research."}}
{"id": "2505.10422", "pdf": "https://arxiv.org/pdf/2505.10422", "abs": "https://arxiv.org/abs/2505.10422", "authors": ["Daniel Weitekamp", "Christopher MacLellan", "Erik Harpstead", "Kenneth Koedinger"], "title": "Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency", "categories": ["cs.LG"], "comment": "To appear in CogSci 2025", "summary": "Human learning relies on specialization -- distinct cognitive mechanisms\nworking together to enable rapid learning. In contrast, most modern neural\nnetworks rely on a single mechanism: gradient descent over an objective\nfunction. This raises the question: might human learners' relatively rapid\nlearning from just tens of examples instead of tens of thousands in data-driven\ndeep learning arise from our ability to use multiple specialized mechanisms of\nlearning in combination? We investigate this question through an ablation\nanalysis of inductive human learning simulations in online tutoring\nenvironments. Comparing reinforcement learning to a more data-efficient\n3-mechanism symbolic rule induction approach, we find that decomposing learning\ninto multiple distinct mechanisms significantly improves data efficiency,\nbringing it in line with human learning. Furthermore, we show that this\ndecomposition has a greater impact on efficiency than the distinction between\nsymbolic and subsymbolic learning alone. Efforts to align data-driven machine\nlearning with human learning often overlook the stark difference in learning\nefficiency. Our findings suggest that integrating multiple specialized learning\nmechanisms may be key to bridging this gap.", "AI": {"tldr": "This study investigates whether human learners' rapid learning from tens of examples is due to their ability to use multiple specialized learning mechanisms. The research compares reinforcement learning to a symbolic rule induction approach and finds that decomposing learning into multiple mechanisms significantly improves data efficiency, aligning it with human learning.", "motivation": "To understand why human learners can learn rapidly from fewer examples compared to data-driven deep learning.", "method": "Ablation analysis of inductive human learning simulations in online tutoring environments comparing reinforcement learning to a symbolic rule induction approach.", "result": "Decomposing learning into multiple distinct mechanisms significantly improves data efficiency, bringing it in line with human learning. The decomposition has a greater impact on efficiency than the distinction between symbolic and subsymbolic learning alone.", "conclusion": "Integrating multiple specialized learning mechanisms may be key to aligning data-driven machine learning with human learning."}}
{"id": "2505.10423", "pdf": "https://arxiv.org/pdf/2505.10423", "abs": "https://arxiv.org/abs/2505.10423", "authors": ["Ari Karchmer", "Eran Malach"], "title": "The Power of Random Features and the Limits of Distribution-Free Gradient Descent", "categories": ["cs.LG"], "comment": null, "summary": "We study the relationship between gradient-based optimization of parametric\nmodels (e.g., neural networks) and optimization of linear combinations of\nrandom features. Our main result shows that if a parametric model can be\nlearned using mini-batch stochastic gradient descent (bSGD) without making\nassumptions about the data distribution, then with high probability, the target\nfunction can also be approximated using a polynomial-sized combination of\nrandom features. The size of this combination depends on the number of gradient\nsteps and numerical precision used in the bSGD process. This finding reveals\nfundamental limitations of distribution-free learning in neural networks\ntrained by gradient descent, highlighting why making assumptions about data\ndistributions is often crucial in practice. Along the way, we also introduce a\nnew theoretical framework called average probabilistic dimension complexity\n(adc), which extends the probabilistic dimension complexity developed by Kamath\net al. (2020). We prove that adc has a polynomial relationship with statistical\nquery dimension, and use this relationship to demonstrate an infinite\nseparation between adc and standard dimension complexity.", "AI": {"tldr": "This paper demonstrates that neural networks trained by gradient descent have fundamental limitations for distribution-free learning, and introduces a new theoretical framework (adc) to highlight these limitations.", "motivation": "To explore the connection between gradient-based optimization of parametric models and optimization of random features, and to investigate the fundamental limitations of distribution-free learning in neural networks trained by gradient descent.", "method": "The paper studies the relationship between gradient-based optimization and optimization of random features, introduces a new theoretical framework called average probabilistic dimension complexity (adc), and proves its relationship with statistical query dimension.", "result": "If a parametric model can be learned using mini-batch stochastic gradient descent without distribution assumptions, then the target function can be approximated using a polynomial-sized combination of random features.", "conclusion": "The paper concludes that there are fundamental limitations to distribution-free learning in neural networks trained by gradient descent, emphasizing the necessity of data distribution assumptions."}}
{"id": "2505.10425", "pdf": "https://arxiv.org/pdf/2505.10425", "abs": "https://arxiv.org/abs/2505.10425", "authors": ["Jingyao Wang", "Wenwen Qiang", "Zeen Song", "Changwen Zheng", "Hui Xiong"], "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLearning to Think (L2T)\u7684\u4fe1\u606f\u8bba\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ee5\u8f83\u5c11\u7684token\u5b9e\u73b0\u6700\u4f18\u63a8\u7406\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u91cf\u5316\u6bcf\u6b21\u4ea4\u4e92\u4e2d\u7684\u4fe1\u606f\u589e\u76ca\u6765\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u63a8\u7406\u6548\u679c\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u957f\u63a8\u7406\u94fe\u548c\u6d6a\u8d39token\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u5f3a\u5316\u5fae\u8c03\u6846\u67b6L2T\uff0c\u5b83\u5c06\u6bcf\u6b21\u67e5\u8be2-\u54cd\u5e94\u4ea4\u4e92\u89c6\u4e3a\u591a\u96c6\u5143\u7ec4\u7684\u5c42\u6b21\u5316\u4f1a\u8bdd\uff0c\u5e76\u63d0\u51fa\u901a\u7528\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u6765\u91cf\u5316\u6bcf\u6b21\u4ea4\u4e92\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6216\u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\u5668\u3002\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8ePAC-Bayes\u754c\u9650\u548cFisher\u4fe1\u606f\u77e9\u9635\u5feb\u901f\u4f30\u8ba1\u6b64\u5956\u52b1\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u5404\u79cd\u63a8\u7406\u57fa\u51c6\u548c\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0cL2T\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u90fd\u5177\u6709\u4f18\u52bf\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u679c\u548c\u6548\u7387\u3002", "conclusion": "L2T\u662f\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u6027\u5e76\u63d0\u9ad8\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4f18\u5316\u6a21\u578b\u66f4\u65b0\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u63a8\u7406\u3002"}}
{"id": "2505.10432", "pdf": "https://arxiv.org/pdf/2505.10432", "abs": "https://arxiv.org/abs/2505.10432", "authors": ["Randy J. Chase", "Katherine Haynes", "Lander Ver Hoef", "Imme Ebert-Uphoff"], "title": "Score-based diffusion nowcasting of GOES imagery", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Clouds and precipitation are important for understanding weather and climate.\nSimulating clouds and precipitation with traditional numerical weather\nprediction is challenging because of the sub-grid parameterizations required.\nMachine learning has been explored for forecasting clouds and precipitation,\nbut early machine learning methods often created blurry forecasts. In this\npaper we explore a newer method, named score-based diffusion, to nowcast (zero\nto three hour forecast) clouds and precipitation. We discuss the background and\nintuition of score-based diffusion models - thus providing a starting point for\nthe community - while exploring the methodology's use for nowcasting\ngeostationary infrared imagery. We experiment with three main types of\ndiffusion models: a standard score-based diffusion model (Diff); a residual\ncorrection diffusion model (CorrDiff); and a latent diffusion model (LDM). Our\nresults show that the diffusion models are able to not only advect existing\nclouds, but also generate and decay clouds, including convective initiation.\nThese results are surprising because the forecasts are initiated with only the\npast 20 mins of infrared satellite imagery. A case study qualitatively shows\nthe preservation of high resolution features longer into the forecast than a\nconventional mean-squared error trained U-Net. The best of the three diffusion\nmodels tested was the CorrDiff approach, outperforming all other diffusion\nmodels, the traditional U-Net, and a persistence forecast by one to two kelvin\non root mean squared error. The diffusion models also enable out-of-the-box\nensemble generation, which shows skillful calibration, with the spread of the\nensemble correlating well to the error.", "AI": {"tldr": "This paper explores using score-based diffusion models to nowcast clouds and precipitation over zero to three hours. It compares different types of diffusion models and finds that a residual correction diffusion model performs best.", "motivation": "Traditional methods for simulating clouds and precipitation are challenging due to sub-grid parameterizations. Early machine learning methods often produced blurry forecasts.", "method": "The authors experimented with three types of diffusion models: standard score-based diffusion model, residual correction diffusion model, and latent diffusion model.", "result": "Diffusion models were able to advect, generate, and decay clouds, including convective initiation, using only past infrared satellite imagery. The residual correction diffusion model performed best.", "conclusion": "Score-based diffusion models show promise for nowcasting clouds and precipitation, particularly in preserving high-resolution features and enabling ensemble generation with calibrated spread."}}
{"id": "2505.10438", "pdf": "https://arxiv.org/pdf/2505.10438", "abs": "https://arxiv.org/abs/2505.10438", "authors": ["David Grasev"], "title": "Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "51 pages, 28 figures", "summary": "Gas turbine engines represent complex highly nonlinear dynamical systems.\nDeriving their physics-based models can be challenging as it requires\nperformance characteristics, that are not always available, and one often has\nto make many simplifying assumptions. In this paper, the limitations of\nconventional experimental methods used to derive component-level and locally\nlinear parameter-varying models are discussed and addressed by employing\nidentification techniques based on data collected from standard engine\noperation under closed-loop control. The rotor dynamics were estimated using\nthe sparse identification of nonlinear dynamics. Subsequently, the autonomous\npart of the dynamics was mapped into an optimally constructed Koopman\neigenfunction space. The process included eigenvalue optimization using\nmetaheuristic algorithms and temporal projection, followed by gradient-based\neigenfunction identification. The resulting Koopman model was validated against\nan in-house reference component-level model. A globally optimal nonlinear\nfeedback controller and a Kalman estimator were then designed in the\neigenfunction space and compared to the classical and gain-scheduled\nproportional-integral controllers, as well as a proposed internal model control\napproach. The eigenmode structure allowed targeting individual modes during the\noptimization process, resulting in a better performance tuning. The results\nshowed that the Koopman-based controller outperformed the other benchmark\ncontrollers in both reference tracking and disturbance rejection, under\nsea-level and varying flight conditions, due to its global nature.", "AI": {"tldr": "This paper discusses the limitations of conventional experimental methods for deriving models of gas turbine engines and proposes an approach using data collected from standard engine operation under closed-loop control. It estimates rotor dynamics, maps autonomous dynamics into an optimally constructed Koopman eigenfunction space, and designs a globally optimal nonlinear feedback controller and a Kalman estimator. The results show that the Koopman-based controller outperforms other controllers in reference tracking and disturbance rejection.", "motivation": "To address the challenges in deriving physics-based models of gas turbine engines which require performance characteristics that are not always available and necessitate many simplifying assumptions.", "method": "Employing identification techniques based on data collected from standard engine operation under closed-loop control, estimating rotor dynamics using sparse identification of nonlinear dynamics, mapping autonomous part of dynamics into an optimally constructed Koopman eigenfunction space, and designing a globally optimal nonlinear feedback controller and a Kalman estimator in the eigenfunction space.", "result": "The Koopman-based controller outperformed other benchmark controllers in both reference tracking and disturbance rejection under sea-level and varying flight conditions.", "conclusion": "The proposed method using data collected from standard engine operation under closed-loop control and the Koopman framework is effective in improving the performance of gas turbine engines."}}
{"id": "2505.10482", "pdf": "https://arxiv.org/pdf/2505.10482", "abs": "https://arxiv.org/abs/2505.10482", "authors": ["Ningyuan Yang", "Jiaxuan Gao", "Feng Gao", "Yi Wu", "Chao Yu"], "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures", "summary": "Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.", "AI": {"tldr": "This paper introduces NCDPO, a new framework that reformulates diffusion policy as a noise-conditioned deterministic policy to improve sample efficiency and final performance in various benchmarks.", "motivation": "Existing diffusion policies can generate sub-optimal trajectories and catastrophic failures due to the sub-optimal and limited coverage of demonstration data. RL-based fine-tuning faces challenges in adapting PPO to diffusion models because of the computational intractability of action likelihood estimation during denoising.", "method": "NCDPO reformulates diffusion policy as a noise-conditioned deterministic policy, enabling tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps.", "result": "NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforms existing methods in sample efficiency and final performance across diverse benchmarks, and shows robustness to the number of denoising timesteps.", "conclusion": "NCDPO improves the performance and sample efficiency of diffusion policies in robotics and game scenarios."}}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472", "abs": "https://arxiv.org/abs/2505.10472", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u764c\u75c7\u4fe1\u606f\u751f\u6210\u4e2d\u7684\u80fd\u529b\u4e0e\u9650\u5236\uff0c\u53d1\u73b0\u901a\u7528\u578b\u6a21\u578b\u8bed\u8a00\u8d28\u91cf\u8f83\u9ad8\uff0c\u533b\u5b66\u4e13\u7528\u6a21\u578b\u53ef\u53ca\u6027\u66f4\u5f3a\uff0c\u4f46\u5747\u9700\u6539\u8fdb\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u7531\u4e8e\u516c\u4f17\u5bf9\u4e73\u817a\u764c\u548c\u5bab\u9888\u764c\u9884\u9632\u3001\u7b5b\u67e5\u548c\u6cbb\u7597\u7684\u7406\u89e3\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bca\u65ad\u5ef6\u8fdf\u548c\u6cbb\u7597\u4e0d\u8db3\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u51c6\u786e\u3001\u5b89\u5168\u4e14\u53ef\u53ca\u7684\u764c\u75c7\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u652f\u6301\u60a3\u8005\u7406\u89e3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u901a\u7528\u578b\u548c\u4e09\u79cd\u533b\u5b66\u4e13\u7528\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u8d28\u91cf\u3001\u5b89\u5168\u6027\u4e0e\u53ef\u4fe1\u5ea6\u4ee5\u53ca\u6c9f\u901a\u53ef\u53ca\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002\u4f7f\u7528\u5b9a\u91cf\u6307\u6807\u3001\u4e13\u5bb6\u5b9a\u6027\u8bc4\u5206\u4ee5\u53caWelch\u65b9\u5dee\u5206\u6790\u3001Games-Howell\u68c0\u9a8c\u548cHedges' g\u7edf\u8ba1\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u901a\u7528\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u6c9f\u901a\u6709\u6548\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1b\u800c\u533b\u5b66\u4e13\u7528\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u7136\u800c\uff0c\u533b\u5b66\u4e13\u7528\u6a21\u578b\u5f80\u5f80\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6f5c\u5728\u5371\u5bb3\u3001\u6bd2\u6027\u53ca\u504f\u89c1\uff0c\u5f71\u54cd\u4e86\u5176\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u764c\u75c7\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\u4e0e\u5c40\u9650\u6027\u3002\u901a\u7528\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u6709\u6548\u6027\u65b9\u9762\u8868\u73b0\u8f83\u597d\uff0c\u800c\u533b\u5b66\u4e13\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6c9f\u901a\u53ef\u53ca\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u7136\u800c\uff0c\u533b\u5b66\u4e13\u7528\u6a21\u578b\u5b58\u5728\u8f83\u9ad8\u7684\u6f5c\u5728\u5371\u5bb3\u3001\u6bd2\u6027\u53ca\u504f\u89c1\uff0c\u8fd9\u964d\u4f4e\u4e86\u5176\u5728\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u4e0a\u7684\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u672a\u6765\u9700\u8981\u5bf9\u6a21\u578b\u8bbe\u8ba1\u8fdb\u884c\u6709\u610f\u56fe\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u51cf\u5c11\u5371\u5bb3\u548c\u504f\u89c1\u3001\u63d0\u5347\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u6539\u5584AI\u751f\u6210\u7684\u5065\u5eb7\u5185\u5bb9\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u51c6\u786e\u3001\u5b89\u5168\u4e14\u53ef\u53ca\u7684\u6570\u5b57\u5065\u5eb7\u5de5\u5177\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2505.10515", "pdf": "https://arxiv.org/pdf/2505.10515", "abs": "https://arxiv.org/abs/2505.10515", "authors": ["Seongun Kim", "Sol A Kim", "Geonhyeong Kim", "Enver Menadjiev", "Chanwoo Lee", "Seongwook Chung", "Nari Kim", "Jaesik Choi"], "title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.", "AI": {"tldr": "Introduce a universal XAI framework named PnPXAI that supports diverse data modalities and neural network models in a Plug-and-Play manner, which can automatically detect model architectures, recommend applicable explanation methods, and optimize hyperparameters for optimal explanations.", "motivation": "Existing XAI frameworks suffer from several limitations such as limited flexibility, restricted number of supported XAI methods, and sub-optimal recommendations of explanations.", "method": "Develop a universal XAI framework named PnPXAI that supports diverse data modalities and neural network models in a Plug-and-Play manner.", "result": "The framework is validated through user surveys and showcases its versatility across various domains, including medicine and finance.", "conclusion": "PnPXAI addresses the limitations of existing XAI frameworks and provides a more flexible, adaptable, and optimized solution for real-world applications."}}
{"id": "2505.10484", "pdf": "https://arxiv.org/pdf/2505.10484", "abs": "https://arxiv.org/abs/2505.10484", "authors": ["Andrea Baisero", "Rupali Bhati", "Shuo Liu", "Aathira Pillai", "Christopher Amato"], "title": "Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Value function decomposition methods for cooperative multi-agent\nreinforcement learning compose joint values from individual per-agent\nutilities, and train them using a joint objective. To ensure that the action\nselection process between individual utilities and joint values remains\nconsistent, it is imperative for the composition to satisfy the\nindividual-global max (IGM) property. Although satisfying IGM itself is\nstraightforward, most existing methods (e.g., VDN, QMIX) have limited\nrepresentation capabilities and are unable to represent the full class of IGM\nvalues, and the one exception that has no such limitation (QPLEX) is\nunnecessarily complex. In this work, we present a simple formulation of the\nfull class of IGM values that naturally leads to the derivation of QFIX, a\nnovel family of value function decomposition models that expand the\nrepresentation capabilities of prior models by means of a thin \"fixing\" layer.\nWe derive multiple variants of QFIX, and implement three variants in two\nwell-known multi-agent frameworks. We perform an empirical evaluation on\nmultiple SMACv2 and Overcooked environments, which confirms that QFIX (i)\nsucceeds in enhancing the performance of prior methods, (ii) learns more stably\nand performs better than its main competitor QPLEX, and (iii) achieves this\nwhile employing the simplest and smallest mixing models.", "AI": {"tldr": "A novel family of value function decomposition models called QFIX is proposed to enhance the performance of cooperative multi-agent reinforcement learning.", "motivation": "Existing methods have limited representation capabilities or are unnecessarily complex.", "method": "QFIX expands the representation capabilities of prior models through a thin 'fixing' layer.", "result": "QFIX outperforms previous methods, learns more stably, and performs better than QPLEX.", "conclusion": "The introduction of QFIX provides a simpler and more effective solution for value function decomposition in cooperative multi-agent reinforcement learning."}}
{"id": "2505.10559", "pdf": "https://arxiv.org/pdf/2505.10559", "abs": "https://arxiv.org/abs/2505.10559", "authors": ["Ziming Liu", "Yizhou Liu", "Jeff Gore", "Max Tegmark"], "title": "Neural Thermodynamic Laws for Large Language Model Training", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "comment": "18 pages, 10 figures", "summary": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.", "AI": {"tldr": "Introduces Neural Thermodynamic Laws (NTL), a framework that reveals new insights into LLM training dynamics by showing how thermodynamic concepts emerge from river-valley loss landscapes.", "motivation": "To understand the laws underlying large language models beyond neural scaling laws.", "method": "Demonstrates emergence of thermodynamic quantities and principles under river-valley loss landscape assumptions.", "result": "Provides intuitive guidelines for designing learning rate schedules.", "conclusion": "Introduces a new framework, NTL, offering fresh perspectives on LLM training dynamics."}}
{"id": "2505.10545", "pdf": "https://arxiv.org/pdf/2505.10545", "abs": "https://arxiv.org/abs/2505.10545", "authors": ["Amira Alakhdar", "Barnabas Poczos", "Newell Washburn"], "title": "Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design", "categories": ["cs.LG"], "comment": null, "summary": "Developing bioactive molecules remains a central, time- and cost-heavy\nchallenge in drug discovery, particularly for novel targets lacking structural\nor functional data. Pharmacophore modeling presents an alternative for\ncapturing the key features required for molecular bioactivity against a\nbiological target. In this work, we present PharmaDiff, a\npharmacophore-conditioned diffusion model for 3D molecular generation.\nPharmaDiff employs a transformer-based architecture to integrate an atom-based\nrepresentation of the 3D pharmacophore into the generative process, enabling\nthe precise generation of 3D molecular graphs that align with predefined\npharmacophore hypotheses. Through comprehensive testing, PharmaDiff\ndemonstrates superior performance in matching 3D pharmacophore constraints\ncompared to ligand-based drug design methods. Additionally, it achieves higher\ndocking scores across a range of proteins in structure-based drug design,\nwithout the need for target protein structures. By integrating pharmacophore\nmodeling with 3D generative techniques, PharmaDiff offers a powerful and\nflexible framework for rational drug design.", "AI": {"tldr": "PharmaDiff is a new method for generating 3D molecular graphs that integrates pharmacophore modeling with 3D generative techniques, showing better performance than traditional methods in drug design.", "motivation": "Developing bioactive molecules is a major challenge in drug discovery, especially for novel targets without structural or functional data. Pharmacophore modeling can capture the key features needed for molecular bioactivity.", "method": "PharmaDiff is a pharmacophore-conditioned diffusion model for 3D molecular generation which uses a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process.", "result": "PharmaDiff outperforms ligand-based drug design methods in matching 3D pharmacophore constraints and achieves higher docking scores across a range of proteins in structure-based drug design without requiring target protein structures.", "conclusion": "PharmaDiff provides a powerful and flexible framework for rational drug design by integrating pharmacophore modeling with 3D generative techniques."}}
{"id": "2505.10556", "pdf": "https://arxiv.org/pdf/2505.10556", "abs": "https://arxiv.org/abs/2505.10556", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "categories": ["cs.LG", "physics.ao-ph"], "comment": "Kermani and Naderi share first authorship. 20 pages, 6 figures and 1\n  table", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "AI": {"tldr": "Use AI and wearable device data to predict how air pollution affects an individual's health.", "motivation": "Air pollution harms health and climate change makes it worse. New data collection methods and AI advancements offer opportunities to improve healthcare.", "method": "Integrate wearable device data with environmental exposure data to train an AI model using adversarial autoencoders and transfer learning.", "result": "AI model accurately predicts health responses to pollution and adapts well to real-world user data.", "conclusion": "This new workflow can help monitor and predict personalized health responses to pollution exposure."}}
